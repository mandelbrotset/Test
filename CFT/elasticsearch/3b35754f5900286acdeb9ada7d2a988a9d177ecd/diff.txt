diff --git a/core/src/main/java/org/elasticsearch/action/ActionModule.java b/core/src/main/java/org/elasticsearch/action/ActionModule.java
index 67f256c..39aa4b7 100644
--- a/core/src/main/java/org/elasticsearch/action/ActionModule.java
+++ b/core/src/main/java/org/elasticsearch/action/ActionModule.java
@@ -149,6 +149,16 @@ import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.get.TransportGetIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.put.TransportPutIndexedScriptAction;
+import org.elasticsearch.action.ingest.IngestActionFilter;
+import org.elasticsearch.action.ingest.IngestProxyActionFilter;
+import org.elasticsearch.action.ingest.DeletePipelineAction;
+import org.elasticsearch.action.ingest.DeletePipelineTransportAction;
+import org.elasticsearch.action.ingest.GetPipelineAction;
+import org.elasticsearch.action.ingest.GetPipelineTransportAction;
+import org.elasticsearch.action.ingest.PutPipelineAction;
+import org.elasticsearch.action.ingest.PutPipelineTransportAction;
+import org.elasticsearch.action.ingest.SimulatePipelineAction;
+import org.elasticsearch.action.ingest.SimulatePipelineTransportAction;
 import org.elasticsearch.action.percolate.MultiPercolateAction;
 import org.elasticsearch.action.percolate.PercolateAction;
 import org.elasticsearch.action.percolate.TransportMultiPercolateAction;
@@ -186,6 +196,8 @@ import org.elasticsearch.action.update.UpdateAction;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.multibindings.MapBinder;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.NodeModule;
 
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -210,13 +222,13 @@ public class ActionModule extends AbstractModule {
             this.transportAction = transportAction;
             this.supportTransportActions = supportTransportActions;
         }
-
-
     }
 
+    private final boolean ingestEnabled;
     private final boolean proxy;
 
-    public ActionModule(boolean proxy) {
+    public ActionModule(boolean ingestEnabled, boolean proxy) {
+        this.ingestEnabled = ingestEnabled;
         this.proxy = proxy;
     }
 
@@ -240,6 +252,13 @@ public class ActionModule extends AbstractModule {
 
     @Override
     protected void configure() {
+        if (proxy == false) {
+            if (ingestEnabled) {
+                registerFilter(IngestActionFilter.class);
+            } else {
+                registerFilter(IngestProxyActionFilter.class);
+            }
+        }
 
         Multibinder<ActionFilter> actionFilterMultibinder = Multibinder.newSetBinder(binder(), ActionFilter.class);
         for (Class<? extends ActionFilter> actionFilter : actionFilters) {
@@ -340,6 +359,11 @@ public class ActionModule extends AbstractModule {
 
         registerAction(FieldStatsAction.INSTANCE, TransportFieldStatsTransportAction.class);
 
+        registerAction(PutPipelineAction.INSTANCE, PutPipelineTransportAction.class);
+        registerAction(GetPipelineAction.INSTANCE, GetPipelineTransportAction.class);
+        registerAction(DeletePipelineAction.INSTANCE, DeletePipelineTransportAction.class);
+        registerAction(SimulatePipelineAction.INSTANCE, SimulatePipelineTransportAction.class);
+
         // register Name -> GenericAction Map that can be injected to instances.
         MapBinder<String, GenericAction> actionsBinder
                 = MapBinder.newMapBinder(binder(), String.class, GenericAction.class);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index a93bdb4..6d6bbd6 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -49,7 +49,6 @@ import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchLocalRequest;
@@ -76,20 +75,17 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
 
     private final BigArrays bigArrays;
 
-    private final FetchPhase fetchPhase;
-
     @Inject
     public TransportValidateQueryAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-            TransportService transportService, IndicesService indicesService, ScriptService scriptService,
-            PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ActionFilters actionFilters,
-            IndexNameExpressionResolver indexNameExpressionResolver, FetchPhase fetchPhase) {
+                                        TransportService transportService, IndicesService indicesService,
+                                        ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                        BigArrays bigArrays, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ValidateQueryAction.NAME, threadPool, clusterService, transportService, actionFilters,
                 indexNameExpressionResolver, ValidateQueryRequest::new, ShardValidateQueryRequest::new, ThreadPool.Names.SEARCH);
         this.indicesService = indicesService;
         this.scriptService = scriptService;
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays;
-        this.fetchPhase = fetchPhase;
     }
 
     @Override
@@ -175,9 +171,11 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
         Engine.Searcher searcher = indexShard.acquireSearcher("validate_query");
 
         DefaultSearchContext searchContext = new DefaultSearchContext(0,
-                new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()), null, searcher,
-                indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(),
-                parseFieldMatcher, SearchService.NO_TIMEOUT, fetchPhase);
+                new ShardSearchLocalRequest(request.types(), request.nowInMillis(), request.filteringAliases()),
+                null, searcher, indexService, indexShard,
+                scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
+                SearchService.NO_TIMEOUT
+        );
         SearchContext.setCurrent(searchContext);
         try {
             searchContext.parsedQuery(queryShardContext.toQuery(request.query()));
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
index 9a7299a..c54b358 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
@@ -289,11 +289,11 @@ public class BulkProcessor implements Closeable {
     }
 
     public BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType) throws Exception {
-        return add(data, defaultIndex, defaultType, null);
+        return add(data, defaultIndex, defaultType, null, null);
     }
 
-    public synchronized BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable Object payload) throws Exception {
-        bulkRequest.add(data, defaultIndex, defaultType, null, null, payload, true);
+    public synchronized BulkProcessor add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultPipeline, @Nullable Object payload) throws Exception {
+        bulkRequest.add(data, defaultIndex, defaultType, null, null, defaultPipeline, payload, true);
         executeIfNeeded();
         return this;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
index 0026064..3bc08d3 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java
@@ -28,6 +28,7 @@ import org.elasticsearch.action.delete.DeleteRequest;
 import org.elasticsearch.action.index.IndexRequest;
 import org.elasticsearch.action.update.UpdateRequest;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -253,17 +254,17 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
      * Adds a framed data in binary format
      */
     public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType) throws Exception {
-        return add(data, defaultIndex, defaultType, null, null, null, true);
+        return add(data, defaultIndex, defaultType, null, null, null, null, true);
     }
 
     /**
      * Adds a framed data in binary format
      */
     public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, boolean allowExplicitIndex) throws Exception {
-        return add(data, defaultIndex, defaultType, null, null, null, allowExplicitIndex);
+        return add(data, defaultIndex, defaultType, null, null, null, null, allowExplicitIndex);
     }
 
-    public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultRouting, @Nullable String[] defaultFields, @Nullable Object payload, boolean allowExplicitIndex) throws Exception {
+    public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultRouting, @Nullable String[] defaultFields, @Nullable String defaultPipeline, @Nullable Object payload, boolean allowExplicitIndex) throws Exception {
         XContent xContent = XContentFactory.xContent(data);
         int line = 0;
         int from = 0;
@@ -304,6 +305,7 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                 long version = Versions.MATCH_ANY;
                 VersionType versionType = VersionType.INTERNAL;
                 int retryOnConflict = 0;
+                String pipeline = defaultPipeline;
 
                 // at this stage, next token can either be END_OBJECT (and use default index and type, with auto generated id)
                 // or START_OBJECT which will have another set of parameters
@@ -344,6 +346,8 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                                 versionType = VersionType.fromString(parser.text());
                             } else if ("_retry_on_conflict".equals(currentFieldName) || "_retryOnConflict".equals(currentFieldName)) {
                                 retryOnConflict = parser.intValue();
+                            } else if ("pipeline".equals(currentFieldName)) {
+                                pipeline = parser.text();
                             } else if ("fields".equals(currentFieldName)) {
                                 throw new IllegalArgumentException("Action/metadata line [" + line + "] contains a simple value for parameter [fields] while a list is expected");
                             } else {
@@ -380,15 +384,15 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
                     if ("index".equals(action)) {
                         if (opType == null) {
                             internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                    .source(data.slice(from, nextMarker - from)), payload);
+                                    .setPipeline(pipeline).source(data.slice(from, nextMarker - from)), payload);
                         } else {
                             internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                    .create("create".equals(opType))
+                                    .create("create".equals(opType)).setPipeline(pipeline)
                                     .source(data.slice(from, nextMarker - from)), payload);
                         }
                     } else if ("create".equals(action)) {
                         internalAdd(new IndexRequest(index, type, id).routing(routing).parent(parent).timestamp(timestamp).ttl(ttl).version(version).versionType(versionType)
-                                .create(true)
+                                .create(true).setPipeline(pipeline)
                                 .source(data.slice(from, nextMarker - from)), payload);
                     } else if ("update".equals(action)) {
                         UpdateRequest updateRequest = new UpdateRequest(index, type, id).routing(routing).parent(parent).retryOnConflict(retryOnConflict)
@@ -479,6 +483,22 @@ public class BulkRequest extends ActionRequest<BulkRequest> implements Composite
         return -1;
     }
 
+    /**
+     * @return Whether this bulk request contains index request with an ingest pipeline enabled.
+     */
+    public boolean hasIndexRequestsWithPipelines() {
+        for (ActionRequest actionRequest : requests) {
+            if (actionRequest instanceof IndexRequest) {
+                IndexRequest indexRequest = (IndexRequest) actionRequest;
+                if (Strings.hasText(indexRequest.getPipeline())) {
+                    return true;
+                }
+            }
+        }
+
+        return false;
+    }
+
     @Override
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
diff --git a/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java b/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
index 1a91b89..7b6253c 100644
--- a/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
+++ b/core/src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java
@@ -44,7 +44,6 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchService;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.internal.ShardSearchLocalRequest;
@@ -69,20 +68,17 @@ public class TransportExplainAction extends TransportSingleShardAction<ExplainRe
 
     private final BigArrays bigArrays;
 
-    private final FetchPhase fetchPhase;
-
     @Inject
     public TransportExplainAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
-            TransportService transportService, IndicesService indicesService, ScriptService scriptService,
-            PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ActionFilters actionFilters,
-            IndexNameExpressionResolver indexNameExpressionResolver, FetchPhase fetchPhase) {
+                                  TransportService transportService, IndicesService indicesService,
+                                  ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                  BigArrays bigArrays, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ExplainAction.NAME, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver,
                 ExplainRequest::new, ThreadPool.Names.GET);
         this.indicesService = indicesService;
         this.scriptService = scriptService;
         this.pageCacheRecycler = pageCacheRecycler;
         this.bigArrays = bigArrays;
-        this.fetchPhase = fetchPhase;
     }
 
     @Override
@@ -115,10 +111,13 @@ public class TransportExplainAction extends TransportSingleShardAction<ExplainRe
             return new ExplainResponse(shardId.getIndex(), request.type(), request.id(), false);
         }
 
-        SearchContext context = new DefaultSearchContext(0,
-                new ShardSearchLocalRequest(new String[] { request.type() }, request.nowInMillis, request.filteringAlias()), null,
-                result.searcher(), indexService, indexShard, scriptService, pageCacheRecycler, bigArrays,
-                threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, SearchService.NO_TIMEOUT, fetchPhase);
+        SearchContext context = new DefaultSearchContext(
+                0, new ShardSearchLocalRequest(new String[]{request.type()}, request.nowInMillis, request.filteringAlias()),
+                null, result.searcher(), indexService, indexShard,
+                scriptService, pageCacheRecycler,
+                bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
+                SearchService.NO_TIMEOUT
+        );
         SearchContext.setCurrent(context);
 
         try {
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
index 9899a54..387f756 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java
@@ -155,6 +155,8 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
 
     private XContentType contentType = Requests.INDEX_CONTENT_TYPE;
 
+    private String pipeline;
+
     public IndexRequest() {
     }
 
@@ -364,6 +366,21 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
     }
 
     /**
+     * Sets the ingest pipeline to be executed before indexing the document
+     */
+    public IndexRequest setPipeline(String pipeline) {
+        this.pipeline = pipeline;
+        return this;
+    }
+
+    /**
+     * Returns the ingest pipeline to be executed before indexing the document
+     */
+    public String getPipeline() {
+        return this.pipeline;
+    }
+
+    /**
      * The source of the document to index, recopied to a new array if it is unsage.
      */
     public BytesReference source() {
@@ -658,6 +675,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         refresh = in.readBoolean();
         version = in.readLong();
         versionType = VersionType.fromValue(in.readByte());
+        pipeline = in.readOptionalString();
     }
 
     @Override
@@ -679,6 +697,7 @@ public class IndexRequest extends ReplicationRequest<IndexRequest> implements Do
         out.writeBoolean(refresh);
         out.writeLong(version);
         out.writeByte(versionType.getValue());
+        out.writeOptionalString(pipeline);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
index f7134d8..4116755 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java
@@ -278,4 +278,12 @@ public class IndexRequestBuilder extends ReplicationRequestBuilder<IndexRequest,
         request.ttl(ttl);
         return this;
     }
+
+    /**
+     * Sets the ingest pipeline to be executed before indexing the document
+     */
+    public IndexRequestBuilder setPipeline(String pipeline) {
+        request.setPipeline(pipeline);
+        return this;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java
new file mode 100644
index 0000000..ba1dd5d
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineAction.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.client.ElasticsearchClient;
+
+public class DeletePipelineAction extends Action<DeletePipelineRequest, WritePipelineResponse, DeletePipelineRequestBuilder> {
+
+    public static final DeletePipelineAction INSTANCE = new DeletePipelineAction();
+    public static final String NAME = "cluster:admin/ingest/pipeline/delete";
+
+    public DeletePipelineAction() {
+        super(NAME);
+    }
+
+    @Override
+    public DeletePipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
+        return new DeletePipelineRequestBuilder(client, this);
+    }
+
+    @Override
+    public WritePipelineResponse newResponse() {
+        return new WritePipelineResponse();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java
new file mode 100644
index 0000000..6e5b9d8
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequest.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionRequestValidationException;
+import org.elasticsearch.action.support.master.AcknowledgedRequest;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+
+import java.io.IOException;
+import java.util.Objects;
+
+import static org.elasticsearch.action.ValidateActions.addValidationError;
+
+public class DeletePipelineRequest extends AcknowledgedRequest<DeletePipelineRequest> {
+
+    private String id;
+
+    public DeletePipelineRequest(String id) {
+        if (id == null) {
+            throw new IllegalArgumentException("id is missing");
+        }
+        this.id = id;
+    }
+
+    DeletePipelineRequest() {
+    }
+
+    public void setId(String id) {
+        this.id = Objects.requireNonNull(id);
+    }
+
+    public String getId() {
+        return id;
+    }
+
+    @Override
+    public ActionRequestValidationException validate() {
+        return null;
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        id = in.readString();
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        out.writeString(id);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java
new file mode 100644
index 0000000..fc14e0d
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineRequestBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.client.ElasticsearchClient;
+
+public class DeletePipelineRequestBuilder extends ActionRequestBuilder<DeletePipelineRequest, WritePipelineResponse, DeletePipelineRequestBuilder> {
+
+    public DeletePipelineRequestBuilder(ElasticsearchClient client, DeletePipelineAction action) {
+        super(client, action, new DeletePipelineRequest());
+    }
+
+    public DeletePipelineRequestBuilder(ElasticsearchClient client, DeletePipelineAction action, String id) {
+        super(client, action, new DeletePipelineRequest(id));
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java
new file mode 100644
index 0000000..6378eb5
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/DeletePipelineTransportAction.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.master.TransportMasterNodeAction;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.block.ClusterBlockException;
+import org.elasticsearch.cluster.block.ClusterBlockLevel;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.ingest.PipelineStore;
+import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+public class DeletePipelineTransportAction extends TransportMasterNodeAction<DeletePipelineRequest, WritePipelineResponse> {
+
+    private final PipelineStore pipelineStore;
+    private final ClusterService clusterService;
+
+    @Inject
+    public DeletePipelineTransportAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
+                                         TransportService transportService, ActionFilters actionFilters,
+                                         IndexNameExpressionResolver indexNameExpressionResolver, NodeService nodeService) {
+        super(settings, DeletePipelineAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, DeletePipelineRequest::new);
+        this.clusterService = clusterService;
+        this.pipelineStore = nodeService.getIngestService().getPipelineStore();
+    }
+
+    @Override
+    protected String executor() {
+        return ThreadPool.Names.SAME;
+    }
+
+    @Override
+    protected WritePipelineResponse newResponse() {
+        return new WritePipelineResponse();
+    }
+
+    @Override
+    protected void masterOperation(DeletePipelineRequest request, ClusterState state, ActionListener<WritePipelineResponse> listener) throws Exception {
+        pipelineStore.delete(clusterService, request, listener);
+    }
+
+    @Override
+    protected ClusterBlockException checkBlock(DeletePipelineRequest request, ClusterState state) {
+        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java
new file mode 100644
index 0000000..f6bc3d9
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineAction.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.client.ElasticsearchClient;
+
+public class GetPipelineAction extends Action<GetPipelineRequest, GetPipelineResponse, GetPipelineRequestBuilder> {
+
+    public static final GetPipelineAction INSTANCE = new GetPipelineAction();
+    public static final String NAME = "cluster:admin/ingest/pipeline/get";
+
+    public GetPipelineAction() {
+        super(NAME);
+    }
+
+    @Override
+    public GetPipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
+        return new GetPipelineRequestBuilder(client, this);
+    }
+
+    @Override
+    public GetPipelineResponse newResponse() {
+        return new GetPipelineResponse();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java
new file mode 100644
index 0000000..6525c26
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequest.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionRequestValidationException;
+import org.elasticsearch.action.support.master.MasterNodeReadRequest;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+
+import java.io.IOException;
+import java.util.Objects;
+
+import static org.elasticsearch.action.ValidateActions.addValidationError;
+
+public class GetPipelineRequest extends MasterNodeReadRequest<GetPipelineRequest> {
+
+    private String[] ids;
+
+    public GetPipelineRequest(String... ids) {
+        if (ids == null || ids.length == 0) {
+            throw new IllegalArgumentException("No ids specified");
+        }
+        this.ids = ids;
+    }
+
+    GetPipelineRequest() {
+    }
+
+    public String[] getIds() {
+        return ids;
+    }
+
+    @Override
+    public ActionRequestValidationException validate() {
+        return null;
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        ids = in.readStringArray();
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        out.writeStringArray(ids);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java
new file mode 100644
index 0000000..f96a5ff
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineRequestBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.support.master.MasterNodeReadOperationRequestBuilder;
+import org.elasticsearch.client.ElasticsearchClient;
+
+public class GetPipelineRequestBuilder extends MasterNodeReadOperationRequestBuilder<GetPipelineRequest, GetPipelineResponse, GetPipelineRequestBuilder> {
+
+    public GetPipelineRequestBuilder(ElasticsearchClient client, GetPipelineAction action) {
+        super(client, action, new GetPipelineRequest());
+    }
+
+    public GetPipelineRequestBuilder(ElasticsearchClient client, GetPipelineAction action, String[] ids) {
+        super(client, action, new GetPipelineRequest(ids));
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java
new file mode 100644
index 0000000..9f0b229
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineResponse.java
@@ -0,0 +1,86 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.StatusToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.ingest.PipelineConfiguration;
+import org.elasticsearch.rest.RestStatus;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+public class GetPipelineResponse extends ActionResponse implements StatusToXContent {
+
+    private List<PipelineConfiguration> pipelines;
+
+    public GetPipelineResponse() {
+    }
+
+    public GetPipelineResponse(List<PipelineConfiguration> pipelines) {
+        this.pipelines = pipelines;
+    }
+
+    public List<PipelineConfiguration> pipelines() {
+        return pipelines;
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        int size = in.readVInt();
+        pipelines = new ArrayList<>(size);
+        for (int i = 0; i < size; i++) {
+            pipelines.add(PipelineConfiguration.readPipelineConfiguration(in));
+        }
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        out.writeVInt(pipelines.size());
+        for (PipelineConfiguration pipeline : pipelines) {
+            pipeline.writeTo(out);
+        }
+    }
+
+    public boolean isFound() {
+        return !pipelines.isEmpty();
+    }
+
+    @Override
+    public RestStatus status() {
+        return isFound() ? RestStatus.OK : RestStatus.NOT_FOUND;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startArray("pipelines");
+        for (PipelineConfiguration pipeline : pipelines) {
+            pipeline.toXContent(builder, params);
+        }
+        builder.endArray();
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java
new file mode 100644
index 0000000..e762d0b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/GetPipelineTransportAction.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.block.ClusterBlockException;
+import org.elasticsearch.cluster.block.ClusterBlockLevel;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.ingest.PipelineStore;
+import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+public class GetPipelineTransportAction extends TransportMasterNodeReadAction<GetPipelineRequest, GetPipelineResponse> {
+
+    private final PipelineStore pipelineStore;
+
+    @Inject
+    public GetPipelineTransportAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
+                                      TransportService transportService, ActionFilters actionFilters,
+                                      IndexNameExpressionResolver indexNameExpressionResolver, NodeService nodeService) {
+        super(settings, GetPipelineAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, GetPipelineRequest::new);
+        this.pipelineStore = nodeService.getIngestService().getPipelineStore();
+    }
+
+    @Override
+    protected String executor() {
+        return ThreadPool.Names.SAME;
+    }
+
+    @Override
+    protected GetPipelineResponse newResponse() {
+        return new GetPipelineResponse();
+    }
+
+    @Override
+    protected void masterOperation(GetPipelineRequest request, ClusterState state, ActionListener<GetPipelineResponse> listener) throws Exception {
+        listener.onResponse(new GetPipelineResponse(pipelineStore.getPipelines(state, request.getIds())));
+    }
+
+    @Override
+    protected ClusterBlockException checkBlock(GetPipelineRequest request, ClusterState state) {
+        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_READ);
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java b/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java
new file mode 100644
index 0000000..b35e24c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java
@@ -0,0 +1,225 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.action.bulk.BulkAction;
+import org.elasticsearch.action.bulk.BulkItemResponse;
+import org.elasticsearch.action.bulk.BulkRequest;
+import org.elasticsearch.action.bulk.BulkResponse;
+import org.elasticsearch.action.index.IndexAction;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.support.ActionFilter;
+import org.elasticsearch.action.support.ActionFilterChain;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.ingest.PipelineExecutionService;
+import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.tasks.Task;
+
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Set;
+
+public final class IngestActionFilter extends AbstractComponent implements ActionFilter {
+
+    private final PipelineExecutionService executionService;
+
+    @Inject
+    public IngestActionFilter(Settings settings, NodeService nodeService) {
+        super(settings);
+        this.executionService = nodeService.getIngestService().getPipelineExecutionService();
+    }
+
+    @Override
+    public <Request extends ActionRequest<Request>, Response extends ActionResponse> void apply(Task task, String action, Request request, ActionListener<Response> listener, ActionFilterChain<Request, Response> chain) {
+        switch (action) {
+            case IndexAction.NAME:
+                IndexRequest indexRequest = (IndexRequest) request;
+                if (Strings.hasText(indexRequest.getPipeline())) {
+                    processIndexRequest(task, action, listener, chain, (IndexRequest) request);
+                } else {
+                    chain.proceed(task, action, request, listener);
+                }
+                break;
+            case BulkAction.NAME:
+                BulkRequest bulkRequest = (BulkRequest) request;
+                if (bulkRequest.hasIndexRequestsWithPipelines()) {
+                    @SuppressWarnings("unchecked")
+                    ActionListener<BulkResponse> actionListener = (ActionListener<BulkResponse>) listener;
+                    processBulkIndexRequest(task, bulkRequest, action, chain, actionListener);
+                } else {
+                    chain.proceed(task, action, request, listener);
+                }
+                break;
+            default:
+                chain.proceed(task, action, request, listener);
+                break;
+        }
+    }
+
+    @Override
+    public <Response extends ActionResponse> void apply(String action, Response response, ActionListener<Response> listener, ActionFilterChain<?, Response> chain) {
+        chain.proceed(action, response, listener);
+    }
+
+    void processIndexRequest(Task task, String action, ActionListener listener, ActionFilterChain chain, IndexRequest indexRequest) {
+
+        executionService.execute(indexRequest, t -> {
+            logger.error("failed to execute pipeline [{}]", t, indexRequest.getPipeline());
+            listener.onFailure(t);
+        }, success -> {
+            // TransportIndexAction uses IndexRequest and same action name on the node that receives the request and the node that
+            // processes the primary action. This could lead to a pipeline being executed twice for the same
+            // index request, hence we set the pipeline to null once its execution completed.
+            indexRequest.setPipeline(null);
+            chain.proceed(task, action, indexRequest, listener);
+        });
+    }
+
+    void processBulkIndexRequest(Task task, BulkRequest original, String action, ActionFilterChain chain, ActionListener<BulkResponse> listener) {
+        BulkRequestModifier bulkRequestModifier = new BulkRequestModifier(original);
+        executionService.execute(() -> bulkRequestModifier, (indexRequest, throwable) -> {
+            logger.debug("failed to execute pipeline [{}] for document [{}/{}/{}]", indexRequest.getPipeline(), indexRequest.index(), indexRequest.type(), indexRequest.id(), throwable);
+            bulkRequestModifier.markCurrentItemAsFailed(throwable);
+        }, (throwable) -> {
+            if (throwable != null) {
+                logger.error("failed to execute pipeline for a bulk request", throwable);
+                listener.onFailure(throwable);
+            } else {
+                BulkRequest bulkRequest = bulkRequestModifier.getBulkRequest();
+                ActionListener<BulkResponse> actionListener = bulkRequestModifier.wrapActionListenerIfNeeded(listener);
+                if (bulkRequest.requests().isEmpty()) {
+                    // at this stage, the transport bulk action can't deal with a bulk request with no requests,
+                    // so we stop and send an empty response back to the client.
+                    // (this will happen if pre-processing all items in the bulk failed)
+                    actionListener.onResponse(new BulkResponse(new BulkItemResponse[0], 0));
+                } else {
+                    chain.proceed(task, action, bulkRequest, actionListener);
+                }
+            }
+        });
+    }
+
+    @Override
+    public int order() {
+        return Integer.MAX_VALUE;
+    }
+
+    final static class BulkRequestModifier implements Iterator<ActionRequest<?>> {
+
+        final BulkRequest bulkRequest;
+        final Set<Integer> failedSlots;
+        final List<BulkItemResponse> itemResponses;
+
+        int currentSlot = -1;
+        int[] originalSlots;
+
+        BulkRequestModifier(BulkRequest bulkRequest) {
+            this.bulkRequest = bulkRequest;
+            this.failedSlots = new HashSet<>();
+            this.itemResponses = new ArrayList<>(bulkRequest.requests().size());
+        }
+
+        @Override
+        public ActionRequest next() {
+            return bulkRequest.requests().get(++currentSlot);
+        }
+
+        @Override
+        public boolean hasNext() {
+            return (currentSlot + 1) < bulkRequest.requests().size();
+        }
+
+        BulkRequest getBulkRequest() {
+            if (itemResponses.isEmpty()) {
+                return bulkRequest;
+            } else {
+                BulkRequest modifiedBulkRequest = new BulkRequest(bulkRequest);
+                modifiedBulkRequest.refresh(bulkRequest.refresh());
+                modifiedBulkRequest.consistencyLevel(bulkRequest.consistencyLevel());
+                modifiedBulkRequest.timeout(bulkRequest.timeout());
+
+                int slot = 0;
+                originalSlots = new int[bulkRequest.requests().size() - failedSlots.size()];
+                for (int i = 0; i < bulkRequest.requests().size(); i++) {
+                    ActionRequest request = bulkRequest.requests().get(i);
+                    if (failedSlots.contains(i) == false) {
+                        modifiedBulkRequest.add(request);
+                        originalSlots[slot++] = i;
+                    }
+                }
+                return modifiedBulkRequest;
+            }
+        }
+
+        ActionListener<BulkResponse> wrapActionListenerIfNeeded(ActionListener<BulkResponse> actionListener) {
+            if (itemResponses.isEmpty()) {
+                return actionListener;
+            } else {
+                return new IngestBulkResponseListener(originalSlots, itemResponses, actionListener);
+            }
+        }
+
+        void markCurrentItemAsFailed(Throwable e) {
+            IndexRequest indexRequest = (IndexRequest) bulkRequest.requests().get(currentSlot);
+            // We hit a error during preprocessing a request, so we:
+            // 1) Remember the request item slot from the bulk, so that we're done processing all requests we know what failed
+            // 2) Add a bulk item failure for this request
+            // 3) Continue with the next request in the bulk.
+            failedSlots.add(currentSlot);
+            BulkItemResponse.Failure failure = new BulkItemResponse.Failure(indexRequest.index(), indexRequest.type(), indexRequest.id(), e);
+            itemResponses.add(new BulkItemResponse(currentSlot, indexRequest.opType().lowercase(), failure));
+        }
+
+    }
+
+    private final static class IngestBulkResponseListener implements ActionListener<BulkResponse> {
+
+        private final int[] originalSlots;
+        private final List<BulkItemResponse> itemResponses;
+        private final ActionListener<BulkResponse> actionListener;
+
+        IngestBulkResponseListener(int[] originalSlots, List<BulkItemResponse> itemResponses, ActionListener<BulkResponse> actionListener) {
+            this.itemResponses = itemResponses;
+            this.actionListener = actionListener;
+            this.originalSlots = originalSlots;
+        }
+
+        @Override
+        public void onResponse(BulkResponse bulkItemResponses) {
+            for (int i = 0; i < bulkItemResponses.getItems().length; i++) {
+                itemResponses.add(originalSlots[i], bulkItemResponses.getItems()[i]);
+            }
+            actionListener.onResponse(new BulkResponse(itemResponses.toArray(new BulkItemResponse[itemResponses.size()]), bulkItemResponses.getTookInMillis()));
+        }
+
+        @Override
+        public void onFailure(Throwable e) {
+            actionListener.onFailure(e);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/IngestProxyActionFilter.java b/core/src/main/java/org/elasticsearch/action/ingest/IngestProxyActionFilter.java
new file mode 100644
index 0000000..39a4b1f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/IngestProxyActionFilter.java
@@ -0,0 +1,125 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionListenerResponseHandler;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.action.bulk.BulkAction;
+import org.elasticsearch.action.bulk.BulkRequest;
+import org.elasticsearch.action.index.IndexAction;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.support.ActionFilter;
+import org.elasticsearch.action.support.ActionFilterChain;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.Randomness;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.tasks.Task;
+import org.elasticsearch.transport.TransportResponse;
+import org.elasticsearch.transport.TransportService;
+
+import java.util.concurrent.atomic.AtomicInteger;
+
+public final class IngestProxyActionFilter implements ActionFilter {
+
+    private final ClusterService clusterService;
+    private final TransportService transportService;
+    private final AtomicInteger randomNodeGenerator = new AtomicInteger(Randomness.get().nextInt());
+
+    @Inject
+    public IngestProxyActionFilter(ClusterService clusterService, TransportService transportService) {
+        this.clusterService = clusterService;
+        this.transportService = transportService;
+    }
+
+    @Override
+    public <Request extends ActionRequest<Request>, Response extends ActionResponse> void apply(Task task, String action, Request request, ActionListener<Response> listener, ActionFilterChain<Request, Response> chain) {
+        Action ingestAction;
+        switch (action) {
+            case IndexAction.NAME:
+                ingestAction = IndexAction.INSTANCE;
+                IndexRequest indexRequest = (IndexRequest) request;
+                if (Strings.hasText(indexRequest.getPipeline())) {
+                    forwardIngestRequest(ingestAction, request, listener);
+                } else {
+                    chain.proceed(task, action, request, listener);
+                }
+                break;
+            case BulkAction.NAME:
+                ingestAction = BulkAction.INSTANCE;
+                BulkRequest bulkRequest = (BulkRequest) request;
+                if (bulkRequest.hasIndexRequestsWithPipelines()) {
+                    forwardIngestRequest(ingestAction, request, listener);
+                } else {
+                    chain.proceed(task, action, request, listener);
+                }
+                break;
+            default:
+                chain.proceed(task, action, request, listener);
+                break;
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    private void forwardIngestRequest(Action<?, ?, ?> action, ActionRequest request, ActionListener<?> listener) {
+        transportService.sendRequest(randomIngestNode(), action.name(), request, new ActionListenerResponseHandler(listener) {
+            @Override
+            public TransportResponse newInstance() {
+                return action.newResponse();
+            }
+
+        });
+    }
+
+    @Override
+    public <Response extends ActionResponse> void apply(String action, Response response, ActionListener<Response> listener, ActionFilterChain<?, Response> chain) {
+        chain.proceed(action, response, listener);
+    }
+
+    @Override
+    public int order() {
+        return Integer.MAX_VALUE;
+    }
+
+    private DiscoveryNode randomIngestNode() {
+        assert clusterService.localNode().isIngestNode() == false;
+        DiscoveryNodes nodes = clusterService.state().getNodes();
+        DiscoveryNode[] ingestNodes = nodes.getIngestNodes().values().toArray(DiscoveryNode.class);
+        if (ingestNodes.length == 0) {
+            throw new IllegalStateException("There are no ingest nodes in this cluster, unable to forward request to an ingest node.");
+        }
+
+        int index = getNodeNumber();
+        return ingestNodes[(index) % ingestNodes.length];
+    }
+
+    private int getNodeNumber() {
+        int index = randomNodeGenerator.incrementAndGet();
+        if (index < 0) {
+            index = 0;
+            randomNodeGenerator.set(0);
+        }
+        return index;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java
new file mode 100644
index 0000000..8f4b417
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineAction.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.action.index.IndexResponse;
+import org.elasticsearch.client.ElasticsearchClient;
+
+public class PutPipelineAction extends Action<PutPipelineRequest, WritePipelineResponse, PutPipelineRequestBuilder> {
+
+    public static final PutPipelineAction INSTANCE = new PutPipelineAction();
+    public static final String NAME = "cluster:admin/ingest/pipeline/put";
+
+    public PutPipelineAction() {
+        super(NAME);
+    }
+
+    @Override
+    public PutPipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
+        return new PutPipelineRequestBuilder(client, this);
+    }
+
+    @Override
+    public WritePipelineResponse newResponse() {
+        return new WritePipelineResponse();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java
new file mode 100644
index 0000000..1041614
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequest.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionRequestValidationException;
+import org.elasticsearch.action.support.master.AcknowledgedRequest;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+
+import java.io.IOException;
+import java.util.Objects;
+
+import static org.elasticsearch.action.ValidateActions.addValidationError;
+
+public class PutPipelineRequest extends AcknowledgedRequest<PutPipelineRequest> {
+
+    private String id;
+    private BytesReference source;
+
+    public PutPipelineRequest(String id, BytesReference source) {
+        if (id == null) {
+            throw new IllegalArgumentException("id is missing");
+        }
+        if (source == null) {
+            throw new IllegalArgumentException("source is missing");
+        }
+
+        this.id = id;
+        this.source = source;
+    }
+
+    PutPipelineRequest() {
+    }
+
+    @Override
+    public ActionRequestValidationException validate() {
+        return null;
+    }
+
+    public String getId() {
+        return id;
+    }
+
+    public BytesReference getSource() {
+        return source;
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        id = in.readString();
+        source = in.readBytesReference();
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        out.writeString(id);
+        out.writeBytesReference(source);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java
new file mode 100644
index 0000000..bd92711
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineRequestBuilder.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.common.bytes.BytesReference;
+
+public class PutPipelineRequestBuilder extends ActionRequestBuilder<PutPipelineRequest, WritePipelineResponse, PutPipelineRequestBuilder> {
+
+    public PutPipelineRequestBuilder(ElasticsearchClient client, PutPipelineAction action) {
+        super(client, action, new PutPipelineRequest());
+    }
+
+    public PutPipelineRequestBuilder(ElasticsearchClient client, PutPipelineAction action, String id, BytesReference source) {
+        super(client, action, new PutPipelineRequest(id, source));
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java
new file mode 100644
index 0000000..31a9112
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/PutPipelineTransportAction.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.master.TransportMasterNodeAction;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.block.ClusterBlockException;
+import org.elasticsearch.cluster.block.ClusterBlockLevel;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.ingest.PipelineStore;
+import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+public class PutPipelineTransportAction extends TransportMasterNodeAction<PutPipelineRequest, WritePipelineResponse> {
+
+    private final PipelineStore pipelineStore;
+    private final ClusterService clusterService;
+
+    @Inject
+    public PutPipelineTransportAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
+                                      TransportService transportService, ActionFilters actionFilters,
+                                      IndexNameExpressionResolver indexNameExpressionResolver, NodeService nodeService) {
+        super(settings, PutPipelineAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, PutPipelineRequest::new);
+        this.clusterService = clusterService;
+        this.pipelineStore = nodeService.getIngestService().getPipelineStore();
+    }
+
+    @Override
+    protected String executor() {
+        return ThreadPool.Names.SAME;
+    }
+
+    @Override
+    protected WritePipelineResponse newResponse() {
+        return new WritePipelineResponse();
+    }
+
+    @Override
+    protected void masterOperation(PutPipelineRequest request, ClusterState state, ActionListener<WritePipelineResponse> listener) throws Exception {
+        pipelineStore.put(clusterService, request, listener);
+    }
+
+    @Override
+    protected ClusterBlockException checkBlock(PutPipelineRequest request, ClusterState state) {
+        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java
new file mode 100644
index 0000000..036703e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentBaseResult.java
@@ -0,0 +1,98 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.ingest.core.IngestDocument;
+
+import java.io.IOException;
+import java.util.Collections;
+
+/**
+ * Holds the end result of what a pipeline did to sample document provided via the simulate api.
+ */
+public final class SimulateDocumentBaseResult implements SimulateDocumentResult<SimulateDocumentBaseResult> {
+
+    private static final SimulateDocumentBaseResult PROTOTYPE = new SimulateDocumentBaseResult(new WriteableIngestDocument(new IngestDocument(Collections.emptyMap(), Collections.emptyMap())));
+
+    private WriteableIngestDocument ingestDocument;
+    private Exception failure;
+
+    public SimulateDocumentBaseResult(IngestDocument ingestDocument) {
+        this.ingestDocument = new WriteableIngestDocument(ingestDocument);
+    }
+
+    private SimulateDocumentBaseResult(WriteableIngestDocument ingestDocument) {
+        this.ingestDocument = ingestDocument;
+    }
+
+    public SimulateDocumentBaseResult(Exception failure) {
+        this.failure = failure;
+    }
+
+    public IngestDocument getIngestDocument() {
+        if (ingestDocument == null) {
+            return null;
+        }
+        return ingestDocument.getIngestDocument();
+    }
+
+    public Exception getFailure() {
+        return failure;
+    }
+
+    public static SimulateDocumentBaseResult readSimulateDocumentSimpleResult(StreamInput in) throws IOException {
+        return PROTOTYPE.readFrom(in);
+    }
+
+    @Override
+    public SimulateDocumentBaseResult readFrom(StreamInput in) throws IOException {
+        if (in.readBoolean()) {
+            Exception exception = in.readThrowable();
+            return new SimulateDocumentBaseResult(exception);
+        }
+        return new SimulateDocumentBaseResult(new WriteableIngestDocument(in));
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        if (failure == null) {
+            out.writeBoolean(false);
+            ingestDocument.writeTo(out);
+        } else {
+            out.writeBoolean(true);
+            out.writeThrowable(failure);
+        }
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (failure == null) {
+            ingestDocument.toXContent(builder, params);
+        } else {
+            ElasticsearchException.renderThrowable(builder, params, failure);
+        }
+        builder.endObject();
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java
new file mode 100644
index 0000000..7e7682b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentResult.java
@@ -0,0 +1,26 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.common.io.stream.Writeable;
+import org.elasticsearch.common.xcontent.ToXContent;
+
+public interface SimulateDocumentResult<T extends SimulateDocumentResult> extends Writeable<T>, ToXContent {
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java
new file mode 100644
index 0000000..d9d705f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulateDocumentVerboseResult.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * Holds the result of what a pipeline did to a sample document via the simulate api, but instead of {@link SimulateDocumentBaseResult}
+ * this result class holds the intermediate result each processor did to the sample document.
+ */
+public final class SimulateDocumentVerboseResult implements SimulateDocumentResult<SimulateDocumentVerboseResult> {
+
+    private static final SimulateDocumentVerboseResult PROTOTYPE = new SimulateDocumentVerboseResult(Collections.emptyList());
+
+    private final List<SimulateProcessorResult> processorResults;
+
+    public SimulateDocumentVerboseResult(List<SimulateProcessorResult> processorResults) {
+        this.processorResults = processorResults;
+    }
+
+    public List<SimulateProcessorResult> getProcessorResults() {
+        return processorResults;
+    }
+
+    public static SimulateDocumentVerboseResult readSimulateDocumentVerboseResultFrom(StreamInput in) throws IOException {
+        return PROTOTYPE.readFrom(in);
+    }
+
+    @Override
+    public SimulateDocumentVerboseResult readFrom(StreamInput in) throws IOException {
+        int size = in.readVInt();
+        List<SimulateProcessorResult> processorResults = new ArrayList<>();
+        for (int i = 0; i < size; i++) {
+            processorResults.add(new SimulateProcessorResult(in));
+        }
+        return new SimulateDocumentVerboseResult(processorResults);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeVInt(processorResults.size());
+        for (SimulateProcessorResult result : processorResults) {
+            result.writeTo(out);
+        }
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        builder.startArray("processor_results");
+        for (SimulateProcessorResult processorResult : processorResults) {
+            processorResult.toXContent(builder, params);
+        }
+        builder.endArray();
+        builder.endObject();
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java
new file mode 100644
index 0000000..30efbe1
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulateExecutionService.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionRunnable;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.core.CompoundProcessor;
+import org.elasticsearch.threadpool.ThreadPool;
+
+import java.util.ArrayList;
+import java.util.List;
+
+class SimulateExecutionService {
+
+    private static final String THREAD_POOL_NAME = ThreadPool.Names.MANAGEMENT;
+
+    private final ThreadPool threadPool;
+
+    SimulateExecutionService(ThreadPool threadPool) {
+        this.threadPool = threadPool;
+    }
+
+    void executeVerboseDocument(Processor processor, IngestDocument ingestDocument, List<SimulateProcessorResult> processorResultList) throws Exception {
+        if (processor instanceof CompoundProcessor) {
+            CompoundProcessor cp = (CompoundProcessor) processor;
+            try {
+                for (Processor p : cp.getProcessors()) {
+                    executeVerboseDocument(p, ingestDocument, processorResultList);
+                }
+            } catch (Exception e) {
+                for (Processor p : cp.getOnFailureProcessors()) {
+                    executeVerboseDocument(p, ingestDocument, processorResultList);
+                }
+            }
+        } else {
+            try {
+                processor.execute(ingestDocument);
+                processorResultList.add(new SimulateProcessorResult(processor.getTag(), new IngestDocument(ingestDocument)));
+            } catch (Exception e) {
+                processorResultList.add(new SimulateProcessorResult(processor.getTag(), e));
+                throw e;
+            }
+        }
+    }
+
+    SimulateDocumentResult executeDocument(Pipeline pipeline, IngestDocument ingestDocument, boolean verbose) {
+        if (verbose) {
+            List<SimulateProcessorResult> processorResultList = new ArrayList<>();
+            IngestDocument currentIngestDocument = new IngestDocument(ingestDocument);
+            CompoundProcessor pipelineProcessor = new CompoundProcessor(pipeline.getProcessors(), pipeline.getOnFailureProcessors());
+            try {
+                executeVerboseDocument(pipelineProcessor, currentIngestDocument, processorResultList);
+            } catch (Exception e) {
+                return new SimulateDocumentBaseResult(e);
+            }
+            return new SimulateDocumentVerboseResult(processorResultList);
+        } else {
+            try {
+                pipeline.execute(ingestDocument);
+                return new SimulateDocumentBaseResult(ingestDocument);
+            } catch (Exception e) {
+                return new SimulateDocumentBaseResult(e);
+            }
+        }
+    }
+
+    public void execute(SimulatePipelineRequest.Parsed request, ActionListener<SimulatePipelineResponse> listener) {
+        threadPool.executor(THREAD_POOL_NAME).execute(new ActionRunnable<SimulatePipelineResponse>(listener) {
+            @Override
+            protected void doRun() throws Exception {
+                List<SimulateDocumentResult> responses = new ArrayList<>();
+                for (IngestDocument ingestDocument : request.getDocuments()) {
+                    responses.add(executeDocument(request.getPipeline(), ingestDocument, request.isVerbose()));
+                }
+                listener.onResponse(new SimulatePipelineResponse(request.getPipeline().getId(), request.isVerbose(), responses));
+            }
+        });
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java
new file mode 100644
index 0000000..c1d219a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineAction.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.client.ElasticsearchClient;
+
+public class SimulatePipelineAction extends Action<SimulatePipelineRequest, SimulatePipelineResponse, SimulatePipelineRequestBuilder> {
+
+    public static final SimulatePipelineAction INSTANCE = new SimulatePipelineAction();
+    public static final String NAME = "cluster:admin/ingest/pipeline/simulate";
+
+    public SimulatePipelineAction() {
+        super(NAME);
+    }
+
+    @Override
+    public SimulatePipelineRequestBuilder newRequestBuilder(ElasticsearchClient client) {
+        return new SimulatePipelineRequestBuilder(client, this);
+    }
+
+    @Override
+    public SimulatePipelineResponse newResponse() {
+        return new SimulatePipelineResponse();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java
new file mode 100644
index 0000000..af18ac5
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequest.java
@@ -0,0 +1,165 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.ActionRequestValidationException;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.ingest.PipelineStore;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+
+import static org.elasticsearch.action.ValidateActions.addValidationError;
+import static org.elasticsearch.ingest.core.IngestDocument.MetaData;
+
+public class SimulatePipelineRequest extends ActionRequest<SimulatePipelineRequest> {
+
+    private String id;
+    private boolean verbose;
+    private BytesReference source;
+
+    public SimulatePipelineRequest(BytesReference source) {
+        if (source == null) {
+            throw new IllegalArgumentException("source is missing");
+        }
+        this.source = source;
+    }
+
+    SimulatePipelineRequest() {
+    }
+
+    @Override
+    public ActionRequestValidationException validate() {
+        return null;
+    }
+
+    public String getId() {
+        return id;
+    }
+
+    public void setId(String id) {
+        this.id = id;
+    }
+
+    public boolean isVerbose() {
+        return verbose;
+    }
+
+    public void setVerbose(boolean verbose) {
+        this.verbose = verbose;
+    }
+
+    public BytesReference getSource() {
+        return source;
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        id = in.readString();
+        verbose = in.readBoolean();
+        source = in.readBytesReference();
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        out.writeString(id);
+        out.writeBoolean(verbose);
+        out.writeBytesReference(source);
+    }
+
+    public static final class Fields {
+        static final String PIPELINE = "pipeline";
+        static final String DOCS = "docs";
+        static final String SOURCE = "_source";
+    }
+
+    static class Parsed {
+        private final List<IngestDocument> documents;
+        private final Pipeline pipeline;
+        private final boolean verbose;
+
+        Parsed(Pipeline pipeline, List<IngestDocument> documents, boolean verbose) {
+            this.pipeline = pipeline;
+            this.documents = Collections.unmodifiableList(documents);
+            this.verbose = verbose;
+        }
+
+        public Pipeline getPipeline() {
+            return pipeline;
+        }
+
+        public List<IngestDocument> getDocuments() {
+            return documents;
+        }
+
+        public boolean isVerbose() {
+            return verbose;
+        }
+    }
+
+    private static final Pipeline.Factory PIPELINE_FACTORY = new Pipeline.Factory();
+    static final String SIMULATED_PIPELINE_ID = "_simulate_pipeline";
+
+    static Parsed parseWithPipelineId(String pipelineId, Map<String, Object> config, boolean verbose, PipelineStore pipelineStore) {
+        if (pipelineId == null) {
+            throw new IllegalArgumentException("param [pipeline] is null");
+        }
+        Pipeline pipeline = pipelineStore.get(pipelineId);
+        List<IngestDocument> ingestDocumentList = parseDocs(config);
+        return new Parsed(pipeline, ingestDocumentList, verbose);
+    }
+
+    static Parsed parse(Map<String, Object> config, boolean verbose, PipelineStore pipelineStore) throws Exception {
+        Map<String, Object> pipelineConfig = ConfigurationUtils.readMap(config, Fields.PIPELINE);
+        Pipeline pipeline = PIPELINE_FACTORY.create(SIMULATED_PIPELINE_ID, pipelineConfig, pipelineStore.getProcessorFactoryRegistry());
+        List<IngestDocument> ingestDocumentList = parseDocs(config);
+        return new Parsed(pipeline, ingestDocumentList, verbose);
+    }
+
+    private static List<IngestDocument> parseDocs(Map<String, Object> config) {
+        List<Map<String, Object>> docs = ConfigurationUtils.readList(config, Fields.DOCS);
+        List<IngestDocument> ingestDocumentList = new ArrayList<>();
+        for (Map<String, Object> dataMap : docs) {
+            Map<String, Object> document = ConfigurationUtils.readMap(dataMap, Fields.SOURCE);
+            IngestDocument ingestDocument = new IngestDocument(ConfigurationUtils.readStringProperty(dataMap, MetaData.INDEX.getFieldName(), "_index"),
+                    ConfigurationUtils.readStringProperty(dataMap, MetaData.TYPE.getFieldName(), "_type"),
+                    ConfigurationUtils.readStringProperty(dataMap, MetaData.ID.getFieldName(), "_id"),
+                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.ROUTING.getFieldName()),
+                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.PARENT.getFieldName()),
+                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.TIMESTAMP.getFieldName()),
+                    ConfigurationUtils.readOptionalStringProperty(dataMap, MetaData.TTL.getFieldName()),
+                    document);
+            ingestDocumentList.add(ingestDocument);
+        }
+        return ingestDocumentList;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java
new file mode 100644
index 0000000..4a13fa1
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineRequestBuilder.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.common.bytes.BytesReference;
+
+public class SimulatePipelineRequestBuilder extends ActionRequestBuilder<SimulatePipelineRequest, SimulatePipelineResponse, SimulatePipelineRequestBuilder> {
+
+    public SimulatePipelineRequestBuilder(ElasticsearchClient client, SimulatePipelineAction action) {
+        super(client, action, new SimulatePipelineRequest());
+    }
+
+    public SimulatePipelineRequestBuilder(ElasticsearchClient client, SimulatePipelineAction action, BytesReference source) {
+        super(client, action, new SimulatePipelineRequest(source));
+    }
+
+    public SimulatePipelineRequestBuilder setId(String id) {
+        request.setId(id);
+        return this;
+    }
+
+    public SimulatePipelineRequestBuilder setVerbose(boolean verbose) {
+        request.setVerbose(verbose);
+        return this;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
new file mode 100644
index 0000000..c7c0822
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineResponse.java
@@ -0,0 +1,103 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+public class SimulatePipelineResponse extends ActionResponse implements ToXContent {
+    private String pipelineId;
+    private boolean verbose;
+    private List<SimulateDocumentResult> results;
+
+    public SimulatePipelineResponse() {
+
+    }
+
+    public SimulatePipelineResponse(String pipelineId, boolean verbose, List<SimulateDocumentResult> responses) {
+        this.pipelineId = pipelineId;
+        this.verbose = verbose;
+        this.results = Collections.unmodifiableList(responses);
+    }
+
+    public String getPipelineId() {
+        return pipelineId;
+    }
+
+    public List<SimulateDocumentResult> getResults() {
+        return results;
+    }
+
+    public boolean isVerbose() {
+        return verbose;
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        out.writeString(pipelineId);
+        out.writeBoolean(verbose);
+        out.writeVInt(results.size());
+        for (SimulateDocumentResult response : results) {
+            response.writeTo(out);
+        }
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        this.pipelineId = in.readString();
+        boolean verbose = in.readBoolean();
+        int responsesLength = in.readVInt();
+        results = new ArrayList<>();
+        for (int i = 0; i < responsesLength; i++) {
+            SimulateDocumentResult<?> simulateDocumentResult;
+            if (verbose) {
+                simulateDocumentResult = SimulateDocumentVerboseResult.readSimulateDocumentVerboseResultFrom(in);
+            } else {
+                simulateDocumentResult = SimulateDocumentBaseResult.readSimulateDocumentSimpleResult(in);
+            }
+            results.add(simulateDocumentResult);
+        }
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startArray(Fields.DOCUMENTS);
+        for (SimulateDocumentResult response : results) {
+            response.toXContent(builder, params);
+        }
+        builder.endArray();
+        return builder;
+    }
+
+    static final class Fields {
+        static final XContentBuilderString DOCUMENTS = new XContentBuilderString("docs");
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
new file mode 100644
index 0000000..5640d7c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulatePipelineTransportAction.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.HandledTransportAction;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.ingest.PipelineStore;
+import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+import java.util.Map;
+
+public class SimulatePipelineTransportAction extends HandledTransportAction<SimulatePipelineRequest, SimulatePipelineResponse> {
+
+    private final PipelineStore pipelineStore;
+    private final SimulateExecutionService executionService;
+
+    @Inject
+    public SimulatePipelineTransportAction(Settings settings, ThreadPool threadPool, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, NodeService nodeService) {
+        super(settings, SimulatePipelineAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, SimulatePipelineRequest::new);
+        this.pipelineStore = nodeService.getIngestService().getPipelineStore();
+        this.executionService = new SimulateExecutionService(threadPool);
+    }
+
+    @Override
+    protected void doExecute(SimulatePipelineRequest request, ActionListener<SimulatePipelineResponse> listener) {
+        final Map<String, Object> source = XContentHelper.convertToMap(request.getSource(), false).v2();
+
+        final SimulatePipelineRequest.Parsed simulateRequest;
+        try {
+            if (request.getId() != null) {
+                simulateRequest = SimulatePipelineRequest.parseWithPipelineId(request.getId(), source, request.isVerbose(), pipelineStore);
+            } else {
+                simulateRequest = SimulatePipelineRequest.parse(source, request.isVerbose(), pipelineStore);
+            }
+        } catch (Exception e) {
+            listener.onFailure(e);
+            return;
+        }
+
+        executionService.execute(simulateRequest, listener);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java b/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java
new file mode 100644
index 0000000..6a38434
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/SimulateProcessorResult.java
@@ -0,0 +1,106 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Writeable;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+
+import java.io.IOException;
+
+public class SimulateProcessorResult implements Writeable<SimulateProcessorResult>, ToXContent {
+    private final String processorTag;
+    private final WriteableIngestDocument ingestDocument;
+    private final Exception failure;
+
+    public SimulateProcessorResult(StreamInput in) throws IOException {
+        this.processorTag = in.readString();
+        if (in.readBoolean()) {
+            this.failure = in.readThrowable();
+            this.ingestDocument = null;
+        } else {
+            this.ingestDocument =  new WriteableIngestDocument(in);
+            this.failure = null;
+        }
+    }
+
+    public SimulateProcessorResult(String processorTag, IngestDocument ingestDocument) {
+        this.processorTag = processorTag;
+        this.ingestDocument = new WriteableIngestDocument(ingestDocument);
+        this.failure = null;
+    }
+
+    public SimulateProcessorResult(String processorTag, Exception failure) {
+        this.processorTag = processorTag;
+        this.failure = failure;
+        this.ingestDocument = null;
+    }
+
+    public IngestDocument getIngestDocument() {
+        if (ingestDocument == null) {
+            return null;
+        }
+        return ingestDocument.getIngestDocument();
+    }
+
+    public String getProcessorTag() {
+        return processorTag;
+    }
+
+    public Exception getFailure() {
+        return failure;
+    }
+
+    @Override
+    public SimulateProcessorResult readFrom(StreamInput in) throws IOException {
+        return new SimulateProcessorResult(in);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(processorTag);
+        if (failure == null) {
+            out.writeBoolean(false);
+            ingestDocument.writeTo(out);
+        } else {
+            out.writeBoolean(true);
+            out.writeThrowable(failure);
+        }
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        if (processorTag != null) {
+            builder.field(AbstractProcessorFactory.TAG_KEY, processorTag);
+        }
+        if (failure == null) {
+            ingestDocument.toXContent(builder, params);
+        } else {
+            ElasticsearchException.renderThrowable(builder, params, failure);
+        }
+        builder.endObject();
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java b/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java
new file mode 100644
index 0000000..885fd9f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/WritePipelineResponse.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.support.master.AcknowledgedResponse;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+
+import java.io.IOException;
+
+public class WritePipelineResponse extends AcknowledgedResponse {
+
+    WritePipelineResponse() {
+    }
+
+    public WritePipelineResponse(boolean acknowledge) {
+        super(acknowledge);
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        readAcknowledged(in);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        writeAcknowledged(out);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java b/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
new file mode 100644
index 0000000..342e4bd
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ingest/WriteableIngestDocument.java
@@ -0,0 +1,105 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Writeable;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.ingest.core.IngestDocument;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Map;
+import java.util.Objects;
+
+final class WriteableIngestDocument implements Writeable<WriteableIngestDocument>, ToXContent {
+
+    private final IngestDocument ingestDocument;
+
+    WriteableIngestDocument(IngestDocument ingestDocument) {
+        assert ingestDocument != null;
+        this.ingestDocument = ingestDocument;
+    }
+
+    WriteableIngestDocument(StreamInput in) throws IOException {
+        Map<String, Object> sourceAndMetadata = in.readMap();
+        @SuppressWarnings("unchecked")
+        Map<String, String> ingestMetadata = (Map<String, String>) in.readGenericValue();
+        this.ingestDocument = new IngestDocument(sourceAndMetadata, ingestMetadata);
+    }
+
+    IngestDocument getIngestDocument() {
+        return ingestDocument;
+    }
+
+
+    @Override
+    public WriteableIngestDocument readFrom(StreamInput in) throws IOException {
+       return new WriteableIngestDocument(in);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeMap(ingestDocument.getSourceAndMetadata());
+        out.writeGenericValue(ingestDocument.getIngestMetadata());
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject("doc");
+        Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
+        for (Map.Entry<IngestDocument.MetaData, String> metadata : metadataMap.entrySet()) {
+            builder.field(metadata.getKey().getFieldName(), metadata.getValue());
+        }
+        builder.field("_source", ingestDocument.getSourceAndMetadata());
+        builder.startObject("_ingest");
+        for (Map.Entry<String, String> ingestMetadata : ingestDocument.getIngestMetadata().entrySet()) {
+            builder.field(ingestMetadata.getKey(), ingestMetadata.getValue());
+        }
+        builder.endObject();
+        builder.endObject();
+        return builder;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) {
+            return true;
+        }
+        if (o == null || getClass() != o.getClass()) {
+            return false;
+        }
+        WriteableIngestDocument that = (WriteableIngestDocument) o;
+        return Objects.equals(ingestDocument, that.ingestDocument);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(ingestDocument);
+    }
+
+    @Override
+    public String toString() {
+        return ingestDocument.toString();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index 1cfcc6c..1557c26 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -29,7 +29,6 @@ import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.highlight.HighlightBuilder;
@@ -363,23 +362,13 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Adds an aggregation to the search operation.
-     *
-     * NORELEASE REMOVE WHEN AGG REFACTORING IS COMPLETE
+     * Adds an get to the search operation.
      */
     public SearchRequestBuilder addAggregation(AbstractAggregationBuilder aggregation) {
         sourceBuilder().aggregation(aggregation);
         return this;
     }
 
-    /**
-     * Adds an aggregation to the search operation.
-     */
-    public SearchRequestBuilder addAggregation(AggregatorFactory aggregation) {
-        sourceBuilder().aggregation(aggregation);
-        return this;
-    }
-
     public SearchRequestBuilder highlighter(HighlightBuilder highlightBuilder) {
         sourceBuilder().highlighter(highlightBuilder);
         return this;
diff --git a/core/src/main/java/org/elasticsearch/client/Client.java b/core/src/main/java/org/elasticsearch/client/Client.java
index e7461da..dbcd912 100644
--- a/core/src/main/java/org/elasticsearch/client/Client.java
+++ b/core/src/main/java/org/elasticsearch/client/Client.java
@@ -51,6 +51,17 @@ import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptResponse;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequest;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
+import org.elasticsearch.action.ingest.DeletePipelineRequest;
+import org.elasticsearch.action.ingest.DeletePipelineRequestBuilder;
+import org.elasticsearch.action.ingest.GetPipelineRequest;
+import org.elasticsearch.action.ingest.GetPipelineRequestBuilder;
+import org.elasticsearch.action.ingest.GetPipelineResponse;
+import org.elasticsearch.action.ingest.PutPipelineRequest;
+import org.elasticsearch.action.ingest.PutPipelineRequestBuilder;
+import org.elasticsearch.action.ingest.SimulatePipelineRequest;
+import org.elasticsearch.action.ingest.SimulatePipelineRequestBuilder;
+import org.elasticsearch.action.ingest.SimulatePipelineResponse;
+import org.elasticsearch.action.ingest.WritePipelineResponse;
 import org.elasticsearch.action.percolate.MultiPercolateRequest;
 import org.elasticsearch.action.percolate.MultiPercolateRequestBuilder;
 import org.elasticsearch.action.percolate.MultiPercolateResponse;
@@ -82,6 +93,7 @@ import org.elasticsearch.action.update.UpdateRequestBuilder;
 import org.elasticsearch.action.update.UpdateResponse;
 import org.elasticsearch.client.support.Headers;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.settings.Settings;
 
@@ -593,6 +605,66 @@ public interface Client extends ElasticsearchClient, Releasable {
     void fieldStats(FieldStatsRequest request, ActionListener<FieldStatsResponse> listener);
 
     /**
+     * Stores an ingest pipeline
+     */
+    void putPipeline(PutPipelineRequest request, ActionListener<WritePipelineResponse> listener);
+
+    /**
+     * Stores an ingest pipeline
+     */
+    ActionFuture<WritePipelineResponse> putPipeline(PutPipelineRequest request);
+
+    /**
+     * Stores an ingest pipeline
+     */
+    PutPipelineRequestBuilder preparePutPipeline(String id, BytesReference source);
+
+    /**
+     * Deletes a stored ingest pipeline
+     */
+    void deletePipeline(DeletePipelineRequest request, ActionListener<WritePipelineResponse> listener);
+
+    /**
+     * Deletes a stored ingest pipeline
+     */
+    ActionFuture<WritePipelineResponse> deletePipeline(DeletePipelineRequest request);
+
+    /**
+     * Deletes a stored ingest pipeline
+     */
+    DeletePipelineRequestBuilder prepareDeletePipeline();
+
+    /**
+     * Returns a stored ingest pipeline
+     */
+    void getPipeline(GetPipelineRequest request, ActionListener<GetPipelineResponse> listener);
+
+    /**
+     * Returns a stored ingest pipeline
+     */
+    ActionFuture<GetPipelineResponse> getPipeline(GetPipelineRequest request);
+
+    /**
+     * Returns a stored ingest pipeline
+     */
+    GetPipelineRequestBuilder prepareGetPipeline(String... ids);
+
+    /**
+     * Simulates an ingest pipeline
+     */
+    void simulatePipeline(SimulatePipelineRequest request, ActionListener<SimulatePipelineResponse> listener);
+
+    /**
+     * Simulates an ingest pipeline
+     */
+    ActionFuture<SimulatePipelineResponse> simulatePipeline(SimulatePipelineRequest request);
+
+    /**
+     * Simulates an ingest pipeline
+     */
+    SimulatePipelineRequestBuilder prepareSimulatePipeline(BytesReference source);
+
+    /**
      * Returns this clients settings
      */
     Settings settings();
diff --git a/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java b/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
index e5e1bea..182f31a 100644
--- a/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
+++ b/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
@@ -272,6 +272,21 @@ import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptAction;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequest;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
+import org.elasticsearch.action.ingest.DeletePipelineAction;
+import org.elasticsearch.action.ingest.DeletePipelineRequest;
+import org.elasticsearch.action.ingest.DeletePipelineRequestBuilder;
+import org.elasticsearch.action.ingest.GetPipelineAction;
+import org.elasticsearch.action.ingest.GetPipelineRequest;
+import org.elasticsearch.action.ingest.GetPipelineRequestBuilder;
+import org.elasticsearch.action.ingest.GetPipelineResponse;
+import org.elasticsearch.action.ingest.PutPipelineAction;
+import org.elasticsearch.action.ingest.PutPipelineRequest;
+import org.elasticsearch.action.ingest.PutPipelineRequestBuilder;
+import org.elasticsearch.action.ingest.SimulatePipelineAction;
+import org.elasticsearch.action.ingest.SimulatePipelineRequest;
+import org.elasticsearch.action.ingest.SimulatePipelineRequestBuilder;
+import org.elasticsearch.action.ingest.SimulatePipelineResponse;
+import org.elasticsearch.action.ingest.WritePipelineResponse;
 import org.elasticsearch.action.percolate.MultiPercolateAction;
 import org.elasticsearch.action.percolate.MultiPercolateRequest;
 import org.elasticsearch.action.percolate.MultiPercolateRequestBuilder;
@@ -319,6 +334,7 @@ import org.elasticsearch.client.ClusterAdminClient;
 import org.elasticsearch.client.ElasticsearchClient;
 import org.elasticsearch.client.IndicesAdminClient;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -794,6 +810,66 @@ public abstract class AbstractClient extends AbstractComponent implements Client
         return new FieldStatsRequestBuilder(this, FieldStatsAction.INSTANCE);
     }
 
+    @Override
+    public void putPipeline(PutPipelineRequest request, ActionListener<WritePipelineResponse> listener) {
+        execute(PutPipelineAction.INSTANCE, request, listener);
+    }
+
+    @Override
+    public ActionFuture<WritePipelineResponse> putPipeline(PutPipelineRequest request) {
+        return execute(PutPipelineAction.INSTANCE, request);
+    }
+
+    @Override
+    public PutPipelineRequestBuilder preparePutPipeline(String id, BytesReference source) {
+        return new PutPipelineRequestBuilder(this, PutPipelineAction.INSTANCE, id, source);
+    }
+
+    @Override
+    public void deletePipeline(DeletePipelineRequest request, ActionListener<WritePipelineResponse> listener) {
+        execute(DeletePipelineAction.INSTANCE, request, listener);
+    }
+
+    @Override
+    public ActionFuture<WritePipelineResponse> deletePipeline(DeletePipelineRequest request) {
+        return execute(DeletePipelineAction.INSTANCE, request);
+    }
+
+    @Override
+    public DeletePipelineRequestBuilder prepareDeletePipeline() {
+        return new DeletePipelineRequestBuilder(this, DeletePipelineAction.INSTANCE);
+    }
+
+    @Override
+    public void getPipeline(GetPipelineRequest request, ActionListener<GetPipelineResponse> listener) {
+        execute(GetPipelineAction.INSTANCE, request, listener);
+    }
+
+    @Override
+    public ActionFuture<GetPipelineResponse> getPipeline(GetPipelineRequest request) {
+        return execute(GetPipelineAction.INSTANCE, request);
+    }
+
+    @Override
+    public GetPipelineRequestBuilder prepareGetPipeline(String... ids) {
+        return new GetPipelineRequestBuilder(this, GetPipelineAction.INSTANCE, ids);
+    }
+
+    @Override
+    public void simulatePipeline(SimulatePipelineRequest request, ActionListener<SimulatePipelineResponse> listener) {
+        execute(SimulatePipelineAction.INSTANCE, request, listener);
+    }
+
+    @Override
+    public ActionFuture<SimulatePipelineResponse> simulatePipeline(SimulatePipelineRequest request) {
+        return execute(SimulatePipelineAction.INSTANCE, request);
+    }
+
+    @Override
+    public SimulatePipelineRequestBuilder prepareSimulatePipeline(BytesReference source) {
+        return new SimulatePipelineRequestBuilder(this, SimulatePipelineAction.INSTANCE, source);
+    }
+
     static class Admin implements AdminClient {
 
         private final ClusterAdmin clusterAdmin;
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
index 3d68e64..9930a9d 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.client.transport;
 
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.TimeUnit;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.action.Action;
 import org.elasticsearch.action.ActionListener;
@@ -59,6 +55,10 @@ import org.elasticsearch.threadpool.ThreadPoolModule;
 import org.elasticsearch.transport.TransportService;
 import org.elasticsearch.transport.netty.NettyTransport;
 
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 
 /**
@@ -116,7 +116,7 @@ public class TransportClient extends AbstractClient {
                 .put("node.client", true)
                 .put(CLIENT_TYPE_SETTING, CLIENT_TYPE);
             return new PluginsService(settingsBuilder.build(), null, null, pluginClasses);
-        };
+        }
 
         /**
          * Builds a new instance of the transport client.
@@ -150,7 +150,7 @@ public class TransportClient extends AbstractClient {
                         // noop
                     }
                 });
-                modules.add(new ActionModule(true));
+                modules.add(new ActionModule(false, true));
                 modules.add(new CircuitBreakerModule(settings));
 
                 pluginsService.processModules(modules);
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
index 002d1a5..0e41dda 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
@@ -54,6 +54,7 @@ import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.store.IndexStoreConfig;
 import org.elasticsearch.indices.recovery.RecoverySettings;
 import org.elasticsearch.indices.ttl.IndicesTTLService;
+import org.elasticsearch.ingest.IngestMetadata;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
@@ -111,6 +112,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
     static {
         // register non plugin custom metadata
         registerPrototype(RepositoriesMetaData.TYPE, RepositoriesMetaData.PROTO);
+        registerPrototype(IngestMetadata.TYPE, IngestMetadata.PROTO);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
index 7dce217..e05bab6 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
@@ -32,6 +32,7 @@ import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.transport.TransportAddressSerializers;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.node.Node;
 
 import java.io.IOException;
 import java.util.Collections;
@@ -87,6 +88,10 @@ public class DiscoveryNode implements Streamable, ToXContent {
         return Booleans.isExplicitTrue(data);
     }
 
+    public static boolean ingestNode(Settings settings) {
+        return Node.NODE_INGEST_SETTING.get(settings);
+    }
+
     public static final List<DiscoveryNode> EMPTY_LIST = Collections.emptyList();
 
     private String nodeName = "";
@@ -316,6 +321,14 @@ public class DiscoveryNode implements Streamable, ToXContent {
         return masterNode();
     }
 
+    /**
+     * Returns a boolean that tells whether this an ingest node or not
+     */
+    public boolean isIngestNode() {
+        String ingest = attributes.get("ingest");
+        return ingest == null ? true : Booleans.parseBooleanExact(ingest);
+    }
+
     public Version version() {
         return this.version;
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
index d07d3c3..e24c25d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
@@ -52,16 +52,20 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
     private final ImmutableOpenMap<String, DiscoveryNode> nodes;
     private final ImmutableOpenMap<String, DiscoveryNode> dataNodes;
     private final ImmutableOpenMap<String, DiscoveryNode> masterNodes;
+    private final ImmutableOpenMap<String, DiscoveryNode> ingestNodes;
 
     private final String masterNodeId;
     private final String localNodeId;
     private final Version minNodeVersion;
     private final Version minNonClientNodeVersion;
 
-    private DiscoveryNodes(ImmutableOpenMap<String, DiscoveryNode> nodes, ImmutableOpenMap<String, DiscoveryNode> dataNodes, ImmutableOpenMap<String, DiscoveryNode> masterNodes, String masterNodeId, String localNodeId, Version minNodeVersion, Version minNonClientNodeVersion) {
+    private DiscoveryNodes(ImmutableOpenMap<String, DiscoveryNode> nodes, ImmutableOpenMap<String, DiscoveryNode> dataNodes,
+                           ImmutableOpenMap<String, DiscoveryNode> masterNodes, ImmutableOpenMap<String, DiscoveryNode> ingestNodes,
+                           String masterNodeId, String localNodeId, Version minNodeVersion, Version minNonClientNodeVersion) {
         this.nodes = nodes;
         this.dataNodes = dataNodes;
         this.masterNodes = masterNodes;
+        this.ingestNodes = ingestNodes;
         this.masterNodeId = masterNodeId;
         this.localNodeId = localNodeId;
         this.minNodeVersion = minNodeVersion;
@@ -165,6 +169,13 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
     }
 
     /**
+     * @return All the ingest nodes arranged by their ids
+     */
+    public ImmutableOpenMap<String, DiscoveryNode> getIngestNodes() {
+        return ingestNodes;
+    }
+
+    /**
      * Get a {@link Map} of the discovered master and data nodes arranged by their ids
      *
      * @return {@link Map} of the discovered master and data nodes arranged by their ids
@@ -654,6 +665,7 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
         public DiscoveryNodes build() {
             ImmutableOpenMap.Builder<String, DiscoveryNode> dataNodesBuilder = ImmutableOpenMap.builder();
             ImmutableOpenMap.Builder<String, DiscoveryNode> masterNodesBuilder = ImmutableOpenMap.builder();
+            ImmutableOpenMap.Builder<String, DiscoveryNode> ingestNodesBuilder = ImmutableOpenMap.builder();
             Version minNodeVersion = Version.CURRENT;
             Version minNonClientNodeVersion = Version.CURRENT;
             for (ObjectObjectCursor<String, DiscoveryNode> nodeEntry : nodes) {
@@ -665,10 +677,16 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
                     masterNodesBuilder.put(nodeEntry.key, nodeEntry.value);
                     minNonClientNodeVersion = Version.smallest(minNonClientNodeVersion, nodeEntry.value.version());
                 }
+                if (nodeEntry.value.isIngestNode()) {
+                    ingestNodesBuilder.put(nodeEntry.key, nodeEntry.value);
+                }
                 minNodeVersion = Version.smallest(minNodeVersion, nodeEntry.value.version());
             }
 
-            return new DiscoveryNodes(nodes.build(), dataNodesBuilder.build(), masterNodesBuilder.build(), masterNodeId, localNodeId, minNodeVersion, minNonClientNodeVersion);
+            return new DiscoveryNodes(
+                nodes.build(), dataNodesBuilder.build(), masterNodesBuilder.build(), ingestNodesBuilder.build(),
+                masterNodeId, localNodeId, minNodeVersion, minNonClientNodeVersion
+            );
         }
 
         public static DiscoveryNodes readFrom(StreamInput in, @Nullable DiscoveryNode localNode) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index 263b8c4..1e01d4c 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -38,8 +38,6 @@ import org.elasticsearch.common.text.Text;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.rescore.RescoreBuilder.Rescorer;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -666,20 +664,6 @@ public abstract class StreamInput extends InputStream {
     }
 
     /**
-     * Reads a {@link AggregatorFactory} from the current stream
-     */
-    public AggregatorFactory readAggregatorFactory() throws IOException {
-        return readNamedWriteable(AggregatorFactory.class);
-    }
-
-    /**
-     * Reads a {@link PipelineAggregatorFactory} from the current stream
-     */
-    public PipelineAggregatorFactory readPipelineAggregatorFactory() throws IOException {
-        return readNamedWriteable(PipelineAggregatorFactory.class);
-    }
-
-    /**
      * Reads a {@link QueryBuilder} from the current stream
      */
     public QueryBuilder readQuery() throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 591081f..74c7acf 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -37,8 +37,6 @@ import org.elasticsearch.common.text.Text;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.rescore.RescoreBuilder.Rescorer;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.joda.time.ReadableInstant;
 
 import java.io.EOFException;
@@ -642,20 +640,6 @@ public abstract class StreamOutput extends OutputStream {
     }
 
     /**
-     * Writes a {@link AggregatorFactory} to the current stream
-     */
-    public void writeAggregatorFactory(AggregatorFactory factory) throws IOException {
-        writeNamedWriteable(factory);
-    }
-
-    /**
-     * Writes a {@link PipelineAggregatorFactory} to the current stream
-     */
-    public void writePipelineAggregatorFactory(PipelineAggregatorFactory factory) throws IOException {
-        writeNamedWriteable(factory);
-    }
-
-    /**
      * Writes a {@link QueryBuilder} to the current stream
      */
     public void writeQuery(QueryBuilder queryBuilder) throws IOException {
@@ -692,7 +676,7 @@ public abstract class StreamOutput extends OutputStream {
         for (T obj: list) {
             obj.writeTo(this);
         }
-    }
+     }
 
      /**
      * Writes a {@link Rescorer} to the current stream
diff --git a/core/src/main/java/org/elasticsearch/common/network/Cidrs.java b/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
index f0bd4fb..d055724 100644
--- a/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
+++ b/core/src/main/java/org/elasticsearch/common/network/Cidrs.java
@@ -113,8 +113,4 @@ public final class Cidrs {
         assert octets.length == 4;
         return octetsToString(octets) + "/" + networkMask;
     }
-
-    public static String createCIDR(long ipAddress, int networkMask) {
-        return octetsToCIDR(longToOctets(ipAddress), networkMask);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java b/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
index 8be907e..fab02b6 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkModule.java
@@ -118,6 +118,10 @@ import org.elasticsearch.rest.action.get.RestGetSourceAction;
 import org.elasticsearch.rest.action.get.RestHeadAction;
 import org.elasticsearch.rest.action.get.RestMultiGetAction;
 import org.elasticsearch.rest.action.index.RestIndexAction;
+import org.elasticsearch.rest.action.ingest.RestDeletePipelineAction;
+import org.elasticsearch.rest.action.ingest.RestGetPipelineAction;
+import org.elasticsearch.rest.action.ingest.RestPutPipelineAction;
+import org.elasticsearch.rest.action.ingest.RestSimulatePipelineAction;
 import org.elasticsearch.rest.action.main.RestMainAction;
 import org.elasticsearch.rest.action.percolate.RestMultiPercolateAction;
 import org.elasticsearch.rest.action.percolate.RestPercolateAction;
@@ -258,7 +262,13 @@ public class NetworkModule extends AbstractModule {
         RestCatAction.class,
 
         // Tasks API
-        RestListTasksAction.class
+        RestListTasksAction.class,
+
+        // Ingest API
+        RestPutPipelineAction.class,
+        RestGetPipelineAction.class,
+        RestDeletePipelineAction.class,
+        RestSimulatePipelineAction.class
     );
 
     private static final List<Class<? extends AbstractCatAction>> builtinCatHandlers = Arrays.asList(
diff --git a/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java b/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
index 7e94ebb..89a2679 100644
--- a/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
+++ b/core/src/main/java/org/elasticsearch/common/rounding/Rounding.java
@@ -19,13 +19,11 @@
 package org.elasticsearch.common.rounding;
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A strategy for rounding long values.
@@ -63,12 +61,6 @@ public abstract class Rounding implements Streamable {
      */
     public abstract long nextRoundingValue(long value);
 
-    @Override
-    public abstract boolean equals(Object obj);
-
-    @Override
-    public abstract int hashCode();
-
     /**
      * Rounding strategy which is based on an interval
      *
@@ -78,8 +70,6 @@ public abstract class Rounding implements Streamable {
 
         final static byte ID = 0;
 
-        public static final ParseField INTERVAL_FIELD = new ParseField("interval");
-
         private long interval;
 
         public Interval() { // for serialization
@@ -136,31 +126,12 @@ public abstract class Rounding implements Streamable {
         public void writeTo(StreamOutput out) throws IOException {
             out.writeVLong(interval);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(interval);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            Interval other = (Interval) obj;
-            return Objects.equals(interval, other.interval);
-        }
     }
 
     public static class FactorRounding extends Rounding {
 
         final static byte ID = 7;
 
-        public static final ParseField FACTOR_FIELD = new ParseField("factor");
-
         private Rounding rounding;
 
         private float factor;
@@ -195,7 +166,7 @@ public abstract class Rounding implements Streamable {
 
         @Override
         public void readFrom(StreamInput in) throws IOException {
-            rounding = Rounding.Streams.read(in);
+            rounding = (TimeZoneRounding) Rounding.Streams.read(in);
             factor = in.readFloat();
         }
 
@@ -204,32 +175,12 @@ public abstract class Rounding implements Streamable {
             Rounding.Streams.write(rounding, out);
             out.writeFloat(factor);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(rounding, factor);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            FactorRounding other = (FactorRounding) obj;
-            return Objects.equals(rounding, other.rounding)
-                    && Objects.equals(factor, other.factor);
-        }
     }
 
     public static class OffsetRounding extends Rounding {
 
         final static byte ID = 8;
 
-        public static final ParseField OFFSET_FIELD = new ParseField("offset");
-
         private Rounding rounding;
 
         private long offset;
@@ -273,24 +224,6 @@ public abstract class Rounding implements Streamable {
             Rounding.Streams.write(rounding, out);
             out.writeLong(offset);
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(rounding, offset);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            OffsetRounding other = (OffsetRounding) obj;
-            return Objects.equals(rounding, other.rounding)
-                    && Objects.equals(offset, other.offset);
-        }
     }
 
     public static class Streams {
diff --git a/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java b/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
index 4189e41..1e6bbb6 100644
--- a/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
+++ b/core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.common.rounding;
 
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
@@ -28,13 +27,10 @@ import org.joda.time.DateTimeZone;
 import org.joda.time.DurationField;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  */
 public abstract class TimeZoneRounding extends Rounding {
-    public static final ParseField INTERVAL_FIELD = new ParseField("interval");
-    public static final ParseField TIME_ZONE_FIELD = new ParseField("time_zone");
 
     public static Builder builder(DateTimeUnit unit) {
         return new Builder(unit);
@@ -161,24 +157,6 @@ public abstract class TimeZoneRounding extends Rounding {
             out.writeByte(unit.id());
             out.writeString(timeZone.getID());
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(unit, timeZone);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            TimeUnitRounding other = (TimeUnitRounding) obj;
-            return Objects.equals(unit, other.unit)
-                    && Objects.equals(timeZone, other.timeZone);
-        }
     }
 
     static class TimeIntervalRounding extends TimeZoneRounding {
@@ -236,23 +214,5 @@ public abstract class TimeZoneRounding extends Rounding {
             out.writeVLong(interval);
             out.writeString(timeZone.getID());
         }
-        
-        @Override
-        public int hashCode() {
-            return Objects.hash(interval, timeZone);
-        }
-        
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            TimeIntervalRounding other = (TimeIntervalRounding) obj;
-            return Objects.equals(interval, other.interval)
-                    && Objects.equals(timeZone, other.timeZone);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
index 4680146..05978a1 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
@@ -175,7 +175,7 @@ public final class ClusterSettings extends AbstractScopedSettings {
         NettyHttpServerTransport.SETTING_CORS_ENABLED,
         NettyHttpServerTransport.SETTING_CORS_MAX_AGE,
         NettyHttpServerTransport.SETTING_HTTP_DETAILED_ERRORS_ENABLED,
-        NettyHttpServerTransport.SETTING_PIPELINING,       
+        NettyHttpServerTransport.SETTING_PIPELINING,
         HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING,
         HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING,
         HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING,
@@ -252,5 +252,6 @@ public final class ClusterSettings extends AbstractScopedSettings {
         URLRepository.ALLOWED_URLS_SETTING,
         URLRepository.REPOSITORIES_LIST_DIRECTORIES_SETTING,
         URLRepository.REPOSITORIES_URL_SETTING,
-        URLRepository.SUPPORTED_PROTOCOLS_SETTING)));
+        URLRepository.SUPPORTED_PROTOCOLS_SETTING,
+        Node.NODE_INGEST_SETTING)));
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java
index 979a1f2..395dcad 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/ObjectParser.java
@@ -223,7 +223,7 @@ public final class ObjectParser<Value, Context> implements BiFunction<XContentPa
             list.add(supplier.get()); // single value
         } else {
             while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
-                if (parser.currentToken().isValue()) {
+                if (parser.currentToken().isValue() || parser.currentToken() == XContentParser.Token.START_OBJECT) {
                     list.add(supplier.get());
                 } else {
                     throw new IllegalStateException("expected value but got [" + parser.currentToken() + "]");
@@ -237,6 +237,11 @@ public final class ObjectParser<Value, Context> implements BiFunction<XContentPa
         declareField((p, v, c) -> consumer.accept(v, objectParser.apply(p, c)), field, ValueType.OBJECT);
     }
 
+    public <T> void declareObjectArray(BiConsumer<Value, List<T>> consumer, BiFunction<XContentParser, Context, T> objectParser, ParseField field) {
+        declareField((p, v, c) -> consumer.accept(v, parseArray(p, () -> objectParser.apply(p, c))), field, ValueType.OBJECT_ARRAY);
+    }
+
+
     public <T> void declareObjectOrDefault(BiConsumer<Value, T> consumer, BiFunction<XContentParser, Context, T> objectParser, Supplier<T> defaultValue, ParseField field) {
         declareField((p, v, c) -> {
             if (p.currentToken() == XContentParser.Token.VALUE_BOOLEAN) {
@@ -333,6 +338,7 @@ public final class ObjectParser<Value, Context> implements BiFunction<XContentPa
         INT_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY, XContentParser.Token.VALUE_NUMBER, XContentParser.Token.VALUE_STRING)),
         BOOLEAN_ARRAY(EnumSet.of(XContentParser.Token.START_ARRAY, XContentParser.Token.VALUE_BOOLEAN)),
         OBJECT(EnumSet.of(XContentParser.Token.START_OBJECT)),
+        OBJECT_ARRAY(EnumSet.of(XContentParser.Token.START_OBJECT, XContentParser.Token.START_ARRAY)),
         OBJECT_OR_BOOLEAN(EnumSet.of(XContentParser.Token.START_OBJECT, XContentParser.Token.VALUE_BOOLEAN)),
         VALUE(EnumSet.of(XContentParser.Token.VALUE_BOOLEAN, XContentParser.Token.VALUE_NULL ,XContentParser.Token.VALUE_EMBEDDED_OBJECT,XContentParser.Token.VALUE_NUMBER,XContentParser.Token.VALUE_STRING));
 
diff --git a/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java b/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java
new file mode 100644
index 0000000..0e50751
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/IngestMetadata.java
@@ -0,0 +1,121 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.cluster.AbstractDiffable;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.collect.HppcMaps;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.ObjectParser;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentParser;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Holds the ingest pipelines that are available in the cluster
+ */
+public final class IngestMetadata extends AbstractDiffable<MetaData.Custom> implements MetaData.Custom {
+
+    public final static String TYPE = "ingest";
+    public final static IngestMetadata PROTO = new IngestMetadata();
+    private static final ParseField PIPELINES_FIELD = new ParseField("pipeline");
+    private static final ObjectParser<List<PipelineConfiguration>, Void> INGEST_METADATA_PARSER = new ObjectParser<>("ingest_metadata", ArrayList::new);
+
+    static {
+        INGEST_METADATA_PARSER.declareObjectArray(List::addAll , PipelineConfiguration.getParser(), PIPELINES_FIELD);
+    }
+
+
+    // We can't use Pipeline class directly in cluster state, because we don't have the processor factories around when
+    // IngestMetadata is registered as custom metadata.
+    private final Map<String, PipelineConfiguration> pipelines;
+
+    private IngestMetadata() {
+        this.pipelines = Collections.emptyMap();
+    }
+
+    public IngestMetadata(Map<String, PipelineConfiguration> pipelines) {
+        this.pipelines = Collections.unmodifiableMap(pipelines);
+    }
+
+    @Override
+    public String type() {
+        return TYPE;
+    }
+
+    public Map<String, PipelineConfiguration> getPipelines() {
+        return pipelines;
+    }
+
+    @Override
+    public MetaData.Custom readFrom(StreamInput in) throws IOException {
+        int size = in.readVInt();
+        Map<String, PipelineConfiguration> pipelines = new HashMap<>(size);
+        for (int i = 0; i < size; i++) {
+            PipelineConfiguration pipeline = PipelineConfiguration.readPipelineConfiguration(in);
+            pipelines.put(pipeline.getId(), pipeline);
+        }
+        return new IngestMetadata(pipelines);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeVInt(pipelines.size());
+        for (PipelineConfiguration pipeline : pipelines.values()) {
+            pipeline.writeTo(out);
+        }
+    }
+
+    @Override
+    public MetaData.Custom fromXContent(XContentParser parser) throws IOException {
+        Map<String, PipelineConfiguration> pipelines = new HashMap<>();
+        List<PipelineConfiguration> configs = INGEST_METADATA_PARSER.parse(parser);
+        for (PipelineConfiguration pipeline : configs) {
+            pipelines.put(pipeline.getId(), pipeline);
+        }
+        return new IngestMetadata(pipelines);
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startArray(PIPELINES_FIELD.getPreferredName());
+        for (PipelineConfiguration pipeline : pipelines.values()) {
+            pipeline.toXContent(builder, params);
+        }
+        builder.endArray();
+        return builder;
+    }
+
+    @Override
+    public EnumSet<MetaData.XContentContext> context() {
+        return MetaData.API_AND_GATEWAY;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/IngestService.java b/core/src/main/java/org/elasticsearch/ingest/IngestService.java
new file mode 100644
index 0000000..bc7cd75
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/IngestService.java
@@ -0,0 +1,63 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.threadpool.ThreadPool;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+/**
+ * Instantiates and wires all the services that the ingest plugin will be needing.
+ * Also the bootstrapper is in charge of starting and stopping the ingest plugin based on the cluster state.
+ */
+public class IngestService implements Closeable {
+
+    private final PipelineStore pipelineStore;
+    private final PipelineExecutionService pipelineExecutionService;
+    private final ProcessorsRegistry processorsRegistry;
+
+    public IngestService(Settings settings, ThreadPool threadPool, ProcessorsRegistry processorsRegistry) {
+        this.processorsRegistry = processorsRegistry;
+        this.pipelineStore = new PipelineStore(settings);
+        this.pipelineExecutionService = new PipelineExecutionService(pipelineStore, threadPool);
+    }
+
+    public PipelineStore getPipelineStore() {
+        return pipelineStore;
+    }
+
+    public PipelineExecutionService getPipelineExecutionService() {
+        return pipelineExecutionService;
+    }
+
+    public void setScriptService(ScriptService scriptService) {
+        pipelineStore.buildProcessorFactoryRegistry(processorsRegistry, scriptService);
+    }
+
+    @Override
+    public void close() throws IOException {
+        pipelineStore.close();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java b/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java
new file mode 100644
index 0000000..b4b5ce8
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/InternalTemplateService.java
@@ -0,0 +1,92 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptContext;
+import org.elasticsearch.script.ScriptService;
+
+import java.util.Collections;
+import java.util.Map;
+
+public class InternalTemplateService implements TemplateService {
+
+    private final ScriptService scriptService;
+
+    InternalTemplateService(ScriptService scriptService) {
+        this.scriptService = scriptService;
+    }
+
+    @Override
+    public Template compile(String template) {
+        int mustacheStart = template.indexOf("{{");
+        int mustacheEnd = template.indexOf("}}");
+        if (mustacheStart != -1 && mustacheEnd != -1 && mustacheStart < mustacheEnd) {
+            Script script = new Script(template, ScriptService.ScriptType.INLINE, "mustache", Collections.emptyMap());
+            CompiledScript compiledScript = scriptService.compile(
+                script,
+                ScriptContext.Standard.INGEST,
+                null /* we can supply null here, because ingest doesn't use indexed scripts */,
+                Collections.emptyMap()
+            );
+            return new Template() {
+                @Override
+                public String execute(Map<String, Object> model) {
+                    ExecutableScript executableScript = scriptService.executable(compiledScript, model);
+                    Object result = executableScript.run();
+                    if (result instanceof BytesReference) {
+                        return ((BytesReference) result).toUtf8();
+                    }
+                    return String.valueOf(result);
+                }
+
+                @Override
+                public String getKey() {
+                    return template;
+                }
+            };
+        } else {
+            return new StringTemplate(template);
+        }
+    }
+
+    class StringTemplate implements Template {
+
+        private final String value;
+
+        public StringTemplate(String value) {
+            this.value = value;
+        }
+
+        @Override
+        public String execute(Map<String, Object> model) {
+            return value;
+        }
+
+        @Override
+        public String getKey() {
+            return value;
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java b/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java
new file mode 100644
index 0000000..90ab2a7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/PipelineConfiguration.java
@@ -0,0 +1,119 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.apache.lucene.util.BytesRef;
+import org.elasticsearch.Build;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Writeable;
+import org.elasticsearch.common.xcontent.ObjectParser;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentParser;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Objects;
+import java.util.function.BiFunction;
+
+/**
+ * Encapsulates a pipeline's id and configuration as a blob
+ */
+public final class PipelineConfiguration implements Writeable<PipelineConfiguration>, ToXContent {
+
+    private final static PipelineConfiguration PROTOTYPE = new PipelineConfiguration(null, null);
+
+    public static PipelineConfiguration readPipelineConfiguration(StreamInput in) throws IOException {
+        return PROTOTYPE.readFrom(in);
+    }
+    private final static ObjectParser<Builder, Void> PARSER = new ObjectParser<>("pipeline_config", Builder::new);
+    static {
+        PARSER.declareString(Builder::setId, new ParseField("id"));
+        PARSER.declareField((parser, builder, aVoid) -> {
+            XContentBuilder contentBuilder = XContentBuilder.builder(parser.contentType().xContent());
+            XContentHelper.copyCurrentStructure(contentBuilder.generator(), parser);
+            builder.setConfig(contentBuilder.bytes());
+        }, new ParseField("config"), ObjectParser.ValueType.OBJECT);
+    }
+
+    public static BiFunction<XContentParser, Void,PipelineConfiguration> getParser() {
+        return (p, c) -> PARSER.apply(p ,c).build();
+    }
+    private static class Builder {
+
+        private String id;
+        private BytesReference config;
+
+        void setId(String id) {
+            this.id = id;
+        }
+
+        void setConfig(BytesReference config) {
+            this.config = config;
+        }
+
+        PipelineConfiguration build() {
+            return new PipelineConfiguration(id, config);
+        }
+    }
+
+    private final String id;
+    // Store config as bytes reference, because the config is only used when the pipeline store reads the cluster state
+    // and the way the map of maps config is read requires a deep copy (it removes instead of gets entries to check for unused options)
+    // also the get pipeline api just directly returns this to the caller
+    private final BytesReference config;
+
+    public PipelineConfiguration(String id, BytesReference config) {
+        this.id = id;
+        this.config = config;
+    }
+
+    public String getId() {
+        return id;
+    }
+
+    public Map<String, Object> getConfigAsMap() {
+        return XContentHelper.convertToMap(config, true).v2();
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        builder.field("id", id);
+        builder.field("config", getConfigAsMap());
+        builder.endObject();
+        return builder;
+    }
+
+    @Override
+    public PipelineConfiguration readFrom(StreamInput in) throws IOException {
+        return new PipelineConfiguration(in.readString(), in.readBytesReference());
+    }
+
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(id);
+        out.writeBytesReference(config);
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java b/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java
new file mode 100644
index 0000000..c6a3b4b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/PipelineExecutionService.java
@@ -0,0 +1,124 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.util.concurrent.AbstractRunnable;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.threadpool.ThreadPool;
+
+import java.util.Map;
+import java.util.function.BiConsumer;
+import java.util.function.Consumer;
+
+public class PipelineExecutionService {
+
+    private final PipelineStore store;
+    private final ThreadPool threadPool;
+
+    public PipelineExecutionService(PipelineStore store, ThreadPool threadPool) {
+        this.store = store;
+        this.threadPool = threadPool;
+    }
+
+    public void execute(IndexRequest request, Consumer<Throwable> failureHandler, Consumer<Boolean> completionHandler) {
+        Pipeline pipeline = getPipeline(request.getPipeline());
+        threadPool.executor(ThreadPool.Names.INDEX).execute(new AbstractRunnable() {
+
+            @Override
+            public void onFailure(Throwable t) {
+                failureHandler.accept(t);
+            }
+
+            @Override
+            protected void doRun() throws Exception {
+                innerExecute(request, pipeline);
+                completionHandler.accept(true);
+            }
+        });
+    }
+
+    public void execute(Iterable<ActionRequest<?>> actionRequests,
+                        BiConsumer<IndexRequest, Throwable> itemFailureHandler,
+                        Consumer<Throwable> completionHandler) {
+        threadPool.executor(ThreadPool.Names.INDEX).execute(new AbstractRunnable() {
+
+            @Override
+            public void onFailure(Throwable t) {
+                completionHandler.accept(t);
+            }
+
+            @Override
+            protected void doRun() throws Exception {
+                for (ActionRequest actionRequest : actionRequests) {
+                    if ((actionRequest instanceof IndexRequest)) {
+                        IndexRequest indexRequest = (IndexRequest) actionRequest;
+                        if (Strings.hasText(indexRequest.getPipeline())) {
+                            try {
+                                innerExecute(indexRequest, getPipeline(indexRequest.getPipeline()));
+                                //this shouldn't be needed here but we do it for consistency with index api which requires it to prevent double execution
+                                indexRequest.setPipeline(null);
+                            } catch (Throwable e) {
+                                itemFailureHandler.accept(indexRequest, e);
+                            }
+                        }
+                    }
+                }
+                completionHandler.accept(null);
+            }
+        });
+    }
+
+    private void innerExecute(IndexRequest indexRequest, Pipeline pipeline) throws Exception {
+        String index = indexRequest.index();
+        String type = indexRequest.type();
+        String id = indexRequest.id();
+        String routing = indexRequest.routing();
+        String parent = indexRequest.parent();
+        String timestamp = indexRequest.timestamp();
+        String ttl = indexRequest.ttl() == null ? null : indexRequest.ttl().toString();
+        Map<String, Object> sourceAsMap = indexRequest.sourceAsMap();
+        IngestDocument ingestDocument = new IngestDocument(index, type, id, routing, parent, timestamp, ttl, sourceAsMap);
+        pipeline.execute(ingestDocument);
+
+        Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
+        //it's fine to set all metadata fields all the time, as ingest document holds their starting values
+        //before ingestion, which might also get modified during ingestion.
+        indexRequest.index(metadataMap.get(IngestDocument.MetaData.INDEX));
+        indexRequest.type(metadataMap.get(IngestDocument.MetaData.TYPE));
+        indexRequest.id(metadataMap.get(IngestDocument.MetaData.ID));
+        indexRequest.routing(metadataMap.get(IngestDocument.MetaData.ROUTING));
+        indexRequest.parent(metadataMap.get(IngestDocument.MetaData.PARENT));
+        indexRequest.timestamp(metadataMap.get(IngestDocument.MetaData.TIMESTAMP));
+        indexRequest.ttl(metadataMap.get(IngestDocument.MetaData.TTL));
+        indexRequest.source(ingestDocument.getSourceAndMetadata());
+    }
+
+    private Pipeline getPipeline(String pipelineId) {
+        Pipeline pipeline = store.get(pipelineId);
+        if (pipeline == null) {
+            throw new IllegalArgumentException("pipeline with id [" + pipelineId + "] does not exist");
+        }
+        return pipeline;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java b/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
new file mode 100644
index 0000000..805f1e4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/PipelineStore.java
@@ -0,0 +1,242 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.ResourceNotFoundException;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ingest.DeletePipelineRequest;
+import org.elasticsearch.action.ingest.PutPipelineRequest;
+import org.elasticsearch.action.ingest.WritePipelineResponse;
+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateListener;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.script.ScriptService;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.function.Function;
+
+public class PipelineStore extends AbstractComponent implements Closeable, ClusterStateListener {
+
+    private final Pipeline.Factory factory = new Pipeline.Factory();
+    private Map<String, Processor.Factory> processorFactoryRegistry;
+
+    // Ideally this should be in IngestMetadata class, but we don't have the processor factories around there.
+    // We know of all the processor factories when a node with all its plugin have been initialized. Also some
+    // processor factories rely on other node services. Custom metadata is statically registered when classes
+    // are loaded, so in the cluster state we just save the pipeline config and here we keep the actual pipelines around.
+    volatile Map<String, Pipeline> pipelines = new HashMap<>();
+
+    public PipelineStore(Settings settings) {
+        super(settings);
+    }
+
+    public void buildProcessorFactoryRegistry(ProcessorsRegistry processorsRegistry, ScriptService scriptService) {
+        Map<String, Processor.Factory> processorFactories = new HashMap<>();
+        TemplateService templateService = new InternalTemplateService(scriptService);
+        for (Map.Entry<String, Function<TemplateService, Processor.Factory<?>>> entry : processorsRegistry.entrySet()) {
+            Processor.Factory processorFactory = entry.getValue().apply(templateService);
+            processorFactories.put(entry.getKey(), processorFactory);
+        }
+        this.processorFactoryRegistry = Collections.unmodifiableMap(processorFactories);
+    }
+
+    @Override
+    public void close() throws IOException {
+        // TODO: When org.elasticsearch.node.Node can close Closable instances we should try to remove this code,
+        // since any wired closable should be able to close itself
+        List<Closeable> closeables = new ArrayList<>();
+        for (Processor.Factory factory : processorFactoryRegistry.values()) {
+            if (factory instanceof Closeable) {
+                closeables.add((Closeable) factory);
+            }
+        }
+        IOUtils.close(closeables);
+    }
+
+    @Override
+    public void clusterChanged(ClusterChangedEvent event) {
+        innerUpdatePipelines(event.state());
+    }
+
+    void innerUpdatePipelines(ClusterState state) {
+        IngestMetadata ingestMetadata = state.getMetaData().custom(IngestMetadata.TYPE);
+        if (ingestMetadata == null) {
+            return;
+        }
+
+        Map<String, Pipeline> pipelines = new HashMap<>();
+        for (PipelineConfiguration pipeline : ingestMetadata.getPipelines().values()) {
+            try {
+                pipelines.put(pipeline.getId(), constructPipeline(pipeline.getId(), pipeline.getConfigAsMap()));
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+        }
+        this.pipelines = Collections.unmodifiableMap(pipelines);
+    }
+
+    /**
+     * Deletes the pipeline specified by id in the request.
+     */
+    public void delete(ClusterService clusterService, DeletePipelineRequest request, ActionListener<WritePipelineResponse> listener) {
+        clusterService.submitStateUpdateTask("delete-pipeline-" + request.getId(), new AckedClusterStateUpdateTask<WritePipelineResponse>(request, listener) {
+
+            @Override
+            protected WritePipelineResponse newResponse(boolean acknowledged) {
+                return new WritePipelineResponse(acknowledged);
+            }
+
+            @Override
+            public ClusterState execute(ClusterState currentState) throws Exception {
+                return innerDelete(request, currentState);
+            }
+        });
+    }
+
+    ClusterState innerDelete(DeletePipelineRequest request, ClusterState currentState) {
+        IngestMetadata currentIngestMetadata = currentState.metaData().custom(IngestMetadata.TYPE);
+        if (currentIngestMetadata == null) {
+            return currentState;
+        }
+        Map<String, PipelineConfiguration> pipelines = currentIngestMetadata.getPipelines();
+        if (pipelines.containsKey(request.getId()) == false) {
+            throw new ResourceNotFoundException("pipeline [{}] is missing", request.getId());
+        } else {
+            pipelines = new HashMap<>(pipelines);
+            pipelines.remove(request.getId());
+            ClusterState.Builder newState = ClusterState.builder(currentState);
+            newState.metaData(MetaData.builder(currentState.getMetaData())
+                .putCustom(IngestMetadata.TYPE, new IngestMetadata(pipelines))
+                .build());
+            return newState.build();
+        }
+    }
+
+    /**
+     * Stores the specified pipeline definition in the request.
+     *
+     * @throws IllegalArgumentException If the pipeline holds incorrect configuration
+     */
+    public void put(ClusterService clusterService, PutPipelineRequest request, ActionListener<WritePipelineResponse> listener) throws IllegalArgumentException {
+        try {
+            // validates the pipeline and processor configuration before submitting a cluster update task:
+            Map<String, Object> pipelineConfig = XContentHelper.convertToMap(request.getSource(), false).v2();
+            constructPipeline(request.getId(), pipelineConfig);
+        } catch (Exception e) {
+            throw new IllegalArgumentException("Invalid pipeline configuration", e);
+        }
+        clusterService.submitStateUpdateTask("put-pipeline-" + request.getId(), new AckedClusterStateUpdateTask<WritePipelineResponse>(request, listener) {
+
+            @Override
+            protected WritePipelineResponse newResponse(boolean acknowledged) {
+                return new WritePipelineResponse(acknowledged);
+            }
+
+            @Override
+            public ClusterState execute(ClusterState currentState) throws Exception {
+                return innerPut(request, currentState);
+            }
+        });
+    }
+
+    ClusterState innerPut(PutPipelineRequest request, ClusterState currentState) {
+        IngestMetadata currentIngestMetadata = currentState.metaData().custom(IngestMetadata.TYPE);
+        Map<String, PipelineConfiguration> pipelines;
+        if (currentIngestMetadata != null) {
+            pipelines = new HashMap<>(currentIngestMetadata.getPipelines());
+        } else {
+            pipelines = new HashMap<>();
+        }
+
+        pipelines.put(request.getId(), new PipelineConfiguration(request.getId(), request.getSource()));
+        ClusterState.Builder newState = ClusterState.builder(currentState);
+        newState.metaData(MetaData.builder(currentState.getMetaData())
+            .putCustom(IngestMetadata.TYPE, new IngestMetadata(pipelines))
+            .build());
+        return newState.build();
+    }
+
+    /**
+     * Returns the pipeline by the specified id
+     */
+    public Pipeline get(String id) {
+        return pipelines.get(id);
+    }
+
+    public Map<String, Processor.Factory> getProcessorFactoryRegistry() {
+        return processorFactoryRegistry;
+    }
+
+    /**
+     * @return pipeline configuration specified by id. If multiple ids or wildcards are specified multiple pipelines
+     * may be returned
+     */
+    // Returning PipelineConfiguration instead of Pipeline, because Pipeline and Processor interface don't
+    // know how to serialize themselves.
+    public List<PipelineConfiguration> getPipelines(ClusterState clusterState, String... ids) {
+        IngestMetadata ingestMetadata = clusterState.getMetaData().custom(IngestMetadata.TYPE);
+        return innerGetPipelines(ingestMetadata, ids);
+    }
+
+    List<PipelineConfiguration> innerGetPipelines(IngestMetadata ingestMetadata, String... ids) {
+        if (ingestMetadata == null) {
+            return Collections.emptyList();
+        }
+
+        List<PipelineConfiguration> result = new ArrayList<>(ids.length);
+        for (String id : ids) {
+            if (Regex.isSimpleMatchPattern(id)) {
+                for (Map.Entry<String, PipelineConfiguration> entry : ingestMetadata.getPipelines().entrySet()) {
+                    if (Regex.simpleMatch(id, entry.getKey())) {
+                        result.add(entry.getValue());
+                    }
+                }
+            } else {
+                PipelineConfiguration pipeline = ingestMetadata.getPipelines().get(id);
+                if (pipeline != null) {
+                    result.add(pipeline);
+                }
+            }
+        }
+        return result;
+    }
+
+    private Pipeline constructPipeline(String id, Map<String, Object> config) throws Exception {
+        return factory.create(id, config, processorFactoryRegistry);
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java b/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java
new file mode 100644
index 0000000..766ba77
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/ProcessorsRegistry.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.core.TemplateService;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.function.Function;
+
+public class ProcessorsRegistry {
+
+    private final Map<String, Function<TemplateService, Processor.Factory<?>>> processorFactoryProviders = new HashMap<>();
+
+    /**
+     * Adds a processor factory under a specific name.
+     */
+    public void registerProcessor(String name, Function<TemplateService, Processor.Factory<?>> processorFactoryProvider) {
+        Function<TemplateService, Processor.Factory<?>> provider = processorFactoryProviders.putIfAbsent(name, processorFactoryProvider);
+        if (provider != null) {
+            throw new IllegalArgumentException("Processor factory already registered for name [" + name + "]");
+        }
+    }
+
+    public Set<Map.Entry<String, Function<TemplateService, Processor.Factory<?>>>> entrySet() {
+        return processorFactoryProviders.entrySet();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessor.java b/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessor.java
new file mode 100644
index 0000000..e709ae3
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessor.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.ingest.core;
+
+/**
+ * An Abstract Processor that holds a processorTag field to be used
+ * by other processors.
+ */
+public abstract class AbstractProcessor implements Processor {
+    protected final String tag;
+
+    protected AbstractProcessor(String tag) {
+        this.tag = tag;
+    }
+
+    @Override
+    public String getTag() {
+        return tag;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessorFactory.java b/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessorFactory.java
new file mode 100644
index 0000000..1082461
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/AbstractProcessorFactory.java
@@ -0,0 +1,39 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.ingest.core;
+
+import java.util.Map;
+
+/**
+ * A processor implementation may modify the data belonging to a document.
+ * Whether changes are made and what exactly is modified is up to the implementation.
+ */
+public abstract class AbstractProcessorFactory<P extends Processor> implements Processor.Factory<P> {
+    public static final String TAG_KEY = "tag";
+
+    @Override
+    public P create(Map<String, Object> config) throws Exception {
+        String tag = ConfigurationUtils.readOptionalStringProperty(config, TAG_KEY);
+        return doCreate(tag, config);
+    }
+
+    protected abstract P doCreate(String tag, Map<String, Object> config) throws Exception;
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java b/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
new file mode 100644
index 0000000..bc5fd19
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/CompoundProcessor.java
@@ -0,0 +1,98 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+//TODO(simonw): can all these classes go into org.elasticsearch.ingest?
+
+package org.elasticsearch.ingest.core;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.stream.Collectors;
+
+/**
+ * A Processor that executes a list of other "processors". It executes a separate list of
+ * "onFailureProcessors" when any of the processors throw an {@link Exception}.
+ */
+public class CompoundProcessor implements Processor {
+    static final String ON_FAILURE_MESSAGE_FIELD = "on_failure_message";
+    static final String ON_FAILURE_PROCESSOR_FIELD = "on_failure_processor";
+
+    private final List<Processor> processors;
+    private final List<Processor> onFailureProcessors;
+
+    public CompoundProcessor(Processor... processor) {
+        this(Arrays.asList(processor), Collections.emptyList());
+    }
+
+    public CompoundProcessor(List<Processor> processors, List<Processor> onFailureProcessors) {
+        super();
+        this.processors = processors;
+        this.onFailureProcessors = onFailureProcessors;
+    }
+
+    public List<Processor> getOnFailureProcessors() {
+        return onFailureProcessors;
+    }
+
+    public List<Processor> getProcessors() {
+        return processors;
+    }
+
+    @Override
+    public String getType() {
+        return "compound";
+    }
+
+    @Override
+    public String getTag() {
+        return "compound-processor-" + Objects.hash(processors, onFailureProcessors);
+    }
+
+    @Override
+    public void execute(IngestDocument ingestDocument) throws Exception {
+        for (Processor processor : processors) {
+            try {
+                processor.execute(ingestDocument);
+            } catch (Exception e) {
+                if (onFailureProcessors.isEmpty()) {
+                    throw e;
+                } else {
+                    executeOnFailure(ingestDocument, e, processor.getType());
+                }
+                break;
+            }
+        }
+    }
+
+    void executeOnFailure(IngestDocument ingestDocument, Exception cause, String failedProcessorType) throws Exception {
+        Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
+        try {
+            ingestMetadata.put(ON_FAILURE_MESSAGE_FIELD, cause.getMessage());
+            ingestMetadata.put(ON_FAILURE_PROCESSOR_FIELD, failedProcessorType);
+            for (Processor processor : onFailureProcessors) {
+                processor.execute(ingestDocument);
+            }
+        } finally {
+            ingestMetadata.remove(ON_FAILURE_MESSAGE_FIELD);
+            ingestMetadata.remove(ON_FAILURE_PROCESSOR_FIELD);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java b/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
new file mode 100644
index 0000000..c620416
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/ConfigurationUtils.java
@@ -0,0 +1,163 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import java.util.List;
+import java.util.Map;
+
+public final class ConfigurationUtils {
+
+    private ConfigurationUtils() {
+    }
+
+    /**
+     * Returns and removes the specified optional property from the specified configuration map.
+     *
+     * If the property value isn't of type string a {@link IllegalArgumentException} is thrown.
+     */
+    public static String readOptionalStringProperty(Map<String, Object> configuration, String propertyName) {
+        Object value = configuration.remove(propertyName);
+        return readString(propertyName, value);
+    }
+
+    /**
+     * Returns and removes the specified property from the specified configuration map.
+     *
+     * If the property value isn't of type string an {@link IllegalArgumentException} is thrown.
+     * If the property is missing an {@link IllegalArgumentException} is thrown
+     */
+    public static String readStringProperty(Map<String, Object> configuration, String propertyName) {
+        return readStringProperty(configuration, propertyName, null);
+    }
+
+    /**
+     * Returns and removes the specified property from the specified configuration map.
+     *
+     * If the property value isn't of type string a {@link IllegalArgumentException} is thrown.
+     * If the property is missing and no default value has been specified a {@link IllegalArgumentException} is thrown
+     */
+    public static String readStringProperty(Map<String, Object> configuration, String propertyName, String defaultValue) {
+        Object value = configuration.remove(propertyName);
+        if (value == null && defaultValue != null) {
+            return defaultValue;
+        } else if (value == null) {
+            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
+        }
+        return readString(propertyName, value);
+    }
+
+    private static String readString(String propertyName, Object value) {
+        if (value == null) {
+            return null;
+        }
+        if (value instanceof String) {
+            return (String) value;
+        }
+        throw new IllegalArgumentException("property [" + propertyName + "] isn't a string, but of type [" + value.getClass().getName() + "]");
+    }
+
+    /**
+     * Returns and removes the specified property of type list from the specified configuration map.
+     *
+     * If the property value isn't of type list an {@link IllegalArgumentException} is thrown.
+     */
+    public static <T> List<T> readOptionalList(Map<String, Object> configuration, String propertyName) {
+        Object value = configuration.remove(propertyName);
+        if (value == null) {
+            return null;
+        }
+        return readList(propertyName, value);
+    }
+
+    /**
+     * Returns and removes the specified property of type list from the specified configuration map.
+     *
+     * If the property value isn't of type list an {@link IllegalArgumentException} is thrown.
+     * If the property is missing an {@link IllegalArgumentException} is thrown
+     */
+    public static <T> List<T> readList(Map<String, Object> configuration, String propertyName) {
+        Object value = configuration.remove(propertyName);
+        if (value == null) {
+            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
+        }
+
+        return readList(propertyName, value);
+    }
+
+    private static <T> List<T> readList(String propertyName, Object value) {
+        if (value instanceof List) {
+            @SuppressWarnings("unchecked")
+            List<T> stringList = (List<T>) value;
+            return stringList;
+        } else {
+            throw new IllegalArgumentException("property [" + propertyName + "] isn't a list, but of type [" + value.getClass().getName() + "]");
+        }
+    }
+
+    /**
+     * Returns and removes the specified property of type map from the specified configuration map.
+     *
+     * If the property value isn't of type map an {@link IllegalArgumentException} is thrown.
+     * If the property is missing an {@link IllegalArgumentException} is thrown
+     */
+    public static <T> Map<String, T> readMap(Map<String, Object> configuration, String propertyName) {
+        Object value = configuration.remove(propertyName);
+        if (value == null) {
+            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
+        }
+
+        return readMap(propertyName, value);
+    }
+
+    /**
+     * Returns and removes the specified property of type map from the specified configuration map.
+     *
+     * If the property value isn't of type map an {@link IllegalArgumentException} is thrown.
+     */
+    public static <T> Map<String, T> readOptionalMap(Map<String, Object> configuration, String propertyName) {
+        Object value = configuration.remove(propertyName);
+        if (value == null) {
+            return null;
+        }
+
+        return readMap(propertyName, value);
+    }
+
+    private static <T> Map<String, T> readMap(String propertyName, Object value) {
+        if (value instanceof Map) {
+            @SuppressWarnings("unchecked")
+            Map<String, T> map = (Map<String, T>) value;
+            return map;
+        } else {
+            throw new IllegalArgumentException("property [" + propertyName + "] isn't a map, but of type [" + value.getClass().getName() + "]");
+        }
+    }
+
+    /**
+     * Returns and removes the specified property as an {@link Object} from the specified configuration map.
+     */
+    public static Object readObject(Map<String, Object> configuration, String propertyName) {
+        Object value = configuration.remove(propertyName);
+        if (value == null) {
+            throw new IllegalArgumentException("required property [" + propertyName + "] is missing");
+        }
+        return value;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java b/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
new file mode 100644
index 0000000..c8f87fa
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java
@@ -0,0 +1,544 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.index.mapper.internal.IdFieldMapper;
+import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.index.mapper.internal.RoutingFieldMapper;
+import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
+import org.elasticsearch.index.mapper.internal.TTLFieldMapper;
+import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
+import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
+
+import java.text.DateFormat;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Objects;
+import java.util.TimeZone;
+
+/**
+ * Represents a single document being captured before indexing and holds the source and metadata (like id, type and index).
+ */
+public final class IngestDocument {
+
+    public final static String INGEST_KEY = "_ingest";
+
+    static final String TIMESTAMP = "timestamp";
+
+    private final Map<String, Object> sourceAndMetadata;
+    private final Map<String, String> ingestMetadata;
+
+    public IngestDocument(String index, String type, String id, String routing, String parent, String timestamp, String ttl, Map<String, Object> source) {
+        this.sourceAndMetadata = new HashMap<>();
+        this.sourceAndMetadata.putAll(source);
+        this.sourceAndMetadata.put(MetaData.INDEX.getFieldName(), index);
+        this.sourceAndMetadata.put(MetaData.TYPE.getFieldName(), type);
+        this.sourceAndMetadata.put(MetaData.ID.getFieldName(), id);
+        if (routing != null) {
+            this.sourceAndMetadata.put(MetaData.ROUTING.getFieldName(), routing);
+        }
+        if (parent != null) {
+            this.sourceAndMetadata.put(MetaData.PARENT.getFieldName(), parent);
+        }
+        if (timestamp != null) {
+            this.sourceAndMetadata.put(MetaData.TIMESTAMP.getFieldName(), timestamp);
+        }
+        if (ttl != null) {
+            this.sourceAndMetadata.put(MetaData.TTL.getFieldName(), ttl);
+        }
+
+        this.ingestMetadata = new HashMap<>();
+        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZZ", Locale.ROOT);
+        df.setTimeZone(TimeZone.getTimeZone("UTC"));
+        this.ingestMetadata.put(TIMESTAMP, df.format(new Date()));
+    }
+
+    /**
+     * Copy constructor that creates a new {@link IngestDocument} which has exactly the same properties as the one provided as argument
+     */
+    public IngestDocument(IngestDocument other) {
+        this(new HashMap<>(other.sourceAndMetadata), new HashMap<>(other.ingestMetadata));
+    }
+
+    /**
+     * Constructor needed for testing that allows to create a new {@link IngestDocument} given the provided elasticsearch metadata,
+     * source and ingest metadata. This is needed because the ingest metadata will be initialized with the current timestamp at
+     * init time, which makes equality comparisons impossible in tests.
+     */
+    public IngestDocument(Map<String, Object> sourceAndMetadata, Map<String, String> ingestMetadata) {
+        this.sourceAndMetadata = sourceAndMetadata;
+        this.ingestMetadata = ingestMetadata;
+    }
+
+    /**
+     * Returns the value contained in the document for the provided path
+     * @param path The path within the document in dot-notation
+     * @param clazz The expected class of the field value
+     * @return the value for the provided path if existing, null otherwise
+     * @throws IllegalArgumentException if the path is null, empty, invalid, if the field doesn't exist
+     * or if the field that is found at the provided path is not of the expected type.
+     */
+    public <T> T getFieldValue(String path, Class<T> clazz) {
+        FieldPath fieldPath = new FieldPath(path);
+        Object context = fieldPath.initialContext;
+        for (String pathElement : fieldPath.pathElements) {
+            context = resolve(pathElement, path, context);
+        }
+        return cast(path, context, clazz);
+    }
+
+    /**
+     * Checks whether the document contains a value for the provided path
+     * @param path The path within the document in dot-notation
+     * @return true if the document contains a value for the field, false otherwise
+     * @throws IllegalArgumentException if the path is null, empty or invalid.
+     */
+    public boolean hasField(String path) {
+        FieldPath fieldPath = new FieldPath(path);
+        Object context = fieldPath.initialContext;
+        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
+            String pathElement = fieldPath.pathElements[i];
+            if (context == null) {
+                return false;
+            }
+            if (context instanceof Map) {
+                @SuppressWarnings("unchecked")
+                Map<String, Object> map = (Map<String, Object>) context;
+                context = map.get(pathElement);
+            } else if (context instanceof List) {
+                @SuppressWarnings("unchecked")
+                List<Object> list = (List<Object>) context;
+                try {
+                    int index = Integer.parseInt(pathElement);
+                    if (index < 0 || index >= list.size()) {
+                        return false;
+                    }
+                    context = list.get(index);
+                } catch (NumberFormatException e) {
+                    return false;
+                }
+
+            } else {
+                return false;
+            }
+        }
+
+        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
+        if (context instanceof Map) {
+            @SuppressWarnings("unchecked")
+            Map<String, Object> map = (Map<String, Object>) context;
+            return map.containsKey(leafKey);
+        }
+        if (context instanceof List) {
+            @SuppressWarnings("unchecked")
+            List<Object> list = (List<Object>) context;
+            try {
+                int index = Integer.parseInt(leafKey);
+                return index >= 0 && index < list.size();
+            } catch (NumberFormatException e) {
+                return false;
+            }
+        }
+        return false;
+    }
+
+    /**
+     * Removes the field identified by the provided path.
+     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
+     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
+     */
+    public void removeField(TemplateService.Template fieldPathTemplate) {
+        removeField(renderTemplate(fieldPathTemplate));
+    }
+
+    /**
+     * Removes the field identified by the provided path.
+     * @param path the path of the field to be removed
+     * @throws IllegalArgumentException if the path is null, empty, invalid or if the field doesn't exist.
+     */
+    public void removeField(String path) {
+        FieldPath fieldPath = new FieldPath(path);
+        Object context = fieldPath.initialContext;
+        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
+            context = resolve(fieldPath.pathElements[i], path, context);
+        }
+
+        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
+        if (context instanceof Map) {
+            @SuppressWarnings("unchecked")
+            Map<String, Object> map = (Map<String, Object>) context;
+            if (map.containsKey(leafKey)) {
+                map.remove(leafKey);
+                return;
+            }
+            throw new IllegalArgumentException("field [" + leafKey + "] not present as part of path [" + path + "]");
+        }
+        if (context instanceof List) {
+            @SuppressWarnings("unchecked")
+            List<Object> list = (List<Object>) context;
+            int index;
+            try {
+                index = Integer.parseInt(leafKey);
+            } catch (NumberFormatException e) {
+                throw new IllegalArgumentException("[" + leafKey + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
+            }
+            if (index < 0 || index >= list.size()) {
+                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
+            }
+            list.remove(index);
+            return;
+        }
+
+        if (context == null) {
+            throw new IllegalArgumentException("cannot remove [" + leafKey + "] from null as part of path [" + path + "]");
+        }
+        throw new IllegalArgumentException("cannot remove [" + leafKey + "] from object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
+    }
+
+    private static Object resolve(String pathElement, String fullPath, Object context) {
+        if (context == null) {
+            throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from null as part of path [" + fullPath + "]");
+        }
+        if (context instanceof Map) {
+            @SuppressWarnings("unchecked")
+            Map<String, Object> map = (Map<String, Object>) context;
+            if (map.containsKey(pathElement)) {
+                return map.get(pathElement);
+            }
+            throw new IllegalArgumentException("field [" + pathElement + "] not present as part of path [" + fullPath + "]");
+        }
+        if (context instanceof List) {
+            @SuppressWarnings("unchecked")
+            List<Object> list = (List<Object>) context;
+            int index;
+            try {
+                index = Integer.parseInt(pathElement);
+            } catch (NumberFormatException e) {
+                throw new IllegalArgumentException("[" + pathElement + "] is not an integer, cannot be used as an index as part of path [" + fullPath + "]", e);
+            }
+            if (index < 0 || index >= list.size()) {
+                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + fullPath + "]");
+            }
+            return list.get(index);
+        }
+        throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from object of type [" + context.getClass().getName() + "] as part of path [" + fullPath + "]");
+    }
+
+    /**
+     * Appends the provided value to the provided path in the document.
+     * Any non existing path element will be created.
+     * If the path identifies a list, the value will be appended to the existing list.
+     * If the path identifies a scalar, the scalar will be converted to a list and
+     * the provided value will be added to the newly created list.
+     * Supports multiple values too provided in forms of list, in that case all the values will be appeneded to the
+     * existing (or newly created) list.
+     * @param path The path within the document in dot-notation
+     * @param value The value or values to append to the existing ones
+     * @throws IllegalArgumentException if the path is null, empty or invalid.
+     */
+    public void appendFieldValue(String path, Object value) {
+        setFieldValue(path, value, true);
+    }
+
+    /**
+     * Appends the provided value to the provided path in the document.
+     * Any non existing path element will be created.
+     * If the path identifies a list, the value will be appended to the existing list.
+     * If the path identifies a scalar, the scalar will be converted to a list and
+     * the provided value will be added to the newly created list.
+     * Supports multiple values too provided in forms of list, in that case all the values will be appeneded to the
+     * existing (or newly created) list.
+     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
+     * @param valueSource The value source that will produce the value or values to append to the existing ones
+     * @throws IllegalArgumentException if the path is null, empty or invalid.
+     */
+    public void appendFieldValue(TemplateService.Template fieldPathTemplate, ValueSource valueSource) {
+        Map<String, Object> model = createTemplateModel();
+        appendFieldValue(fieldPathTemplate.execute(model), valueSource.copyAndResolve(model));
+    }
+
+    /**
+     * Sets the provided value to the provided path in the document.
+     * Any non existing path element will be created.
+     * If the last item in the path is a list, the value will replace the existing list as a whole.
+     * Use {@link #appendFieldValue(String, Object)} to append values to lists instead.
+     * @param path The path within the document in dot-notation
+     * @param value The value to put in for the path key
+     * @throws IllegalArgumentException if the path is null, empty, invalid or if the value cannot be set to the
+     * item identified by the provided path.
+     */
+    public void setFieldValue(String path, Object value) {
+        setFieldValue(path, value, false);
+    }
+
+    /**
+     * Sets the provided value to the provided path in the document.
+     * Any non existing path element will be created. If the last element is a list,
+     * the value will replace the existing list.
+     * @param fieldPathTemplate Resolves to the path with dot-notation within the document
+     * @param valueSource The value source that will produce the value to put in for the path key
+     * @throws IllegalArgumentException if the path is null, empty, invalid or if the value cannot be set to the
+     * item identified by the provided path.
+     */
+    public void setFieldValue(TemplateService.Template fieldPathTemplate, ValueSource valueSource) {
+        Map<String, Object> model = createTemplateModel();
+        setFieldValue(fieldPathTemplate.execute(model), valueSource.copyAndResolve(model), false);
+    }
+
+    private void setFieldValue(String path, Object value, boolean append) {
+        FieldPath fieldPath = new FieldPath(path);
+        Object context = fieldPath.initialContext;
+        for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {
+            String pathElement = fieldPath.pathElements[i];
+            if (context == null) {
+                throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from null as part of path [" + path + "]");
+            }
+            if (context instanceof Map) {
+                @SuppressWarnings("unchecked")
+                Map<String, Object> map = (Map<String, Object>) context;
+                if (map.containsKey(pathElement)) {
+                    context = map.get(pathElement);
+                } else {
+                    HashMap<Object, Object> newMap = new HashMap<>();
+                    map.put(pathElement, newMap);
+                    context = newMap;
+                }
+            } else if (context instanceof List) {
+                @SuppressWarnings("unchecked")
+                List<Object> list = (List<Object>) context;
+                int index;
+                try {
+                    index = Integer.parseInt(pathElement);
+                } catch (NumberFormatException e) {
+                    throw new IllegalArgumentException("[" + pathElement + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
+                }
+                if (index < 0 || index >= list.size()) {
+                    throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
+                }
+                context = list.get(index);
+            } else {
+                throw new IllegalArgumentException("cannot resolve [" + pathElement + "] from object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
+            }
+        }
+
+        String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];
+        if (context == null) {
+            throw new IllegalArgumentException("cannot set [" + leafKey + "] with null parent as part of path [" + path + "]");
+        }
+        if (context instanceof Map) {
+            @SuppressWarnings("unchecked")
+            Map<String, Object> map = (Map<String, Object>) context;
+            if (append) {
+                if (map.containsKey(leafKey)) {
+                    Object object = map.get(leafKey);
+                    List<Object> list = appendValues(object, value);
+                    if (list != object) {
+                        map.put(leafKey, list);
+                    }
+                } else {
+                    List<Object> list = new ArrayList<>();
+                    appendValues(list, value);
+                    map.put(leafKey, list);
+                }
+                return;
+            }
+            map.put(leafKey, value);
+        } else if (context instanceof List) {
+            @SuppressWarnings("unchecked")
+            List<Object> list = (List<Object>) context;
+            int index;
+            try {
+                index = Integer.parseInt(leafKey);
+            } catch (NumberFormatException e) {
+                throw new IllegalArgumentException("[" + leafKey + "] is not an integer, cannot be used as an index as part of path [" + path + "]", e);
+            }
+            if (index < 0 || index >= list.size()) {
+                throw new IllegalArgumentException("[" + index + "] is out of bounds for array with length [" + list.size() + "] as part of path [" + path + "]");
+            }
+            if (append) {
+                Object object = list.get(index);
+                List<Object> newList = appendValues(object, value);
+                if (newList != object) {
+                    list.set(index, newList);
+                }
+                return;
+            }
+            list.set(index, value);
+        } else {
+            throw new IllegalArgumentException("cannot set [" + leafKey + "] with parent object of type [" + context.getClass().getName() + "] as part of path [" + path + "]");
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    private static List<Object> appendValues(Object maybeList, Object value) {
+        List<Object> list;
+        if (maybeList instanceof List) {
+            //maybeList is already a list, we append the provided values to it
+            list = (List<Object>) maybeList;
+        } else {
+            //maybeList is a scalar, we convert it to a list and append the provided values to it
+            list = new ArrayList<>();
+            list.add(maybeList);
+        }
+        appendValues(list, value);
+        return list;
+    }
+
+    private static void appendValues(List<Object> list, Object value) {
+        if (value instanceof List) {
+            @SuppressWarnings("unchecked")
+            List<?> valueList = (List<?>) value;
+            valueList.stream().forEach(list::add);
+        } else {
+            list.add(value);
+        }
+    }
+
+    private static <T> T cast(String path, Object object, Class<T> clazz) {
+        if (object == null) {
+            return null;
+        }
+        if (clazz.isInstance(object)) {
+            return clazz.cast(object);
+        }
+        throw new IllegalArgumentException("field [" + path + "] of type [" + object.getClass().getName() + "] cannot be cast to [" + clazz.getName() + "]");
+    }
+
+    public String renderTemplate(TemplateService.Template template) {
+        return template.execute(createTemplateModel());
+    }
+
+    private Map<String, Object> createTemplateModel() {
+        Map<String, Object> model = new HashMap<>(sourceAndMetadata);
+        model.put(SourceFieldMapper.NAME, sourceAndMetadata);
+        // If there is a field in the source with the name '_ingest' it gets overwritten here,
+        // if access to that field is required then it get accessed via '_source._ingest'
+        model.put(INGEST_KEY, ingestMetadata);
+        return model;
+    }
+
+    /**
+     * one time operation that extracts the metadata fields from the ingest document and returns them.
+     * Metadata fields that used to be accessible as ordinary top level fields will be removed as part of this call.
+     */
+    public Map<MetaData, String> extractMetadata() {
+        Map<MetaData, String> metadataMap = new HashMap<>();
+        for (MetaData metaData : MetaData.values()) {
+            metadataMap.put(metaData, cast(metaData.getFieldName(), sourceAndMetadata.remove(metaData.getFieldName()), String.class));
+        }
+        return metadataMap;
+    }
+
+    /**
+     * Returns the available ingest metadata fields, by default only timestamp, but it is possible to set additional ones.
+     * Use only for reading values, modify them instead using {@link #setFieldValue(String, Object)} and {@link #removeField(String)}
+     */
+    public Map<String, String> getIngestMetadata() {
+        return this.ingestMetadata;
+    }
+
+    /**
+     * Returns the document including its metadata fields, unless {@link #extractMetadata()} has been called, in which case the
+     * metadata fields will not be present anymore.
+     * Modify the document instead using {@link #setFieldValue(String, Object)} and {@link #removeField(String)}
+     */
+    public Map<String, Object> getSourceAndMetadata() {
+        return this.sourceAndMetadata;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (obj == this) { return true; }
+        if (obj == null || getClass() != obj.getClass()) {
+            return false;
+        }
+
+        IngestDocument other = (IngestDocument) obj;
+        return Objects.equals(sourceAndMetadata, other.sourceAndMetadata) &&
+                Objects.equals(ingestMetadata, other.ingestMetadata);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(sourceAndMetadata, ingestMetadata);
+    }
+
+    @Override
+    public String toString() {
+        return "IngestDocument{" +
+                " sourceAndMetadata=" + sourceAndMetadata +
+                ", ingestMetadata=" + ingestMetadata +
+                '}';
+    }
+
+    public enum MetaData {
+        INDEX(IndexFieldMapper.NAME),
+        TYPE(TypeFieldMapper.NAME),
+        ID(IdFieldMapper.NAME),
+        ROUTING(RoutingFieldMapper.NAME),
+        PARENT(ParentFieldMapper.NAME),
+        TIMESTAMP(TimestampFieldMapper.NAME),
+        TTL(TTLFieldMapper.NAME);
+
+        private final String fieldName;
+
+        MetaData(String fieldName) {
+            this.fieldName = fieldName;
+        }
+
+        public String getFieldName() {
+            return fieldName;
+        }
+    }
+
+    private class FieldPath {
+        private final String[] pathElements;
+        private final Object initialContext;
+
+        private FieldPath(String path) {
+            if (Strings.isEmpty(path)) {
+                throw new IllegalArgumentException("path cannot be null nor empty");
+            }
+            String newPath;
+            if (path.startsWith(INGEST_KEY + ".")) {
+                initialContext = ingestMetadata;
+                newPath = path.substring(8, path.length());
+            } else {
+                initialContext = sourceAndMetadata;
+                if (path.startsWith(SourceFieldMapper.NAME + ".")) {
+                    newPath = path.substring(8, path.length());
+                } else {
+                    newPath = path;
+                }
+            }
+            this.pathElements = Strings.splitStringToArray(newPath, '.');
+            if (pathElements.length == 0) {
+                throw new IllegalArgumentException("path [" + path + "] is not valid");
+            }
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java b/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
new file mode 100644
index 0000000..68ba8da
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/Pipeline.java
@@ -0,0 +1,126 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * A pipeline is a list of {@link Processor} instances grouped under a unique id.
+ */
+public final class Pipeline {
+
+    final static String DESCRIPTION_KEY = "description";
+    final static String PROCESSORS_KEY = "processors";
+    final static String ON_FAILURE_KEY = "on_failure";
+
+    private final String id;
+    private final String description;
+    private final CompoundProcessor compoundProcessor;
+
+    public Pipeline(String id, String description, CompoundProcessor compoundProcessor) {
+        this.id = id;
+        this.description = description;
+        this.compoundProcessor = compoundProcessor;
+    }
+
+    /**
+     * Modifies the data of a document to be indexed based on the processor this pipeline holds
+     */
+    public void execute(IngestDocument ingestDocument) throws Exception {
+        compoundProcessor.execute(ingestDocument);
+    }
+
+    /**
+     * The unique id of this pipeline
+     */
+    public String getId() {
+        return id;
+    }
+
+    /**
+     * An optional description of what this pipeline is doing to the data gets processed by this pipeline.
+     */
+    public String getDescription() {
+        return description;
+    }
+
+    /**
+     * Unmodifiable list containing each processor that operates on the data.
+     */
+    public List<Processor> getProcessors() {
+        return compoundProcessor.getProcessors();
+    }
+
+    /**
+     * Unmodifiable list containing each on_failure processor that operates on the data in case of
+     * exception thrown in pipeline processors
+     */
+    public List<Processor> getOnFailureProcessors() {
+        return compoundProcessor.getOnFailureProcessors();
+    }
+
+    public final static class Factory {
+
+        public Pipeline create(String id, Map<String, Object> config, Map<String, Processor.Factory> processorRegistry) throws Exception {
+            String description = ConfigurationUtils.readOptionalStringProperty(config, DESCRIPTION_KEY);
+            List<Processor> processors = readProcessors(PROCESSORS_KEY, processorRegistry, config);
+            List<Processor> onFailureProcessors = readProcessors(ON_FAILURE_KEY, processorRegistry, config);
+            if (config.isEmpty() == false) {
+                throw new IllegalArgumentException("pipeline [" + id + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
+            }
+            CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.unmodifiableList(processors), Collections.unmodifiableList(onFailureProcessors));
+            return new Pipeline(id, description, compoundProcessor);
+        }
+
+        private List<Processor> readProcessors(String fieldName, Map<String, Processor.Factory> processorRegistry, Map<String, Object> config) throws Exception {
+            List<Map<String, Map<String, Object>>> processorConfigs = ConfigurationUtils.readOptionalList(config, fieldName);
+            List<Processor> processors = new ArrayList<>();
+            if (processorConfigs != null) {
+                for (Map<String, Map<String, Object>> processorConfigWithKey : processorConfigs) {
+                    for (Map.Entry<String, Map<String, Object>> entry : processorConfigWithKey.entrySet()) {
+                        processors.add(readProcessor(processorRegistry, entry.getKey(), entry.getValue()));
+                    }
+                }
+            }
+
+            return processors;
+        }
+
+        private Processor readProcessor(Map<String, Processor.Factory> processorRegistry, String type, Map<String, Object> config) throws Exception {
+            Processor.Factory factory = processorRegistry.get(type);
+            if (factory != null) {
+                List<Processor> onFailureProcessors = readProcessors(ON_FAILURE_KEY, processorRegistry, config);
+                Processor processor = factory.create(config);
+                if (config.isEmpty() == false) {
+                    throw new IllegalArgumentException("processor [" + type + "] doesn't support one or more provided configuration parameters " + Arrays.toString(config.keySet().toArray()));
+                }
+                if (onFailureProcessors.isEmpty()) {
+                    return processor;
+                }
+                return new CompoundProcessor(Collections.singletonList(processor), onFailureProcessors);
+            }
+            throw new IllegalArgumentException("No processor type exists with name [" + type + "]");
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/Processor.java b/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
new file mode 100644
index 0000000..f178051
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/Processor.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.ingest.core;
+
+import java.util.Map;
+
+/**
+ * A processor implementation may modify the data belonging to a document.
+ * Whether changes are made and what exactly is modified is up to the implementation.
+ */
+public interface Processor {
+
+    /**
+     * Introspect and potentially modify the incoming data.
+     */
+    void execute(IngestDocument ingestDocument) throws Exception;
+
+    /**
+     * Gets the type of a processor
+     */
+    String getType();
+
+    /**
+     * Gets the tag of a processor.
+     */
+    String getTag();
+
+    /**
+     * A factory that knows how to construct a processor based on a map of maps.
+     */
+    interface Factory<P extends Processor> {
+
+        /**
+         * Creates a processor based on the specified map of maps config.
+         *
+         * Implementations are responsible for removing the used keys, so that after creating a pipeline ingest can
+         * verify if all configurations settings have been used.
+         */
+        P create(Map<String, Object> config) throws Exception;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java b/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java
new file mode 100644
index 0000000..8988c92
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/TemplateService.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.ingest.core;
+
+import java.util.Map;
+
+/**
+ * Abstraction for the ingest template engine used to decouple {@link IngestDocument} from {@link org.elasticsearch.script.ScriptService}.
+ * Allows to compile a template into an ingest {@link Template} object.
+ * A compiled template can be executed by calling its {@link Template#execute(Map)} method.
+ */
+public interface TemplateService {
+
+    Template compile(String template);
+
+    interface Template {
+
+        String execute(Map<String, Object> model);
+
+        String getKey();
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java b/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java
new file mode 100644
index 0000000..e9f09a1
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/core/ValueSource.java
@@ -0,0 +1,191 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+
+/**
+ * Holds a value. If the value is requested a copy is made and optionally template snippets are resolved too.
+ */
+public interface ValueSource {
+
+    /**
+     * Returns a copy of the value this ValueSource holds and resolves templates if there're any.
+     *
+     * For immutable values only a copy of the reference to the value is made.
+     *
+     * @param model The model to be used when resolving any templates
+     * @return copy of the wrapped value
+     */
+    Object copyAndResolve(Map<String, Object> model);
+
+    static ValueSource wrap(Object value, TemplateService templateService) {
+        if (value instanceof Map) {
+            @SuppressWarnings("unchecked")
+            Map<Object, Object> mapValue = (Map) value;
+            Map<ValueSource, ValueSource> valueTypeMap = new HashMap<>(mapValue.size());
+            for (Map.Entry<Object, Object> entry : mapValue.entrySet()) {
+                valueTypeMap.put(wrap(entry.getKey(), templateService), wrap(entry.getValue(), templateService));
+            }
+            return new MapValue(valueTypeMap);
+        } else if (value instanceof List) {
+            @SuppressWarnings("unchecked")
+            List<Object> listValue = (List) value;
+            List<ValueSource> valueSourceList = new ArrayList<>(listValue.size());
+            for (Object item : listValue) {
+                valueSourceList.add(wrap(item, templateService));
+            }
+            return new ListValue(valueSourceList);
+        } else if (value == null || value instanceof Number || value instanceof Boolean) {
+            return new ObjectValue(value);
+        } else if (value instanceof String) {
+            return new TemplatedValue(templateService.compile((String) value));
+        } else {
+            throw new IllegalArgumentException("unexpected value type [" + value.getClass() + "]");
+        }
+    }
+
+    final class MapValue implements ValueSource {
+
+        private final Map<ValueSource, ValueSource> map;
+
+        MapValue(Map<ValueSource, ValueSource> map) {
+            this.map = map;
+        }
+
+        @Override
+        public Object copyAndResolve(Map<String, Object> model) {
+            Map<Object, Object> copy = new HashMap<>();
+            for (Map.Entry<ValueSource, ValueSource> entry : this.map.entrySet()) {
+                copy.put(entry.getKey().copyAndResolve(model), entry.getValue().copyAndResolve(model));
+            }
+            return copy;
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            MapValue mapValue = (MapValue) o;
+            return map.equals(mapValue.map);
+
+        }
+
+        @Override
+        public int hashCode() {
+            return map.hashCode();
+        }
+    }
+
+    final class ListValue implements ValueSource {
+
+        private final List<ValueSource> values;
+
+        ListValue(List<ValueSource> values) {
+            this.values = values;
+        }
+
+        @Override
+        public Object copyAndResolve(Map<String, Object> model) {
+            List<Object> copy = new ArrayList<>(values.size());
+            for (ValueSource value : values) {
+                copy.add(value.copyAndResolve(model));
+            }
+            return copy;
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            ListValue listValue = (ListValue) o;
+            return values.equals(listValue.values);
+
+        }
+
+        @Override
+        public int hashCode() {
+            return values.hashCode();
+        }
+    }
+
+    final class ObjectValue implements ValueSource {
+
+        private final Object value;
+
+        ObjectValue(Object value) {
+            this.value = value;
+        }
+
+        @Override
+        public Object copyAndResolve(Map<String, Object> model) {
+            return value;
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            ObjectValue objectValue = (ObjectValue) o;
+            return Objects.equals(value, objectValue.value);
+        }
+
+        @Override
+        public int hashCode() {
+            return Objects.hashCode(value);
+        }
+    }
+
+    final class TemplatedValue implements ValueSource {
+
+        private final TemplateService.Template template;
+
+        TemplatedValue(TemplateService.Template template) {
+            this.template = template;
+        }
+
+        @Override
+        public Object copyAndResolve(Map<String, Object> model) {
+            return template.execute(model);
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (o == null || getClass() != o.getClass()) return false;
+
+            TemplatedValue templatedValue = (TemplatedValue) o;
+            return Objects.equals(template.getKey(), templatedValue.template.getKey());
+        }
+
+        @Override
+        public int hashCode() {
+            return Objects.hashCode(template.getKey());
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java
new file mode 100644
index 0000000..32e5476
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/AbstractStringProcessor.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Processor;
+
+import java.util.Map;
+
+/**
+ * Base class for processors that manipulate strings and require a single "fields" array config value, which
+ * holds a list of field names in string format.
+ */
+public abstract class AbstractStringProcessor extends AbstractProcessor {
+    private final String field;
+
+    protected AbstractStringProcessor(String tag, String field) {
+        super(tag);
+        this.field = field;
+    }
+
+    public String getField() {
+        return field;
+    }
+
+    @Override
+    public final void execute(IngestDocument document) {
+        String val = document.getFieldValue(field, String.class);
+        if (val == null) {
+            throw new IllegalArgumentException("field [" + field + "] is null, cannot process it.");
+        }
+        document.setFieldValue(field, process(val));
+    }
+
+    protected abstract String process(String value);
+
+    public static abstract class Factory<T extends AbstractStringProcessor> extends AbstractProcessorFactory<T> {
+
+        @Override
+        public T doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            return newProcessor(processorTag, field);
+        }
+
+        protected abstract T newProcessor(String processorTag, String field);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java
new file mode 100644
index 0000000..deff384
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/AppendProcessor.java
@@ -0,0 +1,82 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.ingest.core.ValueSource;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+
+import java.util.Map;
+
+/**
+ * Processor that appends value or values to existing lists. If the field is not present a new list holding the
+ * provided values will be added. If the field is a scalar it will be converted to a single item list and the provided
+ * values will be added to the newly created list.
+ */
+public class AppendProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "append";
+
+    private final TemplateService.Template field;
+    private final ValueSource value;
+
+    AppendProcessor(String tag, TemplateService.Template field, ValueSource value) {
+        super(tag);
+        this.field = field;
+        this.value = value;
+    }
+
+    public TemplateService.Template getField() {
+        return field;
+    }
+
+    public ValueSource getValue() {
+        return value;
+    }
+
+    @Override
+    public void execute(IngestDocument ingestDocument) throws Exception {
+        ingestDocument.appendFieldValue(field, value);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static final class Factory extends AbstractProcessorFactory<AppendProcessor> {
+
+        private final TemplateService templateService;
+
+        public Factory(TemplateService templateService) {
+            this.templateService = templateService;
+        }
+
+        @Override
+        public AppendProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            Object value = ConfigurationUtils.readObject(config, "value");
+            return new AppendProcessor(processorTag, templateService.compile(field), ValueSource.wrap(value, templateService));
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
new file mode 100644
index 0000000..5b6bacf
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/ConvertProcessor.java
@@ -0,0 +1,145 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+
+/**
+ * Processor that converts fields content to a different type. Supported types are: integer, float, boolean and string.
+ * Throws exception if the field is not there or the conversion fails.
+ */
+public class ConvertProcessor extends AbstractProcessor {
+
+    enum Type {
+        INTEGER {
+            @Override
+            public Object convert(Object value) {
+                try {
+                    return Integer.parseInt(value.toString());
+                } catch(NumberFormatException e) {
+                    throw new IllegalArgumentException("unable to convert [" + value + "] to integer", e);
+                }
+
+            }
+        }, FLOAT {
+            @Override
+            public Object convert(Object value) {
+                try {
+                    return Float.parseFloat(value.toString());
+                } catch(NumberFormatException e) {
+                    throw new IllegalArgumentException("unable to convert [" + value + "] to float", e);
+                }
+            }
+        }, BOOLEAN {
+            @Override
+            public Object convert(Object value) {
+                if (value.toString().equalsIgnoreCase("true")) {
+                    return true;
+                } else if (value.toString().equalsIgnoreCase("false")) {
+                    return false;
+                } else {
+                    throw new IllegalArgumentException("[" + value + "] is not a boolean value, cannot convert to boolean");
+                }
+            }
+        }, STRING {
+            @Override
+            public Object convert(Object value) {
+                return value.toString();
+            }
+        };
+
+        @Override
+        public final String toString() {
+            return name().toLowerCase(Locale.ROOT);
+        }
+
+        public abstract Object convert(Object value);
+
+        public static Type fromString(String type) {
+            try {
+                return Type.valueOf(type.toUpperCase(Locale.ROOT));
+            } catch(IllegalArgumentException e) {
+                throw new IllegalArgumentException("type [" + type + "] not supported, cannot convert field.", e);
+            }
+        }
+    }
+
+    public static final String TYPE = "convert";
+
+    private final String field;
+    private final Type convertType;
+
+    ConvertProcessor(String tag, String field, Type convertType) {
+        super(tag);
+        this.field = field;
+        this.convertType = convertType;
+    }
+
+    String getField() {
+        return field;
+    }
+
+    Type getConvertType() {
+        return convertType;
+    }
+
+    @Override
+    public void execute(IngestDocument document) {
+        Object oldValue = document.getFieldValue(field, Object.class);
+        Object newValue;
+        if (oldValue == null) {
+            throw new IllegalArgumentException("Field [" + field + "] is null, cannot be converted to type [" + convertType + "]");
+        }
+
+        if (oldValue instanceof List) {
+            List<?> list = (List<?>) oldValue;
+            List<Object> newList = new ArrayList<>();
+            for (Object value : list) {
+                newList.add(convertType.convert(value));
+            }
+            newValue = newList;
+        } else {
+            newValue = convertType.convert(oldValue);
+        }
+        document.setFieldValue(field, newValue);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractProcessorFactory<ConvertProcessor> {
+        @Override
+        public ConvertProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            Type convertType = Type.fromString(ConfigurationUtils.readStringProperty(config, "type"));
+            return new ConvertProcessor(processorTag, field, convertType);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java b/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java
new file mode 100644
index 0000000..282b291
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/DateFormat.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+import org.joda.time.format.DateTimeFormat;
+import org.joda.time.format.ISODateTimeFormat;
+
+import java.util.Locale;
+import java.util.function.Function;
+
+enum DateFormat {
+    Iso8601 {
+        @Override
+        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
+            return ISODateTimeFormat.dateTimeParser().withZone(timezone)::parseDateTime;
+        }
+    },
+    Unix {
+        @Override
+        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
+            return (date) -> new DateTime((long)(Float.parseFloat(date) * 1000), timezone);
+        }
+    },
+    UnixMs {
+        @Override
+        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
+            return (date) -> new DateTime(Long.parseLong(date), timezone);
+        }
+    },
+    Tai64n {
+        @Override
+        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
+            return (date) -> new DateTime(parseMillis(date), timezone);
+        }
+
+        private long parseMillis(String date) {
+            if (date.startsWith("@")) {
+                date = date.substring(1);
+            }
+            long base = Long.parseLong(date.substring(1, 16), 16);
+            // 1356138046000
+            long rest = Long.parseLong(date.substring(16, 24), 16);
+            return ((base * 1000) - 10000) + (rest/1000000);
+        }
+    },
+    Joda {
+        @Override
+        Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale) {
+            return DateTimeFormat.forPattern(format)
+                .withDefaultYear((new DateTime(DateTimeZone.UTC)).getYear())
+                .withZone(timezone).withLocale(locale)::parseDateTime;
+        }
+    };
+
+    abstract Function<String, DateTime> getFunction(String format, DateTimeZone timezone, Locale locale);
+
+    static DateFormat fromString(String format) {
+        switch (format) {
+            case "ISO8601":
+                return Iso8601;
+            case "UNIX":
+                return Unix;
+            case "UNIX_MS":
+                return UnixMs;
+            case "TAI64N":
+                return Tai64n;
+            default:
+                return Joda;
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java
new file mode 100644
index 0000000..9fc0378
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java
@@ -0,0 +1,132 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+import org.joda.time.format.ISODateTimeFormat;
+
+import java.util.ArrayList;
+import java.util.IllformedLocaleException;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.function.Function;
+
+public final class DateProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "date";
+    static final String DEFAULT_TARGET_FIELD = "@timestamp";
+
+    private final DateTimeZone timezone;
+    private final Locale locale;
+    private final String matchField;
+    private final String targetField;
+    private final List<String> matchFormats;
+    private final List<Function<String, DateTime>> dateParsers;
+
+    DateProcessor(String tag, DateTimeZone timezone, Locale locale, String matchField, List<String> matchFormats, String targetField) {
+        super(tag);
+        this.timezone = timezone;
+        this.locale = locale;
+        this.matchField = matchField;
+        this.targetField = targetField;
+        this.matchFormats = matchFormats;
+        this.dateParsers = new ArrayList<>();
+        for (String matchFormat : matchFormats) {
+            DateFormat dateFormat = DateFormat.fromString(matchFormat);
+            dateParsers.add(dateFormat.getFunction(matchFormat, timezone, locale));
+        }
+    }
+
+    @Override
+    public void execute(IngestDocument ingestDocument) {
+        String value = ingestDocument.getFieldValue(matchField, String.class);
+
+        DateTime dateTime = null;
+        Exception lastException = null;
+        for (Function<String, DateTime> dateParser : dateParsers) {
+            try {
+                dateTime = dateParser.apply(value);
+            } catch (Exception e) {
+                //try the next parser and keep track of the exceptions
+                lastException = ExceptionsHelper.useOrSuppress(lastException, e);
+            }
+        }
+
+        if (dateTime == null) {
+            throw new IllegalArgumentException("unable to parse date [" + value + "]", lastException);
+        }
+
+        ingestDocument.setFieldValue(targetField, ISODateTimeFormat.dateTime().print(dateTime));
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    DateTimeZone getTimezone() {
+        return timezone;
+    }
+
+    Locale getLocale() {
+        return locale;
+    }
+
+    String getMatchField() {
+        return matchField;
+    }
+
+    String getTargetField() {
+        return targetField;
+    }
+
+    List<String> getMatchFormats() {
+        return matchFormats;
+    }
+
+    public static class Factory extends AbstractProcessorFactory<DateProcessor> {
+
+        @SuppressWarnings("unchecked")
+        public DateProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String matchField = ConfigurationUtils.readStringProperty(config, "match_field");
+            String targetField = ConfigurationUtils.readStringProperty(config, "target_field", DEFAULT_TARGET_FIELD);
+            String timezoneString = ConfigurationUtils.readOptionalStringProperty(config, "timezone");
+            DateTimeZone timezone = timezoneString == null ? DateTimeZone.UTC : DateTimeZone.forID(timezoneString);
+            String localeString = ConfigurationUtils.readOptionalStringProperty(config, "locale");
+            Locale locale = Locale.ENGLISH;
+            if (localeString != null) {
+                try {
+                    locale = (new Locale.Builder()).setLanguageTag(localeString).build();
+                } catch (IllformedLocaleException e) {
+                    throw new IllegalArgumentException("Invalid language tag specified: " + localeString);
+                }
+            }
+            List<String> matchFormats = ConfigurationUtils.readList(config, "match_formats");
+            return new DateProcessor(processorTag, timezone, locale, matchField, matchFormats, targetField);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
new file mode 100644
index 0000000..295a988
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/DeDotProcessor.java
@@ -0,0 +1,107 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Processor;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Processor that replaces dots in document field names with a
+ * specified separator.
+ */
+public class DeDotProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "dedot";
+    static final String DEFAULT_SEPARATOR = "_";
+
+    private final String separator;
+
+    DeDotProcessor(String tag, String separator) {
+        super(tag);
+        this.separator = separator;
+    }
+
+    public String getSeparator() {
+        return separator;
+    }
+
+    @Override
+    public void execute(IngestDocument document) {
+        deDot(document.getSourceAndMetadata());
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    /**
+     * Recursively iterates through Maps and Lists in search of map entries with
+     * keys containing dots. The dots in these fields are replaced with {@link #separator}.
+     *
+     * @param obj The current object in context to be checked for dots in its fields.
+     */
+    private void deDot(Object obj) {
+        if (obj instanceof Map) {
+            @SuppressWarnings("unchecked")
+            Map<String, Object> doc = (Map) obj;
+            Iterator<Map.Entry<String, Object>> it = doc.entrySet().iterator();
+            Map<String, Object> deDottedFields = new HashMap<>();
+            while (it.hasNext()) {
+                Map.Entry<String, Object> entry = it.next();
+                deDot(entry.getValue());
+                String fieldName = entry.getKey();
+                if (fieldName.contains(".")) {
+                    String deDottedFieldName = fieldName.replaceAll("\\.", separator);
+                    deDottedFields.put(deDottedFieldName, entry.getValue());
+                    it.remove();
+                }
+            }
+            doc.putAll(deDottedFields);
+        } else if (obj instanceof List) {
+            @SuppressWarnings("unchecked")
+            List<Object> list = (List) obj;
+            for (Object value : list) {
+                deDot(value);
+            }
+        }
+    }
+
+    public static class Factory extends AbstractProcessorFactory<DeDotProcessor> {
+
+        @Override
+        public DeDotProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String separator = ConfigurationUtils.readOptionalStringProperty(config, "separator");
+            if (separator == null) {
+                separator = DEFAULT_SEPARATOR;
+            }
+            return new DeDotProcessor(processorTag, separator);
+        }
+    }
+}
+
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
new file mode 100644
index 0000000..65b4b60
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessor.java
@@ -0,0 +1,75 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.core.TemplateService;
+
+import java.util.Map;
+
+/**
+ * Processor that raises a runtime exception with a provided
+ * error message.
+ */
+public class FailProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "fail";
+
+    private final TemplateService.Template message;
+
+    FailProcessor(String tag, TemplateService.Template message) {
+        super(tag);
+        this.message = message;
+    }
+
+    public TemplateService.Template getMessage() {
+        return message;
+    }
+
+    @Override
+    public void execute(IngestDocument document) {
+        throw new FailProcessorException(document.renderTemplate(message));
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractProcessorFactory<FailProcessor> {
+
+        private final TemplateService templateService;
+
+        public Factory(TemplateService templateService) {
+            this.templateService = templateService;
+        }
+
+        @Override
+        public FailProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String message = ConfigurationUtils.readStringProperty(config, "message");
+            return new FailProcessor(processorTag, templateService.compile(message));
+        }
+    }
+}
+
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java
new file mode 100644
index 0000000..bfdfe11
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/FailProcessorException.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+/**
+ * Exception class thrown by {@link FailProcessor}.
+ *
+ * This exception is caught in the {@link org.elasticsearch.ingest.core.CompoundProcessor} and
+ * then changes the state of {@link org.elasticsearch.ingest.core.IngestDocument}. This
+ * exception should get serialized.
+ */
+public class FailProcessorException extends RuntimeException {
+
+    public FailProcessorException(String message) {
+        super(message);
+    }
+}
+
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
new file mode 100644
index 0000000..3dc4b3f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/GsubProcessor.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.Processor;
+
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * Processor that allows to search for patterns in field content and replace them with corresponding string replacement.
+ * Support fields of string type only, throws exception if a field is of a different type.
+ */
+public class GsubProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "gsub";
+
+    private final String field;
+    private final Pattern pattern;
+    private final String replacement;
+
+    GsubProcessor(String tag, String field, Pattern pattern, String replacement) {
+        super(tag);
+        this.field = field;
+        this.pattern = pattern;
+        this.replacement = replacement;
+    }
+
+    String getField() {
+        return field;
+    }
+
+    Pattern getPattern() {
+        return pattern;
+    }
+
+    String getReplacement() {
+        return replacement;
+    }
+
+
+    @Override
+    public void execute(IngestDocument document) {
+        String oldVal = document.getFieldValue(field, String.class);
+        if (oldVal == null) {
+            throw new IllegalArgumentException("field [" + field + "] is null, cannot match pattern.");
+        }
+        Matcher matcher = pattern.matcher(oldVal);
+        String newVal = matcher.replaceAll(replacement);
+        document.setFieldValue(field, newVal);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractProcessorFactory<GsubProcessor> {
+        @Override
+        public GsubProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            String pattern = ConfigurationUtils.readStringProperty(config, "pattern");
+            String replacement = ConfigurationUtils.readStringProperty(config, "replacement");
+            Pattern searchPattern = Pattern.compile(pattern);
+            return new GsubProcessor(processorTag, field, searchPattern, replacement);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
new file mode 100644
index 0000000..3516929
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/JoinProcessor.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.Processor;
+
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+/**
+ * Processor that joins the different items of an array into a single string value using a separator between each item.
+ * Throws exception is the specified field is not an array.
+ */
+public class JoinProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "join";
+
+    private final String field;
+    private final String separator;
+
+    JoinProcessor(String tag, String field, String separator) {
+        super(tag);
+        this.field = field;
+        this.separator = separator;
+    }
+
+    String getField() {
+        return field;
+    }
+
+    String getSeparator() {
+        return separator;
+    }
+
+    @Override
+    public void execute(IngestDocument document) {
+        List<?> list = document.getFieldValue(field, List.class);
+        if (list == null) {
+            throw new IllegalArgumentException("field [" + field + "] is null, cannot join.");
+        }
+        String joined = list.stream()
+                .map(Object::toString)
+                .collect(Collectors.joining(separator));
+        document.setFieldValue(field, joined);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractProcessorFactory<JoinProcessor> {
+        @Override
+        public JoinProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            String separator = ConfigurationUtils.readStringProperty(config, "separator");
+            return new JoinProcessor(processorTag, field, separator);
+        }
+    }
+}
+
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java
new file mode 100644
index 0000000..617efd9
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/LowercaseProcessor.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import java.util.Locale;
+
+/**
+ * Processor that converts the content of string fields to lowercase.
+ * Throws exception is the field is not of type string.
+ */
+
+public class LowercaseProcessor extends AbstractStringProcessor {
+
+    public static final String TYPE = "lowercase";
+
+    LowercaseProcessor(String processorTag, String field) {
+        super(processorTag, field);
+    }
+
+    @Override
+    protected String process(String value) {
+        return value.toLowerCase(Locale.ROOT);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractStringProcessor.Factory<LowercaseProcessor> {
+        @Override
+        protected LowercaseProcessor newProcessor(String tag, String field) {
+            return new LowercaseProcessor(tag, field);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
new file mode 100644
index 0000000..e994954
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/RemoveProcessor.java
@@ -0,0 +1,74 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.Processor;
+
+import java.util.Map;
+
+/**
+ * Processor that removes existing fields. Nothing happens if the field is not present.
+ */
+public class RemoveProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "remove";
+
+    private final TemplateService.Template field;
+
+    RemoveProcessor(String tag, TemplateService.Template field) {
+        super(tag);
+        this.field = field;
+    }
+
+    public TemplateService.Template getField() {
+        return field;
+    }
+
+    @Override
+    public void execute(IngestDocument document) {
+        document.removeField(field);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractProcessorFactory<RemoveProcessor> {
+
+        private final TemplateService templateService;
+
+        public Factory(TemplateService templateService) {
+            this.templateService = templateService;
+        }
+
+        @Override
+        public RemoveProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            return new RemoveProcessor(processorTag, templateService.compile(field));
+        }
+    }
+}
+
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
new file mode 100644
index 0000000..7726a72
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/RenameProcessor.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.Processor;
+
+import java.util.Map;
+
+/**
+ * Processor that allows to rename existing fields. Will throw exception if the field is not present.
+ */
+public class RenameProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "rename";
+
+    private final String oldFieldName;
+    private final String newFieldName;
+
+    RenameProcessor(String tag, String oldFieldName, String newFieldName) {
+        super(tag);
+        this.oldFieldName = oldFieldName;
+        this.newFieldName = newFieldName;
+    }
+
+    String getOldFieldName() {
+        return oldFieldName;
+    }
+
+    String getNewFieldName() {
+        return newFieldName;
+    }
+
+    @Override
+    public void execute(IngestDocument document) {
+        if (document.hasField(oldFieldName) == false) {
+            throw new IllegalArgumentException("field [" + oldFieldName + "] doesn't exist");
+        }
+        if (document.hasField(newFieldName)) {
+            throw new IllegalArgumentException("field [" + newFieldName + "] already exists");
+        }
+
+        Object oldValue = document.getFieldValue(oldFieldName, Object.class);
+        document.setFieldValue(newFieldName, oldValue);
+        try {
+            document.removeField(oldFieldName);
+        } catch (Exception e) {
+            //remove the new field if the removal of the old one failed
+            document.removeField(newFieldName);
+            throw e;
+        }
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractProcessorFactory<RenameProcessor> {
+        @Override
+        public RenameProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            String newField = ConfigurationUtils.readStringProperty(config, "to");
+            return new RenameProcessor(processorTag, field, newField);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java
new file mode 100644
index 0000000..e046a5f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/SetProcessor.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.ingest.core.ValueSource;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+
+import java.util.Map;
+
+/**
+ * Processor that adds new fields with their corresponding values. If the field is already present, its value
+ * will be replaced with the provided one.
+ */
+public class SetProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "set";
+
+    private final TemplateService.Template field;
+    private final ValueSource value;
+
+    SetProcessor(String tag, TemplateService.Template field, ValueSource value) {
+        super(tag);
+        this.field = field;
+        this.value = value;
+    }
+
+    public TemplateService.Template getField() {
+        return field;
+    }
+
+    public ValueSource getValue() {
+        return value;
+    }
+
+    @Override
+    public void execute(IngestDocument document) {
+        document.setFieldValue(field, value);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static final class Factory extends AbstractProcessorFactory<SetProcessor> {
+
+        private final TemplateService templateService;
+
+        public Factory(TemplateService templateService) {
+            this.templateService = templateService;
+        }
+
+        @Override
+        public SetProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            Object value = ConfigurationUtils.readObject(config, "value");
+            return new SetProcessor(processorTag, templateService.compile(field), ValueSource.wrap(value, templateService));
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java
new file mode 100644
index 0000000..ad0bffb
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/SplitProcessor.java
@@ -0,0 +1,82 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.IngestDocument;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Processor that splits fields content into different items based on the occurrence of a specified separator.
+ * New field value will be an array containing all of the different extracted items.
+ * Throws exception if the field is null or a type other than string.
+ */
+public class SplitProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "split";
+
+    private final String field;
+    private final String separator;
+
+    SplitProcessor(String tag, String field, String separator) {
+        super(tag);
+        this.field = field;
+        this.separator = separator;
+    }
+
+    String getField() {
+        return field;
+    }
+
+    String getSeparator() {
+        return separator;
+    }
+
+    @Override
+    public void execute(IngestDocument document) {
+        String oldVal = document.getFieldValue(field, String.class);
+        if (oldVal == null) {
+            throw new IllegalArgumentException("field [" + field + "] is null, cannot split.");
+        }
+        String[] strings = oldVal.split(separator);
+        List<String> splitList = new ArrayList<>(strings.length);
+        Collections.addAll(splitList, strings);
+        document.setFieldValue(field, splitList);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractProcessorFactory<SplitProcessor> {
+        @Override
+        public SplitProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String field = ConfigurationUtils.readStringProperty(config, "field");
+            return new SplitProcessor(processorTag, field, ConfigurationUtils.readStringProperty(config, "separator"));
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java
new file mode 100644
index 0000000..c66cc84
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/TrimProcessor.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+/**
+ * Processor that trims the content of string fields.
+ * Throws exception is the field is not of type string.
+ */
+public class TrimProcessor extends AbstractStringProcessor {
+
+    public static final String TYPE = "trim";
+
+    TrimProcessor(String processorTag, String field) {
+        super(processorTag, field);
+    }
+
+    @Override
+    protected String process(String value) {
+        return value.trim();
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractStringProcessor.Factory<TrimProcessor> {
+        @Override
+        protected TrimProcessor newProcessor(String tag, String field) {
+            return new TrimProcessor(tag, field);
+        }
+    }
+}
+
diff --git a/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java b/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java
new file mode 100644
index 0000000..e6a1f77
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/ingest/processor/UppercaseProcessor.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import java.util.Locale;
+
+/**
+ * Processor that converts the content of string fields to uppercase.
+ * Throws exception is the field is not of type string.
+ */
+public class UppercaseProcessor extends AbstractStringProcessor {
+
+    public static final String TYPE = "uppercase";
+
+    UppercaseProcessor(String processorTag, String field) {
+        super(processorTag, field);
+    }
+
+    @Override
+    protected String process(String value) {
+        return value.toUpperCase(Locale.ROOT);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    public static class Factory extends AbstractStringProcessor.Factory<UppercaseProcessor> {
+        @Override
+        protected UppercaseProcessor newProcessor(String tag, String field) {
+            return new UppercaseProcessor(tag, field);
+        }
+    }
+}
+
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index 724efe9..f51c7e2 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -30,6 +30,7 @@ import org.elasticsearch.cluster.ClusterModule;
 import org.elasticsearch.cluster.ClusterNameModule;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
+import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.RoutingService;
 import org.elasticsearch.common.StopWatch;
 import org.elasticsearch.common.component.Lifecycle;
@@ -76,6 +77,7 @@ import org.elasticsearch.indices.ttl.IndicesTTLService;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
+import org.elasticsearch.node.service.NodeService;
 import org.elasticsearch.percolator.PercolatorModule;
 import org.elasticsearch.percolator.PercolatorService;
 import org.elasticsearch.plugins.Plugin;
@@ -119,6 +121,7 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
  */
 public class Node implements Releasable {
 
+    public static final Setting<Boolean> NODE_INGEST_SETTING = Setting.boolSetting("node.ingest", true, false, Setting.Scope.CLUSTER);
     private static final String CLIENT_TYPE = "node";
     public static final Setting<Boolean> WRITE_PORTS_FIELD_SETTING = Setting.boolSetting("node.portsfile", false, false, Setting.Scope.CLUSTER);
     private final Lifecycle lifecycle = new Lifecycle();
@@ -190,7 +193,7 @@ public class Node implements Releasable {
             modules.add(new ClusterModule(this.settings));
             modules.add(new IndicesModule());
             modules.add(new SearchModule(settings, namedWriteableRegistry));
-            modules.add(new ActionModule(false));
+            modules.add(new ActionModule(DiscoveryNode.ingestNode(settings), false));
             modules.add(new GatewayModule(settings));
             modules.add(new NodeClientModule());
             modules.add(new PercolatorModule());
@@ -233,6 +236,13 @@ public class Node implements Releasable {
     }
 
     /**
+     * Returns the environment of the node
+     */
+    public Environment getEnvironment() {
+        return environment;
+    }
+
+    /**
      * Start the node. If the node is already started, this method is no-op.
      */
     public Node start() {
@@ -347,6 +357,12 @@ public class Node implements Releasable {
         StopWatch stopWatch = new StopWatch("node_close");
         stopWatch.start("tribe");
         injector.getInstance(TribeService.class).close();
+        stopWatch.stop().start("node_service");
+        try {
+            injector.getInstance(NodeService.class).close();
+        } catch (IOException e) {
+            logger.warn("NodeService close failed", e);
+        }
         stopWatch.stop().start("http");
         if (settings.getAsBoolean("http.enabled", true)) {
             injector.getInstance(HttpServer.class).close();
diff --git a/core/src/main/java/org/elasticsearch/node/NodeModule.java b/core/src/main/java/org/elasticsearch/node/NodeModule.java
index aa52d38..442dc72 100644
--- a/core/src/main/java/org/elasticsearch/node/NodeModule.java
+++ b/core/src/main/java/org/elasticsearch/node/NodeModule.java
@@ -22,9 +22,28 @@ package org.elasticsearch.node;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.util.BigArrays;
+import org.elasticsearch.ingest.ProcessorsRegistry;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.ingest.processor.AppendProcessor;
+import org.elasticsearch.ingest.processor.ConvertProcessor;
+import org.elasticsearch.ingest.processor.DateProcessor;
+import org.elasticsearch.ingest.processor.DeDotProcessor;
+import org.elasticsearch.ingest.processor.FailProcessor;
+import org.elasticsearch.ingest.processor.GsubProcessor;
+import org.elasticsearch.ingest.processor.JoinProcessor;
+import org.elasticsearch.ingest.processor.LowercaseProcessor;
+import org.elasticsearch.ingest.processor.RemoveProcessor;
+import org.elasticsearch.ingest.processor.RenameProcessor;
+import org.elasticsearch.ingest.processor.SetProcessor;
+import org.elasticsearch.ingest.processor.SplitProcessor;
+import org.elasticsearch.ingest.processor.TrimProcessor;
+import org.elasticsearch.ingest.processor.UppercaseProcessor;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.node.service.NodeService;
 
+import java.util.function.Function;
+
 /**
  *
  */
@@ -32,6 +51,7 @@ public class NodeModule extends AbstractModule {
 
     private final Node node;
     private final MonitorService monitorService;
+    private final ProcessorsRegistry processorsRegistry;
 
     // pkg private so tests can mock
     Class<? extends PageCacheRecycler> pageCacheRecyclerImpl = PageCacheRecycler.class;
@@ -40,6 +60,22 @@ public class NodeModule extends AbstractModule {
     public NodeModule(Node node, MonitorService monitorService) {
         this.node = node;
         this.monitorService = monitorService;
+        this.processorsRegistry = new ProcessorsRegistry();
+
+        registerProcessor(DateProcessor.TYPE, (templateService) -> new DateProcessor.Factory());
+        registerProcessor(SetProcessor.TYPE, SetProcessor.Factory::new);
+        registerProcessor(AppendProcessor.TYPE, AppendProcessor.Factory::new);
+        registerProcessor(RenameProcessor.TYPE, (templateService) -> new RenameProcessor.Factory());
+        registerProcessor(RemoveProcessor.TYPE, RemoveProcessor.Factory::new);
+        registerProcessor(SplitProcessor.TYPE, (templateService) -> new SplitProcessor.Factory());
+        registerProcessor(JoinProcessor.TYPE, (templateService) -> new JoinProcessor.Factory());
+        registerProcessor(UppercaseProcessor.TYPE, (templateService) -> new UppercaseProcessor.Factory());
+        registerProcessor(LowercaseProcessor.TYPE, (templateService) -> new LowercaseProcessor.Factory());
+        registerProcessor(TrimProcessor.TYPE, (templateService) -> new TrimProcessor.Factory());
+        registerProcessor(ConvertProcessor.TYPE, (templateService) -> new ConvertProcessor.Factory());
+        registerProcessor(GsubProcessor.TYPE, (templateService) -> new GsubProcessor.Factory());
+        registerProcessor(FailProcessor.TYPE, FailProcessor.Factory::new);
+        registerProcessor(DeDotProcessor.TYPE, (templateService) -> new DeDotProcessor.Factory());
     }
 
     @Override
@@ -58,5 +94,20 @@ public class NodeModule extends AbstractModule {
         bind(Node.class).toInstance(node);
         bind(MonitorService.class).toInstance(monitorService);
         bind(NodeService.class).asEagerSingleton();
+        bind(ProcessorsRegistry.class).toInstance(processorsRegistry);
+    }
+
+    /**
+     * Returns the node
+     */
+    public Node getNode() {
+        return node;
+    }
+
+    /**
+     * Adds a processor factory under a specific type name.
+     */
+    public void registerProcessor(String type, Function<TemplateService, Processor.Factory<?>> processorFactoryProvider) {
+        processorsRegistry.registerProcessor(type, processorFactoryProvider);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/node/service/NodeService.java b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
index b4fe59e..7c385b5 100644
--- a/core/src/main/java/org/elasticsearch/node/service/NodeService.java
+++ b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
@@ -24,20 +24,25 @@ import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.indices.stats.CommonStatsFlags;
+import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.Discovery;
+import org.elasticsearch.env.Environment;
 import org.elasticsearch.http.HttpServer;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.breaker.CircuitBreakerService;
+import org.elasticsearch.ingest.IngestService;
+import org.elasticsearch.ingest.ProcessorsRegistry;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.plugins.PluginsService;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
+import java.io.Closeable;
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
@@ -47,7 +52,7 @@ import static java.util.Collections.unmodifiableMap;
 
 /**
  */
-public class NodeService extends AbstractComponent {
+public class NodeService extends AbstractComponent implements Closeable {
 
     private final ThreadPool threadPool;
     private final MonitorService monitorService;
@@ -55,6 +60,7 @@ public class NodeService extends AbstractComponent {
     private final IndicesService indicesService;
     private final PluginsService pluginService;
     private final CircuitBreakerService circuitBreakerService;
+    private final IngestService ingestService;
     private ScriptService scriptService;
 
     @Nullable
@@ -67,10 +73,10 @@ public class NodeService extends AbstractComponent {
     private final Discovery discovery;
 
     @Inject
-    public NodeService(Settings settings, ThreadPool threadPool, MonitorService monitorService, Discovery discovery,
-                       TransportService transportService, IndicesService indicesService,
-                       PluginsService pluginService, CircuitBreakerService circuitBreakerService,
-                       Version version) {
+    public NodeService(Settings settings, Environment environment, ThreadPool threadPool, MonitorService monitorService,
+                       Discovery discovery, TransportService transportService, IndicesService indicesService,
+                       PluginsService pluginService, CircuitBreakerService circuitBreakerService, Version version,
+                       ProcessorsRegistry processorsRegistry, ClusterService clusterService) {
         super(settings);
         this.threadPool = threadPool;
         this.monitorService = monitorService;
@@ -81,12 +87,15 @@ public class NodeService extends AbstractComponent {
         this.version = version;
         this.pluginService = pluginService;
         this.circuitBreakerService = circuitBreakerService;
+        this.ingestService = new IngestService(settings, threadPool, processorsRegistry);
+        clusterService.add(ingestService.getPipelineStore());
     }
 
     // can not use constructor injection or there will be a circular dependency
     @Inject(optional = true)
     public void setScriptService(ScriptService scriptService) {
         this.scriptService = scriptService;
+        this.ingestService.setScriptService(scriptService);
     }
 
     public void setHttpServer(@Nullable HttpServer httpServer) {
@@ -176,4 +185,13 @@ public class NodeService extends AbstractComponent {
                 discoveryStats ? discovery.stats() : null
         );
     }
+
+    public IngestService getIngestService() {
+        return ingestService;
+    }
+
+    @Override
+    public void close() throws IOException {
+        indicesService.close();
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java b/core/src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java
index 10eeec7..9d091a4 100644
--- a/core/src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java
+++ b/core/src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java
@@ -39,6 +39,8 @@ import org.apache.lucene.util.CloseableThreadLocal;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.index.engine.Engine;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
@@ -76,8 +78,7 @@ class MultiDocumentPercolatorIndex implements PercolatorIndex {
             } else {
                 memoryIndex = new MemoryIndex(true);
             }
-            Analyzer analyzer = context.mapperService().documentMapper(parsedDocument.type()).mappers().indexAnalyzer();
-            memoryIndices[i] = indexDoc(d, analyzer, memoryIndex).createSearcher().getIndexReader();
+            memoryIndices[i] = indexDoc(d, memoryIndex, context, parsedDocument).createSearcher().getIndexReader();
         }
         try {
             MultiReader mReader = new MultiReader(memoryIndices, true);
@@ -101,8 +102,13 @@ class MultiDocumentPercolatorIndex implements PercolatorIndex {
         }
     }
 
-    MemoryIndex indexDoc(ParseContext.Document d, Analyzer analyzer, MemoryIndex memoryIndex) {
+    MemoryIndex indexDoc(ParseContext.Document d, MemoryIndex memoryIndex, PercolateContext context, ParsedDocument parsedDocument) {
         for (IndexableField field : d.getFields()) {
+            Analyzer analyzer = context.analysisService().defaultIndexAnalyzer();
+            DocumentMapper documentMapper = context.mapperService().documentMapper(parsedDocument.type());
+            if (documentMapper != null && documentMapper.mappers().getMapper(field.name()) != null) {
+                analyzer =  documentMapper.mappers().indexAnalyzer();
+            }
             if (field.fieldType().indexOptions() == IndexOptions.NONE && field.name().equals(UidFieldMapper.NAME)) {
                 continue;
             }
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
index 69eaa5c..7d4e18c 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.percolator;
 
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
-
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.LeafReaderContext;
@@ -27,6 +26,7 @@ import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Sort;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.percolate.PercolateShardRequest;
 import org.elasticsearch.action.search.SearchType;
@@ -48,6 +48,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.index.percolator.PercolatorQueriesRegistry;
 import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
@@ -56,7 +57,6 @@ import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -82,6 +82,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.concurrent.ConcurrentMap;
 
 /**
  */
@@ -120,15 +121,13 @@ public class PercolateContext extends SearchContext {
     private Sort sort;
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
     private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
-    private final FetchPhase fetchPhase;
 
     public PercolateContext(PercolateShardRequest request, SearchShardTarget searchShardTarget, IndexShard indexShard,
-            IndexService indexService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, ScriptService scriptService,
-            Query aliasFilter, ParseFieldMatcher parseFieldMatcher, FetchPhase fetchPhase) {
+                            IndexService indexService, PageCacheRecycler pageCacheRecycler,
+                            BigArrays bigArrays, ScriptService scriptService, Query aliasFilter, ParseFieldMatcher parseFieldMatcher) {
         super(parseFieldMatcher, request);
         this.indexShard = indexShard;
         this.indexService = indexService;
-        this.fetchPhase = fetchPhase;
         this.fieldDataService = indexService.fieldData();
         this.mapperService = indexService.mapperService();
         this.searchShardTarget = searchShardTarget;
@@ -160,7 +159,6 @@ public class PercolateContext extends SearchContext {
         this.startTime = 0;
         this.numberOfShards = 0;
         this.onlyCount = true;
-        this.fetchPhase = null;
     }
 
     public IndexSearcher docSearcher() {
@@ -657,11 +655,6 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return fetchPhase;
-    }
-
-    @Override
     public MappedFieldType smartNameFieldType(String name) {
         return mapperService().fullName(name);
     }
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java b/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
index 6733ebd..8edc521 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
@@ -49,14 +49,13 @@ public class PercolateDocumentParser {
     private final HighlightPhase highlightPhase;
     private final SortParseElement sortParseElement;
     private final AggregationPhase aggregationPhase;
-    private final MappingUpdatedAction mappingUpdatedAction;
 
     @Inject
-    public PercolateDocumentParser(HighlightPhase highlightPhase, SortParseElement sortParseElement, AggregationPhase aggregationPhase, MappingUpdatedAction mappingUpdatedAction) {
+    public PercolateDocumentParser(HighlightPhase highlightPhase, SortParseElement sortParseElement,
+                                   AggregationPhase aggregationPhase) {
         this.highlightPhase = highlightPhase;
         this.sortParseElement = sortParseElement;
         this.aggregationPhase = aggregationPhase;
-        this.mappingUpdatedAction = mappingUpdatedAction;
     }
 
     public ParsedDocument parse(PercolateShardRequest request, PercolateContext context, MapperService mapperService, QueryShardContext queryShardContext) {
@@ -98,9 +97,6 @@ public class PercolateDocumentParser {
                         if (docMapper.getMapping() != null) {
                             doc.addDynamicMappingsUpdate(docMapper.getMapping());
                         }
-                        if (doc.dynamicMappingsUpdate() != null) {
-                            mappingUpdatedAction.updateMappingOnMasterSynchronously(request.shardId().getIndex(), request.documentType(), doc.dynamicMappingsUpdate());
-                        }
                         // the document parsing exists the "doc" object, so we need to set the new current field.
                         currentFieldName = parser.currentName();
                     }
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
index b7d813f..6ac0ca6 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.percolator;
 
-
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.memory.ExtendedMemoryIndex;
@@ -53,6 +52,7 @@ import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.fieldvisitor.SingleFieldsVisitor;
+import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.percolator.PercolatorFieldMapper;
@@ -70,7 +70,6 @@ import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
 import org.elasticsearch.search.aggregations.pipeline.SiblingPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.highlight.HighlightField;
 import org.elasticsearch.search.highlight.HighlightPhase;
 import org.elasticsearch.search.internal.SearchContext;
@@ -99,25 +98,23 @@ public class PercolatorService extends AbstractComponent {
     private final HighlightPhase highlightPhase;
     private final AggregationPhase aggregationPhase;
     private final PageCacheRecycler pageCacheRecycler;
+    private final ParseFieldMatcher parseFieldMatcher;
     private final CloseableThreadLocal<MemoryIndex> cache;
     private final IndexNameExpressionResolver indexNameExpressionResolver;
     private final PercolateDocumentParser percolateDocumentParser;
 
     private final PercolatorIndex single;
     private final PercolatorIndex multi;
-    private final ParseFieldMatcher parseFieldMatcher;
-    private final FetchPhase fetchPhase;
 
     @Inject
     public PercolatorService(Settings settings, IndexNameExpressionResolver indexNameExpressionResolver, IndicesService indicesService,
                              PageCacheRecycler pageCacheRecycler, BigArrays bigArrays,
                              HighlightPhase highlightPhase, ClusterService clusterService,
                              AggregationPhase aggregationPhase, ScriptService scriptService,
-                             PercolateDocumentParser percolateDocumentParser, FetchPhase fetchPhase) {
+                             PercolateDocumentParser percolateDocumentParser) {
         super(settings);
         this.indexNameExpressionResolver = indexNameExpressionResolver;
         this.percolateDocumentParser = percolateDocumentParser;
-        this.fetchPhase = fetchPhase;
         this.parseFieldMatcher = new ParseFieldMatcher(settings);
         this.indicesService = indicesService;
         this.pageCacheRecycler = pageCacheRecycler;
@@ -144,7 +141,7 @@ public class PercolatorService extends AbstractComponent {
             long finalCount = 0;
             for (PercolateShardResponse shardResponse : shardResponses) {
                 finalCount += shardResponse.topDocs().totalHits;
-    }
+            }
 
             InternalAggregations reducedAggregations = reduceAggregations(shardResponses, headersContext);
             return new PercolatorService.ReduceResult(finalCount, reducedAggregations);
@@ -188,10 +185,10 @@ public class PercolatorService extends AbstractComponent {
         );
         Query aliasFilter = percolateIndexService.aliasFilter(indexShard.getQueryShardContext(), filteringAliases);
 
-        SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(),
-                request.shardId().id());
-        final PercolateContext context = new PercolateContext(request, searchShardTarget, indexShard, percolateIndexService,
-                pageCacheRecycler, bigArrays, scriptService, aliasFilter, parseFieldMatcher, fetchPhase);
+        SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(), request.shardId().id());
+        final PercolateContext context = new PercolateContext(
+                request, searchShardTarget, indexShard, percolateIndexService, pageCacheRecycler, bigArrays, scriptService, aliasFilter, parseFieldMatcher
+        );
         SearchContext.setCurrent(context);
         try {
             ParsedDocument parsedDocument = percolateDocumentParser.parse(request, context, percolateIndexService.mapperService(), percolateIndexService.getQueryShardContext());
@@ -205,7 +202,8 @@ public class PercolatorService extends AbstractComponent {
 
             // parse the source either into one MemoryIndex, if it is a single document or index multiple docs if nested
             PercolatorIndex percolatorIndex;
-            boolean isNested = indexShard.mapperService().documentMapper(request.documentType()).hasNestedObjects();
+            DocumentMapper documentMapper = indexShard.mapperService().documentMapper(request.documentType());
+            boolean isNested = documentMapper != null && documentMapper.hasNestedObjects();
             if (parsedDocument.docs().size() > 1) {
                 assert isNested;
                 percolatorIndex = multi;
@@ -218,15 +216,15 @@ public class PercolatorService extends AbstractComponent {
             if (context.aggregations() != null) {
                 AggregationContext aggregationContext = new AggregationContext(context);
                 context.aggregations().aggregationContext(aggregationContext);
-                context.aggregations().factories().init(aggregationContext);
-                Aggregator[] aggregators = context.aggregations().factories().createTopLevelAggregators();
+
+                Aggregator[] aggregators = context.aggregations().factories().createTopLevelAggregators(aggregationContext);
                 List<Aggregator> aggregatorCollectors = new ArrayList<>(aggregators.length);
                 for (int i = 0; i < aggregators.length; i++) {
                     if (!(aggregators[i] instanceof GlobalAggregator)) {
                         Aggregator aggregator = aggregators[i];
                         aggregatorCollectors.add(aggregator);
+                    }
                 }
-            }
                 context.aggregations().aggregators(aggregators);
                 aggregatorCollector = BucketCollector.wrap(aggregatorCollectors);
                 aggregatorCollector.preCollection();
@@ -248,14 +246,14 @@ public class PercolatorService extends AbstractComponent {
         }
         if (context.percolateQuery() != null || context.aliasFilter() != null) {
             BooleanQuery.Builder bq = new BooleanQuery.Builder();
-                        if (context.percolateQuery() != null) {
+            if (context.percolateQuery() != null) {
                 bq.add(context.percolateQuery(), MUST);
-                        }
+            }
             if (context.aliasFilter() != null) {
                 bq.add(context.aliasFilter(), FILTER);
-                        }
+            }
             builder.setPercolateQuery(bq.build());
-                    }
+        }
         PercolatorQuery percolatorQuery = builder.build();
 
         if (context.isOnlyCount() || context.size() == 0) {
@@ -264,20 +262,20 @@ public class PercolatorService extends AbstractComponent {
             if (aggregatorCollector != null) {
                 aggregatorCollector.postCollection();
                 aggregationPhase.execute(context);
-                        }
+            }
             return new PercolateShardResponse(new TopDocs(collector.getTotalHits(), Lucene.EMPTY_SCORE_DOCS, 0f), Collections.emptyMap(), Collections.emptyMap(), context);
         } else {
             int size = context.size();
             if (size > context.searcher().getIndexReader().maxDoc()) {
                 // prevent easy OOM if more than the total number of docs that exist is requested...
                 size = context.searcher().getIndexReader().maxDoc();
-        }
+            }
             TopScoreDocCollector collector = TopScoreDocCollector.create(size);
             context.searcher().search(percolatorQuery, MultiCollector.wrap(collector, aggregatorCollector));
             if (aggregatorCollector != null) {
                 aggregatorCollector.postCollection();
                 aggregationPhase.execute(context);
-    }
+            }
 
             TopDocs topDocs = collector.topDocs();
             Map<Integer, String> ids = new HashMap<>(topDocs.scoreDocs.length);
@@ -286,7 +284,7 @@ public class PercolatorService extends AbstractComponent {
                 if (context.trackScores() == false) {
                     // No sort or tracking scores was provided, so use special value to indicate to not show the scores:
                     scoreDoc.score = NO_SCORE;
-        }
+                }
 
                 int segmentIdx = ReaderUtil.subIndex(scoreDoc.doc, context.searcher().getIndexReader().leaves());
                 LeafReaderContext atomicReaderContext = context.searcher().getIndexReader().leaves().get(segmentIdx);
@@ -299,17 +297,17 @@ public class PercolatorService extends AbstractComponent {
                     Query query = queriesRegistry.getPercolateQueries().get(new BytesRef(id));
                     context.parsedQuery(new ParsedQuery(query));
                     context.hitContext().cache().clear();
-                                highlightPhase.hitExecute(context, context.hitContext());
+                    highlightPhase.hitExecute(context, context.hitContext());
                     hls.put(scoreDoc.doc, context.hitContext().hit().getHighlightFields());
-                            }
-                        }
-            return new PercolateShardResponse(topDocs, ids, hls, context);
                 }
             }
+            return new PercolateShardResponse(topDocs, ids, hls, context);
+        }
+    }
 
     public void close() {
         cache.close();
-        }
+    }
 
     private InternalAggregations reduceAggregations(List<PercolateShardResponse> shardResults, HasContextAndHeaders headersContext) {
         if (shardResults.get(0).aggregations() == null) {
@@ -330,12 +328,12 @@ public class PercolatorService extends AbstractComponent {
                 for (SiblingPipelineAggregator pipelineAggregator : pipelineAggregators) {
                     InternalAggregation newAgg = pipelineAggregator.doReduce(new InternalAggregations(newAggs), new InternalAggregation.ReduceContext(bigArrays, scriptService, headersContext));
                     newAggs.add(newAgg);
-            }
+                }
                 aggregations = new InternalAggregations(newAggs);
-        }
+            }
         }
         return aggregations;
-        }
+    }
 
     public final static class ReduceResult {
 
diff --git a/core/src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java b/core/src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java
index 1271872..1d5268e 100644
--- a/core/src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java
+++ b/core/src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java
@@ -28,6 +28,7 @@ import org.apache.lucene.index.memory.MemoryIndex;
 import org.apache.lucene.util.CloseableThreadLocal;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.index.engine.Engine;
+import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
@@ -49,11 +50,15 @@ class SingleDocumentPercolatorIndex implements PercolatorIndex {
     public void prepare(PercolateContext context, ParsedDocument parsedDocument) {
         MemoryIndex memoryIndex = cache.get();
         for (IndexableField field : parsedDocument.rootDoc().getFields()) {
+            Analyzer analyzer = context.analysisService().defaultIndexAnalyzer();
+            DocumentMapper documentMapper = context.mapperService().documentMapper(parsedDocument.type());
+            if (documentMapper != null && documentMapper.mappers().getMapper(field.name()) != null) {
+                analyzer =  documentMapper.mappers().indexAnalyzer();
+            }
             if (field.fieldType().indexOptions() == IndexOptions.NONE && field.name().equals(UidFieldMapper.NAME)) {
                 continue;
             }
             try {
-                Analyzer analyzer = context.mapperService().documentMapper(parsedDocument.type()).mappers().indexAnalyzer();
                 // TODO: instead of passing null here, we can have a CTL<Map<String,TokenStream>> and pass previous,
                 // like the indexer does
                 try (TokenStream tokenStream = field.tokenStream(analyzer, null)) {
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 8e6391e..29da911 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -101,6 +101,7 @@ public class PluginManager {
             "discovery-ec2",
             "discovery-gce",
             "discovery-multicast",
+            "ingest-geoip",
             "lang-javascript",
             "lang-plan-a",
             "lang-python",
diff --git a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
index 37ce03b..df20438 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
@@ -77,6 +77,7 @@ public class RestBulkAction extends BaseRestHandler {
         String defaultType = request.param("type");
         String defaultRouting = request.param("routing");
         String fieldsParam = request.param("fields");
+        String defaultPipeline = request.param("pipeline");
         String[] defaultFields = fieldsParam != null ? Strings.commaDelimitedListToStringArray(fieldsParam) : null;
 
         String consistencyLevel = request.param("consistency");
@@ -85,7 +86,7 @@ public class RestBulkAction extends BaseRestHandler {
         }
         bulkRequest.timeout(request.paramAsTime("timeout", BulkShardRequest.DEFAULT_TIMEOUT));
         bulkRequest.refresh(request.paramAsBoolean("refresh", bulkRequest.refresh()));
-        bulkRequest.add(request.content(), defaultIndex, defaultType, defaultRouting, defaultFields, null, allowExplicitIndex);
+        bulkRequest.add(request.content(), defaultIndex, defaultType, defaultRouting, defaultFields, defaultPipeline, null, allowExplicitIndex);
 
         client.bulk(bulkRequest, new RestBuilderListener<BulkResponse>(channel) {
             @Override
diff --git a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
index 13a9329..0fc1545 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
@@ -77,6 +77,7 @@ public class RestIndexAction extends BaseRestHandler {
         if (request.hasParam("ttl")) {
             indexRequest.ttl(request.param("ttl"));
         }
+        indexRequest.setPipeline(request.param("pipeline"));
         indexRequest.source(request.content());
         indexRequest.timeout(request.paramAsTime("timeout", IndexRequest.DEFAULT_TIMEOUT));
         indexRequest.refresh(request.paramAsBoolean("refresh", indexRequest.refresh()));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java
new file mode 100644
index 0000000..c4526d4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestDeletePipelineAction.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.rest.action.ingest;
+
+import org.elasticsearch.action.ingest.DeletePipelineRequest;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.rest.BaseRestHandler;
+import org.elasticsearch.rest.RestChannel;
+import org.elasticsearch.rest.RestController;
+import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
+
+public class RestDeletePipelineAction extends BaseRestHandler {
+
+    @Inject
+    public RestDeletePipelineAction(Settings settings, RestController controller, Client client) {
+        super(settings, controller, client);
+        controller.registerHandler(RestRequest.Method.DELETE, "/_ingest/pipeline/{id}", this);
+    }
+
+    @Override
+    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
+        DeletePipelineRequest request = new DeletePipelineRequest(restRequest.param("id"));
+        request.masterNodeTimeout(restRequest.paramAsTime("master_timeout", request.masterNodeTimeout()));
+        request.timeout(restRequest.paramAsTime("timeout", request.timeout()));
+        client.deletePipeline(request, new AcknowledgedRestListener<>(channel));
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java
new file mode 100644
index 0000000..b483a84
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestGetPipelineAction.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.rest.action.ingest;
+
+import org.elasticsearch.action.ingest.GetPipelineRequest;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.rest.BaseRestHandler;
+import org.elasticsearch.rest.RestChannel;
+import org.elasticsearch.rest.RestController;
+import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
+
+public class RestGetPipelineAction extends BaseRestHandler {
+
+    @Inject
+    public RestGetPipelineAction(Settings settings, RestController controller, Client client) {
+        super(settings, controller, client);
+        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/{id}", this);
+    }
+
+    @Override
+    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
+        GetPipelineRequest request = new GetPipelineRequest(Strings.splitStringByCommaToArray(restRequest.param("id")));
+        request.masterNodeTimeout(restRequest.paramAsTime("master_timeout", request.masterNodeTimeout()));
+        client.getPipeline(request, new RestStatusToXContentListener<>(channel));
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
new file mode 100644
index 0000000..5cdd9a8
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestPutPipelineAction.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.rest.action.ingest;
+
+import org.elasticsearch.action.ingest.PutPipelineRequest;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.rest.BaseRestHandler;
+import org.elasticsearch.rest.RestChannel;
+import org.elasticsearch.rest.RestController;
+import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
+import org.elasticsearch.rest.action.support.RestActions;
+
+public class RestPutPipelineAction extends BaseRestHandler {
+
+    @Inject
+    public RestPutPipelineAction(Settings settings, RestController controller, Client client) {
+        super(settings, controller, client);
+        controller.registerHandler(RestRequest.Method.PUT, "/_ingest/pipeline/{id}", this);
+    }
+
+    @Override
+    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
+        PutPipelineRequest request = new PutPipelineRequest(restRequest.param("id"), RestActions.getRestContent(restRequest));
+        request.masterNodeTimeout(restRequest.paramAsTime("master_timeout", request.masterNodeTimeout()));
+        request.timeout(restRequest.paramAsTime("timeout", request.timeout()));
+        client.putPipeline(request, new AcknowledgedRestListener<>(channel));
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
new file mode 100644
index 0000000..35cf437
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/rest/action/ingest/RestSimulatePipelineAction.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.rest.action.ingest;
+
+import org.elasticsearch.action.ingest.SimulatePipelineRequest;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.rest.BaseRestHandler;
+import org.elasticsearch.rest.RestChannel;
+import org.elasticsearch.rest.RestController;
+import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.action.support.RestActions;
+import org.elasticsearch.rest.action.support.RestToXContentListener;
+
+public class RestSimulatePipelineAction extends BaseRestHandler {
+
+    @Inject
+    public RestSimulatePipelineAction(Settings settings, RestController controller, Client client) {
+        super(settings, controller, client);
+        controller.registerHandler(RestRequest.Method.POST, "/_ingest/pipeline/{id}/_simulate", this);
+        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/{id}/_simulate", this);
+        controller.registerHandler(RestRequest.Method.POST, "/_ingest/pipeline/_simulate", this);
+        controller.registerHandler(RestRequest.Method.GET, "/_ingest/pipeline/_simulate", this);
+    }
+
+    @Override
+    protected void handleRequest(RestRequest restRequest, RestChannel channel, Client client) throws Exception {
+        SimulatePipelineRequest request = new SimulatePipelineRequest(RestActions.getRestContent(restRequest));
+        request.setId(restRequest.param("id"));
+        request.setVerbose(restRequest.paramAsBoolean("verbose", false));
+        client.simulatePipeline(request, new RestToXContentListener<>(channel));
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/script/Script.java b/core/src/main/java/org/elasticsearch/script/Script.java
index dc0e383..fb2226e 100644
--- a/core/src/main/java/org/elasticsearch/script/Script.java
+++ b/core/src/main/java/org/elasticsearch/script/Script.java
@@ -34,24 +34,12 @@ import org.elasticsearch.script.ScriptService.ScriptType;
 
 import java.io.IOException;
 import java.util.Map;
-import java.util.function.Supplier;
 
 /**
  * Script holds all the parameters necessary to compile or find in cache and then execute a script.
  */
 public class Script implements ToXContent, Streamable {
 
-    /**
-     * A {@link Supplier} implementation for use when reading a {@link Script}
-     * using {@link StreamInput#readOptionalStreamable(Supplier)}
-     */
-    public static final Supplier<Script> SUPPLIER = new Supplier<Script>() {
-
-        @Override
-        public Script get() {
-            return new Script();
-        }
-    };
     public static final ScriptType DEFAULT_TYPE = ScriptType.INLINE;
     private static final ScriptParser PARSER = new ScriptParser();
 
@@ -86,7 +74,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Constructor for Script.
-     *
+     * 
      * @param script
      *            The cache key of the script to be compiled/executed. For
      *            inline scripts this is the actual script source code. For
@@ -124,7 +112,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting the type.
-     *
+     * 
      * @return The type of script -- inline, indexed, or file.
      */
     public ScriptType getType() {
@@ -133,7 +121,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting language.
-     *
+     * 
      * @return The language of the script to be compiled/executed.
      */
     public String getLang() {
@@ -142,7 +130,7 @@ public class Script implements ToXContent, Streamable {
 
     /**
      * Method for getting the parameters.
-     *
+     * 
      * @return The map of parameters the script will be executed with.
      */
     public Map<String, Object> getParams() {
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptContext.java b/core/src/main/java/org/elasticsearch/script/ScriptContext.java
index 4b1b6de..3ab2bb5 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptContext.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptContext.java
@@ -37,7 +37,7 @@ public interface ScriptContext {
      */
     enum Standard implements ScriptContext {
 
-        AGGS("aggs"), SEARCH("search"), UPDATE("update");
+        AGGS("aggs"), SEARCH("search"), UPDATE("update"), INGEST("ingest");
 
         private final String key;
 
diff --git a/core/src/main/java/org/elasticsearch/search/SearchModule.java b/core/src/main/java/org/elasticsearch/search/SearchModule.java
index 9a4caeb..c33471f 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchModule.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchModule.java
@@ -104,7 +104,6 @@ import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionPar
 import org.elasticsearch.index.query.functionscore.weight.WeightBuilder;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.action.SearchServiceTransportAction;
-import org.elasticsearch.search.aggregations.AggregationBinaryParseElement;
 import org.elasticsearch.search.aggregations.AggregationParseElement;
 import org.elasticsearch.search.aggregations.AggregationPhase;
 import org.elasticsearch.search.aggregations.Aggregator;
@@ -136,7 +135,6 @@ import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanc
 import org.elasticsearch.search.aggregations.bucket.range.geodistance.InternalGeoDistance;
 import org.elasticsearch.search.aggregations.bucket.range.ipv4.InternalIPv4Range;
 import org.elasticsearch.search.aggregations.bucket.range.ipv4.IpRangeParser;
-import org.elasticsearch.search.aggregations.bucket.sampler.DiversifiedSamplerParser;
 import org.elasticsearch.search.aggregations.bucket.sampler.InternalSampler;
 import org.elasticsearch.search.aggregations.bucket.sampler.SamplerParser;
 import org.elasticsearch.search.aggregations.bucket.sampler.UnmappedSampler;
@@ -217,24 +215,18 @@ import org.elasticsearch.search.controller.SearchPhaseController;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSubPhase;
-import org.elasticsearch.search.fetch.FieldsParseElement;
 import org.elasticsearch.search.fetch.explain.ExplainFetchSubPhase;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsFetchSubPhase;
 import org.elasticsearch.search.fetch.matchedqueries.MatchedQueriesFetchSubPhase;
 import org.elasticsearch.search.fetch.parent.ParentFieldSubFetchPhase;
 import org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
-import org.elasticsearch.search.fetch.source.FetchSourceParseElement;
 import org.elasticsearch.search.fetch.source.FetchSourceSubPhase;
 import org.elasticsearch.search.fetch.version.VersionFetchSubPhase;
 import org.elasticsearch.search.highlight.HighlightPhase;
 import org.elasticsearch.search.highlight.Highlighter;
-import org.elasticsearch.search.highlight.HighlighterParseElement;
 import org.elasticsearch.search.highlight.Highlighters;
 import org.elasticsearch.search.query.QueryPhase;
-import org.elasticsearch.search.sort.SortParseElement;
 import org.elasticsearch.search.suggest.Suggester;
 import org.elasticsearch.search.suggest.Suggesters;
 
@@ -243,8 +235,8 @@ import org.elasticsearch.search.suggest.Suggesters;
  */
 public class SearchModule extends AbstractModule {
 
-    private final Set<Aggregator.Parser> aggParsers = new HashSet<>();
-    private final Set<PipelineAggregator.Parser> pipelineAggParsers = new HashSet<>();
+    private final Set<Class<? extends Aggregator.Parser>> aggParsers = new HashSet<>();
+    private final Set<Class<? extends PipelineAggregator.Parser>> pipelineAggParsers = new HashSet<>();
     private final Highlighters highlighters = new Highlighters();
     private final Suggesters suggesters = new Suggesters();
     /**
@@ -259,8 +251,8 @@ public class SearchModule extends AbstractModule {
      */
     private final List<Supplier<QueryParser<?>>> queryParsers = new ArrayList<>();
     private final Set<Class<? extends FetchSubPhase>> fetchSubPhases = new HashSet<>();
-    private final Set<SignificanceHeuristicParser> heuristicParsers = new HashSet<>();
-    private final Set<MovAvgModel.AbstractModelParser> modelParsers = new HashSet<>();
+    private final Set<Class<? extends SignificanceHeuristicParser>> heuristicParsers = new HashSet<>();
+    private final Set<Class<? extends MovAvgModel.AbstractModelParser>> modelParsers = new HashSet<>();
 
     private final Settings settings;
     private final NamedWriteableRegistry namedWriteableRegistry;
@@ -305,11 +297,11 @@ public class SearchModule extends AbstractModule {
         fetchSubPhases.add(subPhase);
     }
 
-    public void registerHeuristicParser(SignificanceHeuristicParser parser) {
+    public void registerHeuristicParser(Class<? extends SignificanceHeuristicParser> parser) {
         heuristicParsers.add(parser);
     }
 
-    public void registerModelParser(MovAvgModel.AbstractModelParser parser) {
+    public void registerModelParser(Class<? extends MovAvgModel.AbstractModelParser> parser) {
         modelParsers.add(parser);
     }
 
@@ -318,22 +310,21 @@ public class SearchModule extends AbstractModule {
      *
      * @param parser The parser for the custom aggregator.
      */
-    public void registerAggregatorParser(Aggregator.Parser parser) {
+    public void registerAggregatorParser(Class<? extends Aggregator.Parser> parser) {
         aggParsers.add(parser);
     }
 
-    public void registerPipelineParser(PipelineAggregator.Parser parser) {
+    public void registerPipelineParser(Class<? extends PipelineAggregator.Parser> parser) {
         pipelineAggParsers.add(parser);
     }
 
     @Override
     protected void configure() {
-        IndicesQueriesRegistry indicesQueriesRegistry = buildQueryParserRegistry();
-        bind(IndicesQueriesRegistry.class).toInstance(indicesQueriesRegistry);
         configureSearch();
-        configureAggs(indicesQueriesRegistry);
+        configureAggs();
         configureHighlighters();
         configureSuggesters();
+        bind(IndicesQueriesRegistry.class).toInstance(buildQueryParserRegistry());
         configureFetchSubPhase();
         configureShapes();
     }
@@ -377,68 +368,75 @@ public class SearchModule extends AbstractModule {
        highlighters.bind(binder());
     }
 
-    protected void configureAggs(IndicesQueriesRegistry indicesQueriesRegistry) {
-
-        MovAvgModelParserMapper movAvgModelParserMapper = new MovAvgModelParserMapper(modelParsers);
-
-        SignificanceHeuristicParserMapper significanceHeuristicParserMapper = new SignificanceHeuristicParserMapper(heuristicParsers);
-
-        registerAggregatorParser(new AvgParser());
-        registerAggregatorParser(new SumParser());
-        registerAggregatorParser(new MinParser());
-        registerAggregatorParser(new MaxParser());
-        registerAggregatorParser(new StatsParser());
-        registerAggregatorParser(new ExtendedStatsParser());
-        registerAggregatorParser(new ValueCountParser());
-        registerAggregatorParser(new PercentilesParser());
-        registerAggregatorParser(new PercentileRanksParser());
-        registerAggregatorParser(new CardinalityParser());
-        registerAggregatorParser(new GlobalParser());
-        registerAggregatorParser(new MissingParser());
-        registerAggregatorParser(new FilterParser());
-        registerAggregatorParser(new FiltersParser(indicesQueriesRegistry));
-        registerAggregatorParser(new SamplerParser());
-        registerAggregatorParser(new DiversifiedSamplerParser());
-        registerAggregatorParser(new TermsParser());
-        registerAggregatorParser(new SignificantTermsParser(significanceHeuristicParserMapper, indicesQueriesRegistry));
-        registerAggregatorParser(new RangeParser());
-        registerAggregatorParser(new DateRangeParser());
-        registerAggregatorParser(new IpRangeParser());
-        registerAggregatorParser(new HistogramParser());
-        registerAggregatorParser(new DateHistogramParser());
-        registerAggregatorParser(new GeoDistanceParser());
-        registerAggregatorParser(new GeoHashGridParser());
-        registerAggregatorParser(new NestedParser());
-        registerAggregatorParser(new ReverseNestedParser());
-        registerAggregatorParser(new TopHitsParser(new SortParseElement(), new FetchSourceParseElement(), new HighlighterParseElement(),
-                new FieldDataFieldsParseElement(), new ScriptFieldsParseElement(), new FieldsParseElement()));
-        registerAggregatorParser(new GeoBoundsParser());
-        registerAggregatorParser(new GeoCentroidParser());
-        registerAggregatorParser(new ScriptedMetricParser());
-        registerAggregatorParser(new ChildrenParser());
-
-        registerPipelineParser(new DerivativeParser());
-        registerPipelineParser(new MaxBucketParser());
-        registerPipelineParser(new MinBucketParser());
-        registerPipelineParser(new AvgBucketParser());
-        registerPipelineParser(new SumBucketParser());
-        registerPipelineParser(new StatsBucketParser());
-        registerPipelineParser(new ExtendedStatsBucketParser());
-        registerPipelineParser(new PercentilesBucketParser());
-        registerPipelineParser(new MovAvgParser(movAvgModelParserMapper));
-        registerPipelineParser(new CumulativeSumParser());
-        registerPipelineParser(new BucketScriptParser());
-        registerPipelineParser(new BucketSelectorParser());
-        registerPipelineParser(new SerialDiffParser());
-
-        AggregatorParsers aggregatorParsers = new AggregatorParsers(aggParsers, pipelineAggParsers, namedWriteableRegistry);
-        AggregationParseElement aggParseElement = new AggregationParseElement(aggregatorParsers, indicesQueriesRegistry);
-        AggregationBinaryParseElement aggBinaryParseElement = new AggregationBinaryParseElement(aggregatorParsers, indicesQueriesRegistry);
-        AggregationPhase aggPhase = new AggregationPhase(aggParseElement, aggBinaryParseElement);
-        bind(AggregatorParsers.class).toInstance(aggregatorParsers);
-        ;
-        bind(AggregationParseElement.class).toInstance(aggParseElement);
-        bind(AggregationPhase.class).toInstance(aggPhase);
+    protected void configureAggs() {
+        Multibinder<Aggregator.Parser> multibinderAggParser = Multibinder.newSetBinder(binder(), Aggregator.Parser.class);
+        multibinderAggParser.addBinding().to(AvgParser.class);
+        multibinderAggParser.addBinding().to(SumParser.class);
+        multibinderAggParser.addBinding().to(MinParser.class);
+        multibinderAggParser.addBinding().to(MaxParser.class);
+        multibinderAggParser.addBinding().to(StatsParser.class);
+        multibinderAggParser.addBinding().to(ExtendedStatsParser.class);
+        multibinderAggParser.addBinding().to(ValueCountParser.class);
+        multibinderAggParser.addBinding().to(PercentilesParser.class);
+        multibinderAggParser.addBinding().to(PercentileRanksParser.class);
+        multibinderAggParser.addBinding().to(CardinalityParser.class);
+        multibinderAggParser.addBinding().to(GlobalParser.class);
+        multibinderAggParser.addBinding().to(MissingParser.class);
+        multibinderAggParser.addBinding().to(FilterParser.class);
+        multibinderAggParser.addBinding().to(FiltersParser.class);
+        multibinderAggParser.addBinding().to(SamplerParser.class);
+        multibinderAggParser.addBinding().to(TermsParser.class);
+        multibinderAggParser.addBinding().to(SignificantTermsParser.class);
+        multibinderAggParser.addBinding().to(RangeParser.class);
+        multibinderAggParser.addBinding().to(DateRangeParser.class);
+        multibinderAggParser.addBinding().to(IpRangeParser.class);
+        multibinderAggParser.addBinding().to(HistogramParser.class);
+        multibinderAggParser.addBinding().to(DateHistogramParser.class);
+        multibinderAggParser.addBinding().to(GeoDistanceParser.class);
+        multibinderAggParser.addBinding().to(GeoHashGridParser.class);
+        multibinderAggParser.addBinding().to(NestedParser.class);
+        multibinderAggParser.addBinding().to(ReverseNestedParser.class);
+        multibinderAggParser.addBinding().to(TopHitsParser.class);
+        multibinderAggParser.addBinding().to(GeoBoundsParser.class);
+        multibinderAggParser.addBinding().to(GeoCentroidParser.class);
+        multibinderAggParser.addBinding().to(ScriptedMetricParser.class);
+        multibinderAggParser.addBinding().to(ChildrenParser.class);
+        for (Class<? extends Aggregator.Parser> parser : aggParsers) {
+            multibinderAggParser.addBinding().to(parser);
+        }
+
+        Multibinder<PipelineAggregator.Parser> multibinderPipelineAggParser = Multibinder.newSetBinder(binder(), PipelineAggregator.Parser.class);
+        multibinderPipelineAggParser.addBinding().to(DerivativeParser.class);
+        multibinderPipelineAggParser.addBinding().to(MaxBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(MinBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(AvgBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(SumBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(StatsBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(ExtendedStatsBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(PercentilesBucketParser.class);
+        multibinderPipelineAggParser.addBinding().to(MovAvgParser.class);
+        multibinderPipelineAggParser.addBinding().to(CumulativeSumParser.class);
+        multibinderPipelineAggParser.addBinding().to(BucketScriptParser.class);
+        multibinderPipelineAggParser.addBinding().to(BucketSelectorParser.class);
+        multibinderPipelineAggParser.addBinding().to(SerialDiffParser.class);
+        for (Class<? extends PipelineAggregator.Parser> parser : pipelineAggParsers) {
+            multibinderPipelineAggParser.addBinding().to(parser);
+        }
+        bind(AggregatorParsers.class).asEagerSingleton();
+        bind(AggregationParseElement.class).asEagerSingleton();
+        bind(AggregationPhase.class).asEagerSingleton();
+
+        Multibinder<SignificanceHeuristicParser> heuristicParserMultibinder = Multibinder.newSetBinder(binder(), SignificanceHeuristicParser.class);
+        for (Class<? extends SignificanceHeuristicParser> clazz : heuristicParsers) {
+            heuristicParserMultibinder.addBinding().to(clazz);
+        }
+        bind(SignificanceHeuristicParserMapper.class);
+
+        Multibinder<MovAvgModel.AbstractModelParser> modelParserMultibinder = Multibinder.newSetBinder(binder(), MovAvgModel.AbstractModelParser.class);
+        for (Class<? extends MovAvgModel.AbstractModelParser> clazz : modelParsers) {
+            modelParserMultibinder.addBinding().to(clazz);
+        }
+        bind(MovAvgModelParserMapper.class);
     }
 
     protected void configureSearch() {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index a91014d..9a569ce 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -23,7 +23,6 @@ import com.carrotsearch.hppc.ObjectFloatHashMap;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.ObjectSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.NumericDocValues;
@@ -558,10 +557,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
 
         Engine.Searcher engineSearcher = searcher == null ? indexShard.acquireSearcher("search") : searcher;
 
-        DefaultSearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher,
-                indexService,
-                indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher,
-                defaultSearchTimeout, fetchPhase);
+        DefaultSearchContext context = new DefaultSearchContext(idGenerator.incrementAndGet(), request, shardTarget, engineSearcher, indexService, indexShard, scriptService, pageCacheRecycler, bigArrays, threadPool.estimatedTimeInMillisCounter(), parseFieldMatcher, defaultSearchTimeout);
         SearchContext.setCurrent(context);
 
         try {
@@ -755,7 +751,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
                     // ignore
                 }
                 XContentLocation location = completeAggregationsParser != null ? completeAggregationsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse aggregation source [" + sSource + "]", location, e);
+                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
             }
         }
         if (source.suggest() != null) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
index d1fd5b1..f2a9f08 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBinaryParseElement.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.internal.SearchContext;
 
 /**
@@ -30,8 +29,8 @@ import org.elasticsearch.search.internal.SearchContext;
 public class AggregationBinaryParseElement extends AggregationParseElement {
 
     @Inject
-    public AggregationBinaryParseElement(AggregatorParsers aggregatorParsers, IndicesQueriesRegistry queriesRegistry) {
-        super(aggregatorParsers, queriesRegistry);
+    public AggregationBinaryParseElement(AggregatorParsers aggregatorParsers) {
+        super(aggregatorParsers);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
index b6be24c..ed1ba6c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilder.java
@@ -25,7 +25,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -33,15 +32,11 @@ import java.util.List;
 import java.util.Map;
 
 /**
- * A base class for all bucket aggregation builders. NORELEASE REMOVE WHEN AGG
- * REFACTORING IS COMPLETE
+ * A base class for all bucket aggregation builders.
  */
-@Deprecated
 public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extends AbstractAggregationBuilder {
 
     private List<AbstractAggregationBuilder> aggregations;
-    private List<AggregatorFactory> aggregatorFactories;
-    private List<PipelineAggregatorFactory> pipelineAggregatorFactories;
     private BytesReference aggregationsBinary;
     private Map<String, Object> metaData;
 
@@ -53,8 +48,7 @@ public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extend
     }
 
     /**
-     * Add a sub aggregation to this aggregation. NORELEASE REMOVE THIS WHEN AGG
-     * REFACTOR IS COMPLETE
+     * Add a sub get to this bucket get.
      */
     @SuppressWarnings("unchecked")
     public B subAggregation(AbstractAggregationBuilder aggregation) {
@@ -66,33 +60,8 @@ public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extend
     }
 
     /**
-     * Add a sub aggregation to this aggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public B subAggregation(AggregatorFactory aggregation) {
-        if (aggregatorFactories == null) {
-            aggregatorFactories = new ArrayList<>();
-        }
-        aggregatorFactories.add(aggregation);
-        return (B) this;
-    }
-
-    /**
-     * Add a sub aggregation to this aggregation.
-     */
-    @SuppressWarnings("unchecked")
-    public B subAggregation(PipelineAggregatorFactory aggregation) {
-        if (pipelineAggregatorFactories == null) {
-            pipelineAggregatorFactories = new ArrayList<>();
-        }
-        pipelineAggregatorFactories.add(aggregation);
-        return (B) this;
-    }
-
-    /**
      * Sets a raw (xcontent / json) sub addAggregation.
      */
-    @Deprecated
     public B subAggregation(byte[] aggregationsBinary) {
         return subAggregation(aggregationsBinary, 0, aggregationsBinary.length);
     }
@@ -100,7 +69,6 @@ public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extend
     /**
      * Sets a raw (xcontent / json) sub addAggregation.
      */
-    @Deprecated
     public B subAggregation(byte[] aggregationsBinary, int aggregationsBinaryOffset, int aggregationsBinaryLength) {
         return subAggregation(new BytesArray(aggregationsBinary, aggregationsBinaryOffset, aggregationsBinaryLength));
     }
@@ -108,7 +76,6 @@ public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extend
     /**
      * Sets a raw (xcontent / json) sub addAggregation.
      */
-    @Deprecated
     @SuppressWarnings("unchecked")
     public B subAggregation(BytesReference aggregationsBinary) {
         this.aggregationsBinary = aggregationsBinary;
@@ -118,7 +85,6 @@ public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extend
     /**
      * Sets a raw (xcontent / json) sub addAggregation.
      */
-    @Deprecated
     public B subAggregation(XContentBuilder aggs) {
         return subAggregation(aggs.bytes());
     }
@@ -126,7 +92,6 @@ public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extend
     /**
      * Sets a raw (xcontent / json) sub addAggregation.
      */
-    @Deprecated
     public B subAggregation(Map<String, Object> aggs) {
         try {
             XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
@@ -155,24 +120,12 @@ public abstract class AggregationBuilder<B extends AggregationBuilder<B>> extend
         builder.field(type);
         internalXContent(builder, params);
 
-        if (aggregations != null || aggregatorFactories != null || pipelineAggregatorFactories != null || aggregationsBinary != null) {
+        if (aggregations != null || aggregationsBinary != null) {
 
-            if (aggregations != null || aggregatorFactories != null || pipelineAggregatorFactories != null) {
+            if (aggregations != null) {
                 builder.startObject("aggregations");
-                if (aggregations != null) {
-                    for (AbstractAggregationBuilder subAgg : aggregations) {
-                        subAgg.toXContent(builder, params);
-                    }
-                }
-                if (aggregatorFactories != null) {
-                    for (AggregatorFactory subAgg : aggregatorFactories) {
-                        subAgg.toXContent(builder, params);
-                    }
-                }
-                if (pipelineAggregatorFactories != null) {
-                    for (PipelineAggregatorFactory subAgg : pipelineAggregatorFactories) {
-                        subAgg.toXContent(builder, params);
-                    }
+                for (AbstractAggregationBuilder subAgg : aggregations) {
+                    subAgg.toXContent(builder, params);
                 }
                 builder.endObject();
             }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
index 44878b9..dd9c5a3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java
@@ -50,33 +50,33 @@ import org.elasticsearch.search.aggregations.bucket.significant.SignificantTerms
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
+import org.elasticsearch.search.aggregations.metrics.avg.AvgBuilder;
 import org.elasticsearch.search.aggregations.metrics.cardinality.Cardinality;
-import org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityAggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.cardinality.CardinalityBuilder;
 import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBounds;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator;
+import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsBuilder;
 import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroid;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator;
+import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidBuilder;
 import org.elasticsearch.search.aggregations.metrics.max.Max;
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
+import org.elasticsearch.search.aggregations.metrics.max.MaxBuilder;
 import org.elasticsearch.search.aggregations.metrics.min.Min;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
+import org.elasticsearch.search.aggregations.metrics.min.MinBuilder;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanks;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksBuilder;
 import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
 import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesBuilder;
 import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetric;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator;
+import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricBuilder;
 import org.elasticsearch.search.aggregations.metrics.stats.Stats;
-import org.elasticsearch.search.aggregations.metrics.stats.StatsAggregator;
+import org.elasticsearch.search.aggregations.metrics.stats.StatsBuilder;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
+import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsBuilder;
 import org.elasticsearch.search.aggregations.metrics.sum.Sum;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
+import org.elasticsearch.search.aggregations.metrics.sum.SumBuilder;
 import org.elasticsearch.search.aggregations.metrics.tophits.TopHits;
-import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator;
+import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsBuilder;
 import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCount;
-import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountAggregator;
+import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountBuilder;
 
 /**
  * Utility class to create aggregations.
@@ -89,50 +89,50 @@ public class AggregationBuilders {
     /**
      * Create a new {@link ValueCount} aggregation with the given name.
      */
-    public static ValueCountAggregator.Factory count(String name) {
-        return new ValueCountAggregator.Factory(name, null);
+    public static ValueCountBuilder count(String name) {
+        return new ValueCountBuilder(name);
     }
 
     /**
      * Create a new {@link Avg} aggregation with the given name.
      */
-    public static AvgAggregator.Factory avg(String name) {
-        return new AvgAggregator.Factory(name);
+    public static AvgBuilder avg(String name) {
+        return new AvgBuilder(name);
     }
 
     /**
      * Create a new {@link Max} aggregation with the given name.
      */
-    public static MaxAggregator.Factory max(String name) {
-        return new MaxAggregator.Factory(name);
+    public static MaxBuilder max(String name) {
+        return new MaxBuilder(name);
     }
 
     /**
      * Create a new {@link Min} aggregation with the given name.
      */
-    public static MinAggregator.Factory min(String name) {
-        return new MinAggregator.Factory(name);
+    public static MinBuilder min(String name) {
+        return new MinBuilder(name);
     }
 
     /**
      * Create a new {@link Sum} aggregation with the given name.
      */
-    public static SumAggregator.Factory sum(String name) {
-        return new SumAggregator.Factory(name);
+    public static SumBuilder sum(String name) {
+        return new SumBuilder(name);
     }
 
     /**
      * Create a new {@link Stats} aggregation with the given name.
      */
-    public static StatsAggregator.Factory stats(String name) {
-        return new StatsAggregator.Factory(name);
+    public static StatsBuilder stats(String name) {
+        return new StatsBuilder(name);
     }
 
     /**
      * Create a new {@link ExtendedStats} aggregation with the given name.
      */
-    public static ExtendedStatsAggregator.Factory extendedStats(String name) {
-        return new ExtendedStatsAggregator.Factory(name);
+    public static ExtendedStatsBuilder extendedStats(String name) {
+        return new ExtendedStatsBuilder(name);
     }
 
     /**
@@ -271,35 +271,35 @@ public class AggregationBuilders {
     /**
      * Create a new {@link Cardinality} aggregation with the given name.
      */
-    public static CardinalityAggregatorFactory cardinality(String name) {
-        return new CardinalityAggregatorFactory(name, null);
+    public static CardinalityBuilder cardinality(String name) {
+        return new CardinalityBuilder(name);
     }
 
     /**
      * Create a new {@link TopHits} aggregation with the given name.
      */
-    public static TopHitsAggregator.Factory topHits(String name) {
-        return new TopHitsAggregator.Factory(name);
+    public static TopHitsBuilder topHits(String name) {
+        return new TopHitsBuilder(name);
     }
 
     /**
      * Create a new {@link GeoBounds} aggregation with the given name.
      */
-    public static GeoBoundsAggregator.Factory geoBounds(String name) {
-        return new GeoBoundsAggregator.Factory(name);
+    public static GeoBoundsBuilder geoBounds(String name) {
+        return new GeoBoundsBuilder(name);
     }
 
     /**
      * Create a new {@link GeoCentroid} aggregation with the given name.
      */
-    public static GeoCentroidAggregator.Factory geoCentroid(String name) {
-        return new GeoCentroidAggregator.Factory(name);
+    public static GeoCentroidBuilder geoCentroid(String name) {
+        return new GeoCentroidBuilder(name);
     }
 
     /**
      * Create a new {@link ScriptedMetric} aggregation with the given name.
      */
-    public static ScriptedMetricAggregator.Factory scriptedMetric(String name) {
-        return new ScriptedMetricAggregator.Factory(name);
+    public static ScriptedMetricBuilder scriptedMetric(String name) {
+        return new ScriptedMetricBuilder(name);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
index f4eae59..767cbfb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationParseElement.java
@@ -20,8 +20,6 @@ package org.elasticsearch.search.aggregations;
 
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.internal.SearchContext;
 
@@ -51,20 +49,15 @@ import org.elasticsearch.search.internal.SearchContext;
 public class AggregationParseElement implements SearchParseElement {
 
     private final AggregatorParsers aggregatorParsers;
-    private IndicesQueriesRegistry queriesRegistry;
 
     @Inject
-    public AggregationParseElement(AggregatorParsers aggregatorParsers, IndicesQueriesRegistry queriesRegistry) {
+    public AggregationParseElement(AggregatorParsers aggregatorParsers) {
         this.aggregatorParsers = aggregatorParsers;
-        this.queriesRegistry = queriesRegistry;
     }
 
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(context.parseFieldMatcher());
-        AggregatorFactories factories = aggregatorParsers.parseAggregators(parser, parseContext);
+        AggregatorFactories factories = aggregatorParsers.parseAggregators(parser, context);
         context.aggregations(new SearchContextAggregations(factories));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
index 50b0e06..0681996 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
@@ -72,13 +72,12 @@ public class AggregationPhase implements SearchPhase {
         if (context.aggregations() != null) {
             AggregationContext aggregationContext = new AggregationContext(context);
             context.aggregations().aggregationContext(aggregationContext);
-            context.aggregations().factories().init(aggregationContext);
 
             List<Aggregator> collectors = new ArrayList<>();
             Aggregator[] aggregators;
             try {
                 AggregatorFactories factories = context.aggregations().factories();
-                aggregators = factories.createTopLevelAggregators();
+                aggregators = factories.createTopLevelAggregators(aggregationContext);
                 for (int i = 0; i < aggregators.length; i++) {
                     if (aggregators[i] instanceof GlobalAggregator == false) {
                         collectors.add(aggregators[i]);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
index d82d580..8ee4d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/Aggregator.java
@@ -22,14 +22,11 @@ package org.elasticsearch.search.aggregations;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -62,13 +59,8 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
          * @return                  The resolved aggregator factory or {@code null} in case the aggregation should be skipped
          * @throws java.io.IOException      When parsing fails
          */
-        AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException;
+        AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException;
 
-        /**
-         * @return an empty {@link AggregatorFactory} instance for this parser
-         *         that can be used for deserialization
-         */
-        AggregatorFactory[] getFactoryPrototypes();
     }
 
     /**
@@ -115,7 +107,7 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
     public abstract InternalAggregation buildEmptyAggregation();
 
     /** Aggregation mode for sub aggregations. */
-    public enum SubAggCollectionMode implements Writeable<SubAggCollectionMode> {
+    public enum SubAggCollectionMode {
 
         /**
          * Creates buckets and delegates to child aggregators in a single pass over
@@ -151,19 +143,5 @@ public abstract class Aggregator extends BucketCollector implements Releasable {
             }
             throw new ElasticsearchParseException("no [{}] found for value [{}]", KEY.getPreferredName(), value);
         }
-
-        @Override
-        public SubAggCollectionMode readFrom(StreamInput in) throws IOException {
-            int ordinal = in.readVInt();
-            if (ordinal < 0 || ordinal >= values().length) {
-                throw new IOException("Unknown SubAggCollectionMode ordinal [" + ordinal + "]");
-            }
-            return values()[ordinal];
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(ordinal());
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
index 66818a7..6a1cd27 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java
@@ -18,11 +18,6 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
@@ -31,23 +26,19 @@ import org.elasticsearch.search.aggregations.support.AggregationPath.PathElement
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 
 /**
  *
  */
-public class AggregatorFactories extends ToXContentToBytes implements Writeable<AggregatorFactories> {
+public class AggregatorFactories {
 
-    public static final AggregatorFactories EMPTY = new AggregatorFactories(new AggregatorFactory[0],
-            new ArrayList<PipelineAggregatorFactory>());
+    public static final AggregatorFactories EMPTY = new Empty();
 
     private AggregatorFactory parent;
     private AggregatorFactory[] factories;
@@ -57,18 +48,11 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
         return new Builder();
     }
 
-    private AggregatorFactories(AggregatorFactory[] factories,
-            List<PipelineAggregatorFactory> pipelineAggregators) {
+    private AggregatorFactories(AggregatorFactory[] factories, List<PipelineAggregatorFactory> pipelineAggregators) {
         this.factories = factories;
         this.pipelineAggregatorFactories = pipelineAggregators;
     }
 
-    public void init(AggregationContext context) {
-        for (AggregatorFactory factory : factories) {
-            factory.init(context);
-        }
-    }
-
     public List<PipelineAggregator> createPipelineAggregators() throws IOException {
         List<PipelineAggregator> pipelineAggregators = new ArrayList<>();
         for (PipelineAggregatorFactory factory : this.pipelineAggregatorFactories) {
@@ -89,18 +73,18 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
             // propagate the fact that only bucket 0 will be collected with single-bucket
             // aggs
             final boolean collectsFromSingleBucket = false;
-            aggregators[i] = factories[i].create(parent, collectsFromSingleBucket);
+            aggregators[i] = factories[i].create(parent.context(), parent, collectsFromSingleBucket);
         }
         return aggregators;
     }
 
-    public Aggregator[] createTopLevelAggregators() throws IOException {
+    public Aggregator[] createTopLevelAggregators(AggregationContext ctx) throws IOException {
         // These aggregators are going to be used with a single bucket ordinal, no need to wrap the PER_BUCKET ones
         Aggregator[] aggregators = new Aggregator[factories.length];
         for (int i = 0; i < factories.length; i++) {
             // top-level aggs only get called with bucket 0
             final boolean collectsFromSingleBucket = true;
-            aggregators[i] = factories[i].create(null, collectsFromSingleBucket);
+            aggregators[i] = factories[i].create(ctx, null, collectsFromSingleBucket);
         }
         return aggregators;
     }
@@ -125,12 +109,33 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
         }
     }
 
+    private final static class Empty extends AggregatorFactories {
+
+        private static final AggregatorFactory[] EMPTY_FACTORIES = new AggregatorFactory[0];
+        private static final Aggregator[] EMPTY_AGGREGATORS = new Aggregator[0];
+        private static final List<PipelineAggregatorFactory> EMPTY_PIPELINE_AGGREGATORS = new ArrayList<>();
+
+        private Empty() {
+            super(EMPTY_FACTORIES, EMPTY_PIPELINE_AGGREGATORS);
+        }
+
+        @Override
+        public Aggregator[] createSubAggregators(Aggregator parent) {
+            return EMPTY_AGGREGATORS;
+        }
+
+        @Override
+        public Aggregator[] createTopLevelAggregators(AggregationContext ctx) {
+            return EMPTY_AGGREGATORS;
+        }
+
+    }
+
     public static class Builder {
 
         private final Set<String> names = new HashSet<>();
         private final List<AggregatorFactory> factories = new ArrayList<>();
         private final List<PipelineAggregatorFactory> pipelineAggregatorFactories = new ArrayList<>();
-        private boolean skipResolveOrder;
 
         public Builder addAggregator(AggregatorFactory factory) {
             if (!names.add(factory.name)) {
@@ -145,29 +150,15 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
             return this;
         }
 
-        /**
-         * FOR TESTING ONLY
-         */
-        Builder skipResolveOrder() {
-            this.skipResolveOrder = true;
-            return this;
-        }
-
         public AggregatorFactories build() {
             if (factories.isEmpty() && pipelineAggregatorFactories.isEmpty()) {
                 return EMPTY;
             }
-            List<PipelineAggregatorFactory> orderedpipelineAggregators = null;
-            if (skipResolveOrder) {
-                orderedpipelineAggregators = new ArrayList<>(pipelineAggregatorFactories);
-            } else {
-                orderedpipelineAggregators = resolvePipelineAggregatorOrder(this.pipelineAggregatorFactories, this.factories);
-            }
+            List<PipelineAggregatorFactory> orderedpipelineAggregators = resolvePipelineAggregatorOrder(this.pipelineAggregatorFactories, this.factories);
             return new AggregatorFactories(factories.toArray(new AggregatorFactory[factories.size()]), orderedpipelineAggregators);
         }
 
-        private List<PipelineAggregatorFactory> resolvePipelineAggregatorOrder(List<PipelineAggregatorFactory> pipelineAggregatorFactories,
-                List<AggregatorFactory> aggFactories) {
+        private List<PipelineAggregatorFactory> resolvePipelineAggregatorOrder(List<PipelineAggregatorFactory> pipelineAggregatorFactories, List<AggregatorFactory> aggFactories) {
             Map<String, PipelineAggregatorFactory> pipelineAggregatorFactoriesMap = new HashMap<>();
             for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
                 pipelineAggregatorFactoriesMap.put(factory.getName(), factory);
@@ -262,71 +253,4 @@ public class AggregatorFactories extends ToXContentToBytes implements Writeable<
             return this.pipelineAggregatorFactories;
         }
     }
-
-    @Override
-    public AggregatorFactories readFrom(StreamInput in) throws IOException {
-        int factoriesSize = in.readVInt();
-        AggregatorFactory[] factoriesList = new AggregatorFactory[factoriesSize];
-        for (int i = 0; i < factoriesSize; i++) {
-            AggregatorFactory factory = in.readAggregatorFactory();
-            factoriesList[i] = factory;
-        }
-        int pipelineFactoriesSize = in.readVInt();
-        List<PipelineAggregatorFactory> pipelineAggregatorFactoriesList = new ArrayList<PipelineAggregatorFactory>(pipelineFactoriesSize);
-        for (int i = 0; i < pipelineFactoriesSize; i++) {
-            PipelineAggregatorFactory factory = in.readPipelineAggregatorFactory();
-            pipelineAggregatorFactoriesList.add(factory);
-        }
-        AggregatorFactories aggregatorFactories = new AggregatorFactories(factoriesList,
-                Collections.unmodifiableList(pipelineAggregatorFactoriesList));
-        return aggregatorFactories;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(this.factories.length);
-        for (AggregatorFactory factory : factories) {
-            out.writeAggregatorFactory(factory);
-        }
-        out.writeVInt(this.pipelineAggregatorFactories.size());
-        for (PipelineAggregatorFactory factory : pipelineAggregatorFactories) {
-            out.writePipelineAggregatorFactory(factory);
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (factories != null) {
-            for (AggregatorFactory subAgg : factories) {
-                subAgg.toXContent(builder, params);
-            }
-        }
-        if (pipelineAggregatorFactories != null) {
-            for (PipelineAggregatorFactory subAgg : pipelineAggregatorFactories) {
-                subAgg.toXContent(builder, params);
-            }
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(Arrays.hashCode(factories), pipelineAggregatorFactories);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        AggregatorFactories other = (AggregatorFactories) obj;
-        if (!Objects.deepEquals(factories, other.factories))
-            return false;
-        if (!Objects.equals(pipelineAggregatorFactories, other.pipelineAggregatorFactories))
-            return false;
-        return true;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
index 8f51cc7..680e3ef 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactory.java
@@ -18,18 +18,11 @@
  */
 package org.elasticsearch.search.aggregations;
 
-
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Scorer;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.ObjectArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext.Lifetime;
@@ -37,20 +30,17 @@ import org.elasticsearch.search.internal.SearchContext.Lifetime;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * A factory that knows how to create an {@link Aggregator} of a specific type.
  */
-public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extends ToXContentToBytes
-        implements NamedWriteable<AggregatorFactory<AF>> {
+public abstract class AggregatorFactory {
 
     protected String name;
-    protected Type type;
-    protected AggregatorFactory<?> parent;
+    protected String type;
+    protected AggregatorFactory parent;
     protected AggregatorFactories factories = AggregatorFactories.EMPTY;
     protected Map<String, Object> metaData;
-    private AggregationContext context;
 
     /**
      * Constructs a new aggregator factory.
@@ -58,42 +48,22 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
      * @param name  The aggregation name
      * @param type  The aggregation type
      */
-    public AggregatorFactory(String name, Type type) {
+    public AggregatorFactory(String name, String type) {
         this.name = name;
         this.type = type;
     }
 
     /**
-     * Initializes this factory with the given {@link AggregationContext} ready
-     * to create {@link Aggregator}s
-     */
-    public final void init(AggregationContext context) {
-        this.context = context;
-        doInit(context);
-        this.factories.init(context);
-    }
-
-    /**
-     * Allows the {@link AggregatorFactory} to initialize any state prior to
-     * using it to create {@link Aggregator}s.
-     *
-     * @param context
-     *            the {@link AggregationContext} to use during initialization.
-     */
-    protected void doInit(AggregationContext context) {
-    }
-
-    /**
      * Registers sub-factories with this factory. The sub-factory will be responsible for the creation of sub-aggregators under the
      * aggregator created by this factory.
      *
      * @param subFactories  The sub-factories
      * @return  this factory (fluent interface)
      */
-    public AF subFactories(AggregatorFactories subFactories) {
+    public AggregatorFactory subFactories(AggregatorFactories subFactories) {
         this.factories = subFactories;
         this.factories.setParent(this);
-        return (AF) this;
+        return this;
     }
 
     public String name() {
@@ -111,7 +81,7 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
     /**
      * @return  The parent factory if one exists (will always return {@code null} for top level aggregator factories).
      */
-    public AggregatorFactory<?> parent() {
+    public AggregatorFactory parent() {
         return parent;
     }
 
@@ -121,13 +91,14 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
     /**
      * Creates the aggregator
      *
+     * @param context               The aggregation context
      * @param parent                The parent aggregator (if this is a top level factory, the parent will be {@code null})
      * @param collectsFromSingleBucket  If true then the created aggregator will only be collected with <tt>0</tt> as a bucket ordinal.
      *                              Some factories can take advantage of this in order to return more optimized implementations.
      *
      * @return                      The created aggregator
      */
-    public final Aggregator create(Aggregator parent, boolean collectsFromSingleBucket) throws IOException {
+    public final Aggregator create(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket) throws IOException {
         return createInternal(context, parent, collectsFromSingleBucket, this.factories.createPipelineAggregators(), this.metaData);
     }
 
@@ -138,73 +109,14 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
         this.metaData = metaData;
     }
 
-    @Override
-    public final AggregatorFactory<AF> readFrom(StreamInput in) throws IOException {
-        String name = in.readString();
-        AggregatorFactory<AF> factory = doReadFrom(name, in);
-        factory.factories = AggregatorFactories.EMPTY.readFrom(in);
-        factory.factories.setParent(this);
-        factory.metaData = in.readMap();
-        return factory;
-    }
-
-    // NORELEASE make this abstract when agg refactor complete
-    protected AggregatorFactory<AF> doReadFrom(String name, StreamInput in) throws IOException {
-        return null;
-    }
-
-    @Override
-    public final void writeTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-        doWriteTo(out);
-        factories.writeTo(out);
-        out.writeMap(metaData);
-    }
-
-    // NORELEASE make this abstract when agg refactor complete
-    protected void doWriteTo(StreamOutput out) throws IOException {
-    }
-
-    @Override
-    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(name);
 
-        if (this.metaData != null) {
-            builder.field("meta", this.metaData);
-        }
-        builder.field(type.name());
-        internalXContent(builder, params);
-
-        if (factories != null && factories.count() > 0) {
-            builder.field("aggregations");
-            factories.toXContent(builder, params);
-
-        }
-
-        return builder.endObject();
-    }
-
-    // NORELEASE make this method abstract when agg refactor complete
-    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        return builder;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return type.stream().toUtf8();
-    }
-
-    public String getType() {
-        return type.name();
-    }
 
     /**
      * Utility method. Given an {@link AggregatorFactory} that creates {@link Aggregator}s that only know how
      * to collect bucket <tt>0</tt>, this returns an aggregator that can collect any bucket.
      */
-    protected static Aggregator asMultiBucketAggregator(final AggregatorFactory factory,
-            final AggregationContext context, final Aggregator parent) throws IOException {
-        final Aggregator first = factory.create(parent, true);
+    protected static Aggregator asMultiBucketAggregator(final AggregatorFactory factory, final AggregationContext context, final Aggregator parent) throws IOException {
+        final Aggregator first = factory.create(context, parent, true);
         final BigArrays bigArrays = context.bigArrays();
         return new Aggregator() {
 
@@ -285,7 +197,7 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
                         if (collector == null) {
                             Aggregator aggregator = aggregators.get(bucket);
                             if (aggregator == null) {
-                                aggregator = factory.create(parent, true);
+                                aggregator = factory.create(context, parent, true);
                                 aggregator.preCollection();
                                 aggregators.set(bucket, aggregator);
                             }
@@ -322,41 +234,4 @@ public abstract class AggregatorFactory<AF extends AggregatorFactory<AF>> extend
         };
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(factories, metaData, name, type, doHashCode());
-    }
-
-    // NORELEASE make this method abstract here when agg refactor complete (so
-    // that subclasses are forced to implement it)
-    protected int doHashCode() {
-        throw new UnsupportedOperationException(
-                "This method should be implemented by a sub-class and should not rely on this method. When agg re-factoring is complete this method will be made abstract.");
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        AggregatorFactory<AF> other = (AggregatorFactory<AF>) obj;
-        if (!Objects.equals(name, other.name))
-            return false;
-        if (!Objects.equals(type, other.type))
-            return false;
-        if (!Objects.equals(metaData, other.metaData))
-            return false;
-        if (!Objects.equals(factories, other.factories))
-            return false;
-        return doEquals(obj);
-    }
-
-    // NORELEASE make this method abstract here when agg refactor complete (so
-    // that subclasses are forced to implement it)
-    protected boolean doEquals(Object obj) {
-        throw new UnsupportedOperationException(
-                "This method should be implemented by a sub-class and should not rely on this method. When agg re-factoring is complete this method will be made abstract.");
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
index 9813be0..f38138f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
@@ -18,14 +18,13 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashMap;
@@ -55,28 +54,15 @@ public class AggregatorParsers {
      *            ).
      */
     @Inject
-    public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers,
-            NamedWriteableRegistry namedWriteableRegistry) {
+    public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers) {
         Map<String, Aggregator.Parser> aggParsersBuilder = new HashMap<>(aggParsers.size());
         for (Aggregator.Parser parser : aggParsers) {
             aggParsersBuilder.put(parser.type(), parser);
-            AggregatorFactory[] factoryPrototypes = parser.getFactoryPrototypes();
-            // NORELEASE remove this check when agg refactoring complete
-            if (factoryPrototypes != null) {
-                for (AggregatorFactory factoryPrototype : factoryPrototypes) {
-                    namedWriteableRegistry.registerPrototype(AggregatorFactory.class, factoryPrototype);
-                }
-            }
         }
         this.aggParsers = unmodifiableMap(aggParsersBuilder);
         Map<String, PipelineAggregator.Parser> pipelineAggregatorParsersBuilder = new HashMap<>(pipelineAggregatorParsers.size());
         for (PipelineAggregator.Parser parser : pipelineAggregatorParsers) {
             pipelineAggregatorParsersBuilder.put(parser.type(), parser);
-            PipelineAggregatorFactory factoryPrototype = parser.getFactoryPrototype();
-            // NORELEASE remove this check when agg refactoring complete
-            if (factoryPrototype != null) {
-                namedWriteableRegistry.registerPrototype(PipelineAggregatorFactory.class, factoryPrototype);
-            }
         }
         this.pipelineAggregatorParsers = unmodifiableMap(pipelineAggregatorParsersBuilder);
     }
@@ -107,37 +93,37 @@ public class AggregatorParsers {
      * Parses the aggregation request recursively generating aggregator factories in turn.
      *
      * @param parser    The input xcontent that will be parsed.
-     * @param parseContext   The parse context.
+     * @param context   The search context.
      *
      * @return          The parsed aggregator factories.
      *
      * @throws IOException When parsing fails for unknown reasons.
      */
-    public AggregatorFactories parseAggregators(XContentParser parser, QueryParseContext parseContext) throws IOException {
-        return parseAggregators(parser, parseContext, 0);
+    public AggregatorFactories parseAggregators(XContentParser parser, SearchContext context) throws IOException {
+        return parseAggregators(parser, context, 0);
     }
 
 
-    private AggregatorFactories parseAggregators(XContentParser parser, QueryParseContext parseContext, int level) throws IOException {
+    private AggregatorFactories parseAggregators(XContentParser parser, SearchContext context, int level) throws IOException {
         Matcher validAggMatcher = VALID_AGG_NAME.matcher("");
         AggregatorFactories.Builder factories = new AggregatorFactories.Builder();
 
         XContentParser.Token token = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token != XContentParser.Token.FIELD_NAME) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [aggs]: aggregations definitions must start with the name of the aggregation.");
+                throw new SearchParseException(context, "Unexpected token " + token
+                        + " in [aggs]: aggregations definitions must start with the name of the aggregation.", parser.getTokenLocation());
             }
             final String aggregationName = parser.currentName();
             if (!validAggMatcher.reset(aggregationName).matches()) {
-                throw new ParsingException(parser.getTokenLocation(), "Invalid aggregation name [" + aggregationName
-                        + "]. Aggregation names must be alpha-numeric and can only contain '_' and '-'");
+                throw new SearchParseException(context, "Invalid aggregation name [" + aggregationName
+                        + "]. Aggregation names must be alpha-numeric and can only contain '_' and '-'", parser.getTokenLocation());
             }
 
             token = parser.nextToken();
             if (token != XContentParser.Token.START_OBJECT) {
-                throw new ParsingException(parser.getTokenLocation(), "Aggregation definition for [" + aggregationName + " starts with a ["
-                        + token + "], expected a [" + XContentParser.Token.START_OBJECT + "].");
+                throw new SearchParseException(context, "Aggregation definition for [" + aggregationName + " starts with a [" + token
+                        + "], expected a [" + XContentParser.Token.START_OBJECT + "].", parser.getTokenLocation());
             }
 
             AggregatorFactory aggFactory = null;
@@ -148,8 +134,7 @@ public class AggregatorParsers {
 
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                 if (token != XContentParser.Token.FIELD_NAME) {
-                    throw new ParsingException(
-                            parser.getTokenLocation(), "Expected [" + XContentParser.Token.FIELD_NAME + "] under a ["
+                    throw new SearchParseException(context, "Expected [" + XContentParser.Token.FIELD_NAME + "] under a ["
                             + XContentParser.Token.START_OBJECT + "], but got a [" + token + "] in [" + aggregationName + "]",
                             parser.getTokenLocation());
                 }
@@ -158,8 +143,7 @@ public class AggregatorParsers {
                 token = parser.nextToken();
                 if ("aggregations_binary".equals(fieldName)) {
                     if (subFactories != null) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Found two sub aggregation definitions under [" + aggregationName + "]",
+                        throw new SearchParseException(context, "Found two sub aggregation definitions under [" + aggregationName + "]",
                                 parser.getTokenLocation());
                     }
                     XContentParser binaryParser = null;
@@ -167,17 +151,17 @@ public class AggregatorParsers {
                         byte[] source = parser.binaryValue();
                         binaryParser = XContentFactory.xContent(source).createParser(source);
                     } else {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Expected [" + XContentParser.Token.VALUE_STRING + " or " + XContentParser.Token.VALUE_EMBEDDED_OBJECT
-                                        + "] for [" + fieldName + "], but got a [" + token + "] in [" + aggregationName + "]");
+                        throw new SearchParseException(context, "Expected [" + XContentParser.Token.VALUE_STRING + " or "
+                                + XContentParser.Token.VALUE_EMBEDDED_OBJECT + "] for [" + fieldName + "], but got a [" + token + "] in ["
+                                + aggregationName + "]", parser.getTokenLocation());
                     }
                     XContentParser.Token binaryToken = binaryParser.nextToken();
                     if (binaryToken != XContentParser.Token.START_OBJECT) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Expected [" + XContentParser.Token.START_OBJECT + "] as first token when parsing [" + fieldName
-                                        + "], but got a [" + binaryToken + "] in [" + aggregationName + "]");
+                        throw new SearchParseException(context, "Expected [" + XContentParser.Token.START_OBJECT
+                                + "] as first token when parsing [" + fieldName + "], but got a [" + binaryToken + "] in ["
+                                + aggregationName + "]", parser.getTokenLocation());
                     }
-                    subFactories = parseAggregators(binaryParser, parseContext, level + 1);
+                    subFactories = parseAggregators(binaryParser, context, level + 1);
                 } else if (token == XContentParser.Token.START_OBJECT) {
                     switch (fieldName) {
                     case "meta":
@@ -186,42 +170,42 @@ public class AggregatorParsers {
                     case "aggregations":
                     case "aggs":
                         if (subFactories != null) {
-                            throw new ParsingException(parser.getTokenLocation(),
-                                    "Found two sub aggregation definitions under [" + aggregationName + "]");
+                            throw new SearchParseException(context,
+                                    "Found two sub aggregation definitions under [" + aggregationName + "]", parser.getTokenLocation());
                         }
-                        subFactories = parseAggregators(parser, parseContext, level + 1);
+                        subFactories = parseAggregators(parser, context, level + 1);
                         break;
                     default:
                         if (aggFactory != null) {
-                            throw new ParsingException(parser.getTokenLocation(), "Found two aggregation type definitions in ["
-                                    + aggregationName + "]: [" + aggFactory.type + "] and [" + fieldName + "]");
+                            throw new SearchParseException(context, "Found two aggregation type definitions in [" + aggregationName
+                                    + "]: [" + aggFactory.type + "] and [" + fieldName + "]", parser.getTokenLocation());
                         }
                         if (pipelineAggregatorFactory != null) {
-                            throw new ParsingException(parser.getTokenLocation(), "Found two aggregation type definitions in ["
-                                    + aggregationName + "]: [" + pipelineAggregatorFactory + "] and [" + fieldName + "]");
+                            throw new SearchParseException(context, "Found two aggregation type definitions in [" + aggregationName
+                                    + "]: [" + pipelineAggregatorFactory + "] and [" + fieldName + "]", parser.getTokenLocation());
                         }
 
                         Aggregator.Parser aggregatorParser = parser(fieldName);
                         if (aggregatorParser == null) {
                             PipelineAggregator.Parser pipelineAggregatorParser = pipelineAggregator(fieldName);
                             if (pipelineAggregatorParser == null) {
-                                throw new ParsingException(parser.getTokenLocation(),
-                                        "Could not find aggregator type [" + fieldName + "] in [" + aggregationName + "]");
+                                throw new SearchParseException(context, "Could not find aggregator type [" + fieldName + "] in ["
+                                        + aggregationName + "]", parser.getTokenLocation());
                             } else {
-                                pipelineAggregatorFactory = pipelineAggregatorParser.parse(aggregationName, parser, parseContext);
+                                pipelineAggregatorFactory = pipelineAggregatorParser.parse(aggregationName, parser, context);
                             }
                         } else {
-                            aggFactory = aggregatorParser.parse(aggregationName, parser, parseContext);
+                            aggFactory = aggregatorParser.parse(aggregationName, parser, context);
                         }
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] under ["
-                            + fieldName + "], but got a [" + token + "] in [" + aggregationName + "]");
+                    throw new SearchParseException(context, "Expected [" + XContentParser.Token.START_OBJECT + "] under [" + fieldName
+                            + "], but got a [" + token + "] in [" + aggregationName + "]", parser.getTokenLocation());
                 }
             }
 
             if (aggFactory == null && pipelineAggregatorFactory == null) {
-                throw new ParsingException(parser.getTokenLocation(), "Missing definition for aggregation [" + aggregationName + "]",
+                throw new SearchParseException(context, "Missing definition for aggregation [" + aggregationName + "]",
                         parser.getTokenLocation());
             } else if (aggFactory != null) {
                 assert pipelineAggregatorFactory == null;
@@ -241,8 +225,7 @@ public class AggregatorParsers {
             } else {
                 assert pipelineAggregatorFactory != null;
                 if (subFactories != null) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Aggregation [" + aggregationName + "] cannot define sub-aggregations",
+                    throw new SearchParseException(context, "Aggregation [" + aggregationName + "] cannot define sub-aggregations",
                             parser.getTokenLocation());
                 }
                 if (level == 0) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
index b4e2c88..438e872 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java
@@ -18,11 +18,18 @@
  */
 package org.elasticsearch.search.aggregations.bucket.children;
 
-import org.elasticsearch.common.ParsingException;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.FieldContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +44,7 @@ public class ChildrenParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String childType = null;
 
         XContentParser.Token token;
@@ -49,25 +56,45 @@ public class ChildrenParser implements Aggregator.Parser {
                 if ("type".equals(currentFieldName)) {
                     childType = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (childType == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing [child_type] field for children aggregation [" + aggregationName + "]");
+            throw new SearchParseException(context, "Missing [child_type] field for children aggregation [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
+        ValuesSourceConfig<ValuesSource.Bytes.WithOrdinals.ParentChild> config = new ValuesSourceConfig<>(ValuesSource.Bytes.WithOrdinals.ParentChild.class);
+        DocumentMapper childDocMapper = context.mapperService().documentMapper(childType);
 
-        return new ParentToChildrenAggregator.Factory(aggregationName, childType);
+        String parentType = null;
+        Query parentFilter = null;
+        Query childFilter = null;
+        if (childDocMapper != null) {
+            ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
+            if (!parentFieldMapper.active()) {
+                throw new SearchParseException(context, "[children] no [_parent] field not configured that points to a parent type", parser.getTokenLocation());
             }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new ParentToChildrenAggregator.Factory(null, null) };
+            parentType = parentFieldMapper.type();
+            DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+            if (parentDocMapper != null) {
+                // TODO: use the query API
+                parentFilter = parentDocMapper.typeFilter();
+                childFilter = childDocMapper.typeFilter();
+                ParentChildIndexFieldData parentChildIndexFieldData = context.fieldData().getForField(parentFieldMapper.fieldType());
+                config.fieldContext(new FieldContext(parentFieldMapper.fieldType().name(), parentChildIndexFieldData, parentFieldMapper.fieldType()));
+            } else {
+                config.unmapped(true);
+            }
+        } else {
+            config.unmapped(true);
+        }
+        return new ParentToChildrenAggregator.Factory(aggregationName, config, parentType, parentFilter, childFilter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
index 32dce9a..63819b9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java
@@ -27,18 +27,10 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.LongArray;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -47,25 +39,19 @@ import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.FieldContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 // The RecordingPerReaderBucketCollector assumes per segment recording which isn't the case for this
 // aggregation, for this reason that collector can't be used
 public class ParentToChildrenAggregator extends SingleBucketAggregator {
 
-    static final ParseField TYPE_FIELD = new ParseField("type");
-
     private final String parentType;
     private final Weight childFilter;
     private final Weight parentFilter;
@@ -189,27 +175,17 @@ public class ParentToChildrenAggregator extends SingleBucketAggregator {
         Releasables.close(parentOrdToBuckets, parentOrdToOtherBuckets);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Bytes.WithOrdinals.ParentChild, Factory> {
-
-        private String parentType;
-        private final String childType;
-        private Query parentFilter;
-        private Query childFilter;
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Bytes.WithOrdinals.ParentChild> {
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param childType
-         *            the type of children documents
-         */
-        public Factory(String name, String childType) {
-            super(name, InternalChildren.TYPE, ValuesSourceType.BYTES, ValueType.STRING);
-            this.childType = childType;
-        }
+        private final String parentType;
+        private final Query parentFilter;
+        private final Query childFilter;
 
-        @Override
-        public void doInit(AggregationContext context) {
-            resolveConfig(context);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Bytes.WithOrdinals.ParentChild> config, String parentType, Query parentFilter, Query childFilter) {
+            super(name, InternalChildren.TYPE.name(), config);
+            this.parentType = parentType;
+            this.parentFilter = parentFilter;
+            this.childFilter = childFilter;
         }
 
         @Override
@@ -234,62 +210,5 @@ public class ParentToChildrenAggregator extends SingleBucketAggregator {
                     valuesSource, maxOrd, pipelineAggregators, metaData);
         }
 
-        private void resolveConfig(AggregationContext aggregationContext) {
-            config = new ValuesSourceConfig<>(ValuesSourceType.BYTES);
-            DocumentMapper childDocMapper = aggregationContext.searchContext().mapperService().documentMapper(childType);
-
-            if (childDocMapper != null) {
-                ParentFieldMapper parentFieldMapper = childDocMapper.parentFieldMapper();
-                if (!parentFieldMapper.active()) {
-                    throw new SearchParseException(aggregationContext.searchContext(),
-                            "[children] no [_parent] field not configured that points to a parent type", null); // NOCOMMIT fix exception args
-                }
-                parentType = parentFieldMapper.type();
-                DocumentMapper parentDocMapper = aggregationContext.searchContext().mapperService().documentMapper(parentType);
-                if (parentDocMapper != null) {
-                    parentFilter = parentDocMapper.typeFilter();
-                    childFilter = childDocMapper.typeFilter();
-                    ParentChildIndexFieldData parentChildIndexFieldData = aggregationContext.searchContext().fieldData()
-                            .getForField(parentFieldMapper.fieldType());
-                    config.fieldContext(new FieldContext(parentFieldMapper.fieldType().name(), parentChildIndexFieldData,
-                            parentFieldMapper.fieldType()));
-                } else {
-                    config.unmapped(true);
-                }
-            } else {
-                config.unmapped(true);
-            }
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(TYPE_FIELD.getPreferredName(), childType);
-            return builder;
-        }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            String childType = in.readString();
-            Factory factory = new Factory(name, childType);
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeString(childType);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(childType);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(childType, other.childType);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
index 7e70ffa..30c34c3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java
@@ -23,11 +23,7 @@ import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
@@ -41,7 +37,6 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregate all docs that match a filter.
@@ -86,20 +81,12 @@ public class FilterAggregator extends SingleBucketAggregator {
         return new InternalFilter(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
-        private QueryBuilder<?> filter;
+        private final Query filter;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filter
-         *            Set the filter to use, only documents that match this
-         *            filter will fall into the bucket defined by this
-         *            {@link Filter} aggregation.
-         */
-        public Factory(String name, QueryBuilder<?> filter) {
-            super(name, InternalFilter.TYPE);
+        public Factory(String name, Query filter) {
+            super(name, InternalFilter.TYPE.name());
             this.filter = filter;
         }
 
@@ -116,42 +103,11 @@ public class FilterAggregator extends SingleBucketAggregator {
             IndexSearcher contextSearcher = context.searchContext().searcher();
             if (searcher != contextSearcher) {
                 searcher = contextSearcher;
-                Query filter = this.filter.toQuery(context.searchContext().indexShard().getQueryShardContext());
                 weight = contextSearcher.createNormalizedWeight(filter, false);
             }
             return new FilterAggregator(name, weight, factories, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (filter != null) {
-                filter.toXContent(builder, params);
-            }
-            return builder;
-        }
-
-        @Override
-        protected Factory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, in.readQuery());
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeQuery(filter);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(filter);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(filter, other.filter);
-        }
-
     }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
index 0095351..48702da 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java
@@ -18,13 +18,12 @@
  */
 package org.elasticsearch.search.aggregations.bucket.filter;
 
-import org.elasticsearch.common.ParsingException;
+import org.apache.lucene.search.MatchAllDocsQuery;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.MatchAllQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -39,21 +38,10 @@ public class FilterParser implements Aggregator.Parser {
     }
 
     @Override
-    public FilterAggregator.Factory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
-        QueryBuilder<?> filter = context.parseInnerQueryBuilder();
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ParsedQuery filter = context.indexShard().getQueryShardContext().parseInnerFilter(parser);
 
-        if (filter == null) {
-            throw new ParsingException(null, "filter cannot be null in filter aggregation [{}]", aggregationName);
-        }
-
-        FilterAggregator.Factory factory = new FilterAggregator.Factory(aggregationName,
-                filter == null ? new MatchAllQueryBuilder() : filter);
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new FilterAggregator.Factory(null, null) };
+        return new FilterAggregator.Factory(aggregationName, filter == null ? new MatchAllDocsQuery() : filter.query());
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
index 955802e..c16089e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java
@@ -24,14 +24,7 @@ import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
@@ -47,72 +40,21 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class FiltersAggregator extends BucketsAggregator {
 
-    public static final ParseField FILTERS_FIELD = new ParseField("filters");
-    public static final ParseField OTHER_BUCKET_FIELD = new ParseField("other_bucket");
-    public static final ParseField OTHER_BUCKET_KEY_FIELD = new ParseField("other_bucket_key");
+    static class KeyedFilter {
 
-    public static class KeyedFilter implements Writeable<KeyedFilter>, ToXContent {
+        final String key;
+        final Query filter;
 
-        static final KeyedFilter PROTOTYPE = new KeyedFilter(null, null);
-        private final String key;
-        private final QueryBuilder<?> filter;
-
-        public KeyedFilter(String key, QueryBuilder<?> filter) {
+        KeyedFilter(String key, Query filter) {
             this.key = key;
             this.filter = filter;
         }
-
-        public String key() {
-            return key;
-        }
-
-        public QueryBuilder<?> filter() {
-            return filter;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(key, filter);
-            return builder;
-        }
-
-        @Override
-        public KeyedFilter readFrom(StreamInput in) throws IOException {
-            String key = in.readString();
-            QueryBuilder<?> filter = in.readQuery();
-            return new KeyedFilter(key, filter);
-    }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeString(key);
-            out.writeQuery(filter);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(key, filter);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            KeyedFilter other = (KeyedFilter) obj;
-            return Objects.equals(key, other.key)
-                    && Objects.equals(filter, other.filter);
-        }
     }
 
     private final String[] keys;
@@ -197,71 +139,23 @@ public class FiltersAggregator extends BucketsAggregator {
         return owningBucketOrdinal * totalNumKeys + filterOrd;
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
         private final List<KeyedFilter> filters;
-        private final boolean keyed;
-        private boolean otherBucket = false;
-        private String otherBucketKey = "_other_";
+        private final String[] keys;
+        private boolean keyed;
+        private String otherBucketKey;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filters
-         *            the KeyedFilters to use with this aggregation.
-         */
-        public Factory(String name, List<KeyedFilter> filters) {
-            super(name, InternalFilters.TYPE);
+        public Factory(String name, List<KeyedFilter> filters, boolean keyed, String otherBucketKey) {
+            super(name, InternalFilters.TYPE.name());
             this.filters = filters;
-            this.keyed = true;
-        }
-
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param filters
-         *            the filters to use with this aggregation
-         */
-        public Factory(String name, QueryBuilder<?>... filters) {
-            super(name, InternalFilters.TYPE);
-            List<KeyedFilter> keyedFilters = new ArrayList<>(filters.length);
-            for (int i = 0; i < filters.length; i++) {
-                keyedFilters.add(new KeyedFilter(String.valueOf(i), filters[i]));
-            }
-            this.filters = keyedFilters;
-            this.keyed = false;
-        }
-
-        /**
-         * Set whether to include a bucket for documents not matching any filter
-         */
-        public Factory otherBucket(boolean otherBucket) {
-            this.otherBucket = otherBucket;
-            return this;
-        }
-
-        /**
-         * Get whether to include a bucket for documents not matching any filter
-         */
-        public boolean otherBucket() {
-            return otherBucket;
-        }
-
-        /**
-         * Set the key to use for the bucket for documents not matching any
-         * filter.
-         */
-        public Factory otherBucketKey(String otherBucketKey) {
+            this.keyed = keyed;
             this.otherBucketKey = otherBucketKey;
-            return this;
-        }
-
-        /**
-         * Get the key to use for the bucket for documents not matching any
-         * filter.
-         */
-        public String otherBucketKey() {
-            return otherBucketKey;
+            this.keys = new String[filters.size()];
+            for (int i = 0; i < filters.size(); ++i) {
+                KeyedFilter keyedFilter = filters.get(i);
+                this.keys[i] = keyedFilter.key;
+            }
         }
 
         // TODO: refactor in order to initialize the factory once with its parent,
@@ -269,7 +163,6 @@ public class FiltersAggregator extends BucketsAggregator {
         // (since create may be called thousands of times)
 
         private IndexSearcher searcher;
-        private String[] keys;
         private Weight[] weights;
 
         @Override
@@ -279,93 +172,12 @@ public class FiltersAggregator extends BucketsAggregator {
             if (searcher != contextSearcher) {
                 searcher = contextSearcher;
                 weights = new Weight[filters.size()];
-                keys = new String[filters.size()];
                 for (int i = 0; i < filters.size(); ++i) {
                     KeyedFilter keyedFilter = filters.get(i);
-                    this.keys[i] = keyedFilter.key;
-                    Query filter = keyedFilter.filter.toFilter(context.searchContext().indexShard().getQueryShardContext());
-                    this.weights[i] = contextSearcher.createNormalizedWeight(filter, false);
+                    this.weights[i] = contextSearcher.createNormalizedWeight(keyedFilter.filter, false);
                 }
             }
-            return new FiltersAggregator(name, factories, keys, weights, keyed, otherBucket ? otherBucketKey : null, context, parent,
-                    pipelineAggregators, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (keyed) {
-                builder.startObject(FILTERS_FIELD.getPreferredName());
-                for (KeyedFilter keyedFilter : filters) {
-                    builder.field(keyedFilter.key(), keyedFilter.filter());
-                }
-                builder.endObject();
-            } else {
-                builder.startArray(FILTERS_FIELD.getPreferredName());
-                for (KeyedFilter keyedFilter : filters) {
-                    builder.value(keyedFilter.filter());
-                }
-                builder.endArray();
-            }
-            builder.field(OTHER_BUCKET_FIELD.getPreferredName(), otherBucket);
-            builder.field(OTHER_BUCKET_KEY_FIELD.getPreferredName(), otherBucketKey);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected Factory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<KeyedFilter> filters = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    filters.add(KeyedFilter.PROTOTYPE.readFrom(in));
-                }
-                factory = new Factory(name, filters);
-            } else {
-                int size = in.readVInt();
-                QueryBuilder<?>[] filters = new QueryBuilder<?>[size];
-                for (int i = 0; i < size; i++) {
-                    filters[i] = in.readQuery();
-                }
-                factory = new Factory(name, filters);
-            }
-            factory.otherBucket = in.readBoolean();
-            factory.otherBucketKey = in.readString();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(keyed);
-            if (keyed) {
-                out.writeVInt(filters.size());
-                for (KeyedFilter keyedFilter : filters) {
-                    keyedFilter.writeTo(out);
-                }
-            } else {
-                out.writeVInt(filters.size());
-                for (KeyedFilter keyedFilter : filters) {
-                    out.writeQuery(keyedFilter.filter());
-                }
-            }
-            out.writeBoolean(otherBucket);
-            out.writeString(otherBucketKey);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(filters, keyed, otherBucket, otherBucketKey);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(filters, other.filters)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(otherBucket, other.otherBucket)
-                    && Objects.equals(otherBucketKey, other.otherBucketKey);
+            return new FiltersAggregator(name, factories, keys, weights, keyed, otherBucketKey, context, parent, pipelineAggregators, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
index 8a9beec..8ed3707 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java
@@ -20,19 +20,16 @@
 package org.elasticsearch.search.aggregations.bucket.filters;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.index.query.ParsedQuery;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 
 /**
@@ -43,12 +40,6 @@ public class FiltersParser implements Aggregator.Parser {
     public static final ParseField FILTERS_FIELD = new ParseField("filters");
     public static final ParseField OTHER_BUCKET_FIELD = new ParseField("other_bucket");
     public static final ParseField OTHER_BUCKET_KEY_FIELD = new ParseField("other_bucket_key");
-    private final IndicesQueriesRegistry queriesRegistry;
-
-    @Inject
-    public FiltersParser(IndicesQueriesRegistry queriesRegistry) {
-        this.queriesRegistry = queriesRegistry;
-    }
 
     @Override
     public String type() {
@@ -56,15 +47,15 @@ public class FiltersParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        List<FiltersAggregator.KeyedFilter> keyedFilters = null;
-        List<QueryBuilder<?>> nonKeyedFilters = null;
+        List<FiltersAggregator.KeyedFilter> filters = new ArrayList<>();
 
         XContentParser.Token token = null;
         String currentFieldName = null;
+        Boolean keyed = null;
         String otherBucketKey = null;
-        Boolean otherBucket = false;
+        boolean otherBucket = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -72,53 +63,50 @@ public class FiltersParser implements Aggregator.Parser {
                 if (context.parseFieldMatcher().match(currentFieldName, OTHER_BUCKET_FIELD)) {
                     otherBucket = parser.booleanValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_STRING) {
                 if (context.parseFieldMatcher().match(currentFieldName, OTHER_BUCKET_KEY_FIELD)) {
                     otherBucketKey = parser.text();
+                    otherBucket = true;
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, FILTERS_FIELD)) {
-                    keyedFilters = new ArrayList<>();
+                    keyed = true;
                     String key = null;
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             key = parser.currentName();
                         } else {
-                            QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                            queryParseContext.reset(parser);
-                            queryParseContext.parseFieldMatcher(context.parseFieldMatcher());
-                            QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                            keyedFilters
-                                    .add(new FiltersAggregator.KeyedFilter(key, filter == null ? QueryBuilders.matchAllQuery() : filter));
+                            ParsedQuery filter = context.indexShard().getQueryShardContext().parseInnerFilter(parser);
+                            filters.add(new FiltersAggregator.KeyedFilter(key, filter == null ? Queries.newMatchAllQuery() : filter.query()));
                         }
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, FILTERS_FIELD)) {
-                    nonKeyedFilters = new ArrayList<>();
+                    keyed = false;
+                    int idx = 0;
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                        queryParseContext.reset(parser);
-                        queryParseContext.parseFieldMatcher(context.parseFieldMatcher());
-                        QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                        nonKeyedFilters.add(filter == null ? QueryBuilders.matchAllQuery() : filter);
+                        ParsedQuery filter = context.indexShard().getQueryShardContext().parseInnerFilter(parser);
+                        filters.add(new FiltersAggregator.KeyedFilter(String.valueOf(idx), filter == null ? Queries.newMatchAllQuery()
+                                : filter.query()));
+                        idx++;
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
 
@@ -126,24 +114,7 @@ public class FiltersParser implements Aggregator.Parser {
             otherBucketKey = "_other_";
         }
 
-        FiltersAggregator.Factory factory;
-        if (keyedFilters != null) {
-            factory = new FiltersAggregator.Factory(aggregationName, keyedFilters);
-        } else {
-            factory = new FiltersAggregator.Factory(aggregationName, nonKeyedFilters.toArray(new QueryBuilder<?>[nonKeyedFilters.size()]));
-        }
-        if (otherBucket != null) {
-            factory.otherBucket(otherBucket);
-        }
-        if (otherBucketKey != null) {
-            factory.otherBucketKey(otherBucketKey);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new FiltersAggregator.Factory(null, Collections.emptyList()) };
+        return new FiltersAggregator.Factory(aggregationName, filters, keyed, otherBucketKey);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
index 7f13a1b..6473b5a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java
@@ -21,134 +21,108 @@ package org.elasticsearch.search.aggregations.bucket.geogrid;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.GeoHashUtils;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortingNumericDocValues;
 import org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregates Geo information into cells determined by geohashes of a given precision.
  * WARNING - for high-precision geohashes it may prove necessary to use a {@link GeoBoundingBoxQueryBuilder}
  * aggregation to focus in on a smaller area to avoid generating too many buckets and using too much RAM
  */
-public class GeoHashGridParser extends GeoPointValuesSourceParser {
-
-    public static final int DEFAULT_PRECISION = 5;
-    public static final int DEFAULT_MAX_NUM_CELLS = 10000;
-
-    public GeoHashGridParser() {
-        super(false, false);
-    }
+public class GeoHashGridParser implements Aggregator.Parser {
 
     @Override
     public String type() {
         return InternalGeoHashGrid.TYPE.name();
     }
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new GeoGridFactory(null) };
-    }
 
     @Override
-    protected GeoGridFactory createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoGridFactory factory = new GeoGridFactory(aggregationName);
-        Integer precision = (Integer) otherOptions.get(GeoHashGridParams.FIELD_PRECISION);
-        if (precision != null) {
-            factory.precision(precision);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoHashGrid.TYPE, context).build();
+
+        int precision = GeoHashGridParams.DEFAULT_PRECISION;
+        int requiredSize = GeoHashGridParams.DEFAULT_MAX_NUM_CELLS;
+        int shardSize = -1;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_NUMBER ||
+                    token == XContentParser.Token.VALUE_STRING) { //Be lenient and also allow numbers enclosed in quotes
+                if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_PRECISION)) {
+                    precision = GeoHashGridParams.checkPrecision(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_SIZE)) {
+                    requiredSize = parser.intValue();
+                } else if (context.parseFieldMatcher().match(currentFieldName, GeoHashGridParams.FIELD_SHARD_SIZE)) {
+                    shardSize = parser.intValue();
                 }
-        Integer size = (Integer) otherOptions.get(GeoHashGridParams.FIELD_SIZE);
-        if (size != null) {
-            factory.size(size);
-        }
-        Integer shardSize = (Integer) otherOptions.get(GeoHashGridParams.FIELD_SHARD_SIZE);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
+            } else if (token != XContentParser.Token.START_OBJECT) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        return factory;
-    }
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_NUMBER || token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_PRECISION)) {
-                otherOptions.put(GeoHashGridParams.FIELD_PRECISION, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_SIZE)) {
-                otherOptions.put(GeoHashGridParams.FIELD_SIZE, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, GeoHashGridParams.FIELD_SHARD_SIZE)) {
-                otherOptions.put(GeoHashGridParams.FIELD_SHARD_SIZE, parser.intValue());
-                return true;
+        if (shardSize == 0) {
+            shardSize = Integer.MAX_VALUE;
         }
+
+        if (requiredSize == 0) {
+            requiredSize = Integer.MAX_VALUE;
         }
-        return false;
+
+        if (shardSize < 0) {
+            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            shardSize = BucketUtils.suggestShardSideQueueSize(requiredSize, context.numberOfShards());
         }
 
-    public static class GeoGridFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoGridFactory> {
+        if (shardSize < requiredSize) {
+            shardSize = requiredSize;
+        }
 
-        private int precision = DEFAULT_PRECISION;
-        private int requiredSize = DEFAULT_MAX_NUM_CELLS;
-        private int shardSize = -1;
+        return new GeoGridFactory(aggregationName, vsParser.config(), precision, requiredSize, shardSize);
 
-        public GeoGridFactory(String name) {
-            super(name, InternalGeoHashGrid.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
     }
 
-        public GeoGridFactory precision(int precision) {
-            this.precision = GeoHashGridParams.checkPrecision(precision);
-            return this;
-        }
-
-        public int precision() {
-            return precision;
-        }
 
-        public GeoGridFactory size(int size) {
-            this.requiredSize = size;
-            return this;
-        }
+    static class GeoGridFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        public int size() {
-            return requiredSize;
-        }
+        private final int precision;
+        private final int requiredSize;
+        private final int shardSize;
 
-        public GeoGridFactory shardSize(int shardSize) {
+        public GeoGridFactory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config, int precision, int requiredSize, int shardSize) {
+            super(name, InternalGeoHashGrid.TYPE.name(), config);
+            this.precision = precision;
+            this.requiredSize = requiredSize;
             this.shardSize = shardSize;
-            return this;
-        }
-
-        public int shardSize() {
-            return shardSize;
         }
 
         @Override
@@ -157,7 +131,6 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
             final InternalAggregation aggregation = new InternalGeoHashGrid(name, requiredSize,
                     Collections.<InternalGeoHashGrid.Bucket> emptyList(), pipelineAggregators, metaData);
             return new NonCollectingAggregator(name, aggregationContext, parent, pipelineAggregators, metaData) {
-                @Override
                 public InternalAggregation buildEmptyAggregation() {
                     return aggregation;
                 }
@@ -168,23 +141,6 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
         protected Aggregator doCreateInternal(final ValuesSource.GeoPoint valuesSource, AggregationContext aggregationContext,
                 Aggregator parent, boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                 throws IOException {
-            if (shardSize == 0) {
-                shardSize = Integer.MAX_VALUE;
-            }
-
-            if (requiredSize == 0) {
-                requiredSize = Integer.MAX_VALUE;
-            }
-
-            if (shardSize < 0) {
-                // Use default heuristic to avoid any wrong-ranking caused by
-                // distributed counting
-                shardSize = BucketUtils.suggestShardSideQueueSize(requiredSize, aggregationContext.searchContext().numberOfShards());
-            }
-
-            if (shardSize < requiredSize) {
-                shardSize = requiredSize;
-            }
             if (collectsFromSingleBucket == false) {
                 return asMultiBucketAggregator(this, aggregationContext, parent);
             }
@@ -194,52 +150,6 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
 
         }
 
-        @Override
-        protected GeoGridFactory innerReadFrom(
-                String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            GeoGridFactory factory = new GeoGridFactory(name);
-            factory.precision = in.readVInt();
-            factory.requiredSize = in.readVInt();
-            factory.shardSize = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(precision);
-            out.writeVInt(requiredSize);
-            out.writeVInt(shardSize);
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(GeoHashGridParams.FIELD_PRECISION.getPreferredName(), precision);
-            builder.field(GeoHashGridParams.FIELD_SIZE.getPreferredName(), requiredSize);
-            builder.field(GeoHashGridParams.FIELD_SHARD_SIZE.getPreferredName(), shardSize);
-            return builder;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            GeoGridFactory other = (GeoGridFactory) obj;
-            if (precision != other.precision) {
-                return false;
-            }
-            if (requiredSize != other.requiredSize) {
-                return false;
-            }
-            if (shardSize != other.shardSize) {
-                return false;
-            }
-            return true;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(precision, requiredSize, shardSize);
-        }
-
         private static class CellValues extends SortingNumericDocValues {
             private MultiGeoPointValues geoValues;
             private int precision;
@@ -297,4 +207,4 @@ public class GeoHashGridParser extends GeoPointValuesSourceParser {
 
         }
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
index c0f4e5f..63f47e1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java
@@ -19,9 +19,6 @@
 package org.elasticsearch.search.aggregations.bucket.global;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -71,10 +68,10 @@ public class GlobalAggregator extends SingleBucketAggregator {
         throw new UnsupportedOperationException("global aggregations cannot serve as sub-aggregations, hence should never be called on #buildEmptyAggregations");
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
         public Factory(String name) {
-            super(name, InternalGlobal.TYPE);
+            super(name, InternalGlobal.TYPE.name());
         }
 
         @Override
@@ -90,32 +87,5 @@ public class GlobalAggregator extends SingleBucketAggregator {
             return new GlobalAggregator(name, factories, context, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected Factory doReadFrom(String name, StreamInput in) throws IOException {
-            return new Factory(name);
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            // Nothing to write
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            return true;
-        }
-
-        @Override
-        protected int doHashCode() {
-            return 0;
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
index 52ab6f0..c70cf0f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalParser.java
@@ -19,9 +19,9 @@
 package org.elasticsearch.search.aggregations.bucket.global;
 
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -36,14 +36,9 @@ public class GlobalParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         parser.nextToken();
         return new GlobalAggregator.Factory(aggregationName);
     }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new GlobalAggregator.Factory(null) };
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
index e4f3712..67caf37 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java
@@ -171,7 +171,7 @@ public class DateHistogramBuilder extends ValuesSourceAggregationBuilder<DateHis
         }
 
         if (extendedBoundsMin != null || extendedBoundsMax != null) {
-            builder.startObject(ExtendedBounds.EXTENDED_BOUNDS_FIELD.getPreferredName());
+            builder.startObject(DateHistogramParser.EXTENDED_BOUNDS.getPreferredName());
             if (extendedBoundsMin != null) {
                 builder.field("min", extendedBoundsMin);
             }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
index ba26041..7e99b38 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramInterval.java
@@ -19,16 +19,10 @@
 
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
 /**
  * The interval the date histogram is based on.
  */
-public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
+public class DateHistogramInterval {
 
     public static final DateHistogramInterval SECOND = new DateHistogramInterval("1s");
     public static final DateHistogramInterval MINUTE = new DateHistogramInterval("1m");
@@ -39,10 +33,6 @@ public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
     public static final DateHistogramInterval QUARTER = new DateHistogramInterval("1q");
     public static final DateHistogramInterval YEAR = new DateHistogramInterval("1y");
 
-    public static final DateHistogramInterval readFromStream(StreamInput in) throws IOException {
-        return SECOND.readFrom(in);
-    }
-
     public static DateHistogramInterval seconds(int sec) {
         return new DateHistogramInterval(sec + "s");
     }
@@ -73,14 +63,4 @@ public class DateHistogramInterval implements Writeable<DateHistogramInterval> {
     public String toString() {
         return expression;
     }
-
-    @Override
-    public DateHistogramInterval readFrom(StreamInput in) throws IOException {
-        return new DateHistogramInterval(in.readString());
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(expression);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
index 1cb58f3..694abf2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
@@ -19,24 +19,55 @@
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.rounding.DateTimeUnit;
 import org.elasticsearch.common.rounding.Rounding;
+import org.elasticsearch.common.rounding.TimeZoneRounding;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.DateHistogramFactory;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
-public class DateHistogramParser extends HistogramParser {
+public class DateHistogramParser implements Aggregator.Parser {
+
+    static final ParseField EXTENDED_BOUNDS = new ParseField("extended_bounds");
+    static final ParseField OFFSET = new ParseField("offset");
+    static final ParseField INTERVAL = new ParseField("interval");
+
+    public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
 
-    public DateHistogramParser() {
-        super(true);
+    static {
+        Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
+        dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
+        dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
+        DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
     }
 
     @Override
@@ -45,47 +76,127 @@ public class DateHistogramParser extends HistogramParser {
     }
 
     @Override
-    protected Object parseStringInterval(String text) {
-        return new DateHistogramInterval(text);
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected DateHistogramFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        HistogramAggregator.DateHistogramFactory factory = new HistogramAggregator.DateHistogramFactory(aggregationName);
-        Object interval = otherOptions.get(Rounding.Interval.INTERVAL_FIELD);
-        if (interval == null) {
-            throw new ParsingException(null, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]");
-        } else if (interval instanceof Long) {
-            factory.interval((Long) interval);
-        } else if (interval instanceof DateHistogramInterval) {
-            factory.dateHistogramInterval((DateHistogramInterval) interval);
-        }
-        Long offset = (Long) otherOptions.get(Rounding.OffsetRounding.OFFSET_FIELD);
-        if (offset != null) {
-            factory.offset(offset);
-        }
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateHistogram.TYPE, context)
+                .targetValueType(ValueType.DATE)
+                .formattable(true)
+                .timezoneAware(true)
+                .build();
 
-        ExtendedBounds extendedBounds = (ExtendedBounds) otherOptions.get(ExtendedBounds.EXTENDED_BOUNDS_FIELD);
-        if (extendedBounds != null) {
-            factory.extendedBounds(extendedBounds);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(HistogramAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+        boolean keyed = false;
+        long minDocCount = 0;
+        ExtendedBounds extendedBounds = null;
+        InternalOrder order = (InternalOrder) Histogram.Order.KEY_ASC;
+        String interval = null;
+        long offset = 0;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if (context.parseFieldMatcher().match(currentFieldName, OFFSET)) {
+                    offset = parseOffset(parser.text());
+                } else if (context.parseFieldMatcher().match(currentFieldName, INTERVAL)) {
+                    interval = parser.text();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) {
+                    minDocCount = parser.longValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if ("order".equals(currentFieldName)) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            String dir = parser.text();
+                            boolean asc = "asc".equals(dir);
+                            order = resolveOrder(currentFieldName, asc);
+                            //TODO should we throw an error if the value is not "asc" or "desc"???
+                        }
+                    }
+                } else if (context.parseFieldMatcher().match(currentFieldName, EXTENDED_BOUNDS)) {
+                    extendedBounds = new ExtendedBounds();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.minAsStr = parser.text();
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.maxAsStr = parser.text();
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.min = parser.longValue();
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.max = parser.longValue();
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        } else {
+                            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                    + currentFieldName + "].", parser.getTokenLocation());
+                        }
+                    }
+
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        Long minDocCount = (Long) otherOptions.get(HistogramAggregator.MIN_DOC_COUNT_FIELD);
-        if (minDocCount != null) {
-            factory.minDocCount(minDocCount);
+
+        if (interval == null) {
+            throw new SearchParseException(context,
+                    "Missing required field [interval] for histogram aggregation [" + aggregationName + "]", parser.getTokenLocation());
         }
-        InternalOrder order = (InternalOrder) otherOptions.get(HistogramAggregator.ORDER_FIELD);
-        if (order != null) {
-            factory.order(order);
+
+        TimeZoneRounding.Builder tzRoundingBuilder;
+        DateTimeUnit dateTimeUnit = DATE_FIELD_UNITS.get(interval);
+        if (dateTimeUnit != null) {
+            tzRoundingBuilder = TimeZoneRounding.builder(dateTimeUnit);
+        } else {
+            // the interval is a time value?
+            tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.parseTimeValue(interval, null, getClass().getSimpleName() + ".interval"));
         }
-        return factory;
+
+        Rounding rounding = tzRoundingBuilder
+                .timeZone(vsParser.input().timezone())
+                .offset(offset).build();
+
+        ValuesSourceConfig config = vsParser.config();
+        return new HistogramAggregator.Factory(aggregationName, config, rounding, order, keyed, minDocCount, extendedBounds,
+                new InternalDateHistogram.Factory());
+
     }
 
-    static InternalOrder resolveOrder(String key, boolean asc) {
+    private static InternalOrder resolveOrder(String key, boolean asc) {
         if ("_key".equals(key) || "_time".equals(key)) {
             return (InternalOrder) (asc ? InternalOrder.KEY_ASC : InternalOrder.KEY_DESC);
         }
@@ -95,17 +206,11 @@ public class DateHistogramParser extends HistogramParser {
         return new InternalOrder.Aggregation(key, asc);
     }
 
-    @Override
-    protected long parseStringOffset(String offset) throws IOException {
+    private long parseOffset(String offset) throws IOException {
         if (offset.charAt(0) == '-') {
             return -TimeValue.parseTimeValue(offset.substring(1), null, getClass().getSimpleName() + ".parseOffset").millis();
         }
         int beginIndex = offset.charAt(0) == '+' ? 1 : 0;
         return TimeValue.parseTimeValue(offset.substring(beginIndex), null, getClass().getSimpleName() + ".parseOffset").millis();
     }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { HistogramAggregator.DateHistogramFactory.PROTOTYPE };
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
index 5a2cd58..c703058 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java
@@ -19,32 +19,19 @@
 
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  *
  */
-public class ExtendedBounds implements ToXContent {
-
-    static final ParseField EXTENDED_BOUNDS_FIELD = new ParseField("extended_bounds");
-    static final ParseField MIN_FIELD = new ParseField("min");
-    static final ParseField MAX_FIELD = new ParseField("max");
-
-    private static final ExtendedBounds PROTOTYPE = new ExtendedBounds();
+public class ExtendedBounds {
 
     Long min;
     Long max;
@@ -54,7 +41,7 @@ public class ExtendedBounds implements ToXContent {
 
     ExtendedBounds() {} //for serialization
 
-    public ExtendedBounds(Long min, Long max) {
+    ExtendedBounds(Long min, Long max) {
         this.min = min;
         this.max = max;
     }
@@ -102,71 +89,4 @@ public class ExtendedBounds implements ToXContent {
         }
         return bounds;
     }
-
-    public ExtendedBounds fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String aggregationName)
-            throws IOException {
-        XContentParser.Token token = null;
-        String currentFieldName = null;
-        ExtendedBounds extendedBounds = new ExtendedBounds();
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                if ("min".equals(currentFieldName)) {
-                    extendedBounds.minAsStr = parser.text();
-                } else if ("max".equals(currentFieldName)) {
-                    extendedBounds.maxAsStr = parser.text();
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown extended_bounds key for a " + token
-                            + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "].");
-                }
-            } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                if (parseFieldMatcher.match(currentFieldName, MIN_FIELD)) {
-                    extendedBounds.min = parser.longValue(true);
-                } else if (parseFieldMatcher.match(currentFieldName, MAX_FIELD)) {
-                    extendedBounds.max = parser.longValue(true);
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown extended_bounds key for a " + token
-                            + " in aggregation [" + aggregationName + "]: [" + currentFieldName + "].");
-                }
-            }
-        }
-        return extendedBounds;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(EXTENDED_BOUNDS_FIELD.getPreferredName());
-        if (min != null) {
-            builder.field(MIN_FIELD.getPreferredName(), min);
-        }
-        if (max != null) {
-            builder.field(MAX_FIELD.getPreferredName(), max);
-        }
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(min, max);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        ExtendedBounds other = (ExtendedBounds) obj;
-        return Objects.equals(min, other.min)
-                && Objects.equals(min, other.min);
-    }
-
-    public static ExtendedBounds parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, String aggregationName)
-            throws IOException {
-        return PROTOTYPE.fromXContent(parser, parseFieldMatcher, aggregationName);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
index 9d57cb2..d2ca0a9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java
@@ -21,18 +21,10 @@ package org.elasticsearch.search.aggregations.bucket.histogram;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.util.CollectionUtil;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.internal.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.rounding.DateTimeUnit;
 import org.elasticsearch.common.rounding.Rounding;
-import org.elasticsearch.common.rounding.TimeZoneRounding;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.LongHash;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -41,29 +33,19 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
-
-import static java.util.Collections.unmodifiableMap;
 
 public class HistogramAggregator extends BucketsAggregator {
 
-    public static final ParseField ORDER_FIELD = new ParseField("order");
-    public static final ParseField KEYED_FIELD = new ParseField("keyed");
-    public static final ParseField MIN_DOC_COUNT_FIELD = new ParseField("min_doc_count");
-
     private final ValuesSource.Numeric valuesSource;
     private final ValueFormatter formatter;
     private final Rounding rounding;
@@ -162,117 +144,48 @@ public class HistogramAggregator extends BucketsAggregator {
         Releasables.close(bucketOrds);
     }
 
-    public static class Factory<AF extends Factory<AF>> extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, Factory<AF>> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric> {
 
-        public static final Factory PROTOTYPE = new Factory("");
-
-        private long interval;
-        private long offset = 0;
-        private InternalOrder order = (InternalOrder) Histogram.Order.KEY_ASC;
-        private boolean keyed = false;
-        private long minDocCount = 0;
-        private ExtendedBounds extendedBounds;
+        private final Rounding rounding;
+        private final InternalOrder order;
+        private final boolean keyed;
+        private final long minDocCount;
+        private final ExtendedBounds extendedBounds;
         private final InternalHistogram.Factory<?> histogramFactory;
 
-        public Factory(String name) {
-            this(name, InternalHistogram.HISTOGRAM_FACTORY);
-        }
-
-        private Factory(String name, InternalHistogram.Factory<?> histogramFactory) {
-            super(name, histogramFactory.type(), ValuesSourceType.NUMERIC, histogramFactory.valueType());
-            this.histogramFactory = histogramFactory;
-        }
-
-        public long interval() {
-            return interval;
-        }
-
-        public AF interval(long interval) {
-            this.interval = interval;
-            return (AF) this;
-        }
-
-        public long offset() {
-            return offset;
-        }
-
-        public AF offset(long offset) {
-            this.offset = offset;
-            return (AF) this;
-        }
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> config,
+                       Rounding rounding, InternalOrder order, boolean keyed, long minDocCount,
+                       ExtendedBounds extendedBounds, InternalHistogram.Factory<?> histogramFactory) {
 
-        public Histogram.Order order() {
-            return order;
-        }
-
-        public AF order(Histogram.Order order) {
-            this.order = (InternalOrder) order;
-            return (AF) this;
-        }
-
-        public boolean keyed() {
-            return keyed;
-        }
-
-        public AF keyed(boolean keyed) {
+            super(name, histogramFactory.type(), config);
+            this.rounding = rounding;
+            this.order = order;
             this.keyed = keyed;
-            return (AF) this;
-        }
-
-        public long minDocCount() {
-            return minDocCount;
-        }
-
-        public AF minDocCount(long minDocCount) {
             this.minDocCount = minDocCount;
-            return (AF) this;
-        }
-
-        public ExtendedBounds extendedBounds() {
-            return extendedBounds;
-        }
-
-        public AF extendedBounds(ExtendedBounds extendedBounds) {
             this.extendedBounds = extendedBounds;
-            return (AF) this;
+            this.histogramFactory = histogramFactory;
         }
 
-        public InternalHistogram.Factory<?> getHistogramFactory() {
-            return histogramFactory;
+        public long minDocCount() {
+            return minDocCount;
         }
 
         @Override
         protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent, List<PipelineAggregator> pipelineAggregators,
                 Map<String, Object> metaData) throws IOException {
-            Rounding rounding = createRounding();
             return new HistogramAggregator(name, factories, rounding, order, keyed, minDocCount, extendedBounds, null, config.formatter(),
                     histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
-        protected Rounding createRounding() {
-            if (interval < 1) {
-                throw new ParsingException(null, "[interval] must be 1 or greater for histogram aggregation [" + name() + "]: " + interval);
-            }
-
-            Rounding rounding = new Rounding.Interval(interval);
-            if (offset != 0) {
-                rounding = new Rounding.OffsetRounding(rounding, offset);
-            }
-            return rounding;
-        }
-
         @Override
         protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                throws IOException {
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
             if (collectsFromSingleBucket == false) {
                 return asMultiBucketAggregator(this, aggregationContext, parent);
             }
-            Rounding rounding = createRounding();
-            // we need to round the bounds given by the user and we have to do it for every aggregator we create
+            // we need to round the bounds given by the user and we have to do it for every aggregator we crate
             // as the rounding is not necessarily an idempotent operation.
-            // todo we need to think of a better structure to the factory/agtor
-            // code so we won't need to do that
+            // todo we need to think of a better structure to the factory/agtor code so we won't need to do that
             ExtendedBounds roundedBounds = null;
             if (extendedBounds != null) {
                 // we need to process & validate here using the parser
@@ -283,206 +196,5 @@ public class HistogramAggregator extends BucketsAggregator {
                     config.formatter(), histogramFactory, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-
-            builder.field(Rounding.Interval.INTERVAL_FIELD.getPreferredName(), interval);
-            builder.field(Rounding.OffsetRounding.OFFSET_FIELD.getPreferredName(), offset);
-
-            if (order != null) {
-                builder.field(ORDER_FIELD.getPreferredName());
-                order.toXContent(builder, params);
-            }
-
-            builder.field(KEYED_FIELD.getPreferredName(), keyed);
-
-            builder.field(MIN_DOC_COUNT_FIELD.getPreferredName(), minDocCount);
-
-            if (extendedBounds != null) {
-                extendedBounds.toXContent(builder, params);
-            }
-
-            return builder;
-        }
-
-        @Override
-        public String getWriteableName() {
-            return InternalHistogram.TYPE.name();
-        }
-
-        @Override
-        protected AF innerReadFrom(String name, ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in)
-                throws IOException {
-            Factory<AF> factory = createFactoryFromStream(name, in);
-            factory.interval = in.readVLong();
-            factory.offset = in.readVLong();
-            if (in.readBoolean()) {
-                factory.order = InternalOrder.Streams.readOrder(in);
-            }
-            factory.keyed = in.readBoolean();
-            factory.minDocCount = in.readVLong();
-            if (in.readBoolean()) {
-                factory.extendedBounds = ExtendedBounds.readFrom(in);
-            }
-            return (AF) factory;
-        }
-
-        protected Factory<AF> createFactoryFromStream(String name, StreamInput in)
-                throws IOException {
-            return new Factory<AF>(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            writeFactoryToStream(out);
-            out.writeVLong(interval);
-            out.writeVLong(offset);
-            boolean hasOrder = order != null;
-            out.writeBoolean(hasOrder);
-            if (hasOrder) {
-                InternalOrder.Streams.writeOrder(order, out);
-            }
-            out.writeBoolean(keyed);
-            out.writeVLong(minDocCount);
-            boolean hasExtendedBounds = extendedBounds != null;
-            out.writeBoolean(hasExtendedBounds);
-            if (hasExtendedBounds) {
-                extendedBounds.writeTo(out);
-            }
-        }
-
-        protected void writeFactoryToStream(StreamOutput out) throws IOException {
-            // Default impl does nothing
-    }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(histogramFactory, interval, offset, order, keyed, minDocCount, extendedBounds);
-    }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(histogramFactory, other.histogramFactory)
-                    && Objects.equals(interval, other.interval)
-                    && Objects.equals(offset, other.offset)
-                    && Objects.equals(order, other.order)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(minDocCount, other.minDocCount)
-                    && Objects.equals(extendedBounds, other.extendedBounds);
-        }
-    }
-
-    public static class DateHistogramFactory extends Factory<DateHistogramFactory> {
-
-        public static final DateHistogramFactory PROTOTYPE = new DateHistogramFactory("");
-        public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
-
-        static {
-            Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
-            dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
-            dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
-            dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
-            dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
-            dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
-            dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
-            dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
-            dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
-            dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
-            dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
-            dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
-            dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
-            dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
-            dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
-            dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
-            dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
-            DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
-        }
-
-        private DateHistogramInterval dateHistogramInterval;
-
-        public DateHistogramFactory(String name) {
-            super(name, InternalDateHistogram.HISTOGRAM_FACTORY);
-        }
-
-        /**
-         * Set the interval.
-         */
-        public DateHistogramFactory dateHistogramInterval(DateHistogramInterval dateHistogramInterval) {
-            this.dateHistogramInterval = dateHistogramInterval;
-            return this;
-        }
-
-        public DateHistogramInterval dateHistogramInterval() {
-            return dateHistogramInterval;
-        }
-
-        @Override
-        protected Rounding createRounding() {
-            TimeZoneRounding.Builder tzRoundingBuilder;
-            DateTimeUnit dateTimeUnit = DATE_FIELD_UNITS.get(dateHistogramInterval.toString());
-            if (dateTimeUnit != null) {
-                tzRoundingBuilder = TimeZoneRounding.builder(dateTimeUnit);
-            } else {
-                // the interval is a time value?
-                tzRoundingBuilder = TimeZoneRounding.builder(TimeValue.parseTimeValue(dateHistogramInterval.toString(), null, getClass()
-                        .getSimpleName() + ".interval"));
-            }
-            if (timeZone() != null) {
-                tzRoundingBuilder.timeZone(timeZone());
-            }
-            Rounding rounding = tzRoundingBuilder.offset(offset()).build();
-            return rounding;
-        }
-
-        @Override
-        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-            return super.createUnmapped(aggregationContext, parent, pipelineAggregators, metaData);
-        }
-
-        @Override
-        protected Aggregator doCreateInternal(Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                throws IOException {
-            return super
-                    .doCreateInternal(valuesSource, aggregationContext, parent, collectsFromSingleBucket, pipelineAggregators, metaData);
-        }
-
-        @Override
-        public String getWriteableName() {
-            return InternalDateHistogram.TYPE.name();
-        }
-
-        @Override
-        protected DateHistogramFactory createFactoryFromStream(String name, StreamInput in)
-                throws IOException {
-            DateHistogramFactory factory = new DateHistogramFactory(name);
-            if (in.readBoolean()) {
-                factory.dateHistogramInterval = DateHistogramInterval.readFromStream(in);
-            }
-            return factory;
-        }
-
-        @Override
-        protected void writeFactoryToStream(StreamOutput out) throws IOException {
-            boolean hasDateInterval = dateHistogramInterval != null;
-            out.writeBoolean(hasDateInterval);
-            if (hasDateInterval) {
-                dateHistogramInterval.writeTo(out);
-            }
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(super.innerHashCode(), dateHistogramInterval);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            DateHistogramFactory other = (DateHistogramFactory) obj;
-            return super.innerEquals(obj)
-                    && Objects.equals(dateHistogramInterval, other.dateHistogramInterval);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
index 0e965a5..064e046 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java
@@ -119,7 +119,7 @@ public class HistogramBuilder extends ValuesSourceAggregationBuilder<HistogramBu
         }
 
         if (extendedBoundsMin != null || extendedBoundsMax != null) {
-            builder.startObject(ExtendedBounds.EXTENDED_BOUNDS_FIELD.getPreferredName());
+            builder.startObject(HistogramParser.EXTENDED_BOUNDS.getPreferredName());
             if (extendedBoundsMin != null) {
                 builder.field("min", extendedBoundsMin);
             }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
index 76e5acc..c738251 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java
@@ -19,31 +19,24 @@
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.rounding.Rounding;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.aggregations.support.format.ValueParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Parses the histogram request
  */
-public class HistogramParser extends NumericValuesSourceParser {
+public class HistogramParser implements Aggregator.Parser {
 
-    public HistogramParser() {
-        super(true, true, false);
-    }
-
-    protected HistogramParser(boolean timezoneAware) {
-        super(true, true, timezoneAware);
-    }
+    static final ParseField EXTENDED_BOUNDS = new ParseField("extended_bounds");
 
     @Override
     public String type() {
@@ -51,105 +44,100 @@ public class HistogramParser extends NumericValuesSourceParser {
     }
 
     @Override
-    protected HistogramAggregator.Factory<?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        HistogramAggregator.Factory factory = new HistogramAggregator.Factory(aggregationName);
-        Long interval = (Long) otherOptions.get(Rounding.Interval.INTERVAL_FIELD);
-        if (interval == null) {
-            throw new ParsingException(null, "Missing required field [interval] for histogram aggregation [" + aggregationName + "]");
-        } else {
-            factory.interval(interval);
-        }
-        Long offset = (Long) otherOptions.get(Rounding.OffsetRounding.OFFSET_FIELD);
-        if (offset != null) {
-            factory.offset(offset);
-        }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-        ExtendedBounds extendedBounds = (ExtendedBounds) otherOptions.get(ExtendedBounds.EXTENDED_BOUNDS_FIELD);
-        if (extendedBounds != null) {
-            factory.extendedBounds(extendedBounds);
-        }
-        Boolean keyed = (Boolean) otherOptions.get(HistogramAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        Long minDocCount = (Long) otherOptions.get(HistogramAggregator.MIN_DOC_COUNT_FIELD);
-        if (minDocCount != null) {
-            factory.minDocCount(minDocCount);
-        }
-        InternalOrder order = (InternalOrder) otherOptions.get(HistogramAggregator.ORDER_FIELD);
-        if (order != null) {
-            factory.order(order);
-        }
-        return factory;
-    }
+        ValuesSourceParser vsParser = ValuesSourceParser.numeric(aggregationName, InternalHistogram.TYPE, context)
+                .targetValueType(ValueType.NUMERIC)
+                .formattable(true)
+                .build();
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions)
-            throws IOException {
-        if (token.isValue()) {
-            if (parseFieldMatcher.match(currentFieldName, Rounding.Interval.INTERVAL_FIELD)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    otherOptions.put(Rounding.Interval.INTERVAL_FIELD, parseStringInterval(parser.text()));
-                    return true;
-                } else {
-                    otherOptions.put(Rounding.Interval.INTERVAL_FIELD, parser.longValue());
-                    return true;
-                }
-            } else if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.MIN_DOC_COUNT_FIELD)) {
-                otherOptions.put(HistogramAggregator.MIN_DOC_COUNT_FIELD, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.KEYED_FIELD)) {
-                otherOptions.put(HistogramAggregator.KEYED_FIELD, parser.booleanValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, Rounding.OffsetRounding.OFFSET_FIELD)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    otherOptions.put(Rounding.OffsetRounding.OFFSET_FIELD, parseStringOffset(parser.text()));
-                    return true;
+        boolean keyed = false;
+        long minDocCount = 0;
+        InternalOrder order = (InternalOrder) InternalOrder.KEY_ASC;
+        long interval = -1;
+        ExtendedBounds extendedBounds = null;
+        long offset = 0;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token.isValue()) {
+                if ("interval".equals(currentFieldName)) {
+                    interval = parser.longValue();
+                } else if ("min_doc_count".equals(currentFieldName) || "minDocCount".equals(currentFieldName)) {
+                    minDocCount = parser.longValue();
+                } else if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else if ("offset".equals(currentFieldName)) {
+                    offset = parser.longValue();
                 } else {
-                    otherOptions.put(Rounding.OffsetRounding.OFFSET_FIELD, parser.longValue());
-                    return true;
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            if (parseFieldMatcher.match(currentFieldName, HistogramAggregator.ORDER_FIELD)) {
-                InternalOrder order = null;
-                while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                    if (token == XContentParser.Token.FIELD_NAME) {
-                        currentFieldName = parser.currentName();
-                    } else if (token == XContentParser.Token.VALUE_STRING) {
-                        String dir = parser.text();
-                        boolean asc = "asc".equals(dir);
-                        if (!asc && !"desc".equals(dir)) {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown order direction in aggregation ["
-                                    + aggregationName + "]: [" + dir
-                                    + "]. Should be either [asc] or [desc]");
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if ("order".equals(currentFieldName)) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token == XContentParser.Token.VALUE_STRING) {
+                            String dir = parser.text();
+                            boolean asc = "asc".equals(dir);
+                            if (!asc && !"desc".equals(dir)) {
+                                throw new SearchParseException(context, "Unknown order direction [" + dir + "] in aggregation ["
+                                        + aggregationName + "]. Should be either [asc] or [desc]", parser.getTokenLocation());
+                            }
+                            order = resolveOrder(currentFieldName, asc);
                         }
-                        order = resolveOrder(currentFieldName, asc);
                     }
+                } else if (context.parseFieldMatcher().match(currentFieldName, EXTENDED_BOUNDS)) {
+                    extendedBounds = new ExtendedBounds();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            currentFieldName = parser.currentName();
+                        } else if (token.isValue()) {
+                            if ("min".equals(currentFieldName)) {
+                                extendedBounds.min = parser.longValue(true);
+                            } else if ("max".equals(currentFieldName)) {
+                                extendedBounds.max = parser.longValue(true);
+                            } else {
+                                throw new SearchParseException(context, "Unknown extended_bounds key for a " + token + " in aggregation ["
+                                        + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                            }
+                        }
+                    }
+
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(HistogramAggregator.ORDER_FIELD, order);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, ExtendedBounds.EXTENDED_BOUNDS_FIELD)) {
-                ExtendedBounds extendedBounds = ExtendedBounds.parse(parser, parseFieldMatcher, aggregationName);
-                otherOptions.put(ExtendedBounds.EXTENDED_BOUNDS_FIELD, extendedBounds);
-                return true;
             } else {
-                return false;
+                throw new SearchParseException(context, "Unexpected token " + token + " in aggregation [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
-        } else {
-            return false;
         }
-    }
 
-    protected Object parseStringInterval(String interval) {
-        return Long.valueOf(interval);
-    }
+        if (interval < 1) {
+            throw new SearchParseException(context,
+                    "Missing required field [interval] for histogram aggregation [" + aggregationName + "]", parser.getTokenLocation());
+        }
+
+        Rounding rounding = new Rounding.Interval(interval);
+        if (offset != 0) {
+            rounding = new Rounding.OffsetRounding((Rounding.Interval) rounding, offset);
+        }
+
+        if (extendedBounds != null) {
+            // with numeric histogram, we can process here and fail fast if the bounds are invalid
+            extendedBounds.processAndValidate(aggregationName, context, ValueParser.RAW);
+        }
+
+        return new HistogramAggregator.Factory(aggregationName, vsParser.config(), rounding, order, keyed, minDocCount, extendedBounds,
+                new InternalHistogram.Factory());
 
-    protected long parseStringOffset(String offset) throws IOException {
-        return Long.valueOf(offset);
     }
 
     static InternalOrder resolveOrder(String key, boolean asc) {
@@ -161,9 +149,4 @@ public class HistogramParser extends NumericValuesSourceParser {
         }
         return new InternalOrder.Aggregation(key, asc);
     }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { HistogramAggregator.Factory.PROTOTYPE };
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
index 9808eed..1651886 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.bucket.histogram;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -31,7 +30,6 @@ import org.joda.time.DateTimeZone;
  */
 public class InternalDateHistogram {
 
-    public static final Factory HISTOGRAM_FACTORY = new Factory();
     final static Type TYPE = new Type("date_histogram", "dhisto");
 
     static class Bucket extends InternalHistogram.Bucket {
@@ -67,13 +65,8 @@ public class InternalDateHistogram {
         }
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType valueType() {
-            return ValueType.DATE;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
index c24ab0d..faca359 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java
@@ -34,7 +34,6 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -52,7 +51,6 @@ import java.util.Map;
 public class InternalHistogram<B extends InternalHistogram.Bucket> extends InternalMultiBucketAggregation<InternalHistogram, B> implements
         Histogram {
 
-    public static final Factory<Bucket> HISTOGRAM_FACTORY = new Factory<Bucket>();
     final static Type TYPE = new Type("histogram", "histo");
 
     private final static AggregationStreams.Stream STREAM = new AggregationStreams.Stream() {
@@ -237,12 +235,8 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
         protected Factory() {
         }
 
-        public Type type() {
-            return TYPE;
-        }
-
-        public ValueType valueType() {
-            return ValueType.NUMERIC;
+        public String type() {
+            return TYPE.name();
         }
 
         public InternalHistogram<B> create(String name, List<B> buckets, InternalOrder order, long minDocCount,
@@ -511,7 +505,7 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
     }
 
     @SuppressWarnings("unchecked")
-    protected static <B extends InternalHistogram.Bucket> Factory<B> resolveFactory(String factoryType) {
+    private static <B extends InternalHistogram.Bucket> Factory<B> resolveFactory(String factoryType) {
         if (factoryType.equals(InternalDateHistogram.TYPE.name())) {
             return (Factory<B>) new InternalDateHistogram.Factory();
         } else if (factoryType.equals(TYPE.name())) {
@@ -523,7 +517,7 @@ public class InternalHistogram<B extends InternalHistogram.Bucket> extends Inter
 
     @Override
     protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(factory.type().name());
+        out.writeString(factory.type());
         InternalOrder.Streams.writeOrder(order, out);
         out.writeVLong(minDocCount);
         if (minDocCount == 0) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
index d19a839..9d503a8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java
@@ -25,7 +25,6 @@ import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation;
 
 import java.io.IOException;
 import java.util.Comparator;
-import java.util.Objects;
 
 /**
  * An internal {@link Histogram.Order} strategy which is identified by a unique id.
@@ -65,25 +64,6 @@ class InternalOrder extends Histogram.Order {
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         return builder.startObject().field(key, asc ? "asc" : "desc").endObject();
     }
-    
-    @Override
-    public int hashCode() {
-        return Objects.hash(id, key, asc);
-    }
-    
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        InternalOrder other = (InternalOrder) obj;
-        return Objects.equals(id, other.id)
-                && Objects.equals(key, other.key)
-                && Objects.equals(asc, other.asc);
-    }
 
     static class Aggregation extends InternalOrder {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
index 99868dd..1ae7341 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java
@@ -20,9 +20,6 @@ package org.elasticsearch.search.aggregations.bucket.missing;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -31,10 +28,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
@@ -85,10 +81,10 @@ public class MissingAggregator extends SingleBucketAggregator {
         return new InternalMissing(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource>  {
 
-        public Factory(String name, ValueType targetValueType) {
-            super(name, InternalMissing.TYPE, ValuesSourceType.ANY, targetValueType);
+        public Factory(String name, ValuesSourceConfig valueSourceConfig) {
+            super(name, InternalMissing.TYPE.name(), valueSourceConfig);
         }
 
         @Override
@@ -102,32 +98,6 @@ public class MissingAggregator extends SingleBucketAggregator {
                 boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
             return new MissingAggregator(name, factories, valuesSource, aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new Factory(name, targetValueType);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
index 640ae52..6ecdc12 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java
@@ -18,22 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.bucket.missing;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
-public class MissingParser extends AnyValuesSourceParser {
-
-    public MissingParser() {
-        super(true, true);
-    }
+/**
+ *
+ */
+public class MissingParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -41,19 +38,25 @@ public class MissingParser extends AnyValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MissingAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MissingAggregator.Factory(aggregationName, targetValueType);
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new MissingAggregator.Factory(null, null) };
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalMissing.TYPE, context)
+                .scriptable(false)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return new MissingAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
index eca5be4..fa23cf8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java
@@ -28,11 +28,7 @@ import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.search.join.BitSetProducer;
 import org.apache.lucene.util.BitSet;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
@@ -49,15 +45,12 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class NestedAggregator extends SingleBucketAggregator {
 
-    static final ParseField PATH_FIELD = new ParseField("path");
-
     private BitSetProducer parentFilter;
     private final Query childFilter;
 
@@ -127,7 +120,7 @@ public class NestedAggregator extends SingleBucketAggregator {
             }
         };
     }
-
+        
     @Override
     public InternalAggregation buildAggregation(long owningBucketOrdinal) throws IOException {
         return new InternalNested(name, bucketDocCount(owningBucketOrdinal), bucketAggregations(owningBucketOrdinal), pipelineAggregators(),
@@ -150,29 +143,15 @@ public class NestedAggregator extends SingleBucketAggregator {
         return null;
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
         private final String path;
 
-        /**
-         * @param name
-         *            the name of this aggregation
-         * @param path
-         *            the path to use for this nested aggregation. The path must
-         *            match the path to a nested object in the mappings.
-         */
         public Factory(String name, String path) {
-            super(name, InternalNested.TYPE);
+            super(name, InternalNested.TYPE.name());
             this.path = path;
         }
 
-        /**
-         * Get the path to use for this nested aggregation.
-         */
-        public String path() {
-            return path;
-        }
-
         @Override
         public Aggregator createInternal(AggregationContext context, Aggregator parent, boolean collectsFromSingleBucket,
                 List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
@@ -189,37 +168,6 @@ public class NestedAggregator extends SingleBucketAggregator {
             return new NestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(PATH_FIELD.getPreferredName(), path);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            String path = in.readString();
-            Factory factory = new Factory(name, path);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeString(path);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(path);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(path, other.path);
-        }
-
         private final static class Unmapped extends NonCollectingAggregator {
 
             public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
index 651b63d..ddf6bf1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java
@@ -18,11 +18,11 @@
  */
 package org.elasticsearch.search.aggregations.bucket.nested;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +37,7 @@ public class NestedParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String path = null;
 
         XContentParser.Token token;
@@ -46,27 +46,24 @@ public class NestedParser implements Aggregator.Parser {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.VALUE_STRING) {
-                if (context.parseFieldMatcher().match(currentFieldName, NestedAggregator.PATH_FIELD)) {
+                if ("path".equals(currentFieldName)) {
                     path = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (path == null) {
             // "field" doesn't exist, so we fall back to the context of the ancestors
-            throw new ParsingException(parser.getTokenLocation(), "Missing [path] field for nested aggregation [" + aggregationName + "]");
+            throw new SearchParseException(context, "Missing [path] field for nested aggregation [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
         return new NestedAggregator.Factory(aggregationName, path);
     }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new NestedAggregator.Factory(null, null) };
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
index a567a62..1b9363c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java
@@ -24,11 +24,7 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.join.BitSetProducer;
 import org.apache.lucene.util.BitSet;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
@@ -46,15 +42,12 @@ import org.elasticsearch.search.aggregations.support.AggregationContext;
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class ReverseNestedAggregator extends SingleBucketAggregator {
 
-    static final ParseField PATH_FIELD = new ParseField("path");
-
     private final Query parentFilter;
     private final BitSetProducer parentBitsetProducer;
 
@@ -125,29 +118,13 @@ public class ReverseNestedAggregator extends SingleBucketAggregator {
         return parentFilter;
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
-
-        private String path;
+    public static class Factory extends AggregatorFactory {
 
-        public Factory(String name) {
-            super(name, InternalReverseNested.TYPE);
-        }
+        private final String path;
 
-        /**
-         * Set the path to use for this nested aggregation. The path must match
-         * the path to a nested object in the mappings. If it is not specified
-         * then this aggregation will go back to the root document.
-         */
-        public Factory path(String path) {
+        public Factory(String name, String path) {
+            super(name, InternalReverseNested.TYPE.name());
             this.path = path;
-            return this;
-        }
-
-        /**
-         * Get the path to use for this nested aggregation.
-         */
-        public String path() {
-            return path;
         }
 
         @Override
@@ -175,39 +152,6 @@ public class ReverseNestedAggregator extends SingleBucketAggregator {
             return new ReverseNestedAggregator(name, factories, objectMapper, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (path != null) {
-                builder.field(PATH_FIELD.getPreferredName(), path);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.path = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(path);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(path);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(path, other.path);
-        }
-
         private final static class Unmapped extends NonCollectingAggregator {
 
             public Unmapped(String name, AggregationContext context, Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
index 6e42e91..80ab9f5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedParser.java
@@ -18,11 +18,11 @@
  */
 package org.elasticsearch.search.aggregations.bucket.nested;
 
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -37,7 +37,7 @@ public class ReverseNestedParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         String path = null;
 
         XContentParser.Token token;
@@ -49,23 +49,15 @@ public class ReverseNestedParser implements Aggregator.Parser {
                 if ("path".equals(currentFieldName)) {
                     path = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
-        ReverseNestedAggregator.Factory factory = new ReverseNestedAggregator.Factory(aggregationName);
-        if (path != null) {
-            factory.path(path);
-        }
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new ReverseNestedAggregator.Factory(null) };
+        return new ReverseNestedAggregator.Factory(aggregationName, path);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
index d96e860..5303d7f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java
@@ -29,8 +29,6 @@ import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -223,16 +221,8 @@ public class InternalRange<B extends InternalRange.Bucket, R extends InternalRan
 
     public static class Factory<B extends Bucket, R extends InternalRange<B, R>> {
 
-        public Type type() {
-            return TYPE;
-        }
-
-        public ValuesSourceType getValueSourceType() {
-            return ValuesSourceType.NUMERIC;
-        }
-
-        public ValueType getValueType() {
-            return ValueType.NUMERIC;
+        public String type() {
+            return TYPE.name();
         }
 
         public R create(String name, List<B> ranges, ValueFormatter formatter, boolean keyed, List<PipelineAggregator> pipelineAggregators,
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
index ae6f3e6..125fca4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
@@ -20,14 +20,6 @@ package org.elasticsearch.search.aggregations.bucket.range;
 
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.InPlaceMergeSorter;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -39,10 +31,9 @@ import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BucketsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueParser;
@@ -52,38 +43,21 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class RangeAggregator extends BucketsAggregator {
 
-    public static final ParseField RANGES_FIELD = new ParseField("ranges");
-    public static final ParseField KEYED_FIELD = new ParseField("keyed");
+    public static class Range {
 
-    public static class Range implements Writeable<Range>, ToXContent {
+        public String key;
+        public double from = Double.NEGATIVE_INFINITY;
+        String fromAsStr;
+        public double to = Double.POSITIVE_INFINITY;
+        String toAsStr;
 
-        public static final Range PROTOTYPE = new Range(null, -1, null, -1, null);
-        public static final ParseField KEY_FIELD = new ParseField("key");
-        public static final ParseField FROM_FIELD = new ParseField("from");
-        public static final ParseField TO_FIELD = new ParseField("to");
-
-        protected String key;
-        protected double from = Double.NEGATIVE_INFINITY;
-        protected String fromAsStr;
-        protected double to = Double.POSITIVE_INFINITY;
-        protected String toAsStr;
-
-        public Range(String key, double from, double to) {
-            this(key, from, null, to, null);
-        }
-
-        public Range(String key, String from, String to) {
-            this(key, Double.NEGATIVE_INFINITY, from, Double.POSITIVE_INFINITY, to);
-        }
-
-        protected Range(String key, double from, String fromAsStr, double to, String toAsStr) {
+        public Range(String key, double from, String fromAsStr, double to, String toAsStr) {
             this.key = key;
             this.from = from;
             this.fromAsStr = fromAsStr;
@@ -109,99 +83,6 @@ public class RangeAggregator extends BucketsAggregator {
                 to = parser.parseDouble(toAsStr, context);
             }
         }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            String fromAsStr = in.readOptionalString();
-            String toAsStr = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            return new Range(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeOptionalString(fromAsStr);
-            out.writeOptionalString(toAsStr);
-            out.writeDouble(from);
-            out.writeDouble(to);
-        }
-
-        public Range fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-
-            XContentParser.Token token;
-            String currentFieldName = null;
-            double from = Double.NEGATIVE_INFINITY;
-            String fromAsStr = null;
-            double to = Double.POSITIVE_INFINITY;
-            String toAsStr = null;
-            String key = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        from = parser.doubleValue();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        to = parser.doubleValue();
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        fromAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        toAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, KEY_FIELD)) {
-                        key = parser.text();
-                    }
-                }
-            }
-            return new Range(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (key != null) {
-                builder.field(KEY_FIELD.getPreferredName(), key);
-            }
-            if (Double.isFinite(from)) {
-                builder.field(FROM_FIELD.getPreferredName(), from);
-            }
-            if (Double.isFinite(to)) {
-                builder.field(TO_FIELD.getPreferredName(), to);
-            }
-            if (fromAsStr != null) {
-                builder.field(FROM_FIELD.getPreferredName(), fromAsStr);
-            }
-            if (toAsStr != null) {
-                builder.field(TO_FIELD.getPreferredName(), toAsStr);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(key, from, fromAsStr, to, toAsStr);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            Range other = (Range) obj;
-            return Objects.equals(key, other.key)
-                    && Objects.equals(from, other.from)
-                    && Objects.equals(fromAsStr, other.fromAsStr)
-                    && Objects.equals(to, other.to)
-                    && Objects.equals(toAsStr, other.toAsStr);
-        }
     }
 
     final ValuesSource.Numeric valuesSource;
@@ -213,7 +94,7 @@ public class RangeAggregator extends BucketsAggregator {
     final double[] maxTo;
 
     public RangeAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource, ValueFormat format,
-            InternalRange.Factory rangeFactory, List<? extends Range> ranges, boolean keyed, AggregationContext aggregationContext,
+            InternalRange.Factory rangeFactory, List<Range> ranges, boolean keyed, AggregationContext aggregationContext,
             Aggregator parent, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
 
         super(name, factories, aggregationContext, parent, pipelineAggregators, metaData);
@@ -364,13 +245,12 @@ public class RangeAggregator extends BucketsAggregator {
 
     public static class Unmapped extends NonCollectingAggregator {
 
-        private final List<? extends RangeAggregator.Range> ranges;
+        private final List<RangeAggregator.Range> ranges;
         private final boolean keyed;
         private final InternalRange.Factory factory;
         private final ValueFormatter formatter;
 
-        public Unmapped(String name, List<? extends RangeAggregator.Range> ranges, boolean keyed, ValueFormat format,
-                AggregationContext context,
+        public Unmapped(String name, List<RangeAggregator.Range> ranges, boolean keyed, ValueFormat format, AggregationContext context,
                 Aggregator parent, InternalRange.Factory factory, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                 throws IOException {
 
@@ -396,29 +276,17 @@ public class RangeAggregator extends BucketsAggregator {
         }
     }
 
-    public static class Factory<AF extends Factory<AF>> extends ValuesSourceAggregatorFactory<ValuesSource.Numeric, AF> {
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.Numeric> {
 
         private final InternalRange.Factory rangeFactory;
-        private final List<? extends Range> ranges;
-        private boolean keyed = false;
-
-        public Factory(String name, List<? extends Range> ranges) {
-            this(name, InternalRange.FACTORY, ranges);
-        }
+        private final List<Range> ranges;
+        private final boolean keyed;
 
-        protected Factory(String name, InternalRange.Factory rangeFactory, List<? extends Range> ranges) {
-            super(name, rangeFactory.type(), rangeFactory.getValueSourceType(), rangeFactory.getValueType());
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valueSourceConfig, InternalRange.Factory rangeFactory, List<Range> ranges, boolean keyed) {
+            super(name, rangeFactory.type(), valueSourceConfig);
             this.rangeFactory = rangeFactory;
             this.ranges = ranges;
-        }
-
-        public AF keyed(boolean keyed) {
             this.keyed = keyed;
-            return (AF) this;
-        }
-
-        public boolean keyed() {
-            return keyed;
         }
 
         @Override
@@ -432,51 +300,6 @@ public class RangeAggregator extends BucketsAggregator {
                 boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
             return new RangeAggregator(name, factories, valuesSource, config.format(), rangeFactory, ranges, keyed, aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(RANGES_FIELD.getPreferredName(), ranges);
-            builder.field(KEYED_FIELD.getPreferredName(), keyed);
-            return builder;
-        }
-
-        @Override
-        protected AF innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            Factory<AF> factory = createFactoryFromStream(name, in);
-            factory.keyed = in.readBoolean();
-            return (AF) factory;
-        }
-
-        protected Factory<AF> createFactoryFromStream(String name, StreamInput in) throws IOException {
-            int size = in.readVInt();
-            List<Range> ranges = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                ranges.add(Range.PROTOTYPE.readFrom(in));
-            }
-            return new Factory<AF>(name, ranges);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(ranges.size());
-            for (Range range : ranges) {
-                range.writeTo(out);
-            }
-            out.writeBoolean(keyed);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(ranges, keyed);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(ranges, other.ranges)
-                    && Objects.equals(keyed, other.keyed);
-        }
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
index 8c8e830..e30b84b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java
@@ -18,34 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class RangeParser extends NumericValuesSourceParser {
-
-    public RangeParser() {
-        this(true, true, false);
-    }
-
-    protected RangeParser(boolean scriptable, boolean formattable, boolean timezoneAware) {
-        super(scriptable, formattable, timezoneAware);
-    }
+public class RangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -53,46 +41,75 @@ public class RangeParser extends NumericValuesSourceParser {
     }
 
     @Override
-    protected RangeAggregator.Factory<?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        List<? extends Range> ranges = (List<? extends Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        RangeAggregator.Factory factory = new RangeAggregator.Factory(aggregationName, ranges);
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.RANGES_FIELD)) {
-                List<Range> ranges = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    Range range = parseRange(parser, parseFieldMatcher);
-                    ranges.add(range);
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalRange.TYPE, context)
+                .formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                }
+                            }
+                        }
+                        ranges.add(new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr));
+                    }
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(RangeAggregator.RANGES_FIELD, ranges);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(RangeAggregator.KEYED_FIELD, keyed);
-                return true;
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    protected Range parseRange(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return Range.PROTOTYPE.fromXContent(parser, parseFieldMatcher);
-    }
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
+        }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new RangeAggregator.Factory(null, Collections.emptyList()) };
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalRange.FACTORY, ranges, keyed);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java
deleted file mode 100644
index aba3e18..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeAggregatorFactory.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.date;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Factory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-public class DateRangeAggregatorFactory extends Factory<DateRangeAggregatorFactory> {
-
-    public DateRangeAggregatorFactory(String name, List<Range> ranges) {
-        super(name, InternalDateRange.FACTORY, ranges);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return InternalDateRange.TYPE.name();
-    }
-
-    @Override
-    protected DateRangeAggregatorFactory createFactoryFromStream(String name, StreamInput in) throws IOException {
-        int size = in.readVInt();
-        List<Range> ranges = new ArrayList<>(size);
-        for (int i = 0; i < size; i++) {
-            ranges.add(Range.PROTOTYPE.readFrom(in));
-        }
-        return new DateRangeAggregatorFactory(name, ranges);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
index 97f9266..940e20a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java
@@ -18,26 +18,24 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.date;
 
-import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
-import java.util.Collections;
+import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class DateRangeParser extends RangeParser {
-
-    public DateRangeParser() {
-        super(true, true, true);
-    }
+public class DateRangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -45,19 +43,78 @@ public class DateRangeParser extends RangeParser {
     }
 
     @Override
-    protected DateRangeAggregatorFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        List<Range> ranges = (List<Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        DateRangeAggregatorFactory factory = new DateRangeAggregatorFactory(aggregationName, ranges);
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalDateRange.TYPE, context)
+                .targetValueType(ValueType.DATE)
+                .formattable(true)
+                .build();
+
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
+                            }
+                        }
+                        ranges.add(new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr));
+                    }
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        return factory;
-    }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new DateRangeAggregatorFactory(null, Collections.emptyList()) };
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
+        }
+
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalDateRange.FACTORY, ranges, keyed);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
index 88568bc..ac2c18e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
@@ -116,13 +115,8 @@ public class InternalDateRange extends InternalRange<InternalDateRange.Bucket, I
     public static class Factory extends InternalRange.Factory<InternalDateRange.Bucket, InternalDateRange> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.DATE;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
index 4aece20..e110cc1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java
@@ -21,230 +21,168 @@ package org.elasticsearch.search.aggregations.bucket.range.geodistance;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoDistance.FixedSourceDistance;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Unmapped;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.aggregations.support.GeoPointParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-public class GeoDistanceParser extends GeoPointValuesSourceParser {
+public class GeoDistanceParser implements Aggregator.Parser {
 
     private static final ParseField ORIGIN_FIELD = new ParseField("origin", "center", "point", "por");
-    private static final ParseField UNIT_FIELD = new ParseField("unit");
-    private static final ParseField DISTANCE_TYPE_FIELD = new ParseField("distance_type");
-
-    private GeoPointParser geoPointParser = new GeoPointParser(InternalGeoDistance.TYPE, ORIGIN_FIELD);
-
-    public GeoDistanceParser() {
-        super(true, false);
-    }
 
     @Override
     public String type() {
         return InternalGeoDistance.TYPE.name();
     }
 
-    public static class Range extends RangeAggregator.Range {
-
-        static final Range PROTOTYPE = new Range(null, -1, -1);
-
-        public Range(String key, double from, double to) {
-            super(key(key, from, to), from, to);
-        }
-
-        private static String key(String key, double from, double to) {
-            if (key != null) {
-                return key;
-            }
-            StringBuilder sb = new StringBuilder();
-            sb.append(from == 0 ? "*" : from);
-            sb.append("-");
-            sb.append(Double.isInfinite(to) ? "*" : to);
-            return sb.toString();
-        }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            return new Range(key, from, to);
+    private static String key(String key, double from, double to) {
+        if (key != null) {
+            return key;
         }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeDouble(from);
-            out.writeDouble(to);
-        }
-
+        StringBuilder sb = new StringBuilder();
+        sb.append(from == 0 ? "*" : from);
+        sb.append("-");
+        sb.append(Double.isInfinite(to) ? "*" : to);
+        return sb.toString();
     }
 
     @Override
-    protected GeoDistanceFactory createFactory(
-            String aggregationName, ValuesSourceType valuesSourceType, ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoPoint origin = (GeoPoint) otherOptions.get(ORIGIN_FIELD);
-        List<Range> ranges = (List<Range>) otherOptions.get(RangeAggregator.RANGES_FIELD);
-        GeoDistanceFactory factory = new GeoDistanceFactory(aggregationName, origin, ranges);
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
-        }
-        DistanceUnit unit = (DistanceUnit) otherOptions.get(UNIT_FIELD);
-        if (unit != null) {
-            factory.unit(unit);
-        }
-        GeoDistance distanceType = (GeoDistance) otherOptions.get(DISTANCE_TYPE_FIELD);
-        if (distanceType != null) {
-            factory.distanceType(distanceType);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (geoPointParser.token(aggregationName, currentFieldName, token, parser, parseFieldMatcher, otherOptions)) {
-            return true;
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, UNIT_FIELD)) {
-                DistanceUnit unit = DistanceUnit.fromString(parser.text());
-                otherOptions.put(UNIT_FIELD, unit);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, DISTANCE_TYPE_FIELD)) {
-                GeoDistance distanceType = GeoDistance.fromString(parser.text());
-                otherOptions.put(DISTANCE_TYPE_FIELD, distanceType);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(RangeAggregator.KEYED_FIELD, keyed);
-                return true;
-            }
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, RangeAggregator.RANGES_FIELD)) {
-                List<Range> ranges = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    String fromAsStr = null;
-                    String toAsStr = null;
-                    double from = 0.0;
-                    double to = Double.POSITIVE_INFINITY;
-                    String key = null;
-                    String toOrFromOrKey = null;
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            toOrFromOrKey = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(toOrFromOrKey, Range.FROM_FIELD)) {
-                                from = parser.doubleValue();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.TO_FIELD)) {
-                                to = parser.doubleValue();
-                            }
-                        } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(toOrFromOrKey, Range.KEY_FIELD)) {
-                                key = parser.text();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.FROM_FIELD)) {
-                                fromAsStr = parser.text();
-                            } else if (parseFieldMatcher.match(toOrFromOrKey, Range.TO_FIELD)) {
-                                toAsStr = parser.text();
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoDistance.TYPE, context).build();
+
+        GeoPointParser geoPointParser = new GeoPointParser(aggregationName, InternalGeoDistance.TYPE, context, ORIGIN_FIELD);
+
+        List<RangeAggregator.Range> ranges = null;
+        DistanceUnit unit = DistanceUnit.DEFAULT;
+        GeoDistance distanceType = GeoDistance.DEFAULT;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (geoPointParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if ("unit".equals(currentFieldName)) {
+                    unit = DistanceUnit.fromString(parser.text());
+                } else if ("distance_type".equals(currentFieldName) || "distanceType".equals(currentFieldName)) {
+                    distanceType = GeoDistance.fromString(parser.text());
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        String fromAsStr = null;
+                        String toAsStr = null;
+                        double from = 0.0;
+                        double to = Double.POSITIVE_INFINITY;
+                        String key = null;
+                        String toOrFromOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("key".equals(toOrFromOrKey)) {
+                                    key = parser.text();
+                                } else if ("from".equals(toOrFromOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrKey)) {
+                                    toAsStr = parser.text();
+                                }
                             }
                         }
+                        ranges.add(new RangeAggregator.Range(key(key, from, to), from, fromAsStr, to, toAsStr));
                     }
-                    if (fromAsStr != null || toAsStr != null) {
-                        ranges.add(new Range(key, Double.parseDouble(fromAsStr), Double.parseDouble(toAsStr)));
-                    } else {
-                        ranges.add(new Range(key, from, to));
-                    }
+                } else  {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                otherOptions.put(RangeAggregator.RANGES_FIELD, ranges);
-                return true;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
-
-    public static class GeoDistanceFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, GeoDistanceFactory> {
 
-        private final GeoPoint origin;
-        private final InternalRange.Factory rangeFactory;
-        private final List<Range> ranges;
-        private DistanceUnit unit = DistanceUnit.DEFAULT;
-        private GeoDistance distanceType = GeoDistance.DEFAULT;
-        private boolean keyed = false;
-
-        public GeoDistanceFactory(String name, GeoPoint origin, List<Range> ranges) {
-            this(name, origin, InternalGeoDistance.FACTORY, ranges);
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in geo_distance aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        private GeoDistanceFactory(String name, GeoPoint origin, InternalRange.Factory rangeFactory, List<Range> ranges) {
-            super(name, rangeFactory.type(), rangeFactory.getValueSourceType(), rangeFactory.getValueType());
-            this.origin = origin;
-            this.rangeFactory = rangeFactory;
-            this.ranges = ranges;
+        GeoPoint origin = geoPointParser.geoPoint();
+        if (origin == null) {
+            throw new SearchParseException(context, "Missing [origin] in geo_distance aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-        @Override
-        public String getWriteableName() {
-            return InternalGeoDistance.TYPE.name();
-        }
+        return new GeoDistanceFactory(aggregationName, vsParser.config(), InternalGeoDistance.FACTORY, origin, unit, distanceType, ranges, keyed);
+    }
 
-        public GeoDistanceFactory unit(DistanceUnit unit) {
-            this.unit = unit;
-            return this;
-        }
+    private static class GeoDistanceFactory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        public DistanceUnit unit() {
-            return unit;
-        }
+        private final GeoPoint origin;
+        private final DistanceUnit unit;
+        private final GeoDistance distanceType;
+        private final InternalRange.Factory rangeFactory;
+        private final List<RangeAggregator.Range> ranges;
+        private final boolean keyed;
 
-        public GeoDistanceFactory distanceType(GeoDistance distanceType) {
+        public GeoDistanceFactory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> valueSourceConfig,
+                                  InternalRange.Factory rangeFactory, GeoPoint origin, DistanceUnit unit, GeoDistance distanceType,
+                                  List<RangeAggregator.Range> ranges, boolean keyed) {
+            super(name, rangeFactory.type(), valueSourceConfig);
+            this.origin = origin;
+            this.unit = unit;
             this.distanceType = distanceType;
-            return this;
-        }
-
-        public GeoDistance distanceType() {
-            return distanceType;
-        }
-
-        public GeoDistanceFactory keyed(boolean keyed) {
+            this.rangeFactory = rangeFactory;
+            this.ranges = ranges;
             this.keyed = keyed;
-            return this;
-        }
-
-        public boolean keyed() {
-            return keyed;
         }
 
         @Override
@@ -265,60 +203,6 @@ public class GeoDistanceParser extends GeoPointValuesSourceParser {
                     pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(ORIGIN_FIELD.getPreferredName(), origin);
-            builder.field(RangeAggregator.RANGES_FIELD.getPreferredName(), ranges);
-            builder.field(RangeAggregator.KEYED_FIELD.getPreferredName(), keyed);
-            builder.field(UNIT_FIELD.getPreferredName(), unit);
-            builder.field(DISTANCE_TYPE_FIELD.getPreferredName(), distanceType);
-            return builder;
-        }
-
-        @Override
-        protected GeoDistanceFactory innerReadFrom(
-                String name, ValuesSourceType valuesSourceType, ValueType targetValueType, StreamInput in) throws IOException {
-            GeoPoint origin = new GeoPoint(in.readDouble(), in.readDouble());
-            int size = in.readVInt();
-            List<Range> ranges = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                ranges.add(Range.PROTOTYPE.readFrom(in));
-            }
-            GeoDistanceFactory factory = new GeoDistanceFactory(name, origin, ranges);
-            factory.keyed = in.readBoolean();
-            factory.distanceType = GeoDistance.readGeoDistanceFrom(in);
-            factory.unit = DistanceUnit.readDistanceUnit(in);
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(origin.lat());
-            out.writeDouble(origin.lon());
-            out.writeVInt(ranges.size());
-            for (Range range : ranges) {
-                range.writeTo(out);
-            }
-            out.writeBoolean(keyed);
-            distanceType.writeTo(out);
-            DistanceUnit.writeDistanceUnit(out, unit);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(origin, ranges, keyed, distanceType, unit);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            GeoDistanceFactory other = (GeoDistanceFactory) obj;
-            return Objects.equals(origin, other.origin)
-                    && Objects.equals(ranges, other.ranges)
-                    && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(distanceType, other.distanceType)
-                    && Objects.equals(unit, other.unit);
-        }
-
         private static class DistanceSource extends ValuesSource.Numeric {
 
             private final ValuesSource.GeoPoint source;
@@ -360,9 +244,4 @@ public class GeoDistanceParser extends GeoPointValuesSourceParser {
 
     }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new GeoDistanceFactory(null, null, Collections.emptyList()) };
-    }
-
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
index 2c16d93..c5c67df 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java
@@ -26,8 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -106,18 +104,8 @@ public class InternalGeoDistance extends InternalRange<InternalGeoDistance.Bucke
     public static class Factory extends InternalRange.Factory<InternalGeoDistance.Bucket, InternalGeoDistance> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValuesSourceType getValueSourceType() {
-            return ValuesSourceType.GEOPOINT;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.GEOPOINT;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorFactory.java
deleted file mode 100644
index ab6747a..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeAggregatorFactory.java
+++ /dev/null
@@ -1,197 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.range.ipv4;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.network.Cidrs;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Factory;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
-
-public class IPv4RangeAggregatorFactory extends Factory<IPv4RangeAggregatorFactory> {
-
-    public IPv4RangeAggregatorFactory(String name, List<Range> ranges) {
-        super(name, InternalIPv4Range.FACTORY, ranges);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return InternalIPv4Range.TYPE.name();
-    }
-
-    @Override
-    protected IPv4RangeAggregatorFactory createFactoryFromStream(String name, StreamInput in) throws IOException {
-        int size = in.readVInt();
-        List<Range> ranges = new ArrayList<>(size);
-        for (int i = 0; i < size; i++) {
-            ranges.add(Range.PROTOTYPE.readFrom(in));
-        }
-        return new IPv4RangeAggregatorFactory(name, ranges);
-    }
-
-    public static class Range extends RangeAggregator.Range {
-
-        static final Range PROTOTYPE = new Range(null, -1, null, -1, null, null);
-        static final ParseField MASK_FIELD = new ParseField("mask");
-
-        private String cidr;
-
-        public Range(String key, double from, double to) {
-            super(key, from, to);
-        }
-
-        public Range(String key, String from, String to) {
-            super(key, from, to);
-        }
-
-        public Range(String key, String cidr) {
-            super(key, -1, null, -1, null);
-            this.cidr = cidr;
-            if (cidr != null) {
-                parseMaskRange();
-            }
-        }
-
-        private Range(String key, double from, String fromAsStr, double to, String toAsStr, String cidr) {
-            super(key, from, fromAsStr, to, toAsStr);
-            this.cidr = cidr;
-            if (cidr != null) {
-                parseMaskRange();
-            }
-        }
-
-        public String mask() {
-            return cidr;
-        }
-
-        private void parseMaskRange() throws IllegalArgumentException {
-            long[] fromTo = Cidrs.cidrMaskToMinMax(cidr);
-            from = fromTo[0] == 0 ? Double.NEGATIVE_INFINITY : fromTo[0];
-            to = fromTo[1] == InternalIPv4Range.MAX_IP ? Double.POSITIVE_INFINITY : fromTo[1];
-            if (key == null) {
-                key = cidr;
-            }
-        }
-
-        @Override
-        public Range fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-
-            XContentParser.Token token;
-            String currentFieldName = null;
-            double from = Double.NEGATIVE_INFINITY;
-            String fromAsStr = null;
-            double to = Double.POSITIVE_INFINITY;
-            String toAsStr = null;
-            String key = null;
-            String cidr = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        from = parser.doubleValue();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        to = parser.doubleValue();
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (parseFieldMatcher.match(currentFieldName, FROM_FIELD)) {
-                        fromAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, TO_FIELD)) {
-                        toAsStr = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, KEY_FIELD)) {
-                        key = parser.text();
-                    } else if (parseFieldMatcher.match(currentFieldName, MASK_FIELD)) {
-                        cidr = parser.text();
-                    }
-                }
-            }
-            return new Range(key, from, fromAsStr, to, toAsStr, cidr);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            if (key != null) {
-                builder.field(KEY_FIELD.getPreferredName(), key);
-            }
-            if (cidr != null) {
-                builder.field(MASK_FIELD.getPreferredName(), cidr);
-            } else {
-                if (Double.isFinite(from)) {
-                    builder.field(FROM_FIELD.getPreferredName(), from);
-                }
-                if (Double.isFinite(to)) {
-                    builder.field(TO_FIELD.getPreferredName(), to);
-                }
-                if (fromAsStr != null) {
-                    builder.field(FROM_FIELD.getPreferredName(), fromAsStr);
-                }
-                if (toAsStr != null) {
-                    builder.field(TO_FIELD.getPreferredName(), toAsStr);
-                }
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public Range readFrom(StreamInput in) throws IOException {
-            String key = in.readOptionalString();
-            String fromAsStr = in.readOptionalString();
-            String toAsStr = in.readOptionalString();
-            double from = in.readDouble();
-            double to = in.readDouble();
-            String mask = in.readOptionalString();
-            return new Range(key, from, fromAsStr, to, toAsStr, mask);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(key);
-            out.writeOptionalString(fromAsStr);
-            out.writeOptionalString(toAsStr);
-            out.writeDouble(from);
-            out.writeDouble(to);
-            out.writeOptionalString(cidr);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(super.hashCode(), cidr);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            return super.equals(obj)
-                    && Objects.equals(cidr, ((Range) obj).cidr);
-        }
-
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
index a6c3ed3..e20d1ac 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java
@@ -26,7 +26,6 @@ import org.elasticsearch.search.aggregations.bucket.BucketStreamContext;
 import org.elasticsearch.search.aggregations.bucket.BucketStreams;
 import org.elasticsearch.search.aggregations.bucket.range.InternalRange;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -112,13 +111,8 @@ public class InternalIPv4Range extends InternalRange<InternalIPv4Range.Bucket, I
     public static class Factory extends InternalRange.Factory<InternalIPv4Range.Bucket, InternalIPv4Range> {
 
         @Override
-        public Type type() {
-            return TYPE;
-        }
-
-        @Override
-        public ValueType getValueType() {
-            return ValueType.IP;
+        public String type() {
+            return TYPE.name();
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
index 5c9af1a..dc1e2a6 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java
@@ -18,29 +18,25 @@
  */
 package org.elasticsearch.search.aggregations.bucket.range.ipv4;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.network.Cidrs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.RangeParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Collections;
+import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class IpRangeParser extends RangeParser {
-
-    public IpRangeParser() {
-        super(true, false, false);
-    }
+public class IpRangeParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -48,26 +44,99 @@ public class IpRangeParser extends RangeParser {
     }
 
     @Override
-    protected Range parseRange(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return IPv4RangeAggregatorFactory.Range.PROTOTYPE.fromXContent(parser, parseFieldMatcher);
-            }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected IPv4RangeAggregatorFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        List<IPv4RangeAggregatorFactory.Range> ranges = (List<IPv4RangeAggregatorFactory.Range>) otherOptions
-                .get(RangeAggregator.RANGES_FIELD);
-        IPv4RangeAggregatorFactory factory = new IPv4RangeAggregatorFactory(aggregationName, ranges);
-        Boolean keyed = (Boolean) otherOptions.get(RangeAggregator.KEYED_FIELD);
-        if (keyed != null) {
-            factory.keyed(keyed);
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalIPv4Range.TYPE, context)
+                .targetValueType(ValueType.IP)
+                .formattable(false)
+                .build();
+
+        List<RangeAggregator.Range> ranges = null;
+        boolean keyed = false;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if ("ranges".equals(currentFieldName)) {
+                    ranges = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double from = Double.NEGATIVE_INFINITY;
+                        String fromAsStr = null;
+                        double to = Double.POSITIVE_INFINITY;
+                        String toAsStr = null;
+                        String key = null;
+                        String mask = null;
+                        String toOrFromOrMaskOrKey = null;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                toOrFromOrMaskOrKey = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if ("from".equals(toOrFromOrMaskOrKey)) {
+                                    from = parser.doubleValue();
+                                } else if ("to".equals(toOrFromOrMaskOrKey)) {
+                                    to = parser.doubleValue();
+                                }
+                            } else if (token == XContentParser.Token.VALUE_STRING) {
+                                if ("from".equals(toOrFromOrMaskOrKey)) {
+                                    fromAsStr = parser.text();
+                                } else if ("to".equals(toOrFromOrMaskOrKey)) {
+                                    toAsStr = parser.text();
+                                } else if ("key".equals(toOrFromOrMaskOrKey)) {
+                                    key = parser.text();
+                                } else if ("mask".equals(toOrFromOrMaskOrKey)) {
+                                    mask = parser.text();
+                                }
+                            }
+                        }
+                        RangeAggregator.Range range = new RangeAggregator.Range(key, from, fromAsStr, to, toAsStr);
+                        if (mask != null) {
+                            parseMaskRange(mask, range, aggregationName, context);
+                        }
+                        ranges.add(range);
+                    }
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("keyed".equals(currentFieldName)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
         }
-        return factory;
+
+        if (ranges == null) {
+            throw new SearchParseException(context, "Missing [ranges] in ranges aggregator [" + aggregationName + "]",
+                    parser.getTokenLocation());
         }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new IPv4RangeAggregatorFactory(null, Collections.emptyList()) };
+        return new RangeAggregator.Factory(aggregationName, vsParser.config(), InternalIPv4Range.FACTORY, ranges, keyed);
+    }
+
+    private static void parseMaskRange(String cidr, RangeAggregator.Range range, String aggregationName, SearchContext ctx) {
+        long[] fromTo;
+        try {
+            fromTo = Cidrs.cidrMaskToMinMax(cidr);
+        } catch (IllegalArgumentException e) {
+            throw new SearchParseException(ctx, "invalid CIDR mask [" + cidr + "] in aggregation [" + aggregationName + "]",
+                    null, e);
+        }
+        range.from = fromTo[0] == 0 ? Double.NEGATIVE_INFINITY : fromTo[0];
+        range.to = fromTo[1] == InternalIPv4Range.MAX_IP ? Double.POSITIVE_INFINITY : fromTo[1];
+        if (range.key == null) {
+            range.key = cidr;
+        }
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerAggregationBuilder.java
deleted file mode 100644
index d68e3ea..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerAggregationBuilder.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
-
-import java.io.IOException;
-
-/**
- * Builder for the {@link Sampler} aggregation.
- */
-public class DiversifiedSamplerAggregationBuilder extends ValuesSourceAggregationBuilder<DiversifiedSamplerAggregationBuilder> {
-
-    private int shardSize = SamplerAggregator.Factory.DEFAULT_SHARD_SAMPLE_SIZE;
-
-    int maxDocsPerValue = SamplerAggregator.DiversifiedFactory.MAX_DOCS_PER_VALUE_DEFAULT;
-    String executionHint = null;
-
-    /**
-     * Sole constructor.
-     */
-    public DiversifiedSamplerAggregationBuilder(String name) {
-        super(name, SamplerAggregator.DiversifiedFactory.TYPE.name());
-    }
-
-    /**
-     * Set the max num docs to be returned from each shard.
-     */
-    public DiversifiedSamplerAggregationBuilder shardSize(int shardSize) {
-        this.shardSize = shardSize;
-        return this;
-    }
-
-    public DiversifiedSamplerAggregationBuilder maxDocsPerValue(int maxDocsPerValue) {
-        this.maxDocsPerValue = maxDocsPerValue;
-        return this;
-    }
-
-    public DiversifiedSamplerAggregationBuilder executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    @Override
-    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
-        if (shardSize != SamplerAggregator.Factory.DEFAULT_SHARD_SAMPLE_SIZE) {
-            builder.field(SamplerAggregator.SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-        }
-
-        if (maxDocsPerValue != SamplerAggregator.DiversifiedFactory.MAX_DOCS_PER_VALUE_DEFAULT) {
-            builder.field(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
-        }
-        if (executionHint != null) {
-            builder.field(SamplerAggregator.EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
-        }
-
-        return builder;
-    }
-
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java
deleted file mode 100644
index 99e9f42..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedSamplerParser.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket.sampler;
-
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- *
- */
-public class DiversifiedSamplerParser extends AnyValuesSourceParser {
-
-    public DiversifiedSamplerParser() {
-        super(true, false);
-    }
-
-    @Override
-    public String type() {
-        return SamplerAggregator.DiversifiedFactory.TYPE.name();
-    }
-
-    @Override
-    protected SamplerAggregator.DiversifiedFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        SamplerAggregator.DiversifiedFactory factory = new SamplerAggregator.DiversifiedFactory(aggregationName, valuesSourceType,
-                targetValueType);
-        Integer shardSize = (Integer) otherOptions.get(SamplerAggregator.SHARD_SIZE_FIELD);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
-        }
-        Integer maxDocsPerValue = (Integer) otherOptions.get(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD);
-        if (maxDocsPerValue != null) {
-            factory.maxDocsPerValue(maxDocsPerValue);
-        }
-        String executionHint = (String) otherOptions.get(SamplerAggregator.EXECUTION_HINT_FIELD);
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_NUMBER) {
-            if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.SHARD_SIZE_FIELD)) {
-                int shardSize = parser.intValue();
-                otherOptions.put(SamplerAggregator.SHARD_SIZE_FIELD, shardSize);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD)) {
-                int maxDocsPerValue = parser.intValue();
-                otherOptions.put(SamplerAggregator.MAX_DOCS_PER_VALUE_FIELD, maxDocsPerValue);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, SamplerAggregator.EXECUTION_HINT_FIELD)) {
-                String executionHint = parser.text();
-                otherOptions.put(SamplerAggregator.EXECUTION_HINT_FIELD, executionHint);
-                return true;
-            }
-        }
-        return false;
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new SamplerAggregator.DiversifiedFactory(null, null, null) };
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
index fb444e6..a623735 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java
@@ -29,7 +29,10 @@ import java.io.IOException;
  */
 public class SamplerAggregationBuilder extends ValuesSourceAggregationBuilder<SamplerAggregationBuilder> {
 
-    private int shardSize = SamplerAggregator.Factory.DEFAULT_SHARD_SAMPLE_SIZE;
+    private int shardSize = SamplerParser.DEFAULT_SHARD_SAMPLE_SIZE;
+
+    int maxDocsPerValue = SamplerParser.MAX_DOCS_PER_VALUE_DEFAULT;
+    String executionHint = null;
 
     /**
      * Sole constructor.
@@ -46,10 +49,28 @@ public class SamplerAggregationBuilder extends ValuesSourceAggregationBuilder<Sa
         return this;
     }
 
+    public SamplerAggregationBuilder maxDocsPerValue(int maxDocsPerValue) {
+        this.maxDocsPerValue = maxDocsPerValue;
+        return this;
+    }
+
+    public SamplerAggregationBuilder executionHint(String executionHint) {
+        this.executionHint = executionHint;
+        return this;
+    }
+
     @Override
     protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
-        if (shardSize != SamplerAggregator.Factory.DEFAULT_SHARD_SAMPLE_SIZE) {
-            builder.field(SamplerAggregator.SHARD_SIZE_FIELD.getPreferredName(), shardSize);
+        // builder.startObject();
+        if (shardSize != SamplerParser.DEFAULT_SHARD_SAMPLE_SIZE) {
+            builder.field(SamplerParser.SHARD_SIZE_FIELD.getPreferredName(), shardSize);
+        }
+
+        if (maxDocsPerValue != SamplerParser.MAX_DOCS_PER_VALUE_DEFAULT) {
+            builder.field(SamplerParser.MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
+        }
+        if (executionHint != null) {
+            builder.field(SamplerParser.EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
         }
 
         return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
index f768e96..8cb9809 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java
@@ -21,16 +21,12 @@ package org.elasticsearch.search.aggregations.bucket.sampler;
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
 import org.elasticsearch.search.aggregations.bucket.BestDocsDeferringCollector;
@@ -38,16 +34,14 @@ import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Aggregate on only the top-scoring docs on a shard.
@@ -61,10 +55,6 @@ import java.util.Objects;
  */
 public class SamplerAggregator extends SingleBucketAggregator {
 
-    public static final ParseField SHARD_SIZE_FIELD = new ParseField("shard_size");
-    public static final ParseField MAX_DOCS_PER_VALUE_FIELD = new ParseField("max_docs_per_value");
-    public static final ParseField EXECUTION_HINT_FIELD = new ParseField("execution_hint");
-
 
     public enum ExecutionMode {
 
@@ -190,29 +180,13 @@ public class SamplerAggregator extends SingleBucketAggregator {
         return new InternalSampler(name, 0, buildEmptySubAggregations(), pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
-
-        public static final int DEFAULT_SHARD_SAMPLE_SIZE = 100;
+    public static class Factory extends AggregatorFactory {
 
-        private int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
-
-        public Factory(String name) {
-            super(name, InternalSampler.TYPE);
-        }
+        private int shardSize;
 
-        /**
-         * Set the max num docs to be returned from each shard.
-         */
-        public Factory shardSize(int shardSize) {
+        public Factory(String name, int shardSize) {
+            super(name, InternalSampler.TYPE.name());
             this.shardSize = shardSize;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned from each shard.
-         */
-        public int shardSize() {
-            return shardSize;
         }
 
         @Override
@@ -221,96 +195,19 @@ public class SamplerAggregator extends SingleBucketAggregator {
             return new SamplerAggregator(name, shardSize, factories, context, parent, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.shardSize = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(shardSize);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(shardSize);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(shardSize, other.shardSize);
-        }
-
     }
 
-    public static class DiversifiedFactory extends ValuesSourceAggregatorFactory<ValuesSource, DiversifiedFactory> {
-
-        public static final Type TYPE = new Type("diversified_sampler");
+    public static class DiversifiedFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
 
-        public static final int MAX_DOCS_PER_VALUE_DEFAULT = 1;
-
-        private int shardSize = Factory.DEFAULT_SHARD_SAMPLE_SIZE;
-        private int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
-        private String executionHint = null;
-
-        public DiversifiedFactory(String name, ValuesSourceType valueSourceType, ValueType valueType) {
-            super(name, TYPE, valueSourceType, valueType);
-        }
+        private int shardSize;
+        private int maxDocsPerValue;
+        private String executionHint;
 
-        /**
-         * Set the max num docs to be returned from each shard.
-         */
-        public DiversifiedFactory shardSize(int shardSize) {
+        public DiversifiedFactory(String name, int shardSize, String executionHint, ValuesSourceConfig vsConfig, int maxDocsPerValue) {
+            super(name, InternalSampler.TYPE.name(), vsConfig);
             this.shardSize = shardSize;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned from each shard.
-         */
-        public int shardSize() {
-            return shardSize;
-        }
-
-        /**
-         * Set the max num docs to be returned per value.
-         */
-        public DiversifiedFactory maxDocsPerValue(int maxDocsPerValue) {
             this.maxDocsPerValue = maxDocsPerValue;
-            return this;
-        }
-
-        /**
-         * Get the max num docs to be returned per value.
-         */
-        public int maxDocsPerValue() {
-            return maxDocsPerValue;
-        }
-
-        /**
-         * Set the execution hint.
-         */
-        public DiversifiedFactory executionHint(String executionHint) {
             this.executionHint = executionHint;
-            return this;
-        }
-
-        /**
-         * Get the execution hint.
-         */
-        public String executionHint() {
-            return executionHint;
         }
 
         @Override
@@ -359,45 +256,6 @@ public class SamplerAggregator extends SingleBucketAggregator {
             };
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(SHARD_SIZE_FIELD.getPreferredName(), shardSize);
-            builder.field(MAX_DOCS_PER_VALUE_FIELD.getPreferredName(), maxDocsPerValue);
-            if (executionHint != null) {
-                builder.field(EXECUTION_HINT_FIELD.getPreferredName(), executionHint);
-            }
-            return builder;
-        }
-
-        @Override
-        protected DiversifiedFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            DiversifiedFactory factory = new DiversifiedFactory(name, valuesSourceType, targetValueType);
-            factory.shardSize = in.readVInt();
-            factory.maxDocsPerValue = in.readVInt();
-            factory.executionHint = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(shardSize);
-            out.writeVInt(maxDocsPerValue);
-            out.writeOptionalString(executionHint);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(shardSize, maxDocsPerValue, executionHint);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            DiversifiedFactory other = (DiversifiedFactory) obj;
-            return Objects.equals(shardSize, other.shardSize)
-                    && Objects.equals(maxDocsPerValue, other.maxDocsPerValue)
-                    && Objects.equals(executionHint, other.executionHint);
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
index 995f368..498a7cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java
@@ -18,12 +18,14 @@
  */
 package org.elasticsearch.search.aggregations.bucket.sampler;
 
-
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -32,44 +34,73 @@ import java.io.IOException;
  */
 public class SamplerParser implements Aggregator.Parser {
 
+    public static final int DEFAULT_SHARD_SAMPLE_SIZE = 100;
+    public static final ParseField SHARD_SIZE_FIELD = new ParseField("shard_size");
+    public static final ParseField MAX_DOCS_PER_VALUE_FIELD = new ParseField("max_docs_per_value");
+    public static final ParseField EXECUTION_HINT_FIELD = new ParseField("execution_hint");
+    public static final boolean DEFAULT_USE_GLOBAL_ORDINALS = false;
+    public static final int MAX_DOCS_PER_VALUE_DEFAULT = 1;
+
+
     @Override
     public String type() {
         return InternalSampler.TYPE.name();
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
 
         XContentParser.Token token;
         String currentFieldName = null;
-        Integer shardSize = null;
+        String executionHint = null;
+        int shardSize = DEFAULT_SHARD_SAMPLE_SIZE;
+        int maxDocsPerValue = MAX_DOCS_PER_VALUE_DEFAULT;
+        ValuesSourceParser vsParser = null;
+        boolean diversityChoiceMade = false;
+
+        vsParser = ValuesSourceParser.any(aggregationName, InternalSampler.TYPE, context).scriptable(true).formattable(false).build();
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
             } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                if (context.parseFieldMatcher().match(currentFieldName, SamplerAggregator.SHARD_SIZE_FIELD)) {
+                if (context.parseFieldMatcher().match(currentFieldName, SHARD_SIZE_FIELD)) {
                     shardSize = parser.intValue();
+                } else if (context.parseFieldMatcher().match(currentFieldName, MAX_DOCS_PER_VALUE_FIELD)) {
+                    diversityChoiceMade = true;
+                    maxDocsPerValue = parser.intValue();
+                } else {
+                    throw new SearchParseException(context, "Unsupported property \"" + currentFieldName + "\" for aggregation \""
+                            + aggregationName, parser.getTokenLocation());
+                }
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                if (context.parseFieldMatcher().match(currentFieldName, EXECUTION_HINT_FIELD)) {
+                    executionHint = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unsupported property \"" + currentFieldName + "\" for aggregation \"" + aggregationName);
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                            parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unsupported property \"" + currentFieldName + "\" for aggregation \"" + aggregationName);
+                throw new SearchParseException(context, "Unsupported property \"" + currentFieldName + "\" for aggregation \""
+                        + aggregationName, parser.getTokenLocation());
             }
         }
 
-        SamplerAggregator.Factory factory = new SamplerAggregator.Factory(aggregationName);
-        if (shardSize != null) {
-            factory.shardSize(shardSize);
+        ValuesSourceConfig vsConfig = vsParser.config();
+        if (vsConfig.valid()) {
+            return new SamplerAggregator.DiversifiedFactory(aggregationName, shardSize, executionHint, vsConfig, maxDocsPerValue);
+        } else {
+            if (diversityChoiceMade) {
+                throw new SearchParseException(context, "Sampler aggregation has " + MAX_DOCS_PER_VALUE_FIELD.getPreferredName()
+                        + " setting but no \"field\" or \"script\" setting to provide values for aggregation \"" + aggregationName + "\"",
+                        parser.getTokenLocation());
+
+            }
+            return new SamplerAggregator.Factory(aggregationName, shardSize);
         }
-        return factory;
     }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new SamplerAggregator.Factory(null) };
-    }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
index 226c6d5..9e8ad33 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java
@@ -228,7 +228,7 @@ public class SignificantLongTerms extends InternalSignificantTerms<SignificantLo
         out.writeVLong(minDocCount);
         out.writeVLong(subsetSize);
         out.writeVLong(supersetSize);
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
+        significanceHeuristic.writeTo(out);
         out.writeVInt(buckets.size());
         for (InternalSignificantTerms.Bucket bucket : buckets) {
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
index b047947..6c1ca0a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java
@@ -214,7 +214,7 @@ public class SignificantStringTerms extends InternalSignificantTerms<Significant
         out.writeVLong(minDocCount);
         out.writeVLong(subsetSize);
         out.writeVLong(supersetSize);
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
+        significanceHeuristic.writeTo(out);
         out.writeVInt(buckets.size());
         for (InternalSignificantTerms.Bucket bucket : buckets) {
             bucket.writeTo(out);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
index 2505a2d..399e857 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java
@@ -26,51 +26,33 @@ import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.lucene.index.FilterableTermsEnum;
 import org.elasticsearch.common.lucene.index.FreqTermsEnum;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.bucket.BucketUtils;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicStreams;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, SignificantTermsAggregatorFactory>
-        implements Releasable {
-
-    static final ParseField BACKGROUND_FILTER = new ParseField("background_filter");
-    static final ParseField HEURISTIC = new ParseField("significance_heuristic");
-
-    static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(
-            3, 0, 10, -1);
+public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> implements Releasable {
 
     public SignificanceHeuristic getSignificanceHeuristic() {
         return significanceHeuristic;
@@ -116,8 +98,9 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
                     List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
                 final IncludeExclude.OrdinalsFilter filter = includeExclude == null ? null : includeExclude.convertToOrdinalsFilter();
                 return new GlobalOrdinalsSignificantTermsAggregator.WithHash(name, factories,
-                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter, aggregationContext, parent,
-                        termsAggregatorFactory, pipelineAggregators, metaData);
+                        (ValuesSource.Bytes.WithOrdinals.FieldData) valuesSource, bucketCountThresholds, filter,
+ aggregationContext,
+                        parent, termsAggregatorFactory, pipelineAggregators, metaData);
             }
         };
 
@@ -146,94 +129,33 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
             return parseField.getPreferredName();
         }
     }
-
-    private IncludeExclude includeExclude = null;
-    private String executionHint = null;
+    private final IncludeExclude includeExclude;
+    private final String executionHint;
     private String indexedFieldName;
     private MappedFieldType fieldType;
     private FilterableTermsEnum termsEnum;
     private int numberOfAggregatorsCreated = 0;
-    private QueryBuilder<?> filterBuilder = null;
-    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
-    private SignificanceHeuristic significanceHeuristic = JLHScore.PROTOTYPE;
+    private final Query filter;
+    private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
+    private final SignificanceHeuristic significanceHeuristic;
 
     protected TermsAggregator.BucketCountThresholds getBucketCountThresholds() {
         return new TermsAggregator.BucketCountThresholds(bucketCountThresholds);
     }
 
-    public SignificantTermsAggregatorFactory(String name, ValuesSourceType valuesSourceType, ValueType valueType) {
-        super(name, SignificantStringTerms.TYPE, valuesSourceType, valueType);
-    }
+    public SignificantTermsAggregatorFactory(String name, ValuesSourceConfig valueSourceConfig, TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude,
+                                             String executionHint, Query filter, SignificanceHeuristic significanceHeuristic) {
 
-    public TermsAggregator.BucketCountThresholds bucketCountThresholds() {
-        return bucketCountThresholds;
-    }
-
-    public SignificantTermsAggregatorFactory bucketCountThresholds(TermsAggregator.BucketCountThresholds bucketCountThresholds) {
+        super(name, SignificantStringTerms.TYPE.name(), valueSourceConfig);
         this.bucketCountThresholds = bucketCountThresholds;
-        return this;
-    }
-
-    /**
-     * Expert: sets an execution hint to the aggregation.
-     */
-    public SignificantTermsAggregatorFactory executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    /**
-     * Expert: gets an execution hint to the aggregation.
-     */
-    public String executionHint() {
-        return executionHint;
-    }
-
-    public SignificantTermsAggregatorFactory backgroundFilter(QueryBuilder<?> filterBuilder) {
-        this.filterBuilder = filterBuilder;
-        return this;
-    }
-
-    public QueryBuilder<?> backgroundFilter() {
-        return filterBuilder;
-    }
-
-    /**
-     * Set terms to include and exclude from the aggregation results
-     */
-    public SignificantTermsAggregatorFactory includeExclude(IncludeExclude includeExclude) {
         this.includeExclude = includeExclude;
-        return this;
-    }
-
-    /**
-     * Get terms to include and exclude from the aggregation results
-     */
-    public IncludeExclude includeExclude() {
-        return includeExclude;
-    }
-
-    public SignificantTermsAggregatorFactory significanceHeuristic(SignificanceHeuristic significanceHeuristic) {
+        this.executionHint = executionHint;
         this.significanceHeuristic = significanceHeuristic;
-        return this;
-    }
-
-    public SignificanceHeuristic significanceHeuristic() {
-        return significanceHeuristic;
-    }
-
-    @Override
-    public void doInit(AggregationContext context) {
-        super.doInit(context);
-        setFieldInfo();
-        significanceHeuristic.initialize(context.searchContext());
-    }
-
-    private void setFieldInfo() {
-        if (!config.unmapped()) {
+        if (!valueSourceConfig.unmapped()) {
             this.indexedFieldName = config.fieldContext().field();
             fieldType = SearchContext.current().smartNameFieldType(indexedFieldName);
         }
+        this.filter = filter;
     }
 
     @Override
@@ -259,18 +181,6 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
         }
 
         numberOfAggregatorsCreated++;
-        BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(this.bucketCountThresholds);
-        if (bucketCountThresholds.getShardSize() == DEFAULT_BUCKET_COUNT_THRESHOLDS.getShardSize()) {
-            //The user has not made a shardSize selection .
-            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
-            //but request double the usual amount.
-            //We typically need more than the number of "top" terms requested by other aggregations
-            //as the significance algorithm is in less of a position to down-select at shard-level -
-            //some of the things we want to find have only one occurrence on each shard and as
-            // such are impossible to differentiate from non-significant terms at that early stage.
-            bucketCountThresholds.setShardSize(2 * BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
-                    aggregationContext.searchContext().numberOfShards()));
-        }
 
         if (valuesSource instanceof ValuesSource.Bytes) {
             ExecutionMode execution = null;
@@ -327,14 +237,6 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
         }
         SearchContext searchContext = context.searchContext();
         IndexReader reader = searchContext.searcher().getIndexReader();
-        Query filter = null;
-        try {
-            if (filterBuilder != null) {
-                filter = filterBuilder.toFilter(context.searchContext().indexShard().getQueryShardContext());
-            }
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to create filter: " + filterBuilder.toString(), e);
-        }
         try {
             if (numberOfAggregatorsCreated == 1) {
                 // Setup a termsEnum for sole use by one aggregator
@@ -379,68 +281,4 @@ public class SignificantTermsAggregatorFactory extends ValuesSourceAggregatorFac
             termsEnum = null;
         }
     }
-
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        bucketCountThresholds.toXContent(builder, params);
-        if (executionHint != null) {
-            builder.field(TermsAggregatorFactory.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
-        }
-        if (filterBuilder != null) {
-            builder.field(BACKGROUND_FILTER.getPreferredName(), filterBuilder);
-        }
-        if (includeExclude != null) {
-            includeExclude.toXContent(builder, params);
-        }
-        significanceHeuristic.toXContent(builder, params);
-        return builder;
-    }
-
-    @Override
-    protected SignificantTermsAggregatorFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        SignificantTermsAggregatorFactory factory = new SignificantTermsAggregatorFactory(name, valuesSourceType, targetValueType);
-        factory.bucketCountThresholds = BucketCountThresholds.readFromStream(in);
-        factory.executionHint = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.filterBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            factory.includeExclude = IncludeExclude.readFromStream(in);
-        }
-        factory.significanceHeuristic = SignificanceHeuristicStreams.read(in);
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        bucketCountThresholds.writeTo(out);
-        out.writeOptionalString(executionHint);
-        boolean hasfilterBuilder = filterBuilder != null;
-        out.writeBoolean(hasfilterBuilder);
-        if (hasfilterBuilder) {
-            out.writeQuery(filterBuilder);
-        }
-        boolean hasIncExc = includeExclude != null;
-        out.writeBoolean(hasIncExc);
-        if (hasIncExc) {
-            includeExclude.writeTo(out);
-        }
-        SignificanceHeuristicStreams.writeTo(significanceHeuristic, out);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(bucketCountThresholds, executionHint, filterBuilder, includeExclude, significanceHeuristic);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        SignificantTermsAggregatorFactory other = (SignificantTermsAggregatorFactory) obj;
-        return Objects.equals(bucketCountThresholds, other.bucketCountThresholds)
-                && Objects.equals(executionHint, other.executionHint)
-                && Objects.equals(filterBuilder, other.filterBuilder)
-                && Objects.equals(includeExclude, other.includeExclude)
-                && Objects.equals(significanceHeuristic, other.significanceHeuristic);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
index 6bbb334..b67ce2a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java
@@ -24,8 +24,8 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.search.aggregations.AggregationBuilder;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicBuilder;
+import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParametersParser;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
 
 import java.io.IOException;
 
@@ -88,7 +88,7 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         bucketCountThresholds.setMinDocCount(minDocCount);
         return this;
     }
-
+    
     /**
      * Set the background filter to compare to. Defaults to the whole index.
      */
@@ -96,7 +96,7 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         this.filterBuilder = filter;
         return this;
     }
-
+    
     /**
      * Expert: set the minimum number of documents that a term should match to
      * be retrieved from a shard.
@@ -138,7 +138,7 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         this.includeFlags = flags;
         return this;
     }
-
+    
     /**
      * Define a set of terms that should be aggregated.
      */
@@ -148,8 +148,8 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         }
         this.includeTerms = terms;
         return this;
-    }
-
+    }    
+    
     /**
      * Define a set of terms that should be aggregated.
      */
@@ -159,16 +159,16 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         }
         this.includeTerms = longsArrToStringArr(terms);
         return this;
-    }
-
+    }     
+    
     private String[] longsArrToStringArr(long[] terms) {
         String[] termsAsString = new String[terms.length];
         for (int i = 0; i < terms.length; i++) {
             termsAsString[i] = Long.toString(terms[i]);
         }
         return termsAsString;
-    }
-
+    }      
+    
 
     /**
      * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
@@ -194,7 +194,7 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         this.excludeFlags = flags;
         return this;
     }
-
+    
     /**
      * Define a set of terms that should not be aggregated.
      */
@@ -204,9 +204,9 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         }
         this.excludeTerms = terms;
         return this;
-    }
-
-
+    }    
+    
+    
     /**
      * Define a set of terms that should not be aggregated.
      */
@@ -224,9 +224,9 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         if (field != null) {
             builder.field("field", field);
         }
-        bucketCountThresholds.toXContent(builder, params);
+        bucketCountThresholds.toXContent(builder);
         if (executionHint != null) {
-            builder.field(TermsAggregatorFactory.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
+            builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
         }
         if (includePattern != null) {
             if (includeFlags == 0) {
@@ -241,7 +241,7 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         if (includeTerms != null) {
             builder.array("include", includeTerms);
         }
-
+        
         if (excludePattern != null) {
             if (excludeFlags == 0) {
                 builder.field("exclude", excludePattern);
@@ -255,10 +255,10 @@ public class SignificantTermsBuilder extends AggregationBuilder<SignificantTerms
         if (excludeTerms != null) {
             builder.array("exclude", excludeTerms);
         }
-
+        
         if (filterBuilder != null) {
-            builder.field(SignificantTermsAggregatorFactory.BACKGROUND_FILTER.getPreferredName());
-            filterBuilder.toXContent(builder, params);
+            builder.field(SignificantTermsParametersParser.BACKGROUND_FILTER.getPreferredName());
+            filterBuilder.toXContent(builder, params); 
         }
         if (significanceHeuristicBuilder != null) {
             significanceHeuristicBuilder.toXContent(builder, params);
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java
new file mode 100644
index 0000000..0202298
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParametersParser.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.significant;
+
+import org.apache.lucene.search.Query;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
+import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParametersParser;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+
+public class SignificantTermsParametersParser extends AbstractTermsParametersParser {
+
+    private static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(3, 0, 10, -1);
+    private final SignificanceHeuristicParserMapper significanceHeuristicParserMapper;
+
+    public SignificantTermsParametersParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper) {
+        this.significanceHeuristicParserMapper = significanceHeuristicParserMapper;
+    }
+
+    public Query getFilter() {
+        return filter;
+    }
+
+    private Query filter = null;
+
+    private SignificanceHeuristic significanceHeuristic;
+
+    @Override
+    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
+    }
+
+    static final ParseField BACKGROUND_FILTER = new ParseField("background_filter");
+
+    @Override
+    public void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException {
+
+        if (token == XContentParser.Token.START_OBJECT) {
+            SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserMapper.get(currentFieldName);
+            if (significanceHeuristicParser != null) {
+                significanceHeuristic = significanceHeuristicParser.parse(parser, context.parseFieldMatcher(), context);
+            } else if (context.parseFieldMatcher().match(currentFieldName, BACKGROUND_FILTER)) {
+                filter = context.indexShard().getQueryShardContext().parseInnerFilter(parser).query();
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else {
+            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName
+                    + "].", parser.getTokenLocation());
+        }
+    }
+
+    public SignificanceHeuristic getSignificanceHeuristic() {
+        return significanceHeuristic;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
index 3579736..28e0fb5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java
@@ -18,41 +18,31 @@
  */
 package org.elasticsearch.search.aggregations.bucket.significant;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.BucketUtils;
+import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParser;
 import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristicParserMapper;
-import org.elasticsearch.search.aggregations.bucket.terms.AbstractTermsParser;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class SignificantTermsParser extends AbstractTermsParser {
+public class SignificantTermsParser implements Aggregator.Parser {
+
     private final SignificanceHeuristicParserMapper significanceHeuristicParserMapper;
-    private final IndicesQueriesRegistry queriesRegistry;
 
     @Inject
-    public SignificantTermsParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper,
-            IndicesQueriesRegistry queriesRegistry) {
+    public SignificantTermsParser(SignificanceHeuristicParserMapper significanceHeuristicParserMapper) {
         this.significanceHeuristicParserMapper = significanceHeuristicParserMapper;
-        this.queriesRegistry = queriesRegistry;
     }
 
     @Override
@@ -61,59 +51,32 @@ public class SignificantTermsParser extends AbstractTermsParser {
     }
 
     @Override
-    protected SignificantTermsAggregatorFactory doCreateFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions) {
-        SignificantTermsAggregatorFactory factory = new SignificantTermsAggregatorFactory(aggregationName, valuesSourceType,
-                targetValueType);
-        if (bucketCountThresholds != null) {
-            factory.bucketCountThresholds(bucketCountThresholds);
-        }
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        if (incExc != null) {
-            factory.includeExclude(incExc);
-        }
-        QueryBuilder<?> backgroundFilter = (QueryBuilder<?>) otherOptions.get(SignificantTermsAggregatorFactory.BACKGROUND_FILTER);
-        if (backgroundFilter != null) {
-            factory.backgroundFilter(backgroundFilter);
-        }
-        SignificanceHeuristic significanceHeuristic = (SignificanceHeuristic) otherOptions.get(SignificantTermsAggregatorFactory.HEURISTIC);
-        if (significanceHeuristic != null) {
-            factory.significanceHeuristic(significanceHeuristic);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        SignificantTermsParametersParser aggParser = new SignificantTermsParametersParser(significanceHeuristicParserMapper);
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, SignificantStringTerms.TYPE, context)
+                .scriptable(false)
+                .formattable(true)
+                .build();
+        IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
+        aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
 
-    @Override
-    public boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Token token,
-            String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_OBJECT) {
-            SignificanceHeuristicParser significanceHeuristicParser = significanceHeuristicParserMapper.get(currentFieldName);
-            if (significanceHeuristicParser != null) {
-                SignificanceHeuristic significanceHeuristic = significanceHeuristicParser.parse(parser, parseFieldMatcher);
-                otherOptions.put(SignificantTermsAggregatorFactory.HEURISTIC, significanceHeuristic);
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SignificantTermsAggregatorFactory.BACKGROUND_FILTER)) {
-                QueryParseContext queryParseContext = new QueryParseContext(queriesRegistry);
-                queryParseContext.reset(parser);
-                queryParseContext.parseFieldMatcher(parseFieldMatcher);
-                QueryBuilder<?> filter = queryParseContext.parseInnerQueryBuilder();
-                otherOptions.put(SignificantTermsAggregatorFactory.BACKGROUND_FILTER, filter);
-                return true;
-            }
+        TermsAggregator.BucketCountThresholds bucketCountThresholds = aggParser.getBucketCountThresholds();
+        if (bucketCountThresholds.getShardSize() == aggParser.getDefaultBucketCountThresholds().getShardSize()) {
+            //The user has not made a shardSize selection .
+            //Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            //but request double the usual amount.
+            //We typically need more than the number of "top" terms requested by other aggregations
+            //as the significance algorithm is in less of a position to down-select at shard-level -
+            //some of the things we want to find have only one occurrence on each shard and as
+            // such are impossible to differentiate from non-significant terms at that early stage.
+            bucketCountThresholds.setShardSize(2 * BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(), context.numberOfShards()));
         }
-        return false;
-    }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new SignificantTermsAggregatorFactory(null, null, null) };
-    }
-
-    @Override
-    protected BucketCountThresholds getDefaultBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(SignificantTermsAggregatorFactory.DEFAULT_BUCKET_COUNT_THRESHOLDS);
+        bucketCountThresholds.ensureValidity();
+        SignificanceHeuristic significanceHeuristic = aggParser.getSignificanceHeuristic();
+        if (significanceHeuristic == null) {
+            significanceHeuristic = JLHScore.INSTANCE;
+        }
+        return new SignificantTermsAggregatorFactory(aggregationName, vsParser.config(), bucketCountThresholds, aggParser.getIncludeExclude(), aggParser.getExecutionHint(), aggParser.getFilter(), significanceHeuristic);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
index bcad058..c7569db 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java
@@ -58,9 +58,9 @@ public class UnmappedSignificantTerms extends InternalSignificantTerms<UnmappedS
     UnmappedSignificantTerms() {} // for serialization
 
     public UnmappedSignificantTerms(String name, int requiredSize, long minDocCount, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) {
-        //We pass zero for index/subset sizes because for the purpose of significant term analysis
-        // we assume an unmapped index's size is irrelevant to the proceedings.
-        super(0, 0, name, requiredSize, minDocCount, JLHScore.PROTOTYPE, BUCKETS, pipelineAggregators, metaData);
+        //We pass zero for index/subset sizes because for the purpose of significant term analysis 
+        // we assume an unmapped index's size is irrelevant to the proceedings. 
+        super(0, 0, name, requiredSize, minDocCount, JLHScore.INSTANCE, BUCKETS, pipelineAggregators, metaData);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
index c68e47a..cc9303a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ChiSquare.java
@@ -23,14 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 public class ChiSquare extends NXYSignificanceHeuristic {
 
-    static final ChiSquare PROTOTYPE = new ChiSquare(false, false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("chi_square");
 
     public ChiSquare(boolean includeNegatives, boolean backgroundIsSuperset) {
@@ -52,6 +51,18 @@ public class ChiSquare extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new ChiSquare(in.readBoolean(), in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates Chi^2
      * see "Information Retrieval", Manning et al., Eq. 13.19
@@ -69,21 +80,9 @@ public class ChiSquare extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new ChiSquare(in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        super.build(builder);
-        builder.endObject();
-        return builder;
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+        super.writeTo(out);
     }
 
     public static class ChiSquareParser extends NXYParser {
@@ -107,7 +106,7 @@ public class ChiSquare extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             super.build(builder);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
index 6da05ed..99ee7c7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
@@ -29,13 +29,12 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class GND extends NXYSignificanceHeuristic {
 
-    static final GND PROTOTYPE = new GND(false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("gnd");
 
     public GND(boolean backgroundIsSuperset) {
@@ -58,6 +57,18 @@ public class GND extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new GND(in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates Google Normalized Distance, as described in "The Google Similarity Distance", Cilibrasi and Vitanyi, 2007
      * link: http://arxiv.org/pdf/cs/0412098v3.pdf
@@ -87,28 +98,11 @@ public class GND extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new GND(in.readBoolean());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
         out.writeBoolean(backgroundIsSuperset);
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        builder.field(BACKGROUND_IS_SUPERSET.getPreferredName(), backgroundIsSuperset);
-        builder.endObject();
-        return builder;
-    }
-
     public static class GNDParser extends NXYParser {
 
         @Override
@@ -122,7 +116,7 @@ public class GND extends NXYSignificanceHeuristic {
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String givenName = parser.currentName();
             boolean backgroundIsSuperset = true;
@@ -149,7 +143,7 @@ public class GND extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             builder.field(BACKGROUND_IS_SUPERSET.getPreferredName(), backgroundIsSuperset);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
index 753c9cc..97264e7 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
@@ -22,42 +22,38 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class JLHScore extends SignificanceHeuristic {
 
-    public static final JLHScore PROTOTYPE = new JLHScore();
+    public static final JLHScore INSTANCE = new JLHScore();
 
-    protected static final ParseField NAMES_FIELD = new ParseField("jlh");
+    protected static final String[] NAMES = {"jlh"};
 
     private JLHScore() {}
 
-    @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return PROTOTYPE;
-    }
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return readFrom(in);
+        }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
+        @Override
+        public String getName() {
+            return NAMES[0];
+        }
+    };
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-        return builder;
+    public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        return INSTANCE;
     }
 
     /**
@@ -105,21 +101,26 @@ public class JLHScore extends SignificanceHeuristic {
         return absoluteProbabilityChange * relativeProbabilityChange;
     }
 
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+    }
+
     public static class JLHScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [jhl] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
             }
-            return PROTOTYPE;
+            return new JLHScore();
         }
 
         @Override
         public String[] getNames() {
-            return NAMES_FIELD.getAllNamesIncludedDeprecated();
+            return NAMES;
         }
     }
 
@@ -127,7 +128,7 @@ public class JLHScore extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+            builder.startObject(STREAM.getName()).endObject();
             return builder;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
index d20b5f3..b4529b8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/MutualInformation.java
@@ -23,14 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 public class MutualInformation extends NXYSignificanceHeuristic {
 
-    static final MutualInformation PROTOTYPE = new MutualInformation(false, false);
-
     protected static final ParseField NAMES_FIELD = new ParseField("mutual_information");
 
     private static final double log2 = Math.log(2.0);
@@ -54,6 +53,18 @@ public class MutualInformation extends NXYSignificanceHeuristic {
         return result;
     }
 
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return new MutualInformation(in.readBoolean(), in.readBoolean());
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
     /**
      * Calculates mutual information
      * see "Information Retrieval", Manning et al., Eq. 13.17
@@ -102,21 +113,9 @@ public class MutualInformation extends NXYSignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return new MutualInformation(in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        super.build(builder);
-        builder.endObject();
-        return builder;
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+        super.writeTo(out);
     }
 
     public static class MutualInformationParser extends NXYParser {
@@ -140,7 +139,7 @@ public class MutualInformation extends NXYSignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             super.build(builder);
             builder.endObject();
             return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
index d3f98f2..c6a6924 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
@@ -135,15 +136,10 @@ public abstract class NXYSignificanceHeuristic extends SignificanceHeuristic {
         }
     }
 
-    protected void build(XContentBuilder builder) throws IOException {
-        builder.field(INCLUDE_NEGATIVES_FIELD.getPreferredName(), includeNegatives).field(BACKGROUND_IS_SUPERSET.getPreferredName(),
-                backgroundIsSuperset);
-    }
-
     public static abstract class NXYParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String givenName = parser.currentName();
             boolean includeNegatives = false;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
index e6cfe9a..aceae8c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
@@ -22,42 +22,38 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public class PercentageScore extends SignificanceHeuristic {
 
-    public static final PercentageScore PROTOTYPE = new PercentageScore();
+    public static final PercentageScore INSTANCE = new PercentageScore();
 
-    protected static final ParseField NAMES_FIELD = new ParseField("percentage");
+    protected static final String[] NAMES = {"percentage"};
 
     private PercentageScore() {}
 
-    @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        return PROTOTYPE;
-    }
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            return readFrom(in);
+        }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
+        @Override
+        public String getName() {
+            return NAMES[0];
+        }
+    };
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-        return builder;
+    public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        return INSTANCE;
     }
 
     /**
@@ -74,21 +70,26 @@ public class PercentageScore extends SignificanceHeuristic {
         return (double) subsetFreq / (double) supersetFreq;
    }
 
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
+    }
+
     public static class PercentageScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [percentage] significance heuristic. expected an empty object, but got [{}] instead", parser.currentToken());
             }
-            return PROTOTYPE;
+            return new PercentageScore();
         }
 
         @Override
         public String[] getNames() {
-            return NAMES_FIELD.getAllNamesIncludedDeprecated();
+            return NAMES;
         }
     }
 
@@ -96,7 +97,7 @@ public class PercentageScore extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+            builder.startObject(STREAM.getName()).endObject();
             return builder;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
index 2956a2d..9efea00 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
@@ -22,7 +22,6 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.HasContextAndHeaders;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -45,12 +44,9 @@ import java.io.IOException;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Objects;
 
 public class ScriptHeuristic extends SignificanceHeuristic {
 
-    static final ScriptHeuristic PROTOTYPE = new ScriptHeuristic(null);
-
     protected static final ParseField NAMES_FIELD = new ParseField("script_heuristic");
     private final LongAccessor subsetSizeHolder;
     private final LongAccessor supersetSizeHolder;
@@ -59,11 +55,31 @@ public class ScriptHeuristic extends SignificanceHeuristic {
     ExecutableScript searchScript = null;
     Script script;
 
-    public ScriptHeuristic(Script script) {
+    public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+        @Override
+        public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+            Script script = Script.readScript(in);
+            return new ScriptHeuristic(null, script);
+        }
+
+        @Override
+        public String getName() {
+            return NAMES_FIELD.getPreferredName();
+        }
+    };
+
+    public ScriptHeuristic(ExecutableScript searchScript, Script script) {
         subsetSizeHolder = new LongAccessor();
         supersetSizeHolder = new LongAccessor();
         subsetDfHolder = new LongAccessor();
         supersetDfHolder = new LongAccessor();
+        this.searchScript = searchScript;
+        if (searchScript != null) {
+            searchScript.setNextVar("_subset_freq", subsetDfHolder);
+            searchScript.setNextVar("_subset_size", subsetSizeHolder);
+            searchScript.setNextVar("_superset_freq", supersetDfHolder);
+            searchScript.setNextVar("_superset_size", supersetSizeHolder);
+        }
         this.script = script;
 
 
@@ -71,16 +87,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
     @Override
     public void initialize(InternalAggregation.ReduceContext context) {
-        initialize(context.scriptService(), context);
-    }
-
-    @Override
-    public void initialize(SearchContext context) {
-        initialize(context.scriptService(), context);
-    }
-
-    public void initialize(ScriptService scriptService, HasContextAndHeaders hasContextAndHeaders) {
-        searchScript = scriptService.executable(script, ScriptContext.Standard.AGGS, hasContextAndHeaders, Collections.emptyMap());
+        searchScript = context.scriptService().executable(script, ScriptContext.Standard.AGGS, context, Collections.emptyMap());
         searchScript.setNextVar("_subset_freq", subsetDfHolder);
         searchScript.setNextVar("_subset_size", subsetSizeHolder);
         searchScript.setNextVar("_superset_freq", supersetDfHolder);
@@ -114,54 +121,20 @@ public class ScriptHeuristic extends SignificanceHeuristic {
     }
 
     @Override
-    public String getWriteableName() {
-        return NAMES_FIELD.getPreferredName();
-    }
-
-    @Override
-    public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
-        Script script = Script.readScript(in);
-        return new ScriptHeuristic(script);
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
+        out.writeString(STREAM.getName());
         script.writeTo(out);
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.startObject(NAMES_FIELD.getPreferredName());
-        builder.field(ScriptField.SCRIPT.getPreferredName());
-        script.toXContent(builder, builderParams);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(script);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        ScriptHeuristic other = (ScriptHeuristic) obj;
-        return Objects.equals(script, other.script);
-    }
-
     public static class ScriptHeuristicParser implements SignificanceHeuristicParser {
+        private final ScriptService scriptService;
 
-        public ScriptHeuristicParser() {
+        public ScriptHeuristicParser(ScriptService scriptService) {
+            this.scriptService = scriptService;
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                 throws IOException, QueryShardException {
             String heuristicName = parser.currentName();
             Script script = null;
@@ -200,7 +173,13 @@ public class ScriptHeuristic extends SignificanceHeuristic {
             if (script == null) {
                 throw new ElasticsearchParseException("failed to parse [{}] significance heuristic. no script found in script_heuristic", heuristicName);
             }
-            return new ScriptHeuristic(script);
+            ExecutableScript searchScript;
+            try {
+                searchScript = scriptService.executable(script, ScriptContext.Standard.AGGS, context, Collections.emptyMap());
+            } catch (Exception e) {
+                throw new ElasticsearchParseException("failed to parse [{}] significance heuristic. the script [{}] could not be loaded", e, script, heuristicName);
+            }
+            return new ScriptHeuristic(searchScript, script);
         }
 
         @Override
@@ -220,7 +199,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName());
+            builder.startObject(STREAM.getName());
             builder.field(ScriptField.SCRIPT.getPreferredName());
             script.toXContent(builder, builderParams);
             builder.endObject();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
index 972696b..4f12277 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristic.java
@@ -20,12 +20,12 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.internal.SearchContext;
 
-public abstract class SignificanceHeuristic implements NamedWriteable<SignificanceHeuristic>, ToXContent {
+import java.io.IOException;
+
+public abstract class SignificanceHeuristic {
     /**
      * @param subsetFreq   The frequency of the term in the selected sample
      * @param subsetSize   The size of the selected sample (typically number of docs)
@@ -35,6 +35,8 @@ public abstract class SignificanceHeuristic implements NamedWriteable<Significan
      */
     public abstract double getScore(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize);
 
+    abstract public void writeTo(StreamOutput out) throws IOException;
+
     protected void checkFrequencyValidity(long subsetFreq, long subsetSize, long supersetFreq, long supersetSize, String scoreFunctionName) {
         if (subsetFreq < 0 || subsetSize < 0 || supersetFreq < 0 || supersetSize < 0) {
             throw new IllegalArgumentException("Frequencies of subset and superset must be positive in " + scoreFunctionName + ".getScore()");
@@ -50,8 +52,4 @@ public abstract class SignificanceHeuristic implements NamedWriteable<Significan
     public void initialize(InternalAggregation.ReduceContext reduceContext) {
 
     }
-
-    public void initialize(SearchContext context) {
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
index aa430dc..92baa43 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParser.java
@@ -23,12 +23,13 @@ package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
 public interface SignificanceHeuristicParser {
 
-    SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException,
+    SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context) throws IOException,
             ParsingException;
 
     String[] getNames();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
index a9b385c..07e74ec 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicParserMapper.java
@@ -21,6 +21,8 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.script.ScriptService;
+
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
@@ -31,14 +33,14 @@ public class SignificanceHeuristicParserMapper {
     protected final Map<String, SignificanceHeuristicParser> significanceHeuristicParsers;
 
     @Inject
-    public SignificanceHeuristicParserMapper(Set<SignificanceHeuristicParser> parsers) {
+    public SignificanceHeuristicParserMapper(Set<SignificanceHeuristicParser> parsers, ScriptService scriptService) {
         Map<String, SignificanceHeuristicParser> map = new HashMap<>();
         add(map, new JLHScore.JLHScoreParser());
         add(map, new PercentageScore.PercentageScoreParser());
         add(map, new MutualInformation.MutualInformationParser());
         add(map, new ChiSquare.ChiSquareParser());
         add(map, new GND.GNDParser());
-        add(map, new ScriptHeuristic.ScriptHeuristicParser());
+        add(map, new ScriptHeuristic.ScriptHeuristicParser(scriptService));
         for (SignificanceHeuristicParser parser : parsers) {
             add(map, parser);
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
index 2ffe5ec..198f129 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/SignificanceHeuristicStreams.java
@@ -19,7 +19,6 @@
 package org.elasticsearch.search.aggregations.bucket.significant.heuristics;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 
 import java.io.IOException;
 import java.util.Collections;
@@ -32,26 +31,21 @@ import java.util.Map;
  */
 public class SignificanceHeuristicStreams {
 
-    private static Map<String, SignificanceHeuristic> STREAMS = Collections.emptyMap();
+    private static Map<String, Stream> STREAMS = Collections.emptyMap();
 
     static {
-        HashMap<String, SignificanceHeuristic> map = new HashMap<>();
-        map.put(JLHScore.NAMES_FIELD.getPreferredName(), JLHScore.PROTOTYPE);
-        map.put(PercentageScore.NAMES_FIELD.getPreferredName(), PercentageScore.PROTOTYPE);
-        map.put(MutualInformation.NAMES_FIELD.getPreferredName(), MutualInformation.PROTOTYPE);
-        map.put(GND.NAMES_FIELD.getPreferredName(), GND.PROTOTYPE);
-        map.put(ChiSquare.NAMES_FIELD.getPreferredName(), ChiSquare.PROTOTYPE);
-        map.put(ScriptHeuristic.NAMES_FIELD.getPreferredName(), ScriptHeuristic.PROTOTYPE);
+        HashMap<String, Stream> map = new HashMap<>();
+        map.put(JLHScore.STREAM.getName(), JLHScore.STREAM);
+        map.put(PercentageScore.STREAM.getName(), PercentageScore.STREAM);
+        map.put(MutualInformation.STREAM.getName(), MutualInformation.STREAM);
+        map.put(GND.STREAM.getName(), GND.STREAM);
+        map.put(ChiSquare.STREAM.getName(), ChiSquare.STREAM);
+        map.put(ScriptHeuristic.STREAM.getName(), ScriptHeuristic.STREAM);
         STREAMS = Collections.unmodifiableMap(map);
     }
 
     public static SignificanceHeuristic read(StreamInput in) throws IOException {
-        return stream(in.readString()).readFrom(in);
-    }
-
-    public static void writeTo(SignificanceHeuristic significanceHeuristic, StreamOutput out) throws IOException {
-        out.writeString(significanceHeuristic.getWriteableName());
-        significanceHeuristic.writeTo(out);
+        return stream(in.readString()).readResult(in);
     }
 
     /**
@@ -65,18 +59,17 @@ public class SignificanceHeuristicStreams {
     }
 
     /**
-     * Registers the given prototype.
+     * Registers the given stream and associate it with the given types.
      *
-     * @param prototype
-     *            The prototype to register
+     * @param stream The stream to register
      */
-    public static synchronized void registerPrototype(SignificanceHeuristic prototype) {
-        if (STREAMS.containsKey(prototype.getWriteableName())) {
-            throw new IllegalArgumentException("Can't register stream with name [" + prototype.getWriteableName() + "] more than once");
+    public static synchronized void registerStream(Stream stream) {
+        if (STREAMS.containsKey(stream.getName())) {
+            throw new IllegalArgumentException("Can't register stream with name [" + stream.getName() + "] more than once");
         }
-        HashMap<String, SignificanceHeuristic> map = new HashMap<>();
+        HashMap<String, Stream> map = new HashMap<>();
         map.putAll(STREAMS);
-        map.put(prototype.getWriteableName(), prototype);
+        map.put(stream.getName(), stream);
         STREAMS = Collections.unmodifiableMap(map);
     }
 
@@ -86,7 +79,7 @@ public class SignificanceHeuristicStreams {
      * @param name The given name
      * @return The associated stream
      */
-    private static synchronized SignificanceHeuristic stream(String name) {
+    private static synchronized Stream stream(String name) {
         return STREAMS.get(name);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
new file mode 100644
index 0000000..891526c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParametersParser.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+public abstract class AbstractTermsParametersParser {
+
+    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
+    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
+    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
+    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
+    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
+    public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
+    
+
+    //These are the results of the parsing.
+    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds();
+
+    private String executionHint = null;
+    
+    private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
+
+
+    IncludeExclude includeExclude;
+
+    public TermsAggregator.BucketCountThresholds getBucketCountThresholds() {return bucketCountThresholds;}
+
+    //These are the results of the parsing.
+
+    public String getExecutionHint() {
+        return executionHint;
+    }
+
+    public IncludeExclude getIncludeExclude() {
+        return includeExclude;
+    }
+    
+    public SubAggCollectionMode getCollectionMode() {
+        return collectMode;
+    }
+
+    public void parse(String aggregationName, XContentParser parser, SearchContext context, ValuesSourceParser vsParser, IncludeExclude.Parser incExcParser) throws IOException {
+        bucketCountThresholds = getDefaultBucketCountThresholds();
+        XContentParser.Token token;
+        String currentFieldName = null;
+
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (incExcParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                if (context.parseFieldMatcher().match(currentFieldName, EXECUTION_HINT_FIELD_NAME)) {
+                    executionHint = parser.text();
+                } else if(context.parseFieldMatcher().match(currentFieldName, SubAggCollectionMode.KEY)){
+                    collectMode = SubAggCollectionMode.parse(parser.text(), context.parseFieldMatcher());
+                } else if (context.parseFieldMatcher().match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setRequiredSize(parser.intValue());
+                } else {
+                    parseSpecial(aggregationName, parser, context, token, currentFieldName);
+                }
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if (context.parseFieldMatcher().match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setRequiredSize(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, SHARD_SIZE_FIELD_NAME)) {
+                    bucketCountThresholds.setShardSize(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, MIN_DOC_COUNT_FIELD_NAME)) {
+                    bucketCountThresholds.setMinDocCount(parser.intValue());
+                } else if (context.parseFieldMatcher().match(currentFieldName, SHARD_MIN_DOC_COUNT_FIELD_NAME)) {
+                    bucketCountThresholds.setShardMinDocCount(parser.longValue());
+                } else {
+                    parseSpecial(aggregationName, parser, context, token, currentFieldName);
+                }
+            } else {
+                parseSpecial(aggregationName, parser, context, token, currentFieldName);
+            }
+        }
+        includeExclude = incExcParser.includeExclude();
+    }
+
+    public abstract void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException;
+
+    protected abstract TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds();
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java
deleted file mode 100644
index 5097dd7..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractTermsParser.java
+++ /dev/null
@@ -1,130 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.terms;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
-
-public abstract class AbstractTermsParser extends AnyValuesSourceParser {
-
-    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
-    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
-    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
-    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
-    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
-
-    public IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
-
-    protected AbstractTermsParser() {
-        super(true, true);
-    }
-
-    @Override
-    protected final ValuesSourceAggregatorFactory<ValuesSource, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        BucketCountThresholds bucketCountThresholds = getDefaultBucketCountThresholds();
-        Integer requiredSize = (Integer) otherOptions.get(REQUIRED_SIZE_FIELD_NAME);
-        if (requiredSize != null && requiredSize != -1) {
-            bucketCountThresholds.setRequiredSize(requiredSize);
-        }
-        Integer shardSize = (Integer) otherOptions.get(SHARD_SIZE_FIELD_NAME);
-        if (shardSize != null && shardSize != -1) {
-            bucketCountThresholds.setShardSize(shardSize);
-        }
-        Long minDocCount = (Long) otherOptions.get(MIN_DOC_COUNT_FIELD_NAME);
-        if (minDocCount != null && minDocCount != -1) {
-            bucketCountThresholds.setMinDocCount(minDocCount);
-        }
-        Long shardMinDocCount = (Long) otherOptions.get(SHARD_MIN_DOC_COUNT_FIELD_NAME);
-        if (shardMinDocCount != null && shardMinDocCount != -1) {
-            bucketCountThresholds.setShardMinDocCount(shardMinDocCount);
-        }
-        SubAggCollectionMode collectMode = (SubAggCollectionMode) otherOptions.get(SubAggCollectionMode.KEY);
-        String executionHint = (String) otherOptions.get(EXECUTION_HINT_FIELD_NAME);
-        IncludeExclude incExc = incExcParser.createIncludeExclude(otherOptions);
-        return doCreateFactory(aggregationName, valuesSourceType, targetValueType, bucketCountThresholds, collectMode, executionHint,
-                incExc,
-                otherOptions);
-    }
-
-    protected abstract ValuesSourceAggregatorFactory<ValuesSource, ?> doCreateFactory(String aggregationName,
-            ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions);
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (incExcParser.token(currentFieldName, token, parser, parseFieldMatcher, otherOptions)) {
-            return true;
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            if (parseFieldMatcher.match(currentFieldName, EXECUTION_HINT_FIELD_NAME)) {
-                otherOptions.put(EXECUTION_HINT_FIELD_NAME, parser.text());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SubAggCollectionMode.KEY)) {
-                otherOptions.put(SubAggCollectionMode.KEY, SubAggCollectionMode.parse(parser.text(), parseFieldMatcher));
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
-                otherOptions.put(REQUIRED_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-            if (parseFieldMatcher.match(currentFieldName, REQUIRED_SIZE_FIELD_NAME)) {
-                otherOptions.put(REQUIRED_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SHARD_SIZE_FIELD_NAME)) {
-                otherOptions.put(SHARD_SIZE_FIELD_NAME, parser.intValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, MIN_DOC_COUNT_FIELD_NAME)) {
-                otherOptions.put(MIN_DOC_COUNT_FIELD_NAME, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, SHARD_MIN_DOC_COUNT_FIELD_NAME)) {
-                otherOptions.put(SHARD_MIN_DOC_COUNT_FIELD_NAME, parser.longValue());
-                return true;
-            } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-                return true;
-            }
-        } else if (parseSpecial(aggregationName, parser, parseFieldMatcher, token, currentFieldName, otherOptions)) {
-            return true;
-        }
-        return false;
-    }
-
-    public abstract boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher,
-            XContentParser.Token token, String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException;
-
-    protected abstract TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds();
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
index b5e1e81..4e3e28a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java
@@ -38,7 +38,6 @@ import java.util.Comparator;
 import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
-import java.util.Objects;
 
 /**
  *
@@ -264,23 +263,6 @@ class InternalOrder extends Terms.Order {
             return new CompoundOrderComparator(orderElements, aggregator);
         }
 
-        @Override
-        public int hashCode() {
-            return Objects.hash(orderElements);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            CompoundOrder other = (CompoundOrder) obj;
-            return Objects.equals(orderElements, other.orderElements);
-        }
-
         public static class CompoundOrderComparator implements Comparator<Terms.Bucket> {
 
             private List<Terms.Order> compoundOrder;
@@ -324,7 +306,7 @@ class InternalOrder extends Terms.Order {
         }
 
         public static Terms.Order readOrder(StreamInput in) throws IOException {
-            return readOrder(in, false);
+            return readOrder(in, true);
         }
 
         public static Terms.Order readOrder(StreamInput in, boolean absoluteOrder) throws IOException {
@@ -350,22 +332,4 @@ class InternalOrder extends Terms.Order {
             }
         }
     }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(id, asc);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        InternalOrder other = (InternalOrder) obj;
-        return Objects.equals(id, other.id)
-                && Objects.equals(asc, other.asc);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
index 16fb7f1..224e42c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java
@@ -82,7 +82,7 @@ public interface Terms extends MultiBucketsAggregation {
      * Get the bucket for the given term, or null if there is no such bucket.
      */
     Bucket getBucketByKey(String term);
-
+    
     /**
      * Get an upper bound of the error on document counts in this aggregation.
      */
@@ -166,11 +166,5 @@ public interface Terms extends MultiBucketsAggregation {
 
         abstract byte id();
 
-        @Override
-        public abstract int hashCode();
-
-        @Override
-        public abstract boolean equals(Object obj);
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
index 7ea88c9..7971d1f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregator.java
@@ -21,10 +21,7 @@
 package org.elasticsearch.search.aggregations.bucket.terms;
 
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -39,136 +36,99 @@ import java.io.IOException;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 
 public abstract class TermsAggregator extends BucketsAggregator {
 
-    public static class BucketCountThresholds implements Writeable<BucketCountThresholds>, ToXContent {
-
-        private static final BucketCountThresholds PROTOTYPE = new BucketCountThresholds(-1, -1, -1, -1);
-
-        private long minDocCount;
-        private long shardMinDocCount;
-        private int requiredSize;
-        private int shardSize;
-
-        public static BucketCountThresholds readFromStream(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
-        }
+    public static class BucketCountThresholds {
+        private Explicit<Long> minDocCount;
+        private Explicit<Long> shardMinDocCount;
+        private Explicit<Integer> requiredSize;
+        private Explicit<Integer> shardSize;
 
         public BucketCountThresholds(long minDocCount, long shardMinDocCount, int requiredSize, int shardSize) {
-            this.minDocCount = minDocCount;
-            this.shardMinDocCount = shardMinDocCount;
-            this.requiredSize = requiredSize;
-            this.shardSize = shardSize;
+            this.minDocCount = new Explicit<>(minDocCount, false);
+            this.shardMinDocCount =  new Explicit<>(shardMinDocCount, false);
+            this.requiredSize = new Explicit<>(requiredSize, false);
+            this.shardSize = new Explicit<>(shardSize, false);
+        }
+        public BucketCountThresholds() {
+            this(-1, -1, -1, -1);
         }
 
         public BucketCountThresholds(BucketCountThresholds bucketCountThresholds) {
-            this(bucketCountThresholds.minDocCount, bucketCountThresholds.shardMinDocCount, bucketCountThresholds.requiredSize,
-                    bucketCountThresholds.shardSize);
+            this(bucketCountThresholds.minDocCount.value(), bucketCountThresholds.shardMinDocCount.value(), bucketCountThresholds.requiredSize.value(), bucketCountThresholds.shardSize.value());
         }
 
         public void ensureValidity() {
 
-            if (shardSize == 0) {
+            if (shardSize.value() == 0) {
                 setShardSize(Integer.MAX_VALUE);
             }
 
-            if (requiredSize == 0) {
+            if (requiredSize.value() == 0) {
                 setRequiredSize(Integer.MAX_VALUE);
             }
             // shard_size cannot be smaller than size as we need to at least fetch <size> entries from every shards in order to return <size>
-            if (shardSize < requiredSize) {
-                setShardSize(requiredSize);
+            if (shardSize.value() < requiredSize.value()) {
+                setShardSize(requiredSize.value());
             }
 
             // shard_min_doc_count should not be larger than min_doc_count because this can cause buckets to be removed that would match the min_doc_count criteria
-            if (shardMinDocCount > minDocCount) {
-                setShardMinDocCount(minDocCount);
+            if (shardMinDocCount.value() > minDocCount.value()) {
+                setShardMinDocCount(minDocCount.value());
             }
 
-            if (requiredSize < 0 || minDocCount < 0) {
+            if (requiredSize.value() < 0 || minDocCount.value() < 0) {
                 throw new ElasticsearchException("parameters [requiredSize] and [minDocCount] must be >=0 in terms aggregation.");
             }
         }
 
         public long getShardMinDocCount() {
-            return shardMinDocCount;
+            return shardMinDocCount.value();
         }
 
         public void setShardMinDocCount(long shardMinDocCount) {
-            this.shardMinDocCount = shardMinDocCount;
+            this.shardMinDocCount = new Explicit<>(shardMinDocCount, true);
         }
 
         public long getMinDocCount() {
-            return minDocCount;
+            return minDocCount.value();
         }
 
         public void setMinDocCount(long minDocCount) {
-            this.minDocCount = minDocCount;
+            this.minDocCount = new Explicit<>(minDocCount, true);
         }
 
         public int getRequiredSize() {
-            return requiredSize;
+            return requiredSize.value();
         }
 
         public void setRequiredSize(int requiredSize) {
-            this.requiredSize = requiredSize;
+            this.requiredSize = new Explicit<>(requiredSize, true);
         }
 
         public int getShardSize() {
-            return shardSize;
+            return shardSize.value();
         }
 
         public void setShardSize(int shardSize) {
-            this.shardSize = shardSize;
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(TermsAggregatorFactory.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), requiredSize);
-            builder.field(TermsAggregatorFactory.SHARD_SIZE_FIELD_NAME.getPreferredName(), shardSize);
-            builder.field(TermsAggregatorFactory.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), minDocCount);
-            builder.field(TermsAggregatorFactory.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), shardMinDocCount);
-            return builder;
+            this.shardSize = new Explicit<>(shardSize, true);
         }
 
-        @Override
-        public BucketCountThresholds readFrom(StreamInput in) throws IOException {
-            int requiredSize = in.readInt();
-            int shardSize = in.readInt();
-            long minDocCount = in.readLong();
-            long shardMinDocCount = in.readLong();
-            return new BucketCountThresholds(minDocCount, shardMinDocCount, requiredSize, shardSize);
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeInt(requiredSize);
-            out.writeInt(shardSize);
-            out.writeLong(minDocCount);
-            out.writeLong(shardMinDocCount);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(requiredSize, shardSize, minDocCount, shardMinDocCount);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
+        public void toXContent(XContentBuilder builder) throws IOException {
+            if (requiredSize.explicit()) {
+                builder.field(AbstractTermsParametersParser.REQUIRED_SIZE_FIELD_NAME.getPreferredName(), requiredSize.value());
+            }
+            if (shardSize.explicit()) {
+                builder.field(AbstractTermsParametersParser.SHARD_SIZE_FIELD_NAME.getPreferredName(), shardSize.value());
+            }
+            if (minDocCount.explicit()) {
+                builder.field(AbstractTermsParametersParser.MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), minDocCount.value());
             }
-            if (getClass() != obj.getClass()) {
-                return false;
+            if (shardMinDocCount.explicit()) {
+                builder.field(AbstractTermsParametersParser.SHARD_MIN_DOC_COUNT_FIELD_NAME.getPreferredName(), shardMinDocCount.value());
             }
-            BucketCountThresholds other = (BucketCountThresholds) obj;
-            return Objects.equals(requiredSize, other.requiredSize)
-                    && Objects.equals(shardSize, other.shardSize)
-                    && Objects.equals(minDocCount, other.minDocCount)
-                    && Objects.equals(shardMinDocCount, other.shardMinDocCount);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
index 0458f5f..270dc00 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java
@@ -21,51 +21,27 @@ package org.elasticsearch.search.aggregations.bucket.terms;
 import org.apache.lucene.search.IndexSearcher;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.NonCollectingAggregator;
-import org.elasticsearch.search.aggregations.bucket.BucketUtils;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-/**
- *
- */
-public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource, TermsAggregatorFactory> {
-
-    public static final ParseField EXECUTION_HINT_FIELD_NAME = new ParseField("execution_hint");
-    public static final ParseField SHARD_SIZE_FIELD_NAME = new ParseField("shard_size");
-    public static final ParseField MIN_DOC_COUNT_FIELD_NAME = new ParseField("min_doc_count");
-    public static final ParseField SHARD_MIN_DOC_COUNT_FIELD_NAME = new ParseField("shard_min_doc_count");
-    public static final ParseField REQUIRED_SIZE_FIELD_NAME = new ParseField("size");
-
-    static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(1, 0, 10,
-            -1);
-    public static final ParseField SHOW_TERM_DOC_COUNT_ERROR = new ParseField("show_term_doc_count_error");
-    public static final ParseField ORDER_FIELD = new ParseField("order");
+public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<ValuesSource> {
 
     public enum ExecutionMode {
 
@@ -182,106 +158,28 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
         }
     }
 
-    private List<Terms.Order> orders = Collections.singletonList(Terms.Order.count(false));
-    private IncludeExclude includeExclude = null;
-    private String executionHint = null;
-    private SubAggCollectionMode collectMode = SubAggCollectionMode.DEPTH_FIRST;
-    private TermsAggregator.BucketCountThresholds bucketCountThresholds = new TermsAggregator.BucketCountThresholds(
-            DEFAULT_BUCKET_COUNT_THRESHOLDS);
-    private boolean showTermDocCountError = false;
-
-    public TermsAggregatorFactory(String name, ValuesSourceType valuesSourceType, ValueType valueType) {
-        super(name, StringTerms.TYPE, valuesSourceType, valueType);
-    }
-
-    public TermsAggregator.BucketCountThresholds bucketCountThresholds() {
-        return bucketCountThresholds;
-    }
-
-    public TermsAggregatorFactory bucketCountThresholds(TermsAggregator.BucketCountThresholds bucketCountThresholds) {
-        this.bucketCountThresholds = bucketCountThresholds;
-        return this;
-    }
-
-    /**
-     * Sets the order in which the buckets will be returned.
-     */
-    public TermsAggregatorFactory order(List<Terms.Order> order) {
-        this.orders = order;
-        return this;
-    }
-
-    /**
-     * Gets the order in which the buckets will be returned.
-     */
-    public List<Terms.Order> order() {
-        return orders;
-    }
-
-    /**
-     * Expert: sets an execution hint to the aggregation.
-     */
-    public TermsAggregatorFactory executionHint(String executionHint) {
-        this.executionHint = executionHint;
-        return this;
-    }
-
-    /**
-     * Expert: gets an execution hint to the aggregation.
-     */
-    public String executionHint() {
-        return executionHint;
-    }
-
-    /**
-     * Expert: set the collection mode.
-     */
-    public TermsAggregatorFactory collectMode(SubAggCollectionMode mode) {
-        this.collectMode = mode;
-        return this;
-    }
-
-    /**
-     * Expert: get the collection mode.
-     */
-    public SubAggCollectionMode collectMode() {
-        return collectMode;
-    }
-
-    /**
-     * Set terms to include and exclude from the aggregation results
-     */
-    public TermsAggregatorFactory includeExclude(IncludeExclude includeExclude) {
+    private final Terms.Order order;
+    private final IncludeExclude includeExclude;
+    private final String executionHint;
+    private final SubAggCollectionMode collectMode;
+    private final TermsAggregator.BucketCountThresholds bucketCountThresholds;
+    private final boolean showTermDocCountError;
+
+    public TermsAggregatorFactory(String name, ValuesSourceConfig config, Terms.Order order,
+            TermsAggregator.BucketCountThresholds bucketCountThresholds, IncludeExclude includeExclude, String executionHint,
+            SubAggCollectionMode executionMode, boolean showTermDocCountError) {
+        super(name, StringTerms.TYPE.name(), config);
+        this.order = order;
         this.includeExclude = includeExclude;
-        return this;
-    }
-
-    /**
-     * Get terms to include and exclude from the aggregation results
-     */
-    public IncludeExclude includeExclude() {
-        return includeExclude;
-    }
-
-    /**
-     * Get whether doc count error will be return for individual terms
-     */
-    public boolean showTermDocCountError() {
-        return showTermDocCountError;
-    }
-
-    /**
-     * Set whether doc count error will be return for individual terms
-     */
-    public TermsAggregatorFactory showTermDocCountError(boolean showTermDocCountError) {
+        this.executionHint = executionHint;
+        this.bucketCountThresholds = bucketCountThresholds;
+        this.collectMode = executionMode;
         this.showTermDocCountError = showTermDocCountError;
-        return this;
     }
 
     @Override
     protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
             List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-        Terms.Order order = resolveOrder(orders);
         final InternalAggregation aggregation = new UnmappedTerms(name, order, bucketCountThresholds.getRequiredSize(),
                 bucketCountThresholds.getShardSize(), bucketCountThresholds.getMinDocCount(), pipelineAggregators, metaData);
         return new NonCollectingAggregator(name, aggregationContext, parent, factories, pipelineAggregators, metaData) {
@@ -297,38 +195,13 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
         };
     }
 
-    private Order resolveOrder(List<Order> orders) {
-        Terms.Order order;
-        if (orders.size() == 1 && (orders.get(0) == InternalOrder.TERM_ASC || orders.get(0) == InternalOrder.TERM_DESC)) {
-            // If order is only terms order then we don't need compound
-            // ordering
-            order = orders.get(0);
-        } else {
-            // for all other cases we need compound order so term order asc
-            // can be added to make the order deterministic
-            order = Order.compound(orders);
-        }
-        return order;
-    }
-
     @Override
     protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
             boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
             throws IOException {
-        Terms.Order order = resolveOrder(orders);
         if (collectsFromSingleBucket == false) {
             return asMultiBucketAggregator(this, aggregationContext, parent);
         }
-        BucketCountThresholds bucketCountThresholds = new BucketCountThresholds(this.bucketCountThresholds);
-        if (!(order == InternalOrder.TERM_ASC || order == InternalOrder.TERM_DESC)
-                && bucketCountThresholds.getShardSize() == DEFAULT_BUCKET_COUNT_THRESHOLDS.getShardSize()) {
-            // The user has not made a shardSize selection. Use default
-            // heuristic to avoid any wrong-ranking caused by distributed
-            // counting
-            bucketCountThresholds.setShardSize(BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
-                    aggregationContext.searchContext().numberOfShards()));
-        }
-        bucketCountThresholds.ensureValidity();
         if (valuesSource instanceof ValuesSource.Bytes) {
             ExecutionMode execution = null;
             if (executionHint != null) {
@@ -408,76 +281,4 @@ public class TermsAggregatorFactory extends ValuesSourceAggregatorFactory<Values
                 + "]. It can only be applied to numeric or string fields.");
     }
 
-    @Override
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        bucketCountThresholds.toXContent(builder, params);
-        builder.field(SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
-        if (executionHint != null) {
-            builder.field(TermsAggregatorFactory.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
-        }
-        builder.startArray(ORDER_FIELD.getPreferredName());
-        for (Terms.Order order : orders) {
-            order.toXContent(builder, params);
-        }
-        builder.endArray();
-        builder.field(SubAggCollectionMode.KEY.getPreferredName(), collectMode.parseField().getPreferredName());
-        if (includeExclude != null) {
-            includeExclude.toXContent(builder, params);
-        }
-        return builder;
-    }
-
-    @Override
-    protected TermsAggregatorFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        TermsAggregatorFactory factory = new TermsAggregatorFactory(name, valuesSourceType, targetValueType);
-        factory.bucketCountThresholds = BucketCountThresholds.readFromStream(in);
-        factory.collectMode = SubAggCollectionMode.BREADTH_FIRST.readFrom(in);
-        factory.executionHint = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.includeExclude = IncludeExclude.readFromStream(in);
-        }
-        int numOrders = in.readVInt();
-        List<Terms.Order> orders = new ArrayList<>(numOrders);
-        for (int i = 0; i < numOrders; i++) {
-            orders.add(InternalOrder.Streams.readOrder(in));
-        }
-        factory.orders = orders;
-        factory.showTermDocCountError = in.readBoolean();
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        bucketCountThresholds.writeTo(out);
-        collectMode.writeTo(out);
-        out.writeOptionalString(executionHint);
-        boolean hasIncExc = includeExclude != null;
-        out.writeBoolean(hasIncExc);
-        if (hasIncExc) {
-            includeExclude.writeTo(out);
-        }
-        out.writeVInt(orders.size());
-        for (Terms.Order order : orders) {
-            InternalOrder.Streams.writeOrder(order, out);
-        }
-        out.writeBoolean(showTermDocCountError);
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(bucketCountThresholds, collectMode, executionHint, includeExclude, orders, showTermDocCountError);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        TermsAggregatorFactory other = (TermsAggregatorFactory) obj;
-        return Objects.equals(bucketCountThresholds, other.bucketCountThresholds)
-                && Objects.equals(collectMode, other.collectMode)
-                && Objects.equals(executionHint, other.executionHint)
-                && Objects.equals(includeExclude, other.includeExclude)
-                && Objects.equals(orders, other.orders)
-                && Objects.equals(showTermDocCountError, other.showTermDocCountError);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
index 3d8aff9..9bc1f7a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java
@@ -97,7 +97,7 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
         this.includePattern = regex;
         return this;
     }
-
+    
     /**
      * Define a set of terms that should be aggregated.
      */
@@ -107,8 +107,8 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
         }
         this.includeTerms = terms;
         return this;
-    }
-
+    }    
+    
     /**
      * Define a set of terms that should be aggregated.
      */
@@ -118,16 +118,16 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
         }
         this.includeTerms = longsArrToStringArr(terms);
         return this;
-    }
-
+    }     
+    
     private String[] longsArrToStringArr(long[] terms) {
         String[] termsAsString = new String[terms.length];
         for (int i = 0; i < terms.length; i++) {
             termsAsString[i] = Long.toString(terms[i]);
         }
         return termsAsString;
-    }
-
+    }      
+    
 
     /**
      * Define a set of terms that should be aggregated.
@@ -146,7 +146,7 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
             termsAsString[i] = Double.toString(terms[i]);
         }
         return termsAsString;
-    }
+    }    
 
     /**
      * Define a regular expression that will filter out terms that should be excluded from the aggregation. The regular
@@ -161,7 +161,7 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
         this.excludePattern = regex;
         return this;
     }
-
+    
     /**
      * Define a set of terms that should not be aggregated.
      */
@@ -171,9 +171,9 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
         }
         this.excludeTerms = terms;
         return this;
-    }
-
-
+    }    
+    
+    
     /**
      * Define a set of terms that should not be aggregated.
      */
@@ -194,9 +194,9 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
         }
         this.excludeTerms = doubleArrToStringArr(terms);
         return this;
-    }
-
-
+    }    
+    
+    
 
     /**
      * When using scripts, the value type indicates the types of the values the script is generating.
@@ -241,13 +241,13 @@ public class TermsBuilder extends ValuesSourceAggregationBuilder<TermsBuilder> {
     @Override
     protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
 
-        bucketCountThresholds.toXContent(builder, params);
+        bucketCountThresholds.toXContent(builder);
 
         if (showTermDocCountError != null) {
-            builder.field(TermsAggregatorFactory.SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
+            builder.field(AbstractTermsParametersParser.SHOW_TERM_DOC_COUNT_ERROR.getPreferredName(), showTermDocCountError);
         }
         if (executionHint != null) {
-            builder.field(TermsAggregatorFactory.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
+            builder.field(AbstractTermsParametersParser.EXECUTION_HINT_FIELD_NAME.getPreferredName(), executionHint);
         }
         if (valueType != null) {
             builder.field("value_type", valueType.name().toLowerCase(Locale.ROOT));
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java
new file mode 100644
index 0000000..c8138b7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParametersParser.java
@@ -0,0 +1,144 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.search.aggregations.bucket.terms;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+
+public class TermsParametersParser extends AbstractTermsParametersParser {
+
+    private static final TermsAggregator.BucketCountThresholds DEFAULT_BUCKET_COUNT_THRESHOLDS = new TermsAggregator.BucketCountThresholds(1, 0, 10, -1);
+
+    public List<OrderElement> getOrderElements() {
+        return orderElements;
+    }
+    
+    public boolean showTermDocCountError() {
+        return showTermDocCountError;
+    }
+
+    List<OrderElement> orderElements;
+    private boolean showTermDocCountError = false;
+
+    public TermsParametersParser() {
+        orderElements = new ArrayList<>(1);
+        orderElements.add(new OrderElement("_count", false));
+    }
+
+    @Override
+    public void parseSpecial(String aggregationName, XContentParser parser, SearchContext context, XContentParser.Token token, String currentFieldName) throws IOException {
+        if (token == XContentParser.Token.START_OBJECT) {
+            if ("order".equals(currentFieldName)) {
+                this.orderElements = Collections.singletonList(parseOrderParam(aggregationName, parser, context));
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else if (token == XContentParser.Token.START_ARRAY) {
+            if ("order".equals(currentFieldName)) {
+                orderElements = new ArrayList<>();
+                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                    if (token == XContentParser.Token.START_OBJECT) {
+                        OrderElement orderParam = parseOrderParam(aggregationName, parser, context);
+                        orderElements.add(orderParam);
+                    } else {
+                        throw new SearchParseException(context, "Order elements must be of type object in [" + aggregationName + "].",
+                                parser.getTokenLocation());
+                    }
+                }
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+            if (context.parseFieldMatcher().match(currentFieldName, SHOW_TERM_DOC_COUNT_ERROR)) {
+                showTermDocCountError = parser.booleanValue();
+            }
+        } else {
+            throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName
+                    + "].", parser.getTokenLocation());
+        }
+    }
+
+    private OrderElement parseOrderParam(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        XContentParser.Token token;
+        OrderElement orderParam = null;
+        String orderKey = null;
+        boolean orderAsc = false;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                orderKey = parser.currentName();
+            } else if (token == XContentParser.Token.VALUE_STRING) {
+                String dir = parser.text();
+                if ("asc".equalsIgnoreCase(dir)) {
+                    orderAsc = true;
+                } else if ("desc".equalsIgnoreCase(dir)) {
+                    orderAsc = false;
+                } else {
+                    throw new SearchParseException(context, "Unknown terms order direction [" + dir + "] in terms aggregation ["
+                            + aggregationName + "]", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " for [order] in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+        if (orderKey == null) {
+            throw new SearchParseException(context, "Must specify at least one field for [order] in [" + aggregationName + "].",
+                    parser.getTokenLocation());
+        } else {
+            orderParam = new OrderElement(orderKey, orderAsc);
+        }
+        return orderParam;
+    }
+
+    static class OrderElement {
+        private final String key;
+        private final boolean asc;
+
+        public OrderElement(String key, boolean asc) {
+            this.key = key;
+            this.asc = asc;
+        }
+
+        public String key() {
+            return key;
+        }
+
+        public boolean asc() {
+            return asc;
+        }
+
+        
+    }
+
+    @Override
+    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
+        return new TermsAggregator.BucketCountThresholds(DEFAULT_BUCKET_COUNT_THRESHOLDS);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
index 5b8368a..478309d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java
@@ -18,30 +18,24 @@
  */
 package org.elasticsearch.search.aggregations.bucket.terms;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.bucket.BucketUtils;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms.Order;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregator.BucketCountThresholds;
+import org.elasticsearch.search.aggregations.bucket.terms.TermsParametersParser.OrderElement;
 import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 
 /**
  *
  */
-public class TermsParser extends AbstractTermsParser {
-
+public class TermsParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -49,123 +43,37 @@ public class TermsParser extends AbstractTermsParser {
     }
 
     @Override
-    protected TermsAggregatorFactory doCreateFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, BucketCountThresholds bucketCountThresholds, SubAggCollectionMode collectMode, String executionHint,
-            IncludeExclude incExc, Map<ParseField, Object> otherOptions) {
-        TermsAggregatorFactory factory = new TermsAggregatorFactory(aggregationName, valuesSourceType, targetValueType);
-        List<OrderElement> orderElements = (List<OrderElement>) otherOptions.get(TermsAggregatorFactory.ORDER_FIELD);
-        if (orderElements != null) {
-            List<Terms.Order> orders = new ArrayList<>(orderElements.size());
-            for (OrderElement orderElement : orderElements) {
-                orders.add(resolveOrder(orderElement.key(), orderElement.asc()));
-            }
-            factory.order(orders);
-        }
-        if (bucketCountThresholds != null) {
-            factory.bucketCountThresholds(bucketCountThresholds);
-        }
-        if (collectMode != null) {
-            factory.collectMode(collectMode);
-        }
-        if (executionHint != null) {
-            factory.executionHint(executionHint);
-        }
-        if (incExc != null) {
-            factory.includeExclude(incExc);
-        }
-        Boolean showTermDocCountError = (Boolean) otherOptions.get(TermsAggregatorFactory.SHOW_TERM_DOC_COUNT_ERROR);
-        if (showTermDocCountError != null) {
-            factory.showTermDocCountError(showTermDocCountError);
-        }
-        return factory;
-    }
-
-    @Override
-    public boolean parseSpecial(String aggregationName, XContentParser parser, ParseFieldMatcher parseFieldMatcher, Token token,
-            String currentFieldName, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_OBJECT) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorFactory.ORDER_FIELD)) {
-                otherOptions.put(TermsAggregatorFactory.ORDER_FIELD, Collections.singletonList(parseOrderParam(aggregationName, parser)));
-                return true;
-            }
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorFactory.ORDER_FIELD)) {
-                List<OrderElement> orderElements = new ArrayList<>();
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    if (token == XContentParser.Token.START_OBJECT) {
-                        OrderElement orderParam = parseOrderParam(aggregationName, parser);
-                        orderElements.add(orderParam);
-                    } else {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Order elements must be of type object in [" + aggregationName + "].");
-                    }
-                }
-                otherOptions.put(TermsAggregatorFactory.ORDER_FIELD, orderElements);
-                return true;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, TermsAggregatorFactory.SHOW_TERM_DOC_COUNT_ERROR)) {
-                otherOptions.put(TermsAggregatorFactory.SHOW_TERM_DOC_COUNT_ERROR, parser.booleanValue());
-                return true;
-            }
-        }
-        return false;
-    }
-
-    private OrderElement parseOrderParam(String aggregationName, XContentParser parser) throws IOException {
-        XContentParser.Token token;
-        OrderElement orderParam = null;
-        String orderKey = null;
-        boolean orderAsc = false;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                orderKey = parser.currentName();
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                String dir = parser.text();
-                if ("asc".equalsIgnoreCase(dir)) {
-                    orderAsc = true;
-                } else if ("desc".equalsIgnoreCase(dir)) {
-                    orderAsc = false;
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown terms order direction [" + dir + "] in terms aggregation [" + aggregationName + "]");
-                }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " for [order] in [" + aggregationName + "].");
-            }
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        TermsParametersParser aggParser = new TermsParametersParser();
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, StringTerms.TYPE, context).scriptable(true).formattable(true).build();
+        IncludeExclude.Parser incExcParser = new IncludeExclude.Parser();
+        aggParser.parse(aggregationName, parser, context, vsParser, incExcParser);
+
+        List<OrderElement> orderElements = aggParser.getOrderElements();
+        List<Terms.Order> orders = new ArrayList<>(orderElements.size());
+        for (OrderElement orderElement : orderElements) {
+            orders.add(resolveOrder(orderElement.key(), orderElement.asc()));
         }
-        if (orderKey == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Must specify at least one field for [order] in [" + aggregationName + "].");
-        } else {
-            orderParam = new OrderElement(orderKey, orderAsc);
+        Terms.Order order;
+        if (orders.size() == 1 && (orders.get(0) == InternalOrder.TERM_ASC || orders.get(0) == InternalOrder.TERM_DESC))
+        {
+            // If order is only terms order then we don't need compound ordering
+            order = orders.get(0);
         }
-        return orderParam;
-    }
-
-    static class OrderElement {
-        private final String key;
-        private final boolean asc;
-
-        public OrderElement(String key, boolean asc) {
-            this.key = key;
-            this.asc = asc;
+        else
+        {
+            // for all other cases we need compound order so term order asc can be added to make the order deterministic
+            order = Order.compound(orders);
         }
-
-        public String key() {
-            return key;
+        TermsAggregator.BucketCountThresholds bucketCountThresholds = aggParser.getBucketCountThresholds();
+        if (!(order == InternalOrder.TERM_ASC || order == InternalOrder.TERM_DESC)
+                && bucketCountThresholds.getShardSize() == aggParser.getDefaultBucketCountThresholds().getShardSize()) {
+            // The user has not made a shardSize selection. Use default heuristic to avoid any wrong-ranking caused by distributed counting
+            bucketCountThresholds.setShardSize(BucketUtils.suggestShardSideQueueSize(bucketCountThresholds.getRequiredSize(),
+                    context.numberOfShards()));
         }
-
-        public boolean asc() {
-            return asc;
-        }
-
-    }
-
-    @Override
-    public TermsAggregator.BucketCountThresholds getDefaultBucketCountThresholds() {
-        return new TermsAggregator.BucketCountThresholds(TermsAggregatorFactory.DEFAULT_BUCKET_COUNT_THRESHOLDS);
+        bucketCountThresholds.ensureValidity();
+        return new TermsAggregatorFactory(aggregationName, vsParser.config(), order, bucketCountThresholds, aggParser.getIncludeExclude(), aggParser.getExecutionHint(), aggParser.getCollectionMode(), aggParser.showTermDocCountError());
     }
 
     static Terms.Order resolveOrder(String key, boolean asc) {
@@ -178,9 +86,4 @@ public class TermsParser extends AbstractTermsParser {
         return Order.aggregation(key, asc);
     }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new TermsAggregatorFactory(null, null, null) };
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
index f6df150..9c33a98 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java
@@ -34,22 +34,12 @@ import org.apache.lucene.util.automaton.CompiledAutomaton;
 import org.apache.lucene.util.automaton.Operations;
 import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.WithOrdinals;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.HashSet;
-import java.util.Map;
-import java.util.Objects;
 import java.util.Set;
 import java.util.SortedSet;
 import java.util.TreeSet;
@@ -58,16 +48,7 @@ import java.util.TreeSet;
  * Defines the include/exclude regular expression filtering for string terms aggregation. In this filtering logic,
  * exclusion has precedence, where the {@code include} is evaluated first and then the {@code exclude}.
  */
-public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
-
-    private static final IncludeExclude PROTOTYPE = new IncludeExclude(Collections.emptySortedSet(), Collections.emptySortedSet());
-    private static final ParseField INCLUDE_FIELD = new ParseField("include");
-    private static final ParseField EXCLUDE_FIELD = new ParseField("exclude");
-    private static final ParseField PATTERN_FIELD = new ParseField("pattern");
-
-    public static IncludeExclude readFromStream(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
+public class IncludeExclude {
 
     // The includeValue and excludeValue ByteRefs which are the result of the parsing
     // process are converted into a LongFilter when used on numeric fields
@@ -302,14 +283,18 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
 
     public static class Parser {
 
-        public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser,
-                ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
+        String include = null;
+        String exclude = null;
+        SortedSet<BytesRef> includeValues;
+        SortedSet<BytesRef> excludeValues;
+
+        public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
 
             if (token == XContentParser.Token.VALUE_STRING) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
-                    otherOptions.put(INCLUDE_FIELD, parser.text());
-                } else if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
-                    otherOptions.put(EXCLUDE_FIELD, parser.text());
+                if ("include".equals(currentFieldName)) {
+                    include = parser.text();
+                } else if ("exclude".equals(currentFieldName)) {
+                    exclude = parser.text();
                 } else {
                     return false;
                 }
@@ -317,35 +302,35 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
             }
 
             if (token == XContentParser.Token.START_ARRAY) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
-                    otherOptions.put(INCLUDE_FIELD, new TreeSet<>(parseArrayToSet(parser)));
+                if ("include".equals(currentFieldName)) {
+                     includeValues = new TreeSet<>(parseArrayToSet(parser));
                      return true;
                 }
-                if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
-                    otherOptions.put(EXCLUDE_FIELD, new TreeSet<>(parseArrayToSet(parser)));
+                if ("exclude".equals(currentFieldName)) {
+                      excludeValues = new TreeSet<>(parseArrayToSet(parser));
                       return true;
                 }
                 return false;
             }
 
             if (token == XContentParser.Token.START_OBJECT) {
-                if (parseFieldMatcher.match(currentFieldName, INCLUDE_FIELD)) {
+                if ("include".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             currentFieldName = parser.currentName();
                         } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(currentFieldName, PATTERN_FIELD)) {
-                                otherOptions.put(INCLUDE_FIELD, parser.text());
+                            if ("pattern".equals(currentFieldName)) {
+                                include = parser.text();
                             }
                         }
                     }
-                } else if (parseFieldMatcher.match(currentFieldName, EXCLUDE_FIELD)) {
+                } else if ("exclude".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                         if (token == XContentParser.Token.FIELD_NAME) {
                             currentFieldName = parser.currentName();
                         } else if (token == XContentParser.Token.VALUE_STRING) {
-                            if (parseFieldMatcher.match(currentFieldName, PATTERN_FIELD)) {
-                                otherOptions.put(EXCLUDE_FIELD, parser.text());
+                            if ("pattern".equals(currentFieldName)) {
+                                exclude = parser.text();
                             }
                         }
                     }
@@ -357,7 +342,6 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
 
             return false;
         }
-
         private Set<BytesRef> parseArrayToSet(XContentParser parser) throws IOException {
             final Set<BytesRef> set = new HashSet<>();
             if (parser.currentToken() != XContentParser.Token.START_ARRAY) {
@@ -372,27 +356,7 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
             return set;
         }
 
-        public IncludeExclude createIncludeExclude(Map<ParseField, Object> otherOptions) {
-            Object includeObject = otherOptions.get(INCLUDE_FIELD);
-            String include = null;
-            SortedSet<BytesRef> includeValues = null;
-            if (includeObject != null) {
-                if (includeObject instanceof String) {
-                    include = (String) includeObject;
-                } else if (includeObject instanceof SortedSet) {
-                    includeValues = (SortedSet<BytesRef>) includeObject;
-                }
-            }
-            Object excludeObject = otherOptions.get(EXCLUDE_FIELD);
-            String exclude = null;
-            SortedSet<BytesRef> excludeValues = null;
-            if (excludeObject != null) {
-                if (excludeObject instanceof String) {
-                    exclude = (String) excludeObject;
-                } else if (excludeObject instanceof SortedSet) {
-                    excludeValues = (SortedSet<BytesRef>) excludeObject;
-                }
-            }
+        public IncludeExclude includeExclude() {
             RegExp includePattern =  include != null ? new RegExp(include) : null;
             RegExp excludePattern = exclude != null ? new RegExp(exclude) : null;
             if (includePattern != null || excludePattern != null) {
@@ -480,111 +444,4 @@ public class IncludeExclude implements Writeable<IncludeExclude>, ToXContent {
         return result;
     }
 
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (include != null) {
-            builder.field(INCLUDE_FIELD.getPreferredName(), include.getOriginalString());
-        }
-        if (includeValues != null) {
-            builder.startArray(INCLUDE_FIELD.getPreferredName());
-            for (BytesRef value : includeValues) {
-                builder.value(value.utf8ToString());
-            }
-            builder.endArray();
-        }
-        if (exclude != null) {
-            builder.field(EXCLUDE_FIELD.getPreferredName(), exclude.getOriginalString());
-        }
-        if (excludeValues != null) {
-            builder.startArray(EXCLUDE_FIELD.getPreferredName());
-            for (BytesRef value : excludeValues) {
-                builder.value(value.utf8ToString());
-            }
-            builder.endArray();
-        }
-        return builder;
-    }
-
-    @Override
-    public IncludeExclude readFrom(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            String includeString = in.readOptionalString();
-            RegExp include = null;
-            if (includeString != null) {
-                include = new RegExp(includeString);
-            }
-            String excludeString = in.readOptionalString();
-            RegExp exclude = null;
-            if (excludeString != null) {
-                exclude = new RegExp(excludeString);
-            }
-            return new IncludeExclude(include, exclude);
-        } else {
-            SortedSet<BytesRef> includes = null;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                includes = new TreeSet<>();
-                for (int i = 0; i < size; i++) {
-                    includes.add(in.readBytesRef());
-                }
-            }
-            SortedSet<BytesRef> excludes = null;
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                excludes = new TreeSet<>();
-                for (int i = 0; i < size; i++) {
-                    excludes.add(in.readBytesRef());
-                }
-            }
-            return new IncludeExclude(includes, excludes);
-        }
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        boolean regexBased = isRegexBased();
-        out.writeBoolean(regexBased);
-        if (regexBased) {
-            out.writeOptionalString(include == null ? null : include.getOriginalString());
-            out.writeOptionalString(exclude == null ? null : exclude.getOriginalString());
-        } else {
-            boolean hasIncludes = includeValues != null;
-            out.writeBoolean(hasIncludes);
-            if (hasIncludes) {
-                out.writeVInt(includeValues.size());
-                for (BytesRef value : includeValues) {
-                    out.writeBytesRef(value);
-                }
-            }
-            boolean hasExcludes = excludeValues != null;
-            out.writeBoolean(hasExcludes);
-            if (hasExcludes) {
-                out.writeVInt(excludeValues.size());
-                for (BytesRef value : excludeValues) {
-                    out.writeBytesRef(value);
-                }
-            }
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(include == null ? null : include.getOriginalString(), exclude == null ? null : exclude.getOriginalString(),
-                includeValues, excludeValues);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        } if (getClass() != obj.getClass()) {
-            return false;
-        }
-        IncludeExclude other = (IncludeExclude) obj;
-        return Objects.equals(include == null ? null : include.getOriginalString(), other.include == null ? null : other.include.getOriginalString())
-                && Objects.equals(exclude == null ? null : exclude.getOriginalString(), other.exclude == null ? null : other.exclude.getOriginalString())
-                && Objects.equals(includeValues, other.includeValues)
-                && Objects.equals(excludeValues, other.excludeValues);
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java
new file mode 100644
index 0000000..6847a9a
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/NumericValuesSourceMetricsAggregatorParser.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics;
+
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+public abstract class NumericValuesSourceMetricsAggregatorParser<S extends InternalNumericMetricsAggregation> implements Aggregator.Parser {
+
+    protected final InternalAggregation.Type aggType;
+
+    protected NumericValuesSourceMetricsAggregatorParser(InternalAggregation.Type aggType) {
+        this.aggType = aggType;
+    }
+
+    @Override
+    public String type() {
+        return aggType.name();
+    }
+
+    @Override
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, aggType, context).formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return createFactory(aggregationName, vsParser.config());
+    }
+
+    protected abstract AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config);
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
index fbada14..67a6f19 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java
@@ -19,13 +19,10 @@
 package org.elasticsearch.search.aggregations.metrics.avg;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -34,10 +31,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -117,10 +113,10 @@ public class AvgAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalAvg(name, 0.0, 0l, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalAvg.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, String type, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, type, valuesSourceConfig);
         }
 
         @Override
@@ -136,32 +132,6 @@ public class AvgAggregator extends NumericMetricsAggregator.SingleValue {
                 throws IOException {
             return new AvgAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new AvgAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java
new file mode 100644
index 0000000..143f39b
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.avg;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Avg} aggregation.
+ */
+public class AvgBuilder extends ValuesSourceMetricsAggregationBuilder<AvgBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public AvgBuilder(String name) {
+        super(name, InternalAvg.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
index 110a368..1c2b2be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java
@@ -18,46 +18,23 @@
  */
 package org.elasticsearch.search.aggregations.metrics.avg;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class AvgParser extends NumericValuesSourceParser {
+public class AvgParser extends NumericValuesSourceMetricsAggregatorParser<InternalAvg> {
 
     public AvgParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalAvg.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected AvgAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new AvgAggregator.Factory(aggregationName);
+        super(InternalAvg.TYPE);
     }
 
     @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new AvgAggregator.Factory(null) };
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new AvgAggregator.Factory(aggregationName, type(), config);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
index 2ea7793..1b2d5fc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityAggregatorFactory.java
@@ -19,62 +19,29 @@
 
 package org.elasticsearch.search.aggregations.metrics.cardinality;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
-public final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource, CardinalityAggregatorFactory> {
+final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource> {
 
-    public static final ParseField PRECISION_THRESHOLD_FIELD = new ParseField("precision_threshold");
+    private final long precisionThreshold;
 
-    private Long precisionThreshold = null;
-
-    public CardinalityAggregatorFactory(String name, ValueType targetValueType) {
-        super(name, InternalCardinality.TYPE, ValuesSourceType.ANY, targetValueType);
-    }
-
-    /**
-     * Set a precision threshold. Higher values improve accuracy but also
-     * increase memory usage.
-     */
-    public CardinalityAggregatorFactory precisionThreshold(long precisionThreshold) {
+    CardinalityAggregatorFactory(String name, ValuesSourceConfig config, long precisionThreshold) {
+        super(name, InternalCardinality.TYPE.name(), config);
         this.precisionThreshold = precisionThreshold;
-        return this;
-    }
-
-    /**
-     * Get the precision threshold. Higher values improve accuracy but also
-     * increase memory usage. Will return <code>null</code> if the
-     * precisionThreshold has not been set yet.
-     */
-    public Long precisionThreshold() {
-        return precisionThreshold;
-    }
-
-    /**
-     * @deprecated no replacement - values will always be rehashed
-     */
-    @Deprecated
-    public void rehash(boolean rehash) {
-        // Deprecated all values are already rehashed so do nothing
     }
 
     private int precision(Aggregator parent) {
-        return precisionThreshold == null ? defaultPrecision(parent) : HyperLogLogPlusPlus.precisionFromThreshold(precisionThreshold);
+        return precisionThreshold < 0 ? defaultPrecision(parent) : HyperLogLogPlusPlus.precisionFromThreshold(precisionThreshold);
     }
 
     @Override
@@ -90,44 +57,6 @@ public final class CardinalityAggregatorFactory extends ValuesSourceAggregatorFa
                 metaData);
     }
 
-    @Override
-    protected CardinalityAggregatorFactory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, StreamInput in) throws IOException {
-        CardinalityAggregatorFactory factory = new CardinalityAggregatorFactory(name, targetValueType);
-        if (in.readBoolean()) {
-            factory.precisionThreshold = in.readLong();
-        }
-        return factory;
-    }
-
-    @Override
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-        boolean hasPrecisionThreshold = precisionThreshold != null;
-        out.writeBoolean(hasPrecisionThreshold);
-        if (hasPrecisionThreshold) {
-            out.writeLong(precisionThreshold);
-        }
-    }
-
-    @Override
-    public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        if (precisionThreshold != null) {
-            builder.field(PRECISION_THRESHOLD_FIELD.getPreferredName(), precisionThreshold);
-        }
-        return builder;
-    }
-
-    @Override
-    protected int innerHashCode() {
-        return Objects.hash(precisionThreshold);
-    }
-
-    @Override
-    protected boolean innerEquals(Object obj) {
-        CardinalityAggregatorFactory other = (CardinalityAggregatorFactory) obj;
-        return Objects.equals(precisionThreshold, other.precisionThreshold);
-    }
-
     /*
      * If one of the parent aggregators is a MULTI_BUCKET one, we might want to lower the precision
      * because otherwise it might be memory-intensive. On the other hand, for top-level aggregators
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java
new file mode 100644
index 0000000..7680aa0
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityBuilder.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.cardinality;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link Cardinality} aggregation.
+ */
+public class CardinalityBuilder extends ValuesSourceMetricsAggregationBuilder<CardinalityBuilder> {
+
+    private Long precisionThreshold;
+    private Boolean rehash;
+
+    /**
+     * Sole constructor.
+     */
+    public CardinalityBuilder(String name) {
+        super(name, InternalCardinality.TYPE.name());
+    }
+
+    /**
+     * Set a precision threshold. Higher values improve accuracy but also
+     * increase memory usage.
+     */
+    public CardinalityBuilder precisionThreshold(long precisionThreshold) {
+        this.precisionThreshold = precisionThreshold;
+        return this;
+    }
+
+    /**
+     * Expert: set to false in case values of this field can already be treated
+     * as 64-bits hash values.
+     */
+    public CardinalityBuilder rehash(boolean rehash) {
+        this.rehash = rehash;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+        if (precisionThreshold != null) {
+            builder.field("precision_threshold", precisionThreshold);
+        }
+        if (rehash != null) {
+            builder.field("rehash", rehash);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
index 1b2876b..6833945 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityParser.java
@@ -20,59 +20,56 @@
 package org.elasticsearch.search.aggregations.metrics.cardinality;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 
-public class CardinalityParser extends AnyValuesSourceParser {
+public class CardinalityParser implements Aggregator.Parser {
 
+    private static final ParseField PRECISION_THRESHOLD = new ParseField("precision_threshold");
     private static final ParseField REHASH = new ParseField("rehash").withAllDeprecated("no replacement - values will always be rehashed");
 
-    public CardinalityParser() {
-        super(true, false);
-    }
-
     @Override
     public String type() {
         return InternalCardinality.TYPE.name();
     }
 
     @Override
-    protected CardinalityAggregatorFactory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        CardinalityAggregatorFactory factory = new CardinalityAggregatorFactory(aggregationName, targetValueType);
-        Long precisionThreshold = (Long) otherOptions.get(CardinalityAggregatorFactory.PRECISION_THRESHOLD_FIELD);
-        if (precisionThreshold != null) {
-            factory.precisionThreshold(precisionThreshold);
-        }
-        return factory;
-    }
+    public AggregatorFactory parse(String name, XContentParser parser, SearchContext context) throws IOException {
 
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token.isValue()) {
-            if (parseFieldMatcher.match(currentFieldName, CardinalityAggregatorFactory.PRECISION_THRESHOLD_FIELD)) {
-                otherOptions.put(CardinalityAggregatorFactory.PRECISION_THRESHOLD_FIELD, parser.longValue());
-                return true;
-            } else if (parseFieldMatcher.match(currentFieldName, REHASH)) {
-                // ignore
-                return true;
+        ValuesSourceParser<?> vsParser = ValuesSourceParser.any(name, InternalCardinality.TYPE, context).formattable(false).build();
+
+        long precisionThreshold = -1;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token.isValue()) {
+                if (context.parseFieldMatcher().match(currentFieldName, REHASH)) {
+                    // ignore
+                } else if (context.parseFieldMatcher().match(currentFieldName, PRECISION_THRESHOLD)) {
+                    precisionThreshold = parser.longValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + name + "]: [" + currentFieldName
+                            + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + name + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new CardinalityAggregatorFactory(null, null) };
+        return new CardinalityAggregatorFactory(name, vsParser.config(), precisionThreshold);
+
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
index 2f8b7ae..8c6159e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsAggregator.java
@@ -20,14 +20,10 @@
 package org.elasticsearch.search.aggregations.metrics.geobounds;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -36,20 +32,16 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public final class GeoBoundsAggregator extends MetricsAggregator {
 
-    static final ParseField WRAP_LONGITUDE_FIELD = new ParseField("wrap_longitude");
-
     private final ValuesSource.GeoPoint valuesSource;
     private final boolean wrapLongitude;
     DoubleArray tops;
@@ -168,33 +160,19 @@ public final class GeoBoundsAggregator extends MetricsAggregator {
         return new InternalGeoBounds(name, Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY,
                 Double.NEGATIVE_INFINITY, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, wrapLongitude, pipelineAggregators(), metaData());
     }
-
+    
     @Override
     public void doClose() {
         Releasables.close(tops, bottoms, posLefts, posRights, negLefts, negRights);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint, Factory> {
-
-        private boolean wrapLongitude = true;
+    public static class Factory extends ValuesSourceAggregatorFactory<ValuesSource.GeoPoint> {
 
-        public Factory(String name) {
-            super(name, InternalGeoBounds.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
+        private final boolean wrapLongitude;
 
-        /**
-         * Set whether to wrap longitudes. Defaults to true.
-         */
-        public Factory wrapLongitude(boolean wrapLongitude) {
+        protected Factory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config, boolean wrapLongitude) {
+            super(name, InternalGeoBounds.TYPE.name(), config);
             this.wrapLongitude = wrapLongitude;
-            return this;
-        }
-
-        /**
-         * Get whether to wrap longitudes.
-         */
-        public boolean wrapLongitude() {
-            return wrapLongitude;
         }
 
         @Override
@@ -210,35 +188,5 @@ public final class GeoBoundsAggregator extends MetricsAggregator {
             return new GeoBoundsAggregator(name, aggregationContext, parent, valuesSource, wrapLongitude, pipelineAggregators, metaData);
         }
 
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.wrapLongitude = in.readBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(wrapLongitude);
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(WRAP_LONGITUDE_FIELD.getPreferredName(), wrapLongitude);
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(wrapLongitude);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(wrapLongitude, other.wrapLongitude);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java
new file mode 100644
index 0000000..582ab8c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsBuilder.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.geobounds;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.ValuesSourceAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link GeoBounds} aggregation.
+ */
+public class GeoBoundsBuilder extends ValuesSourceAggregationBuilder<GeoBoundsBuilder> {
+
+    private Boolean wrapLongitude;
+
+    /**
+     * Sole constructor.
+     */
+    public GeoBoundsBuilder(String name) {
+        super(name, InternalGeoBounds.TYPE.name());
+    }
+
+    /**
+     * Set whether to wrap longitudes. Defaults to true.
+     */
+    public GeoBoundsBuilder wrapLongitude(boolean wrapLongitude) {
+        this.wrapLongitude = wrapLongitude;
+        return this;
+    }
+
+    @Override
+    protected XContentBuilder doInternalXContent(XContentBuilder builder, Params params) throws IOException {
+        if (wrapLongitude != null) {
+            builder.field("wrap_longitude", wrapLongitude);
+        }
+        return builder;
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
index 44ab63c..de1fea2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geobounds/GeoBoundsParser.java
@@ -19,23 +19,18 @@
 
 package org.elasticsearch.search.aggregations.metrics.geobounds;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource.GeoPoint;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
-public class GeoBoundsParser extends GeoPointValuesSourceParser {
-
-    public GeoBoundsParser() {
-        super(false, false);
-    }
+public class GeoBoundsParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -43,31 +38,33 @@ public class GeoBoundsParser extends GeoPointValuesSourceParser {
     }
 
     @Override
-    protected GeoBoundsAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        GeoBoundsAggregator.Factory factory = new GeoBoundsAggregator.Factory(aggregationName);
-        Boolean wrapLongitude = (Boolean) otherOptions.get(GeoBoundsAggregator.WRAP_LONGITUDE_FIELD);
-        if (wrapLongitude != null) {
-            factory.wrapLongitude(wrapLongitude);
-        }
-        return factory;
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, GeoBoundsAggregator.WRAP_LONGITUDE_FIELD)) {
-                otherOptions.put(GeoBoundsAggregator.WRAP_LONGITUDE_FIELD, parser.booleanValue());
-                return true;
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ValuesSourceParser<GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoBounds.TYPE, context)
+                .targetValueType(ValueType.GEOPOINT)
+                .formattable(true)
+                .build();
+        boolean wrapLongitude = true;
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+                
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if ("wrap_longitude".equals(currentFieldName) || "wrapLongitude".equals(currentFieldName)) {
+                    wrapLongitude = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
             }
         }
-        return false;
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new GeoBoundsAggregator.Factory(null) };
+        return new GeoBoundsAggregator.Factory(aggregationName, vsParser.config(), wrapLongitude);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
index 39aa6bc..b99db25 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidAggregator.java
@@ -22,24 +22,21 @@ package org.elasticsearch.search.aggregations.metrics.geocentroid;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.GeoUtils;
 import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.LeafBucketCollector;
 import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
+import org.elasticsearch.search.aggregations.metrics.geobounds.InternalGeoBounds;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 import java.io.IOException;
 import java.util.List;
@@ -125,10 +122,9 @@ public final class GeoCentroidAggregator extends MetricsAggregator {
         Releasables.close(centroids, counts);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.GeoPoint, Factory> {
-
-        public Factory(String name) {
-            super(name, InternalGeoCentroid.TYPE, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.GeoPoint> {
+        protected Factory(String name, ValuesSourceConfig<ValuesSource.GeoPoint> config) {
+            super(name, InternalGeoBounds.TYPE.name(), config);
         }
 
         @Override
@@ -143,31 +139,5 @@ public final class GeoCentroidAggregator extends MetricsAggregator {
                 throws IOException {
             return new GeoCentroidAggregator(name, aggregationContext, parent, valuesSource, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            return new Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java
new file mode 100644
index 0000000..9d6823c
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidBuilder.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.geocentroid;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder class for {@link org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator}
+ */
+public class GeoCentroidBuilder extends ValuesSourceMetricsAggregationBuilder<GeoCentroidBuilder> {
+
+    public GeoCentroidBuilder(String name) {
+        super(name, InternalGeoCentroid.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
index 1c1b195..49a7bc8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/geocentroid/GeoCentroidParser.java
@@ -19,26 +19,21 @@
 
 package org.elasticsearch.search.aggregations.metrics.geocentroid;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.GeoPointValuesSourceParser;
 import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Parser class for {@link org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator}
  */
-public class GeoCentroidParser extends GeoPointValuesSourceParser {
-
-    public GeoCentroidParser() {
-        super(true, false);
-    }
+public class GeoCentroidParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -46,19 +41,23 @@ public class GeoCentroidParser extends GeoPointValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected GeoCentroidAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new GeoCentroidAggregator.Factory(aggregationName);
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new GeoCentroidAggregator.Factory(null) };
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        ValuesSourceParser<ValuesSource.GeoPoint> vsParser = ValuesSourceParser.geoPoint(aggregationName, InternalGeoCentroid.TYPE, context)
+                .targetValueType(ValueType.GEOPOINT)
+                .formattable(true)
+                .build();
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else {
+                throw new SearchParseException(context, "Unknown key for a " + token + " in aggregation [" + aggregationName + "]: ["
+                        + currentFieldName + "].", parser.getTokenLocation());
+            }
+        }
+        return new GeoCentroidAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
index cd28dbb..e70fc7f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.search.aggregations.metrics.max;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.MultiValueMode;
@@ -35,10 +32,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -118,10 +114,10 @@ public class MaxAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalMax(name, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalMax.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalMax.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
@@ -136,33 +132,6 @@ public class MaxAggregator extends NumericMetricsAggregator.SingleValue {
                 throws IOException {
             return new MaxAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MaxAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
-
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java
new file mode 100644
index 0000000..b433573
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.max;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Max} aggregation.
+ */
+public class MaxBuilder extends ValuesSourceMetricsAggregationBuilder<MaxBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public MaxBuilder(String name) {
+        super(name, InternalMax.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
index 4ca7cc3..a5cdac5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java
@@ -18,46 +18,23 @@
  */
 package org.elasticsearch.search.aggregations.metrics.max;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class MaxParser extends NumericValuesSourceParser {
+public class MaxParser extends NumericValuesSourceMetricsAggregatorParser<InternalMax> {
 
     public MaxParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalMax.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MaxAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MaxAggregator.Factory(aggregationName);
+        super(InternalMax.TYPE);
     }
 
     @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new MaxAggregator.Factory(null) };
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new MaxAggregator.Factory(aggregationName, config);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
index 5242422..3a9bd40 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.search.aggregations.metrics.min;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.MultiValueMode;
@@ -35,10 +32,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -117,10 +113,10 @@ public class MinAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalMin(name, Double.POSITIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalMin.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalMin.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
@@ -135,32 +131,6 @@ public class MinAggregator extends NumericMetricsAggregator.SingleValue {
                 throws IOException {
             return new MinAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new MinAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java
new file mode 100644
index 0000000..e5342de
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.min;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Min} aggregation.
+ */
+public class MinBuilder extends ValuesSourceMetricsAggregationBuilder<MinBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public MinBuilder(String name) {
+        super(name, InternalMin.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
index 25083c0..e7dde9c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java
@@ -18,46 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.min;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class MinParser extends NumericValuesSourceParser {
+public class MinParser extends NumericValuesSourceMetricsAggregatorParser<InternalMin> {
 
     public MinParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalMin.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected MinAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new MinAggregator.Factory(aggregationName);
+        super(InternalMin.TYPE);
     }
 
     @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new MinAggregator.Factory(null) };
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new MinAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
index 707f5fb..0b30040 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java
@@ -21,19 +21,21 @@ package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import com.carrotsearch.hppc.DoubleArrayList;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentParser.Token;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
+import java.util.Arrays;
 
-public abstract class AbstractPercentilesParser extends NumericValuesSourceParser {
+public abstract class AbstractPercentilesParser implements Aggregator.Parser {
 
     public static final ParseField KEYED_FIELD = new ParseField("keyed");
     public static final ParseField METHOD_FIELD = new ParseField("method");
@@ -43,95 +45,139 @@ public abstract class AbstractPercentilesParser extends NumericValuesSourceParse
     private boolean formattable;
 
     public AbstractPercentilesParser(boolean formattable) {
-        super(true, formattable, false);
+        this.formattable = formattable;
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (token == XContentParser.Token.START_ARRAY) {
-            if (parseFieldMatcher.match(currentFieldName, keysField())) {
-                DoubleArrayList values = new DoubleArrayList(10);
-                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                    double value = parser.doubleValue();
-                    values.add(value);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalTDigestPercentiles.TYPE, context)
+                .formattable(formattable).build();
+
+        double[] keys = null;
+        boolean keyed = true;
+        Double compression = null;
+        Integer numberOfSignificantValueDigits = null;
+        PercentilesMethod method = null;
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if (context.parseFieldMatcher().match(currentFieldName, keysField())) {
+                    DoubleArrayList values = new DoubleArrayList(10);
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        double value = parser.doubleValue();
+                        values.add(value);
+                    }
+                    keys = values.toArray();
+                    Arrays.sort(keys);
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
-                double[] keys = values.toArray();
-                otherOptions.put(keysField(), keys);
-                return true;
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            if (parseFieldMatcher.match(currentFieldName, KEYED_FIELD)) {
-                boolean keyed = parser.booleanValue();
-                otherOptions.put(KEYED_FIELD, keyed);
-                return true;
-            } else {
-                return false;
-            }
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            PercentilesMethod method = PercentilesMethod.resolveFromName(currentFieldName);
-            if (method == null) {
-                return false;
-            } else {
-                otherOptions.put(METHOD_FIELD, method);
-                switch (method) {
-                case TDIGEST:
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(currentFieldName, COMPRESSION_FIELD)) {
-                                double compression = parser.doubleValue();
-                                otherOptions.put(COMPRESSION_FIELD, compression);
+            } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
+                if (context.parseFieldMatcher().match(currentFieldName, KEYED_FIELD)) {
+                    keyed = parser.booleanValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else if (token == XContentParser.Token.START_OBJECT) {
+                if (method != null) {
+                    throw new SearchParseException(context, "Found multiple methods in [" + aggregationName + "]: [" + currentFieldName
+                            + "]. only one of [" + PercentilesMethod.TDIGEST.getName() + "] and [" + PercentilesMethod.HDR.getName()
+                            + "] may be used.", parser.getTokenLocation());
+                }
+                method = PercentilesMethod.resolveFromName(currentFieldName);
+                if (method == null) {
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                            parser.getTokenLocation());
+                } else {
+                    switch (method) {
+                    case TDIGEST:
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                currentFieldName = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if (context.parseFieldMatcher().match(currentFieldName, COMPRESSION_FIELD)) {
+                                    compression = parser.doubleValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
                             } else {
-                                return false;
+                                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                        + currentFieldName + "].", parser.getTokenLocation());
                             }
-                        } else {
-                            return false;
                         }
-                    }
-                    break;
-                case HDR:
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                            if (parseFieldMatcher.match(currentFieldName, NUMBER_SIGNIFICANT_DIGITS_FIELD)) {
-                                int numberOfSignificantValueDigits = parser.intValue();
-                                otherOptions.put(NUMBER_SIGNIFICANT_DIGITS_FIELD, numberOfSignificantValueDigits);
+                        break;
+                    case HDR:
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                            if (token == XContentParser.Token.FIELD_NAME) {
+                                currentFieldName = parser.currentName();
+                            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                                if (context.parseFieldMatcher().match(currentFieldName, NUMBER_SIGNIFICANT_DIGITS_FIELD)) {
+                                    numberOfSignificantValueDigits = parser.intValue();
+                                } else {
+                                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName
+                                            + "]: [" + currentFieldName + "].", parser.getTokenLocation());
+                                }
                             } else {
-                                return false;
+                                throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                        + currentFieldName + "].", parser.getTokenLocation());
                             }
-                        } else {
-                            return false;
                         }
+                        break;
                     }
-                    break;
                 }
-                return true;
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        PercentilesMethod method = (PercentilesMethod) otherOptions.getOrDefault(METHOD_FIELD, PercentilesMethod.TDIGEST);
-
-        double[] cdfValues = (double[]) otherOptions.get(keysField());
-        Double compression = (Double) otherOptions.get(COMPRESSION_FIELD);
-        Integer numberOfSignificantValueDigits = (Integer) otherOptions.get(NUMBER_SIGNIFICANT_DIGITS_FIELD);
-        Boolean keyed = (Boolean) otherOptions.get(KEYED_FIELD);
-        return buildFactory(aggregationName, cdfValues, method, compression, numberOfSignificantValueDigits, keyed);
+        if (method == null) {
+            method = PercentilesMethod.TDIGEST;
+        }
+
+        switch (method) {
+        case TDIGEST:
+            if (numberOfSignificantValueDigits != null) {
+                throw new SearchParseException(context, "[number_of_significant_value_digits] cannot be used with method [tdigest] in ["
+                        + aggregationName + "].", parser.getTokenLocation());
+            }
+            if (compression == null) {
+                compression = 100.0;
+            }
+            break;
+        case HDR:
+            if (compression != null) {
+                throw new SearchParseException(context, "[compression] cannot be used with method [hdr] in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+            if (numberOfSignificantValueDigits == null) {
+                numberOfSignificantValueDigits = 3;
+            }
+            break;
+
+        default:
+            // Shouldn't get here but if we do, throw a parse exception for
+            // invalid method
+            throw new SearchParseException(context, "Unknown value for [" + currentFieldName + "] in [" + aggregationName + "]: [" + method
+                    + "].", parser.getTokenLocation());
+        }
+
+        return buildFactory(context, aggregationName, vsParser.config(), keys, method, compression,
+                numberOfSignificantValueDigits, keyed);
     }
 
-    protected abstract ValuesSourceAggregatorFactory<Numeric, ?> buildFactory(String aggregationName, double[] cdfValues,
-            PercentilesMethod method,
-            Double compression,
-            Integer numberOfSignificantValueDigits, Boolean keyed);
+    protected abstract AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> config,
+            double[] cdfValues, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed);
 
     protected abstract ParseField keysField();
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
index 553800a..51e900a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java
@@ -19,12 +19,14 @@
 package org.elasticsearch.search.aggregations.metrics.percentiles;
 
 import org.elasticsearch.common.ParseField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregator;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentileRanks;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregator;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 /**
  *
@@ -48,40 +50,19 @@ public class PercentileRanksParser extends AbstractPercentilesParser {
     }
 
     @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> buildFactory(String aggregationName, double[] keys, PercentilesMethod method,
-            Double compression, Integer numberOfSignificantValueDigits, Boolean keyed) {
+    protected AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> valuesSourceConfig,
+            double[] keys, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed) {
+        if (keys == null) {
+            throw new SearchParseException(context, "Missing token values in [" + aggregationName + "].", null);
+        }
         if (method == PercentilesMethod.TDIGEST) {
-            TDigestPercentileRanksAggregator.Factory factory = new TDigestPercentileRanksAggregator.Factory(aggregationName);
-            if (keys != null) {
-                factory.values(keys);
-            }
-            if (compression != null) {
-                factory.compression(compression);
-            }
-            if (keyed != null) {
-                factory.keyed(keyed);
-            }
-            return factory;
+            return new TDigestPercentileRanksAggregator.Factory(aggregationName, valuesSourceConfig, keys, compression, keyed);
         } else if (method == PercentilesMethod.HDR) {
-            HDRPercentileRanksAggregator.Factory factory = new HDRPercentileRanksAggregator.Factory(aggregationName);
-            if (keys != null) {
-                factory.values(keys);
-            }
-            if (numberOfSignificantValueDigits != null) {
-                factory.numberOfSignificantValueDigits(numberOfSignificantValueDigits);
-            }
-            if (keyed != null) {
-                factory.keyed(keyed);
-            }
-            return factory;
+            return new HDRPercentileRanksAggregator.Factory(aggregationName, valuesSourceConfig, keys, numberOfSignificantValueDigits,
+                    keyed);
         } else {
             throw new AssertionError();
         }
     }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new TDigestPercentileRanksAggregator.Factory(null), new HDRPercentileRanksAggregator.Factory(null) };
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
index f482069..6fbb2cc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java
@@ -24,7 +24,8 @@ import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercenti
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
 import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregator;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.internal.SearchContext;
 
 /**
  *
@@ -37,7 +38,7 @@ public class PercentilesParser extends AbstractPercentilesParser {
         super(true);
     }
 
-    public final static double[] DEFAULT_PERCENTS = new double[] { 1, 5, 25, 50, 75, 95, 99 };
+    private final static double[] DEFAULT_PERCENTS = new double[] { 1, 5, 25, 50, 75, 95, 99 };
 
     @Override
     public String type() {
@@ -50,40 +51,18 @@ public class PercentilesParser extends AbstractPercentilesParser {
     }
 
     @Override
-    protected ValuesSourceAggregatorFactory<Numeric, ?> buildFactory(String aggregationName, double[] keys, PercentilesMethod method,
-            Double compression, Integer numberOfSignificantValueDigits, Boolean keyed) {
+    protected AggregatorFactory buildFactory(SearchContext context, String aggregationName, ValuesSourceConfig<Numeric> valuesSourceConfig,
+            double[] keys, PercentilesMethod method, Double compression, Integer numberOfSignificantValueDigits, boolean keyed) {
+        if (keys == null) {
+            keys = DEFAULT_PERCENTS;
+        }
         if (method == PercentilesMethod.TDIGEST) {
-            TDigestPercentilesAggregator.Factory factory = new TDigestPercentilesAggregator.Factory(aggregationName);
-            if (keys != null) {
-                factory.percents(keys);
-            }
-            if (compression != null) {
-                factory.compression(compression);
-            }
-            if (keyed != null) {
-                factory.keyed(keyed);
-            }
-            return factory;
+            return new TDigestPercentilesAggregator.Factory(aggregationName, valuesSourceConfig, keys, compression, keyed);
         } else if (method == PercentilesMethod.HDR) {
-            HDRPercentilesAggregator.Factory factory = new HDRPercentilesAggregator.Factory(aggregationName);
-            if (keys != null) {
-                factory.percents(keys);
-            }
-            if (numberOfSignificantValueDigits != null) {
-                factory.numberOfSignificantValueDigits(numberOfSignificantValueDigits);
-            }
-            if (keyed != null) {
-                factory.keyed(keyed);
-            }
-            return factory;
+            return new HDRPercentilesAggregator.Factory(aggregationName, valuesSourceConfig, keys, numberOfSignificantValueDigits, keyed);
         } else {
             throw new AssertionError();
         }
     }
 
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new TDigestPercentilesAggregator.Factory(null), new HDRPercentilesAggregator.Factory(null) };
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
index a0dc9ed..d132fdb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java
@@ -19,28 +19,19 @@
 package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
 
 import org.HdrHistogram.DoubleHistogram;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.metrics.percentiles.AbstractPercentilesParser;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksParser;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesMethod;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
@@ -83,61 +74,18 @@ public class HDRPercentileRanksAggregator extends AbstractHDRPercentilesAggregat
         }
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        private double[] values;
-        private int numberOfSignificantValueDigits = 3;
-        private boolean keyed = false;
+        private final double[] values;
+        private final int numberOfSignificantValueDigits;
+        private final boolean keyed;
 
-        public Factory(String name) {
-            super(name, InternalHDRPercentileRanks.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
-
-        /**
-         * Set the values to compute percentiles from.
-         */
-        public Factory values(double[] values) {
-            double[] sortedValues = Arrays.copyOf(values, values.length);
-            Arrays.sort(sortedValues);
-            this.values = sortedValues;
-            return this;
-        }
-
-        /**
-         * Get the values to compute percentiles from.
-         */
-        public double[] values() {
-            return values;
-        }
-
-        /**
-         * Set whether the XContent response should be keyed
-         */
-        public Factory keyed(boolean keyed) {
-            this.keyed = keyed;
-            return this;
-        }
-
-        /**
-         * Get whether the XContent response should be keyed
-         */
-        public boolean keyed() {
-            return keyed;
-        }
-
-        /**
-         * Expert: set the number of significant digits in the values.
-         */
-        public Factory numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double[] values,
+                int numberOfSignificantValueDigits, boolean keyed) {
+            super(name, InternalHDRPercentiles.TYPE.name(), valuesSourceConfig);
+            this.values = values;
             this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-            return this;
-        }
-
-        /**
-         * Expert: set the number of significant digits in the values.
-         */
-        public int numberOfSignificantValueDigits() {
-            return numberOfSignificantValueDigits;
+            this.keyed = keyed;
         }
 
         @Override
@@ -154,44 +102,5 @@ public class HDRPercentileRanksAggregator extends AbstractHDRPercentilesAggregat
             return new HDRPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, numberOfSignificantValueDigits,
                     keyed, config.formatter(), pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.values = in.readDoubleArray();
-            factory.keyed = in.readBoolean();
-            factory.numberOfSignificantValueDigits = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDoubleArray(values);
-            out.writeBoolean(keyed);
-            out.writeVInt(numberOfSignificantValueDigits);
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(PercentileRanksParser.VALUES_FIELD.getPreferredName(), values);
-            builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-            builder.startObject(PercentilesMethod.HDR.getName());
-            builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.deepEquals(values, other.values) && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(Arrays.hashCode(values), keyed, numberOfSignificantValueDigits);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
index 55f0c1f..d1c0a62 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java
@@ -19,28 +19,20 @@
 package org.elasticsearch.search.aggregations.metrics.percentiles.hdr;
 
 import org.HdrHistogram.DoubleHistogram;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.metrics.percentiles.AbstractPercentilesParser;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesMethod;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesParser;
+import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
@@ -84,61 +76,18 @@ public class HDRPercentilesAggregator extends AbstractHDRPercentilesAggregator {
                 formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        private double[] percents = PercentilesParser.DEFAULT_PERCENTS;
-        private int numberOfSignificantValueDigits = 3;
-        private boolean keyed = false;
+        private final double[] percents;
+        private final int numberOfSignificantValueDigits;
+        private final boolean keyed;
 
-        public Factory(String name) {
-            super(name, InternalHDRPercentiles.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
-
-        /**
-         * Set the percentiles to compute.
-         */
-        public Factory percents(double[] percents) {
-            double[] sortedPercents = Arrays.copyOf(percents, percents.length);
-            Arrays.sort(sortedPercents);
-            this.percents = sortedPercents;
-            return this;
-        }
-
-        /**
-         * Get the percentiles to compute.
-         */
-        public double[] percents() {
-            return percents;
-        }
-
-        /**
-         * Set whether the XContent response should be keyed
-         */
-        public Factory keyed(boolean keyed) {
-            this.keyed = keyed;
-            return this;
-        }
-
-        /**
-         * Get whether the XContent response should be keyed
-         */
-        public boolean keyed() {
-            return keyed;
-        }
-
-        /**
-         * Expert: set the number of significant digits in the values.
-         */
-        public Factory numberOfSignificantValueDigits(int numberOfSignificantValueDigits) {
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double[] percents,
+                int numberOfSignificantValueDigits, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.percents = percents;
             this.numberOfSignificantValueDigits = numberOfSignificantValueDigits;
-            return this;
-        }
-
-        /**
-         * Expert: set the number of significant digits in the values.
-         */
-        public int numberOfSignificantValueDigits() {
-            return numberOfSignificantValueDigits;
+            this.keyed = keyed;
         }
 
         @Override
@@ -155,44 +104,5 @@ public class HDRPercentilesAggregator extends AbstractHDRPercentilesAggregator {
             return new HDRPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, numberOfSignificantValueDigits,
                     keyed, config.formatter(), pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.percents = in.readDoubleArray();
-            factory.keyed = in.readBoolean();
-            factory.numberOfSignificantValueDigits = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDoubleArray(percents);
-            out.writeBoolean(keyed);
-            out.writeVInt(numberOfSignificantValueDigits);
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(PercentilesParser.PERCENTS_FIELD.getPreferredName(), percents);
-            builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-            builder.startObject(PercentilesMethod.HDR.getName());
-            builder.field(AbstractPercentilesParser.NUMBER_SIGNIFICANT_DIGITS_FIELD.getPreferredName(), numberOfSignificantValueDigits);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.deepEquals(percents, other.percents) && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(numberOfSignificantValueDigits, other.numberOfSignificantValueDigits);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(Arrays.hashCode(percents), keyed, numberOfSignificantValueDigits);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
index 8f56a2c..95c9f06 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java
@@ -18,28 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.metrics.percentiles.tdigest;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.metrics.percentiles.AbstractPercentilesParser;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentileRanksParser;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesMethod;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
@@ -78,63 +69,18 @@ public class TDigestPercentileRanksAggregator extends AbstractTDigestPercentiles
         }
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        private double[] values;
-        private double compression = 100.0;
-        private boolean keyed = false;
+        private final double[] values;
+        private final double compression;
+        private final boolean keyed;
 
-        public Factory(String name) {
-            super(name, InternalTDigestPercentileRanks.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
-
-        /**
-         * Set the values to compute percentiles from.
-         */
-        public Factory values(double[] values) {
-            double[] sortedValues = Arrays.copyOf(values, values.length);
-            Arrays.sort(sortedValues);
-            this.values = sortedValues;
-            return this;
-        }
-
-        /**
-         * Get the values to compute percentiles from.
-         */
-        public double[] values() {
-            return values;
-        }
-
-        /**
-         * Set whether the XContent response should be keyed
-         */
-        public Factory keyed(boolean keyed) {
-            this.keyed = keyed;
-            return this;
-        }
-
-        /**
-         * Get whether the XContent response should be keyed
-         */
-        public boolean keyed() {
-            return keyed;
-        }
-
-        /**
-         * Expert: set the compression. Higher values improve accuracy but also
-         * memory usage.
-         */
-        public Factory compression(double compression) {
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig,
+                double[] values, double compression, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.values = values;
             this.compression = compression;
-            return this;
-        }
-
-        /**
-         * Expert: set the compression. Higher values improve accuracy but also
-         * memory usage.
-         */
-        public double compression() {
-            return compression;
+            this.keyed = keyed;
         }
 
         @Override
@@ -151,44 +97,5 @@ public class TDigestPercentileRanksAggregator extends AbstractTDigestPercentiles
             return new TDigestPercentileRanksAggregator(name, valuesSource, aggregationContext, parent, values, compression, keyed,
                     config.formatter(), pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.values = in.readDoubleArray();
-            factory.keyed = in.readBoolean();
-            factory.compression = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDoubleArray(values);
-            out.writeBoolean(keyed);
-            out.writeDouble(compression);
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(PercentileRanksParser.VALUES_FIELD.getPreferredName(), values);
-            builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-            builder.startObject(PercentilesMethod.TDIGEST.getName());
-            builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.deepEquals(values, other.values) && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(compression, other.compression);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(Arrays.hashCode(values), keyed, compression);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
index ac864d6..43800bf 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java
@@ -18,28 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.metrics.percentiles.tdigest;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.metrics.percentiles.AbstractPercentilesParser;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesMethod;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesParser;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
@@ -78,63 +69,18 @@ public class TDigestPercentilesAggregator extends AbstractTDigestPercentilesAggr
         return new InternalTDigestPercentiles(name, keys, new TDigestState(compression), keyed, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        private double[] percents = PercentilesParser.DEFAULT_PERCENTS;
-        private double compression = 100.0;
-        private boolean keyed = false;
+        private final double[] percents;
+        private final double compression;
+        private final boolean keyed;
 
-        public Factory(String name) {
-            super(name, InternalTDigestPercentiles.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
-
-        /**
-         * Set the percentiles to compute.
-         */
-        public Factory percents(double[] percents) {
-            double[] sortedPercents = Arrays.copyOf(percents, percents.length);
-            Arrays.sort(sortedPercents);
-            this.percents = sortedPercents;
-            return this;
-        }
-
-        /**
-         * Get the percentiles to compute.
-         */
-        public double[] percents() {
-            return percents;
-        }
-
-        /**
-         * Set whether the XContent response should be keyed
-         */
-        public Factory keyed(boolean keyed) {
-            this.keyed = keyed;
-            return this;
-        }
-
-        /**
-         * Get whether the XContent response should be keyed
-         */
-        public boolean keyed() {
-            return keyed;
-        }
-
-        /**
-         * Expert: set the compression. Higher values improve accuracy but also
-         * memory usage.
-         */
-        public Factory compression(double compression) {
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig,
+                double[] percents, double compression, boolean keyed) {
+            super(name, InternalTDigestPercentiles.TYPE.name(), valuesSourceConfig);
+            this.percents = percents;
             this.compression = compression;
-            return this;
-        }
-
-        /**
-         * Expert: set the compression. Higher values improve accuracy but also
-         * memory usage.
-         */
-        public double compression() {
-            return compression;
+            this.keyed = keyed;
         }
 
         @Override
@@ -151,44 +97,5 @@ public class TDigestPercentilesAggregator extends AbstractTDigestPercentilesAggr
             return new TDigestPercentilesAggregator(name, valuesSource, aggregationContext, parent, percents, compression, keyed,
                     config.formatter(), pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.percents = in.readDoubleArray();
-            factory.keyed = in.readBoolean();
-            factory.compression = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDoubleArray(percents);
-            out.writeBoolean(keyed);
-            out.writeDouble(compression);
-        }
-
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(PercentilesParser.PERCENTS_FIELD.getPreferredName(), percents);
-            builder.field(AbstractPercentilesParser.KEYED_FIELD.getPreferredName(), keyed);
-            builder.startObject(PercentilesMethod.TDIGEST.getName());
-            builder.field(AbstractPercentilesParser.COMPRESSION_FIELD.getPreferredName(), compression);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.deepEquals(percents, other.percents) && Objects.equals(keyed, other.keyed)
-                    && Objects.equals(compression, other.compression);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(Arrays.hashCode(percents), keyed, compression);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
index 97852b4..6603c62 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
@@ -20,9 +20,6 @@
 package org.elasticsearch.search.aggregations.metrics.scripted;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.LeafSearchScript;
 import org.elasticsearch.script.Script;
@@ -47,7 +44,6 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
-import java.util.Objects;
 
 public class ScriptedMetricAggregator extends MetricsAggregator {
 
@@ -110,7 +106,7 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
         return new InternalScriptedMetric(name, null, reduceScript, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
         private Script initScript;
         private Script mapScript;
@@ -118,85 +114,14 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
         private Script reduceScript;
         private Map<String, Object> params;
 
-        public Factory(String name) {
-            super(name, InternalScriptedMetric.TYPE);
-        }
-
-        /**
-         * Set the <tt>init</tt> script.
-         */
-        public Factory initScript(Script initScript) {
+        public Factory(String name, Script initScript, Script mapScript, Script combineScript, Script reduceScript,
+                Map<String, Object> params) {
+            super(name, InternalScriptedMetric.TYPE.name());
             this.initScript = initScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>init</tt> script.
-         */
-        public Script initScript() {
-            return initScript;
-        }
-
-        /**
-         * Set the <tt>map</tt> script.
-         */
-        public Factory mapScript(Script mapScript) {
             this.mapScript = mapScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>map</tt> script.
-         */
-        public Script mapScript() {
-            return mapScript;
-        }
-
-        /**
-         * Set the <tt>combine</tt> script.
-         */
-        public Factory combineScript(Script combineScript) {
             this.combineScript = combineScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>combine</tt> script.
-         */
-        public Script combineScript() {
-            return combineScript;
-        }
-
-        /**
-         * Set the <tt>reduce</tt> script.
-         */
-        public Factory reduceScript(Script reduceScript) {
             this.reduceScript = reduceScript;
-            return this;
-        }
-
-        /**
-         * Get the <tt>reduce</tt> script.
-         */
-        public Script reduceScript() {
-            return reduceScript;
-        }
-
-        /**
-         * Set parameters that will be available in the <tt>init</tt>,
-         * <tt>map</tt> and <tt>combine</tt> phases.
-         */
-        public Factory params(Map<String, Object> params) {
             this.params = params;
-            return this;
-        }
-
-        /**
-         * Get parameters that will be available in the <tt>init</tt>,
-         * <tt>map</tt> and <tt>combine</tt> phases.
-         */
-        public Map<String, Object> params() {
-            return params;
         }
 
         @Override
@@ -264,73 +189,6 @@ public class ScriptedMetricAggregator extends MetricsAggregator {
             return clone;
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
-            builder.startObject();
-            if (initScript != null) {
-                builder.field(ScriptedMetricParser.INIT_SCRIPT_FIELD.getPreferredName(), initScript);
-            }
-
-            if (mapScript != null) {
-                builder.field(ScriptedMetricParser.MAP_SCRIPT_FIELD.getPreferredName(), mapScript);
-            }
-
-            if (combineScript != null) {
-                builder.field(ScriptedMetricParser.COMBINE_SCRIPT_FIELD.getPreferredName(), combineScript);
-            }
-
-            if (reduceScript != null) {
-                builder.field(ScriptedMetricParser.REDUCE_SCRIPT_FIELD.getPreferredName(), reduceScript);
-            }
-            if (params != null) {
-                builder.field(ScriptedMetricParser.PARAMS_FIELD.getPreferredName());
-                builder.map(params);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.initScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.mapScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.combineScript = in.readOptionalStreamable(Script.SUPPLIER);
-            factory.reduceScript = in.readOptionalStreamable(Script.SUPPLIER);
-            if (in.readBoolean()) {
-                factory.params = in.readMap();
-            }
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalStreamable(initScript);
-            out.writeOptionalStreamable(mapScript);
-            out.writeOptionalStreamable(combineScript);
-            out.writeOptionalStreamable(reduceScript);
-            boolean hasParams = params != null;
-            out.writeBoolean(hasParams);
-            if (hasParams) {
-                out.writeMap(params);
-            }
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(initScript, mapScript, combineScript, reduceScript, params);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(initScript, other.initScript)
-                    && Objects.equals(mapScript, other.mapScript)
-                    && Objects.equals(combineScript, other.combineScript)
-                    && Objects.equals(reduceScript, other.reduceScript)
-                    && Objects.equals(params, other.params);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
new file mode 100644
index 0000000..803123f
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.scripted;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.metrics.MetricsAggregationBuilder;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * Builder for the {@link ScriptedMetric} aggregation.
+ */
+public class ScriptedMetricBuilder extends MetricsAggregationBuilder<ScriptedMetricBuilder> {
+
+    private Script initScript = null;
+    private Script mapScript = null;
+    private Script combineScript = null;
+    private Script reduceScript = null;
+    private Map<String, Object> params = null;
+
+    /**
+     * Sole constructor.
+     */
+    public ScriptedMetricBuilder(String name) {
+        super(name, InternalScriptedMetric.TYPE.name());
+    }
+
+    /**
+     * Set the <tt>init</tt> script.
+     */
+    public ScriptedMetricBuilder initScript(Script initScript) {
+        this.initScript = initScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>map</tt> script.
+     */
+    public ScriptedMetricBuilder mapScript(Script mapScript) {
+        this.mapScript = mapScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>combine</tt> script.
+     */
+    public ScriptedMetricBuilder combineScript(Script combineScript) {
+        this.combineScript = combineScript;
+        return this;
+    }
+
+    /**
+     * Set the <tt>reduce</tt> script.
+     */
+    public ScriptedMetricBuilder reduceScript(Script reduceScript) {
+        this.reduceScript = reduceScript;
+        return this;
+    }
+
+    /**
+     * Set parameters that will be available in the <tt>init</tt>, <tt>map</tt>
+     * and <tt>combine</tt> phases.
+     */
+    public ScriptedMetricBuilder params(Map<String, Object> params) {
+        this.params = params;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params builderParams) throws IOException {
+
+        if (initScript != null) {
+            builder.field(ScriptedMetricParser.INIT_SCRIPT_FIELD.getPreferredName(), initScript);
+        }
+
+        if (mapScript != null) {
+            builder.field(ScriptedMetricParser.MAP_SCRIPT_FIELD.getPreferredName(), mapScript);
+        }
+
+        if (combineScript != null) {
+            builder.field(ScriptedMetricParser.COMBINE_SCRIPT_FIELD.getPreferredName(), combineScript);
+        }
+
+        if (reduceScript != null) {
+            builder.field(ScriptedMetricParser.REDUCE_SCRIPT_FIELD.getPreferredName(), reduceScript);
+        }
+        if (params != null) {
+            builder.field(ScriptedMetricParser.PARAMS_FIELD.getPreferredName());
+            builder.map(params);
+        }
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
index f5e6172..528078c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricParser.java
@@ -20,14 +20,14 @@
 package org.elasticsearch.search.aggregations.metrics.scripted;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.HashSet;
@@ -35,7 +35,7 @@ import java.util.Map;
 import java.util.Set;
 
 public class ScriptedMetricParser implements Aggregator.Parser {
-
+    
     public static final String INIT_SCRIPT = "init_script";
     public static final String MAP_SCRIPT = "map_script";
     public static final String COMBINE_SCRIPT = "combine_script";
@@ -54,7 +54,7 @@ public class ScriptedMetricParser implements Aggregator.Parser {
     }
 
     @Override
-    public AggregatorFactory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
         Script initScript = null;
         Script mapScript = null;
         Script combineScript = null;
@@ -87,16 +87,17 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, REDUCE_PARAMS_FIELD)) {
                   reduceParams = parser.map();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token.isValue()) {
                 if (!scriptParameterParser.token(currentFieldName, token, parser, context.parseFieldMatcher())) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + aggregationName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + aggregationName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
 
@@ -106,8 +107,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 initScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (initScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "init_script params are not supported. Parameters for the init_script must be specified in the params field on the scripted_metric aggregator not inside the init_script object");
+            throw new SearchParseException(
+                    context,
+                    "init_script params are not supported. Parameters for the init_script must be specified in the params field on the scripted_metric aggregator not inside the init_script object",
+                    parser.getTokenLocation());
         }
 
         if (mapScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -116,8 +119,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 mapScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (mapScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "map_script params are not supported. Parameters for the map_script must be specified in the params field on the scripted_metric aggregator not inside the map_script object");
+            throw new SearchParseException(
+                    context,
+                    "map_script params are not supported. Parameters for the map_script must be specified in the params field on the scripted_metric aggregator not inside the map_script object",
+                    parser.getTokenLocation());
         }
 
         if (combineScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -126,8 +131,10 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 combineScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), params);
             }
         } else if (combineScript.getParams() != null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "combine_script params are not supported. Parameters for the combine_script must be specified in the params field on the scripted_metric aggregator not inside the combine_script object");
+            throw new SearchParseException(
+                    context,
+                    "combine_script params are not supported. Parameters for the combine_script must be specified in the params field on the scripted_metric aggregator not inside the combine_script object",
+                    parser.getTokenLocation());
         }
 
         if (reduceScript == null) { // Didn't find anything using the new API so try using the old one instead
@@ -136,23 +143,11 @@ public class ScriptedMetricParser implements Aggregator.Parser {
                 reduceScript = new Script(scriptValue.script(), scriptValue.scriptType(), scriptParameterParser.lang(), reduceParams);
             }
         }
-
+        
         if (mapScript == null) {
-            throw new ParsingException(parser.getTokenLocation(), "map_script field is required in [" + aggregationName + "].");
+            throw new SearchParseException(context, "map_script field is required in [" + aggregationName + "].", parser.getTokenLocation());
         }
-
-        ScriptedMetricAggregator.Factory factory = new ScriptedMetricAggregator.Factory(aggregationName);
-        factory.initScript(initScript);
-        factory.mapScript(mapScript);
-        factory.combineScript(combineScript);
-        factory.reduceScript(reduceScript);
-        factory.params(params);
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new ScriptedMetricAggregator.Factory(null) };
+        return new ScriptedMetricAggregator.Factory(aggregationName, initScript, mapScript, combineScript, reduceScript, params);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
index 0e4e24a..6e648cb 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
@@ -19,13 +19,10 @@
 package org.elasticsearch.search.aggregations.metrics.stats;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -34,10 +31,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -159,10 +155,10 @@ public class StatsAggregator extends NumericMetricsAggregator.MultiValue {
         return new InternalStats(name, 0, 0, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalStats.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalStats.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
@@ -177,32 +173,6 @@ public class StatsAggregator extends NumericMetricsAggregator.MultiValue {
                 throws IOException {
             return new StatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new StatsAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java
new file mode 100644
index 0000000..3b6e549
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.stats;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Stats} aggregation.
+ */
+public class StatsBuilder extends ValuesSourceMetricsAggregationBuilder<StatsBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public StatsBuilder(String name) {
+        super(name, InternalStats.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
index db08df7..86c85e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
@@ -18,45 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.stats;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class StatsParser extends NumericValuesSourceParser {
+public class StatsParser extends NumericValuesSourceMetricsAggregatorParser<InternalStats> {
 
     public StatsParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalStats.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected StatsAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new StatsAggregator.Factory(aggregationName);
+        super(InternalStats.TYPE);
     }
 
     @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new StatsAggregator.Factory(null) };
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new StatsAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
index 3ab11a7..86a6481 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java
@@ -19,14 +19,10 @@
 package org.elasticsearch.search.aggregations.metrics.stats.extended;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -35,24 +31,20 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
 public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue {
 
-    public static final ParseField SIGMA_FIELD = new ParseField("sigma");
-
     final ValuesSource.Numeric valuesSource;
     final ValueFormatter formatter;
     final double sigma;
@@ -196,21 +188,14 @@ public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue
         Releasables.close(counts, maxes, mins, sumOfSqrs, sums);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        private double sigma = 2.0;
+        private final double sigma;
 
-        public Factory(String name) {
-            super(name, InternalExtendedStats.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig, double sigma) {
+            super(name, InternalExtendedStats.TYPE.name(), valuesSourceConfig);
 
-        public Factory sigma(double sigma) {
             this.sigma = sigma;
-            return this;
-        }
-
-        public double sigma() {
-            return sigma;
         }
 
         @Override
@@ -227,35 +212,5 @@ public class ExtendedStatsAggregator extends NumericMetricsAggregator.MultiValue
             return new ExtendedStatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, sigma,
                     pipelineAggregators, metaData);
         }
-
-        @Override
-        protected Factory innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) throws IOException {
-            ExtendedStatsAggregator.Factory factory = new ExtendedStatsAggregator.Factory(name);
-            factory.sigma = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(sigma);
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(SIGMA_FIELD.getPreferredName(), sigma);
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(sigma);
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(sigma, other.sigma);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java
new file mode 100644
index 0000000..28f4d73
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsBuilder.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.stats.extended;
+
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link ExtendedStats} aggregation.
+ */
+public class ExtendedStatsBuilder extends ValuesSourceMetricsAggregationBuilder<ExtendedStatsBuilder> {
+
+    private Double sigma;
+
+    /**
+     * Sole constructor.
+     */
+    public ExtendedStatsBuilder(String name) {
+        super(name, InternalExtendedStats.TYPE.name());
+    }
+
+    public ExtendedStatsBuilder sigma(double sigma) {
+        this.sigma = sigma;
+        return this;
+    }
+
+    @Override
+    protected void internalXContent(XContentBuilder builder, Params params) throws IOException {
+        super.internalXContent(builder, params);
+
+        if (sigma != null) {
+            builder.field("sigma", sigma);
+        }
+
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
index 91e7db2..b29ce08 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java
@@ -19,55 +19,65 @@
 package org.elasticsearch.search.aggregations.metrics.stats.extended;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class ExtendedStatsParser extends NumericValuesSourceParser {
+public class ExtendedStatsParser  implements Aggregator.Parser {
 
-    public ExtendedStatsParser() {
-        super(true, true, false);
-    }
+    static final ParseField SIGMA = new ParseField("sigma");
 
     @Override
     public String type() {
         return InternalExtendedStats.TYPE.name();
     }
 
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config, double sigma) {
+        return new ExtendedStatsAggregator.Factory(aggregationName, config, sigma);
+    }
+
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (parseFieldMatcher.match(currentFieldName, ExtendedStatsAggregator.SIGMA_FIELD)) {
-            if (token.isValue()) {
-                otherOptions.put(ExtendedStatsAggregator.SIGMA_FIELD, parser.doubleValue());
-                return true;
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser<ValuesSource.Numeric> vsParser = ValuesSourceParser.numeric(aggregationName, InternalExtendedStats.TYPE, context).formattable(true)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        double sigma = 2.0;
+
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (vsParser.token(currentFieldName, token, parser)) {
+                continue;
+            } else if (token == XContentParser.Token.VALUE_NUMBER) {
+                if (context.parseFieldMatcher().match(currentFieldName, SIGMA)) {
+                    sigma = parser.doubleValue();
+                } else {
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
+                }
+            } else {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
             }
         }
-        return false;
-    }
 
-    @Override
-    protected ExtendedStatsAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        ExtendedStatsAggregator.Factory factory = new ExtendedStatsAggregator.Factory(aggregationName);
-        Double sigma = (Double) otherOptions.get(ExtendedStatsAggregator.SIGMA_FIELD);
-        if (sigma != null) {
-            factory.sigma(sigma);
+        if (sigma < 0) {
+            throw new SearchParseException(context, "[sigma] must not be negative. Value provided was" + sigma, parser.getTokenLocation());
         }
-        return factory;
-    }
 
-    @Override
-    public AggregatorFactory<?>[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new ExtendedStatsAggregator.Factory(null) };
+        return createFactory(aggregationName, vsParser.config(), sigma);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
index 441c6be..8a6b40c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.search.aggregations.metrics.sum;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -33,11 +30,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSource.Numeric;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -110,10 +105,10 @@ public class SumAggregator extends NumericMetricsAggregator.SingleValue {
         return new InternalSum(name, 0.0, formatter, pipelineAggregators(), metaData());
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, Factory> {
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
 
-        public Factory(String name) {
-            super(name, InternalSum.TYPE, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalSum.TYPE.name(), valuesSourceConfig);
         }
 
         @Override
@@ -128,32 +123,6 @@ public class SumAggregator extends NumericMetricsAggregator.SingleValue {
                 throws IOException {
             return new SumAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
         }
-
-        @Override
-        protected ValuesSourceAggregatorFactory<Numeric, Factory> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new SumAggregator.Factory(name);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java
new file mode 100644
index 0000000..53e8780
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumBuilder.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.sum;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link Sum} aggregation.
+ */
+public class SumBuilder extends ValuesSourceMetricsAggregationBuilder<SumBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public SumBuilder(String name) {
+        super(name, InternalSum.TYPE.name());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
index b633023..b43285a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java
@@ -18,45 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.metrics.sum;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.NumericValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.io.IOException;
-import java.util.Map;
+import org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 
 /**
  *
  */
-public class SumParser extends NumericValuesSourceParser {
+public class SumParser extends NumericValuesSourceMetricsAggregatorParser<InternalSum> {
 
     public SumParser() {
-        super(true, true, false);
-    }
-
-    @Override
-    public String type() {
-        return InternalSum.TYPE.name();
-    }
-
-    @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected SumAggregator.Factory createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new SumAggregator.Factory(aggregationName);
+        super(InternalSum.TYPE);
     }
 
     @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new SumAggregator.Factory(null) };
+    protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
+        return new SumAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
index 2bbb44b..82dea48 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java
@@ -30,23 +30,9 @@ import org.apache.lucene.search.TopDocsCollector;
 import org.apache.lucene.search.TopFieldCollector;
 import org.apache.lucene.search.TopFieldDocs;
 import org.apache.lucene.search.TopScoreDocCollector;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.util.LongObjectPagedHashMap;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.AggregationInitializationException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
@@ -57,29 +43,15 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.MetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
 import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext.FieldDataField;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHits;
 import org.elasticsearch.search.internal.SubSearchContext;
-import org.elasticsearch.search.sort.SortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.sort.SortParseElement;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  */
@@ -209,598 +181,26 @@ public class TopHitsAggregator extends MetricsAggregator {
         Releasables.close(topDocsCollectors);
     }
 
-    public static class Factory extends AggregatorFactory<Factory> {
+    public static class Factory extends AggregatorFactory {
 
-        private static final SortParseElement sortParseElement = new SortParseElement();
-        private int from = 0;
-        private int size = 3;
-        private boolean explain = false;
-        private boolean version = false;
-        private boolean trackScores = false;
-        private List<BytesReference> sorts = null;
-        private HighlightBuilder highlightBuilder;
-        private List<String> fieldNames;
-        private List<String> fieldDataFields;
-        private List<ScriptField> scriptFields;
-        private FetchSourceContext fetchSourceContext;
+        private final FetchPhase fetchPhase;
+        private final SubSearchContext subSearchContext;
 
-        public Factory(String name) {
-            super(name, InternalTopHits.TYPE);
-        }
-
-        /**
-         * From index to start the search from. Defaults to <tt>0</tt>.
-         */
-        public Factory from(int from) {
-            this.from = from;
-            return this;
-        }
-
-        /**
-         * Gets the from index to start the search from.
-         **/
-        public int from() {
-            return from;
-        }
-
-        /**
-         * The number of search hits to return. Defaults to <tt>10</tt>.
-         */
-        public Factory size(int size) {
-            this.size = size;
-            return this;
-        }
-
-        /**
-         * Gets the number of search hits to return.
-         */
-        public int size() {
-            return size;
-        }
-
-        /**
-         * Adds a sort against the given field name and the sort ordering.
-         *
-         * @param name
-         *            The name of the field
-         * @param order
-         *            The sort ordering
-         */
-        public Factory sort(String name, SortOrder order) {
-            sort(SortBuilders.fieldSort(name).order(order));
-            return this;
-        }
-
-        /**
-         * Add a sort against the given field name.
-         *
-         * @param name
-         *            The name of the field to sort by
-         */
-        public Factory sort(String name) {
-            sort(SortBuilders.fieldSort(name));
-            return this;
-        }
-
-        /**
-         * Adds a sort builder.
-         */
-        public Factory sort(SortBuilder sort) {
-            try {
-                if (sorts == null) {
-                    sorts = new ArrayList<>();
-                }
-                // NORELEASE when sort has been refactored and made writeable
-                // add the sortBuilcer to the List directly instead of
-                // serialising to XContent
-                XContentBuilder builder = XContentFactory.jsonBuilder();
-                builder.startObject();
-                sort.toXContent(builder, EMPTY_PARAMS);
-                builder.endObject();
-                sorts.add(builder.bytes());
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-            return this;
-        }
-
-        /**
-         * Adds a sort builder.
-         */
-        public Factory sorts(List<BytesReference> sorts) {
-            if (this.sorts == null) {
-                this.sorts = new ArrayList<>();
-            }
-            for (BytesReference sort : sorts) {
-                this.sorts.add(sort);
-            }
-            return this;
-        }
-
-        /**
-         * Gets the bytes representing the sort builders for this request.
-         */
-        public List<BytesReference> sorts() {
-            return sorts;
-        }
-
-        /**
-         * Adds highlight to perform as part of the search.
-         */
-        public Factory highlighter(HighlightBuilder highlightBuilder) {
-            this.highlightBuilder = highlightBuilder;
-            return this;
-        }
-
-        /**
-         * Gets the hightlighter builder for this request.
-         */
-        public HighlightBuilder highlighter() {
-            return highlightBuilder;
-        }
-
-        /**
-         * Indicates whether the response should contain the stored _source for
-         * every hit
-         */
-        public Factory fetchSource(boolean fetch) {
-            if (this.fetchSourceContext == null) {
-                this.fetchSourceContext = new FetchSourceContext(fetch);
-            } else {
-                this.fetchSourceContext.fetchSource(fetch);
-            }
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an
-         * "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param include
-         *            An optional include (optionally wildcarded) pattern to
-         *            filter the returned _source
-         * @param exclude
-         *            An optional exclude (optionally wildcarded) pattern to
-         *            filter the returned _source
-         */
-        public Factory fetchSource(@Nullable String include, @Nullable String exclude) {
-            fetchSource(include == null ? Strings.EMPTY_ARRAY : new String[] { include },
-                    exclude == null ? Strings.EMPTY_ARRAY : new String[] { exclude });
-            return this;
-        }
-
-        /**
-         * Indicate that _source should be returned with every hit, with an
-         * "include" and/or "exclude" set which can include simple wildcard
-         * elements.
-         *
-         * @param includes
-         *            An optional list of include (optionally wildcarded)
-         *            pattern to filter the returned _source
-         * @param excludes
-         *            An optional list of exclude (optionally wildcarded)
-         *            pattern to filter the returned _source
-         */
-        public Factory fetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
-            fetchSourceContext = new FetchSourceContext(includes, excludes);
-            return this;
-        }
-
-        /**
-         * Indicate how the _source should be fetched.
-         */
-        public Factory fetchSource(@Nullable FetchSourceContext fetchSourceContext) {
-            this.fetchSourceContext = fetchSourceContext;
-            return this;
-        }
-
-        /**
-         * Gets the {@link FetchSourceContext} which defines how the _source
-         * should be fetched.
-         */
-        public FetchSourceContext fetchSource() {
-            return fetchSourceContext;
-        }
-
-        /**
-         * Adds a field to load and return (note, it must be stored) as part of
-         * the search request. If none are specified, the source of the document
-         * will be return.
-         */
-        public Factory field(String name) {
-            if (fieldNames == null) {
-                fieldNames = new ArrayList<>();
-            }
-            fieldNames.add(name);
-            return this;
-        }
-
-        /**
-         * Sets the fields to load and return as part of the search request. If
-         * none are specified, the source of the document will be returned.
-         */
-        public Factory fields(List<String> fields) {
-            this.fieldNames = fields;
-            return this;
-        }
-
-        /**
-         * Sets no fields to be loaded, resulting in only id and type to be
-         * returned per field.
-         */
-        public Factory noFields() {
-            this.fieldNames = Collections.emptyList();
-            return this;
-        }
-
-        /**
-         * Gets the fields to load and return as part of the search request.
-         */
-        public List<String> fields() {
-            return fieldNames;
-        }
-
-        /**
-         * Adds a field to load from the field data cache and return as part of
-         * the search request.
-         */
-        public Factory fieldDataField(String name) {
-            if (fieldDataFields == null) {
-                fieldDataFields = new ArrayList<>();
-            }
-            fieldDataFields.add(name);
-            return this;
-        }
-
-        /**
-         * Adds fields to load from the field data cache and return as part of
-         * the search request.
-         */
-        public Factory fieldDataFields(List<String> names) {
-            if (fieldDataFields == null) {
-                fieldDataFields = new ArrayList<>();
-            }
-            fieldDataFields.addAll(names);
-            return this;
-        }
-
-        /**
-         * Gets the field-data fields.
-         */
-        public List<String> fieldDataFields() {
-            return fieldDataFields;
-        }
-
-        /**
-         * Adds a script field under the given name with the provided script.
-         *
-         * @param name
-         *            The name of the field
-         * @param script
-         *            The script
-         */
-        public Factory scriptField(String name, Script script) {
-            scriptField(name, script, false);
-            return this;
-        }
-
-        /**
-         * Adds a script field under the given name with the provided script.
-         *
-         * @param name
-         *            The name of the field
-         * @param script
-         *            The script
-         */
-        public Factory scriptField(String name, Script script, boolean ignoreFailure) {
-            if (scriptFields == null) {
-                scriptFields = new ArrayList<>();
-            }
-            scriptFields.add(new ScriptField(name, script, ignoreFailure));
-            return this;
-        }
-
-        public Factory scriptFields(List<ScriptField> scriptFields) {
-            if (this.scriptFields == null) {
-                this.scriptFields = new ArrayList<>();
-            }
-            this.scriptFields.addAll(scriptFields);
-            return this;
-        }
-
-        /**
-         * Gets the script fields.
-         */
-        public List<ScriptField> scriptFields() {
-            return scriptFields;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned
-         * with an explanation of the hit (ranking).
-         */
-        public Factory explain(boolean explain) {
-            this.explain = explain;
-            return this;
-        }
-
-        /**
-         * Indicates whether each search hit will be returned with an
-         * explanation of the hit (ranking)
-         */
-        public boolean explain() {
-            return explain;
-        }
-
-        /**
-         * Should each {@link org.elasticsearch.search.SearchHit} be returned
-         * with a version associated with it.
-         */
-        public Factory version(boolean version) {
-            this.version = version;
-            return this;
-        }
-
-        /**
-         * Indicates whether the document's version will be included in the
-         * search hits.
-         */
-        public boolean version() {
-            return version;
-        }
-
-        /**
-         * Applies when sorting, and controls if scores will be tracked as well.
-         * Defaults to <tt>false</tt>.
-         */
-        public Factory trackScores(boolean trackScores) {
-            this.trackScores = trackScores;
-            return this;
-        }
-
-        /**
-         * Indicates whether scores will be tracked for this request.
-         */
-        public boolean trackScores() {
-            return trackScores;
+        public Factory(String name, FetchPhase fetchPhase, SubSearchContext subSearchContext) {
+            super(name, InternalTopHits.TYPE.name());
+            this.fetchPhase = fetchPhase;
+            this.subSearchContext = subSearchContext;
         }
 
         @Override
         public Aggregator createInternal(AggregationContext aggregationContext, Aggregator parent, boolean collectsFromSingleBucket,
                 List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-            SubSearchContext subSearchContext = new SubSearchContext(aggregationContext.searchContext());
-            subSearchContext.explain(explain);
-            subSearchContext.version(version);
-            subSearchContext.trackScores(trackScores);
-            subSearchContext.from(from);
-            subSearchContext.size(size);
-            if (sorts != null) {
-                XContentParser completeSortParser = null;
-                try {
-                    XContentBuilder completeSortBuilder = XContentFactory.jsonBuilder();
-                    completeSortBuilder.startObject();
-                    completeSortBuilder.startArray("sort");
-                    for (BytesReference sort : sorts) {
-                        XContentParser parser = XContentFactory.xContent(sort).createParser(sort);
-                        parser.nextToken();
-                        completeSortBuilder.copyCurrentStructure(parser);
-                    }
-                    completeSortBuilder.endArray();
-                    completeSortBuilder.endObject();
-                    BytesReference completeSortBytes = completeSortBuilder.bytes();
-                    completeSortParser = XContentFactory.xContent(completeSortBytes).createParser(completeSortBytes);
-                    completeSortParser.nextToken();
-                    completeSortParser.nextToken();
-                    completeSortParser.nextToken();
-                    sortParseElement.parse(completeSortParser, subSearchContext);
-                } catch (Exception e) {
-                    XContentLocation location = completeSortParser != null ? completeSortParser.getTokenLocation() : null;
-                    throw new ParsingException(location, "failed to parse sort source in aggregation [" + name + "]", e);
-                }
-            }
-            if (fieldNames != null) {
-                subSearchContext.fieldNames().addAll(fieldNames);
-            }
-            if (fieldDataFields != null) {
-                FieldDataFieldsContext fieldDataFieldsContext = subSearchContext
-                        .getFetchSubPhaseContext(FieldDataFieldsFetchSubPhase.CONTEXT_FACTORY);
-                for (String field : fieldDataFields) {
-                    fieldDataFieldsContext.add(new FieldDataField(field));
-                }
-                fieldDataFieldsContext.setHitExecutionNeeded(true);
-            }
-            if (scriptFields != null) {
-                for (ScriptField field : scriptFields) {
-                    SearchScript searchScript = subSearchContext.scriptService().search(subSearchContext.lookup(), field.script(),
-                            ScriptContext.Standard.SEARCH, Collections.emptyMap());
-                    subSearchContext.scriptFields().add(new org.elasticsearch.search.fetch.script.ScriptFieldsContext.ScriptField(
-                            field.fieldName(), searchScript, field.ignoreFailure()));
-                }
-            }
-            if (fetchSourceContext != null) {
-                subSearchContext.fetchSourceContext(fetchSourceContext);
-            }
-            if (highlightBuilder != null) {
-                subSearchContext.highlight(highlightBuilder.build(aggregationContext.searchContext().indexShard().getQueryShardContext()));
-            }
-            return new TopHitsAggregator(aggregationContext.searchContext().fetchPhase(), subSearchContext, name, aggregationContext,
-                    parent, pipelineAggregators, metaData);
+            return new TopHitsAggregator(fetchPhase, subSearchContext, name, aggregationContext, parent, pipelineAggregators, metaData);
         }
 
         @Override
-        public Factory subFactories(AggregatorFactories subFactories) {
+        public AggregatorFactory subFactories(AggregatorFactories subFactories) {
             throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
         }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject();
-            builder.field(SearchSourceBuilder.FROM_FIELD.getPreferredName(), from);
-            builder.field(SearchSourceBuilder.SIZE_FIELD.getPreferredName(), size);
-            builder.field(SearchSourceBuilder.VERSION_FIELD.getPreferredName(), version);
-            builder.field(SearchSourceBuilder.EXPLAIN_FIELD.getPreferredName(), explain);
-            if (fetchSourceContext != null) {
-                builder.field(SearchSourceBuilder._SOURCE_FIELD.getPreferredName(), fetchSourceContext);
-            }
-            if (fieldNames != null) {
-                if (fieldNames.size() == 1) {
-                    builder.field(SearchSourceBuilder.FIELDS_FIELD.getPreferredName(), fieldNames.get(0));
-                } else {
-                    builder.startArray(SearchSourceBuilder.FIELDS_FIELD.getPreferredName());
-                    for (String fieldName : fieldNames) {
-                        builder.value(fieldName);
-                    }
-                    builder.endArray();
-                }
-            }
-            if (fieldDataFields != null) {
-                builder.startArray(SearchSourceBuilder.FIELDDATA_FIELDS_FIELD.getPreferredName());
-                for (String fieldDataField : fieldDataFields) {
-                    builder.value(fieldDataField);
-                }
-                builder.endArray();
-            }
-            if (scriptFields != null) {
-                builder.startObject(SearchSourceBuilder.SCRIPT_FIELDS_FIELD.getPreferredName());
-                for (ScriptField scriptField : scriptFields) {
-                    scriptField.toXContent(builder, params);
-                }
-                builder.endObject();
-            }
-            if (sorts != null) {
-                builder.startArray(SearchSourceBuilder.SORT_FIELD.getPreferredName());
-                for (BytesReference sort : sorts) {
-                    XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(sort);
-                    parser.nextToken();
-                    builder.copyCurrentStructure(parser);
-                }
-                builder.endArray();
-            }
-            if (trackScores) {
-                builder.field(SearchSourceBuilder.TRACK_SCORES_FIELD.getPreferredName(), true);
-            }
-            if (highlightBuilder != null) {
-                this.highlightBuilder.toXContent(builder, params);
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        protected AggregatorFactory doReadFrom(String name, StreamInput in) throws IOException {
-            Factory factory = new Factory(name);
-            factory.explain = in.readBoolean();
-            factory.fetchSourceContext = FetchSourceContext.optionalReadFromStream(in);
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<String> fieldDataFields = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    fieldDataFields.add(in.readString());
-                }
-                factory.fieldDataFields = fieldDataFields;
-            }
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<String> fieldNames = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    fieldNames.add(in.readString());
-                }
-                factory.fieldNames = fieldNames;
-            }
-            factory.from = in.readVInt();
-            if (in.readBoolean()) {
-                factory.highlightBuilder = HighlightBuilder.PROTOTYPE.readFrom(in);
-            }
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<ScriptField> scriptFields = new ArrayList<>(size);
-                for (int i = 0; i < size; i++) {
-                    scriptFields.add(ScriptField.PROTOTYPE.readFrom(in));
-                }
-                factory.scriptFields = scriptFields;
-            }
-            factory.size = in.readVInt();
-            if (in.readBoolean()) {
-                int size = in.readVInt();
-                List<BytesReference> sorts = new ArrayList<>();
-                for (int i = 0; i < size; i++) {
-                    sorts.add(in.readBytesReference());
-                }
-                factory.sorts = sorts;
-            }
-            factory.trackScores = in.readBoolean();
-            factory.version = in.readBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeBoolean(explain);
-            FetchSourceContext.optionalWriteToStream(fetchSourceContext, out);
-            boolean hasFieldDataFields = fieldDataFields != null;
-            out.writeBoolean(hasFieldDataFields);
-            if (hasFieldDataFields) {
-                out.writeVInt(fieldDataFields.size());
-                for (String fieldName : fieldDataFields) {
-                    out.writeString(fieldName);
-                }
-            }
-            boolean hasFieldNames = fieldNames != null;
-            out.writeBoolean(hasFieldNames);
-            if (hasFieldNames) {
-                out.writeVInt(fieldNames.size());
-                for (String fieldName : fieldNames) {
-                    out.writeString(fieldName);
-                }
-            }
-            out.writeVInt(from);
-            boolean hasHighlighter = highlightBuilder != null;
-            out.writeBoolean(hasHighlighter);
-            if (hasHighlighter) {
-                highlightBuilder.writeTo(out);
-            }
-            boolean hasScriptFields = scriptFields != null;
-            out.writeBoolean(hasScriptFields);
-            if (hasScriptFields) {
-                out.writeVInt(scriptFields.size());
-                for (ScriptField scriptField : scriptFields) {
-                    scriptField.writeTo(out);
-                }
-            }
-            out.writeVInt(size);
-            boolean hasSorts = sorts != null;
-            out.writeBoolean(hasSorts);
-            if (hasSorts) {
-                out.writeVInt(sorts.size());
-                for (BytesReference sort : sorts) {
-                    out.writeBytesReference(sort);
-                }
-            }
-            out.writeBoolean(trackScores);
-            out.writeBoolean(version);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(explain, fetchSourceContext, fieldDataFields, fieldNames, from, highlightBuilder, scriptFields, size, sorts,
-                    trackScores, version);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(explain, other.explain)
-                    && Objects.equals(fetchSourceContext, other.fetchSourceContext)
-                    && Objects.equals(fieldDataFields, other.fieldDataFields)
-                    && Objects.equals(fieldNames, other.fieldNames)
-                    && Objects.equals(from, other.from)
-                    && Objects.equals(highlightBuilder, other.highlightBuilder)
-                    && Objects.equals(scriptFields, other.scriptFields)
-                    && Objects.equals(size, other.size)
-                    && Objects.equals(sorts, other.sorts)
-                    && Objects.equals(trackScores, other.trackScores)
-                    && Objects.equals(version, other.version);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
new file mode 100644
index 0000000..1efd4a7
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
@@ -0,0 +1,204 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.tophits;
+
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
+import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.highlight.HighlightBuilder;
+import org.elasticsearch.search.sort.SortBuilder;
+import org.elasticsearch.search.sort.SortOrder;
+
+import java.io.IOException;
+
+/**
+ * Builder for the {@link TopHits} aggregation.
+ */
+public class TopHitsBuilder extends AbstractAggregationBuilder {
+
+    private SearchSourceBuilder sourceBuilder;
+
+    /**
+     * Sole constructor.
+     */
+    public TopHitsBuilder(String name) {
+        super(name, InternalTopHits.TYPE.name());
+    }
+
+    /**
+     * The index to start to return hits from. Defaults to <tt>0</tt>.
+     */
+    public TopHitsBuilder setFrom(int from) {
+        sourceBuilder().from(from);
+        return this;
+    }
+
+
+    /**
+     * The number of search hits to return. Defaults to <tt>10</tt>.
+     */
+    public TopHitsBuilder setSize(int size) {
+        sourceBuilder().size(size);
+        return this;
+    }
+
+    /**
+     * Applies when sorting, and controls if scores will be tracked as well. Defaults to
+     * <tt>false</tt>.
+     */
+    public TopHitsBuilder setTrackScores(boolean trackScores) {
+        sourceBuilder().trackScores(trackScores);
+        return this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with an
+     * explanation of the hit (ranking).
+     */
+    public TopHitsBuilder setExplain(boolean explain) {
+        sourceBuilder().explain(explain);
+        return this;
+    }
+
+    /**
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with its
+     * version.
+     */
+    public TopHitsBuilder setVersion(boolean version) {
+        sourceBuilder().version(version);
+        return this;
+    }
+
+    /**
+     * Adds a field to loaded and returned.
+     */
+    public TopHitsBuilder addField(String field) {
+        sourceBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Sets no fields to be loaded, resulting in only id and type to be returned per field.
+     */
+    public TopHitsBuilder setNoFields() {
+        sourceBuilder().noFields();
+        return this;
+    }
+
+    /**
+     * Indicates whether the response should contain the stored _source for every hit
+     */
+    public TopHitsBuilder setFetchSource(boolean fetch) {
+        sourceBuilder().fetchSource(fetch);
+        return this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param include An optional include (optionally wildcarded) pattern to filter the returned _source
+     * @param exclude An optional exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public TopHitsBuilder setFetchSource(@Nullable String include, @Nullable String exclude) {
+        sourceBuilder().fetchSource(include, exclude);
+        return this;
+    }
+
+    /**
+     * Indicate that _source should be returned with every hit, with an "include" and/or "exclude" set which can include simple wildcard
+     * elements.
+     *
+     * @param includes An optional list of include (optionally wildcarded) pattern to filter the returned _source
+     * @param excludes An optional list of exclude (optionally wildcarded) pattern to filter the returned _source
+     */
+    public TopHitsBuilder setFetchSource(@Nullable String[] includes, @Nullable String[] excludes) {
+        sourceBuilder().fetchSource(includes, excludes);
+        return this;
+    }
+
+    /**
+     * Adds a field data based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name The field to get from the field data cache
+     */
+    public TopHitsBuilder addFieldDataField(String name) {
+        sourceBuilder().fieldDataField(name);
+        return this;
+    }
+
+    /**
+     * Adds a script based field to load and return. The field does not have to be stored,
+     * but its recommended to use non analyzed or numeric fields.
+     *
+     * @param name   The name that will represent this value in the return hit
+     * @param script The script to use
+     */
+    public TopHitsBuilder addScriptField(String name, Script script) {
+        sourceBuilder().scriptField(name, script);
+        return this;
+    }
+
+    /**
+     * Adds a sort against the given field name and the sort ordering.
+     *
+     * @param field The name of the field
+     * @param order The sort ordering
+     */
+    public TopHitsBuilder addSort(String field, SortOrder order) {
+        sourceBuilder().sort(field, order);
+        return this;
+    }
+
+    /**
+     * Adds a generic sort builder.
+     *
+     * @see org.elasticsearch.search.sort.SortBuilders
+     */
+    public TopHitsBuilder addSort(SortBuilder sort) {
+        sourceBuilder().sort(sort);
+        return this;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(getName()).field(type);
+        sourceBuilder().toXContent(builder, params);
+        return builder.endObject();
+    }
+
+    private SearchSourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new SearchSourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
+    public HighlightBuilder highlighter() {
+        return sourceBuilder().highlighter();
+    }
+
+    public TopHitsBuilder highlighter(HighlightBuilder highlightBuilder) {
+        sourceBuilder().highlighter(highlightBuilder);
+        return this;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
index 70c370b..50b3482 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java
@@ -18,37 +18,30 @@
  */
 package org.elasticsearch.search.aggregations.metrics.tophits;
 
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField;
+import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FieldsParseElement;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.fetch.source.FetchSourceParseElement;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.highlight.HighlighterParseElement;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.internal.SubSearchContext;
 import org.elasticsearch.search.sort.SortParseElement;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
 
 /**
  *
  */
 public class TopHitsParser implements Aggregator.Parser {
 
+    private final FetchPhase fetchPhase;
     private final SortParseElement sortParseElement;
     private final FetchSourceParseElement sourceParseElement;
     private final HighlighterParseElement highlighterParseElement;
@@ -57,9 +50,10 @@ public class TopHitsParser implements Aggregator.Parser {
     private final FieldsParseElement fieldsParseElement;
 
     @Inject
-    public TopHitsParser(SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement,
+    public TopHitsParser(FetchPhase fetchPhase, SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement,
             HighlighterParseElement highlighterParseElement, FieldDataFieldsParseElement fieldDataFieldsParseElement,
             ScriptFieldsParseElement scriptFieldsParseElement, FieldsParseElement fieldsParseElement) {
+        this.fetchPhase = fetchPhase;
         this.sortParseElement = sortParseElement;
         this.sourceParseElement = sourceParseElement;
         this.highlighterParseElement = highlighterParseElement;
@@ -74,140 +68,74 @@ public class TopHitsParser implements Aggregator.Parser {
     }
 
     @Override
-    public TopHitsAggregator.Factory parse(String aggregationName, XContentParser parser, QueryParseContext context) throws IOException {
-        TopHitsAggregator.Factory factory = new TopHitsAggregator.Factory(aggregationName);
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+        SubSearchContext subSearchContext = new SubSearchContext(context);
         XContentParser.Token token;
         String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FROM_FIELD)) {
-                    factory.from(parser.intValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SIZE_FIELD)) {
-                    factory.size(parser.intValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.VERSION_FIELD)) {
-                    factory.version(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.EXPLAIN_FIELD)) {
-                    factory.explain(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.TRACK_SCORES_FIELD)) {
-                    factory.trackScores(parser.booleanValue());
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    fieldNames.add(parser.text());
-                    factory.fields(fieldNames);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    factory.sort(parser.text());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELDS_FIELD)) {
-                    List<ScriptField> scriptFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        String scriptFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            Script script = null;
-                            boolean ignoreFailure = false;
-                            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                                if (token == XContentParser.Token.FIELD_NAME) {
-                                    currentFieldName = parser.currentName();
-                                } else if (token.isValue()) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else if (context.parseFieldMatcher().match(currentFieldName,
-                                            SearchSourceBuilder.IGNORE_FAILURE_FIELD)) {
-                                        ignoreFailure = parser.booleanValue();
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(),
-                                                "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                                parser.getTokenLocation());
-                                    }
-                                } else if (token == XContentParser.Token.START_OBJECT) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(),
-                                                "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                                parser.getTokenLocation());
-                                    }
-                                } else {
-                                    throw new ParsingException(parser.getTokenLocation(),
-                                            "Unknown key for a " + token + " in [" + currentFieldName + "].", parser.getTokenLocation());
-                                }
-                            }
-                            scriptFields.add(new ScriptField(scriptFieldName, script, ignoreFailure));
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
+        try {
+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (token == XContentParser.Token.FIELD_NAME) {
+                    currentFieldName = parser.currentName();
+                } else if ("sort".equals(currentFieldName)) {
+                    sortParseElement.parse(parser, subSearchContext);
+                } else if ("_source".equals(currentFieldName)) {
+                    sourceParseElement.parse(parser, subSearchContext);
+                } else if ("fields".equals(currentFieldName)) {
+                    fieldsParseElement.parse(parser, subSearchContext);
+                } else if (token.isValue()) {
+                    switch (currentFieldName) {
+                        case "from":
+                            subSearchContext.from(parser.intValue());
+                            break;
+                        case "size":
+                            subSearchContext.size(parser.intValue());
+                            break;
+                        case "track_scores":
+                        case "trackScores":
+                            subSearchContext.trackScores(parser.booleanValue());
+                            break;
+                        case "version":
+                            subSearchContext.version(parser.booleanValue());
+                            break;
+                        case "explain":
+                            subSearchContext.explain(parser.booleanValue());
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.scriptFields(scriptFields);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.HIGHLIGHT_FIELD)) {
-                    factory.highlighter(HighlightBuilder.PROTOTYPE.fromXContent(context));
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                    sorts.add(xContentBuilder.bytes());
-                    factory.sorts(sorts);
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_ARRAY) {
-
-                if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldNames.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    factory.fields(fieldNames);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.FIELDDATA_FIELDS_FIELD)) {
-                    List<String> fieldDataFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldDataFields.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING
-                                    + "] in [" + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
+                } else if (token == XContentParser.Token.START_OBJECT) {
+                    switch (currentFieldName) {
+                        case "highlight":
+                            highlighterParseElement.parse(parser, subSearchContext);
+                            break;
+                        case "scriptFields":
+                        case "script_fields":
+                            scriptFieldsParseElement.parse(parser, subSearchContext);
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.fieldDataFields(fieldDataFields);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder.SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.jsonBuilder().copyCurrentStructure(parser);
-                        sorts.add(xContentBuilder.bytes());
+                } else if (token == XContentParser.Token.START_ARRAY) {
+                    switch (currentFieldName) {
+                        case "fielddataFields":
+                        case "fielddata_fields":
+                            fieldDataFieldsParseElement.parse(parser, subSearchContext);
+                            break;
+                        default:
+                        throw new SearchParseException(context, "Unknown key for a " + token + " in [" + aggregationName + "]: ["
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
-                    factory.sorts(sorts);
-                } else if (context.parseFieldMatcher().match(currentFieldName, SearchSourceBuilder._SOURCE_FIELD)) {
-                    factory.fetchSource(FetchSourceContext.parse(parser, context));
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
+                    throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
                             parser.getTokenLocation());
                 }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                        parser.getTokenLocation());
             }
+        } catch (Exception e) {
+            throw ExceptionsHelper.convertToElastic(e);
         }
-        return factory;
-    }
-
-    @Override
-    public AggregatorFactory[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new TopHitsAggregator.Factory(null) };
+        return new TopHitsAggregator.Factory(aggregationName, fetchPhase, subSearchContext);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
index 3a3a413..867d876 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.search.aggregations.metrics.valuecount;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.fielddata.SortedBinaryDocValues;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -33,10 +30,9 @@ import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
 import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSource;
 import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
@@ -112,10 +108,10 @@ public class ValueCountAggregator extends NumericMetricsAggregator.SingleValue {
         Releasables.close(counts);
     }
 
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource, Factory> {
+    public static class Factory<VS extends ValuesSource> extends ValuesSourceAggregatorFactory.LeafOnly<VS> {
 
-        public Factory(String name, ValueType targetValueType) {
-            super(name, InternalValueCount.TYPE, ValuesSourceType.ANY, targetValueType);
+        public Factory(String name, ValuesSourceConfig<VS> config) {
+            super(name, InternalValueCount.TYPE.name(), config);
         }
 
         @Override
@@ -125,39 +121,13 @@ public class ValueCountAggregator extends NumericMetricsAggregator.SingleValue {
         }
 
         @Override
-        protected Aggregator doCreateInternal(ValuesSource valuesSource, AggregationContext aggregationContext, Aggregator parent,
+        protected Aggregator doCreateInternal(VS valuesSource, AggregationContext aggregationContext, Aggregator parent,
                 boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
                 throws IOException {
             return new ValueCountAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators,
                     metaData);
         }
 
-        @Override
-        protected ValuesSourceAggregatorFactory<ValuesSource, Factory> innerReadFrom(String name, ValuesSourceType valuesSourceType,
-                ValueType targetValueType, StreamInput in) {
-            return new ValueCountAggregator.Factory(name, targetValueType);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        public XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(Object obj) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java
new file mode 100644
index 0000000..1d37914
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.aggregations.metrics.valuecount;
+
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+
+/**
+ * Builder for the {@link ValueCount} aggregation.
+ */
+public class ValueCountBuilder extends ValuesSourceMetricsAggregationBuilder<ValueCountBuilder> {
+
+    /**
+     * Sole constructor.
+     */
+    public ValueCountBuilder(String name) {
+        super(name, InternalValueCount.TYPE.name());
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
index 08ce4bf..764f6ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java
@@ -18,27 +18,19 @@
  */
 package org.elasticsearch.search.aggregations.metrics.valuecount;
 
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.AnyValuesSourceParser;
-import org.elasticsearch.search.aggregations.support.ValueType;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
+import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
-public class ValueCountParser extends AnyValuesSourceParser {
-
-    public ValueCountParser() {
-        super(true, true);
-    }
+public class ValueCountParser implements Aggregator.Parser {
 
     @Override
     public String type() {
@@ -46,19 +38,22 @@ public class ValueCountParser extends AnyValuesSourceParser {
     }
 
     @Override
-    protected boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        return false;
-    }
-
-    @Override
-    protected ValuesSourceAggregatorFactory<ValuesSource, ValueCountAggregator.Factory> createFactory(String aggregationName,
-            ValuesSourceType valuesSourceType, ValueType targetValueType, Map<ParseField, Object> otherOptions) {
-        return new ValueCountAggregator.Factory(aggregationName, targetValueType);
-    }
-
-    @Override
-    public AggregatorFactory<?>[] getFactoryPrototypes() {
-        return new AggregatorFactory[] { new ValueCountAggregator.Factory(null, null) };
+    public AggregatorFactory parse(String aggregationName, XContentParser parser, SearchContext context) throws IOException {
+
+        ValuesSourceParser vsParser = ValuesSourceParser.any(aggregationName, InternalValueCount.TYPE, context)
+                .build();
+
+        XContentParser.Token token;
+        String currentFieldName = null;
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (!vsParser.token(currentFieldName, token, parser)) {
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + aggregationName + "].",
+                        parser.getTokenLocation());
+            }
+        }
+
+        return new ValueCountAggregator.Factory(aggregationName, vsParser.config());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
index 24d2913..881a8e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java
@@ -20,17 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentLocation;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;
 import org.elasticsearch.search.aggregations.InvalidAggregationPathException;
 import org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation;
 import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativeParser;
 import org.elasticsearch.search.aggregations.support.AggregationPath;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -62,7 +62,7 @@ public class BucketHelpers {
          * @param text    GapPolicy in string format (e.g. "ignore")
          * @return        GapPolicy enum
          */
-        public static GapPolicy parse(QueryParseContext context, String text, XContentLocation tokenLocation) {
+        public static GapPolicy parse(SearchContext context, String text, XContentLocation tokenLocation) {
             GapPolicy result = null;
             for (GapPolicy policy : values()) {
                 if (context.parseFieldMatcher().match(text, policy.parseField)) {
@@ -79,7 +79,7 @@ public class BucketHelpers {
                 for (GapPolicy policy : values()) {
                     validNames.add(policy.getName());
                 }
-                throw new ParsingException(tokenLocation, "Invalid gap policy: [" + text + "], accepted values: " + validNames);
+                throw new SearchParseException(context, "Invalid gap policy: [" + text + "], accepted values: " + validNames, tokenLocation);
             }
             return result;
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
index a3c1805..b2ee037 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregator.java
@@ -25,10 +25,10 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.Map;
@@ -38,7 +38,7 @@ public abstract class PipelineAggregator implements Streamable {
     /**
      * Parses the pipeline aggregation request and creates the appropriate
      * pipeline aggregator factory for it.
-     *
+     * 
      * @see PipelineAggregatorFactory
      */
     public static interface Parser {
@@ -56,7 +56,7 @@ public abstract class PipelineAggregator implements Streamable {
         /**
          * Returns the pipeline aggregator factory with which this parser is
          * associated.
-         *
+         * 
          * @param pipelineAggregatorName
          *            The name of the pipeline aggregation
          * @param parser
@@ -67,13 +67,7 @@ public abstract class PipelineAggregator implements Streamable {
          * @throws java.io.IOException
          *             When parsing fails
          */
-        PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context) throws IOException;
-
-        /**
-         * @return an empty {@link PipelineAggregatorFactory} instance for this
-         *         parser that can be used for deserialization
-         */
-        PipelineAggregatorFactory getFactoryPrototype();
+        PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException;
 
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
index 70f34de..6fc0185 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorFactory.java
@@ -18,25 +18,17 @@
  */
 package org.elasticsearch.search.aggregations.pipeline;
 
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * A factory that knows how to create an {@link PipelineAggregator} of a
  * specific type.
  */
-public abstract class PipelineAggregatorFactory extends ToXContentToBytes implements NamedWriteable<PipelineAggregatorFactory>, ToXContent {
+public abstract class PipelineAggregatorFactory {
 
     protected String name;
     protected String type;
@@ -61,10 +53,6 @@ public abstract class PipelineAggregatorFactory extends ToXContentToBytes implem
         return name;
     }
 
-    public String type() {
-        return type;
-    }
-
     /**
      * Validates the state of this factory (makes sure the factory is properly
      * configured)
@@ -102,108 +90,4 @@ public abstract class PipelineAggregatorFactory extends ToXContentToBytes implem
         return bucketsPaths;
     }
 
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(name);
-        out.writeStringArray(bucketsPaths);
-        doWriteTo(out);
-        out.writeMap(metaData);
-    }
-
-    // NORELEASE make this abstract when agg refactor complete
-    protected void doWriteTo(StreamOutput out) throws IOException {
-    }
-
-    // NORELEASE remove this method when agg refactor complete
-    @Override
-    public String getWriteableName() {
-        return type;
-    }
-
-    @Override
-    public PipelineAggregatorFactory readFrom(StreamInput in) throws IOException {
-        String name = in.readString();
-        String[] bucketsPaths = in.readStringArray();
-        PipelineAggregatorFactory factory = doReadFrom(name, bucketsPaths, in);
-        factory.metaData = in.readMap();
-        return factory;
-    }
-
-    // NORELEASE make this abstract when agg refactor complete
-    protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-        return null;
-    }
-
-    @Override
-    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(getName());
-
-        if (this.metaData != null) {
-            builder.field("meta", this.metaData);
-        }
-        builder.startObject(type);
-
-        if (!overrideBucketsPath() && bucketsPaths != null) {
-            builder.startArray(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName());
-            for (String path : bucketsPaths) {
-                builder.value(path);
-            }
-            builder.endArray();
-        }
-
-        internalXContent(builder, params);
-
-        builder.endObject();
-
-        return builder.endObject();
-    }
-
-    /**
-     * @return <code>true</code> if the {@link PipelineAggregatorFactory}
-     *         overrides the XContent rendering of the bucketPath option.
-     */
-    protected boolean overrideBucketsPath() {
-        return false;
-    }
-
-    // NORELEASE make this method abstract when agg refactor complete
-    protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        return builder;
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(Arrays.hashCode(bucketsPaths), metaData, name, type, doHashCode());
-    }
-
-    // NORELEASE make this method abstract here when agg refactor complete (so
-    // that subclasses are forced to implement it)
-    protected int doHashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        PipelineAggregatorFactory other = (PipelineAggregatorFactory) obj;
-        if (!Objects.equals(name, other.name))
-            return false;
-        if (!Objects.equals(type, other.type))
-            return false;
-        if (!Objects.deepEquals(bucketsPaths, other.bucketsPaths))
-            return false;
-        if (!Objects.equals(metaData, other.metaData))
-            return false;
-        return doEquals(obj);
-    }
-
-    // NORELEASE make this method abstract here when agg refactor complete (so
-    // that subclasses are forced to implement it)
-    protected boolean doEquals(Object obj) {
-        return true;
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java
deleted file mode 100644
index 73096ef..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsFactory.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
-import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.Objects;
-
-public abstract class BucketMetricsFactory<AF extends BucketMetricsFactory<AF>> extends PipelineAggregatorFactory {
-
-    private String format = null;
-    private GapPolicy gapPolicy = GapPolicy.SKIP;
-
-    public BucketMetricsFactory(String name, String type, String[] bucketsPaths) {
-        super(name, type, bucketsPaths);
-    }
-
-    /**
-     * Sets the format to use on the output of this aggregation.
-     */
-    public AF format(String format) {
-        this.format = format;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the format to use on the output of this aggregation.
-     */
-    public String format() {
-        return format;
-    }
-
-    protected ValueFormatter formatter() {
-        if (format != null) {
-            return ValueFormat.Patternable.Number.format(format).formatter();
-        } else {
-            return ValueFormatter.RAW;
-        }
-    }
-
-    /**
-     * Sets the gap policy to use for this aggregation.
-     */
-    public AF gapPolicy(GapPolicy gapPolicy) {
-        this.gapPolicy = gapPolicy;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the gap policy to use for this aggregation.
-     */
-    public GapPolicy gapPolicy() {
-        return gapPolicy;
-    }
-
-    @Override
-    protected abstract PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException;
-
-    @Override
-    public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
-            List<PipelineAggregatorFactory> pipelineAggregatorFactories) {
-        if (bucketsPaths.length != 1) {
-            throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
-                    + " must contain a single entry for aggregation [" + name + "]");
-        }
-    }
-
-    @Override
-    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        if (format != null) {
-            builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);
-        }
-        if (gapPolicy != null) {
-            builder.field(BucketMetricsParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-        }
-        doXContentBody(builder, params);
-        return builder;
-    }
-
-    protected abstract XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException;
-
-    @Override
-    protected final PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-        BucketMetricsFactory factory = innerReadFrom(name, bucketsPaths, in);
-        factory.format = in.readOptionalString();
-        factory.gapPolicy = GapPolicy.readFrom(in);
-        return factory;
-    }
-
-    protected abstract BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException;
-
-    @Override
-    protected final void doWriteTo(StreamOutput out) throws IOException {
-        innerWriteTo(out);
-        out.writeOptionalString(format);
-        gapPolicy.writeTo(out);
-    }
-
-    protected abstract void innerWriteTo(StreamOutput out) throws IOException;
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(format, gapPolicy, innerHashCode());
-    }
-
-    protected abstract int innerHashCode();
-
-    @Override
-    protected final boolean doEquals(Object obj) {
-        BucketMetricsFactory other = (BucketMetricsFactory) obj;
-        return Objects.equals(format, other.format)
-                && Objects.equals(gapPolicy, other.gapPolicy)
-                && innerEquals(other);
-    }
-
-    protected abstract boolean innerEquals(BucketMetricsFactory other);
-
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
index 4fe8eae..3cf084b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/BucketMetricsParser.java
@@ -20,12 +20,14 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -46,13 +48,12 @@ public abstract class BucketMetricsParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public final PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public final PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
         Map<String, Object> leftover = new HashMap<>(5);
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -85,34 +86,34 @@ public abstract class BucketMetricsParser implements PipelineAggregator.Parser {
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        BucketMetricsFactory factory = null;
+        ValueFormatter formatter = null;
+        if (format != null) {
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
+        }
+
+        PipelineAggregatorFactory factory = null;
         try {
-            factory = buildFactory(pipelineAggregatorName, bucketsPaths, leftover);
-            if (format != null) {
-                factory.format(format);
-            }
-            if (gapPolicy != null) {
-                factory.gapPolicy(gapPolicy);
-            }
+            factory = buildFactory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter, leftover);
         } catch (ParseException exception) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Could not parse settings for aggregation [" + pipelineAggregatorName + "].", exception);
+            throw new SearchParseException(context, "Could not parse settings for aggregation ["
+                    + pipelineAggregatorName + "].", null, exception);
         }
 
         if (leftover.size() > 0) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Unexpected tokens " + leftover.keySet() + " in [" + pipelineAggregatorName + "].");
+            throw new SearchParseException(context, "Unexpected tokens " + leftover.keySet() + " in [" + pipelineAggregatorName + "].", null);
         }
         assert(factory != null);
 
         return factory;
     }
 
-    protected abstract BucketMetricsFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths,
-            Map<String, Object> unparsedParams) throws ParseException;
+    protected abstract PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException;
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
index 4589e74..658284f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,11 +33,8 @@ public class AvgBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, Map<String, Object> unparsedParams) {
-        return new AvgBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths);
-    }
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new AvgBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new AvgBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
index b7625b2..3ab134c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/avg/AvgBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -30,7 +28,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -89,15 +86,20 @@ public class AvgBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalSimpleValue(name(), avgValue, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String[] bucketsPaths) {
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new AvgBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new AvgBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -108,31 +110,6 @@ public class AvgBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
index 9114ade..4cd584a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -33,13 +34,9 @@ public class MaxBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, Map<String, Object> unparsedParams) {
-        return new MaxBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MaxBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+                                                     ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new MaxBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
index c8b1007..95a70af 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/max/MaxBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
@@ -96,15 +93,20 @@ public class MaxBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalBucketMetricValue(name(), keys, maxValue, formatter, Collections.emptyList(), metaData());
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String[] bucketsPaths) {
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new MaxBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new MaxBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -116,31 +118,6 @@ public class MaxBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
index 474bef7..db7bc9b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,14 +33,9 @@ public class MinBucketParser extends BucketMetricsParser {
         return MinBucketPipelineAggregator.TYPE.name();
     }
 
-    @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, Map<String, Object> unparsedParams) {
-        return new MinBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MinBucketPipelineAggregator.Factory(null, null);
-    }
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new MinBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
+    };
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
index 685a5f5..755b206 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/min/MinBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.InternalBucketMetricValue;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
@@ -97,15 +94,20 @@ public class MinBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalBucketMetricValue(name(), keys, minValue, formatter, Collections.emptyList(), metaData());
     };
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String[] bucketsPaths) {
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new MinBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new MinBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -117,31 +119,6 @@ public class MinBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
index 36babbe..7c9da5c 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketParser.java
@@ -21,13 +21,15 @@ package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
 
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.text.ParseException;
 import java.util.List;
 import java.util.Map;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+
 
 public class PercentilesBucketParser extends BucketMetricsParser {
 
@@ -39,10 +41,10 @@ public class PercentilesBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, Map<String, Object> unparsedParams)
-            throws ParseException {
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+                                                     ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException {
 
-        double[] percents = null;
+        double[] percents = new double[] { 1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0 };
         int counter = 0;
         Object percentParam = unparsedParams.get(PERCENTS.getPreferredName());
 
@@ -65,16 +67,6 @@ public class PercentilesBucketParser extends BucketMetricsParser {
             }
         }
 
-        PercentilesBucketPipelineAggregator.Factory factory = new PercentilesBucketPipelineAggregator.Factory(pipelineAggregatorName,
-                bucketsPaths);
-        if (percents != null) {
-            factory.percents(percents);
-        }
-        return factory;
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new PercentilesBucketPipelineAggregator.Factory(null, null);
+        return new PercentilesBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter, percents);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
index 00918ba..24e8204 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/percentile/PercentilesBucketPipelineAggregator.java
@@ -19,33 +19,28 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile;
 
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
+
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 
 public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAggregator {
 
     public final static Type TYPE = new Type("percentiles_bucket");
-    public final ParseField PERCENTS_FIELD = new ParseField("percents");
 
     public final static PipelineAggregatorStreams.Stream STREAM = new PipelineAggregatorStreams.Stream() {
         @Override
@@ -124,32 +119,22 @@ public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAg
         out.writeDoubleArray(percents);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private double[] percents = new double[] { 1.0, 5.0, 25.0, 50.0, 75.0, 95.0, 99.0 };
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+        private final double[] percents;
 
-        public Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter, double[] percents) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Get the percentages to calculate percentiles for in this aggregation
-         */
-        public double[] percents() {
-            return percents;
-        }
-
-        /**
-         * Set the percentages to calculate percentiles for in this aggregation
-         */
-        public Factory percents(double[] percents) {
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
             this.percents = percents;
-            return this;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new PercentilesBucketPipelineAggregator(name, percents, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new PercentilesBucketPipelineAggregator(name, percents, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -168,37 +153,6 @@ public class PercentilesBucketPipelineAggregator extends BucketMetricsPipelineAg
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            if (percents != null) {
-                builder.field(PercentilesBucketParser.PERCENTS.getPreferredName(), percents);
-            }
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.percents = in.readDoubleArray();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDoubleArray(percents);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Arrays.hashCode(percents);
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory obj) {
-            Factory other = (Factory) obj;
-            return Objects.deepEquals(percents, other.percents);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
index 1183062..b250447 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,12 +33,8 @@ public class StatsBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, Map<String, Object> unparsedParams) {
-        return new StatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new StatsBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new StatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
index c51ac3e..66726ce 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/StatsBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,7 +27,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -95,15 +92,20 @@ public class StatsBucketPipelineAggregator extends BucketMetricsPipelineAggregat
         return new InternalStatsBucket(name(), count, sum, min, max, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String[] bucketsPaths) {
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new StatsBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new StatsBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -115,31 +117,6 @@ public class StatsBucketPipelineAggregator extends BucketMetricsPipelineAggregat
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
index 2308030..b4d1f18 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java
@@ -20,9 +20,10 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
 
 import org.elasticsearch.common.ParseField;
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.text.ParseException;
 import java.util.Map;
@@ -36,10 +37,10 @@ public class ExtendedStatsBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, Map<String, Object> unparsedParams)
-            throws ParseException {
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) throws ParseException {
 
-        Double sigma = null;
+        double sigma = 2.0;
         Object param = unparsedParams.get(SIGMA.getPreferredName());
 
         if (param != null) {
@@ -51,16 +52,6 @@ public class ExtendedStatsBucketParser extends BucketMetricsParser {
                         + param.getClass().getSimpleName() + "` provided instead", 0);
             }
         }
-        ExtendedStatsBucketPipelineAggregator.Factory factory = new ExtendedStatsBucketPipelineAggregator.Factory(pipelineAggregatorName,
-                bucketsPaths);
-        if (sigma != null) {
-            factory.sigma(sigma);
-        }
-        return factory;
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new ExtendedStatsBucketPipelineAggregator.Factory(null, null);
+        return new ExtendedStatsBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, sigma, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
index 1adec45..6a7f2be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -29,14 +27,12 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.io.IOException;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipelineAggregator {
 
@@ -101,34 +97,22 @@ public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipeline
         return new InternalExtendedStatsBucket(name(), count, sum, min, max, sumOfSqrs, sigma, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        private double sigma = 2.0;
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+        private final double sigma;
 
-        public Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, double sigma, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Set the value of sigma to use when calculating the standard deviation
-         * bounds
-         */
-        public Factory sigma(double sigma) {
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
             this.sigma = sigma;
-            return this;
-        }
-
-        /**
-         * Get the value of sigma to use when calculating the standard deviation
-         * bounds
-         */
-        public double sigma() {
-            return sigma;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new ExtendedStatsBucketPipelineAggregator(name, bucketsPaths, sigma, gapPolicy(), formatter(), metaData);
+            return new ExtendedStatsBucketPipelineAggregator(name, bucketsPaths, sigma, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -145,35 +129,6 @@ public class ExtendedStatsBucketPipelineAggregator extends BucketMetricsPipeline
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            builder.field(ExtendedStatsBucketParser.SIGMA.getPreferredName(), sigma);
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.sigma = in.readDouble();
-            return factory;
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            out.writeDouble(sigma);
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return Objects.hash(sigma);
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(sigma, other.sigma);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
index f318c75..3fad95d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketParser.java
@@ -19,9 +19,10 @@
 
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
 
+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
 import java.util.Map;
 
@@ -32,12 +33,8 @@ public class SumBucketParser extends BucketMetricsParser {
     }
 
     @Override
-    protected BucketMetricsFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, Map<String, Object> unparsedParams) {
-        return new SumBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths);
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new SumBucketPipelineAggregator.Factory(null, null);
+    protected PipelineAggregatorFactory buildFactory(String pipelineAggregatorName, String[] bucketsPaths, GapPolicy gapPolicy,
+            ValueFormatter formatter, Map<String, Object> unparsedParams) {
+        return new SumBucketPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, gapPolicy, formatter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
index e8a6d90..138bd63 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/sum/SumBucketPipelineAggregator.java
@@ -20,8 +20,6 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum;
 
 import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
@@ -30,7 +28,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsFactory;
 import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsPipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 
@@ -85,15 +82,20 @@ public class SumBucketPipelineAggregator extends BucketMetricsPipelineAggregator
         return new InternalSimpleValue(name(), sum, formatter, pipelineAggregators, metadata);
     }
 
-    public static class Factory extends BucketMetricsFactory<Factory> {
+    public static class Factory extends PipelineAggregatorFactory {
 
-        public Factory(String name, String[] bucketsPaths) {
+        private final ValueFormatter formatter;
+        private final GapPolicy gapPolicy;
+
+        public Factory(String name, String[] bucketsPaths, GapPolicy gapPolicy, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
+            this.gapPolicy = gapPolicy;
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new SumBucketPipelineAggregator(name, bucketsPaths, gapPolicy(), formatter(), metaData);
+            return new SumBucketPipelineAggregator(name, bucketsPaths, gapPolicy, formatter, metaData);
         }
 
         @Override
@@ -105,31 +107,6 @@ public class SumBucketPipelineAggregator extends BucketMetricsPipelineAggregator
             }
         }
 
-        @Override
-        protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-            return builder;
-        }
-
-        @Override
-        protected BucketMetricsFactory innerReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            return new Factory(name, bucketsPaths);
-        }
-
-        @Override
-        protected void innerWriteTo(StreamOutput out) throws IOException {
-            // Do nothing, no extra state to write to stream
-        }
-
-        @Override
-        protected int innerHashCode() {
-            return 0;
-        }
-
-        @Override
-        protected boolean innerEquals(BucketMetricsFactory other) {
-            return true;
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
index d97d51a..05ff7e9 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptParser.java
@@ -20,18 +20,19 @@
 package org.elasticsearch.search.aggregations.pipeline.bucketscript;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -48,13 +49,13 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         Script script = null;
         String currentFieldName = null;
         Map<String, String> bucketsPathsMap = null;
         String format = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -70,8 +71,8 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
                     script = Script.parse(parser, context.parseFieldMatcher());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -85,8 +86,8 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put("_value" + i, paths.get(i));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
@@ -98,37 +99,33 @@ public class BucketScriptParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put(entry.getKey(), String.valueOf(entry.getValue()));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPathsMap == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for series_arithmetic aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for series_arithmetic aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
         if (script == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
-                    + "] for series_arithmetic aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
+                    + "] for series_arithmetic aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        BucketScriptPipelineAggregator.Factory factory = new BucketScriptPipelineAggregator.Factory(reducerName, bucketsPathsMap, script);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new BucketScriptPipelineAggregator.Factory(null, Collections.emptyMap(), null);
+        return new BucketScriptPipelineAggregator.Factory(reducerName, bucketsPathsMap, script, formatter, gapPolicy);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
index 1d519ba..76cb15e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
@@ -21,11 +21,9 @@ package org.elasticsearch.search.aggregations.pipeline.bucketscript;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -39,7 +37,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -49,8 +46,6 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -162,112 +157,22 @@ public class BucketScriptPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private final Script script;
-        private final Map<String, String> bucketsPathsMap;
-        private String format = null;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
+        private Script script;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private Map<String, String> bucketsPathsMap;
 
-        public Factory(String name, Map<String, String> bucketsPathsMap, Script script) {
+        public Factory(String name, Map<String, String> bucketsPathsMap, Script script, ValueFormatter formatter, GapPolicy gapPolicy) {
             super(name, TYPE.name(), bucketsPathsMap.values().toArray(new String[bucketsPathsMap.size()]));
             this.bucketsPathsMap = bucketsPathsMap;
             this.script = script;
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
-        }
-
-        /**
-         * Sets the gap policy to use for this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the gap policy to use for this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new BucketScriptPipelineAggregator(name, bucketsPathsMap, script, formatter(), gapPolicy, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(BucketScriptParser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
-            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-            if (format != null) {
-                builder.field(BucketScriptParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            return builder;
-        }
-
-        @Override
-        protected boolean overrideBucketsPath() {
-            return true;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Map<String, String> bucketsPathsMap = new HashMap<String, String>();
-            int mapSize = in.readVInt();
-            for (int i = 0; i < mapSize; i++) {
-                bucketsPathsMap.put(in.readString(), in.readString());
-            }
-            Script script = Script.readScript(in);
-            Factory factory = new Factory(name, bucketsPathsMap, script);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(bucketsPathsMap.size());
-            for (Entry<String, String> e : bucketsPathsMap.entrySet()) {
-                out.writeString(e.getKey());
-                out.writeString(e.getValue());
-            }
-            script.writeTo(out);
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(bucketsPathsMap, script, format, gapPolicy);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(bucketsPathsMap, other.bucketsPathsMap) && Objects.equals(script, other.script)
-                    && Objects.equals(format, other.format) && Objects.equals(gapPolicy, other.gapPolicy);
+            return new BucketScriptPipelineAggregator(name, bucketsPathsMap, script, formatter, gapPolicy, metaData);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
index 9843e87..f3e2ead 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumParser.java
@@ -20,11 +20,13 @@
 package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -42,8 +44,7 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
@@ -58,8 +59,8 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
                     bucketsPaths = new String[] { parser.text() };
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -70,30 +71,28 @@ public class CumulativeSumParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for derivative aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        CumulativeSumPipelineAggregator.Factory factory = new CumulativeSumPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new CumulativeSumPipelineAggregator.Factory(null, null);
+        return new CumulativeSumPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
index fd98c6e..49c6f4f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.aggregations.pipeline.cumulativesum;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
@@ -34,8 +33,6 @@ import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.BucketMetricsParser;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -43,7 +40,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -113,38 +109,16 @@ public class CumulativeSumPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
+        private final ValueFormatter formatter;
 
-        public Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
+            this.formatter = formatter;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new CumulativeSumPipelineAggregator(name, bucketsPaths, formatter(), metaData);
+            return new CumulativeSumPipelineAggregator(name, bucketsPaths, formatter, metaData);
         }
 
         @Override
@@ -165,35 +139,5 @@ public class CumulativeSumPipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);
-            }
-            return builder;
-        }
-
-        @Override
-        protected final PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected final void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format);
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
index dd27914..f413987 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativeParser.java
@@ -20,12 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.derivative;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.rounding.DateTimeUnit;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramParser;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -43,14 +48,13 @@ public class DerivativeParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
         String units = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -65,8 +69,8 @@ public class DerivativeParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, UNIT)) {
                     units = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -77,36 +81,41 @@ public class DerivativeParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for derivative aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        DerivativePipelineAggregator.Factory factory = new DerivativePipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
-        }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
+
+        Long xAxisUnits = null;
         if (units != null) {
-            factory.units(units);
+            DateTimeUnit dateTimeUnit = DateHistogramParser.DATE_FIELD_UNITS.get(units);
+            if (dateTimeUnit != null) {
+                xAxisUnits = dateTimeUnit.field().getDurationField().getUnitMillis();
+            } else {
+                TimeValue timeValue = TimeValue.parseTimeValue(units, null, getClass().getSimpleName() + ".unit");
+                if (timeValue != null) {
+                    xAxisUnits = timeValue.getMillis();
                 }
-        return factory;
-    }
+            }
+        }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new DerivativePipelineAggregator.Factory(null, null);
+        return new DerivativePipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter, gapPolicy, xAxisUnits);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
index e71a4f1..855fea8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java
@@ -21,9 +21,6 @@ package org.elasticsearch.search.aggregations.pipeline.derivative;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.rounding.DateTimeUnit;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -36,7 +33,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 import org.joda.time.DateTime;
@@ -45,7 +41,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -159,61 +154,19 @@ public class DerivativePipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private String units;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private Long xAxisUnits;
 
-        public Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter, GapPolicy gapPolicy, Long xAxisUnits) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        public String format() {
-            return format;
-        }
-
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        public Factory units(String units) {
-            this.units = units;
-            return this;
-        }
-
-        public String units() {
-            return units;
+            this.xAxisUnits = xAxisUnits;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            ValueFormatter formatter;
-            if (format != null) {
-                formatter = ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                formatter = ValueFormatter.RAW;
-            }
-            Long xAxisUnits = null;
-            if (units != null) {
-                DateTimeUnit dateTimeUnit = HistogramAggregator.DateHistogramFactory.DATE_FIELD_UNITS.get(units);
-                if (dateTimeUnit != null) {
-                    xAxisUnits = dateTimeUnit.field().getDurationField().getUnitMillis();
-                } else {
-                    TimeValue timeValue = TimeValue.parseTimeValue(units, null, getClass().getSimpleName() + ".unit");
-                    if (timeValue != null) {
-                        xAxisUnits = timeValue.getMillis();
-                    }
-                }
-            }
             return new DerivativePipelineAggregator(name, bucketsPaths, formatter, gapPolicy, xAxisUnits, metaData);
         }
 
@@ -235,61 +188,5 @@ public class DerivativePipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            if (in.readBoolean()) {
-                factory.gapPolicy = GapPolicy.readFrom(in);
-            }
-            factory.units = in.readOptionalString();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            boolean hasGapPolicy = gapPolicy != null;
-            out.writeBoolean(hasGapPolicy);
-            if (hasGapPolicy) {
-                gapPolicy.writeTo(out);
-            }
-            out.writeOptionalString(units);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(DerivativeParser.FORMAT.getPreferredName(), format);
-            }
-            if (gapPolicy != null) {
-                builder.field(DerivativeParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            }
-            if (units != null) {
-                builder.field(DerivativeParser.UNIT.getPreferredName(), units);
-            }
-            return builder;
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            if (!Objects.equals(format, other.format)) {
-                return false;
-            }
-            if (!Objects.equals(gapPolicy, other.gapPolicy)) {
-                return false;
-            }
-            if (!Objects.equals(units, other.units)) {
-                return false;
-            }
-            return true;
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, units);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
index cca0166..e2623b5 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorParser.java
@@ -20,18 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.having;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -48,12 +47,12 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         Script script = null;
         String currentFieldName = null;
         Map<String, String> bucketsPathsMap = null;
-        GapPolicy gapPolicy = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -67,8 +66,8 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
                     script = Script.parse(parser, context.parseFieldMatcher());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -82,8 +81,8 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put("_value" + i, paths.get(i));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
@@ -95,36 +94,26 @@ public class BucketSelectorParser implements PipelineAggregator.Parser {
                         bucketsPathsMap.put(entry.getKey(), String.valueOf(entry.getValue()));
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPathsMap == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for bucket_selector aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for bucket_selector aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
         if (script == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
-                    + "] for bucket_selector aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + ScriptField.SCRIPT.getPreferredName()
+                    + "] for bucket_selector aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        BucketSelectorPipelineAggregator.Factory factory = new BucketSelectorPipelineAggregator.Factory(reducerName, bucketsPathsMap,
-                script);
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        return factory;
-
-    }
-
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new BucketSelectorPipelineAggregator.Factory(null, Collections.emptyMap(), null);
+        return new BucketSelectorPipelineAggregator.Factory(reducerName, bucketsPathsMap, script, gapPolicy);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
index b81e0da..edc3b4e 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
@@ -22,11 +22,9 @@ package org.elasticsearch.search.aggregations.pipeline.having;
 
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.script.CompiledScript;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
@@ -37,7 +35,6 @@ import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -45,8 +42,6 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Objects;
 
 import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;
 
@@ -142,84 +137,20 @@ public class BucketSelectorPipelineAggregator extends PipelineAggregator {
     public static class Factory extends PipelineAggregatorFactory {
 
         private Script script;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
+        private GapPolicy gapPolicy;
         private Map<String, String> bucketsPathsMap;
 
-        public Factory(String name, Map<String, String> bucketsPathsMap, Script script) {
+        public Factory(String name, Map<String, String> bucketsPathsMap, Script script, GapPolicy gapPolicy) {
             super(name, TYPE.name(), bucketsPathsMap.values().toArray(new String[bucketsPathsMap.size()]));
             this.bucketsPathsMap = bucketsPathsMap;
             this.script = script;
-        }
-
-        /**
-         * Sets the gap policy to use for this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the gap policy to use for this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
             return new BucketSelectorPipelineAggregator(name, bucketsPathsMap, script, gapPolicy, metaData);
         }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.field(BucketScriptParser.BUCKETS_PATH.getPreferredName(), bucketsPathsMap);
-            builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-            builder.field(BucketScriptParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            return builder;
-        }
-
-        @Override
-        protected boolean overrideBucketsPath() {
-            return true;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Map<String, String> bucketsPathsMap = new HashMap<String, String>();
-            int mapSize = in.readVInt();
-            for (int i = 0; i < mapSize; i++) {
-                bucketsPathsMap.put(in.readString(), in.readString());
-            }
-            Script script = Script.readScript(in);
-            Factory factory = new Factory(name, bucketsPathsMap, script);
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeVInt(bucketsPathsMap.size());
-            for (Entry<String, String> e : bucketsPathsMap.entrySet()) {
-                out.writeString(e.getKey());
-                out.writeString(e.getValue());
-            }
-            script.writeTo(out);
-            gapPolicy.writeTo(out);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(bucketsPathsMap, script, gapPolicy);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(bucketsPathsMap, other.bucketsPathsMap) && Objects.equals(script, other.script)
-                    && Objects.equals(gapPolicy, other.gapPolicy);
-        }
-
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
index 566eb92..5856735 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgParser.java
@@ -20,15 +20,17 @@
 package org.elasticsearch.search.aggregations.pipeline.movavg;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelParserMapper;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -57,18 +59,17 @@ public class MovAvgParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, QueryParseContext context)
-            throws IOException {
+    public PipelineAggregatorFactory parse(String pipelineAggregatorName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
 
-        GapPolicy gapPolicy = null;
-        Integer window = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
+        int window = 5;
         Map<String, Object> settings = null;
-        String model = null;
-        Integer predict = null;
+        String model = "simple";
+        int predict = 0;
         Boolean minimize = null;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -78,18 +79,20 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                 if (context.parseFieldMatcher().match(currentFieldName, WINDOW)) {
                     window = parser.intValue();
                     if (window <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(), "[" + currentFieldName + "] value must be a positive, "
-                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].");
+                        throw new SearchParseException(context, "[" + currentFieldName + "] value must be a positive, "
+                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].",
+                                parser.getTokenLocation());
                     }
                 } else if (context.parseFieldMatcher().match(currentFieldName, PREDICT)) {
                     predict = parser.intValue();
                     if (predict <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(), "[" + currentFieldName + "] value must be a positive integer."
-                                + "  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].");
+                        throw new SearchParseException(context, "[" + currentFieldName + "] value must be a positive, "
+                                + "non-zero integer.  Value supplied was [" + predict + "] in [" + pipelineAggregatorName + "].",
+                                parser.getTokenLocation());
                     }
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_STRING) {
                 if (context.parseFieldMatcher().match(currentFieldName, FORMAT)) {
@@ -101,8 +104,8 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, MODEL)) {
                     model = parser.text();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -113,71 +116,66 @@ public class MovAvgParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (context.parseFieldMatcher().match(currentFieldName, SETTINGS)) {
                     settings = parser.map();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_BOOLEAN) {
                 if (context.parseFieldMatcher().match(currentFieldName, MINIMIZE)) {
                     minimize = parser.booleanValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + pipelineAggregatorName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " in [" + pipelineAggregatorName + "].");
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + pipelineAggregatorName + "].",
+                        parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(), "Missing required field [" + BUCKETS_PATH.getPreferredName()
-                    + "] for movingAvg aggregation [" + pipelineAggregatorName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for movingAvg aggregation [" + pipelineAggregatorName + "]", parser.getTokenLocation());
         }
 
-        MovAvgPipelineAggregator.Factory factory = new MovAvgPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths);
+        ValueFormatter formatter = null;
         if (format != null) {
-            factory.format(format);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        } else {
+            formatter = ValueFormatter.RAW;
         }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
-        }
-        if (window != null) {
-            factory.window(window);
-        }
-        if (predict != null) {
-            factory.predict(predict);
+
+        MovAvgModel.AbstractModelParser modelParser = movAvgModelParserMapper.get(model);
+        if (modelParser == null) {
+            throw new SearchParseException(context, "Unknown model [" + model + "] specified.  Valid options are:"
+                    + movAvgModelParserMapper.getAllNames().toString(), parser.getTokenLocation());
         }
-        if (model != null) {
-            MovAvgModel.AbstractModelParser modelParser = movAvgModelParserMapper.get(model);
-            if (modelParser == null) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unknown model [" + model + "] specified.  Valid options are:" + movAvgModelParserMapper.getAllNames().toString());
-            }
 
-            MovAvgModel movAvgModel;
-            try {
-                movAvgModel = modelParser.parse(settings, pipelineAggregatorName, window, context.parseFieldMatcher());
-            } catch (ParseException exception) {
-                throw new ParsingException(parser.getTokenLocation(), "Could not parse settings for model [" + model + "].", exception);
-            }
-            factory.model(movAvgModel);
+        MovAvgModel movAvgModel;
+        try {
+            movAvgModel = modelParser.parse(settings, pipelineAggregatorName, window, context.parseFieldMatcher());
+        } catch (ParseException exception) {
+            throw new SearchParseException(context, "Could not parse settings for model [" + model + "].", null, exception);
         }
-        if (minimize != null) {
-            factory.minimize(minimize);
+
+        // If the user doesn't set a preference for cost minimization, ask what the model prefers
+        if (minimize == null) {
+            minimize = movAvgModel.minimizeByDefault();
+        } else if (minimize && !movAvgModel.canBeMinimized()) {
+            // If the user asks to minimize, but this model doesn't support it, throw exception
+            throw new SearchParseException(context, "The [" + model + "] model cannot be minimized.", null);
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new MovAvgPipelineAggregator.Factory(null, null);
+
+        return new MovAvgPipelineAggregator.Factory(pipelineAggregatorName, bucketsPaths, formatter, gapPolicy, window, predict,
+                movAvgModel, minimize);
     }
 
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
index c56ca26..4f7034b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java
@@ -22,7 +22,6 @@ package org.elasticsearch.search.aggregations.pipeline.movavg;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.aggregations.InternalAggregation;
@@ -38,8 +37,6 @@ import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelStreams;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 import org.joda.time.DateTime;
@@ -49,7 +46,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.ListIterator;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
@@ -280,157 +276,32 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private int window = 5;
-        private MovAvgModel model = new SimpleModel();
-        private int predict = 0;
-        private Boolean minimize;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private int window;
+        private MovAvgModel model;
+        private int predict;
+        private boolean minimize;
 
-        public Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, ValueFormatter formatter, GapPolicy gapPolicy,
+                       int window, int predict, MovAvgModel model, boolean minimize) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        /**
-         * Sets the GapPolicy to use on the output of this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the GapPolicy to use on the output of this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
-        }
-
-        /**
-         * Sets the window size for the moving average. This window will "slide"
-         * across the series, and the values inside that window will be used to
-         * calculate the moving avg value
-         *
-         * @param window
-         *            Size of window
-         */
-        public Factory window(int window) {
             this.window = window;
-            return this;
-        }
-
-        /**
-         * Gets the window size for the moving average. This window will "slide"
-         * across the series, and the values inside that window will be used to
-         * calculate the moving avg value
-         */
-        public int window() {
-            return window;
-        }
-
-        /**
-         * Sets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         *
-         * @param model
-         *            A MovAvgModel which has been prepopulated with settings
-         */
-        public Factory model(MovAvgModel model) {
             this.model = model;
-            return this;
-        }
-
-        /**
-         * Gets a MovAvgModel for the Moving Average. The model is used to
-         * define what type of moving average you want to use on the series
-         */
-        public MovAvgModel model() {
-            return model;
-        }
-
-        /**
-         * Sets the number of predictions that should be returned. Each
-         * prediction will be spaced at the intervals specified in the
-         * histogram. E.g "predict: 2" will return two new buckets at the end of
-         * the histogram with the predicted values.
-         *
-         * @param predict
-         *            Number of predictions to make
-         */
-        public Factory predict(int predict) {
             this.predict = predict;
-            return this;
-        }
-
-        /**
-         * Gets the number of predictions that should be returned. Each
-         * prediction will be spaced at the intervals specified in the
-         * histogram. E.g "predict: 2" will return two new buckets at the end of
-         * the histogram with the predicted values.
-         */
-        public int predict() {
-            return predict;
-        }
-
-        /**
-         * Sets whether the model should be fit to the data using a cost
-         * minimizing algorithm.
-         *
-         * @param minimize
-         *            If the model should be fit to the underlying data
-         */
-        public Factory minimize(boolean minimize) {
             this.minimize = minimize;
-            return this;
-        }
-
-        /**
-         * Gets whether the model should be fit to the data using a cost
-         * minimizing algorithm.
-         */
-        public Boolean minimize() {
-            return minimize;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            // If the user doesn't set a preference for cost minimization, ask
-            // what the model prefers
-            boolean minimize = this.minimize == null ? model.minimizeByDefault() : this.minimize;
-            return new MovAvgPipelineAggregator(name, bucketsPaths, formatter(), gapPolicy, window, predict, model, minimize, metaData);
+            return new MovAvgPipelineAggregator(name, bucketsPaths, formatter, gapPolicy, window, predict, model, minimize, metaData);
         }
 
         @Override
         public void doValidate(AggregatorFactory parent, AggregatorFactory[] aggFactories,
                 List<PipelineAggregatorFactory> pipelineAggregatoractories) {
-            if (minimize != null && minimize && !model.canBeMinimized()) {
-                // If the user asks to minimize, but this model doesn't support
-                // it, throw exception
-                throw new IllegalStateException("The [" + model + "] model cannot be minimized for aggregation [" + name + "]");
-            }
             if (bucketsPaths.length != 1) {
                 throw new IllegalStateException(PipelineAggregator.Parser.BUCKETS_PATH.getPreferredName()
                         + " must contain a single entry for aggregation [" + name + "]");
@@ -447,60 +318,5 @@ public class MovAvgPipelineAggregator extends PipelineAggregator {
             }
         }
 
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(MovAvgParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(MovAvgParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            model.toXContent(builder, params);
-            builder.field(MovAvgParser.WINDOW.getPreferredName(), window);
-            if (predict > 0) {
-                builder.field(MovAvgParser.PREDICT.getPreferredName(), predict);
-            }
-            if (minimize != null) {
-                builder.field(MovAvgParser.MINIMIZE.getPreferredName(), minimize);
-            }
-            return builder;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            factory.window = in.readVInt();
-            factory.model = MovAvgModelStreams.read(in);
-            factory.predict = in.readVInt();
-            factory.minimize = in.readOptionalBoolean();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-            out.writeVInt(window);
-            model.writeTo(out);
-            out.writeVInt(predict);
-            out.writeOptionalBoolean(minimize);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, window, model, predict, minimize);
-        }
-
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format)
-                    && Objects.equals(gapPolicy, other.gapPolicy)
-                    && Objects.equals(window, other.window)
-                    && Objects.equals(model, other.model)
-                    && Objects.equals(predict, other.predict)
-                    && Objects.equals(minimize, other.minimize);
-        }
-
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
index c424de8..84de794 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/EwmaModel.java
@@ -32,16 +32,13 @@ import java.text.ParseException;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a exponentially weighted moving average
  */
 public class EwmaModel extends MovAvgModel {
 
-    private static final EwmaModel PROTOTYPE = new EwmaModel();
     protected static final ParseField NAME_FIELD = new ParseField("ewma");
-    public static final double DEFAULT_ALPHA = 0.3;
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -51,10 +48,6 @@ public class EwmaModel extends MovAvgModel {
      */
     private final double alpha;
 
-    public EwmaModel() {
-        this(DEFAULT_ALPHA);
-    }
-
     public EwmaModel(double alpha) {
         this.alpha = alpha;
     }
@@ -104,7 +97,7 @@ public class EwmaModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new EwmaModel(in.readDouble());
         }
 
         @Override
@@ -114,42 +107,11 @@ public class EwmaModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new EwmaModel(in.readDouble());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        EwmaModel other = (EwmaModel) obj;
-        return Objects.equals(alpha, other.alpha);
-    }
-
     public static class SingleExpModelParser extends AbstractModelParser {
 
         @Override
@@ -161,7 +123,7 @@ public class EwmaModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
             checkUnrecognizedParams(settings);
             return new EwmaModel(alpha);
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
index 8734b71..fe0321b 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltLinearModel.java
@@ -31,17 +31,13 @@ import java.io.IOException;
 import java.text.ParseException;
 import java.util.Collection;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a doubly exponential weighted moving average
  */
 public class HoltLinearModel extends MovAvgModel {
 
-    private static final HoltLinearModel PROTOTYPE = new HoltLinearModel();
     protected static final ParseField NAME_FIELD = new ParseField("holt");
-    public static final double DEFAULT_ALPHA = 0.3;
-    public static final double DEFAULT_BETA = 0.1;
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -59,10 +55,6 @@ public class HoltLinearModel extends MovAvgModel {
      */
     private final double beta;
 
-    public HoltLinearModel() {
-        this(DEFAULT_ALPHA, DEFAULT_BETA);
-    }
-
     public HoltLinearModel(double alpha, double beta) {
         this.alpha = alpha;
         this.beta = beta;
@@ -165,7 +157,7 @@ public class HoltLinearModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new HoltLinearModel(in.readDouble(), in.readDouble());
         }
 
         @Override
@@ -175,45 +167,12 @@ public class HoltLinearModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.field("beta", beta);
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new HoltLinearModel(in.readDouble(), in.readDouble());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
         out.writeDouble(beta);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha, beta);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        HoltLinearModel other = (HoltLinearModel) obj;
-        return Objects.equals(alpha, other.alpha) 
-                && Objects.equals(beta, other.beta);
-    }
-
     public static class DoubleExpModelParser extends AbstractModelParser {
 
         @Override
@@ -225,8 +184,8 @@ public class HoltLinearModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
-            double beta = parseDoubleParam(settings, "beta", DEFAULT_BETA);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
+            double beta = parseDoubleParam(settings, "beta", 0.1);
             checkUnrecognizedParams(settings);
             return new HoltLinearModel(alpha, beta);
         }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
index 9f5ecad..55cf6be 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/HoltWintersModel.java
@@ -37,7 +37,6 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Calculate a triple exponential weighted moving average
@@ -45,13 +44,6 @@ import java.util.Objects;
 public class HoltWintersModel extends MovAvgModel {
 
     protected static final ParseField NAME_FIELD = new ParseField("holt_winters");
-    public static final double DEFAULT_ALPHA = 0.3;
-    public static final double DEFAULT_BETA = 0.1;
-    public static final double DEFAULT_GAMMA = 0.3;
-    public static final int DEFAULT_PERIOD = 1;
-    public static final SeasonalityType DEFAULT_SEASONALITY_TYPE = SeasonalityType.ADDITIVE;
-    public static final boolean DEFAULT_PAD = false;
-    private static final HoltWintersModel PROTOTYPE = new HoltWintersModel();
 
     /**
      * Controls smoothing of data.  Also known as "level" value.
@@ -167,9 +159,6 @@ public class HoltWintersModel extends MovAvgModel {
         }
     }
 
-    public HoltWintersModel() {
-        this(DEFAULT_ALPHA, DEFAULT_BETA, DEFAULT_GAMMA, DEFAULT_PERIOD, DEFAULT_SEASONALITY_TYPE, DEFAULT_PAD);
-    }
 
     public HoltWintersModel(double alpha, double beta, double gamma, int period, SeasonalityType seasonalityType, boolean pad) {
         this.alpha = alpha;
@@ -284,8 +273,8 @@ public class HoltWintersModel extends MovAvgModel {
             s += vs[i];
             b += (vs[i + period] - vs[i]) / period;
         }
-        s /= period;
-        b /= period;
+        s /= (double) period;
+        b /= (double) period;
         last_s = s;
 
         // Calculate first seasonal
@@ -335,7 +324,14 @@ public class HoltWintersModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            double alpha = in.readDouble();
+            double beta = in.readDouble();
+            double gamma = in.readDouble();
+            int period = in.readVInt();
+            SeasonalityType type = SeasonalityType.readFrom(in);
+            boolean pad = in.readBoolean();
+
+            return new HoltWintersModel(alpha, beta, gamma, period, type, pad);
         }
 
         @Override
@@ -345,26 +341,6 @@ public class HoltWintersModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        builder.startObject(MovAvgParser.SETTINGS.getPreferredName());
-        builder.field("alpha", alpha);
-        builder.field("beta", beta);
-        builder.field("gamma", gamma);
-        builder.field("period", period);
-        builder.field("pad", pad);
-        builder.field("type", seasonalityType.getName());
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new HoltWintersModel(in.readDouble(), in.readDouble(), in.readDouble(), in.readVInt(), SeasonalityType.readFrom(in),
-                in.readBoolean());
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
         out.writeDouble(alpha);
@@ -375,28 +351,6 @@ public class HoltWintersModel extends MovAvgModel {
         out.writeBoolean(pad);
     }
 
-    @Override
-    public int hashCode() {
-        return Objects.hash(alpha, beta, gamma, period, seasonalityType, pad);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        HoltWintersModel other = (HoltWintersModel) obj;
-        return Objects.equals(alpha, other.alpha) 
-                && Objects.equals(beta, other.beta)
-                && Objects.equals(gamma, other.gamma)
-                && Objects.equals(period, other.period)
-                && Objects.equals(seasonalityType, other.seasonalityType)
-                && Objects.equals(pad, other.pad);
-    }
-
     public static class HoltWintersModelParser extends AbstractModelParser {
 
         @Override
@@ -408,10 +362,10 @@ public class HoltWintersModel extends MovAvgModel {
         public MovAvgModel parse(@Nullable Map<String, Object> settings, String pipelineName, int windowSize,
                                  ParseFieldMatcher parseFieldMatcher) throws ParseException {
 
-            double alpha = parseDoubleParam(settings, "alpha", DEFAULT_ALPHA);
-            double beta = parseDoubleParam(settings, "beta", DEFAULT_BETA);
-            double gamma = parseDoubleParam(settings, "gamma", DEFAULT_GAMMA);
-            int period = parseIntegerParam(settings, "period", DEFAULT_PERIOD);
+            double alpha = parseDoubleParam(settings, "alpha", 0.3);
+            double beta = parseDoubleParam(settings, "beta", 0.1);
+            double gamma = parseDoubleParam(settings, "gamma", 0.3);
+            int period = parseIntegerParam(settings, "period", 1);
 
             if (windowSize < 2 * period) {
                 throw new ParseException("Field [window] must be at least twice as large as the period when " +
@@ -419,7 +373,7 @@ public class HoltWintersModel extends MovAvgModel {
                         + (2 * period), 0);
             }
 
-            SeasonalityType seasonalityType = DEFAULT_SEASONALITY_TYPE;
+            SeasonalityType seasonalityType = SeasonalityType.ADDITIVE;
 
             if (settings != null) {
                 Object value = settings.get("type");
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
index a5dfddf..264a425 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/LinearModel.java
@@ -40,7 +40,6 @@ import java.util.Map;
  */
 public class LinearModel extends MovAvgModel {
 
-    private static final LinearModel PROTOTYPE = new LinearModel();
     protected static final ParseField NAME_FIELD = new ParseField("linear");
 
 
@@ -86,7 +85,7 @@ public class LinearModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new LinearModel();
         }
 
         @Override
@@ -96,17 +95,6 @@ public class LinearModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new LinearModel();
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
     }
@@ -133,20 +121,4 @@ public class LinearModel extends MovAvgModel {
             return builder;
         }
     }
-
-    @Override
-    public int hashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        return true;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
index 92f4615..4bfac9d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/MovAvgModel.java
@@ -22,8 +22,6 @@ package org.elasticsearch.search.aggregations.pipeline.movavg.models;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-import org.elasticsearch.common.xcontent.ToXContent;
 
 import java.io.IOException;
 import java.text.ParseException;
@@ -31,7 +29,7 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.Map;
 
-public abstract class MovAvgModel implements Writeable<MovAvgModel>, ToXContent {
+public abstract class MovAvgModel {
 
     /**
      * Should this model be fit to the data via a cost minimizing algorithm by default?
@@ -118,21 +116,13 @@ public abstract class MovAvgModel implements Writeable<MovAvgModel>, ToXContent
      *
      * @param out   Output stream
      */
-    @Override
     public abstract void writeTo(StreamOutput out) throws IOException;
 
     /**
      * Clone the model, returning an exact copy
      */
-    @Override
     public abstract MovAvgModel clone();
 
-    @Override
-    public abstract int hashCode();
-
-    @Override
-    public abstract boolean equals(Object obj);
-
     /**
      * Abstract class which also provides some concrete parsing functionality.
      */
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
index 619654e..e0c7781 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/models/SimpleModel.java
@@ -38,7 +38,6 @@ import java.util.Map;
  */
 public class SimpleModel extends MovAvgModel {
 
-    private static final SimpleModel PROTOTYPE = new SimpleModel();
     protected static final ParseField NAME_FIELD = new ParseField("simple");
 
 
@@ -79,7 +78,7 @@ public class SimpleModel extends MovAvgModel {
     public static final MovAvgModelStreams.Stream STREAM = new MovAvgModelStreams.Stream() {
         @Override
         public MovAvgModel readResult(StreamInput in) throws IOException {
-            return PROTOTYPE.readFrom(in);
+            return new SimpleModel();
         }
 
         @Override
@@ -89,17 +88,6 @@ public class SimpleModel extends MovAvgModel {
     };
 
     @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(MovAvgParser.MODEL.getPreferredName(), NAME_FIELD.getPreferredName());
-        return builder;
-    }
-
-    @Override
-    public MovAvgModel readFrom(StreamInput in) throws IOException {
-        return new SimpleModel();
-    }
-
-    @Override
     public void writeTo(StreamOutput out) throws IOException {
         out.writeString(STREAM.getName());
     }
@@ -126,20 +114,4 @@ public class SimpleModel extends MovAvgModel {
             return builder;
         }
     }
-
-    @Override
-    public int hashCode() {
-        return 0;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (getClass() != obj.getClass()) {
-            return false;
-        }
-        return true;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
index 9b48d1c..109cbcc 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffParser.java
@@ -20,17 +20,20 @@
 package org.elasticsearch.search.aggregations.pipeline.serialdiff;
 
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
+
 public class SerialDiffParser implements PipelineAggregator.Parser {
 
     public static final ParseField FORMAT = new ParseField("format");
@@ -43,13 +46,13 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
     }
 
     @Override
-    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, QueryParseContext context) throws IOException {
+    public PipelineAggregatorFactory parse(String reducerName, XContentParser parser, SearchContext context) throws IOException {
         XContentParser.Token token;
         String currentFieldName = null;
         String[] bucketsPaths = null;
         String format = null;
-        GapPolicy gapPolicy = null;
-        Integer lag = null;
+        GapPolicy gapPolicy = GapPolicy.SKIP;
+        int lag = 1;
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -62,21 +65,20 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
                 } else if (context.parseFieldMatcher().match(currentFieldName, GAP_POLICY)) {
                     gapPolicy = GapPolicy.parse(context, parser.text(), parser.getTokenLocation());
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.VALUE_NUMBER) {
                 if (context.parseFieldMatcher().match(currentFieldName, LAG)) {
                     lag = parser.intValue(true);
                     if (lag <= 0) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Lag must be a positive, non-zero integer.  Value supplied was" +
+                        throw new SearchParseException(context, "Lag must be a positive, non-zero integer.  Value supplied was" +
                                 lag + " in [" + reducerName + "]: ["
-                                        + currentFieldName + "].");
+                                + currentFieldName + "].", parser.getTokenLocation());
                     }
                 }  else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if (context.parseFieldMatcher().match(currentFieldName, BUCKETS_PATH)) {
@@ -87,36 +89,28 @@ public class SerialDiffParser implements PipelineAggregator.Parser {
                     }
                     bucketsPaths = paths.toArray(new String[paths.size()]);
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unknown key for a " + token + " in [" + reducerName + "]: [" + currentFieldName + "].");
+                    throw new SearchParseException(context, "Unknown key for a " + token + " in [" + reducerName + "]: ["
+                            + currentFieldName + "].", parser.getTokenLocation());
                 }
             } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unexpected token " + token + " in [" + reducerName + "].",
+                throw new SearchParseException(context, "Unexpected token " + token + " in [" + reducerName + "].",
                         parser.getTokenLocation());
             }
         }
 
         if (bucketsPaths == null) {
-            throw new ParsingException(parser.getTokenLocation(),
-                    "Missing required field [" + BUCKETS_PATH.getPreferredName() + "] for derivative aggregation [" + reducerName + "]");
+            throw new SearchParseException(context, "Missing required field [" + BUCKETS_PATH.getPreferredName()
+                    + "] for derivative aggregation [" + reducerName + "]", parser.getTokenLocation());
         }
 
-        SerialDiffPipelineAggregator.Factory factory = new SerialDiffPipelineAggregator.Factory(reducerName, bucketsPaths);
-        if (lag != null) {
-            factory.lag(lag);
-        }
+        ValueFormatter formatter;
         if (format != null) {
-            factory.format(format);
-        }
-        if (gapPolicy != null) {
-            factory.gapPolicy(gapPolicy);
+            formatter = ValueFormat.Patternable.Number.format(format).formatter();
+        }  else {
+            formatter = ValueFormatter.RAW;
         }
-        return factory;
-    }
 
-    @Override
-    public PipelineAggregatorFactory getFactoryPrototype() {
-        return new SerialDiffPipelineAggregator.Factory(null, null);
+        return new SerialDiffPipelineAggregator.Factory(reducerName, bucketsPaths, formatter, gapPolicy, lag);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
index db42812..5df97d3 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java
@@ -23,18 +23,15 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;
 import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.InternalAggregations;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorStreams;
-import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
 import org.elasticsearch.search.aggregations.support.format.ValueFormatterStreams;
 
@@ -42,10 +39,10 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 import java.util.stream.Collectors;
 import java.util.stream.StreamSupport;
 
+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
 import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;
 
 public class SerialDiffPipelineAggregator extends PipelineAggregator {
@@ -147,108 +144,20 @@ public class SerialDiffPipelineAggregator extends PipelineAggregator {
 
     public static class Factory extends PipelineAggregatorFactory {
 
-        private String format;
-        private GapPolicy gapPolicy = GapPolicy.SKIP;
-        private int lag = 1;
+        private final ValueFormatter formatter;
+        private GapPolicy gapPolicy;
+        private int lag;
 
-        public Factory(String name, String[] bucketsPaths) {
+        public Factory(String name, String[] bucketsPaths, @Nullable ValueFormatter formatter, GapPolicy gapPolicy, int lag) {
             super(name, TYPE.name(), bucketsPaths);
-        }
-
-        /**
-         * Sets the lag to use when calculating the serial difference.
-         */
-        public Factory lag(int lag) {
-            this.lag = lag;
-            return this;
-        }
-
-        /**
-         * Gets the lag to use when calculating the serial difference.
-         */
-        public int lag() {
-            return lag;
-        }
-
-        /**
-         * Sets the format to use on the output of this aggregation.
-         */
-        public Factory format(String format) {
-            this.format = format;
-            return this;
-        }
-
-        /**
-         * Gets the format to use on the output of this aggregation.
-         */
-        public String format() {
-            return format;
-        }
-
-        /**
-         * Sets the GapPolicy to use on the output of this aggregation.
-         */
-        public Factory gapPolicy(GapPolicy gapPolicy) {
+            this.formatter = formatter;
             this.gapPolicy = gapPolicy;
-            return this;
-        }
-
-        /**
-         * Gets the GapPolicy to use on the output of this aggregation.
-         */
-        public GapPolicy gapPolicy() {
-            return gapPolicy;
-        }
-
-        protected ValueFormatter formatter() {
-            if (format != null) {
-                return ValueFormat.Patternable.Number.format(format).formatter();
-            } else {
-                return ValueFormatter.RAW;
-            }
+            this.lag = lag;
         }
 
         @Override
         protected PipelineAggregator createInternal(Map<String, Object> metaData) throws IOException {
-            return new SerialDiffPipelineAggregator(name, bucketsPaths, formatter(), gapPolicy, lag, metaData);
-        }
-
-        @Override
-        protected XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-            if (format != null) {
-                builder.field(SerialDiffParser.FORMAT.getPreferredName(), format);
-            }
-            builder.field(SerialDiffParser.GAP_POLICY.getPreferredName(), gapPolicy.getName());
-            builder.field(SerialDiffParser.LAG.getPreferredName(), lag);
-            return builder;
-        }
-
-        @Override
-        protected PipelineAggregatorFactory doReadFrom(String name, String[] bucketsPaths, StreamInput in) throws IOException {
-            Factory factory = new Factory(name, bucketsPaths);
-            factory.format = in.readOptionalString();
-            factory.gapPolicy = GapPolicy.readFrom(in);
-            factory.lag = in.readVInt();
-            return factory;
-        }
-
-        @Override
-        protected void doWriteTo(StreamOutput out) throws IOException {
-            out.writeOptionalString(format);
-            gapPolicy.writeTo(out);
-            out.writeVInt(lag);
-        }
-
-        @Override
-        protected int doHashCode() {
-            return Objects.hash(format, gapPolicy, lag);
-        }
-        @Override
-        protected boolean doEquals(Object obj) {
-            Factory other = (Factory) obj;
-            return Objects.equals(format, other.format)
-                    && Objects.equals(gapPolicy, other.gapPolicy)
-                    && Objects.equals(lag, other.lag);
+            return new SerialDiffPipelineAggregator(name, bucketsPaths, formatter, gapPolicy, lag, metaData);
         }
 
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java
deleted file mode 100644
index 12bae3b..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AbstractValuesSourceParser.java
+++ /dev/null
@@ -1,208 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.joda.time.DateTimeZone;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- *
- */
-public abstract class AbstractValuesSourceParser<VS extends ValuesSource>
-        implements Aggregator.Parser {
-    static final ParseField TIME_ZONE = new ParseField("time_zone");
-
-    public abstract static class AnyValuesSourceParser extends AbstractValuesSourceParser<ValuesSource> {
-
-        protected AnyValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.ANY, null);
-        }
-    }
-
-    public abstract static class NumericValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.Numeric> {
-
-        protected NumericValuesSourceParser(boolean scriptable, boolean formattable, boolean timezoneAware) {
-            super(scriptable, formattable, timezoneAware, ValuesSourceType.NUMERIC, ValueType.NUMERIC);
-        }
-    }
-
-    public abstract static class BytesValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.Bytes> {
-
-        protected BytesValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.BYTES, ValueType.STRING);
-        }
-    }
-
-    public abstract static class GeoPointValuesSourceParser extends AbstractValuesSourceParser<ValuesSource.GeoPoint> {
-
-        protected GeoPointValuesSourceParser(boolean scriptable, boolean formattable) {
-            super(scriptable, formattable, false, ValuesSourceType.GEOPOINT, ValueType.GEOPOINT);
-        }
-    }
-
-    private boolean scriptable = true;
-    private boolean formattable = false;
-    private boolean timezoneAware = false;
-    private ValuesSourceType valuesSourceType = null;
-    private ValueType targetValueType = null;
-
-    private AbstractValuesSourceParser(boolean scriptable, boolean formattable, boolean timezoneAware, ValuesSourceType valuesSourceType,
-            ValueType targetValueType) {
-        this.timezoneAware = timezoneAware;
-        this.valuesSourceType = valuesSourceType;
-        this.targetValueType = targetValueType;
-        this.scriptable = scriptable;
-        this.formattable = formattable;
-    }
-
-    @Override
-    public final ValuesSourceAggregatorFactory<VS, ?> parse(String aggregationName, XContentParser parser, QueryParseContext context)
-            throws IOException {
-
-        String field = null;
-        Script script = null;
-        ValueType valueType = null;
-        String format = null;
-        Object missing = null;
-        DateTimeZone timezone = null;
-        Map<ParseField, Object> otherOptions = new HashMap<>();
-
-        XContentParser.Token token;
-        String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if ("missing".equals(currentFieldName) && token.isValue()) {
-                missing = parser.objectText();
-            } else if (timezoneAware && context.parseFieldMatcher().match(currentFieldName, TIME_ZONE)) {
-                if (token == XContentParser.Token.VALUE_STRING) {
-                    timezone = DateTimeZone.forID(parser.text());
-                } else if (token == XContentParser.Token.VALUE_NUMBER) {
-                    timezone = DateTimeZone.forOffsetHours(parser.intValue());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (token == XContentParser.Token.VALUE_STRING) {
-                if ("field".equals(currentFieldName)) {
-                    field = parser.text();
-                } else if (formattable && "format".equals(currentFieldName)) {
-                    format = parser.text();
-                } else if (scriptable) {
-                    if ("value_type".equals(currentFieldName) || "valueType".equals(currentFieldName)) {
-                        valueType = ValueType.resolveForScript(parser.text());
-                        if (targetValueType != null && valueType.isNotA(targetValueType)) {
-                            throw new ParsingException(parser.getTokenLocation(),
-                                    type() + " aggregation [" + aggregationName + "] was configured with an incompatible value type ["
-                                            + valueType + "]. [" + type() + "] aggregation can only work on value of type ["
-                                            + targetValueType + "]");
-                        }
-                    } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                        throw new ParsingException(parser.getTokenLocation(),
-                                "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                    }
-                } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (scriptable && token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, ScriptField.SCRIPT)) {
-                    script = Script.parse(parser, context.parseFieldMatcher());
-                } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                    throw new ParsingException(parser.getTokenLocation(),
-                            "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-                }
-            } else if (!token(aggregationName, currentFieldName, token, parser, context.parseFieldMatcher(), otherOptions)) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "Unexpected token " + token + " [" + currentFieldName + "] in [" + aggregationName + "].");
-            }
-        }
-
-        ValuesSourceAggregatorFactory<VS, ?> factory = createFactory(aggregationName, this.valuesSourceType, this.targetValueType,
-                otherOptions);
-        factory.field(field);
-        factory.script(script);
-        factory.valueType(valueType);
-        factory.format(format);
-        factory.missing(missing);
-        factory.timeZone(timezone);
-        return factory;
-    }
-
-    /**
-     * Creates a {@link ValuesSourceAggregatorFactory} from the information
-     * gathered by the subclass. Options parsed in
-     * {@link AbstractValuesSourceParser} itself will be added to the factory
-     * after it has been returned by this method.
-     *
-     * @param aggregationName
-     *            the name of the aggregation
-     * @param valuesSourceType
-     *            the type of the {@link ValuesSource}
-     * @param targetValueType
-     *            the target type of the final value output by the aggregation
-     * @param otherOptions
-     *            a {@link Map} containing the extra options parsed by the
-     *            {@link #token(String, String, org.elasticsearch.common.xcontent.XContentParser.Token, XContentParser, ParseFieldMatcher, Map)}
-     *            method
-     * @return the created factory
-     */
-    protected abstract ValuesSourceAggregatorFactory<VS, ?> createFactory(String aggregationName, ValuesSourceType valuesSourceType,
-            ValueType targetValueType, Map<ParseField, Object> otherOptions);
-
-    /**
-     * Allows subclasses of {@link AbstractValuesSourceParser} to parse extra
-     * parameters and store them in a {@link Map} which will later be passed to
-     * {@link #createFactory(String, ValuesSourceType, ValueType, Map)}.
-     *
-     * @param aggregationName
-     *            the name of the aggregation
-     * @param currentFieldName
-     *            the name of the current field being parsed
-     * @param token
-     *            the current token for the parser
-     * @param parser
-     *            the parser
-     * @param parseFieldMatcher
-     *            the {@link ParseFieldMatcher} to use to match field names
-     * @param otherOptions
-     *            a {@link Map} of options to be populated by successive calls
-     *            to this method which will then be passed to the
-     *            {@link #createFactory(String, ValuesSourceType, ValueType, Map)}
-     *            method
-     * @return <code>true</code> if the current token was correctly parsed,
-     *         <code>false</code> otherwise
-     * @throws IOException
-     *             if an error occurs whilst parsing
-     */
-    protected abstract boolean token(String aggregationName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException;
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
index 14e8481..ee91782 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
@@ -70,11 +70,13 @@ public class AggregationContext {
             if (config.missing == null) {
                 // otherwise we will have values because of the missing value
                 vs = null;
-            } else if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+            } else if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.Numeric.EMPTY;
-            } else if (config.valueSourceType == ValuesSourceType.GEOPOINT) {
+            } else if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.GeoPoint.EMPTY;
-            } else if (config.valueSourceType == ValuesSourceType.ANY || config.valueSourceType == ValuesSourceType.BYTES) {
+            } else if (ValuesSource.class.isAssignableFrom(config.valueSourceType)
+                    || ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType)
+                    || ValuesSource.Bytes.WithOrdinals.class.isAssignableFrom(config.valueSourceType)) {
                 vs = (VS) ValuesSource.Bytes.EMPTY;
             } else {
                 throw new SearchParseException(searchContext, "Can't deal with unmapped ValuesSource type " + config.valueSourceType, null);
@@ -130,20 +132,19 @@ public class AggregationContext {
      */
     private <VS extends ValuesSource> VS originalValuesSource(ValuesSourceConfig<VS> config) throws IOException {
         if (config.fieldContext == null) {
-            if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+            if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
                 return (VS) numericScript(config);
             }
-            if (config.valueSourceType == ValuesSourceType.BYTES) {
+            if (ValuesSource.Bytes.class.isAssignableFrom(config.valueSourceType)) {
                 return (VS) bytesScript(config);
             }
-            throw new AggregationExecutionException("value source of type [" + config.valueSourceType.name()
-                    + "] is not supported by scripts");
+            throw new AggregationExecutionException("value source of type [" + config.valueSourceType.getSimpleName() + "] is not supported by scripts");
         }
 
-        if (config.valueSourceType == ValuesSourceType.NUMERIC) {
+        if (ValuesSource.Numeric.class.isAssignableFrom(config.valueSourceType)) {
             return (VS) numericField(config);
         }
-        if (config.valueSourceType == ValuesSourceType.GEOPOINT) {
+        if (ValuesSource.GeoPoint.class.isAssignableFrom(config.valueSourceType)) {
             return (VS) geoPointField(config);
         }
         // falling back to bytes values
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
index fd2f363..3dfab20 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/GeoPointParser.java
@@ -19,39 +19,41 @@
 
 package org.elasticsearch.search.aggregations.support;
 
-
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  *
  */
 public class GeoPointParser {
 
+    private final String aggName;
     private final InternalAggregation.Type aggType;
+    private final SearchContext context;
     private final ParseField field;
 
-    public GeoPointParser(InternalAggregation.Type aggType, ParseField field) {
+    GeoPoint point;
+
+    public GeoPointParser(String aggName, InternalAggregation.Type aggType, SearchContext context, ParseField field) {
+        this.aggName = aggName;
         this.aggType = aggType;
+        this.context = context;
         this.field = field;
     }
 
-    public boolean token(String aggName, String currentFieldName, XContentParser.Token token, XContentParser parser,
-            ParseFieldMatcher parseFieldMatcher, Map<ParseField, Object> otherOptions) throws IOException {
-        if (!parseFieldMatcher.match(currentFieldName, field)) {
+    public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
+        if (!context.parseFieldMatcher().match(currentFieldName, field)) {
             return false;
         }
         if (token == XContentParser.Token.VALUE_STRING) {
-            GeoPoint point = new GeoPoint();
+            point = new GeoPoint();
             point.resetFromString(parser.text());
-            otherOptions.put(field, point);
             return true;
         }
         if (token == XContentParser.Token.START_ARRAY) {
@@ -63,12 +65,12 @@ public class GeoPointParser {
                 } else if (Double.isNaN(lat)) {
                     lat = parser.doubleValue();
                 } else {
-                    throw new ParsingException(parser.getTokenLocation(), "malformed [" + currentFieldName + "] geo point array in ["
-                            + aggName + "] " + aggType + " aggregation. a geo point array must be of the form [lon, lat]");
+                    throw new SearchParseException(context, "malformed [" + currentFieldName + "] geo point array in [" +
+                            aggName + "] " + aggType + " aggregation. a geo point array must be of the form [lon, lat]", 
+                            parser.getTokenLocation());
                 }
             }
-            GeoPoint point = new GeoPoint(lat, lon);
-            otherOptions.put(field, point);
+            point = new GeoPoint(lat, lon);
             return true;
         }
         if (token == XContentParser.Token.START_OBJECT) {
@@ -86,15 +88,17 @@ public class GeoPointParser {
                 }
             }
             if (Double.isNaN(lat) || Double.isNaN(lon)) {
-                throw new ParsingException(parser.getTokenLocation(),
-                        "malformed [" + currentFieldName + "] geo point object. either [lat] or [lon] (or both) are " + "missing in ["
-                                + aggName + "] " + aggType + " aggregation");
+                throw new SearchParseException(context, "malformed [" + currentFieldName + "] geo point object. either [lat] or [lon] (or both) are " +
+                        "missing in [" + aggName + "] " + aggType + " aggregation", parser.getTokenLocation());
             }
-            GeoPoint point = new GeoPoint(lat, lon);
-            otherOptions.put(field, point);
+            point = new GeoPoint(lat, lon);
             return true;
         }
         return false;
     }
 
+    public GeoPoint geoPoint() {
+        return point;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
index bdf1e55..0ae99b4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValueType.java
@@ -19,33 +19,26 @@
 
 package org.elasticsearch.search.aggregations.support;
 
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
 import org.elasticsearch.index.fielddata.IndexNumericFieldData;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 
-import java.io.IOException;
-
 /**
  *
  */
-public enum ValueType implements Writeable<ValueType> {
+public enum ValueType {
 
     @Deprecated
-    ANY((byte) 0, "any", ValuesSourceType.ANY, IndexFieldData.class, ValueFormat.RAW), STRING((byte) 1, "string", ValuesSourceType.BYTES,
-            IndexFieldData.class,
+    ANY("any", ValuesSource.class, IndexFieldData.class, ValueFormat.RAW), STRING("string", ValuesSource.Bytes.class, IndexFieldData.class,
             ValueFormat.RAW),
- LONG((byte) 2, "byte|short|integer|long", ValuesSourceType.NUMERIC,
-            IndexNumericFieldData.class, ValueFormat.RAW) {
+    LONG("byte|short|integer|long", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    DOUBLE((byte) 3, "float|double", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    DOUBLE("float|double", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
@@ -56,31 +49,31 @@ public enum ValueType implements Writeable<ValueType> {
             return true;
         }
     },
-    NUMBER((byte) 4, "number", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    NUMBER("number", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    DATE((byte) 5, "date", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.DateTime.DEFAULT) {
+    DATE("date", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.DateTime.DEFAULT) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    IP((byte) 6, "ip", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.IPv4) {
+    IP("ip", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.IPv4) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    NUMERIC((byte) 7, "numeric", ValuesSourceType.NUMERIC, IndexNumericFieldData.class, ValueFormat.RAW) {
+    NUMERIC("numeric", ValuesSource.Numeric.class, IndexNumericFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isNumeric() {
             return true;
         }
     },
-    GEOPOINT((byte) 8, "geo_point", ValuesSourceType.GEOPOINT, IndexGeoPointFieldData.class, ValueFormat.RAW) {
+    GEOPOINT("geo_point", ValuesSource.GeoPoint.class, IndexGeoPointFieldData.class, ValueFormat.RAW) {
         @Override
         public boolean isGeoPoint() {
             return true;
@@ -88,14 +81,11 @@ public enum ValueType implements Writeable<ValueType> {
     };
 
     final String description;
-    final ValuesSourceType valuesSourceType;
+    final Class<? extends ValuesSource> valuesSourceType;
     final Class<? extends IndexFieldData> fieldDataType;
     final ValueFormat defaultFormat;
-    private final byte id;
 
-    private ValueType(byte id, String description, ValuesSourceType valuesSourceType, Class<? extends IndexFieldData> fieldDataType,
-            ValueFormat defaultFormat) {
-        this.id = id;
+    private ValueType(String description, Class<? extends ValuesSource> valuesSourceType, Class<? extends IndexFieldData> fieldDataType, ValueFormat defaultFormat) {
         this.description = description;
         this.valuesSourceType = valuesSourceType;
         this.fieldDataType = fieldDataType;
@@ -106,7 +96,7 @@ public enum ValueType implements Writeable<ValueType> {
         return description;
     }
 
-    public ValuesSourceType getValuesSourceType() {
+    public Class<? extends ValuesSource> getValuesSourceType() {
         return valuesSourceType;
     }
 
@@ -115,7 +105,7 @@ public enum ValueType implements Writeable<ValueType> {
     }
 
     public boolean isA(ValueType valueType) {
-        return valueType.valuesSourceType == valuesSourceType &&
+        return valueType.valuesSourceType.isAssignableFrom(valuesSourceType) &&
                 valueType.fieldDataType.isAssignableFrom(fieldDataType);
     }
 
@@ -159,20 +149,4 @@ public enum ValueType implements Writeable<ValueType> {
     public String toString() {
         return description;
     }
-
-    @Override
-    public ValueType readFrom(StreamInput in) throws IOException {
-        byte id = in.readByte();
-        for (ValueType valueType : values()) {
-            if (id == valueType.id) {
-                return valueType;
-            }
-        }
-        throw new IOException("No valueType found for id [" + id + "]");
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeByte(id);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
index 68b33cc..d0eaec2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceAggregatorFactory.java
@@ -18,195 +18,40 @@
  */
 package org.elasticsearch.search.aggregations.support;
 
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
-import org.elasticsearch.index.fielddata.IndexNumericFieldData;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.mapper.core.NumberFieldMapper;
-import org.elasticsearch.index.mapper.ip.IpFieldMapper;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.AggregationInitializationException;
 import org.elasticsearch.search.aggregations.Aggregator;
 import org.elasticsearch.search.aggregations.AggregatorFactories;
 import org.elasticsearch.search.aggregations.AggregatorFactory;
-import org.elasticsearch.search.aggregations.InternalAggregation.Type;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
 import org.elasticsearch.search.aggregations.support.format.ValueFormat;
-import org.elasticsearch.search.internal.SearchContext;
-import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
-import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  *
  */
-public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF extends ValuesSourceAggregatorFactory<VS, AF>>
-        extends AggregatorFactory<AF> {
+public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource> extends AggregatorFactory {
 
-    public static abstract class LeafOnly<VS extends ValuesSource, AF extends ValuesSourceAggregatorFactory<VS, AF>>
-            extends ValuesSourceAggregatorFactory<VS, AF> {
+    public static abstract class LeafOnly<VS extends ValuesSource> extends ValuesSourceAggregatorFactory<VS> {
 
-        protected LeafOnly(String name, Type type, ValuesSourceParser.Input<VS> input) {
-            super(name, type, input);
-        }
-
-        protected LeafOnly(String name, Type type, ValuesSourceType valuesSourceType, ValueType targetValueType) {
-            super(name, type, valuesSourceType, targetValueType);
+        protected LeafOnly(String name, String type, ValuesSourceConfig<VS> valuesSourceConfig) {
+            super(name, type, valuesSourceConfig);
         }
 
         @Override
-        public AF subFactories(AggregatorFactories subFactories) {
+        public AggregatorFactory subFactories(AggregatorFactories subFactories) {
             throw new AggregationInitializationException("Aggregator [" + name + "] of type [" + type + "] cannot accept sub-aggregations");
         }
     }
 
-    private final ValuesSourceType valuesSourceType;
-    private final ValueType targetValueType;
-    private String field = null;
-    private Script script = null;
-    private ValueType valueType = null;
-    private String format = null;
-    private Object missing = null;
-    private DateTimeZone timeZone;
     protected ValuesSourceConfig<VS> config;
 
-    // NORELEASE remove this method when aggs refactoring complete
-    /**
-     * This constructor remains here until all subclasses have been moved to the
-     * new constructor. This also means moving from using
-     * {@link ValuesSourceParser} to using {@link AbstractValuesSourceParser}.
-     */
-    @Deprecated
-    protected ValuesSourceAggregatorFactory(String name, Type type, ValuesSourceParser.Input<VS> input) {
+    protected ValuesSourceAggregatorFactory(String name, String type, ValuesSourceConfig<VS> config) {
         super(name, type);
-        this.valuesSourceType = input.valuesSourceType;
-        this.targetValueType = input.targetValueType;
-        this.field = input.field;
-        this.script = input.script;
-        this.valueType = input.valueType;
-        this.format = input.format;
-        this.missing = input.missing;
-        this.timeZone = input.timezone;
-    }
-
-    protected ValuesSourceAggregatorFactory(String name, Type type, ValuesSourceType valuesSourceType, ValueType targetValueType) {
-        super(name, type);
-        this.valuesSourceType = valuesSourceType;
-        this.targetValueType = targetValueType;
-    }
-
-    /**
-     * Sets the field to use for this aggregation.
-     */
-    public AF field(String field) {
-        this.field = field;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the field to use for this aggregation.
-     */
-    public String field() {
-        return field;
-    }
-
-    /**
-     * Sets the script to use for this aggregation.
-     */
-    public AF script(Script script) {
-        this.script = script;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the script to use for this aggregation.
-     */
-    public Script script() {
-        return script;
-    }
-
-    /**
-     * Sets the {@link ValueType} for the value produced by this aggregation
-     */
-    public AF valueType(ValueType valueType) {
-        this.valueType = valueType;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the {@link ValueType} for the value produced by this aggregation
-     */
-    public ValueType valueType() {
-        return valueType;
-    }
-
-    /**
-     * Sets the format to use for the output of the aggregation.
-     */
-    public AF format(String format) {
-        this.format = format;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the format to use for the output of the aggregation.
-     */
-    public String format() {
-        return format;
-    }
-
-    /**
-     * Sets the value to use when the aggregation finds a missing value in a
-     * document
-     */
-    public AF missing(Object missing) {
-        this.missing = missing;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the value to use when the aggregation finds a missing value in a
-     * document
-     */
-    public Object missing() {
-        return missing;
-    }
-
-    /**
-     * Sets the time zone to use for this aggregation
-     */
-    public AF timeZone(DateTimeZone timeZone) {
-        this.timeZone = timeZone;
-        return (AF) this;
-    }
-
-    /**
-     * Gets the time zone to use for this aggregation
-     */
-    public DateTimeZone timeZone() {
-        return timeZone;
-    }
-
-    @Override
-    public void doInit(AggregationContext context) {
-        this.config = config(context);
-        if (config == null || !config.valid()) {
-            resolveValuesSourceConfigFromAncestors(name, this.parent, config.valueSourceType());
-        }
-
+        this.config = config;
     }
 
     @Override
@@ -221,101 +66,9 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF
 
     @Override
     public void doValidate() {
-    }
-
-    public ValuesSourceConfig<VS> config(AggregationContext context) {
-
-        ValueType valueType = this.valueType != null ? this.valueType : targetValueType;
-
-        if (field == null) {
-            if (script == null) {
-                ValuesSourceConfig<VS> config = new ValuesSourceConfig(ValuesSourceType.ANY);
-                config.format = resolveFormat(null, valueType);
-                return config;
-            }
-            ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : this.valuesSourceType;
-            if (valuesSourceType == null || valuesSourceType == ValuesSourceType.ANY) {
-                // the specific value source type is undefined, but for scripts,
-                // we need to have a specific value source
-                // type to know how to handle the script values, so we fallback
-                // on Bytes
-                valuesSourceType = ValuesSourceType.BYTES;
-            }
-            ValuesSourceConfig<VS> config = new ValuesSourceConfig<VS>(valuesSourceType);
-            config.missing = missing;
-            config.format = resolveFormat(format, valueType);
-            config.script = createScript(script, context.searchContext());
-            config.scriptValueType = valueType;
-            return config;
-        }
-
-        MappedFieldType fieldType = context.searchContext().smartNameFieldType(field);
-        if (fieldType == null) {
-            ValuesSourceType valuesSourceType = valueType != null ? valueType.getValuesSourceType() : this.valuesSourceType;
-            ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType);
-            config.missing = missing;
-            config.format = resolveFormat(format, valueType);
-            config.unmapped = true;
-            if (valueType != null) {
-                // todo do we really need this for unmapped?
-                config.scriptValueType = valueType;
-            }
-            return config;
-        }
-
-        IndexFieldData<?> indexFieldData = context.searchContext().fieldData().getForField(fieldType);
-
-        ValuesSourceConfig config;
-        if (valuesSourceType == ValuesSourceType.ANY) {
-            if (indexFieldData instanceof IndexNumericFieldData) {
-                config = new ValuesSourceConfig<>(ValuesSourceType.NUMERIC);
-            } else if (indexFieldData instanceof IndexGeoPointFieldData) {
-                config = new ValuesSourceConfig<>(ValuesSourceType.GEOPOINT);
-            } else {
-                config = new ValuesSourceConfig<>(ValuesSourceType.BYTES);
-            }
-        } else {
-            config = new ValuesSourceConfig(valuesSourceType);
-        }
-
-        config.fieldContext = new FieldContext(field, indexFieldData, fieldType);
-        config.missing = missing;
-        config.script = createScript(script, context.searchContext());
-        config.format = resolveFormat(format, this.timeZone, fieldType);
-        return config;
-    }
-
-    private SearchScript createScript(Script script, SearchContext context) {
-        return script == null ? null
-                : context.scriptService().search(context.lookup(), script, ScriptContext.Standard.AGGS, Collections.emptyMap());
-    }
-
-    private static ValueFormat resolveFormat(@Nullable String format, @Nullable ValueType valueType) {
-        if (valueType == null) {
-            return ValueFormat.RAW; // we can't figure it out
-        }
-        ValueFormat valueFormat = valueType.defaultFormat;
-        if (valueFormat != null && valueFormat instanceof ValueFormat.Patternable && format != null) {
-            return ((ValueFormat.Patternable) valueFormat).create(format);
-        }
-        return valueFormat;
-    }
-
-    private static ValueFormat resolveFormat(@Nullable String format, @Nullable DateTimeZone timezone, MappedFieldType fieldType) {
-        if (fieldType instanceof DateFieldMapper.DateFieldType) {
-            return format != null ? ValueFormat.DateTime.format(format, timezone) : ValueFormat.DateTime.mapper(
-                    (DateFieldMapper.DateFieldType) fieldType, timezone);
-        }
-        if (fieldType instanceof IpFieldMapper.IpFieldType) {
-            return ValueFormat.IPv4;
-        }
-        if (fieldType instanceof BooleanFieldMapper.BooleanFieldType) {
-            return ValueFormat.BOOLEAN;
-        }
-        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
-            return format != null ? ValueFormat.Number.format(format) : ValueFormat.RAW;
+        if (config == null || !config.valid()) {
+            resolveValuesSourceConfigFromAncestors(name, parent, config.valueSourceType());
         }
-        return ValueFormat.RAW;
     }
 
     protected abstract Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
@@ -325,18 +78,16 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF
             boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
             throws IOException;
 
-    private void resolveValuesSourceConfigFromAncestors(String aggName, AggregatorFactory parent, ValuesSourceType requiredValuesSourceType) {
+    private void resolveValuesSourceConfigFromAncestors(String aggName, AggregatorFactory parent, Class<VS> requiredValuesSourceType) {
         ValuesSourceConfig config;
         while (parent != null) {
             if (parent instanceof ValuesSourceAggregatorFactory) {
                 config = ((ValuesSourceAggregatorFactory) parent).config;
                 if (config != null && config.valid()) {
-                    if (requiredValuesSourceType == null || requiredValuesSourceType == ValuesSourceType.ANY
-                            || requiredValuesSourceType == config.valueSourceType) {
+                    if (requiredValuesSourceType == null || requiredValuesSourceType.isAssignableFrom(config.valueSourceType)) {
                         ValueFormat format = config.format;
                         this.config = config;
-                        // if the user explicitly defined a format pattern,
-                        // we'll do our best to keep it even when we inherit the
+                        // if the user explicitly defined a format pattern, we'll do our best to keep it even when we inherit the
                         // value source form one of the ancestor aggregations
                         if (this.config.formatPattern != null && format != null && format instanceof ValueFormat.Patternable) {
                             this.config.format = ((ValueFormat.Patternable) format).create(this.config.formatPattern);
@@ -349,136 +100,4 @@ public abstract class ValuesSourceAggregatorFactory<VS extends ValuesSource, AF
         }
         throw new AggregationExecutionException("could not find the appropriate value context to perform aggregation [" + aggName + "]");
     }
-
-    @Override
-    protected final void doWriteTo(StreamOutput out) throws IOException {
-        valuesSourceType.writeTo(out);
-        boolean hasTargetValueType = targetValueType != null;
-        out.writeBoolean(hasTargetValueType);
-        if (hasTargetValueType) {
-            targetValueType.writeTo(out);
-        }
-        innerWriteTo(out);
-        out.writeOptionalString(field);
-        boolean hasScript = script != null;
-        out.writeBoolean(hasScript);
-        if (hasScript) {
-            script.writeTo(out);
-        }
-        boolean hasValueType = valueType != null;
-        out.writeBoolean(hasValueType);
-        if (hasValueType) {
-            valueType.writeTo(out);
-        }
-        out.writeOptionalString(format);
-        out.writeGenericValue(missing);
-        boolean hasTimeZone = timeZone != null;
-        out.writeBoolean(hasTimeZone);
-        if (hasTimeZone) {
-            out.writeString(timeZone.getID());
-        }
-    }
-
-    // NORELEASE make this abstract when agg refactor complete
-    protected void innerWriteTo(StreamOutput out) throws IOException {
-    }
-
-    @Override
-    protected final ValuesSourceAggregatorFactory<VS, AF> doReadFrom(String name, StreamInput in) throws IOException {
-        ValuesSourceType valuesSourceType = ValuesSourceType.ANY.readFrom(in);
-        ValueType targetValueType = null;
-        if (in.readBoolean()) {
-            targetValueType = ValueType.STRING.readFrom(in);
-        }
-        ValuesSourceAggregatorFactory<VS, AF> factory = innerReadFrom(name, valuesSourceType, targetValueType, in);
-        factory.field = in.readOptionalString();
-        if (in.readBoolean()) {
-            factory.script = Script.readScript(in);
-        }
-        if (in.readBoolean()) {
-            factory.valueType = ValueType.STRING.readFrom(in);
-        }
-        factory.format = in.readOptionalString();
-        factory.missing = in.readGenericValue();
-        if (in.readBoolean()) {
-            factory.timeZone = DateTimeZone.forID(in.readString());
-        }
-        return factory;
-    }
-
-    // NORELEASE make this abstract when agg refactor complete
-    protected ValuesSourceAggregatorFactory<VS, AF> innerReadFrom(String name, ValuesSourceType valuesSourceType, ValueType targetValueType,
-            StreamInput in) throws IOException {
-        return null;
-    }
-
-    @Override
-    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        if (field != null) {
-            builder.field("field", field);
-        }
-        if (script != null) {
-            builder.field("script", script);
-        }
-        if (missing != null) {
-            builder.field("missing", missing);
-        }
-        if (format != null) {
-            builder.field("format", format);
-        }
-        if (timeZone != null) {
-            builder.field("time_zone", timeZone);
-        }
-        doXContentBody(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    // NORELEASE make this abstract when agg refactor complete
-    protected XContentBuilder doXContentBody(XContentBuilder builder, Params params) throws IOException {
-        return builder;
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(field, format, missing, script, targetValueType, timeZone, valueType, valuesSourceType,
-                innerHashCode());
-    }
-
-    // NORELEASE make this method abstract here when agg refactor complete (so
-    // that subclasses are forced to implement it)
-    protected int innerHashCode() {
-        throw new UnsupportedOperationException(
-                "This method should be implemented by a sub-class and should not rely on this method. When agg re-factoring is complete this method will be made abstract.");
-    }
-
-    @Override
-    protected final boolean doEquals(Object obj) {
-        ValuesSourceAggregatorFactory<?, ?> other = (ValuesSourceAggregatorFactory<?, ?>) obj;
-        if (!Objects.equals(field, other.field))
-            return false;
-        if (!Objects.equals(format, other.format))
-            return false;
-        if (!Objects.equals(missing, other.missing))
-            return false;
-        if (!Objects.equals(script, other.script))
-            return false;
-        if (!Objects.equals(targetValueType, other.targetValueType))
-            return false;
-        if (!Objects.equals(timeZone, other.timeZone))
-            return false;
-        if (!Objects.equals(valueType, other.valueType))
-            return false;
-        if (!Objects.equals(valuesSourceType, other.valuesSourceType))
-            return false;
-        return innerEquals(obj);
-    }
-
-    // NORELEASE make this method abstract here when agg refactor complete (so
-    // that subclasses are forced to implement it)
-    protected boolean innerEquals(Object obj) {
-        throw new UnsupportedOperationException(
-                "This method should be implemented by a sub-class and should not rely on this method. When agg re-factoring is complete this method will be made abstract.");
-    }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
index 35e7293..a831f2f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceConfig.java
@@ -28,7 +28,7 @@ import org.elasticsearch.search.aggregations.support.format.ValueParser;
  */
 public class ValuesSourceConfig<VS extends ValuesSource> {
 
-    final ValuesSourceType valueSourceType;
+    final Class<VS> valueSourceType;
     FieldContext fieldContext;
     SearchScript script;
     ValueType scriptValueType;
@@ -37,11 +37,11 @@ public class ValuesSourceConfig<VS extends ValuesSource> {
     ValueFormat format = ValueFormat.RAW;
     Object missing;
 
-    public ValuesSourceConfig(ValuesSourceType valueSourceType) {
+    public ValuesSourceConfig(Class<VS> valueSourceType) {
         this.valueSourceType = valueSourceType;
     }
 
-    public ValuesSourceType valueSourceType() {
+    public Class<VS> valueSourceType() {
         return valueSourceType;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
index e25bf39..fced5fd 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
@@ -19,14 +19,26 @@
 
 package org.elasticsearch.search.aggregations.support;
 
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.fielddata.IndexNumericFieldData;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.elasticsearch.index.mapper.core.NumberFieldMapper;
+import org.elasticsearch.index.mapper.ip.IpFieldMapper;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
+import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.script.SearchScript;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.support.format.ValueFormat;
 import org.elasticsearch.search.internal.SearchContext;
 import org.joda.time.DateTimeZone;
 
@@ -35,55 +47,38 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
-// NORELEASE remove this class when aggs refactoring complete
 /**
- * @deprecated use {@link AbstractValuesSourceParser} instead. This class will
- *             be removed when aggs refactoring is complete.
+ *
  */
-@Deprecated
 public class ValuesSourceParser<VS extends ValuesSource> {
 
     static final ParseField TIME_ZONE = new ParseField("time_zone");
 
     public static Builder any(String aggName, InternalAggregation.Type aggType, SearchContext context) {
-        return new Builder<>(aggName, aggType, context, ValuesSource.class, ValuesSourceType.ANY);
+        return new Builder<>(aggName, aggType, context, ValuesSource.class);
     }
 
     public static Builder<ValuesSource.Numeric> numeric(String aggName, InternalAggregation.Type aggType, SearchContext context) {
-        return new Builder<>(aggName, aggType, context, ValuesSource.Numeric.class, ValuesSourceType.NUMERIC)
-                .targetValueType(ValueType.NUMERIC);
+        return new Builder<>(aggName, aggType, context, ValuesSource.Numeric.class).targetValueType(ValueType.NUMERIC);
     }
 
     public static Builder<ValuesSource.Bytes> bytes(String aggName, InternalAggregation.Type aggType, SearchContext context) {
-        return new Builder<>(aggName, aggType, context, ValuesSource.Bytes.class, ValuesSourceType.BYTES).targetValueType(ValueType.STRING);
+        return new Builder<>(aggName, aggType, context, ValuesSource.Bytes.class).targetValueType(ValueType.STRING);
     }
 
     public static Builder<ValuesSource.GeoPoint> geoPoint(String aggName, InternalAggregation.Type aggType, SearchContext context) {
-        return new Builder<>(aggName, aggType, context, ValuesSource.GeoPoint.class, ValuesSourceType.GEOPOINT).targetValueType(
-                ValueType.GEOPOINT).scriptable(false);
+        return new Builder<>(aggName, aggType, context, ValuesSource.GeoPoint.class).targetValueType(ValueType.GEOPOINT).scriptable(false);
     }
 
-    // NORELEASE remove this class when aggs refactoring complete
-    /**
-     * @deprecated use {@link AbstractValuesSourceParser} instead. This class
-     *             will be removed when aggs refactoring is complete.
-     */
-    @Deprecated
-    public static class Input<VS> {
-        String field = null;
-        Script script = null;
+    public static class Input {
+        private String field = null;
+        private Script script = null;
         @Deprecated
-        Map<String, Object> params = null; // TODO Remove in 3.0
-        ValueType valueType = null;
-        String format = null;
-        Object missing = null;
-        ValuesSourceType valuesSourceType = null;
-        ValueType targetValueType = null;
-        DateTimeZone timezone = DateTimeZone.UTC;
-
-        public boolean valid() {
-            return field != null || script != null;
-        }
+        private Map<String, Object> params = null; // TODO Remove in 3.0
+        private ValueType valueType = null;
+        private String format = null;
+        private Object missing = null;
+        private DateTimeZone timezone = DateTimeZone.UTC;
 
         public DateTimeZone timezone() {
             return this.timezone;
@@ -93,19 +88,21 @@ public class ValuesSourceParser<VS extends ValuesSource> {
     private final String aggName;
     private final InternalAggregation.Type aggType;
     private final SearchContext context;
+    private final Class<VS> valuesSourceType;
 
     private boolean scriptable = true;
     private boolean formattable = false;
     private boolean timezoneAware = false;
+    private ValueType targetValueType = null;
     private ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
 
-    private Input<VS> input = new Input<VS>();
+    private Input input = new Input();
 
-    private ValuesSourceParser(String aggName, InternalAggregation.Type aggType, SearchContext context, ValuesSourceType valuesSourceType) {
+    private ValuesSourceParser(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourceType) {
         this.aggName = aggName;
         this.aggType = aggType;
         this.context = context;
-        input.valuesSourceType = valuesSourceType;
+        this.valuesSourceType = valuesSourceType;
     }
 
     public boolean token(String currentFieldName, XContentParser.Token token, XContentParser parser) throws IOException {
@@ -123,10 +120,11 @@ public class ValuesSourceParser<VS extends ValuesSource> {
             } else if (scriptable) {
                 if ("value_type".equals(currentFieldName) || "valueType".equals(currentFieldName)) {
                     input.valueType = ValueType.resolveForScript(parser.text());
-                    if (input.targetValueType != null && input.valueType.isNotA(input.targetValueType)) {
-                        throw new SearchParseException(context, aggType.name() + " aggregation [" + aggName
-                                + "] was configured with an incompatible value type [" + input.valueType + "]. [" + aggType
-                                + "] aggregation can only work on value of type [" + input.targetValueType + "]", parser.getTokenLocation());
+                    if (targetValueType != null && input.valueType.isNotA(targetValueType)) {
+                        throw new SearchParseException(context, aggType.name() + " aggregation [" + aggName +
+                                "] was configured with an incompatible value type [" + input.valueType + "]. [" + aggType +
+                                "] aggregation can only work on value of type [" + targetValueType + "]",
+                                parser.getTokenLocation());
                     }
                 } else if (!scriptParameterParser.token(currentFieldName, token, parser, context.parseFieldMatcher())) {
                     return false;
@@ -159,9 +157,9 @@ public class ValuesSourceParser<VS extends ValuesSource> {
         return false;
     }
 
-    public Input<VS> input() {
-        if (input.script == null) { // Didn't find anything using the new API so
-                                    // try using the old one instead
+    public ValuesSourceConfig<VS> config() {
+
+        if (input.script == null) { // Didn't find anything using the new API so try using the old one instead
             ScriptParameterValue scriptValue = scriptParameterParser.getDefaultScriptParameterValue();
             if (scriptValue != null) {
                 if (input.params == null) {
@@ -171,21 +169,104 @@ public class ValuesSourceParser<VS extends ValuesSource> {
             }
         }
 
-        return input;
+        ValueType valueType = input.valueType != null ? input.valueType : targetValueType;
+
+        if (input.field == null) {
+            if (input.script == null) {
+                ValuesSourceConfig<VS> config = new ValuesSourceConfig(ValuesSource.class);
+                config.format = resolveFormat(null, valueType);
+                return config;
+            }
+            Class valuesSourceType = valueType != null ? (Class<VS>) valueType.getValuesSourceType() : this.valuesSourceType;
+            if (valuesSourceType == null || valuesSourceType == ValuesSource.class) {
+                // the specific value source type is undefined, but for scripts, we need to have a specific value source
+                // type to know how to handle the script values, so we fallback on Bytes
+                valuesSourceType = ValuesSource.Bytes.class;
+            }
+            ValuesSourceConfig<VS> config = new ValuesSourceConfig<VS>(valuesSourceType);
+            config.missing = input.missing;
+            config.format = resolveFormat(input.format, valueType);
+            config.script = createScript();
+            config.scriptValueType = valueType;
+            return config;
+        }
+
+        MappedFieldType fieldType = context.smartNameFieldType(input.field);
+        if (fieldType == null) {
+            Class<VS> valuesSourceType = valueType != null ? (Class<VS>) valueType.getValuesSourceType() : this.valuesSourceType;
+            ValuesSourceConfig<VS> config = new ValuesSourceConfig<>(valuesSourceType);
+            config.missing = input.missing;
+            config.format = resolveFormat(input.format, valueType);
+            config.unmapped = true;
+            if (valueType != null) {
+                // todo do we really need this for unmapped?
+                config.scriptValueType = valueType;
+            }
+            return config;
+        }
+
+        IndexFieldData<?> indexFieldData = context.fieldData().getForField(fieldType);
+
+        ValuesSourceConfig config;
+        if (valuesSourceType == ValuesSource.class) {
+            if (indexFieldData instanceof IndexNumericFieldData) {
+                config = new ValuesSourceConfig<>(ValuesSource.Numeric.class);
+            } else if (indexFieldData instanceof IndexGeoPointFieldData) {
+                config = new ValuesSourceConfig<>(ValuesSource.GeoPoint.class);
+            } else {
+                config = new ValuesSourceConfig<>(ValuesSource.Bytes.class);
             }
+        } else {
+            config = new ValuesSourceConfig(valuesSourceType);
+        }
+
+        config.fieldContext = new FieldContext(input.field, indexFieldData, fieldType);
+        config.missing = input.missing;
+        config.script = createScript();
+        config.format = resolveFormat(input.format, input.timezone, fieldType);
+        return config;
+    }
+
+    private SearchScript createScript() {
+        return input.script == null ? null : context.scriptService().search(context.lookup(), input.script, ScriptContext.Standard.AGGS, Collections.emptyMap());
+    }
+
+    private static ValueFormat resolveFormat(@Nullable String format, @Nullable ValueType valueType) {
+        if (valueType == null) {
+            return ValueFormat.RAW; // we can't figure it out
+        }
+        ValueFormat valueFormat = valueType.defaultFormat;
+        if (valueFormat != null && valueFormat instanceof ValueFormat.Patternable && format != null) {
+            return ((ValueFormat.Patternable) valueFormat).create(format);
+        }
+        return valueFormat;
+    }
+
+    private static ValueFormat resolveFormat(@Nullable String format, @Nullable DateTimeZone timezone,  MappedFieldType fieldType) {
+        if (fieldType instanceof  DateFieldMapper.DateFieldType) {
+            return format != null ? ValueFormat.DateTime.format(format, timezone) : ValueFormat.DateTime.mapper((DateFieldMapper.DateFieldType) fieldType, timezone);
+        }
+        if (fieldType instanceof IpFieldMapper.IpFieldType) {
+            return ValueFormat.IPv4;
+        }
+        if (fieldType instanceof BooleanFieldMapper.BooleanFieldType) {
+            return ValueFormat.BOOLEAN;
+        }
+        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
+            return format != null ? ValueFormat.Number.format(format) : ValueFormat.RAW;
+        }
+        return ValueFormat.RAW;
+    }
+
+    public Input input() {
+        return this.input;
+    }
 
-    // NORELEASE remove this class when aggs refactoring complete
-    /**
-     * @deprecated use {@link AbstractValuesSourceParser} instead. This class
-     *             will be removed when aggs refactoring is complete.
-     */
-    @Deprecated
     public static class Builder<VS extends ValuesSource> {
 
         private final ValuesSourceParser<VS> parser;
 
-        private Builder(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourcecClass,
-                ValuesSourceType valuesSourceType) {
+        private Builder(String aggName, InternalAggregation.Type aggType, SearchContext context, Class<VS> valuesSourceType) {
             parser = new ValuesSourceParser<>(aggName, aggType, context, valuesSourceType);
         }
 
@@ -205,7 +286,7 @@ public class ValuesSourceParser<VS extends ValuesSource> {
         }
 
         public Builder<VS> targetValueType(ValueType valueType) {
-            parser.input.targetValueType = valueType;
+            parser.targetValueType = valueType;
             return this;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java
deleted file mode 100644
index 46b5698..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceType.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
-/*
- * The ordinal values for this class are tested in ValuesSourceTypeTests to
- * ensure that the ordinal for each value does not change and break bwc
- */
-public enum ValuesSourceType implements Writeable<ValuesSourceType> {
-
-    ANY,
-    NUMERIC,
-    BYTES,
-    GEOPOINT;
-
-    @Override
-    public ValuesSourceType readFrom(StreamInput in) throws IOException {
-        int ordinal = in.readVInt();
-        if (ordinal < 0 || ordinal >= values().length) {
-            throw new IOException("Unknown ValuesSourceType ordinal [" + ordinal + "]");
-        }
-        return values()[ordinal];
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(ordinal());
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index 801ba22..fe7e606 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -21,7 +21,6 @@ package org.elasticsearch.search.builder;
 
 import com.carrotsearch.hppc.ObjectFloatHashMap;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.action.support.ToXContentToBytes;
 import org.elasticsearch.common.Nullable;
@@ -42,7 +41,6 @@ import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
-import org.elasticsearch.search.aggregations.AggregatorFactory;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.highlight.HighlightBuilder;
@@ -383,8 +381,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     /**
      * Add an aggregation to perform as part of the search.
-     *
-     * NORELEASE REMOVE WHEN AGG REFACTORING IS COMPLETE
      */
     public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
         try {
@@ -403,25 +399,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Add an aggregation to perform as part of the search.
-     */
-    public SearchSourceBuilder aggregation(AggregatorFactory aggregation) {
-        try {
-            if (aggregations == null) {
-                aggregations = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            aggregation.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            aggregations.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /**
      * Gets the bytes representing the aggregation builders for this request.
      */
     public List<BytesReference> aggregations() {
@@ -1103,7 +1080,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
             this(fieldName, script, false);
         }
 
-        public ScriptField(String fieldName, Script script, boolean ignoreFailure) {
+        private ScriptField(String fieldName, Script script, boolean ignoreFailure) {
             this.fieldName = fieldName;
             this.script = script;
             this.ignoreFailure = ignoreFailure;
diff --git a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
index 6159f67..e5113bb 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
@@ -57,7 +57,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -149,16 +148,16 @@ public class DefaultSearchContext extends SearchContext {
 
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
     private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
-    private FetchPhase fetchPhase;
 
-    public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget, Engine.Searcher engineSearcher,
-            IndexService indexService, IndexShard indexShard, ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
-            BigArrays bigArrays, Counter timeEstimateCounter, ParseFieldMatcher parseFieldMatcher, TimeValue timeout,
-            FetchPhase fetchPhase) {
+    public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget,
+                                Engine.Searcher engineSearcher, IndexService indexService, IndexShard indexShard,
+                                ScriptService scriptService, PageCacheRecycler pageCacheRecycler,
+                                BigArrays bigArrays, Counter timeEstimateCounter, ParseFieldMatcher parseFieldMatcher,
+                                TimeValue timeout
+    ) {
         super(parseFieldMatcher, request);
         this.id = id;
         this.request = request;
-        this.fetchPhase = fetchPhase;
         this.searchType = request.searchType();
         this.shardTarget = shardTarget;
         this.engineSearcher = engineSearcher;
@@ -729,11 +728,6 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return fetchPhase;
-    }
-
-    @Override
     public FetchSearchResult fetchResult() {
         return fetchResult;
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
index eb87a37..eaa1493 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
@@ -40,7 +40,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -493,11 +492,6 @@ public abstract class FilteredSearchContext extends SearchContext {
     }
 
     @Override
-    public FetchPhase fetchPhase() {
-        return in.fetchPhase();
-    }
-
-    @Override
     public MappedFieldType smartNameFieldType(String name) {
         return in.smartNameFieldType(name);
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index af9d108..76164b5 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -47,7 +47,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -305,8 +304,6 @@ public abstract class SearchContext extends DelegatingHasContextAndHeaders imple
 
     public abstract QuerySearchResult queryResult();
 
-    public abstract FetchPhase fetchPhase();
-
     public abstract FetchSearchResult fetchResult();
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java b/core/src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java
index 36b651a..e9a9c8d 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java
@@ -22,11 +22,8 @@ package org.elasticsearch.search.sort;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
 
 import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
 
 /**
  * Script sort builder allows to sort based on a custom script expression.
@@ -35,17 +32,8 @@ public class ScriptSortBuilder extends SortBuilder {
 
     private Script script;
 
-    @Deprecated
-    private String scriptString;
-
     private final String type;
 
-    @Deprecated
-    private String lang;
-
-    @Deprecated
-    private Map<String, Object> params;
-
     private SortOrder order;
 
     private String sortMode;
@@ -66,66 +54,6 @@ public class ScriptSortBuilder extends SortBuilder {
     }
 
     /**
-     * Constructs a script sort builder with the script and the type.
-     *
-     * @param script
-     *            The script to use.
-     * @param type
-     *            The type, can either be "string" or "number".
-     *
-     * @deprecated Use {@link #ScriptSortBuilder(Script, String)} instead.
-     */
-    @Deprecated
-    public ScriptSortBuilder(String script, String type) {
-        this.scriptString = script;
-        this.type = type;
-    }
-
-    /**
-     * Adds a parameter to the script.
-     *
-     * @param name
-     *            The name of the parameter.
-     * @param value
-     *            The value of the parameter.
-     *
-     * @deprecated Use {@link #ScriptSortBuilder(Script, String)} instead.
-     */
-    @Deprecated
-    public ScriptSortBuilder param(String name, Object value) {
-        if (params == null) {
-            params = new HashMap<>();
-        }
-        params.put(name, value);
-        return this;
-    }
-
-    /**
-     * Sets parameters for the script.
-     *
-     * @param params
-     *            The script parameters
-     *
-     * @deprecated Use {@link #ScriptSortBuilder(Script, String)} instead.
-     */
-    @Deprecated
-    public ScriptSortBuilder setParams(Map<String, Object> params) {
-        this.params = params;
-        return this;
-    }
-
-    /**
-     * The language of the script.
-     *
-     * @deprecated Use {@link #ScriptSortBuilder(Script, String)} instead.
-     */
-    @Deprecated
-    public ScriptSortBuilder lang(String lang) {
-        this.lang = lang;
-        return this;
-    }
-
-    /**
      * Sets the sort order.
      */
     @Override
@@ -172,12 +100,7 @@ public class ScriptSortBuilder extends SortBuilder {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params builderParams) throws IOException {
         builder.startObject("_script");
-        if (script == null) {
-
-            builder.field("script", new Script(scriptString, ScriptType.INLINE, lang, params));
-        } else {
-            builder.field("script", script);
-        }
+        builder.field("script", script);
         builder.field("type", type);
         if (order == SortOrder.DESC) {
             builder.field("reverse", true);
@@ -189,7 +112,7 @@ public class ScriptSortBuilder extends SortBuilder {
             builder.field("nested_path", nestedPath);
         }
         if (nestedFilter != null) {
-            builder.field("nested_filter", nestedFilter, params);
+            builder.field("nested_filter", nestedFilter, builderParams);
         }
         builder.endObject();
         return builder;
diff --git a/core/src/main/java/org/elasticsearch/search/sort/SortBuilders.java b/core/src/main/java/org/elasticsearch/search/sort/SortBuilders.java
index 01134ca..9a843c4 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/SortBuilders.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/SortBuilders.java
@@ -55,20 +55,6 @@ public class SortBuilders {
     }
 
     /**
-     * Constructs a new script based sort.
-     *
-     * @param script
-     *            The script to use.
-     * @param type
-     *            The type, can either be "string" or "number".
-     * @deprecated Use {@link #scriptSort(Script, String)} instead.
-     */
-    @Deprecated
-    public static ScriptSortBuilder scriptSort(String script, String type) {
-        return new ScriptSortBuilder(script, type);
-    }
-
-    /**
      * A geo distance based sort.
      *
      * @param fieldName The geo point like field name.
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index 8c73e38..d46f7dc 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -43,6 +43,7 @@ OFFICIAL PLUGINS
     - discovery-ec2
     - discovery-gce
     - discovery-multicast
+    - ingest-geoip
     - lang-javascript
     - lang-plan-a
     - lang-python
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java b/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java
new file mode 100644
index 0000000..aa30c89
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/BulkRequestModifierTests.java
@@ -0,0 +1,165 @@
+package org.elasticsearch.action.ingest;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.bulk.BulkItemResponse;
+import org.elasticsearch.action.bulk.BulkRequest;
+import org.elasticsearch.action.bulk.BulkResponse;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.index.IndexResponse;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.Matchers;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.nullValue;
+import static org.mockito.Mockito.mock;
+
+public class BulkRequestModifierTests extends ESTestCase {
+
+    public void testBulkRequestModifier() {
+        int numRequests = scaledRandomIntBetween(8, 64);
+        BulkRequest bulkRequest = new BulkRequest();
+        for (int i = 0; i < numRequests; i++) {
+            bulkRequest.add(new IndexRequest("_index", "_type", String.valueOf(i)).source("{}"));
+        }
+        CaptureActionListener actionListener = new CaptureActionListener();
+        IngestActionFilter.BulkRequestModifier bulkRequestModifier = new IngestActionFilter.BulkRequestModifier(bulkRequest);
+
+        int i = 0;
+        Set<Integer> failedSlots = new HashSet<>();
+        while (bulkRequestModifier.hasNext()) {
+            bulkRequestModifier.next();
+            if (randomBoolean()) {
+                bulkRequestModifier.markCurrentItemAsFailed(new RuntimeException());
+                failedSlots.add(i);
+            }
+            i++;
+        }
+
+        assertThat(bulkRequestModifier.getBulkRequest().requests().size(), equalTo(numRequests - failedSlots.size()));
+        // simulate that we actually executed the modified bulk request:
+        ActionListener<BulkResponse> result = bulkRequestModifier.wrapActionListenerIfNeeded(actionListener);
+        result.onResponse(new BulkResponse(new BulkItemResponse[numRequests - failedSlots.size()], 0));
+
+        BulkResponse bulkResponse = actionListener.getResponse();
+        for (int j = 0; j < bulkResponse.getItems().length; j++) {
+            if (failedSlots.contains(j)) {
+                BulkItemResponse item =  bulkResponse.getItems()[j];
+                assertThat(item.isFailed(), is(true));
+                assertThat(item.getFailure().getIndex(), equalTo("_index"));
+                assertThat(item.getFailure().getType(), equalTo("_type"));
+                assertThat(item.getFailure().getId(), equalTo(String.valueOf(j)));
+                assertThat(item.getFailure().getMessage(), equalTo("java.lang.RuntimeException"));
+            } else {
+                assertThat(bulkResponse.getItems()[j], nullValue());
+            }
+        }
+    }
+
+    public void testPipelineFailures() {
+        BulkRequest originalBulkRequest = new BulkRequest();
+        for (int i = 0; i < 32; i++) {
+            originalBulkRequest.add(new IndexRequest("index", "type", String.valueOf(i)));
+        }
+
+        IngestActionFilter.BulkRequestModifier modifier = new IngestActionFilter.BulkRequestModifier(originalBulkRequest);
+        for (int i = 0; modifier.hasNext(); i++) {
+            modifier.next();
+            if (i % 2 == 0) {
+                modifier.markCurrentItemAsFailed(new RuntimeException());
+            }
+        }
+
+        // So half of the requests have "failed", so only the successful requests are left:
+        BulkRequest bulkRequest = modifier.getBulkRequest();
+        assertThat(bulkRequest.requests().size(), Matchers.equalTo(16));
+
+        List<BulkItemResponse> responses = new ArrayList<>();
+        ActionListener<BulkResponse> bulkResponseListener = modifier.wrapActionListenerIfNeeded(new ActionListener<BulkResponse>() {
+            @Override
+            public void onResponse(BulkResponse bulkItemResponses) {
+                responses.addAll(Arrays.asList(bulkItemResponses.getItems()));
+            }
+
+            @Override
+            public void onFailure(Throwable e) {
+            }
+        });
+
+        List<BulkItemResponse> originalResponses = new ArrayList<>();
+        for (ActionRequest actionRequest : bulkRequest.requests()) {
+            IndexRequest indexRequest = (IndexRequest) actionRequest;
+            IndexResponse indexResponse = new IndexResponse(new ShardId("index", 0), indexRequest.type(), indexRequest.id(), 1, true);
+            originalResponses.add(new BulkItemResponse(Integer.parseInt(indexRequest.id()), indexRequest.opType().lowercase(), indexResponse));
+        }
+        bulkResponseListener.onResponse(new BulkResponse(originalResponses.toArray(new BulkItemResponse[originalResponses.size()]), 0));
+
+        assertThat(responses.size(), Matchers.equalTo(32));
+        for (int i = 0; i < 32; i++) {
+            assertThat(responses.get(i).getId(), Matchers.equalTo(String.valueOf(i)));
+        }
+    }
+
+    public void testNoFailures() {
+        BulkRequest originalBulkRequest = new BulkRequest();
+        for (int i = 0; i < 32; i++) {
+            originalBulkRequest.add(new IndexRequest("index", "type", String.valueOf(i)));
+        }
+
+        IngestActionFilter.BulkRequestModifier modifier = new IngestActionFilter.BulkRequestModifier(originalBulkRequest);
+        while (modifier.hasNext()) {
+            modifier.next();
+        }
+
+        BulkRequest bulkRequest = modifier.getBulkRequest();
+        assertThat(bulkRequest, Matchers.sameInstance(originalBulkRequest));
+        @SuppressWarnings("unchecked")
+        ActionListener<BulkResponse> actionListener = mock(ActionListener.class);
+        assertThat(modifier.wrapActionListenerIfNeeded(actionListener), Matchers.sameInstance(actionListener));
+    }
+
+    private static class CaptureActionListener implements ActionListener<BulkResponse> {
+
+        private BulkResponse response;
+
+        @Override
+        public void onResponse(BulkResponse bulkItemResponses) {
+            this.response = bulkItemResponses ;
+        }
+
+        @Override
+        public void onFailure(Throwable e) {
+        }
+
+        public BulkResponse getResponse() {
+            return response;
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java b/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java
new file mode 100644
index 0000000..e1ffe94
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/IngestActionFilterTests.java
@@ -0,0 +1,249 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.bulk.BulkAction;
+import org.elasticsearch.action.bulk.BulkRequest;
+import org.elasticsearch.action.delete.DeleteRequest;
+import org.elasticsearch.action.index.IndexAction;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.support.ActionFilterChain;
+import org.elasticsearch.action.update.UpdateRequest;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.ingest.IngestService;
+import org.elasticsearch.ingest.PipelineExecutionService;
+import org.elasticsearch.ingest.PipelineStore;
+import org.elasticsearch.ingest.core.CompoundProcessor;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.tasks.Task;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.junit.Before;
+import org.mockito.stubbing.Answer;
+
+import java.util.function.Consumer;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.nullValue;
+import static org.mockito.Matchers.same;
+import static org.mockito.Mockito.any;
+import static org.mockito.Mockito.doAnswer;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyZeroInteractions;
+import static org.mockito.Mockito.when;
+
+public class IngestActionFilterTests extends ESTestCase {
+
+    private IngestActionFilter filter;
+    private PipelineExecutionService executionService;
+
+    @Before
+    public void setup() {
+        executionService = mock(PipelineExecutionService.class);
+        IngestService ingestService = mock(IngestService.class);
+        when(ingestService.getPipelineExecutionService()).thenReturn(executionService);
+        NodeService nodeService = mock(NodeService.class);
+        when(nodeService.getIngestService()).thenReturn(ingestService);
+        filter = new IngestActionFilter(Settings.EMPTY, nodeService);
+    }
+
+    public void testApplyNoPipelineId() throws Exception {
+        IndexRequest indexRequest = new IndexRequest();
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+
+        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
+
+        verify(actionFilterChain).proceed(task, IndexAction.NAME, indexRequest, actionListener);
+        verifyZeroInteractions(executionService, actionFilterChain);
+    }
+
+    public void testApplyBulkNoPipelineId() throws Exception {
+        BulkRequest bulkRequest = new BulkRequest();
+        bulkRequest.add(new IndexRequest());
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+
+        filter.apply(task, BulkAction.NAME, bulkRequest, actionListener, actionFilterChain);
+
+        verify(actionFilterChain).proceed(task, BulkAction.NAME, bulkRequest, actionListener);
+        verifyZeroInteractions(executionService, actionFilterChain);
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testApplyIngestIdViaRequestParam() throws Exception {
+        Task task = mock(Task.class);
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id");
+        indexRequest.source("field", "value");
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+
+        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
+
+        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
+        verifyZeroInteractions(actionFilterChain);
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testApplyExecuted() throws Exception {
+        Task task = mock(Task.class);
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id");
+        indexRequest.source("field", "value");
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+
+        Answer answer = invocationOnMock -> {
+            @SuppressWarnings("unchecked")
+            Consumer<Boolean> listener = (Consumer) invocationOnMock.getArguments()[2];
+            listener.accept(true);
+            return null;
+        };
+        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), any(Consumer.class), any(Consumer.class));
+        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
+
+        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
+        verify(actionFilterChain).proceed(task, IndexAction.NAME, indexRequest, actionListener);
+        verifyZeroInteractions(actionListener);
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testApplyFailed() throws Exception {
+        Task task = mock(Task.class);
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id");
+        indexRequest.source("field", "value");
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+
+        RuntimeException exception = new RuntimeException();
+        Answer answer = invocationOnMock -> {
+            Consumer<Throwable> handler = (Consumer) invocationOnMock.getArguments()[1];
+            handler.accept(exception);
+            return null;
+        };
+        doAnswer(answer).when(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
+        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
+
+        verify(executionService).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
+        verify(actionListener).onFailure(exception);
+        verifyZeroInteractions(actionFilterChain);
+    }
+
+    public void testApplyWithBulkRequest() throws Exception {
+        Task task = mock(Task.class);
+        ThreadPool threadPool = mock(ThreadPool.class);
+        when(threadPool.executor(any())).thenReturn(Runnable::run);
+        PipelineStore store = mock(PipelineStore.class);
+
+        Processor processor = new Processor() {
+            @Override
+            public void execute(IngestDocument ingestDocument) {
+                ingestDocument.setFieldValue("field2", "value2");
+            }
+
+            @Override
+            public String getType() {
+                return null;
+            }
+
+            @Override
+            public String getTag() {
+                return null;
+            }
+        };
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
+        executionService = new PipelineExecutionService(store, threadPool);
+        IngestService ingestService = mock(IngestService.class);
+        when(ingestService.getPipelineExecutionService()).thenReturn(executionService);
+        NodeService nodeService = mock(NodeService.class);
+        when(nodeService.getIngestService()).thenReturn(ingestService);
+        filter = new IngestActionFilter(Settings.EMPTY, nodeService);
+
+        BulkRequest bulkRequest = new BulkRequest();
+        int numRequest = scaledRandomIntBetween(8, 64);
+        for (int i = 0; i < numRequest; i++) {
+            if (rarely()) {
+                ActionRequest request;
+                if (randomBoolean()) {
+                    request = new DeleteRequest("_index", "_type", "_id");
+                } else {
+                    request = new UpdateRequest("_index", "_type", "_id");
+                }
+                bulkRequest.add(request);
+            } else {
+                IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id");
+                indexRequest.source("field1", "value1");
+                bulkRequest.add(indexRequest);
+            }
+        }
+
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+
+        filter.apply(task, BulkAction.NAME, bulkRequest, actionListener, actionFilterChain);
+
+        assertBusy(() -> {
+            verify(actionFilterChain).proceed(task, BulkAction.NAME, bulkRequest, actionListener);
+            verifyZeroInteractions(actionListener);
+
+            int assertedRequests = 0;
+            for (ActionRequest actionRequest : bulkRequest.requests()) {
+                if (actionRequest instanceof IndexRequest) {
+                    IndexRequest indexRequest = (IndexRequest) actionRequest;
+                    assertThat(indexRequest.sourceAsMap().size(), equalTo(2));
+                    assertThat(indexRequest.sourceAsMap().get("field1"), equalTo("value1"));
+                    assertThat(indexRequest.sourceAsMap().get("field2"), equalTo("value2"));
+                }
+                assertedRequests++;
+            }
+            assertThat(assertedRequests, equalTo(numRequest));
+        });
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testIndexApiSinglePipelineExecution() {
+        Answer answer = invocationOnMock -> {
+            @SuppressWarnings("unchecked")
+            Consumer<Boolean> listener = (Consumer) invocationOnMock.getArguments()[2];
+            listener.accept(true);
+            return null;
+        };
+        doAnswer(answer).when(executionService).execute(any(IndexRequest.class), any(Consumer.class), any(Consumer.class));
+
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id").source("field", "value");
+        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
+        assertThat(indexRequest.getPipeline(), nullValue());
+        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
+        verify(executionService, times(1)).execute(same(indexRequest), any(Consumer.class), any(Consumer.class));
+        verify(actionFilterChain, times(2)).proceed(task, IndexAction.NAME, indexRequest, actionListener);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/IngestProxyActionFilterTests.java b/core/src/test/java/org/elasticsearch/action/ingest/IngestProxyActionFilterTests.java
new file mode 100644
index 0000000..fa9728c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/IngestProxyActionFilterTests.java
@@ -0,0 +1,251 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.bulk.BulkAction;
+import org.elasticsearch.action.bulk.BulkRequest;
+import org.elasticsearch.action.bulk.BulkResponse;
+import org.elasticsearch.action.index.IndexAction;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.index.IndexResponse;
+import org.elasticsearch.action.support.ActionFilterChain;
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.common.transport.DummyTransportAddress;
+import org.elasticsearch.tasks.Task;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.VersionUtils;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportRequest;
+import org.elasticsearch.transport.TransportResponse;
+import org.elasticsearch.transport.TransportResponseHandler;
+import org.elasticsearch.transport.TransportService;
+import org.hamcrest.CustomTypeSafeMatcher;
+import org.mockito.stubbing.Answer;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.argThat;
+import static org.mockito.Matchers.eq;
+import static org.mockito.Matchers.same;
+import static org.mockito.Mockito.doAnswer;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyZeroInteractions;
+import static org.mockito.Mockito.when;
+
+public class IngestProxyActionFilterTests extends ESTestCase {
+
+    private TransportService transportService;
+
+    @SuppressWarnings("unchecked")
+    private IngestProxyActionFilter buildFilter(int ingestNodes, int totalNodes) {
+        ClusterState.Builder clusterState = new ClusterState.Builder(new ClusterName("_name"));
+        DiscoveryNodes.Builder builder = new DiscoveryNodes.Builder();
+        DiscoveryNode localNode = null;
+        for (int i = 0; i < totalNodes; i++) {
+            String nodeId = "node" + i;
+            Map<String, String> attributes = new HashMap<>();
+            if (i >= ingestNodes) {
+                attributes.put("ingest", "false");
+            } else if (randomBoolean()) {
+                attributes.put("ingest", "true");
+            }
+            DiscoveryNode node = new DiscoveryNode(nodeId, nodeId, DummyTransportAddress.INSTANCE, attributes, VersionUtils.randomVersion(random()));
+            builder.put(node);
+            if (i == totalNodes - 1) {
+                localNode = node;
+            }
+        }
+        clusterState.nodes(builder);
+        ClusterService clusterService = mock(ClusterService.class);
+        when(clusterService.localNode()).thenReturn(localNode);
+        when(clusterService.state()).thenReturn(clusterState.build());
+        transportService = mock(TransportService.class);
+        return new IngestProxyActionFilter(clusterService, transportService);
+    }
+
+    public void testApplyNoIngestNodes() {
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+        int totalNodes = randomIntBetween(1, 5);
+        IngestProxyActionFilter filter = buildFilter(0, totalNodes);
+
+        String action;
+        ActionRequest request;
+        if (randomBoolean()) {
+            action = IndexAction.NAME;
+            request = new IndexRequest().setPipeline("_id");
+        } else {
+            action = BulkAction.NAME;
+            request = new BulkRequest().add(new IndexRequest().setPipeline("_id"));
+        }
+        try {
+            filter.apply(task, action, request, actionListener, actionFilterChain);
+            fail("should have failed because there are no ingest nodes");
+        } catch(IllegalStateException e) {
+            assertThat(e.getMessage(), equalTo("There are no ingest nodes in this cluster, unable to forward request to an ingest node."));
+        }
+        verifyZeroInteractions(transportService);
+        verifyZeroInteractions(actionFilterChain);
+        verifyZeroInteractions(actionListener);
+    }
+
+    public void testApplyNoPipelineId() {
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+        int totalNodes = randomIntBetween(1, 5);
+        IngestProxyActionFilter filter = buildFilter(randomIntBetween(0, totalNodes - 1), totalNodes);
+
+        String action;
+        ActionRequest request;
+        if (randomBoolean()) {
+            action = IndexAction.NAME;
+            request = new IndexRequest();
+        } else {
+            action = BulkAction.NAME;
+            request = new BulkRequest().add(new IndexRequest());
+        }
+        filter.apply(task, action, request, actionListener, actionFilterChain);
+        verifyZeroInteractions(transportService);
+        verify(actionFilterChain).proceed(any(Task.class), eq(action), same(request), same(actionListener));
+        verifyZeroInteractions(actionListener);
+    }
+
+    public void testApplyAnyAction() {
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+        ActionRequest request = mock(ActionRequest.class);
+        int totalNodes = randomIntBetween(1, 5);
+        IngestProxyActionFilter filter = buildFilter(randomIntBetween(0, totalNodes - 1), totalNodes);
+
+        String action = randomAsciiOfLengthBetween(1, 20);
+        filter.apply(task, action, request, actionListener, actionFilterChain);
+        verifyZeroInteractions(transportService);
+        verify(actionFilterChain).proceed(any(Task.class), eq(action), same(request), same(actionListener));
+        verifyZeroInteractions(actionListener);
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testApplyIndexRedirect() {
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+        int totalNodes = randomIntBetween(2, 5);
+        IngestProxyActionFilter filter = buildFilter(randomIntBetween(1, totalNodes - 1), totalNodes);
+        Answer<Void> answer = invocationOnMock -> {
+            TransportResponseHandler transportResponseHandler = (TransportResponseHandler) invocationOnMock.getArguments()[3];
+            transportResponseHandler.handleResponse(new IndexResponse());
+            return null;
+        };
+        doAnswer(answer).when(transportService).sendRequest(any(DiscoveryNode.class), any(String.class), any(TransportRequest.class), any(TransportResponseHandler.class));
+
+        IndexRequest indexRequest = new IndexRequest().setPipeline("_id");
+        filter.apply(task, IndexAction.NAME, indexRequest, actionListener, actionFilterChain);
+
+        verify(transportService).sendRequest(argThat(new IngestNodeMatcher()), eq(IndexAction.NAME), same(indexRequest), any(TransportResponseHandler.class));
+        verifyZeroInteractions(actionFilterChain);
+        verify(actionListener).onResponse(any(IndexResponse.class));
+        verify(actionListener, never()).onFailure(any(TransportException.class));
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testApplyBulkRedirect() {
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+        int totalNodes = randomIntBetween(2, 5);
+        IngestProxyActionFilter filter = buildFilter(randomIntBetween(1, totalNodes - 1), totalNodes);
+        Answer<Void> answer = invocationOnMock -> {
+            TransportResponseHandler transportResponseHandler = (TransportResponseHandler) invocationOnMock.getArguments()[3];
+            transportResponseHandler.handleResponse(new BulkResponse(null, -1));
+            return null;
+        };
+        doAnswer(answer).when(transportService).sendRequest(any(DiscoveryNode.class), any(String.class), any(TransportRequest.class), any(TransportResponseHandler.class));
+
+        BulkRequest bulkRequest = new BulkRequest();
+        bulkRequest.add(new IndexRequest().setPipeline("_id"));
+        int numNoPipelineRequests = randomIntBetween(0, 10);
+        for (int i = 0; i < numNoPipelineRequests; i++) {
+            bulkRequest.add(new IndexRequest());
+        }
+        filter.apply(task, BulkAction.NAME, bulkRequest, actionListener, actionFilterChain);
+
+        verify(transportService).sendRequest(argThat(new IngestNodeMatcher()), eq(BulkAction.NAME), same(bulkRequest), any(TransportResponseHandler.class));
+        verifyZeroInteractions(actionFilterChain);
+        verify(actionListener).onResponse(any(BulkResponse.class));
+        verify(actionListener, never()).onFailure(any(TransportException.class));
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testApplyFailures() {
+        Task task = mock(Task.class);
+        ActionListener actionListener = mock(ActionListener.class);
+        ActionFilterChain actionFilterChain = mock(ActionFilterChain.class);
+        int totalNodes = randomIntBetween(2, 5);
+        IngestProxyActionFilter filter = buildFilter(randomIntBetween(1, totalNodes - 1), totalNodes);
+        Answer<Void> answer = invocationOnMock -> {
+            TransportResponseHandler transportResponseHandler = (TransportResponseHandler) invocationOnMock.getArguments()[3];
+            transportResponseHandler.handleException(new TransportException(new IllegalArgumentException()));
+            return null;
+        };
+        doAnswer(answer).when(transportService).sendRequest(any(DiscoveryNode.class), any(String.class), any(TransportRequest.class), any(TransportResponseHandler.class));
+
+        String action;
+        ActionRequest request;
+        if (randomBoolean()) {
+            action = IndexAction.NAME;
+            request = new IndexRequest().setPipeline("_id");
+        } else {
+            action = BulkAction.NAME;
+            request = new BulkRequest().add(new IndexRequest().setPipeline("_id"));
+        }
+
+        filter.apply(task, action, request, actionListener, actionFilterChain);
+
+        verify(transportService).sendRequest(argThat(new IngestNodeMatcher()), eq(action), same(request), any(TransportResponseHandler.class));
+        verifyZeroInteractions(actionFilterChain);
+        verify(actionListener).onFailure(any(TransportException.class));
+        verify(actionListener, never()).onResponse(any(TransportResponse.class));
+    }
+
+    private static class IngestNodeMatcher extends CustomTypeSafeMatcher<DiscoveryNode> {
+        private IngestNodeMatcher() {
+            super("discovery node should be an ingest node");
+        }
+
+        @Override
+        protected boolean matchesSafely(DiscoveryNode node) {
+            return node.isIngestNode();
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java
new file mode 100644
index 0000000..882fca7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/SimulateDocumentSimpleResultTests.java
@@ -0,0 +1,57 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.instanceOf;
+
+public class SimulateDocumentSimpleResultTests extends ESTestCase {
+
+    public void testSerialization() throws IOException {
+        boolean isFailure = randomBoolean();
+        SimulateDocumentBaseResult simulateDocumentBaseResult;
+        if (isFailure) {
+            simulateDocumentBaseResult = new SimulateDocumentBaseResult(new IllegalArgumentException("test"));
+        } else {
+            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+            simulateDocumentBaseResult = new SimulateDocumentBaseResult(ingestDocument);
+        }
+
+        BytesStreamOutput out = new BytesStreamOutput();
+        simulateDocumentBaseResult.writeTo(out);
+        StreamInput streamInput = StreamInput.wrap(out.bytes());
+        SimulateDocumentBaseResult otherSimulateDocumentBaseResult = SimulateDocumentBaseResult.readSimulateDocumentSimpleResult(streamInput);
+
+        assertThat(otherSimulateDocumentBaseResult.getIngestDocument(), equalTo(simulateDocumentBaseResult.getIngestDocument()));
+        if (isFailure) {
+            assertThat(otherSimulateDocumentBaseResult.getFailure(), instanceOf(IllegalArgumentException.class));
+            IllegalArgumentException e = (IllegalArgumentException) otherSimulateDocumentBaseResult.getFailure();
+            assertThat(e.getMessage(), equalTo("test"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
new file mode 100644
index 0000000..d58b9bf
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/SimulateExecutionServiceTests.java
@@ -0,0 +1,206 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.TestProcessor;
+import org.elasticsearch.ingest.core.CompoundProcessor;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.junit.After;
+import org.junit.Before;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.sameInstance;
+
+public class SimulateExecutionServiceTests extends ESTestCase {
+
+    private ThreadPool threadPool;
+    private SimulateExecutionService executionService;
+    private Pipeline pipeline;
+    private Processor processor;
+    private IngestDocument ingestDocument;
+
+    @Before
+    public void setup() {
+        threadPool = new ThreadPool(
+                Settings.builder()
+                        .put("name", getClass().getName())
+                        .build()
+        );
+        executionService = new SimulateExecutionService(threadPool);
+        processor = new TestProcessor("id", "mock", ingestDocument -> {});
+        pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
+        ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+    }
+
+    @After
+    public void destroy() {
+        threadPool.shutdown();
+    }
+
+    public void testExecuteVerboseDocumentSimple() throws Exception {
+        List<SimulateProcessorResult> processorResultList = new ArrayList<>();
+        executionService.executeVerboseDocument(processor, ingestDocument, processorResultList);
+        SimulateProcessorResult result = new SimulateProcessorResult("id", ingestDocument);
+        assertThat(processorResultList.size(), equalTo(1));
+        assertThat(processorResultList.get(0).getProcessorTag(), equalTo(result.getProcessorTag()));
+        assertThat(processorResultList.get(0).getIngestDocument(), equalTo(result.getIngestDocument()));
+        assertThat(processorResultList.get(0).getFailure(), nullValue());
+    }
+
+    public void testExecuteVerboseDocumentSimpleException() throws Exception {
+        RuntimeException exception = new RuntimeException("mock_exception");
+        TestProcessor processor = new TestProcessor("id", "mock", ingestDocument -> { throw exception; });
+        List<SimulateProcessorResult> processorResultList = new ArrayList<>();
+        try {
+            executionService.executeVerboseDocument(processor, ingestDocument, processorResultList);
+            fail("should throw exception");
+        } catch (RuntimeException e) {
+            assertThat(e.getMessage(), equalTo("mock_exception"));
+        }
+        SimulateProcessorResult result = new SimulateProcessorResult("id", exception);
+        assertThat(processorResultList.size(), equalTo(1));
+        assertThat(processorResultList.get(0).getProcessorTag(), equalTo(result.getProcessorTag()));
+        assertThat(processorResultList.get(0).getFailure(), equalTo(result.getFailure()));
+    }
+
+    public void testExecuteVerboseDocumentCompoundSuccess() throws Exception {
+        TestProcessor processor1 = new TestProcessor("p1", "mock", ingestDocument -> { });
+        TestProcessor processor2 = new TestProcessor("p2", "mock", ingestDocument -> { });
+
+        Processor compoundProcessor = new CompoundProcessor(processor1, processor2);
+        List<SimulateProcessorResult> processorResultList = new ArrayList<>();
+        executionService.executeVerboseDocument(compoundProcessor, ingestDocument, processorResultList);
+        assertThat(processor1.getInvokedCounter(), equalTo(1));
+        assertThat(processor2.getInvokedCounter(), equalTo(1));
+        assertThat(processorResultList.size(), equalTo(2));
+        assertThat(processorResultList.get(0).getProcessorTag(), equalTo("p1"));
+        assertThat(processorResultList.get(0).getIngestDocument(), equalTo(ingestDocument));
+        assertThat(processorResultList.get(0).getFailure(), nullValue());
+        assertThat(processorResultList.get(1).getProcessorTag(), equalTo("p2"));
+        assertThat(processorResultList.get(1).getIngestDocument(), equalTo(ingestDocument));
+        assertThat(processorResultList.get(1).getFailure(), nullValue());
+    }
+
+    public void testExecuteVerboseDocumentCompoundOnFailure() throws Exception {
+        TestProcessor processor1 = new TestProcessor("p1", "mock", ingestDocument -> { });
+        TestProcessor processor2 = new TestProcessor("p2", "mock", ingestDocument -> { throw new RuntimeException("p2_exception"); });
+        TestProcessor onFailureProcessor1 = new TestProcessor("fail_p1", "mock", ingestDocument -> { });
+        TestProcessor onFailureProcessor2 = new TestProcessor("fail_p2", "mock", ingestDocument -> { throw new RuntimeException("fail_p2_exception"); });
+        TestProcessor onFailureProcessor3 = new TestProcessor("fail_p3", "mock", ingestDocument -> { });
+        CompoundProcessor onFailureCompoundProcessor = new CompoundProcessor(Collections.singletonList(onFailureProcessor2), Collections.singletonList(onFailureProcessor3));
+
+        Processor compoundProcessor = new CompoundProcessor(Arrays.asList(processor1, processor2), Arrays.asList(onFailureProcessor1, onFailureCompoundProcessor));
+        List<SimulateProcessorResult> processorResultList = new ArrayList<>();
+        executionService.executeVerboseDocument(compoundProcessor, ingestDocument, processorResultList);
+        assertThat(processor1.getInvokedCounter(), equalTo(1));
+        assertThat(processor2.getInvokedCounter(), equalTo(1));
+        assertThat(onFailureProcessor1.getInvokedCounter(), equalTo(1));
+        assertThat(onFailureProcessor2.getInvokedCounter(), equalTo(1));
+        assertThat(onFailureProcessor3.getInvokedCounter(), equalTo(1));
+        assertThat(processorResultList.size(), equalTo(5));
+        assertThat(processorResultList.get(0).getProcessorTag(), equalTo("p1"));
+        assertThat(processorResultList.get(1).getProcessorTag(), equalTo("p2"));
+        assertThat(processorResultList.get(2).getProcessorTag(), equalTo("fail_p1"));
+        assertThat(processorResultList.get(3).getProcessorTag(), equalTo("fail_p2"));
+        assertThat(processorResultList.get(4).getProcessorTag(), equalTo("fail_p3"));
+    }
+
+    public void testExecuteVerboseItem() throws Exception {
+        TestProcessor processor = new TestProcessor("test-id", "mock", ingestDocument -> {});
+        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
+        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, true);
+        assertThat(processor.getInvokedCounter(), equalTo(2));
+        assertThat(actualItemResponse, instanceOf(SimulateDocumentVerboseResult.class));
+        SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) actualItemResponse;
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(2));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getProcessorTag(), equalTo("test-id"));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), not(sameInstance(ingestDocument)));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), equalTo(ingestDocument));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument().getSourceAndMetadata(), not(sameInstance(ingestDocument.getSourceAndMetadata())));
+
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure(), nullValue());
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getProcessorTag(), equalTo("test-id"));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), not(sameInstance(ingestDocument)));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), equalTo(ingestDocument));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument().getSourceAndMetadata(), not(sameInstance(ingestDocument.getSourceAndMetadata())));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument().getSourceAndMetadata(),
+            not(sameInstance(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument().getSourceAndMetadata())));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getFailure(), nullValue());
+    }
+
+    public void testExecuteItem() throws Exception {
+        TestProcessor processor = new TestProcessor("processor_0", "mock", ingestDocument -> {});
+        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
+        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, false);
+        assertThat(processor.getInvokedCounter(), equalTo(2));
+        assertThat(actualItemResponse, instanceOf(SimulateDocumentBaseResult.class));
+        SimulateDocumentBaseResult simulateDocumentBaseResult = (SimulateDocumentBaseResult) actualItemResponse;
+        assertThat(simulateDocumentBaseResult.getIngestDocument(), equalTo(ingestDocument));
+        assertThat(simulateDocumentBaseResult.getFailure(), nullValue());
+    }
+
+    public void testExecuteVerboseItemWithFailure() throws Exception {
+        TestProcessor processor1 = new TestProcessor("processor_0", "mock", ingestDocument -> { throw new RuntimeException("processor failed"); });
+        TestProcessor processor2 = new TestProcessor("processor_1", "mock", ingestDocument -> {});
+        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(Collections.singletonList(processor1), Collections.singletonList(processor2)));
+        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, true);
+        assertThat(processor1.getInvokedCounter(), equalTo(1));
+        assertThat(processor2.getInvokedCounter(), equalTo(1));
+        assertThat(actualItemResponse, instanceOf(SimulateDocumentVerboseResult.class));
+        SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) actualItemResponse;
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(2));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getProcessorTag(), equalTo("processor_0"));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getIngestDocument(), nullValue());
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure(), instanceOf(RuntimeException.class));
+        RuntimeException runtimeException = (RuntimeException) simulateDocumentVerboseResult.getProcessorResults().get(0).getFailure();
+        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getProcessorTag(), equalTo("processor_1"));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), not(sameInstance(ingestDocument)));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getIngestDocument(), equalTo(ingestDocument));
+        assertThat(simulateDocumentVerboseResult.getProcessorResults().get(1).getFailure(), nullValue());
+    }
+
+    public void testExecuteItemWithFailure() throws Exception {
+        TestProcessor processor = new TestProcessor(ingestDocument -> { throw new RuntimeException("processor failed"); });
+        Pipeline pipeline = new Pipeline("_id", "_description", new CompoundProcessor(processor, processor));
+        SimulateDocumentResult actualItemResponse = executionService.executeDocument(pipeline, ingestDocument, false);
+        assertThat(processor.getInvokedCounter(), equalTo(1));
+        assertThat(actualItemResponse, instanceOf(SimulateDocumentBaseResult.class));
+        SimulateDocumentBaseResult simulateDocumentBaseResult = (SimulateDocumentBaseResult) actualItemResponse;
+        assertThat(simulateDocumentBaseResult.getIngestDocument(), nullValue());
+        assertThat(simulateDocumentBaseResult.getFailure(), instanceOf(RuntimeException.class));
+        RuntimeException runtimeException = (RuntimeException) simulateDocumentBaseResult.getFailure();
+        assertThat(runtimeException.getMessage(), equalTo("processor failed"));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java
new file mode 100644
index 0000000..c0e7d69
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineRequestParsingTests.java
@@ -0,0 +1,181 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.ingest.PipelineStore;
+import org.elasticsearch.ingest.TestProcessor;
+import org.elasticsearch.ingest.core.CompoundProcessor;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import static org.elasticsearch.action.ingest.SimulatePipelineRequest.Fields;
+import static org.elasticsearch.ingest.core.IngestDocument.MetaData.ID;
+import static org.elasticsearch.ingest.core.IngestDocument.MetaData.INDEX;
+import static org.elasticsearch.ingest.core.IngestDocument.MetaData.TYPE;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.nullValue;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+public class SimulatePipelineRequestParsingTests extends ESTestCase {
+
+    private PipelineStore store;
+
+    @Before
+    public void init() throws IOException {
+        TestProcessor processor = new TestProcessor(ingestDocument -> {});
+        CompoundProcessor pipelineCompoundProcessor = new CompoundProcessor(processor);
+        Pipeline pipeline = new Pipeline(SimulatePipelineRequest.SIMULATED_PIPELINE_ID, null, pipelineCompoundProcessor);
+        Map<String, Processor.Factory> processorRegistry = new HashMap<>();
+        processorRegistry.put("mock_processor", mock(Processor.Factory.class));
+        store = mock(PipelineStore.class);
+        when(store.get(SimulatePipelineRequest.SIMULATED_PIPELINE_ID)).thenReturn(pipeline);
+        when(store.getProcessorFactoryRegistry()).thenReturn(processorRegistry);
+    }
+
+    public void testParseUsingPipelineStore() throws Exception {
+        int numDocs = randomIntBetween(1, 10);
+
+        Map<String, Object> requestContent = new HashMap<>();
+        List<Map<String, Object>> docs = new ArrayList<>();
+        List<Map<String, Object>> expectedDocs = new ArrayList<>();
+        requestContent.put(Fields.DOCS, docs);
+        for (int i = 0; i < numDocs; i++) {
+            Map<String, Object> doc = new HashMap<>();
+            String index = randomAsciiOfLengthBetween(1, 10);
+            String type = randomAsciiOfLengthBetween(1, 10);
+            String id = randomAsciiOfLengthBetween(1, 10);
+            doc.put(INDEX.getFieldName(), index);
+            doc.put(TYPE.getFieldName(), type);
+            doc.put(ID.getFieldName(), id);
+            String fieldName = randomAsciiOfLengthBetween(1, 10);
+            String fieldValue = randomAsciiOfLengthBetween(1, 10);
+            doc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
+            docs.add(doc);
+            Map<String, Object> expectedDoc = new HashMap<>();
+            expectedDoc.put(INDEX.getFieldName(), index);
+            expectedDoc.put(TYPE.getFieldName(), type);
+            expectedDoc.put(ID.getFieldName(), id);
+            expectedDoc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
+            expectedDocs.add(expectedDoc);
+        }
+
+        SimulatePipelineRequest.Parsed actualRequest = SimulatePipelineRequest.parseWithPipelineId(SimulatePipelineRequest.SIMULATED_PIPELINE_ID, requestContent, false, store);
+        assertThat(actualRequest.isVerbose(), equalTo(false));
+        assertThat(actualRequest.getDocuments().size(), equalTo(numDocs));
+        Iterator<Map<String, Object>> expectedDocsIterator = expectedDocs.iterator();
+        for (IngestDocument ingestDocument : actualRequest.getDocuments()) {
+            Map<String, Object> expectedDocument = expectedDocsIterator.next();
+            Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
+            assertThat(metadataMap.get(INDEX), equalTo(expectedDocument.get(INDEX.getFieldName())));
+            assertThat(metadataMap.get(TYPE), equalTo(expectedDocument.get(TYPE.getFieldName())));
+            assertThat(metadataMap.get(ID), equalTo(expectedDocument.get(ID.getFieldName())));
+            assertThat(ingestDocument.getSourceAndMetadata(), equalTo(expectedDocument.get(Fields.SOURCE)));
+        }
+
+        assertThat(actualRequest.getPipeline().getId(), equalTo(SimulatePipelineRequest.SIMULATED_PIPELINE_ID));
+        assertThat(actualRequest.getPipeline().getDescription(), nullValue());
+        assertThat(actualRequest.getPipeline().getProcessors().size(), equalTo(1));
+    }
+
+    public void testParseWithProvidedPipeline() throws Exception {
+        int numDocs = randomIntBetween(1, 10);
+
+        Map<String, Object> requestContent = new HashMap<>();
+        List<Map<String, Object>> docs = new ArrayList<>();
+        List<Map<String, Object>> expectedDocs = new ArrayList<>();
+        requestContent.put(Fields.DOCS, docs);
+        for (int i = 0; i < numDocs; i++) {
+            Map<String, Object> doc = new HashMap<>();
+            String index = randomAsciiOfLengthBetween(1, 10);
+            String type = randomAsciiOfLengthBetween(1, 10);
+            String id = randomAsciiOfLengthBetween(1, 10);
+            doc.put(INDEX.getFieldName(), index);
+            doc.put(TYPE.getFieldName(), type);
+            doc.put(ID.getFieldName(), id);
+            String fieldName = randomAsciiOfLengthBetween(1, 10);
+            String fieldValue = randomAsciiOfLengthBetween(1, 10);
+            doc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
+            docs.add(doc);
+            Map<String, Object> expectedDoc = new HashMap<>();
+            expectedDoc.put(INDEX.getFieldName(), index);
+            expectedDoc.put(TYPE.getFieldName(), type);
+            expectedDoc.put(ID.getFieldName(), id);
+            expectedDoc.put(Fields.SOURCE, Collections.singletonMap(fieldName, fieldValue));
+            expectedDocs.add(expectedDoc);
+        }
+
+        Map<String, Object> pipelineConfig = new HashMap<>();
+        List<Map<String, Object>> processors = new ArrayList<>();
+        int numProcessors = randomIntBetween(1, 10);
+        for (int i = 0; i < numProcessors; i++) {
+            Map<String, Object> processorConfig = new HashMap<>();
+            List<Map<String, Object>> onFailureProcessors = new ArrayList<>();
+            int numOnFailureProcessors = randomIntBetween(0, 1);
+            for (int j = 0; j < numOnFailureProcessors; j++) {
+                onFailureProcessors.add(Collections.singletonMap("mock_processor", Collections.emptyMap()));
+            }
+            if (numOnFailureProcessors > 0) {
+                processorConfig.put("on_failure", onFailureProcessors);
+            }
+            processors.add(Collections.singletonMap("mock_processor", processorConfig));
+        }
+        pipelineConfig.put("processors", processors);
+
+        List<Map<String, Object>> onFailureProcessors = new ArrayList<>();
+        int numOnFailureProcessors = randomIntBetween(0, 1);
+        for (int i = 0; i < numOnFailureProcessors; i++) {
+            onFailureProcessors.add(Collections.singletonMap("mock_processor", Collections.emptyMap()));
+        }
+        if (numOnFailureProcessors > 0) {
+            pipelineConfig.put("on_failure", onFailureProcessors);
+        }
+
+        requestContent.put(Fields.PIPELINE, pipelineConfig);
+
+        SimulatePipelineRequest.Parsed actualRequest = SimulatePipelineRequest.parse(requestContent, false, store);
+        assertThat(actualRequest.isVerbose(), equalTo(false));
+        assertThat(actualRequest.getDocuments().size(), equalTo(numDocs));
+        Iterator<Map<String, Object>> expectedDocsIterator = expectedDocs.iterator();
+        for (IngestDocument ingestDocument : actualRequest.getDocuments()) {
+            Map<String, Object> expectedDocument = expectedDocsIterator.next();
+            Map<IngestDocument.MetaData, String> metadataMap = ingestDocument.extractMetadata();
+            assertThat(metadataMap.get(INDEX), equalTo(expectedDocument.get(INDEX.getFieldName())));
+            assertThat(metadataMap.get(TYPE), equalTo(expectedDocument.get(TYPE.getFieldName())));
+            assertThat(metadataMap.get(ID), equalTo(expectedDocument.get(ID.getFieldName())));
+            assertThat(ingestDocument.getSourceAndMetadata(), equalTo(expectedDocument.get(Fields.SOURCE)));
+        }
+
+        assertThat(actualRequest.getPipeline().getId(), equalTo(SimulatePipelineRequest.SIMULATED_PIPELINE_ID));
+        assertThat(actualRequest.getPipeline().getDescription(), nullValue());
+        assertThat(actualRequest.getPipeline().getProcessors().size(), equalTo(numProcessors));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java
new file mode 100644
index 0000000..12a62f0
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/SimulatePipelineResponseTests.java
@@ -0,0 +1,117 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.instanceOf;
+import static org.hamcrest.CoreMatchers.nullValue;
+
+public class SimulatePipelineResponseTests extends ESTestCase {
+
+    public void testSerialization() throws IOException {
+        boolean isVerbose = randomBoolean();
+        int numResults = randomIntBetween(1, 10);
+        List<SimulateDocumentResult> results = new ArrayList<>(numResults);
+        for (int i = 0; i < numResults; i++) {
+            boolean isFailure = randomBoolean();
+            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+            if (isVerbose) {
+                int numProcessors = randomIntBetween(1, 10);
+                List<SimulateProcessorResult> processorResults = new ArrayList<>(numProcessors);
+                for (int j = 0; j < numProcessors; j++) {
+                    String processorTag = randomAsciiOfLengthBetween(1, 10);
+                    SimulateProcessorResult processorResult;
+                    if (isFailure) {
+                        processorResult = new SimulateProcessorResult(processorTag, new IllegalArgumentException("test"));
+                    } else {
+                        processorResult = new SimulateProcessorResult(processorTag, ingestDocument);
+                    }
+                    processorResults.add(processorResult);
+                }
+                results.add(new SimulateDocumentVerboseResult(processorResults));
+            } else {
+                results.add(new SimulateDocumentBaseResult(ingestDocument));
+                SimulateDocumentBaseResult simulateDocumentBaseResult;
+                if (isFailure) {
+                    simulateDocumentBaseResult = new SimulateDocumentBaseResult(new IllegalArgumentException("test"));
+                } else {
+                    simulateDocumentBaseResult = new SimulateDocumentBaseResult(ingestDocument);
+                }
+                results.add(simulateDocumentBaseResult);
+            }
+        }
+
+        SimulatePipelineResponse response = new SimulatePipelineResponse(randomAsciiOfLengthBetween(1, 10), isVerbose, results);
+        BytesStreamOutput out = new BytesStreamOutput();
+        response.writeTo(out);
+        StreamInput streamInput = StreamInput.wrap(out.bytes());
+        SimulatePipelineResponse otherResponse = new SimulatePipelineResponse();
+        otherResponse.readFrom(streamInput);
+
+        assertThat(otherResponse.getPipelineId(), equalTo(response.getPipelineId()));
+        assertThat(otherResponse.getResults().size(), equalTo(response.getResults().size()));
+
+        Iterator<SimulateDocumentResult> expectedResultIterator = response.getResults().iterator();
+        for (SimulateDocumentResult result : otherResponse.getResults()) {
+            if (isVerbose) {
+                SimulateDocumentVerboseResult expectedSimulateDocumentVerboseResult = (SimulateDocumentVerboseResult) expectedResultIterator.next();
+                assertThat(result, instanceOf(SimulateDocumentVerboseResult.class));
+                SimulateDocumentVerboseResult simulateDocumentVerboseResult = (SimulateDocumentVerboseResult) result;
+                assertThat(simulateDocumentVerboseResult.getProcessorResults().size(), equalTo(expectedSimulateDocumentVerboseResult.getProcessorResults().size()));
+                Iterator<SimulateProcessorResult> expectedProcessorResultIterator = expectedSimulateDocumentVerboseResult.getProcessorResults().iterator();
+                for (SimulateProcessorResult simulateProcessorResult : simulateDocumentVerboseResult.getProcessorResults()) {
+                    SimulateProcessorResult expectedProcessorResult = expectedProcessorResultIterator.next();
+                    assertThat(simulateProcessorResult.getProcessorTag(), equalTo(expectedProcessorResult.getProcessorTag()));
+                    assertThat(simulateProcessorResult.getIngestDocument(), equalTo(expectedProcessorResult.getIngestDocument()));
+                    if (expectedProcessorResult.getFailure() == null) {
+                        assertThat(simulateProcessorResult.getFailure(), nullValue());
+                    } else {
+                        assertThat(simulateProcessorResult.getFailure(), instanceOf(IllegalArgumentException.class));
+                        IllegalArgumentException e = (IllegalArgumentException) simulateProcessorResult.getFailure();
+                        assertThat(e.getMessage(), equalTo("test"));
+                    }
+                }
+            } else {
+                SimulateDocumentBaseResult expectedSimulateDocumentBaseResult = (SimulateDocumentBaseResult) expectedResultIterator.next();
+                assertThat(result, instanceOf(SimulateDocumentBaseResult.class));
+                SimulateDocumentBaseResult simulateDocumentBaseResult = (SimulateDocumentBaseResult) result;
+                assertThat(simulateDocumentBaseResult.getIngestDocument(), equalTo(expectedSimulateDocumentBaseResult.getIngestDocument()));
+                if (expectedSimulateDocumentBaseResult.getFailure() == null) {
+                    assertThat(simulateDocumentBaseResult.getFailure(), nullValue());
+                } else {
+                    assertThat(simulateDocumentBaseResult.getFailure(), instanceOf(IllegalArgumentException.class));
+                    IllegalArgumentException e = (IllegalArgumentException) simulateDocumentBaseResult.getFailure();
+                    assertThat(e.getMessage(), equalTo("test"));
+                }
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java b/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java
new file mode 100644
index 0000000..0885475
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/SimulateProcessorResultTests.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+
+public class SimulateProcessorResultTests extends ESTestCase {
+
+    public void testSerialization() throws IOException {
+        String processorTag = randomAsciiOfLengthBetween(1, 10);
+        boolean isFailure = randomBoolean();
+        SimulateProcessorResult simulateProcessorResult;
+        if (isFailure) {
+            simulateProcessorResult = new SimulateProcessorResult(processorTag, new IllegalArgumentException("test"));
+        } else {
+            IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+            simulateProcessorResult = new SimulateProcessorResult(processorTag, ingestDocument);
+        }
+
+        BytesStreamOutput out = new BytesStreamOutput();
+        simulateProcessorResult.writeTo(out);
+        StreamInput streamInput = StreamInput.wrap(out.bytes());
+        SimulateProcessorResult otherSimulateProcessorResult = new SimulateProcessorResult(streamInput);
+        assertThat(otherSimulateProcessorResult.getProcessorTag(), equalTo(simulateProcessorResult.getProcessorTag()));
+        assertThat(otherSimulateProcessorResult.getIngestDocument(), equalTo(simulateProcessorResult.getIngestDocument()));
+        if (isFailure) {
+            assertThat(otherSimulateProcessorResult.getFailure(), instanceOf(IllegalArgumentException.class));
+            IllegalArgumentException e = (IllegalArgumentException) otherSimulateProcessorResult.getFailure();
+            assertThat(e.getMessage(), equalTo("test"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java b/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
new file mode 100644
index 0000000..8d3c812
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/ingest/WriteableIngestDocumentTests.java
@@ -0,0 +1,114 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.ingest;
+
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
+
+public class WriteableIngestDocumentTests extends ESTestCase {
+
+    public void testEqualsAndHashcode() throws Exception {
+        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
+        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
+        for (int i = 0; i < numFields; i++) {
+            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
+        }
+        Map<String, String> ingestMetadata = new HashMap<>();
+        numFields = randomIntBetween(1, 5);
+        for (int i = 0; i < numFields; i++) {
+            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
+        }
+        WriteableIngestDocument ingestDocument = new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
+
+        boolean changed = false;
+        Map<String, Object> otherSourceAndMetadata;
+        if (randomBoolean()) {
+            otherSourceAndMetadata = RandomDocumentPicks.randomSource(random());
+            changed = true;
+        } else {
+            otherSourceAndMetadata = new HashMap<>(sourceAndMetadata);
+        }
+        if (randomBoolean()) {
+            numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
+            for (int i = 0; i < numFields; i++) {
+                otherSourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
+            }
+            changed = true;
+        }
+
+        Map<String, String> otherIngestMetadata;
+        if (randomBoolean()) {
+            otherIngestMetadata = new HashMap<>();
+            numFields = randomIntBetween(1, 5);
+            for (int i = 0; i < numFields; i++) {
+                otherIngestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
+            }
+            changed = true;
+        } else {
+            otherIngestMetadata = Collections.unmodifiableMap(ingestMetadata);
+        }
+
+        WriteableIngestDocument otherIngestDocument = new WriteableIngestDocument(new IngestDocument(otherSourceAndMetadata, otherIngestMetadata));
+        if (changed) {
+            assertThat(ingestDocument, not(equalTo(otherIngestDocument)));
+            assertThat(otherIngestDocument, not(equalTo(ingestDocument)));
+        } else {
+            assertThat(ingestDocument, equalTo(otherIngestDocument));
+            assertThat(otherIngestDocument, equalTo(ingestDocument));
+            assertThat(ingestDocument.hashCode(), equalTo(otherIngestDocument.hashCode()));
+            WriteableIngestDocument thirdIngestDocument = new WriteableIngestDocument(new IngestDocument(Collections.unmodifiableMap(sourceAndMetadata), Collections.unmodifiableMap(ingestMetadata)));
+            assertThat(thirdIngestDocument, equalTo(ingestDocument));
+            assertThat(ingestDocument, equalTo(thirdIngestDocument));
+            assertThat(ingestDocument.hashCode(), equalTo(thirdIngestDocument.hashCode()));
+        }
+    }
+
+    public void testSerialization() throws IOException {
+        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
+        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
+        for (int i = 0; i < numFields; i++) {
+            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
+        }
+        Map<String, String> ingestMetadata = new HashMap<>();
+        numFields = randomIntBetween(1, 5);
+        for (int i = 0; i < numFields; i++) {
+            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
+        }
+        Map<String, Object> document = RandomDocumentPicks.randomSource(random());
+        WriteableIngestDocument writeableIngestDocument = new WriteableIngestDocument(new IngestDocument(sourceAndMetadata, ingestMetadata));
+
+        BytesStreamOutput out = new BytesStreamOutput();
+        writeableIngestDocument.writeTo(out);
+        StreamInput streamInput = StreamInput.wrap(out.bytes());
+        WriteableIngestDocument otherWriteableIngestDocument = new WriteableIngestDocument(streamInput);
+        assertThat(otherWriteableIngestDocument, equalTo(writeableIngestDocument));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java b/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
index 5aaed6b..0a51f5d 100644
--- a/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
@@ -63,6 +63,7 @@ import static org.hamcrest.Matchers.nullValue;
  *
  */
 public class SimpleIndexTemplateIT extends ESIntegTestCase {
+
     public void testSimpleIndexTemplateTests() throws Exception {
         // clean all templates setup by the framework.
         client().admin().indices().prepareDeleteTemplate("*").get();
diff --git a/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java b/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
new file mode 100644
index 0000000..ae724a5
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/IngestClientIT.java
@@ -0,0 +1,231 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.action.bulk.BulkItemResponse;
+import org.elasticsearch.action.bulk.BulkRequest;
+import org.elasticsearch.action.bulk.BulkResponse;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.index.IndexResponse;
+import org.elasticsearch.action.ingest.DeletePipelineRequest;
+import org.elasticsearch.action.ingest.GetPipelineRequest;
+import org.elasticsearch.action.ingest.GetPipelineResponse;
+import org.elasticsearch.action.ingest.PutPipelineRequest;
+import org.elasticsearch.action.ingest.SimulateDocumentBaseResult;
+import org.elasticsearch.action.ingest.SimulatePipelineRequest;
+import org.elasticsearch.action.ingest.SimulatePipelineResponse;
+import org.elasticsearch.action.ingest.WritePipelineResponse;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.node.NodeModule;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.ESIntegTestCase;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.core.Is.is;
+
+@ESIntegTestCase.ClusterScope(minNumDataNodes = 2)
+public class IngestClientIT extends ESIntegTestCase {
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        // TODO: Remove this method once gets in: https://github.com/elastic/elasticsearch/issues/16019
+        if (nodeOrdinal % 2 == 0) {
+            return Settings.builder().put("node.ingest", false).put(super.nodeSettings(nodeOrdinal)).build();
+        }
+        return super.nodeSettings(nodeOrdinal);
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(IngestPlugin.class);
+    }
+
+    public void testSimulate() throws Exception {
+        BytesReference pipelineSource = jsonBuilder().startObject()
+            .field("description", "my_pipeline")
+            .startArray("processors")
+            .startObject()
+            .startObject("test")
+            .endObject()
+            .endObject()
+            .endArray()
+            .endObject().bytes();
+        client().preparePutPipeline("_id", pipelineSource)
+                .get();
+        GetPipelineResponse getResponse = client().prepareGetPipeline("_id")
+                .get();
+        assertThat(getResponse.isFound(), is(true));
+        assertThat(getResponse.pipelines().size(), equalTo(1));
+        assertThat(getResponse.pipelines().get(0).getId(), equalTo("_id"));
+
+        BytesReference bytes = jsonBuilder().startObject()
+            .startArray("docs")
+            .startObject()
+            .field("_index", "index")
+            .field("_type", "type")
+            .field("_id", "id")
+            .startObject("_source")
+            .field("foo", "bar")
+            .field("fail", false)
+            .endObject()
+            .endObject()
+            .endArray()
+            .endObject().bytes();
+        SimulatePipelineResponse response;
+        if (randomBoolean()) {
+            response = client().prepareSimulatePipeline(bytes)
+                .setId("_id").get();
+        } else {
+            SimulatePipelineRequest request = new SimulatePipelineRequest(bytes);
+            request.setId("_id");
+            response = client().simulatePipeline(request).get();
+        }
+        assertThat(response.isVerbose(), equalTo(false));
+        assertThat(response.getPipelineId(), equalTo("_id"));
+        assertThat(response.getResults().size(), equalTo(1));
+        assertThat(response.getResults().get(0), instanceOf(SimulateDocumentBaseResult.class));
+        SimulateDocumentBaseResult simulateDocumentBaseResult = (SimulateDocumentBaseResult) response.getResults().get(0);
+        Map<String, Object> source = new HashMap<>();
+        source.put("foo", "bar");
+        source.put("fail", false);
+        source.put("processed", true);
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, source);
+        assertThat(simulateDocumentBaseResult.getIngestDocument().getSourceAndMetadata(), equalTo(ingestDocument.getSourceAndMetadata()));
+        assertThat(simulateDocumentBaseResult.getFailure(), nullValue());
+    }
+
+    public void testBulkWithIngestFailures() throws Exception {
+        createIndex("index");
+
+        BytesReference source = jsonBuilder().startObject()
+            .field("description", "my_pipeline")
+            .startArray("processors")
+            .startObject()
+            .startObject("test")
+            .endObject()
+            .endObject()
+            .endArray()
+            .endObject().bytes();
+        PutPipelineRequest putPipelineRequest = new PutPipelineRequest("_id", source);
+        client().putPipeline(putPipelineRequest).get();
+
+        int numRequests = scaledRandomIntBetween(32, 128);
+        BulkRequest bulkRequest = new BulkRequest();
+        for (int i = 0; i < numRequests; i++) {
+            IndexRequest indexRequest = new IndexRequest("index", "type", Integer.toString(i)).setPipeline("_id");
+            indexRequest.source("field", "value", "fail", i % 2 == 0);
+            bulkRequest.add(indexRequest);
+        }
+
+        BulkResponse response = client().bulk(bulkRequest).actionGet();
+        assertThat(response.getItems().length, equalTo(bulkRequest.requests().size()));
+        for (int i = 0; i < bulkRequest.requests().size(); i++) {
+            BulkItemResponse itemResponse = response.getItems()[i];
+            if (i % 2 == 0) {
+                BulkItemResponse.Failure failure = itemResponse.getFailure();
+                assertThat(failure.getMessage(), equalTo("java.lang.IllegalArgumentException: test processor failed"));
+            } else {
+                IndexResponse indexResponse = itemResponse.getResponse();
+                assertThat(indexResponse.getId(), equalTo(Integer.toString(i)));
+                assertThat(indexResponse.isCreated(), is(true));
+            }
+        }
+    }
+
+    public void test() throws Exception {
+        BytesReference source = jsonBuilder().startObject()
+            .field("description", "my_pipeline")
+            .startArray("processors")
+            .startObject()
+            .startObject("test")
+            .endObject()
+            .endObject()
+            .endArray()
+            .endObject().bytes();
+        PutPipelineRequest putPipelineRequest = new PutPipelineRequest("_id", source);
+        client().putPipeline(putPipelineRequest).get();
+
+        GetPipelineRequest getPipelineRequest = new GetPipelineRequest("_id");
+        GetPipelineResponse getResponse = client().getPipeline(getPipelineRequest).get();
+        assertThat(getResponse.isFound(), is(true));
+        assertThat(getResponse.pipelines().size(), equalTo(1));
+        assertThat(getResponse.pipelines().get(0).getId(), equalTo("_id"));
+
+        client().prepareIndex("test", "type", "1").setPipeline("_id").setSource("field", "value", "fail", false).get();
+
+        Map<String, Object> doc = client().prepareGet("test", "type", "1")
+                .get().getSourceAsMap();
+        assertThat(doc.get("field"), equalTo("value"));
+        assertThat(doc.get("processed"), equalTo(true));
+
+        client().prepareBulk().add(
+                client().prepareIndex("test", "type", "2").setSource("field", "value2", "fail", false).setPipeline("_id")).get();
+        doc = client().prepareGet("test", "type", "2").get().getSourceAsMap();
+        assertThat(doc.get("field"), equalTo("value2"));
+        assertThat(doc.get("processed"), equalTo(true));
+
+        DeletePipelineRequest deletePipelineRequest = new DeletePipelineRequest("_id");
+        WritePipelineResponse response = client().deletePipeline(deletePipelineRequest).get();
+        assertThat(response.isAcknowledged(), is(true));
+
+        getResponse = client().prepareGetPipeline("_id").get();
+        assertThat(getResponse.isFound(), is(false));
+        assertThat(getResponse.pipelines().size(), equalTo(0));
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> getMockPlugins() {
+        return Collections.singletonList(TestSeedPlugin.class);
+    }
+
+    public static class IngestPlugin extends Plugin {
+
+        @Override
+        public String name() {
+            return "ingest";
+        }
+
+        @Override
+        public String description() {
+            return "ingest mock";
+        }
+
+        public void onModule(NodeModule nodeModule) {
+            nodeModule.registerProcessor("test", templateService -> config ->
+                new TestProcessor("id", "test", ingestDocument -> {
+                    ingestDocument.setFieldValue("processed", true);
+                    if (ingestDocument.getFieldValue("fail", Boolean.class)) {
+                        throw new IllegalArgumentException("test processor failed");
+                    }
+                })
+            );
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/IngestMetadataTests.java b/core/src/test/java/org/elasticsearch/ingest/IngestMetadataTests.java
new file mode 100644
index 0000000..a6cf123
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/IngestMetadataTests.java
@@ -0,0 +1,64 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+public class IngestMetadataTests extends ESTestCase {
+
+    public void testFromXContent() throws IOException {
+        PipelineConfiguration pipeline = new PipelineConfiguration(
+            "1",new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field\", \"value\": \"_value\"}}]}")
+        );
+        PipelineConfiguration pipeline2 = new PipelineConfiguration(
+            "2",new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field1\", \"value\": \"_value1\"}}]}")
+        );
+        Map<String, PipelineConfiguration> map = new HashMap<>();
+        map.put(pipeline.getId(), pipeline);
+        map.put(pipeline2.getId(), pipeline2);
+        IngestMetadata ingestMetadata = new IngestMetadata(map);
+        XContentBuilder builder = XContentFactory.jsonBuilder();
+        builder.prettyPrint();
+        builder.startObject();
+        ingestMetadata.toXContent(builder, ToXContent.EMPTY_PARAMS);
+        builder.endObject();
+        String string = builder.string();
+        final XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(string);
+        MetaData.Custom custom = ingestMetadata.fromXContent(parser);
+        assertTrue(custom instanceof IngestMetadata);
+        IngestMetadata m = (IngestMetadata) custom;
+        assertEquals(2, m.getPipelines().size());
+        assertEquals("1", m.getPipelines().get("1").getId());
+        assertEquals("2", m.getPipelines().get("2").getId());
+        assertEquals(pipeline.getConfigAsMap(), m.getPipelines().get("1").getConfigAsMap());
+        assertEquals(pipeline2.getConfigAsMap(), m.getPipelines().get("2").getConfigAsMap());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java b/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java
new file mode 100644
index 0000000..9126a51
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/PipelineExecutionServiceTests.java
@@ -0,0 +1,366 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ElasticsearchParseException;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.bulk.BulkRequest;
+import org.elasticsearch.action.delete.DeleteRequest;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.update.UpdateRequest;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.ingest.core.CompoundProcessor;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.hamcrest.CustomTypeSafeMatcher;
+import org.junit.Before;
+import org.mockito.ArgumentMatcher;
+import org.mockito.invocation.InvocationOnMock;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.Objects;
+import java.util.function.BiConsumer;
+import java.util.function.Consumer;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.mockito.Matchers.eq;
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.anyBoolean;
+import static org.mockito.Matchers.anyString;
+import static org.mockito.Matchers.argThat;
+import static org.mockito.Mockito.doAnswer;
+import static org.mockito.Mockito.doThrow;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
+public class PipelineExecutionServiceTests extends ESTestCase {
+
+    private PipelineStore store;
+    private PipelineExecutionService executionService;
+
+    @Before
+    public void setup() {
+        store = mock(PipelineStore.class);
+        ThreadPool threadPool = mock(ThreadPool.class);
+        when(threadPool.executor(anyString())).thenReturn(Runnable::run);
+        executionService = new PipelineExecutionService(store, threadPool);
+    }
+
+    public void testExecuteIndexPipelineDoesNotExist() {
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        try {
+            executionService.execute(indexRequest, failureHandler, completionHandler);
+            fail("IllegalArgumentException expected");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("pipeline with id [_id] does not exist"));
+        }
+        verify(failureHandler, never()).accept(any(Throwable.class));
+        verify(completionHandler, never()).accept(anyBoolean());
+    }
+
+    public void testExecuteBulkPipelineDoesNotExist() {
+        CompoundProcessor processor = mock(CompoundProcessor.class);
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
+        BulkRequest bulkRequest = new BulkRequest();
+
+        IndexRequest indexRequest1 = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        bulkRequest.add(indexRequest1);
+        IndexRequest indexRequest2 = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("does_not_exist");
+        bulkRequest.add(indexRequest2);
+        @SuppressWarnings("unchecked")
+        BiConsumer<IndexRequest, Throwable> failureHandler = mock(BiConsumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> completionHandler = mock(Consumer.class);
+        executionService.execute(bulkRequest.requests(), failureHandler, completionHandler);
+        verify(failureHandler, times(1)).accept(
+            argThat(new CustomTypeSafeMatcher<IndexRequest>("failure handler was not called with the expected arguments") {
+                @Override
+                protected boolean matchesSafely(IndexRequest item) {
+                    return item == indexRequest2;
+                }
+
+            }),
+            argThat(new CustomTypeSafeMatcher<IllegalArgumentException>("failure handler was not called with the expected arguments") {
+                @Override
+                protected boolean matchesSafely(IllegalArgumentException iae) {
+                    return "pipeline with id [does_not_exist] does not exist".equals(iae.getMessage());
+                }
+            })
+        );
+        verify(completionHandler, times(1)).accept(null);
+    }
+
+    public void testExecuteSuccess() throws Exception {
+        CompoundProcessor processor = mock(CompoundProcessor.class);
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
+
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+        verify(failureHandler, never()).accept(any());
+        verify(completionHandler, times(1)).accept(true);
+    }
+
+    public void testExecutePropagateAllMetaDataUpdates() throws Exception {
+        CompoundProcessor processor = mock(CompoundProcessor.class);
+        doAnswer((InvocationOnMock invocationOnMock) -> {
+            IngestDocument ingestDocument = (IngestDocument) invocationOnMock.getArguments()[0];
+            for (IngestDocument.MetaData metaData : IngestDocument.MetaData.values()) {
+                if (metaData == IngestDocument.MetaData.TTL) {
+                    ingestDocument.setFieldValue(IngestDocument.MetaData.TTL.getFieldName(), "5w");
+                } else {
+                    ingestDocument.setFieldValue(metaData.getFieldName(), "update" + metaData.getFieldName());
+                }
+
+            }
+            return null;
+        }).when(processor).execute(any());
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
+
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+        verify(processor).execute(any());
+        verify(failureHandler, never()).accept(any());
+        verify(completionHandler, times(1)).accept(true);
+
+        assertThat(indexRequest.index(), equalTo("update_index"));
+        assertThat(indexRequest.type(), equalTo("update_type"));
+        assertThat(indexRequest.id(), equalTo("update_id"));
+        assertThat(indexRequest.routing(), equalTo("update_routing"));
+        assertThat(indexRequest.parent(), equalTo("update_parent"));
+        assertThat(indexRequest.timestamp(), equalTo("update_timestamp"));
+        assertThat(indexRequest.ttl(), equalTo(new TimeValue(3024000000L)));
+    }
+
+    public void testExecuteFailure() throws Exception {
+        CompoundProcessor processor = mock(CompoundProcessor.class);
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", processor));
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
+        verify(completionHandler, never()).accept(anyBoolean());
+    }
+
+    public void testExecuteSuccessWithOnFailure() throws Exception {
+        Processor processor = mock(Processor.class);
+        Processor onFailureProcessor = mock(Processor.class);
+        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(new CompoundProcessor(onFailureProcessor)));
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+        verify(failureHandler, never()).accept(any(RuntimeException.class));
+        verify(completionHandler, times(1)).accept(true);
+    }
+
+    public void testExecuteFailureWithOnFailure() throws Exception {
+        Processor processor = mock(Processor.class);
+        Processor onFailureProcessor = mock(Processor.class);
+        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(new CompoundProcessor(onFailureProcessor)));
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        doThrow(new RuntimeException()).when(onFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
+        verify(completionHandler, never()).accept(anyBoolean());
+    }
+
+    public void testExecuteFailureWithNestedOnFailure() throws Exception {
+        Processor processor = mock(Processor.class);
+        Processor onFailureProcessor = mock(Processor.class);
+        Processor onFailureOnFailureProcessor = mock(Processor.class);
+        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor),
+            Collections.singletonList(new CompoundProcessor(Collections.singletonList(onFailureProcessor), Collections.singletonList(onFailureOnFailureProcessor))));
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", compoundProcessor));
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        doThrow(new RuntimeException()).when(onFailureOnFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        doThrow(new RuntimeException()).when(onFailureProcessor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        doThrow(new RuntimeException()).when(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+        verify(processor).execute(eqID("_index", "_type", "_id", Collections.emptyMap()));
+        verify(failureHandler, times(1)).accept(any(RuntimeException.class));
+        verify(completionHandler, never()).accept(anyBoolean());
+    }
+
+    public void testExecuteSetTTL() throws Exception {
+        Processor processor = new TestProcessor(ingestDocument -> ingestDocument.setFieldValue("_ttl", "5d"));
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
+
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+
+        assertThat(indexRequest.ttl(), equalTo(TimeValue.parseTimeValue("5d", null, "ttl")));
+        verify(failureHandler, never()).accept(any());
+        verify(completionHandler, times(1)).accept(true);
+    }
+
+    public void testExecuteSetInvalidTTL() throws Exception {
+        Processor processor = new TestProcessor(ingestDocument -> ingestDocument.setFieldValue("_ttl", "abc"));
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", new CompoundProcessor(processor)));
+
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").source(Collections.emptyMap()).setPipeline("_id");
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+        verify(failureHandler, times(1)).accept(any(ElasticsearchParseException.class));
+        verify(completionHandler, never()).accept(anyBoolean());
+    }
+
+    public void testExecuteProvidedTTL() throws Exception {
+        when(store.get("_id")).thenReturn(new Pipeline("_id", "_description", mock(CompoundProcessor.class)));
+
+        IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline("_id")
+                .source(Collections.emptyMap())
+                .ttl(1000L);
+        Consumer<Throwable> failureHandler = mock(Consumer.class);
+        Consumer<Boolean> completionHandler = mock(Consumer.class);
+        executionService.execute(indexRequest, failureHandler, completionHandler);
+
+        assertThat(indexRequest.ttl(), equalTo(new TimeValue(1000L)));
+        verify(failureHandler, never()).accept(any());
+        verify(completionHandler, times(1)).accept(true);
+    }
+
+    public void testBulkRequestExecutionWithFailures() throws Exception {
+        BulkRequest bulkRequest = new BulkRequest();
+        String pipelineId = "_id";
+
+        int numRequest = scaledRandomIntBetween(8, 64);
+        int numIndexRequests = 0;
+        for (int i = 0; i < numRequest; i++) {
+            ActionRequest request;
+            if (randomBoolean()) {
+                if (randomBoolean()) {
+                    request = new DeleteRequest("_index", "_type", "_id");
+                } else {
+                    request = new UpdateRequest("_index", "_type", "_id");
+                }
+            } else {
+                IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline(pipelineId);
+                indexRequest.source("field1", "value1");
+                request = indexRequest;
+                numIndexRequests++;
+            }
+            bulkRequest.add(request);
+        }
+
+        CompoundProcessor processor = mock(CompoundProcessor.class);
+        Exception error = new RuntimeException();
+        doThrow(error).when(processor).execute(any());
+        when(store.get(pipelineId)).thenReturn(new Pipeline(pipelineId, null, processor));
+
+        BiConsumer<IndexRequest, Throwable> requestItemErrorHandler = mock(BiConsumer.class);
+        Consumer<Throwable> completionHandler = mock(Consumer.class);
+        executionService.execute(bulkRequest.requests(), requestItemErrorHandler, completionHandler);
+
+        verify(requestItemErrorHandler, times(numIndexRequests)).accept(any(IndexRequest.class), eq(error));
+        verify(completionHandler, times(1)).accept(null);
+    }
+
+    public void testBulkRequestExecution() throws Exception {
+        BulkRequest bulkRequest = new BulkRequest();
+        String pipelineId = "_id";
+
+        int numRequest = scaledRandomIntBetween(8, 64);
+        for (int i = 0; i < numRequest; i++) {
+            IndexRequest indexRequest = new IndexRequest("_index", "_type", "_id").setPipeline(pipelineId);
+            indexRequest.source("field1", "value1");
+            bulkRequest.add(indexRequest);
+        }
+
+        when(store.get(pipelineId)).thenReturn(new Pipeline(pipelineId, null, new CompoundProcessor()));
+
+        @SuppressWarnings("unchecked")
+        BiConsumer<IndexRequest, Throwable> requestItemErrorHandler = mock(BiConsumer.class);
+        @SuppressWarnings("unchecked")
+        Consumer<Throwable> completionHandler = mock(Consumer.class);
+        executionService.execute(bulkRequest.requests(), requestItemErrorHandler, completionHandler);
+
+        verify(requestItemErrorHandler, never()).accept(any(), any());
+        verify(completionHandler, times(1)).accept(null);
+    }
+
+    private IngestDocument eqID(String index, String type, String id, Map<String, Object> source) {
+        return argThat(new IngestDocumentMatcher(index, type, id, source));
+    }
+
+    private class IngestDocumentMatcher extends ArgumentMatcher<IngestDocument> {
+
+        private final IngestDocument ingestDocument;
+
+        public IngestDocumentMatcher(String index, String type, String id, Map<String, Object> source) {
+            this.ingestDocument = new IngestDocument(index, type, id, null, null, null, null, source);
+        }
+
+        @Override
+        public boolean matches(Object o) {
+            if (o.getClass() == IngestDocument.class) {
+                IngestDocument otherIngestDocument = (IngestDocument) o;
+                //ingest metadata will not be the same (timestamp differs every time)
+                return Objects.equals(ingestDocument.getSourceAndMetadata(), otherIngestDocument.getSourceAndMetadata());
+            }
+            return false;
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java b/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
new file mode 100644
index 0000000..117b95b
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/PipelineStoreTests.java
@@ -0,0 +1,182 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ResourceNotFoundException;
+import org.elasticsearch.action.ingest.DeletePipelineRequest;
+import org.elasticsearch.action.ingest.PutPipelineRequest;
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.ingest.core.Pipeline;
+import org.elasticsearch.ingest.processor.SetProcessor;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
+import static org.mockito.Mockito.mock;
+
+public class PipelineStoreTests extends ESTestCase {
+
+    private PipelineStore store;
+
+    @Before
+    public void init() throws Exception {
+        store = new PipelineStore(Settings.EMPTY);
+        ProcessorsRegistry registry = new ProcessorsRegistry();
+        registry.registerProcessor("set", (templateService) -> new SetProcessor.Factory(TestTemplateService.instance()));
+        store.buildProcessorFactoryRegistry(registry, null);
+    }
+
+    public void testUpdatePipelines() {
+        ClusterState clusterState = ClusterState.builder(new ClusterName("_name")).build();
+        store.innerUpdatePipelines(clusterState);
+        assertThat(store.pipelines.size(), is(0));
+
+        PipelineConfiguration pipeline = new PipelineConfiguration(
+            "_id",new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field\", \"value\": \"_value\"}}]}")
+        );
+        IngestMetadata ingestMetadata = new IngestMetadata(Collections.singletonMap("_id", pipeline));
+        clusterState = ClusterState.builder(clusterState)
+            .metaData(MetaData.builder().putCustom(IngestMetadata.TYPE, ingestMetadata))
+            .build();
+        store.innerUpdatePipelines(clusterState);
+        assertThat(store.pipelines.size(), is(1));
+        assertThat(store.pipelines.get("_id").getId(), equalTo("_id"));
+        assertThat(store.pipelines.get("_id").getDescription(), nullValue());
+        assertThat(store.pipelines.get("_id").getProcessors().size(), equalTo(1));
+        assertThat(store.pipelines.get("_id").getProcessors().get(0).getType(), equalTo("set"));
+    }
+
+    public void testPut() {
+        String id = "_id";
+        Pipeline pipeline = store.get(id);
+        assertThat(pipeline, nullValue());
+        ClusterState clusterState = ClusterState.builder(new ClusterName("_name")).build();
+
+        // add a new pipeline:
+        PutPipelineRequest putRequest = new PutPipelineRequest(id, new BytesArray("{\"processors\": []}"));
+        clusterState = store.innerPut(putRequest, clusterState);
+        store.innerUpdatePipelines(clusterState);
+        pipeline = store.get(id);
+        assertThat(pipeline, notNullValue());
+        assertThat(pipeline.getId(), equalTo(id));
+        assertThat(pipeline.getDescription(), nullValue());
+        assertThat(pipeline.getProcessors().size(), equalTo(0));
+
+        // overwrite existing pipeline:
+        putRequest = new PutPipelineRequest(id, new BytesArray("{\"processors\": [], \"description\": \"_description\"}"));
+        clusterState = store.innerPut(putRequest, clusterState);
+        store.innerUpdatePipelines(clusterState);
+        pipeline = store.get(id);
+        assertThat(pipeline, notNullValue());
+        assertThat(pipeline.getId(), equalTo(id));
+        assertThat(pipeline.getDescription(), equalTo("_description"));
+        assertThat(pipeline.getProcessors().size(), equalTo(0));
+    }
+
+    public void testDelete() {
+        PipelineConfiguration config = new PipelineConfiguration(
+            "_id",new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field\", \"value\": \"_value\"}}]}")
+        );
+        IngestMetadata ingestMetadata = new IngestMetadata(Collections.singletonMap("_id", config));
+        ClusterState clusterState = ClusterState.builder(new ClusterName("_name"))
+            .metaData(MetaData.builder().putCustom(IngestMetadata.TYPE, ingestMetadata))
+            .build();
+        store.innerUpdatePipelines(clusterState);
+        assertThat(store.get("_id"), notNullValue());
+
+        // Delete pipeline:
+        DeletePipelineRequest deleteRequest = new DeletePipelineRequest("_id");
+        clusterState = store.innerDelete(deleteRequest, clusterState);
+        store.innerUpdatePipelines(clusterState);
+        assertThat(store.get("_id"), nullValue());
+
+        // Delete existing pipeline:
+        try {
+            store.innerDelete(deleteRequest, clusterState);
+            fail("exception expected");
+        } catch (ResourceNotFoundException e) {
+            assertThat(e.getMessage(), equalTo("pipeline [_id] is missing"));
+        }
+    }
+
+    public void testGetPipelines() {
+        Map<String, PipelineConfiguration> configs = new HashMap<>();
+        configs.put("_id1", new PipelineConfiguration(
+            "_id1", new BytesArray("{\"processors\": []}")
+        ));
+        configs.put("_id2", new PipelineConfiguration(
+            "_id2", new BytesArray("{\"processors\": []}")
+        ));
+
+        assertThat(store.innerGetPipelines(null, "_id1").isEmpty(), is(true));
+
+        IngestMetadata ingestMetadata = new IngestMetadata(configs);
+        List<PipelineConfiguration> pipelines = store.innerGetPipelines(ingestMetadata, "_id1");
+        assertThat(pipelines.size(), equalTo(1));
+        assertThat(pipelines.get(0).getId(), equalTo("_id1"));
+
+        pipelines = store.innerGetPipelines(ingestMetadata, "_id1", "_id2");
+        assertThat(pipelines.size(), equalTo(2));
+        assertThat(pipelines.get(0).getId(), equalTo("_id1"));
+        assertThat(pipelines.get(1).getId(), equalTo("_id2"));
+
+        pipelines = store.innerGetPipelines(ingestMetadata, "_id*");
+        pipelines.sort((o1, o2) -> o1.getId().compareTo(o2.getId()));
+        assertThat(pipelines.size(), equalTo(2));
+        assertThat(pipelines.get(0).getId(), equalTo("_id1"));
+        assertThat(pipelines.get(1).getId(), equalTo("_id2"));
+    }
+
+    public void testCrud() throws Exception {
+        String id = "_id";
+        Pipeline pipeline = store.get(id);
+        assertThat(pipeline, nullValue());
+        ClusterState clusterState = ClusterState.builder(new ClusterName("_name")).build(); // Start empty
+
+        PutPipelineRequest putRequest = new PutPipelineRequest(id, new BytesArray("{\"processors\": [{\"set\" : {\"field\": \"_field\", \"value\": \"_value\"}}]}"));
+        clusterState = store.innerPut(putRequest, clusterState);
+        store.innerUpdatePipelines(clusterState);
+        pipeline = store.get(id);
+        assertThat(pipeline, notNullValue());
+        assertThat(pipeline.getId(), equalTo(id));
+        assertThat(pipeline.getDescription(), nullValue());
+        assertThat(pipeline.getProcessors().size(), equalTo(1));
+        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("set"));
+
+        DeletePipelineRequest deleteRequest = new DeletePipelineRequest(id);
+        clusterState = store.innerDelete(deleteRequest, clusterState);
+        store.innerUpdatePipelines(clusterState);
+        pipeline = store.get(id);
+        assertThat(pipeline, nullValue());
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java b/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java
new file mode 100644
index 0000000..ad18488
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/ProcessorsRegistryTests.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Map;
+import java.util.Set;
+import java.util.function.Function;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class ProcessorsRegistryTests extends ESTestCase {
+
+    public void testAddProcessor() {
+        ProcessorsRegistry processorsRegistry = new ProcessorsRegistry();
+        TestProcessor.Factory factory1 = new TestProcessor.Factory();
+        processorsRegistry.registerProcessor("1", (templateService) -> factory1);
+        TestProcessor.Factory factory2 = new TestProcessor.Factory();
+        processorsRegistry.registerProcessor("2", (templateService) -> factory2);
+        TestProcessor.Factory factory3 = new TestProcessor.Factory();
+        try {
+            processorsRegistry.registerProcessor("1", (templateService) -> factory3);
+            fail("addProcessor should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("Processor factory already registered for name [1]"));
+        }
+
+        Set<Map.Entry<String, Function<TemplateService, Processor.Factory<?>>>> entrySet = processorsRegistry.entrySet();
+        assertThat(entrySet.size(), equalTo(2));
+        for (Map.Entry<String, Function<TemplateService, Processor.Factory<?>>> entry : entrySet) {
+            if (entry.getKey().equals("1")) {
+                assertThat(entry.getValue().apply(null), equalTo(factory1));
+            } else if (entry.getKey().equals("2")) {
+                assertThat(entry.getValue().apply(null), equalTo(factory2));
+            } else {
+                fail("unexpected processor id [" + entry.getKey() + "]");
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java
new file mode 100644
index 0000000..f21644e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/core/CompoundProcessorTests.java
@@ -0,0 +1,117 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import org.elasticsearch.ingest.TestProcessor;
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.ingest.processor.AppendProcessor;
+import org.elasticsearch.ingest.processor.SetProcessor;
+import org.elasticsearch.ingest.processor.SplitProcessor;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+public class CompoundProcessorTests extends ESTestCase {
+    private IngestDocument ingestDocument;
+
+    @Before
+    public void init() {
+        ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
+    }
+
+    public void testEmpty() throws Exception {
+        CompoundProcessor processor = new CompoundProcessor();
+        assertThat(processor.getProcessors().isEmpty(), is(true));
+        assertThat(processor.getOnFailureProcessors().isEmpty(), is(true));
+        processor.execute(ingestDocument);
+    }
+
+    public void testSingleProcessor() throws Exception {
+        TestProcessor processor = new TestProcessor(ingestDocument -> {});
+        CompoundProcessor compoundProcessor = new CompoundProcessor(processor);
+        assertThat(compoundProcessor.getProcessors().size(), equalTo(1));
+        assertThat(compoundProcessor.getProcessors().get(0), equalTo(processor));
+        assertThat(compoundProcessor.getOnFailureProcessors().isEmpty(), is(true));
+        compoundProcessor.execute(ingestDocument);
+        assertThat(processor.getInvokedCounter(), equalTo(1));
+    }
+
+    public void testSingleProcessorWithException() throws Exception {
+        TestProcessor processor = new TestProcessor(ingestDocument -> {throw new RuntimeException("error");});
+        CompoundProcessor compoundProcessor = new CompoundProcessor(processor);
+        assertThat(compoundProcessor.getProcessors().size(), equalTo(1));
+        assertThat(compoundProcessor.getProcessors().get(0), equalTo(processor));
+        assertThat(compoundProcessor.getOnFailureProcessors().isEmpty(), is(true));
+        try {
+            compoundProcessor.execute(ingestDocument);
+            fail("should throw exception");
+        } catch (Exception e) {
+            assertThat(e.getMessage(), equalTo("error"));
+        }
+        assertThat(processor.getInvokedCounter(), equalTo(1));
+    }
+
+    public void testSingleProcessorWithOnFailureProcessor() throws Exception {
+        TestProcessor processor1 = new TestProcessor("id", "first", ingestDocument -> {throw new RuntimeException("error");});
+        TestProcessor processor2 = new TestProcessor(ingestDocument -> {
+            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
+            assertThat(ingestMetadata.size(), equalTo(2));
+            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
+            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("first"));
+        });
+
+        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor1), Collections.singletonList(processor2));
+        compoundProcessor.execute(ingestDocument);
+
+        assertThat(processor1.getInvokedCounter(), equalTo(1));
+        assertThat(processor2.getInvokedCounter(), equalTo(1));
+    }
+
+    public void testSingleProcessorWithNestedFailures() throws Exception {
+        TestProcessor processor = new TestProcessor("id", "first", ingestDocument -> {throw new RuntimeException("error");});
+        TestProcessor processorToFail = new TestProcessor("id", "second", ingestDocument -> {
+            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
+            assertThat(ingestMetadata.size(), equalTo(2));
+            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
+            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("first"));
+            throw new RuntimeException("error");
+        });
+        TestProcessor lastProcessor = new TestProcessor(ingestDocument -> {
+            Map<String, String> ingestMetadata = ingestDocument.getIngestMetadata();
+            assertThat(ingestMetadata.size(), equalTo(2));
+            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("error"));
+            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_FIELD), equalTo("second"));
+        });
+        CompoundProcessor compoundOnFailProcessor = new CompoundProcessor(Collections.singletonList(processorToFail), Collections.singletonList(lastProcessor));
+        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(processor), Collections.singletonList(compoundOnFailProcessor));
+        compoundProcessor.execute(ingestDocument);
+
+        assertThat(processorToFail.getInvokedCounter(), equalTo(1));
+        assertThat(lastProcessor.getInvokedCounter(), equalTo(1));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java b/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
new file mode 100644
index 0000000..958378f
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/core/ConfigurationUtilsTests.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+
+public class ConfigurationUtilsTests extends ESTestCase {
+    private Map<String, Object> config;
+
+    @Before
+    public void setConfig() {
+        config = new HashMap<>();
+        config.put("foo", "bar");
+        config.put("arr", Arrays.asList("1", "2", "3"));
+        List<Integer> list = new ArrayList<>();
+        list.add(2);
+        config.put("int", list);
+        config.put("ip", "127.0.0.1");
+        Map<String, Object> fizz = new HashMap<>();
+        fizz.put("buzz", "hello world");
+        config.put("fizz", fizz);
+    }
+
+    public void testReadStringProperty() {
+        String val = ConfigurationUtils.readStringProperty(config, "foo");
+        assertThat(val, equalTo("bar"));
+    }
+
+    public void testReadStringPropertyInvalidType() {
+        try {
+            ConfigurationUtils.readStringProperty(config, "arr");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("property [arr] isn't a string, but of type [java.util.Arrays$ArrayList]"));
+        }
+    }
+
+    // TODO(talevy): Issue with generics. This test should fail, "int" is of type List<Integer>
+    public void testOptional_InvalidType() {
+        List<String> val = ConfigurationUtils.readList(config, "int");
+        assertThat(val, equalTo(Arrays.asList(2)));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java b/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java
new file mode 100644
index 0000000..56d1fa7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/core/IngestDocumentTests.java
@@ -0,0 +1,976 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.text.DateFormat;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.both;
+import static org.hamcrest.Matchers.endsWith;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThanOrEqualTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.lessThanOrEqualTo;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.sameInstance;
+
+public class IngestDocumentTests extends ESTestCase {
+
+    private IngestDocument ingestDocument;
+
+    @Before
+    public void setIngestDocument() {
+        Map<String, Object> document = new HashMap<>();
+        Map<String, Object> ingestMap = new HashMap<>();
+        ingestMap.put("timestamp", "bogus_timestamp");
+        document.put("_ingest", ingestMap);
+        document.put("foo", "bar");
+        document.put("int", 123);
+        Map<String, Object> innerObject = new HashMap<>();
+        innerObject.put("buzz", "hello world");
+        innerObject.put("foo_null", null);
+        innerObject.put("1", "bar");
+        List<String> innerInnerList = new ArrayList<>();
+        innerInnerList.add("item1");
+        List<Object> innerList = new ArrayList<>();
+        innerList.add(innerInnerList);
+        innerObject.put("list", innerList);
+        document.put("fizz", innerObject);
+        List<Map<String, Object>> list = new ArrayList<>();
+        Map<String, Object> value = new HashMap<>();
+        value.put("field", "value");
+        list.add(value);
+        list.add(null);
+
+        document.put("list", list);
+        ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+    }
+
+    public void testSimpleGetFieldValue() {
+        assertThat(ingestDocument.getFieldValue("foo", String.class), equalTo("bar"));
+        assertThat(ingestDocument.getFieldValue("int", Integer.class), equalTo(123));
+        assertThat(ingestDocument.getFieldValue("_source.foo", String.class), equalTo("bar"));
+        assertThat(ingestDocument.getFieldValue("_source.int", Integer.class), equalTo(123));
+        assertThat(ingestDocument.getFieldValue("_index", String.class), equalTo("index"));
+        assertThat(ingestDocument.getFieldValue("_type", String.class), equalTo("type"));
+        assertThat(ingestDocument.getFieldValue("_id", String.class), equalTo("id"));
+        assertThat(ingestDocument.getFieldValue("_ingest.timestamp", String.class), both(notNullValue()).and(not(equalTo("bogus_timestamp"))));
+        assertThat(ingestDocument.getFieldValue("_source._ingest.timestamp", String.class), equalTo("bogus_timestamp"));
+    }
+
+    public void testGetSourceObject() {
+        try {
+            ingestDocument.getFieldValue("_source", Object.class);
+            fail("get field value should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [_source] not present as part of path [_source]"));
+        }
+    }
+
+    public void testGetIngestObject() {
+        assertThat(ingestDocument.getFieldValue("_ingest", Map.class), notNullValue());
+    }
+
+    public void testGetEmptyPathAfterStrippingOutPrefix() {
+        try {
+            ingestDocument.getFieldValue("_source.", Object.class);
+            fail("get field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
+        }
+
+        try {
+            ingestDocument.getFieldValue("_ingest.", Object.class);
+            fail("get field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
+        }
+    }
+
+    public void testGetFieldValueNullValue() {
+        assertThat(ingestDocument.getFieldValue("fizz.foo_null", Object.class), nullValue());
+    }
+
+    public void testSimpleGetFieldValueTypeMismatch() {
+        try {
+            ingestDocument.getFieldValue("int", String.class);
+            fail("getFieldValue should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [int] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
+        }
+
+        try {
+            ingestDocument.getFieldValue("foo", Integer.class);
+            fail("getFieldValue should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [foo] of type [java.lang.String] cannot be cast to [java.lang.Integer]"));
+        }
+    }
+
+    public void testNestedGetFieldValue() {
+        assertThat(ingestDocument.getFieldValue("fizz.buzz", String.class), equalTo("hello world"));
+        assertThat(ingestDocument.getFieldValue("fizz.1", String.class), equalTo("bar"));
+    }
+
+    public void testNestedGetFieldValueTypeMismatch() {
+        try {
+            ingestDocument.getFieldValue("foo.foo.bar", String.class);
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("cannot resolve [foo] from object of type [java.lang.String] as part of path [foo.foo.bar]"));
+        }
+    }
+
+    public void testListGetFieldValue() {
+        assertThat(ingestDocument.getFieldValue("list.0.field", String.class), equalTo("value"));
+    }
+
+    public void testListGetFieldValueNull() {
+        assertThat(ingestDocument.getFieldValue("list.1", String.class), nullValue());
+    }
+
+    public void testListGetFieldValueIndexNotNumeric() {
+        try {
+            ingestDocument.getFieldValue("list.test.field", String.class);
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test.field]"));
+        }
+    }
+
+    public void testListGetFieldValueIndexOutOfBounds() {
+        try {
+            ingestDocument.getFieldValue("list.10.field", String.class);
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10.field]"));
+        }
+    }
+
+    public void testGetFieldValueNotFound() {
+        try {
+            ingestDocument.getFieldValue("not.here", String.class);
+            fail("get field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [not] not present as part of path [not.here]"));
+        }
+    }
+
+    public void testGetFieldValueNotFoundNullParent() {
+        try {
+            ingestDocument.getFieldValue("fizz.foo_null.not_there", String.class);
+            fail("get field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("cannot resolve [not_there] from null as part of path [fizz.foo_null.not_there]"));
+        }
+    }
+
+    public void testGetFieldValueNull() {
+        try {
+            ingestDocument.getFieldValue(null, String.class);
+            fail("get field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
+        }
+    }
+
+    public void testGetFieldValueEmpty() {
+        try {
+            ingestDocument.getFieldValue("", String.class);
+            fail("get field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
+        }
+    }
+
+    public void testHasField() {
+        assertTrue(ingestDocument.hasField("fizz"));
+        assertTrue(ingestDocument.hasField("_index"));
+        assertTrue(ingestDocument.hasField("_type"));
+        assertTrue(ingestDocument.hasField("_id"));
+        assertTrue(ingestDocument.hasField("_source.fizz"));
+        assertTrue(ingestDocument.hasField("_ingest.timestamp"));
+    }
+
+    public void testHasFieldNested() {
+        assertTrue(ingestDocument.hasField("fizz.buzz"));
+        assertTrue(ingestDocument.hasField("_source._ingest.timestamp"));
+    }
+
+    public void testListHasField() {
+        assertTrue(ingestDocument.hasField("list.0.field"));
+    }
+
+    public void testListHasFieldNull() {
+        assertTrue(ingestDocument.hasField("list.1"));
+    }
+
+    public void testListHasFieldIndexOutOfBounds() {
+        assertFalse(ingestDocument.hasField("list.10"));
+    }
+
+    public void testListHasFieldIndexNotNumeric() {
+        assertFalse(ingestDocument.hasField("list.test"));
+    }
+
+    public void testNestedHasFieldTypeMismatch() {
+        assertFalse(ingestDocument.hasField("foo.foo.bar"));
+    }
+
+    public void testHasFieldNotFound() {
+        assertFalse(ingestDocument.hasField("not.here"));
+    }
+
+    public void testHasFieldNotFoundNullParent() {
+        assertFalse(ingestDocument.hasField("fizz.foo_null.not_there"));
+    }
+
+    public void testHasFieldNestedNotFound() {
+        assertFalse(ingestDocument.hasField("fizz.doesnotexist"));
+    }
+
+    public void testHasFieldNull() {
+        try {
+            ingestDocument.hasField(null);
+            fail("has field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
+        }
+    }
+
+    public void testHasFieldNullValue() {
+        assertTrue(ingestDocument.hasField("fizz.foo_null"));
+    }
+
+    public void testHasFieldEmpty() {
+        try {
+            ingestDocument.hasField("");
+            fail("has field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
+        }
+    }
+
+    public void testHasFieldSourceObject() {
+        assertThat(ingestDocument.hasField("_source"), equalTo(false));
+    }
+
+    public void testHasFieldIngestObject() {
+        assertThat(ingestDocument.hasField("_ingest"), equalTo(true));
+    }
+
+    public void testHasFieldEmptyPathAfterStrippingOutPrefix() {
+        try {
+            ingestDocument.hasField("_source.");
+            fail("has field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
+        }
+
+        try {
+            ingestDocument.hasField("_ingest.");
+            fail("has field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
+        }
+    }
+
+    public void testSimpleSetFieldValue() {
+        ingestDocument.setFieldValue("new_field", "foo");
+        assertThat(ingestDocument.getSourceAndMetadata().get("new_field"), equalTo("foo"));
+        ingestDocument.setFieldValue("_ttl", "ttl");
+        assertThat(ingestDocument.getSourceAndMetadata().get("_ttl"), equalTo("ttl"));
+        ingestDocument.setFieldValue("_source.another_field", "bar");
+        assertThat(ingestDocument.getSourceAndMetadata().get("another_field"), equalTo("bar"));
+        ingestDocument.setFieldValue("_ingest.new_field", "new_value");
+        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(2));
+        assertThat(ingestDocument.getIngestMetadata().get("new_field"), equalTo("new_value"));
+        ingestDocument.setFieldValue("_ingest.timestamp", "timestamp");
+        assertThat(ingestDocument.getIngestMetadata().get("timestamp"), equalTo("timestamp"));
+    }
+
+    public void testSetFieldValueNullValue() {
+        ingestDocument.setFieldValue("new_field", null);
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(true));
+        assertThat(ingestDocument.getSourceAndMetadata().get("new_field"), nullValue());
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testNestedSetFieldValue() {
+        ingestDocument.setFieldValue("a.b.c.d", "foo");
+        assertThat(ingestDocument.getSourceAndMetadata().get("a"), instanceOf(Map.class));
+        Map<String, Object> a = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("a");
+        assertThat(a.get("b"), instanceOf(Map.class));
+        Map<String, Object> b = (Map<String, Object>) a.get("b");
+        assertThat(b.get("c"), instanceOf(Map.class));
+        Map<String, Object> c = (Map<String, Object>) b.get("c");
+        assertThat(c.get("d"), instanceOf(String.class));
+        String d = (String) c.get("d");
+        assertThat(d, equalTo("foo"));
+    }
+
+    public void testSetFieldValueOnExistingField() {
+        ingestDocument.setFieldValue("foo", "newbar");
+        assertThat(ingestDocument.getSourceAndMetadata().get("foo"), equalTo("newbar"));
+    }
+
+    @SuppressWarnings("unchecked")
+    public void testSetFieldValueOnExistingParent() {
+        ingestDocument.setFieldValue("fizz.new", "bar");
+        assertThat(ingestDocument.getSourceAndMetadata().get("fizz"), instanceOf(Map.class));
+        Map<String, Object> innerMap = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(innerMap.get("new"), instanceOf(String.class));
+        String value = (String) innerMap.get("new");
+        assertThat(value, equalTo("bar"));
+    }
+
+    public void testSetFieldValueOnExistingParentTypeMismatch() {
+        try {
+            ingestDocument.setFieldValue("fizz.buzz.new", "bar");
+            fail("add field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("cannot set [new] with parent object of type [java.lang.String] as part of path [fizz.buzz.new]"));
+        }
+    }
+
+    public void testSetFieldValueOnExistingNullParent() {
+        try {
+            ingestDocument.setFieldValue("fizz.foo_null.test", "bar");
+            fail("add field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("cannot set [test] with null parent as part of path [fizz.foo_null.test]"));
+        }
+    }
+
+    public void testSetFieldValueNullName() {
+        try {
+            ingestDocument.setFieldValue(null, "bar");
+            fail("add field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
+        }
+    }
+
+    public void testSetSourceObject() {
+        ingestDocument.setFieldValue("_source", "value");
+        assertThat(ingestDocument.getSourceAndMetadata().get("_source"), equalTo("value"));
+    }
+
+    public void testSetIngestObject() {
+        ingestDocument.setFieldValue("_ingest", "value");
+        assertThat(ingestDocument.getSourceAndMetadata().get("_ingest"), equalTo("value"));
+    }
+
+    public void testSetIngestSourceObject() {
+        //test that we don't strip out the _source prefix when _ingest is used
+        ingestDocument.setFieldValue("_ingest._source", "value");
+        assertThat(ingestDocument.getIngestMetadata().get("_source"), equalTo("value"));
+    }
+
+    public void testSetEmptyPathAfterStrippingOutPrefix() {
+        try {
+            ingestDocument.setFieldValue("_source.", "value");
+            fail("set field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
+        }
+
+        try {
+            ingestDocument.setFieldValue("_ingest.", "_value");
+            fail("set field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
+        }
+    }
+
+    public void testListSetFieldValueNoIndexProvided() {
+        ingestDocument.setFieldValue("list", "value");
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(String.class));
+        assertThat(object, equalTo("value"));
+    }
+
+    public void testListAppendFieldValue() {
+        ingestDocument.appendFieldValue("list", "new_value");
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(3));
+        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
+        assertThat(list.get(1), nullValue());
+        assertThat(list.get(2), equalTo("new_value"));
+    }
+
+    public void testListAppendFieldValues() {
+        ingestDocument.appendFieldValue("list", Arrays.asList("item1", "item2", "item3"));
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(5));
+        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
+        assertThat(list.get(1), nullValue());
+        assertThat(list.get(2), equalTo("item1"));
+        assertThat(list.get(3), equalTo("item2"));
+        assertThat(list.get(4), equalTo("item3"));
+    }
+
+    public void testAppendFieldValueToNonExistingList() {
+        ingestDocument.appendFieldValue("non_existing_list", "new_value");
+        Object object = ingestDocument.getSourceAndMetadata().get("non_existing_list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(1));
+        assertThat(list.get(0), equalTo("new_value"));
+    }
+
+    public void testAppendFieldValuesToNonExistingList() {
+        ingestDocument.appendFieldValue("non_existing_list", Arrays.asList("item1", "item2", "item3"));
+        Object object = ingestDocument.getSourceAndMetadata().get("non_existing_list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(3));
+        assertThat(list.get(0), equalTo("item1"));
+        assertThat(list.get(1), equalTo("item2"));
+        assertThat(list.get(2), equalTo("item3"));
+    }
+
+    public void testAppendFieldValueConvertStringToList() {
+        ingestDocument.appendFieldValue("fizz.buzz", "new_value");
+        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(object, instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) object;
+        object = map.get("buzz");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), equalTo("hello world"));
+        assertThat(list.get(1), equalTo("new_value"));
+    }
+
+    public void testAppendFieldValuesConvertStringToList() {
+        ingestDocument.appendFieldValue("fizz.buzz", Arrays.asList("item1", "item2", "item3"));
+        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(object, instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) object;
+        object = map.get("buzz");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(4));
+        assertThat(list.get(0), equalTo("hello world"));
+        assertThat(list.get(1), equalTo("item1"));
+        assertThat(list.get(2), equalTo("item2"));
+        assertThat(list.get(3), equalTo("item3"));
+    }
+
+    public void testAppendFieldValueConvertIntegerToList() {
+        ingestDocument.appendFieldValue("int", 456);
+        Object object = ingestDocument.getSourceAndMetadata().get("int");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), equalTo(123));
+        assertThat(list.get(1), equalTo(456));
+    }
+
+    public void testAppendFieldValuesConvertIntegerToList() {
+        ingestDocument.appendFieldValue("int", Arrays.asList(456, 789));
+        Object object = ingestDocument.getSourceAndMetadata().get("int");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(3));
+        assertThat(list.get(0), equalTo(123));
+        assertThat(list.get(1), equalTo(456));
+        assertThat(list.get(2), equalTo(789));
+    }
+
+    public void testAppendFieldValueConvertMapToList() {
+        ingestDocument.appendFieldValue("fizz", Collections.singletonMap("field", "value"));
+        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(object, instanceOf(List.class));
+        List<?> list = (List<?>) object;
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) list.get(0);
+        assertThat(map.size(), equalTo(4));
+        assertThat(list.get(1), equalTo(Collections.singletonMap("field", "value")));
+    }
+
+    public void testAppendFieldValueToNull() {
+        ingestDocument.appendFieldValue("fizz.foo_null", "new_value");
+        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(object, instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) object;
+        object = map.get("foo_null");
+        assertThat(object, instanceOf(List.class));
+        List<?> list = (List<?>) object;
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), nullValue());
+        assertThat(list.get(1), equalTo("new_value"));
+    }
+
+    public void testAppendFieldValueToListElement() {
+        ingestDocument.appendFieldValue("fizz.list.0", "item2");
+        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(object, instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) object;
+        object = map.get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(1));
+        object = list.get(0);
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<String> innerList = (List<String>) object;
+        assertThat(innerList.size(), equalTo(2));
+        assertThat(innerList.get(0), equalTo("item1"));
+        assertThat(innerList.get(1), equalTo("item2"));
+    }
+
+    public void testAppendFieldValuesToListElement() {
+        ingestDocument.appendFieldValue("fizz.list.0", Arrays.asList("item2", "item3", "item4"));
+        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(object, instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) object;
+        object = map.get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(1));
+        object = list.get(0);
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<String> innerList = (List<String>) object;
+        assertThat(innerList.size(), equalTo(4));
+        assertThat(innerList.get(0), equalTo("item1"));
+        assertThat(innerList.get(1), equalTo("item2"));
+        assertThat(innerList.get(2), equalTo("item3"));
+        assertThat(innerList.get(3), equalTo("item4"));
+    }
+
+    public void testAppendFieldValueConvertStringListElementToList() {
+        ingestDocument.appendFieldValue("fizz.list.0.0", "new_value");
+        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(object, instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) object;
+        object = map.get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(1));
+        object = list.get(0);
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> innerList = (List<Object>) object;
+        object = innerList.get(0);
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<String> innerInnerList = (List<String>) object;
+        assertThat(innerInnerList.size(), equalTo(2));
+        assertThat(innerInnerList.get(0), equalTo("item1"));
+        assertThat(innerInnerList.get(1), equalTo("new_value"));
+    }
+
+    public void testAppendFieldValuesConvertStringListElementToList() {
+        ingestDocument.appendFieldValue("fizz.list.0.0", Arrays.asList("item2", "item3", "item4"));
+        Object object = ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(object, instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) object;
+        object = map.get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(1));
+        object = list.get(0);
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> innerList = (List<Object>) object;
+        object = innerList.get(0);
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<String> innerInnerList = (List<String>) object;
+        assertThat(innerInnerList.size(), equalTo(4));
+        assertThat(innerInnerList.get(0), equalTo("item1"));
+        assertThat(innerInnerList.get(1), equalTo("item2"));
+        assertThat(innerInnerList.get(2), equalTo("item3"));
+        assertThat(innerInnerList.get(3), equalTo("item4"));
+    }
+
+    public void testAppendFieldValueListElementConvertMapToList() {
+        ingestDocument.appendFieldValue("list.0", Collections.singletonMap("item2", "value2"));
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(List.class));
+        List<?> list = (List<?>) object;
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), instanceOf(List.class));
+        assertThat(list.get(1), nullValue());
+        list = (List<?>) list.get(0);
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
+        assertThat(list.get(1), equalTo(Collections.singletonMap("item2", "value2")));
+    }
+
+    public void testAppendFieldValueToNullListElement() {
+        ingestDocument.appendFieldValue("list.1", "new_value");
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(List.class));
+        List<?> list = (List<?>) object;
+        assertThat(list.get(1), instanceOf(List.class));
+        list = (List<?>) list.get(1);
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), nullValue());
+        assertThat(list.get(1), equalTo("new_value"));
+    }
+
+    public void testAppendFieldValueToListOfMaps() {
+        ingestDocument.appendFieldValue("list", Collections.singletonMap("item2", "value2"));
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(3));
+        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
+        assertThat(list.get(1), nullValue());
+        assertThat(list.get(2), equalTo(Collections.singletonMap("item2", "value2")));
+    }
+
+    public void testListSetFieldValueIndexProvided() {
+        ingestDocument.setFieldValue("list.1", "value");
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "value")));
+        assertThat(list.get(1), equalTo("value"));
+    }
+
+    public void testSetFieldValueListAsPartOfPath() {
+        ingestDocument.setFieldValue("list.0.field", "new_value");
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(2));
+        assertThat(list.get(0), equalTo(Collections.singletonMap("field", "new_value")));
+        assertThat(list.get(1), nullValue());
+    }
+
+    public void testListSetFieldValueIndexNotNumeric() {
+        try {
+            ingestDocument.setFieldValue("list.test", "value");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test]"));
+        }
+
+        try {
+            ingestDocument.setFieldValue("list.test.field", "new_value");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test.field]"));
+        }
+    }
+
+    public void testListSetFieldValueIndexOutOfBounds() {
+        try {
+            ingestDocument.setFieldValue("list.10", "value");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10]"));
+        }
+
+        try {
+            ingestDocument.setFieldValue("list.10.field", "value");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10.field]"));
+        }
+    }
+
+    public void testSetFieldValueEmptyName() {
+        try {
+            ingestDocument.setFieldValue("", "bar");
+            fail("add field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
+        }
+    }
+
+    public void testRemoveField() {
+        ingestDocument.removeField("foo");
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(7));
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("foo"), equalTo(false));
+        ingestDocument.removeField("_index");
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(6));
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("_index"), equalTo(false));
+        ingestDocument.removeField("_source.fizz");
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(5));
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(false));
+        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(1));
+        ingestDocument.removeField("_ingest.timestamp");
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(5));
+        assertThat(ingestDocument.getIngestMetadata().size(), equalTo(0));
+    }
+
+    public void testRemoveInnerField() {
+        ingestDocument.removeField("fizz.buzz");
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
+        assertThat(ingestDocument.getSourceAndMetadata().get("fizz"), instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("fizz");
+        assertThat(map.size(), equalTo(3));
+        assertThat(map.containsKey("buzz"), equalTo(false));
+
+        ingestDocument.removeField("fizz.foo_null");
+        assertThat(map.size(), equalTo(2));
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
+
+        ingestDocument.removeField("fizz.1");
+        assertThat(map.size(), equalTo(1));
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
+
+        ingestDocument.removeField("fizz.list");
+        assertThat(map.size(), equalTo(0));
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("fizz"), equalTo(true));
+    }
+
+    public void testRemoveNonExistingField() {
+        try {
+            ingestDocument.removeField("does_not_exist");
+            fail("remove field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [does_not_exist] not present as part of path [does_not_exist]"));
+        }
+    }
+
+    public void testRemoveExistingParentTypeMismatch() {
+        try {
+            ingestDocument.removeField("foo.foo.bar");
+            fail("remove field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("cannot resolve [foo] from object of type [java.lang.String] as part of path [foo.foo.bar]"));
+        }
+    }
+
+    public void testRemoveSourceObject() {
+        try {
+            ingestDocument.removeField("_source");
+            fail("remove field should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [_source] not present as part of path [_source]"));
+        }
+    }
+
+    public void testRemoveIngestObject() {
+        ingestDocument.removeField("_ingest");
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(7));
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("_ingest"), equalTo(false));
+    }
+
+    public void testRemoveEmptyPathAfterStrippingOutPrefix() {
+        try {
+            ingestDocument.removeField("_source.");
+            fail("set field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path [_source.] is not valid"));
+        }
+
+        try {
+            ingestDocument.removeField("_ingest.");
+            fail("set field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path [_ingest.] is not valid"));
+        }
+    }
+
+    public void testListRemoveField() {
+        ingestDocument.removeField("list.0.field");
+        assertThat(ingestDocument.getSourceAndMetadata().size(), equalTo(8));
+        assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
+        Object object = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(object, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<Object> list = (List<Object>) object;
+        assertThat(list.size(), equalTo(2));
+        object = list.get(0);
+        assertThat(object, instanceOf(Map.class));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> map = (Map<String, Object>) object;
+        assertThat(map.size(), equalTo(0));
+        ingestDocument.removeField("list.0");
+        assertThat(list.size(), equalTo(1));
+        assertThat(list.get(0), nullValue());
+    }
+
+    public void testRemoveFieldValueNotFoundNullParent() {
+        try {
+            ingestDocument.removeField("fizz.foo_null.not_there");
+            fail("get field value should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("cannot remove [not_there] from null as part of path [fizz.foo_null.not_there]"));
+        }
+    }
+
+    public void testNestedRemoveFieldTypeMismatch() {
+        try {
+            ingestDocument.removeField("fizz.1.bar");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("cannot remove [bar] from object of type [java.lang.String] as part of path [fizz.1.bar]"));
+        }
+    }
+
+    public void testListRemoveFieldIndexNotNumeric() {
+        try {
+            ingestDocument.removeField("list.test");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[test] is not an integer, cannot be used as an index as part of path [list.test]"));
+        }
+    }
+
+    public void testListRemoveFieldIndexOutOfBounds() {
+        try {
+            ingestDocument.removeField("list.10");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[10] is out of bounds for array with length [2] as part of path [list.10]"));
+        }
+    }
+
+    public void testRemoveNullField() {
+        try {
+            ingestDocument.removeField((String) null);
+            fail("remove field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
+        }
+    }
+
+    public void testRemoveEmptyField() {
+        try {
+            ingestDocument.removeField("");
+            fail("remove field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("path cannot be null nor empty"));
+        }
+    }
+
+    public void testEqualsAndHashcode() throws Exception {
+        Map<String, Object> sourceAndMetadata = RandomDocumentPicks.randomSource(random());
+        int numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
+        for (int i = 0; i < numFields; i++) {
+            sourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
+        }
+        Map<String, String> ingestMetadata = new HashMap<>();
+        numFields = randomIntBetween(1, 5);
+        for (int i = 0; i < numFields; i++) {
+            ingestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
+        }
+        IngestDocument ingestDocument = new IngestDocument(sourceAndMetadata, ingestMetadata);
+
+        boolean changed = false;
+        Map<String, Object> otherSourceAndMetadata;
+        if (randomBoolean()) {
+            otherSourceAndMetadata = RandomDocumentPicks.randomSource(random());
+            changed = true;
+        } else {
+            otherSourceAndMetadata = new HashMap<>(sourceAndMetadata);
+        }
+        if (randomBoolean()) {
+            numFields = randomIntBetween(1, IngestDocument.MetaData.values().length);
+            for (int i = 0; i < numFields; i++) {
+                otherSourceAndMetadata.put(randomFrom(IngestDocument.MetaData.values()).getFieldName(), randomAsciiOfLengthBetween(5, 10));
+            }
+            changed = true;
+        }
+
+        Map<String, String> otherIngestMetadata;
+        if (randomBoolean()) {
+            otherIngestMetadata = new HashMap<>();
+            numFields = randomIntBetween(1, 5);
+            for (int i = 0; i < numFields; i++) {
+                otherIngestMetadata.put(randomAsciiOfLengthBetween(5, 10), randomAsciiOfLengthBetween(5, 10));
+            }
+            changed = true;
+        } else {
+            otherIngestMetadata = Collections.unmodifiableMap(ingestMetadata);
+        }
+
+        IngestDocument otherIngestDocument = new IngestDocument(otherSourceAndMetadata, otherIngestMetadata);
+        if (changed) {
+            assertThat(ingestDocument, not(equalTo(otherIngestDocument)));
+            assertThat(otherIngestDocument, not(equalTo(ingestDocument)));
+        } else {
+            assertThat(ingestDocument, equalTo(otherIngestDocument));
+            assertThat(otherIngestDocument, equalTo(ingestDocument));
+            assertThat(ingestDocument.hashCode(), equalTo(otherIngestDocument.hashCode()));
+            IngestDocument thirdIngestDocument = new IngestDocument(Collections.unmodifiableMap(sourceAndMetadata), Collections.unmodifiableMap(ingestMetadata));
+            assertThat(thirdIngestDocument, equalTo(ingestDocument));
+            assertThat(ingestDocument, equalTo(thirdIngestDocument));
+            assertThat(ingestDocument.hashCode(), equalTo(thirdIngestDocument.hashCode()));
+        }
+    }
+
+    public void testIngestMetadataTimestamp() throws Exception {
+        long before = System.currentTimeMillis();
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        long after = System.currentTimeMillis();
+        String timestampString = ingestDocument.getIngestMetadata().get("timestamp");
+        assertThat(timestampString, notNullValue());
+        assertThat(timestampString, endsWith("+0000"));
+        DateFormat df = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZZ", Locale.ROOT);
+        Date timestamp = df.parse(timestampString);
+        assertThat(timestamp.getTime(), greaterThanOrEqualTo(before));
+        assertThat(timestamp.getTime(), lessThanOrEqualTo(after));
+    }
+
+    public void testCopyConstructor() {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        IngestDocument copy = new IngestDocument(ingestDocument);
+        assertThat(ingestDocument.getSourceAndMetadata(), not(sameInstance(copy.getSourceAndMetadata())));
+        assertThat(ingestDocument.getSourceAndMetadata(), equalTo(copy.getSourceAndMetadata()));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java
new file mode 100644
index 0000000..2292903
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/core/PipelineFactoryTests.java
@@ -0,0 +1,101 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import org.elasticsearch.ingest.TestProcessor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.Matchers.nullValue;
+
+public class PipelineFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        Map<String, Object> processorConfig0 = new HashMap<>();
+        Map<String, Object> processorConfig1 = new HashMap<>();
+        processorConfig0.put(AbstractProcessorFactory.TAG_KEY, "first-processor");
+        Map<String, Object> pipelineConfig = new HashMap<>();
+        pipelineConfig.put(Pipeline.DESCRIPTION_KEY, "_description");
+        pipelineConfig.put(Pipeline.PROCESSORS_KEY, Arrays.asList(Collections.singletonMap("test", processorConfig0), Collections.singletonMap("test", processorConfig1)));
+        Pipeline.Factory factory = new Pipeline.Factory();
+        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
+        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
+        assertThat(pipeline.getId(), equalTo("_id"));
+        assertThat(pipeline.getDescription(), equalTo("_description"));
+        assertThat(pipeline.getProcessors().size(), equalTo(2));
+        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("test-processor"));
+        assertThat(pipeline.getProcessors().get(0).getTag(), equalTo("first-processor"));
+        assertThat(pipeline.getProcessors().get(1).getType(), equalTo("test-processor"));
+        assertThat(pipeline.getProcessors().get(1).getTag(), nullValue());
+    }
+
+    public void testCreateWithPipelineOnFailure() throws Exception {
+        Map<String, Object> processorConfig = new HashMap<>();
+        Map<String, Object> pipelineConfig = new HashMap<>();
+        pipelineConfig.put(Pipeline.DESCRIPTION_KEY, "_description");
+        pipelineConfig.put(Pipeline.PROCESSORS_KEY, Collections.singletonList(Collections.singletonMap("test", processorConfig)));
+        pipelineConfig.put(Pipeline.ON_FAILURE_KEY, Collections.singletonList(Collections.singletonMap("test", processorConfig)));
+        Pipeline.Factory factory = new Pipeline.Factory();
+        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
+        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
+        assertThat(pipeline.getId(), equalTo("_id"));
+        assertThat(pipeline.getDescription(), equalTo("_description"));
+        assertThat(pipeline.getProcessors().size(), equalTo(1));
+        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("test-processor"));
+        assertThat(pipeline.getOnFailureProcessors().size(), equalTo(1));
+        assertThat(pipeline.getOnFailureProcessors().get(0).getType(), equalTo("test-processor"));
+    }
+
+    public void testCreateUnusedProcessorOptions() throws Exception {
+        Map<String, Object> processorConfig = new HashMap<>();
+        processorConfig.put("unused", "value");
+        Map<String, Object> pipelineConfig = new HashMap<>();
+        pipelineConfig.put(Pipeline.DESCRIPTION_KEY, "_description");
+        pipelineConfig.put(Pipeline.PROCESSORS_KEY, Collections.singletonList(Collections.singletonMap("test", processorConfig)));
+        Pipeline.Factory factory = new Pipeline.Factory();
+        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
+        try {
+            factory.create("_id", pipelineConfig, processorRegistry);
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("processor [test] doesn't support one or more provided configuration parameters [unused]"));
+        }
+    }
+
+    public void testCreateProcessorsWithOnFailureProperties() throws Exception {
+        Map<String, Object> processorConfig = new HashMap<>();
+        processorConfig.put(Pipeline.ON_FAILURE_KEY, Collections.singletonList(Collections.singletonMap("test", new HashMap<>())));
+
+        Map<String, Object> pipelineConfig = new HashMap<>();
+        pipelineConfig.put(Pipeline.DESCRIPTION_KEY, "_description");
+        pipelineConfig.put(Pipeline.PROCESSORS_KEY, Collections.singletonList(Collections.singletonMap("test", processorConfig)));
+        Pipeline.Factory factory = new Pipeline.Factory();
+        Map<String, Processor.Factory> processorRegistry = Collections.singletonMap("test", new TestProcessor.Factory());
+        Pipeline pipeline = factory.create("_id", pipelineConfig, processorRegistry);
+        assertThat(pipeline.getId(), equalTo("_id"));
+        assertThat(pipeline.getDescription(), equalTo("_description"));
+        assertThat(pipeline.getProcessors().size(), equalTo(1));
+        assertThat(pipeline.getProcessors().get(0).getType(), equalTo("compound"));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java b/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java
new file mode 100644
index 0000000..f2aa9f3
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/core/ValueSourceTests.java
@@ -0,0 +1,72 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.core;
+
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.sameInstance;
+
+public class ValueSourceTests extends ESTestCase {
+
+    public void testDeepCopy() {
+        int iterations = scaledRandomIntBetween(8, 64);
+        for (int i = 0; i < iterations; i++) {
+            Map<String, Object> map = RandomDocumentPicks.randomSource(random());
+            ValueSource valueSource = ValueSource.wrap(map, TestTemplateService.instance());
+            Object copy = valueSource.copyAndResolve(Collections.emptyMap());
+            assertThat("iteration: " + i, copy, equalTo(map));
+            assertThat("iteration: " + i, copy, not(sameInstance(map)));
+        }
+    }
+
+    public void testCopyDoesNotChangeProvidedMap() {
+        Map<String, Object> myPreciousMap = new HashMap<>();
+        myPreciousMap.put("field2", "value2");
+
+        IngestDocument ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
+        ingestDocument.setFieldValue(TestTemplateService.instance().compile("field1"), ValueSource.wrap(myPreciousMap, TestTemplateService.instance()));
+        ingestDocument.removeField("field1.field2");
+
+        assertThat(myPreciousMap.size(), equalTo(1));
+        assertThat(myPreciousMap.get("field2"), equalTo("value2"));
+    }
+
+    public void testCopyDoesNotChangeProvidedList() {
+        List<String> myPreciousList = new ArrayList<>();
+        myPreciousList.add("value");
+
+        IngestDocument ingestDocument = new IngestDocument(new HashMap<>(), new HashMap<>());
+        ingestDocument.setFieldValue(TestTemplateService.instance().compile("field1"), ValueSource.wrap(myPreciousList, TestTemplateService.instance()));
+        ingestDocument.removeField("field1.0");
+
+        assertThat(myPreciousList.size(), equalTo(1));
+        assertThat(myPreciousList.get(0), equalTo("value"));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java b/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java
new file mode 100644
index 0000000..1113a4b
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/AbstractStringProcessorTestCase.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Collections;
+import java.util.HashMap;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+
+public abstract class AbstractStringProcessorTestCase extends ESTestCase {
+
+    protected abstract AbstractStringProcessor newProcessor(String field);
+
+    protected String modifyInput(String input) {
+        return input;
+    }
+
+    protected abstract String expectedResult(String input);
+
+    public void testProcessor() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        String fieldValue = RandomDocumentPicks.randomString(random());
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, modifyInput(fieldValue));
+        Processor processor = newProcessor(fieldName);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult(fieldValue)));
+    }
+
+    public void testFieldNotFound() throws Exception {
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        Processor processor = newProcessor(fieldName);
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        try {
+            processor.execute(ingestDocument);
+            fail("processor should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
+        }
+    }
+
+    public void testNullValue() throws Exception {
+        Processor processor = newProcessor("field");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
+        try {
+            processor.execute(ingestDocument);
+            fail("processor should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [field] is null, cannot process it."));
+        }
+    }
+
+    public void testNonStringValue() throws Exception {
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        Processor processor = newProcessor(fieldName);
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        ingestDocument.setFieldValue(fieldName, randomInt());
+        try {
+            processor.execute(ingestDocument);
+            fail("processor should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
new file mode 100644
index 0000000..b72c144
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorFactoryTests.java
@@ -0,0 +1,94 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class AppendProcessorFactoryTests extends ESTestCase {
+
+    private AppendProcessor.Factory factory;
+
+    @Before
+    public void init() {
+        factory = new AppendProcessor.Factory(TestTemplateService.instance());
+    }
+
+    public void testCreate() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        Object value;
+        if (randomBoolean()) {
+            value = "value1";
+        } else {
+            value = Arrays.asList("value1", "value2", "value3");
+        }
+        config.put("value", value);
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        AppendProcessor appendProcessor = factory.create(config);
+        assertThat(appendProcessor.getTag(), equalTo(processorTag));
+        assertThat(appendProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
+        assertThat(appendProcessor.getValue().copyAndResolve(Collections.emptyMap()), equalTo(value));
+    }
+
+    public void testCreateNoFieldPresent() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("value", "value1");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+
+    public void testCreateNoValuePresent() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
+        }
+    }
+
+    public void testCreateNullValue() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        config.put("value", null);
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java
new file mode 100644
index 0000000..4a78ba6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/AppendProcessorTests.java
@@ -0,0 +1,209 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.ingest.core.ValueSource;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.not;
+import static org.hamcrest.CoreMatchers.sameInstance;
+
+public class AppendProcessorTests extends ESTestCase {
+
+    public void testAppendValuesToExistingList() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        Scalar scalar = randomFrom(Scalar.values());
+        List<Object> list = new ArrayList<>();
+        int size = randomIntBetween(0, 10);
+        for (int i = 0; i < size; i++) {
+            list.add(scalar.randomValue());
+        }
+        List<Object> checkList = new ArrayList<>(list);
+        String field = RandomDocumentPicks.addRandomField(random(), ingestDocument, list);
+        List<Object> values = new ArrayList<>();
+        Processor appendProcessor;
+        if (randomBoolean()) {
+            Object value = scalar.randomValue();
+            values.add(value);
+            appendProcessor = createAppendProcessor(field, value);
+        } else {
+            int valuesSize = randomIntBetween(0, 10);
+            for (int i = 0; i < valuesSize; i++) {
+                values.add(scalar.randomValue());
+            }
+            appendProcessor = createAppendProcessor(field, values);
+        }
+        appendProcessor.execute(ingestDocument);
+        Object fieldValue = ingestDocument.getFieldValue(field, Object.class);
+        assertThat(fieldValue, sameInstance(list));
+        assertThat(list.size(), equalTo(size + values.size()));
+        for (int i = 0; i < size; i++) {
+            assertThat(list.get(i), equalTo(checkList.get(i)));
+        }
+        for (int i = size; i < size + values.size(); i++) {
+            assertThat(list.get(i), equalTo(values.get(i - size)));
+        }
+    }
+
+    public void testAppendValuesToNonExistingList() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String field = RandomDocumentPicks.randomFieldName(random());
+        Scalar scalar = randomFrom(Scalar.values());
+        List<Object> values = new ArrayList<>();
+        Processor appendProcessor;
+        if (randomBoolean()) {
+            Object value = scalar.randomValue();
+            values.add(value);
+            appendProcessor = createAppendProcessor(field, value);
+        } else {
+            int valuesSize = randomIntBetween(0, 10);
+            for (int i = 0; i < valuesSize; i++) {
+                values.add(scalar.randomValue());
+            }
+            appendProcessor = createAppendProcessor(field, values);
+        }
+        appendProcessor.execute(ingestDocument);
+        List list = ingestDocument.getFieldValue(field, List.class);
+        assertThat(list, not(sameInstance(values)));
+        assertThat(list, equalTo(values));
+    }
+
+    public void testConvertScalarToList() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        Scalar scalar = randomFrom(Scalar.values());
+        Object initialValue = scalar.randomValue();
+        String field = RandomDocumentPicks.addRandomField(random(), ingestDocument, initialValue);
+        List<Object> values = new ArrayList<>();
+        Processor appendProcessor;
+        if (randomBoolean()) {
+            Object value = scalar.randomValue();
+            values.add(value);
+            appendProcessor = createAppendProcessor(field, value);
+        } else {
+            int valuesSize = randomIntBetween(0, 10);
+            for (int i = 0; i < valuesSize; i++) {
+                values.add(scalar.randomValue());
+            }
+            appendProcessor = createAppendProcessor(field, values);
+        }
+        appendProcessor.execute(ingestDocument);
+        List fieldValue = ingestDocument.getFieldValue(field, List.class);
+        assertThat(fieldValue.size(), equalTo(values.size() + 1));
+        assertThat(fieldValue.get(0), equalTo(initialValue));
+        for (int i = 1; i < values.size() + 1; i++) {
+            assertThat(fieldValue.get(i), equalTo(values.get(i - 1)));
+        }
+    }
+
+    public void testAppendMetadata() throws Exception {
+        //here any metadata field value becomes a list, which won't make sense in most of the cases,
+        // but support for append is streamlined like for set so we test it
+        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
+        List<String> values = new ArrayList<>();
+        Processor appendProcessor;
+        if (randomBoolean()) {
+            String value = randomAsciiOfLengthBetween(1, 10);
+            values.add(value);
+            appendProcessor = createAppendProcessor(randomMetaData.getFieldName(), value);
+        } else {
+            int valuesSize = randomIntBetween(0, 10);
+            for (int i = 0; i < valuesSize; i++) {
+                values.add(randomAsciiOfLengthBetween(1, 10));
+            }
+            appendProcessor = createAppendProcessor(randomMetaData.getFieldName(), values);
+        }
+
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        Object initialValue = ingestDocument.getSourceAndMetadata().get(randomMetaData.getFieldName());
+        appendProcessor.execute(ingestDocument);
+        List list = ingestDocument.getFieldValue(randomMetaData.getFieldName(), List.class);
+        if (initialValue == null) {
+            assertThat(list, equalTo(values));
+        } else {
+            assertThat(list.size(), equalTo(values.size() + 1));
+            assertThat(list.get(0), equalTo(initialValue));
+            for (int i = 1; i < list.size(); i++) {
+                assertThat(list.get(i), equalTo(values.get(i - 1)));
+            }
+        }
+    }
+
+    private static Processor createAppendProcessor(String fieldName, Object fieldValue) {
+        TemplateService templateService = TestTemplateService.instance();
+        return new AppendProcessor(randomAsciiOfLength(10), templateService.compile(fieldName), ValueSource.wrap(fieldValue, templateService));
+    }
+
+    private enum Scalar {
+        INTEGER {
+            @Override
+            Object randomValue() {
+                return randomInt();
+            }
+        }, DOUBLE {
+            @Override
+            Object randomValue() {
+                return randomDouble();
+            }
+        }, FLOAT {
+            @Override
+            Object randomValue() {
+                return randomFloat();
+            }
+        }, BOOLEAN {
+            @Override
+            Object randomValue() {
+                return randomBoolean();
+            }
+        }, STRING {
+            @Override
+            Object randomValue() {
+                return randomAsciiOfLengthBetween(1, 10);
+            }
+        }, MAP {
+            @Override
+            Object randomValue() {
+                int numItems = randomIntBetween(1, 10);
+                Map<String, Object> map = new HashMap<>(numItems);
+                for (int i = 0; i < numItems; i++) {
+                    map.put(randomAsciiOfLengthBetween(1, 10), randomFrom(Scalar.values()).randomValue());
+                }
+                return map;
+            }
+        }, NULL {
+            @Override
+            Object randomValue() {
+                return null;
+            }
+        };
+
+        abstract Object randomValue();
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
new file mode 100644
index 0000000..7064331
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorFactoryTests.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.Matchers;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class ConvertProcessorFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        ConvertProcessor.Type type = randomFrom(ConvertProcessor.Type.values());
+        config.put("field", "field1");
+        config.put("type", type.toString());
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        ConvertProcessor convertProcessor = factory.create(config);
+        assertThat(convertProcessor.getTag(), equalTo(processorTag));
+        assertThat(convertProcessor.getField(), equalTo("field1"));
+        assertThat(convertProcessor.getConvertType(), equalTo(type));
+    }
+
+    public void testCreateUnsupportedType() throws Exception {
+        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String type = "type-" + randomAsciiOfLengthBetween(1, 10);
+        config.put("field", "field1");
+        config.put("type", type);
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), Matchers.equalTo("type [" + type + "] not supported, cannot convert field."));
+        }
+    }
+
+    public void testCreateNoFieldPresent() throws Exception {
+        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String type = "type-" + randomAsciiOfLengthBetween(1, 10);
+        config.put("type", type);
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), Matchers.equalTo("required property [field] is missing"));
+        }
+    }
+
+    public void testCreateNoTypePresent() throws Exception {
+        ConvertProcessor.Factory factory = new ConvertProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), Matchers.equalTo("required property [type] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java
new file mode 100644
index 0000000..1350eba
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/ConvertProcessorTests.java
@@ -0,0 +1,268 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+
+import static org.elasticsearch.ingest.processor.ConvertProcessor.Type;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+
+public class ConvertProcessorTests extends ESTestCase {
+
+    public void testConvertInt() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        int randomInt = randomInt();
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, randomInt);
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.INTEGER);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, Integer.class), equalTo(randomInt));
+    }
+
+    public void testConvertIntList() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        int numItems = randomIntBetween(1, 10);
+        List<String> fieldValue = new ArrayList<>();
+        List<Integer> expectedList = new ArrayList<>();
+        for (int j = 0; j < numItems; j++) {
+            int randomInt = randomInt();
+            fieldValue.add(Integer.toString(randomInt));
+            expectedList.add(randomInt);
+        }
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.INTEGER);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
+    }
+
+    public void testConvertIntError() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        String value = "string-" + randomAsciiOfLengthBetween(1, 10);
+        ingestDocument.setFieldValue(fieldName, value);
+
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.INTEGER);
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("unable to convert [" + value + "] to integer"));
+        }
+    }
+
+    public void testConvertFloat() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        Map<String, Float> expectedResult = new HashMap<>();
+        float randomFloat = randomFloat();
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, randomFloat);
+        expectedResult.put(fieldName, randomFloat);
+
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.FLOAT);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, Float.class), equalTo(randomFloat));
+    }
+
+    public void testConvertFloatList() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        int numItems = randomIntBetween(1, 10);
+        List<String> fieldValue = new ArrayList<>();
+        List<Float> expectedList = new ArrayList<>();
+        for (int j = 0; j < numItems; j++) {
+            float randomFloat = randomFloat();
+            fieldValue.add(Float.toString(randomFloat));
+            expectedList.add(randomFloat);
+        }
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.FLOAT);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
+    }
+
+    public void testConvertFloatError() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        String value = "string-" + randomAsciiOfLengthBetween(1, 10);
+        ingestDocument.setFieldValue(fieldName, value);
+
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.FLOAT);
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("unable to convert [" + value + "] to float"));
+        }
+    }
+
+    public void testConvertBoolean() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        boolean randomBoolean = randomBoolean();
+        String booleanString = Boolean.toString(randomBoolean);
+        if (randomBoolean) {
+            booleanString = booleanString.toUpperCase(Locale.ROOT);
+        }
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, booleanString);
+
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.BOOLEAN);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, Boolean.class), equalTo(randomBoolean));
+    }
+
+    public void testConvertBooleanList() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        int numItems = randomIntBetween(1, 10);
+        List<String> fieldValue = new ArrayList<>();
+        List<Boolean> expectedList = new ArrayList<>();
+        for (int j = 0; j < numItems; j++) {
+            boolean randomBoolean = randomBoolean();
+            String booleanString = Boolean.toString(randomBoolean);
+            if (randomBoolean) {
+                booleanString = booleanString.toUpperCase(Locale.ROOT);
+            }
+            fieldValue.add(booleanString);
+            expectedList.add(randomBoolean);
+        }
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.BOOLEAN);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
+    }
+
+    public void testConvertBooleanError() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        String fieldValue;
+        if (randomBoolean()) {
+            fieldValue = "string-" + randomAsciiOfLengthBetween(1, 10);
+        } else {
+            //verify that only proper boolean values are supported and we are strict about it
+            fieldValue = randomFrom("on", "off", "yes", "no", "0", "1");
+        }
+        ingestDocument.setFieldValue(fieldName, fieldValue);
+
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.BOOLEAN);
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch(Exception e) {
+            assertThat(e.getMessage(), equalTo("[" + fieldValue + "] is not a boolean value, cannot convert to boolean"));
+        }
+    }
+
+    public void testConvertString() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        Object fieldValue;
+        String expectedFieldValue;
+        switch(randomIntBetween(0, 2)) {
+            case 0:
+                float randomFloat = randomFloat();
+                fieldValue = randomFloat;
+                expectedFieldValue = Float.toString(randomFloat);
+                break;
+            case 1:
+                int randomInt = randomInt();
+                fieldValue = randomInt;
+                expectedFieldValue = Integer.toString(randomInt);
+                break;
+            case 2:
+                boolean randomBoolean = randomBoolean();
+                fieldValue = randomBoolean;
+                expectedFieldValue = Boolean.toString(randomBoolean);
+                break;
+            default:
+                throw new UnsupportedOperationException();
+        }
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
+
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.STRING);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedFieldValue));
+    }
+
+    public void testConvertStringList() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        int numItems = randomIntBetween(1, 10);
+        List<Object> fieldValue = new ArrayList<>();
+        List<String> expectedList = new ArrayList<>();
+        for (int j = 0; j < numItems; j++) {
+            Object randomValue;
+            String randomValueString;
+            switch(randomIntBetween(0, 2)) {
+                case 0:
+                    float randomFloat = randomFloat();
+                    randomValue = randomFloat;
+                    randomValueString = Float.toString(randomFloat);
+                    break;
+                case 1:
+                    int randomInt = randomInt();
+                    randomValue = randomInt;
+                    randomValueString = Integer.toString(randomInt);
+                    break;
+                case 2:
+                    boolean randomBoolean = randomBoolean();
+                    randomValue = randomBoolean;
+                    randomValueString = Boolean.toString(randomBoolean);
+                    break;
+                default:
+                    throw new UnsupportedOperationException();
+            }
+            fieldValue.add(randomValue);
+            expectedList.add(randomValueString);
+        }
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, Type.STRING);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(expectedList));
+    }
+
+    public void testConvertNonExistingField() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        Type type = randomFrom(Type.values());
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), fieldName, type);
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
+        }
+    }
+
+    public void testConvertNullField() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
+        Type type = randomFrom(Type.values());
+        Processor processor = new ConvertProcessor(randomAsciiOfLength(10), "field", type);
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("Field [field] is null, cannot be converted to type [" + type + "]"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java
new file mode 100644
index 0000000..401dd44
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/DateFormatTests.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.test.ESTestCase;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.format.DateTimeFormatter;
+import java.util.Locale;
+import java.util.function.Function;
+
+import static org.hamcrest.core.IsEqual.equalTo;
+
+public class DateFormatTests extends ESTestCase {
+
+    public void testParseJoda() {
+        Function<String, DateTime> jodaFunction = DateFormat.Joda.getFunction("MMM dd HH:mm:ss Z", DateTimeZone.forOffsetHours(-8), Locale.ENGLISH);
+        assertThat(Instant.ofEpochMilli(jodaFunction.apply("Nov 24 01:29:01 -0800").getMillis())
+                        .atZone(ZoneId.of("GMT-8"))
+                        .format(DateTimeFormatter.ofPattern("MM dd HH:mm:ss", Locale.ENGLISH)),
+                equalTo("11 24 01:29:01"));
+    }
+
+    public void testParseUnixMs() {
+        assertThat(DateFormat.UnixMs.getFunction(null, DateTimeZone.UTC, null).apply("1000500").getMillis(), equalTo(1000500L));
+    }
+
+    public void testParseUnix() {
+        assertThat(DateFormat.Unix.getFunction(null, DateTimeZone.UTC, null).apply("1000.5").getMillis(), equalTo(1000500L));
+    }
+
+    public void testParseISO8601() {
+        assertThat(DateFormat.Iso8601.getFunction(null, DateTimeZone.UTC, null).apply("2001-01-01T00:00:00-0800").getMillis(), equalTo(978336000000L));
+    }
+
+    public void testParseISO8601Failure() {
+        Function<String, DateTime> function = DateFormat.Iso8601.getFunction(null, DateTimeZone.UTC, null);
+        try {
+            function.apply("2001-01-0:00-0800");
+            fail("parse should have failed");
+        } catch(IllegalArgumentException e) {
+            //all good
+        }
+    }
+
+    public void testTAI64NParse() {
+        String input = "4000000050d506482dbdf024";
+        String expected = "2012-12-22T03:00:46.767+02:00";
+        assertThat(DateFormat.Tai64n.getFunction(null, DateTimeZone.forOffsetHours(2), null).apply((randomBoolean() ? "@" : "") + input).toString(), equalTo(expected));
+    }
+
+    public void testFromString() {
+        assertThat(DateFormat.fromString("UNIX_MS"), equalTo(DateFormat.UnixMs));
+        assertThat(DateFormat.fromString("unix_ms"), equalTo(DateFormat.Joda));
+        assertThat(DateFormat.fromString("UNIX"), equalTo(DateFormat.Unix));
+        assertThat(DateFormat.fromString("unix"), equalTo(DateFormat.Joda));
+        assertThat(DateFormat.fromString("ISO8601"), equalTo(DateFormat.Iso8601));
+        assertThat(DateFormat.fromString("iso8601"), equalTo(DateFormat.Joda));
+        assertThat(DateFormat.fromString("TAI64N"), equalTo(DateFormat.Tai64n));
+        assertThat(DateFormat.fromString("tai64n"), equalTo(DateFormat.Joda));
+        assertThat(DateFormat.fromString("prefix-" + randomAsciiOfLengthBetween(1, 10)), equalTo(DateFormat.Joda));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
new file mode 100644
index 0000000..a145a7c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorFactoryTests.java
@@ -0,0 +1,189 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.joda.time.DateTimeZone;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+
+public class DateProcessorFactoryTests extends ESTestCase {
+
+    public void testBuildDefaults() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        DateProcessor processor = factory.create(config);
+        assertThat(processor.getTag(), equalTo(processorTag));
+        assertThat(processor.getMatchField(), equalTo(sourceField));
+        assertThat(processor.getTargetField(), equalTo(DateProcessor.DEFAULT_TARGET_FIELD));
+        assertThat(processor.getMatchFormats(), equalTo(Collections.singletonList("dd/MM/yyyyy")));
+        assertThat(processor.getLocale(), equalTo(Locale.ENGLISH));
+        assertThat(processor.getTimezone(), equalTo(DateTimeZone.UTC));
+    }
+
+    public void testMatchFieldIsMandatory() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String targetField = randomAsciiOfLengthBetween(1, 10);
+        config.put("target_field", targetField);
+        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
+
+        try {
+            factory.create(config);
+            fail("processor creation should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("required property [match_field] is missing"));
+        }
+    }
+
+    public void testMatchFormatsIsMandatory() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        String targetField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("target_field", targetField);
+
+        try {
+            factory.create(config);
+            fail("processor creation should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("required property [match_formats] is missing"));
+        }
+    }
+
+    public void testParseLocale() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
+        Locale locale = randomLocale(random());
+        config.put("locale", locale.toLanguageTag());
+
+        DateProcessor processor = factory.create(config);
+        assertThat(processor.getLocale().toLanguageTag(), equalTo(locale.toLanguageTag()));
+    }
+
+    public void testParseInvalidLocale() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
+        config.put("locale", "invalid_locale");
+        try {
+            factory.create(config);
+            fail("should fail with invalid locale");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("Invalid language tag specified: invalid_locale"));
+        }
+    }
+
+    public void testParseTimezone() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
+
+        DateTimeZone timezone = randomTimezone();
+        config.put("timezone", timezone.getID());
+        DateProcessor processor = factory.create(config);
+        assertThat(processor.getTimezone(), equalTo(timezone));
+    }
+
+    public void testParseInvalidTimezone() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("match_formats", Collections.singletonList("dd/MM/yyyyy"));
+        config.put("timezone", "invalid_timezone");
+        try {
+            factory.create(config);
+            fail("invalid timezone should fail");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("The datetime zone id 'invalid_timezone' is not recognised"));
+        }
+    }
+
+    //we generate a timezone out of the available ones in joda, some available in the jdk are not available in joda by default
+    private static DateTimeZone randomTimezone() {
+        List<String> ids = new ArrayList<>(DateTimeZone.getAvailableIDs());
+        Collections.sort(ids);
+        return DateTimeZone.forID(randomFrom(ids));
+    }
+
+
+    public void testParseMatchFormats() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("match_formats", Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy"));
+
+        DateProcessor processor = factory.create(config);
+        assertThat(processor.getMatchFormats(), equalTo(Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy")));
+    }
+
+    public void testParseMatchFormatsFailure() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("match_formats", "dd/MM/yyyy");
+
+        try {
+            factory.create(config);
+            fail("processor creation should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("property [match_formats] isn't a list, but of type [java.lang.String]"));
+        }
+    }
+
+    public void testParseTargetField() throws Exception {
+        DateProcessor.Factory factory = new DateProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        String sourceField = randomAsciiOfLengthBetween(1, 10);
+        String targetField = randomAsciiOfLengthBetween(1, 10);
+        config.put("match_field", sourceField);
+        config.put("target_field", targetField);
+        config.put("match_formats", Arrays.asList("dd/MM/yyyy", "dd-MM-yyyy"));
+
+        DateProcessor processor = factory.create(config);
+        assertThat(processor.getTargetField(), equalTo(targetField));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java
new file mode 100644
index 0000000..5daab95
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/DateProcessorTests.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.test.ESTestCase;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.containsString;
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class DateProcessorTests extends ESTestCase {
+
+    public void testJodaPattern() {
+        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
+                "date_as_string", Collections.singletonList("yyyy dd MM hh:mm:ss"), "date_as_date");
+        Map<String, Object> document = new HashMap<>();
+        document.put("date_as_string", "2010 12 06 11:05:15");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T11:05:15.000+02:00"));
+    }
+
+    public void testJodaPatternMultipleFormats() {
+        List<String> matchFormats = new ArrayList<>();
+        matchFormats.add("yyyy dd MM");
+        matchFormats.add("dd/MM/yyyy");
+        matchFormats.add("dd-MM-yyyy");
+        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
+                "date_as_string", matchFormats, "date_as_date");
+
+        Map<String, Object> document = new HashMap<>();
+        document.put("date_as_string", "2010 12 06");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
+
+        document = new HashMap<>();
+        document.put("date_as_string", "12/06/2010");
+        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
+
+        document = new HashMap<>();
+        document.put("date_as_string", "12-06-2010");
+        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
+
+        document = new HashMap<>();
+        document.put("date_as_string", "2010");
+        ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        try {
+            dateProcessor.execute(ingestDocument);
+            fail("processor should have failed due to not supported date format");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("unable to parse date [2010]"));
+        }
+    }
+
+    public void testInvalidJodaPattern() {
+        try {
+            new DateProcessor(randomAsciiOfLength(10), DateTimeZone.UTC, randomLocale(random()),
+                "date_as_string", Collections.singletonList("invalid pattern"), "date_as_date");
+            fail("date processor initialization should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("Illegal pattern component: i"));
+        }
+    }
+
+    public void testJodaPatternLocale() {
+        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forID("Europe/Amsterdam"), Locale.ITALIAN,
+                "date_as_string", Collections.singletonList("yyyy dd MMM"), "date_as_date");
+        Map<String, Object> document = new HashMap<>();
+        document.put("date_as_string", "2010 12 giugno");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2010-06-12T00:00:00.000+02:00"));
+    }
+
+    public void testJodaPatternDefaultYear() {
+        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forID("Europe/Amsterdam"), Locale.ENGLISH,
+                "date_as_string", Collections.singletonList("dd/MM"), "date_as_date");
+        Map<String, Object> document = new HashMap<>();
+        document.put("date_as_string", "12/06");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo(DateTime.now().getYear() + "-06-12T00:00:00.000+02:00"));
+    }
+
+    public void testTAI64N() {
+        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.forOffsetHours(2), randomLocale(random()),
+                "date_as_string", Collections.singletonList("TAI64N"), "date_as_date");
+        Map<String, Object> document = new HashMap<>();
+        String dateAsString = (randomBoolean() ? "@" : "") + "4000000050d506482dbdf024";
+        document.put("date_as_string", dateAsString);
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("2012-12-22T03:00:46.767+02:00"));
+    }
+
+    public void testUnixMs() {
+        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.UTC, randomLocale(random()),
+                "date_as_string", Collections.singletonList("UNIX_MS"), "date_as_date");
+        Map<String, Object> document = new HashMap<>();
+        document.put("date_as_string", "1000500");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("1970-01-01T00:16:40.500Z"));
+    }
+
+    public void testUnix() {
+        DateProcessor dateProcessor = new DateProcessor(randomAsciiOfLength(10), DateTimeZone.UTC, randomLocale(random()),
+                "date_as_string", Collections.singletonList("UNIX"), "date_as_date");
+        Map<String, Object> document = new HashMap<>();
+        document.put("date_as_string", "1000.5");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        dateProcessor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("date_as_date", String.class), equalTo("1970-01-01T00:16:40.500Z"));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java
new file mode 100644
index 0000000..63eee56
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorFactoryTests.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class DeDotProcessorFactoryTests extends ESTestCase {
+
+    private DeDotProcessor.Factory factory;
+
+    @Before
+    public void init() {
+        factory = new DeDotProcessor.Factory();
+    }
+
+    public void testCreate() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("separator", "_");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        DeDotProcessor deDotProcessor = factory.create(config);
+        assertThat(deDotProcessor.getSeparator(), equalTo("_"));
+        assertThat(deDotProcessor.getTag(), equalTo(processorTag));
+    }
+
+    public void testCreateMissingSeparatorField() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        DeDotProcessor deDotProcessor = factory.create(config);
+        assertThat(deDotProcessor.getSeparator(), equalTo(DeDotProcessor.DEFAULT_SEPARATOR));
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java
new file mode 100644
index 0000000..a0c87d7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/DeDotProcessorTests.java
@@ -0,0 +1,75 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class DeDotProcessorTests extends ESTestCase {
+
+    public void testSimple() throws Exception {
+        Map<String, Object> source = new HashMap<>();
+        source.put("a.b", "hello world!");
+        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
+        String separator = randomUnicodeOfCodepointLengthBetween(1, 10);
+        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), separator);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getSourceAndMetadata().get("a" + separator + "b" ), equalTo("hello world!"));
+    }
+
+    public void testSimpleMap() throws Exception {
+        Map<String, Object> source = new HashMap<>();
+        Map<String, Object> subField = new HashMap<>();
+        subField.put("b.c", "hello world!");
+        source.put("a", subField);
+        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
+        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), "_");
+        processor.execute(ingestDocument);
+
+        IngestDocument expectedDocument = new IngestDocument(
+            Collections.singletonMap("a", Collections.singletonMap("b_c", "hello world!")),
+            Collections.emptyMap());
+        assertThat(ingestDocument, equalTo(expectedDocument));
+    }
+
+    public void testSimpleList() throws Exception {
+        Map<String, Object> source = new HashMap<>();
+        Map<String, Object> subField = new HashMap<>();
+        subField.put("b.c", "hello world!");
+        source.put("a", Arrays.asList(subField));
+        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
+        Processor processor = new DeDotProcessor(randomAsciiOfLength(10), "_");
+        processor.execute(ingestDocument);
+
+        IngestDocument expectedDocument = new IngestDocument(
+            Collections.singletonMap("a",
+                Collections.singletonList(Collections.singletonMap("b_c", "hello world!"))),
+            Collections.emptyMap());
+        assertThat(ingestDocument, equalTo(expectedDocument));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
new file mode 100644
index 0000000..993c7cc
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorFactoryTests.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class FailProcessorFactoryTests extends ESTestCase {
+
+    private FailProcessor.Factory factory;
+
+    @Before
+    public void init() {
+        factory = new FailProcessor.Factory(TestTemplateService.instance());
+    }
+
+    public void testCreate() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("message", "error");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        FailProcessor failProcessor = factory.create(config);
+        assertThat(failProcessor.getTag(), equalTo(processorTag));
+        assertThat(failProcessor.getMessage().execute(Collections.emptyMap()), equalTo("error"));
+    }
+
+    public void testCreateMissingMessageField() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [message] is missing"));
+        }
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java
new file mode 100644
index 0000000..3fdc207
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/FailProcessorTests.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.test.ESTestCase;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class FailProcessorTests extends ESTestCase {
+
+    public void test() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        String message = randomAsciiOfLength(10);
+        Processor processor = new FailProcessor(randomAsciiOfLength(10), new TestTemplateService.MockTemplate(message));
+        try {
+            processor.execute(ingestDocument);
+            fail("fail processor should throw an exception");
+        } catch (FailProcessorException e) {
+            assertThat(e.getMessage(), equalTo(message));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
new file mode 100644
index 0000000..fd62f6c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorFactoryTests.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class GsubProcessorFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        GsubProcessor.Factory factory = new GsubProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        config.put("pattern", "\\.");
+        config.put("replacement", "-");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        GsubProcessor gsubProcessor = factory.create(config);
+        assertThat(gsubProcessor.getTag(), equalTo(processorTag));
+        assertThat(gsubProcessor.getField(), equalTo("field1"));
+        assertThat(gsubProcessor.getPattern().toString(), equalTo("\\."));
+        assertThat(gsubProcessor.getReplacement(), equalTo("-"));
+    }
+
+    public void testCreateNoFieldPresent() throws Exception {
+        GsubProcessor.Factory factory = new GsubProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("pattern", "\\.");
+        config.put("replacement", "-");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+
+    public void testCreateNoPatternPresent() throws Exception {
+        GsubProcessor.Factory factory = new GsubProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        config.put("replacement", "-");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [pattern] is missing"));
+        }
+    }
+
+    public void testCreateNoReplacementPresent() throws Exception {
+        GsubProcessor.Factory factory = new GsubProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        config.put("pattern", "\\.");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [replacement] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java
new file mode 100644
index 0000000..fe44f33
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/GsubProcessorTests.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.regex.Pattern;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+
+public class GsubProcessorTests extends ESTestCase {
+
+    public void testGsub() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, "127.0.0.1");
+        Processor processor = new GsubProcessor(randomAsciiOfLength(10), fieldName, Pattern.compile("\\."), "-");
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo("127-0-0-1"));
+    }
+
+    public void testGsubNotAStringValue() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        ingestDocument.setFieldValue(fieldName, 123);
+        Processor processor = new GsubProcessor(randomAsciiOfLength(10), fieldName, Pattern.compile("\\."), "-");
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execution should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
+        }
+    }
+
+    public void testGsubFieldNotFound() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        Processor processor = new GsubProcessor(randomAsciiOfLength(10), fieldName, Pattern.compile("\\."), "-");
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execution should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
+        }
+    }
+
+    public void testGsubNullValue() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
+        Processor processor = new GsubProcessor(randomAsciiOfLength(10), "field", Pattern.compile("\\."), "-");
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execution should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [field] is null, cannot match pattern."));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
new file mode 100644
index 0000000..2af2b09
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorFactoryTests.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class JoinProcessorFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        JoinProcessor.Factory factory = new JoinProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        config.put("separator", "-");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        JoinProcessor joinProcessor = factory.create(config);
+        assertThat(joinProcessor.getTag(), equalTo(processorTag));
+        assertThat(joinProcessor.getField(), equalTo("field1"));
+        assertThat(joinProcessor.getSeparator(), equalTo("-"));
+    }
+
+    public void testCreateNoFieldPresent() throws Exception {
+        JoinProcessor.Factory factory = new JoinProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("separator", "-");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+
+    public void testCreateNoSeparatorPresent() throws Exception {
+        JoinProcessor.Factory factory = new JoinProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [separator] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java
new file mode 100644
index 0000000..2aa3ac2
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/JoinProcessorTests.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+
+public class JoinProcessorTests extends ESTestCase {
+
+    private static final String[] SEPARATORS = new String[]{"-", "_", "."};
+
+    public void testJoinStrings() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        int numItems = randomIntBetween(1, 10);
+        String separator = randomFrom(SEPARATORS);
+        List<String> fieldValue = new ArrayList<>(numItems);
+        String expectedResult = "";
+        for (int j = 0; j < numItems; j++) {
+            String value = randomAsciiOfLengthBetween(1, 10);
+            fieldValue.add(value);
+            expectedResult += value;
+            if (j < numItems - 1) {
+                expectedResult += separator;
+            }
+        }
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
+        Processor processor = new JoinProcessor(randomAsciiOfLength(10), fieldName, separator);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult));
+    }
+
+    public void testJoinIntegers() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        int numItems = randomIntBetween(1, 10);
+        String separator = randomFrom(SEPARATORS);
+        List<Integer> fieldValue = new ArrayList<>(numItems);
+        String expectedResult = "";
+        for (int j = 0; j < numItems; j++) {
+            int value = randomInt();
+            fieldValue.add(value);
+            expectedResult += value;
+            if (j < numItems - 1) {
+                expectedResult += separator;
+            }
+        }
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, fieldValue);
+        Processor processor = new JoinProcessor(randomAsciiOfLength(10), fieldName, separator);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, String.class), equalTo(expectedResult));
+    }
+
+    public void testJoinNonListField() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        ingestDocument.setFieldValue(fieldName, randomAsciiOfLengthBetween(1, 10));
+        Processor processor = new JoinProcessor(randomAsciiOfLength(10), fieldName, "-");
+        try {
+            processor.execute(ingestDocument);
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.String] cannot be cast to [java.util.List]"));
+        }
+    }
+
+    public void testJoinNonExistingField() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        Processor processor = new JoinProcessor(randomAsciiOfLength(10), fieldName, "-");
+        try {
+            processor.execute(ingestDocument);
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
+        }
+    }
+
+    public void testJoinNullValue() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
+        Processor processor = new JoinProcessor(randomAsciiOfLength(10), "field", "-");
+        try {
+            processor.execute(ingestDocument);
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [field] is null, cannot join."));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
new file mode 100644
index 0000000..6a4a67e
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorFactoryTests.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class LowercaseProcessorFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        LowercaseProcessor.Factory factory = new LowercaseProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        LowercaseProcessor uppercaseProcessor = factory.create(config);
+        assertThat(uppercaseProcessor.getTag(), equalTo(processorTag));
+        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
+    }
+
+    public void testCreateMissingField() throws Exception {
+        LowercaseProcessor.Factory factory = new LowercaseProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java
new file mode 100644
index 0000000..77e22b0
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/LowercaseProcessorTests.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import java.util.Locale;
+
+public class LowercaseProcessorTests extends AbstractStringProcessorTestCase {
+    @Override
+    protected AbstractStringProcessor newProcessor(String field) {
+        return new LowercaseProcessor(randomAsciiOfLength(10), field);
+    }
+
+    @Override
+    protected String expectedResult(String input) {
+        return input.toLowerCase(Locale.ROOT);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
new file mode 100644
index 0000000..0b03150
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorFactoryTests.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class RemoveProcessorFactoryTests extends ESTestCase {
+
+    private RemoveProcessor.Factory factory;
+
+    @Before
+    public void init() {
+        factory = new RemoveProcessor.Factory(TestTemplateService.instance());
+    }
+
+    public void testCreate() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        RemoveProcessor removeProcessor = factory.create(config);
+        assertThat(removeProcessor.getTag(), equalTo(processorTag));
+        assertThat(removeProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
+    }
+
+    public void testCreateMissingField() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java
new file mode 100644
index 0000000..d134b02
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/RemoveProcessorTests.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.HashMap;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+
+public class RemoveProcessorTests extends ESTestCase {
+
+    public void testRemoveFields() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        String field = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
+        Processor processor = new RemoveProcessor(randomAsciiOfLength(10), new TestTemplateService.MockTemplate(field));
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.hasField(field), equalTo(false));
+    }
+
+    public void testRemoveNonExistingField() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        Processor processor = new RemoveProcessor(randomAsciiOfLength(10), new TestTemplateService.MockTemplate(fieldName));
+        try {
+            processor.execute(ingestDocument);
+            fail("remove field should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
new file mode 100644
index 0000000..21f5c66
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorFactoryTests.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class RenameProcessorFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        RenameProcessor.Factory factory = new RenameProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "old_field");
+        config.put("to", "new_field");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        RenameProcessor renameProcessor = factory.create(config);
+        assertThat(renameProcessor.getTag(), equalTo(processorTag));
+        assertThat(renameProcessor.getOldFieldName(), equalTo("old_field"));
+        assertThat(renameProcessor.getNewFieldName(), equalTo("new_field"));
+    }
+
+    public void testCreateNoFieldPresent() throws Exception {
+        RenameProcessor.Factory factory = new RenameProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("to", "new_field");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+
+    public void testCreateNoToPresent() throws Exception {
+        RenameProcessor.Factory factory = new RenameProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "old_field");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [to] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java
new file mode 100644
index 0000000..1f9bdda
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/RenameProcessorTests.java
@@ -0,0 +1,173 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.nullValue;
+
+public class RenameProcessorTests extends ESTestCase {
+
+    public void testRename() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
+        Object fieldValue = ingestDocument.getFieldValue(fieldName, Object.class);
+        String newFieldName;
+        do {
+            newFieldName = RandomDocumentPicks.randomFieldName(random());
+        } while (RandomDocumentPicks.canAddField(newFieldName, ingestDocument) == false || newFieldName.equals(fieldName));
+        Processor processor = new RenameProcessor(randomAsciiOfLength(10), fieldName, newFieldName);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(newFieldName, Object.class), equalTo(fieldValue));
+    }
+
+    public void testRenameArrayElement() throws Exception {
+        Map<String, Object> document = new HashMap<>();
+        List<String> list = new ArrayList<>();
+        list.add("item1");
+        list.add("item2");
+        list.add("item3");
+        document.put("list", list);
+        List<Map<String, String>> one = new ArrayList<>();
+        one.add(Collections.singletonMap("one", "one"));
+        one.add(Collections.singletonMap("two", "two"));
+        document.put("one", one);
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+
+        Processor processor = new RenameProcessor(randomAsciiOfLength(10), "list.0", "item");
+        processor.execute(ingestDocument);
+        Object actualObject = ingestDocument.getSourceAndMetadata().get("list");
+        assertThat(actualObject, instanceOf(List.class));
+        @SuppressWarnings("unchecked")
+        List<String> actualList = (List<String>) actualObject;
+        assertThat(actualList.size(), equalTo(2));
+        assertThat(actualList.get(0), equalTo("item2"));
+        assertThat(actualList.get(1), equalTo("item3"));
+        actualObject = ingestDocument.getSourceAndMetadata().get("item");
+        assertThat(actualObject, instanceOf(String.class));
+        assertThat(actualObject, equalTo("item1"));
+
+        processor = new RenameProcessor(randomAsciiOfLength(10), "list.0", "list.3");
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("[3] is out of bounds for array with length [2] as part of path [list.3]"));
+            assertThat(actualList.size(), equalTo(2));
+            assertThat(actualList.get(0), equalTo("item2"));
+            assertThat(actualList.get(1), equalTo("item3"));
+        }
+    }
+
+    public void testRenameNonExistingField() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        Processor processor = new RenameProcessor(randomAsciiOfLength(10), fieldName, RandomDocumentPicks.randomFieldName(random()));
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] doesn't exist"));
+        }
+    }
+
+    public void testRenameNewFieldAlreadyExists() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
+        Processor processor = new RenameProcessor(randomAsciiOfLength(10), RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument), fieldName);
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] already exists"));
+        }
+    }
+
+    public void testRenameExistingFieldNullValue() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        ingestDocument.setFieldValue(fieldName, null);
+        String newFieldName = RandomDocumentPicks.randomFieldName(random());
+        Processor processor = new RenameProcessor(randomAsciiOfLength(10), fieldName, newFieldName);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.hasField(fieldName), equalTo(false));
+        assertThat(ingestDocument.hasField(newFieldName), equalTo(true));
+        assertThat(ingestDocument.getFieldValue(newFieldName, Object.class), nullValue());
+    }
+
+    public void testRenameAtomicOperationSetFails() throws Exception {
+        Map<String, Object> source = new HashMap<String, Object>() {
+            @Override
+            public Object put(String key, Object value) {
+                if (key.equals("new_field")) {
+                    throw new UnsupportedOperationException();
+                }
+                return super.put(key, value);
+            }
+        };
+        source.put("list", Collections.singletonList("item"));
+
+        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
+        Processor processor = new RenameProcessor(randomAsciiOfLength(10), "list", "new_field");
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch(UnsupportedOperationException e) {
+            //the set failed, the old field has not been removed
+            assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
+            assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(false));
+        }
+    }
+
+    public void testRenameAtomicOperationRemoveFails() throws Exception {
+        Map<String, Object> source = new HashMap<String, Object>() {
+            @Override
+            public Object remove(Object key) {
+                if (key.equals("list")) {
+                    throw new UnsupportedOperationException();
+                }
+                return super.remove(key);
+            }
+        };
+        source.put("list", Collections.singletonList("item"));
+
+        IngestDocument ingestDocument = new IngestDocument(source, Collections.emptyMap());
+        Processor processor = new RenameProcessor(randomAsciiOfLength(10), "list", "new_field");
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch (UnsupportedOperationException e) {
+            //the set failed, the old field has not been removed
+            assertThat(ingestDocument.getSourceAndMetadata().containsKey("list"), equalTo(true));
+            assertThat(ingestDocument.getSourceAndMetadata().containsKey("new_field"), equalTo(false));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
new file mode 100644
index 0000000..a58ee49
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorFactoryTests.java
@@ -0,0 +1,88 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class SetProcessorFactoryTests extends ESTestCase {
+
+    private SetProcessor.Factory factory;
+
+    @Before
+    public void init() {
+        factory = new SetProcessor.Factory(TestTemplateService.instance());
+    }
+
+    public void testCreate() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        config.put("value", "value1");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        SetProcessor setProcessor = factory.create(config);
+        assertThat(setProcessor.getTag(), equalTo(processorTag));
+        assertThat(setProcessor.getField().execute(Collections.emptyMap()), equalTo("field1"));
+        assertThat(setProcessor.getValue().copyAndResolve(Collections.emptyMap()), equalTo("value1"));
+    }
+
+    public void testCreateNoFieldPresent() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("value", "value1");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+
+    public void testCreateNoValuePresent() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
+        }
+    }
+
+    public void testCreateNullValue() throws Exception {
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        config.put("value", null);
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [value] is missing"));
+        }
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java
new file mode 100644
index 0000000..283825c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/SetProcessorTests.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.ingest.TestTemplateService;
+import org.elasticsearch.ingest.core.ValueSource;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+import org.hamcrest.Matchers;
+
+import java.util.HashMap;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class SetProcessorTests extends ESTestCase {
+
+    public void testSetExistingFields() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        String fieldName = RandomDocumentPicks.randomExistingFieldName(random(), ingestDocument);
+        Object fieldValue = RandomDocumentPicks.randomFieldValue(random());
+        Processor processor = createSetProcessor(fieldName, fieldValue);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.hasField(fieldName), equalTo(true));
+        assertThat(ingestDocument.getFieldValue(fieldName, Object.class), equalTo(fieldValue));
+    }
+
+    public void testSetNewFields() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        //used to verify that there are no conflicts between subsequent fields going to be added
+        IngestDocument testIngestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        Object fieldValue = RandomDocumentPicks.randomFieldValue(random());
+        String fieldName = RandomDocumentPicks.addRandomField(random(), testIngestDocument, fieldValue);
+        Processor processor = createSetProcessor(fieldName, fieldValue);
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.hasField(fieldName), equalTo(true));
+        assertThat(ingestDocument.getFieldValue(fieldName, Object.class), equalTo(fieldValue));
+    }
+
+    public void testSetFieldsTypeMismatch() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        ingestDocument.setFieldValue("field", "value");
+        Processor processor = createSetProcessor("field.inner", "value");
+        try {
+            processor.execute(ingestDocument);
+            fail("processor execute should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("cannot set [inner] with parent object of type [java.lang.String] as part of path [field.inner]"));
+        }
+    }
+
+    public void testSetMetadata() throws Exception {
+        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
+        Processor processor = createSetProcessor(randomMetaData.getFieldName(), "_value");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class), Matchers.equalTo("_value"));
+    }
+
+    private static Processor createSetProcessor(String fieldName, Object fieldValue) {
+        TemplateService templateService = TestTemplateService.instance();
+        return new SetProcessor(randomAsciiOfLength(10), templateService.compile(fieldName), ValueSource.wrap(fieldValue, templateService));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
new file mode 100644
index 0000000..7267544
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorFactoryTests.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class SplitProcessorFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        SplitProcessor.Factory factory = new SplitProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        config.put("separator", "\\.");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        SplitProcessor splitProcessor = factory.create(config);
+        assertThat(splitProcessor.getTag(), equalTo(processorTag));
+        assertThat(splitProcessor.getField(), equalTo("field1"));
+        assertThat(splitProcessor.getSeparator(), equalTo("\\."));
+    }
+
+    public void testCreateNoFieldPresent() throws Exception {
+        SplitProcessor.Factory factory = new SplitProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("separator", "\\.");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+
+    public void testCreateNoSeparatorPresent() throws Exception {
+        SplitProcessor.Factory factory = new SplitProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [separator] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java
new file mode 100644
index 0000000..e1c8a62
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/SplitProcessorTests.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+
+public class SplitProcessorTests extends ESTestCase {
+
+    public void testSplit() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random());
+        String fieldName = RandomDocumentPicks.addRandomField(random(), ingestDocument, "127.0.0.1");
+        Processor processor = new SplitProcessor(randomAsciiOfLength(10), fieldName, "\\.");
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(fieldName, List.class), equalTo(Arrays.asList("127", "0", "0", "1")));
+    }
+
+    public void testSplitFieldNotFound() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        Processor processor = new SplitProcessor(randomAsciiOfLength(10), fieldName, "\\.");
+        try {
+            processor.execute(ingestDocument);
+            fail("split processor should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), containsString("not present as part of path [" + fieldName + "]"));
+        }
+    }
+
+    public void testSplitNullValue() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), Collections.singletonMap("field", null));
+        Processor processor = new SplitProcessor(randomAsciiOfLength(10), "field", "\\.");
+        try {
+            processor.execute(ingestDocument);
+            fail("split processor should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [field] is null, cannot split."));
+        }
+    }
+
+    public void testSplitNonStringValue() throws Exception {
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        ingestDocument.setFieldValue(fieldName, randomInt());
+        Processor processor = new SplitProcessor(randomAsciiOfLength(10), fieldName, "\\.");
+        try {
+            processor.execute(ingestDocument);
+            fail("split processor should have failed");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
+        }
+    }
+
+    public void testSplitAppendable() throws Exception {
+        Map<String, Object> splitConfig = new HashMap<>();
+        splitConfig.put("field", "flags");
+        splitConfig.put("separator", "\\|");
+        Processor splitProcessor = (new SplitProcessor.Factory()).create(splitConfig);
+        Map<String, Object> source = new HashMap<>();
+        source.put("flags", "new|hot|super|fun|interesting");
+        IngestDocument ingestDocument = new IngestDocument(source, new HashMap<>());
+        splitProcessor.execute(ingestDocument);
+        @SuppressWarnings("unchecked")
+        List<String> flags = (List<String>)ingestDocument.getFieldValue("flags", List.class);
+        assertThat(flags, equalTo(Arrays.asList("new", "hot", "super", "fun", "interesting")));
+        ingestDocument.appendFieldValue("flags", "additional_flag");
+        assertThat(ingestDocument.getFieldValue("flags", List.class), equalTo(Arrays.asList("new", "hot", "super", "fun", "interesting", "additional_flag")));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
new file mode 100644
index 0000000..350aaa6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorFactoryTests.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class TrimProcessorFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        TrimProcessor.Factory factory = new TrimProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        TrimProcessor uppercaseProcessor = factory.create(config);
+        assertThat(uppercaseProcessor.getTag(), equalTo(processorTag));
+        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
+    }
+
+    public void testCreateMissingField() throws Exception {
+        TrimProcessor.Factory factory = new TrimProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java
new file mode 100644
index 0000000..a0e5fde
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/TrimProcessorTests.java
@@ -0,0 +1,50 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+public class TrimProcessorTests extends AbstractStringProcessorTestCase {
+
+    @Override
+    protected AbstractStringProcessor newProcessor(String field) {
+        return new TrimProcessor(randomAsciiOfLength(10), field);
+    }
+
+    @Override
+    protected String modifyInput(String input) {
+        String updatedFieldValue = "";
+        updatedFieldValue = addWhitespaces(updatedFieldValue);
+        updatedFieldValue += input;
+        updatedFieldValue = addWhitespaces(updatedFieldValue);
+        return updatedFieldValue;
+    }
+
+    @Override
+    protected String expectedResult(String input) {
+        return input.trim();
+    }
+
+    private static String addWhitespaces(String input) {
+        int prefixLength = randomIntBetween(0, 10);
+        for (int i = 0; i < prefixLength; i++) {
+            input += ' ';
+        }
+        return input;
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
new file mode 100644
index 0000000..2220438
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorFactoryTests.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+
+public class UppercaseProcessorFactoryTests extends ESTestCase {
+
+    public void testCreate() throws Exception {
+        UppercaseProcessor.Factory factory = new UppercaseProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field1");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        UppercaseProcessor uppercaseProcessor = factory.create(config);
+        assertThat(uppercaseProcessor.getTag(), equalTo(processorTag));
+        assertThat(uppercaseProcessor.getField(), equalTo("field1"));
+    }
+
+    public void testCreateMissingField() throws Exception {
+        UppercaseProcessor.Factory factory = new UppercaseProcessor.Factory();
+        Map<String, Object> config = new HashMap<>();
+        try {
+            factory.create(config);
+            fail("factory create should have failed");
+        } catch(IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("required property [field] is missing"));
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java
new file mode 100644
index 0000000..4ab61f7
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/ingest/processor/UppercaseProcessorTests.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.processor;
+
+import java.util.Locale;
+
+public class UppercaseProcessorTests extends AbstractStringProcessorTestCase {
+
+    @Override
+    protected AbstractStringProcessor newProcessor(String field) {
+        return new UppercaseProcessor(randomAsciiOfLength(10), field);
+    }
+
+    @Override
+    protected String expectedResult(String input) {
+        return input.toUpperCase(Locale.ROOT);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java b/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
index a5d7ab5..eecc71f 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolateDocumentParserTests.java
@@ -23,9 +23,7 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.search.TermQuery;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.percolate.PercolateShardRequest;
-import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.Index;
@@ -89,14 +87,9 @@ public class PercolateDocumentParserTests extends ESTestCase {
         queryShardContext = new QueryShardContext(indexSettings, null, null, null, mapperService, null, null, indicesQueriesRegistry);
 
         HighlightPhase highlightPhase = new HighlightPhase(Settings.EMPTY, new Highlighters());
-        AggregatorParsers aggregatorParsers = new AggregatorParsers(Collections.emptySet(), Collections.emptySet(),
-                new NamedWriteableRegistry());
-        AggregationPhase aggregationPhase = new AggregationPhase(new AggregationParseElement(aggregatorParsers, indicesQueriesRegistry),
-                new AggregationBinaryParseElement(aggregatorParsers, indicesQueriesRegistry));
-        MappingUpdatedAction mappingUpdatedAction = Mockito.mock(MappingUpdatedAction.class);
-        parser = new PercolateDocumentParser(
-                highlightPhase, new SortParseElement(), aggregationPhase, mappingUpdatedAction
-        );
+        AggregatorParsers aggregatorParsers = new AggregatorParsers(Collections.emptySet(), Collections.emptySet());
+        AggregationPhase aggregationPhase = new AggregationPhase(new AggregationParseElement(aggregatorParsers), new AggregationBinaryParseElement(aggregatorParsers));
+        parser = new PercolateDocumentParser(highlightPhase, new SortParseElement(), aggregationPhase);
 
         request = Mockito.mock(PercolateShardRequest.class);
         Mockito.when(request.shardId()).thenReturn(new ShardId(new Index("_index"), 0));
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
index 4a15b65..22183bd 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
@@ -175,7 +175,7 @@ public class PercolatorIT extends ESIntegTestCase {
     }
 
     public void testSimple2() throws Exception {
-        assertAcked(prepareCreate("test").addMapping("type1", "field1", "type=long,doc_values=true"));
+        assertAcked(prepareCreate("test").addMapping("type1", "field1", "type=long,doc_values=true", "field2", "type=string"));
         ensureGreen();
 
         // introduce the doc
@@ -1577,92 +1577,6 @@ public class PercolatorIT extends ESIntegTestCase {
         assertEquals(response.getMatches()[0].getId().string(), "Q");
     }
 
-    public void testPercolationWithDynamicTemplates() throws Exception {
-        assertAcked(prepareCreate("idx").addMapping("type", jsonBuilder().startObject().startObject("type")
-                .field("dynamic", false)
-                .startObject("properties")
-                .startObject("custom")
-                .field("dynamic", true)
-                .field("type", "object")
-                .field("include_in_all", false)
-                .endObject()
-                .endObject()
-                .startArray("dynamic_templates")
-                .startObject()
-                .startObject("custom_fields")
-                .field("path_match", "custom.*")
-                .startObject("mapping")
-                .field("index", "not_analyzed")
-                .endObject()
-                .endObject()
-                .endObject()
-                .endArray()
-                .endObject().endObject()));
-        ensureGreen("idx");
-
-        try {
-            client().prepareIndex("idx", PercolatorService.TYPE_NAME, "1")
-                    .setSource(jsonBuilder().startObject().field("query", QueryBuilders.queryStringQuery("color:red")).endObject())
-                    .get();
-            fail();
-        } catch (MapperParsingException e) {
-        }
-        refresh();
-
-        PercolateResponse percolateResponse = client().preparePercolate().setDocumentType("type")
-                .setPercolateDoc(new PercolateSourceBuilder.DocBuilder().setDoc(jsonBuilder().startObject().startObject("custom").field("color", "blue").endObject().endObject()))
-                .get();
-
-        assertMatchCount(percolateResponse, 0l);
-        assertThat(percolateResponse.getMatches(), arrayWithSize(0));
-
-        // The previous percolate request introduced the custom.color field, so now we register the query again
-        // and the field name `color` will be resolved to `custom.color` field in mapping via smart field mapping resolving.
-        client().prepareIndex("idx", PercolatorService.TYPE_NAME, "1")
-                .setSource(jsonBuilder().startObject().field("query", QueryBuilders.queryStringQuery("custom.color:red")).endObject())
-                .get();
-        client().prepareIndex("idx", PercolatorService.TYPE_NAME, "2")
-                .setSource(jsonBuilder().startObject().field("query", QueryBuilders.queryStringQuery("custom.color:blue")).field("type", "type").endObject())
-                .get();
-        refresh();
-
-        // The second request will yield a match, since the query during the proper field during parsing.
-        percolateResponse = client().preparePercolate().setDocumentType("type")
-                .setPercolateDoc(new PercolateSourceBuilder.DocBuilder().setDoc(jsonBuilder().startObject().startObject("custom").field("color", "blue").endObject().endObject()))
-                .get();
-
-        assertMatchCount(percolateResponse, 1l);
-        assertThat(percolateResponse.getMatches()[0].getId().string(), equalTo("2"));
-    }
-
-    public void testUpdateMappingDynamicallyWhilePercolating() throws Exception {
-        createIndex("test");
-        ensureSearchable();
-
-        // percolation source
-        XContentBuilder percolateDocumentSource = XContentFactory.jsonBuilder().startObject().startObject("doc")
-                .field("field1", 1)
-                .field("field2", "value")
-                .endObject().endObject();
-
-        PercolateResponse response = client().preparePercolate()
-                .setIndices("test").setDocumentType("type1")
-                .setSource(percolateDocumentSource).execute().actionGet();
-        assertAllSuccessful(response);
-        assertMatchCount(response, 0l);
-        assertThat(response.getMatches(), arrayWithSize(0));
-
-        assertMappingOnMaster("test", "type1");
-
-        GetMappingsResponse mappingsResponse = client().admin().indices().prepareGetMappings("test").get();
-        assertThat(mappingsResponse.getMappings().get("test"), notNullValue());
-        assertThat(mappingsResponse.getMappings().get("test").get("type1"), notNullValue());
-        assertThat(mappingsResponse.getMappings().get("test").get("type1").getSourceAsMap().isEmpty(), is(false));
-        Map<String, Object> properties = (Map<String, Object>) mappingsResponse.getMappings().get("test").get("type1").getSourceAsMap().get("properties");
-        assertThat(((Map<String, String>) properties.get("field1")).get("type"), equalTo("long"));
-        assertThat(((Map<String, String>) properties.get("field2")).get("type"), equalTo("string"));
-    }
-
     public void testDontReportDeletedPercolatorDocs() throws Exception {
         client().admin().indices().prepareCreate("test").execute().actionGet();
         ensureGreen();
diff --git a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
index 8ef7bcc..d639411 100644
--- a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
@@ -63,7 +63,8 @@ public class FileScriptTests extends ESTestCase {
             .put("script.engine." + MockScriptEngine.NAME + ".file.aggs", false)
             .put("script.engine." + MockScriptEngine.NAME + ".file.search", false)
             .put("script.engine." + MockScriptEngine.NAME + ".file.mapping", false)
-            .put("script.engine." + MockScriptEngine.NAME + ".file.update", false).build();
+            .put("script.engine." + MockScriptEngine.NAME + ".file.update", false)
+            .put("script.engine." + MockScriptEngine.NAME + ".file.ingest", false).build();
         ScriptService scriptService = makeScriptService(settings);
         Script script = new Script("script1", ScriptService.ScriptType.FILE, MockScriptEngine.NAME, null);
         for (ScriptContext context : ScriptContext.Standard.values()) {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
index dd93b87..178c7ff 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationCollectorTests.java
@@ -19,12 +19,9 @@
 
 package org.elasticsearch.search.aggregations;
 
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.aggregations.support.AggregationContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESSingleNodeTestCase;
@@ -37,7 +34,7 @@ public class AggregationCollectorTests extends ESSingleNodeTestCase {
         IndexService index = createIndex("idx");
         client().prepareIndex("idx", "type", "1").setSource("f", 5).execute().get();
         client().admin().indices().prepareRefresh("idx").get();
-
+        
         // simple field aggregation, no scores needed
         String fieldAgg = "{ \"my_terms\": {\"terms\": {\"field\": \"f\"}}}";
         assertFalse(needsScores(index, fieldAgg));
@@ -61,17 +58,12 @@ public class AggregationCollectorTests extends ESSingleNodeTestCase {
 
     private boolean needsScores(IndexService index, String agg) throws IOException {
         AggregatorParsers parser = getInstanceFromNode(AggregatorParsers.class);
-        IndicesQueriesRegistry queriesRegistry = getInstanceFromNode(IndicesQueriesRegistry.class);
         XContentParser aggParser = JsonXContent.jsonXContent.createParser(agg);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(aggParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
         aggParser.nextToken();
         SearchContext searchContext = createSearchContext(index);
-        final AggregatorFactories factories = parser.parseAggregators(aggParser, parseContext);
+        final AggregatorFactories factories = parser.parseAggregators(aggParser, searchContext);
         AggregationContext aggregationContext = new AggregationContext(searchContext);
-        factories.init(aggregationContext);
-        final Aggregator[] aggregators = factories.createTopLevelAggregators();
+        final Aggregator[] aggregators = factories.createTopLevelAggregators(aggregationContext);
         assertEquals(1, aggregators.length);
         return aggregators[0].needsScores();
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java
deleted file mode 100644
index 278db79..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/BaseAggregationTestCase.java
+++ /dev/null
@@ -1,289 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class BaseAggregationTestCase<AF extends AggregatorFactory> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static AggregatorParsers aggParsers;
-    private static IndicesQueriesRegistry queriesRegistry;
-    private static ParseFieldMatcher parseFieldMatcher;
-
-    protected abstract AF createTestAggregatorFactory();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("name", BaseAggregationTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-
-        namedWriteableRegistry =  new NamedWriteableRegistry();
-        index = new Index("test");
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings, new SettingsFilter(settings)),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new ScriptModule(settings),
-                new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                },
-                new IndexSettingsModule(index, settings),
-
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }
-        ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    @After
-    public void afterTest() {
-        SearchContext.removeCurrent();
-    }
-
-    /**
-     * Generic test that creates new AggregatorFactory from the test
-     * AggregatorFactory and checks both for equality and asserts equality on
-     * the two queries.
-     */
-    public void testFromXContent() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        AggregatorFactories factories = AggregatorFactories.builder().addAggregator(testAgg).build();
-        String contentString = factories.toString();
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.name, parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.type.name(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        AggregatorFactory newAgg = aggParsers.parser(testAgg.getType()).parse(testAgg.name, parser, parseContext);
-        assertSame(XContentParser.Token.END_OBJECT, parser.currentToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertNull(parser.nextToken());
-        assertNotNull(newAgg);
-        assertNotSame(newAgg, testAgg);
-        assertEquals(testAgg, newAgg);
-        assertEquals(testAgg.hashCode(), newAgg.hashCode());
-    }
-
-    /**
-     * Test serialization and deserialization of the test AggregatorFactory.
-     */
-
-    public void testSerialization() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testAgg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                AggregatorFactory prototype = (AggregatorFactory) namedWriteableRegistry.getPrototype(AggregatorFactory.class, testAgg.getWriteableName());
-                AggregatorFactory deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testAgg);
-                assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());
-                assertNotSame(deserializedQuery, testAgg);
-            }
-        }
-    }
-
-
-    public void testEqualsAndHashcode() throws IOException {
-        AF firstAgg = createTestAggregatorFactory();
-        assertFalse("aggregation is equal to null", firstAgg.equals(null));
-        assertFalse("aggregation is equal to incompatible type", firstAgg.equals(""));
-        assertTrue("aggregation is not equal to self", firstAgg.equals(firstAgg));
-        assertThat("same aggregation's hashcode returns different values if called multiple times", firstAgg.hashCode(),
-                equalTo(firstAgg.hashCode()));
-
-        AF secondQuery = copyAggregation(firstAgg);
-        assertTrue("aggregation is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("aggregation is not equal to its copy", firstAgg.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstAgg));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstAgg.hashCode()));
-
-        AF thirdQuery = copyAggregation(secondQuery);
-        assertTrue("aggregation is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("aggregation is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(),
-                equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstAgg.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", firstAgg.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstAgg));
-    }
-
-    // we use the streaming infra to create a copy of the query provided as
-    // argument
-    private AF copyAggregation(AF agg) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            agg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                AggregatorFactory prototype = (AggregatorFactory) namedWriteableRegistry.getPrototype(AggregatorFactory.class, agg.getWriteableName());
-                @SuppressWarnings("unchecked")
-                AF secondAgg = (AF) prototype.readFrom(in);
-                return secondAgg;
-            }
-        }
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    public String randomNumericField() {
-        int randomInt = randomInt(3);
-        switch (randomInt) {
-        case 0:
-            return DATE_FIELD_NAME;
-        case 1:
-            return DOUBLE_FIELD_NAME;
-        case 2:
-        default:
-            return INT_FIELD_NAME;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java
deleted file mode 100644
index 30b6584..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/BasePipelineAggregationTestCase.java
+++ /dev/null
@@ -1,292 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.SearchModule;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-public abstract class BasePipelineAggregationTestCase<AF extends PipelineAggregatorFactory> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    private static AggregatorParsers aggParsers;
-    private static ParseFieldMatcher parseFieldMatcher;
-    private static IndicesQueriesRegistry queriesRegistry;
-
-    protected abstract AF createTestAggregatorFactory();
-
-    /**
-     * Setup for the whole base test class.
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("name", BasePipelineAggregationTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-
-        namedWriteableRegistry = new NamedWriteableRegistry();
-        index = new Index("test");
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings, new SettingsFilter(settings)),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new ScriptModule(settings),
-                new IndicesModule() {
-
-                    @Override
-                    protected void configure() {
-                        bindMapperExtension();
-                    }
-                }, new SearchModule(settings, namedWriteableRegistry) {
-                    @Override
-                    protected void configureSearch() {
-                        // Skip me
-                    }
-                    @Override
-                    protected void configureSuggesters() {
-                        // Skip me
-                    }
-                },
-                new IndexSettingsModule(index, settings),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);
-                    }
-                }
-        ).createInjector();
-        aggParsers = injector.getInstance(AggregatorParsers.class);
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            currentTypes[i] = type;
-        }
-        queriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        parseFieldMatcher = ParseFieldMatcher.STRICT;
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        aggParsers = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        TestSearchContext testSearchContext = new TestSearchContext();
-        testSearchContext.setTypes(types);
-        SearchContext.setCurrent(testSearchContext);
-    }
-
-    @After
-    public void afterTest() {
-        SearchContext.removeCurrent();
-    }
-
-    /**
-     * Generic test that creates new AggregatorFactory from the test
-     * AggregatorFactory and checks both for equality and asserts equality on
-     * the two queries.
-     */
-
-    public void testFromXContent() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        AggregatorFactories factories = AggregatorFactories.builder().skipResolveOrder().addPipelineAggregator(testAgg).build();
-        String contentString = factories.toString();
-        System.out.println(contentString);
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        QueryParseContext parseContext = new QueryParseContext(queriesRegistry);
-        parseContext.reset(parser);
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.name(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.FIELD_NAME, parser.nextToken());
-        assertEquals(testAgg.type(), parser.currentName());
-        assertSame(XContentParser.Token.START_OBJECT, parser.nextToken());
-        PipelineAggregatorFactory newAgg = aggParsers.pipelineAggregator(testAgg.getWriteableName()).parse(testAgg.name(), parser,
-                parseContext);
-        assertSame(XContentParser.Token.END_OBJECT, parser.currentToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertSame(XContentParser.Token.END_OBJECT, parser.nextToken());
-        assertNull(parser.nextToken());
-        assertNotNull(newAgg);
-        assertNotSame(newAgg, testAgg);
-        assertEquals(testAgg, newAgg);
-        assertEquals(testAgg.hashCode(), newAgg.hashCode());
-    }
-
-    /**
-     * Test serialization and deserialization of the test AggregatorFactory.
-     */
-
-    public void testSerialization() throws IOException {
-        AF testAgg = createTestAggregatorFactory();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testAgg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                PipelineAggregatorFactory prototype = aggParsers.pipelineAggregator(testAgg.getWriteableName()).getFactoryPrototype();
-                PipelineAggregatorFactory deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testAgg);
-                assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());
-                assertNotSame(deserializedQuery, testAgg);
-            }
-        }
-    }
-
-
-    public void testEqualsAndHashcode() throws IOException {
-        AF firstAgg = createTestAggregatorFactory();
-        assertFalse("aggregation is equal to null", firstAgg.equals(null));
-        assertFalse("aggregation is equal to incompatible type", firstAgg.equals(""));
-        assertTrue("aggregation is not equal to self", firstAgg.equals(firstAgg));
-        assertThat("same aggregation's hashcode returns different values if called multiple times", firstAgg.hashCode(),
-                equalTo(firstAgg.hashCode()));
-
-        AF secondQuery = copyAggregation(firstAgg);
-        assertTrue("aggregation is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("aggregation is not equal to its copy", firstAgg.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstAgg));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstAgg.hashCode()));
-
-        AF thirdQuery = copyAggregation(secondQuery);
-        assertTrue("aggregation is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("aggregation is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", secondQuery.hashCode(),
-                equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstAgg.equals(thirdQuery));
-        assertThat("aggregation copy's hashcode is different from original hashcode", firstAgg.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstAgg));
-    }
-
-    // we use the streaming infra to create a copy of the query provided as
-    // argument
-    private AF copyAggregation(AF agg) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            agg.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                PipelineAggregatorFactory prototype = aggParsers.pipelineAggregator(agg.getWriteableName()).getFactoryPrototype();
-                @SuppressWarnings("unchecked")
-                AF secondAgg = (AF) prototype.readFrom(in);
-                return secondAgg;
-            }
-        }
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    public String randomNumericField() {
-        int randomInt = randomInt(3);
-        switch (randomInt) {
-        case 0:
-            return DATE_FIELD_NAME;
-        case 1:
-            return DOUBLE_FIELD_NAME;
-        case 2:
-        default:
-            return INT_FIELD_NAME;
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java
deleted file mode 100644
index 131144d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/SubAggCollectionModeTests.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class SubAggCollectionModeTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(SubAggCollectionMode.DEPTH_FIRST.ordinal(), equalTo(0));
-        assertThat(SubAggCollectionMode.BREADTH_FIRST.ordinal(), equalTo(1));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SubAggCollectionMode.DEPTH_FIRST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            SubAggCollectionMode.BREADTH_FIRST.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SubAggCollectionMode.BREADTH_FIRST.readFrom(in), equalTo(SubAggCollectionMode.DEPTH_FIRST));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(SubAggCollectionMode.BREADTH_FIRST.readFrom(in), equalTo(SubAggCollectionMode.BREADTH_FIRST));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(2, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                SubAggCollectionMode.BREADTH_FIRST.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown SubAggCollectionMode ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
index 715505d..e0c7d23 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenIT.java
@@ -175,7 +175,7 @@ public class ChildrenIT extends ESIntegTestCase {
                 .setQuery(matchQuery("randomized", false))
                 .addAggregation(
                         terms("category").field("category").size(0).subAggregation(
-                                children("to_comment").childType("comment").subAggregation(topHits("top_comments").sort("_uid", SortOrder.ASC))
+                                children("to_comment").childType("comment").subAggregation(topHits("top_comments").addSort("_uid", SortOrder.ASC))
                         )
                 ).get();
         assertSearchResponse(searchResponse);
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java
deleted file mode 100644
index 88e0fba..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator;
-import org.elasticsearch.search.aggregations.bucket.children.ParentToChildrenAggregator.Factory;
-
-public class ChildrenTests extends BaseAggregationTestCase<ParentToChildrenAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String childType = randomAsciiOfLengthBetween(5, 40);
-        Factory factory = new Factory(name, childType);
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java
deleted file mode 100644
index ed3696d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-import org.elasticsearch.search.aggregations.bucket.range.date.DateRangeAggregatorFactory;
-
-import java.util.ArrayList;
-import java.util.List;
-
-public class DateRangeTests extends BaseAggregationTestCase<DateRangeAggregatorFactory> {
-
-    @Override
-    protected DateRangeAggregatorFactory createTestAggregatorFactory() {
-        int numRanges = randomIntBetween(1, 10);
-        List<Range> ranges = new ArrayList<>(numRanges);
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            if (randomBoolean()) {
-                ranges.add(new Range(key, from, to));
-            } else {
-                String fromAsStr = Double.isInfinite(from) ? null : String.valueOf(from);
-                String toAsStr = Double.isInfinite(to) ? null : String.valueOf(to);
-                ranges.add(new Range(key, fromAsStr, toAsStr));
-            }
-        }
-        DateRangeAggregatorFactory factory = new DateRangeAggregatorFactory("foo", ranges);
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java
deleted file mode 100644
index 2a77719..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerIT.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.index.query.TermQueryBuilder;
-import org.elasticsearch.search.aggregations.bucket.sampler.DiversifiedSamplerAggregationBuilder;
-import org.elasticsearch.search.aggregations.bucket.sampler.Sampler;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms.Bucket;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
-import org.elasticsearch.search.aggregations.metrics.max.Max;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.Collection;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.max;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.sampler;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-
-/**
- * Tests the Sampler aggregation
- */
-@ESIntegTestCase.SuiteScopeTestCase
-public class DiversifiedSamplerIT extends ESIntegTestCase {
-
-    public static final int NUM_SHARDS = 2;
-
-    public String randomExecutionHint() {
-        return randomBoolean() ? null : randomFrom(SamplerAggregator.ExecutionMode.values()).toString();
-    }
-
-
-    @Override
-    public void setupSuiteScopeCluster() throws Exception {
-        assertAcked(prepareCreate("test").setSettings(SETTING_NUMBER_OF_SHARDS, NUM_SHARDS, SETTING_NUMBER_OF_REPLICAS, 0).addMapping(
-                "book", "author", "type=string,index=not_analyzed", "name", "type=string,index=analyzed", "genre",
-                "type=string,index=not_analyzed", "price", "type=float"));
-        createIndex("idx_unmapped");
-        // idx_unmapped_author is same as main index but missing author field
-        assertAcked(prepareCreate("idx_unmapped_author").setSettings(SETTING_NUMBER_OF_SHARDS, NUM_SHARDS, SETTING_NUMBER_OF_REPLICAS, 0)
-                .addMapping("book", "name", "type=string,index=analyzed", "genre", "type=string,index=not_analyzed", "price", "type=float"));
-
-        ensureGreen();
-        String data[] = {
-                // "id,cat,name,price,inStock,author_t,series_t,sequence_i,genre_s",
-                "0553573403,book,A Game of Thrones,7.99,true,George R.R. Martin,A Song of Ice and Fire,1,fantasy",
-                "0553579908,book,A Clash of Kings,7.99,true,George R.R. Martin,A Song of Ice and Fire,2,fantasy",
-                "055357342X,book,A Storm of Swords,7.99,true,George R.R. Martin,A Song of Ice and Fire,3,fantasy",
-                "0553293354,book,Foundation,17.99,true,Isaac Asimov,Foundation Novels,1,scifi",
-                "0812521390,book,The Black Company,6.99,false,Glen Cook,The Chronicles of The Black Company,1,fantasy",
-                "0812550706,book,Ender's Game,6.99,true,Orson Scott Card,Ender,1,scifi",
-                "0441385532,book,Jhereg,7.95,false,Steven Brust,Vlad Taltos,1,fantasy",
-                "0380014300,book,Nine Princes In Amber,6.99,true,Roger Zelazny,the Chronicles of Amber,1,fantasy",
-                "0805080481,book,The Book of Three,5.99,true,Lloyd Alexander,The Chronicles of Prydain,1,fantasy",
-                "080508049X,book,The Black Cauldron,5.99,true,Lloyd Alexander,The Chronicles of Prydain,2,fantasy"
-
-            };
-
-        for (int i = 0; i < data.length; i++) {
-            String[] parts = data[i].split(",");
-            client().prepareIndex("test", "book", "" + i).setSource("author", parts[5], "name", parts[2], "genre", parts[8], "price",Float.parseFloat(parts[3])).get();
-            client().prepareIndex("idx_unmapped_author", "book", "" + i).setSource("name", parts[2], "genre", parts[8],"price",Float.parseFloat(parts[3])).get();
-        }
-        client().admin().indices().refresh(new RefreshRequest("test")).get();
-    }
-
-    public void testIssue10719() throws Exception {
-        // Tests that we can refer to nested elements under a sample in a path
-        // statement
-        boolean asc = randomBoolean();
-        SearchResponse response = client().prepareSearch("test").setTypes("book").setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(terms("genres")
-                        .field("genre")
-                        .order(Terms.Order.aggregation("sample>max_price.value", asc))
-                        .subAggregation(sampler("sample").shardSize(100)
-                                .subAggregation(max("max_price").field("price")))
-                ).execute().actionGet();
-        assertSearchResponse(response);
-        Terms genres = response.getAggregations().get("genres");
-        Collection<Bucket> genreBuckets = genres.getBuckets();
-        // For this test to be useful we need >1 genre bucket to compare
-        assertThat(genreBuckets.size(), greaterThan(1));
-        double lastMaxPrice = asc ? Double.MIN_VALUE : Double.MAX_VALUE;
-        for (Terms.Bucket genreBucket : genres.getBuckets()) {
-            Sampler sample = genreBucket.getAggregations().get("sample");
-            Max maxPriceInGenre = sample.getAggregations().get("max_price");
-            double price = maxPriceInGenre.getValue();
-            if (asc) {
-                assertThat(price, greaterThanOrEqualTo(lastMaxPrice));
-            } else {
-                assertThat(price, lessThanOrEqualTo(lastMaxPrice));
-            }
-            lastMaxPrice = price;
-        }
-
-    }
-
-    public void testSimpleDiversity() throws Exception {
-        int MAX_DOCS_PER_AUTHOR = 1;
-        DiversifiedSamplerAggregationBuilder sampleAgg = new DiversifiedSamplerAggregationBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
-        SearchResponse response = client().prepareSearch("test")
-                .setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy"))
-                .setFrom(0).setSize(60)
-                .addAggregation(sampleAgg)
-                .execute()
-                .actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        Terms authors = sample.getAggregations().get("authors");
-        Collection<Bucket> testBuckets = authors.getBuckets();
-
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-        }
-    }
-
-    public void testNestedDiversity() throws Exception {
-        // Test multiple samples gathered under buckets made by a parent agg
-        int MAX_DOCS_PER_AUTHOR = 1;
-        TermsBuilder rootTerms = new TermsBuilder("genres").field("genre");
-
-        DiversifiedSamplerAggregationBuilder sampleAgg = new DiversifiedSamplerAggregationBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
-
-        rootTerms.subAggregation(sampleAgg);
-        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
-                .addAggregation(rootTerms).execute().actionGet();
-        assertSearchResponse(response);
-        Terms genres = response.getAggregations().get("genres");
-        Collection<Bucket> genreBuckets = genres.getBuckets();
-        for (Terms.Bucket genreBucket : genreBuckets) {
-            Sampler sample = genreBucket.getAggregations().get("sample");
-            Terms authors = sample.getAggregations().get("authors");
-            Collection<Bucket> testBuckets = authors.getBuckets();
-
-            for (Terms.Bucket testBucket : testBuckets) {
-                assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-            }
-        }
-    }
-
-    public void testNestedSamples() throws Exception {
-        // Test samples nested under samples
-        int MAX_DOCS_PER_AUTHOR = 1;
-        int MAX_DOCS_PER_GENRE = 2;
-        DiversifiedSamplerAggregationBuilder rootSample = new DiversifiedSamplerAggregationBuilder("genreSample").shardSize(100)
-                .field("genre")
-                .maxDocsPerValue(MAX_DOCS_PER_GENRE);
-
-        DiversifiedSamplerAggregationBuilder sampleAgg = new DiversifiedSamplerAggregationBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
-        sampleAgg.subAggregation(new TermsBuilder("genres").field("genre"));
-
-        rootSample.subAggregation(sampleAgg);
-        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH).addAggregation(rootSample)
-                .execute().actionGet();
-        assertSearchResponse(response);
-        Sampler genreSample = response.getAggregations().get("genreSample");
-        Sampler sample = genreSample.getAggregations().get("sample");
-
-        Terms genres = sample.getAggregations().get("genres");
-        Collection<Bucket> testBuckets = genres.getBuckets();
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_GENRE));
-        }
-
-        Terms authors = sample.getAggregations().get("authors");
-        testBuckets = authors.getBuckets();
-        for (Terms.Bucket testBucket : testBuckets) {
-            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
-        }
-    }
-
-    public void testPartiallyUnmappedDiversifyField() throws Exception {
-        // One of the indexes is missing the "author" field used for
-        // diversifying results
-        DiversifiedSamplerAggregationBuilder sampleAgg = new DiversifiedSamplerAggregationBuilder("sample").shardSize(100).field("author")
-                .maxDocsPerValue(1);
-        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
-        SearchResponse response = client().prepareSearch("idx_unmapped_author", "test").setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg)
-                .execute().actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        assertThat(sample.getDocCount(), greaterThan(0l));
-        Terms authors = sample.getAggregations().get("authors");
-        assertThat(authors.getBuckets().size(), greaterThan(0));
-    }
-
-    public void testWhollyUnmappedDiversifyField() throws Exception {
-        //All of the indices are missing the "author" field used for diversifying results
-        int MAX_DOCS_PER_AUTHOR = 1;
-        DiversifiedSamplerAggregationBuilder sampleAgg = new DiversifiedSamplerAggregationBuilder("sample").shardSize(100);
-        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
-        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
-        SearchResponse response = client().prepareSearch("idx_unmapped", "idx_unmapped_author").setSearchType(SearchType.QUERY_AND_FETCH)
-                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
-        assertSearchResponse(response);
-        Sampler sample = response.getAggregations().get("sample");
-        assertThat(sample.getDocCount(), equalTo(0l));
-        Terms authors = sample.getAggregations().get("authors");
-        assertNull(authors);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java
deleted file mode 100644
index 42245c8..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/DiversifiedSamplerTests.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator.ExecutionMode;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-public class DiversifiedSamplerTests extends BaseAggregationTestCase<SamplerAggregator.DiversifiedFactory> {
-
-    @Override
-    protected final SamplerAggregator.DiversifiedFactory createTestAggregatorFactory() {
-        SamplerAggregator.DiversifiedFactory factory = new SamplerAggregator.DiversifiedFactory("foo", ValuesSourceType.ANY,
-                null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.maxDocsPerValue(randomIntBetween(1, 1000));
-        }
-        if (randomBoolean()) {
-            factory.shardSize(randomIntBetween(1, 1000));
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(ExecutionMode.values()).toString());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
index 6dbd309..2235b00 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersIT.java
@@ -330,7 +330,7 @@ public class FiltersIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("idx")
                 .addAggregation(
-                        filters("tags").otherBucket(true).otherBucketKey("foobar")
+                        filters("tags").otherBucketKey("foobar")
                         .filter("tag1", termQuery("tag", "tag1"))
                         .filter("tag2", termQuery("tag", "tag2")))
                 .execute().actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java
deleted file mode 100644
index 9d579ad..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceRangeTests.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.geo.GeoPoint;
-import org.elasticsearch.common.unit.DistanceUnit;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.GeoDistanceFactory;
-import org.elasticsearch.search.aggregations.bucket.range.geodistance.GeoDistanceParser.Range;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
-
-import java.util.ArrayList;
-import java.util.List;
-
-public class GeoDistanceRangeTests extends BaseAggregationTestCase<GeoDistanceFactory> {
-
-    @Override
-    protected GeoDistanceFactory createTestAggregatorFactory() {
-        int numRanges = randomIntBetween(1, 10);
-        List<Range> ranges = new ArrayList<>(numRanges);
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? 0 : randomIntBetween(0, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.compare(from, 0) == 0 ? randomIntBetween(0, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            ranges.add(new Range(key, from, to));
-        }
-        GeoPoint origin = RandomShapeGenerator.randomPoint(getRandom());
-        GeoDistanceFactory factory = new GeoDistanceFactory("foo", origin, ranges);
-        factory.field(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing("0, 0");
-        }
-        if (randomBoolean()) {
-            factory.unit(randomFrom(DistanceUnit.values()));
-        }
-        if (randomBoolean()) {
-            factory.distanceType(randomFrom(GeoDistance.values()));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java
deleted file mode 100644
index 8836ece..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser.GeoGridFactory;
-
-public class GeoHashGridTests extends BaseAggregationTestCase<GeoGridFactory> {
-
-    @Override
-    protected GeoGridFactory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        GeoGridFactory factory = new GeoGridFactory(name);
-        if (randomBoolean()) {
-            int precision = randomIntBetween(1, 12);
-            factory.precision(precision);
-        }
-        if (randomBoolean()) {
-            int size = randomInt(5);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            }
-            factory.size(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(5);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            }
-            factory.shardSize(shardSize);
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java
deleted file mode 100644
index 6529e0b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/GlobalTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator;
-import org.elasticsearch.search.aggregations.bucket.global.GlobalAggregator.Factory;
-
-public class GlobalTests extends BaseAggregationTestCase<GlobalAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        return new GlobalAggregator.Factory(randomAsciiOfLengthBetween(3, 20));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java
deleted file mode 100644
index ed0b17b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds;
-import org.elasticsearch.search.aggregations.bucket.histogram.Histogram.Order;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator;
-import org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.Factory;
-
-public class HistogramTests extends BaseAggregationTestCase<HistogramAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory("foo");
-        factory.field(INT_FIELD_NAME);
-        factory.interval(randomIntBetween(1, 100000));
-        if (randomBoolean()) {
-            long extendedBoundsMin = randomIntBetween(-100000, 100000);
-            long extendedBoundsMax = randomIntBetween((int) extendedBoundsMin, 200000);
-            factory.extendedBounds(new ExtendedBounds(extendedBoundsMin, extendedBoundsMax));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.minDocCount(randomIntBetween(0, 100));
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            factory.offset(randomIntBetween(0, 100000));
-        }
-        if (randomBoolean()) {
-            int branch = randomInt(5);
-            switch (branch) {
-            case 0:
-                factory.order(Order.COUNT_ASC);
-                break;
-            case 1:
-                factory.order(Order.COUNT_DESC);
-                break;
-            case 2:
-                factory.order(Order.KEY_ASC);
-                break;
-            case 3:
-                factory.order(Order.KEY_DESC);
-                break;
-            case 4:
-                factory.order(Order.aggregation("foo", true));
-                break;
-            case 5:
-                factory.order(Order.aggregation("foo", false));
-                break;
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java
deleted file mode 100644
index 6457d0b..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.common.network.Cidrs;
-import org.elasticsearch.index.mapper.ip.IpFieldMapper;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.range.ipv4.IPv4RangeAggregatorFactory.Range;
-
-import java.util.ArrayList;
-import java.util.List;
-
-public class IPv4RangeTests extends BaseAggregationTestCase<IPv4RangeAggregatorFactory> {
-
-    @Override
-    protected IPv4RangeAggregatorFactory createTestAggregatorFactory() {
-        int numRanges = randomIntBetween(1, 10);
-        List<Range> ranges = new ArrayList<>(numRanges);
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            if (randomBoolean()) {
-                double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-                double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                        : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                                : randomIntBetween((int) from, Integer.MAX_VALUE));
-                if (randomBoolean()) {
-                    ranges.add(new Range(key, from, to));
-                } else {
-                    String fromAsStr = Double.isInfinite(from) ? null : IpFieldMapper.longToIp((long) from);
-                    String toAsStr = Double.isInfinite(to) ? null : IpFieldMapper.longToIp((long) to);
-                    ranges.add(new Range(key, fromAsStr, toAsStr));
-                }
-            } else {
-                int mask = randomInt(32);
-                long ipAsLong = randomIntBetween(0, Integer.MAX_VALUE);
-
-                long blockSize = 1L << (32 - mask);
-                ipAsLong = ipAsLong - (ipAsLong & (blockSize - 1));
-                String cidr = Cidrs.createCIDR(ipAsLong, mask);
-                ranges.add(new Range(key, cidr));
-            }
-        }
-        IPv4RangeAggregatorFactory factory = new IPv4RangeAggregatorFactory("foo", ranges);
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
index 465dbce..bfd6837 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/NaNSortingIT.java
@@ -26,12 +26,9 @@ import org.elasticsearch.search.aggregations.Aggregation;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
+import org.elasticsearch.search.aggregations.metrics.MetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
 import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStats;
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -48,10 +45,8 @@ public class NaNSortingIT extends ESIntegTestCase {
     private enum SubAggregation {
         AVG("avg") {
             @Override
-            public AvgAggregator.Factory builder() {
-                AvgAggregator.Factory factory = avg(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return avg(name).field("numeric_field");
             }
             @Override
             public double getValue(Aggregation aggregation) {
@@ -60,10 +55,8 @@ public class NaNSortingIT extends ESIntegTestCase {
         },
         VARIANCE("variance") {
             @Override
-            public ExtendedStatsAggregator.Factory builder() {
-                ExtendedStatsAggregator.Factory factory = extendedStats(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return extendedStats(name).field("numeric_field");
             }
             @Override
             public String sortKey() {
@@ -76,10 +69,8 @@ public class NaNSortingIT extends ESIntegTestCase {
         },
         STD_DEVIATION("std_deviation"){
             @Override
-            public ExtendedStatsAggregator.Factory builder() {
-                ExtendedStatsAggregator.Factory factory = extendedStats(name);
-                factory.field("numeric_field");
-                return factory;
+            public MetricsAggregationBuilder<?> builder() {
+                return extendedStats(name).field("numeric_field");
             }
             @Override
             public String sortKey() {
@@ -97,7 +88,7 @@ public class NaNSortingIT extends ESIntegTestCase {
 
         public String name;
 
-        public abstract ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, ? extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, ?>> builder();
+        public abstract MetricsAggregationBuilder<?> builder();
 
         public String sortKey() {
             return name;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java
deleted file mode 100644
index 9271e89..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Factory;
-import org.elasticsearch.search.aggregations.bucket.range.RangeAggregator.Range;
-
-import java.util.ArrayList;
-import java.util.List;
-
-public class RangeTests extends BaseAggregationTestCase<RangeAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        int numRanges = randomIntBetween(1, 10);
-        List<Range> ranges = new ArrayList<>(numRanges);
-        for (int i = 0; i < numRanges; i++) {
-            String key = null;
-            if (randomBoolean()) {
-                key = randomAsciiOfLengthBetween(1, 20);
-            }
-            double from = randomBoolean() ? Double.NEGATIVE_INFINITY : randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE - 1000);
-            double to = randomBoolean() ? Double.POSITIVE_INFINITY
-                    : (Double.isInfinite(from) ? randomIntBetween(Integer.MIN_VALUE, Integer.MAX_VALUE)
-                            : randomIntBetween((int) from, Integer.MAX_VALUE));
-            if (randomBoolean()) {
-                ranges.add(new Range(key, from, to));
-            } else {
-                String fromAsStr = Double.isInfinite(from) ? null : String.valueOf(from);
-                String toAsStr = Double.isInfinite(to) ? null : String.valueOf(to);
-                ranges.add(new Range(key, fromAsStr, toAsStr));
-            }
-        }
-        Factory factory = new Factory("foo", ranges);
-        factory.field(INT_FIELD_NAME);
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing(randomIntBetween(0, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
index bcc40e8..2535ca3 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerIT.java
@@ -123,7 +123,7 @@ public class SamplerIT extends ESIntegTestCase {
 
     }
 
-    public void testSimpleSampler() throws Exception {
+    public void testNoDiversity() throws Exception {
         SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
         sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
         SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
@@ -140,6 +140,86 @@ public class SamplerIT extends ESIntegTestCase {
         assertThat(maxBooksPerAuthor, equalTo(3l));
     }
 
+    public void testSimpleDiversity() throws Exception {
+        int MAX_DOCS_PER_AUTHOR = 1;
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("test")
+                .setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy"))
+                .setFrom(0).setSize(60)
+                .addAggregation(sampleAgg)
+                .execute()
+                .actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        Terms authors = sample.getAggregations().get("authors");
+        Collection<Bucket> testBuckets = authors.getBuckets();
+
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+        }
+    }
+
+    public void testNestedDiversity() throws Exception {
+        // Test multiple samples gathered under buckets made by a parent agg
+        int MAX_DOCS_PER_AUTHOR = 1;
+        TermsBuilder rootTerms = new TermsBuilder("genres").field("genre");
+
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+
+        rootTerms.subAggregation(sampleAgg);
+        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH)
+                .addAggregation(rootTerms).execute().actionGet();
+        assertSearchResponse(response);
+        Terms genres = response.getAggregations().get("genres");
+        Collection<Bucket> genreBuckets = genres.getBuckets();
+        for (Terms.Bucket genreBucket : genreBuckets) {
+            Sampler sample = genreBucket.getAggregations().get("sample");
+            Terms authors = sample.getAggregations().get("authors");
+            Collection<Bucket> testBuckets = authors.getBuckets();
+
+            for (Terms.Bucket testBucket : testBuckets) {
+                assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+            }
+        }
+    }
+
+    public void testNestedSamples() throws Exception {
+        // Test samples nested under samples
+        int MAX_DOCS_PER_AUTHOR = 1;
+        int MAX_DOCS_PER_GENRE = 2;
+        SamplerAggregationBuilder rootSample = new SamplerAggregationBuilder("genreSample").shardSize(100).field("genre")
+                .maxDocsPerValue(MAX_DOCS_PER_GENRE);
+
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        sampleAgg.subAggregation(new TermsBuilder("genres").field("genre"));
+
+        rootSample.subAggregation(sampleAgg);
+        SearchResponse response = client().prepareSearch("test").setSearchType(SearchType.QUERY_AND_FETCH).addAggregation(rootSample)
+                .execute().actionGet();
+        assertSearchResponse(response);
+        Sampler genreSample = response.getAggregations().get("genreSample");
+        Sampler sample = genreSample.getAggregations().get("sample");
+
+        Terms genres = sample.getAggregations().get("genres");
+        Collection<Bucket> testBuckets = genres.getBuckets();
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_GENRE));
+        }
+
+        Terms authors = sample.getAggregations().get("authors");
+        testBuckets = authors.getBuckets();
+        for (Terms.Bucket testBucket : testBuckets) {
+            assertThat(testBucket.getDocCount(), lessThanOrEqualTo((long) NUM_SHARDS * MAX_DOCS_PER_AUTHOR));
+        }
+    }
+
     public void testUnmappedChildAggNoDiversity() throws Exception {
         SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
         sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
@@ -174,4 +254,34 @@ public class SamplerIT extends ESIntegTestCase {
         assertThat(authors.getBuckets().size(), greaterThan(0));
     }
 
+    public void testPartiallyUnmappedDiversifyField() throws Exception {
+        // One of the indexes is missing the "author" field used for
+        // diversifying results
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100).field("author").maxDocsPerValue(1);
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("idx_unmapped_author", "test").setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg)
+                .execute().actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        assertThat(sample.getDocCount(), greaterThan(0l));
+        Terms authors = sample.getAggregations().get("authors");
+        assertThat(authors.getBuckets().size(), greaterThan(0));
+    }
+
+    public void testWhollyUnmappedDiversifyField() throws Exception {
+        //All of the indices are missing the "author" field used for diversifying results
+        int MAX_DOCS_PER_AUTHOR = 1;
+        SamplerAggregationBuilder sampleAgg = new SamplerAggregationBuilder("sample").shardSize(100);
+        sampleAgg.field("author").maxDocsPerValue(MAX_DOCS_PER_AUTHOR).executionHint(randomExecutionHint());
+        sampleAgg.subAggregation(new TermsBuilder("authors").field("author"));
+        SearchResponse response = client().prepareSearch("idx_unmapped", "idx_unmapped_author").setSearchType(SearchType.QUERY_AND_FETCH)
+                .setQuery(new TermQueryBuilder("genre", "fantasy")).setFrom(0).setSize(60).addAggregation(sampleAgg).execute().actionGet();
+        assertSearchResponse(response);
+        Sampler sample = response.getAggregations().get("sample");
+        assertThat(sample.getDocCount(), equalTo(0l));
+        Terms authors = sample.getAggregations().get("authors");
+        assertNull(authors);
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java
deleted file mode 100644
index b591253..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregator;
-
-public class SamplerTests extends BaseAggregationTestCase<SamplerAggregator.Factory> {
-
-    @Override
-    protected final SamplerAggregator.Factory createTestAggregatorFactory() {
-        SamplerAggregator.Factory factory = new SamplerAggregator.Factory("foo");
-        if (randomBoolean()) {
-            factory.shardSize(randomIntBetween(1, 1000));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
index eb41430..6c1e7df 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
@@ -20,7 +20,6 @@ package org.elasticsearch.search.aggregations.bucket;
 
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -53,6 +52,7 @@ import org.elasticsearch.search.aggregations.bucket.significant.heuristics.Signi
 import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.elasticsearch.search.aggregations.bucket.terms.TermsBuilder;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.search.aggregations.bucket.SharedSignificantTermsTestMethods;
 
@@ -163,7 +163,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
     public static class CustomSignificanceHeuristicPlugin extends Plugin {
 
         static {
-            SignificanceHeuristicStreams.registerPrototype(SimpleHeuristic.PROTOTYPE);
+            SignificanceHeuristicStreams.registerStream(SimpleHeuristic.STREAM);
         }
 
         @Override
@@ -177,7 +177,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         }
 
         public void onModule(SearchModule significanceModule) {
-            significanceModule.registerHeuristicParser(new SimpleHeuristic.SimpleHeuristicParser());
+            significanceModule.registerHeuristicParser(SimpleHeuristic.SimpleHeuristicParser.class);
         }
         public void onModule(ScriptModule module) {
             module.registerScript(NativeSignificanceScoreScriptNoParams.NATIVE_SIGNIFICANCE_SCORE_SCRIPT_NO_PARAMS, NativeSignificanceScoreScriptNoParams.Factory.class);
@@ -187,30 +187,24 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
     public static class SimpleHeuristic extends SignificanceHeuristic {
 
-        static final SimpleHeuristic PROTOTYPE = new SimpleHeuristic();
+        protected static final String[] NAMES = {"simple"};
 
-        protected static final ParseField NAMES_FIELD = new ParseField("simple");
+        public static final SignificanceHeuristicStreams.Stream STREAM = new SignificanceHeuristicStreams.Stream() {
+            @Override
+            public SignificanceHeuristic readResult(StreamInput in) throws IOException {
+                return readFrom(in);
+            }
 
-        @Override
-        public String getWriteableName() {
-            return NAMES_FIELD.getPreferredName();
-        }
+            @Override
+            public String getName() {
+                return NAMES[0];
+            }
+        };
 
-        @Override
-        public SignificanceHeuristic readFrom(StreamInput in) throws IOException {
+        public static SignificanceHeuristic readFrom(StreamInput in) throws IOException {
             return new SimpleHeuristic();
         }
 
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
-            return builder;
-        }
-
         /**
          * @param subsetFreq   The frequency of the term in the selected sample
          * @param subsetSize   The size of the selected sample (typically number of docs)
@@ -223,10 +217,15 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
             return subsetFreq / subsetSize > supersetFreq / supersetSize ? 2.0 : 1.0;
         }
 
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeString(STREAM.getName());
+        }
+
         public static class SimpleHeuristicParser implements SignificanceHeuristicParser {
 
             @Override
-            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher)
+            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher, SearchContext context)
                     throws IOException, QueryShardException {
                 parser.nextToken();
                 return new SimpleHeuristic();
@@ -234,7 +233,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
             @Override
             public String[] getNames() {
-                return NAMES_FIELD.getAllNamesIncludedDeprecated();
+                return NAMES;
             }
         }
 
@@ -242,7 +241,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
 
             @Override
             public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-                builder.startObject(NAMES_FIELD.getPreferredName()).endObject();
+                builder.startObject(STREAM.getName()).endObject();
                 return builder;
             }
         }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java
deleted file mode 100644
index 8ad928e..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java
+++ /dev/null
@@ -1,226 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.RegExp;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ChiSquare;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.GND;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.MutualInformation;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.ScriptHeuristic;
-import org.elasticsearch.search.aggregations.bucket.significant.heuristics.SignificanceHeuristic;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-public class SignificantTermsTests extends BaseAggregationTestCase<SignificantTermsAggregatorFactory> {
-
-    private static final String[] executionHints;
-
-    static {
-        ExecutionMode[] executionModes = ExecutionMode.values();
-        executionHints = new String[executionModes.length];
-        for (int i = 0; i < executionModes.length; i++) {
-            executionHints[i] = executionModes[i].toString();
-        }
-    }
-
-    @Override
-    protected SignificantTermsAggregatorFactory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        SignificantTermsAggregatorFactory factory = new SignificantTermsAggregatorFactory(name, ValuesSourceType.ANY, null);
-        String field = randomAsciiOfLengthBetween(3, 20);
-        int randomFieldBranch = randomInt(2);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        default:
-            fail();
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            int size = randomInt(4);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setRequiredSize(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(4);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardSize(shardSize);
-        }
-        if (randomBoolean()) {
-            int minDocCount = randomInt(4);
-            switch (minDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                minDocCount = randomInt();
-                break;
-            }
-            factory.bucketCountThresholds().setMinDocCount(minDocCount);
-        }
-        if (randomBoolean()) {
-            int shardMinDocCount = randomInt(4);
-            switch (shardMinDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardMinDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardMinDocCount(shardMinDocCount);
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(executionHints));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            IncludeExclude incExc = null;
-            switch (randomInt(5)) {
-            case 0:
-                incExc = new IncludeExclude(new RegExp("foobar"), null);
-                break;
-            case 1:
-                incExc = new IncludeExclude(null, new RegExp("foobaz"));
-                break;
-            case 2:
-                incExc = new IncludeExclude(new RegExp("foobar"), new RegExp("foobaz"));
-                break;
-            case 3:
-                SortedSet<BytesRef> includeValues = new TreeSet<>();
-                int numIncs = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs; i++) {
-                    includeValues.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues = null;
-                incExc = new IncludeExclude(includeValues, excludeValues);
-                break;
-            case 4:
-                SortedSet<BytesRef> includeValues2 = null;
-                SortedSet<BytesRef> excludeValues2 = new TreeSet<>();
-                int numExcs2 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs2; i++) {
-                    excludeValues2.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues2, excludeValues2);
-                break;
-            case 5:
-                SortedSet<BytesRef> includeValues3 = new TreeSet<>();
-                int numIncs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs3; i++) {
-                    includeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues3 = new TreeSet<>();
-                int numExcs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs3; i++) {
-                    excludeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues3, excludeValues3);
-                break;
-            default:
-                fail();
-            }
-            factory.includeExclude(incExc);
-        }
-        if (randomBoolean()) {
-            SignificanceHeuristic significanceHeuristic = null;
-            switch (randomInt(5)) {
-            case 0:
-                significanceHeuristic = PercentageScore.PROTOTYPE;
-                break;
-            case 1:
-                significanceHeuristic = new ChiSquare(randomBoolean(), randomBoolean());
-                break;
-            case 2:
-                significanceHeuristic = new GND(randomBoolean());
-                break;
-            case 3:
-                significanceHeuristic = new MutualInformation(randomBoolean(), randomBoolean());
-                break;
-            case 4:
-                significanceHeuristic = new ScriptHeuristic(new Script("foo"));
-                break;
-            case 5:
-                significanceHeuristic = JLHScore.PROTOTYPE;
-                break;
-            default:
-                fail();
-            }
-            factory.significanceHeuristic(significanceHeuristic);
-        }
-        if (randomBoolean()) {
-            factory.backgroundFilter(QueryBuilders.termsQuery("foo", "bar"));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java
deleted file mode 100644
index fb57b6d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/TermsTests.java
+++ /dev/null
@@ -1,232 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.RegExp;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory;
-import org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.ExecutionMode;
-import org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude;
-import org.elasticsearch.search.aggregations.support.ValuesSourceType;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-public class TermsTests extends BaseAggregationTestCase<TermsAggregatorFactory> {
-
-    private static final String[] executionHints;
-
-    static {
-        ExecutionMode[] executionModes = ExecutionMode.values();
-        executionHints = new String[executionModes.length];
-        for (int i = 0; i < executionModes.length; i++) {
-            executionHints[i] = executionModes[i].toString();
-        }
-    }
-
-    @Override
-    protected TermsAggregatorFactory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        TermsAggregatorFactory factory = new TermsAggregatorFactory(name, ValuesSourceType.ANY, null);
-        String field = randomAsciiOfLengthBetween(3, 20);
-        int randomFieldBranch = randomInt(2);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        default:
-            fail();
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            int size = randomInt(4);
-            switch (size) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                size = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setRequiredSize(size);
-
-        }
-        if (randomBoolean()) {
-            int shardSize = randomInt(4);
-            switch (shardSize) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardSize = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardSize(shardSize);
-        }
-        if (randomBoolean()) {
-            int minDocCount = randomInt(4);
-            switch (minDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                minDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setMinDocCount(minDocCount);
-        }
-        if (randomBoolean()) {
-            int shardMinDocCount = randomInt(4);
-            switch (shardMinDocCount) {
-            case 0:
-                break;
-            case 1:
-            case 2:
-            case 3:
-            case 4:
-                shardMinDocCount = randomInt();
-                break;
-            default:
-                fail();
-            }
-            factory.bucketCountThresholds().setShardMinDocCount(shardMinDocCount);
-        }
-        if (randomBoolean()) {
-            factory.collectMode(randomFrom(SubAggCollectionMode.values()));
-        }
-        if (randomBoolean()) {
-            factory.executionHint(randomFrom(executionHints));
-        }
-        if (randomBoolean()) {
-            factory.format("###.##");
-        }
-        if (randomBoolean()) {
-            IncludeExclude incExc = null;
-            switch (randomInt(5)) {
-            case 0:
-                incExc = new IncludeExclude(new RegExp("foobar"), null);
-                break;
-            case 1:
-                incExc = new IncludeExclude(null, new RegExp("foobaz"));
-                break;
-            case 2:
-                incExc = new IncludeExclude(new RegExp("foobar"), new RegExp("foobaz"));
-                break;
-            case 3:
-                SortedSet<BytesRef> includeValues = new TreeSet<>();
-                int numIncs = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs; i++) {
-                    includeValues.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues = null;
-                incExc = new IncludeExclude(includeValues, excludeValues);
-                break;
-            case 4:
-                SortedSet<BytesRef> includeValues2 = null;
-                SortedSet<BytesRef> excludeValues2 = new TreeSet<>();
-                int numExcs2 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs2; i++) {
-                    excludeValues2.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues2, excludeValues2);
-                break;
-            case 5:
-                SortedSet<BytesRef> includeValues3 = new TreeSet<>();
-                int numIncs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numIncs3; i++) {
-                    includeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                SortedSet<BytesRef> excludeValues3 = new TreeSet<>();
-                int numExcs3 = randomIntBetween(1, 20);
-                for (int i = 0; i < numExcs3; i++) {
-                    excludeValues3.add(new BytesRef(randomAsciiOfLengthBetween(1, 30)));
-                }
-                incExc = new IncludeExclude(includeValues3, excludeValues3);
-                break;
-            default:
-                fail();
-            }
-            factory.includeExclude(incExc);
-        }
-        if (randomBoolean()) {
-            List<Terms.Order> order = randomOrder();
-            factory.order(order);
-        }
-        if (randomBoolean()) {
-            factory.showTermDocCountError(randomBoolean());
-        }
-        return factory;
-    }
-
-    private List<Terms.Order> randomOrder() {
-        List<Terms.Order> orders = new ArrayList<>();
-        switch (randomInt(4)) {
-        case 0:
-            orders.add(Terms.Order.term(randomBoolean()));
-            break;
-        case 1:
-            orders.add(Terms.Order.count(randomBoolean()));
-            break;
-        case 2:
-            orders.add(Terms.Order.aggregation(randomAsciiOfLengthBetween(3, 20), randomBoolean()));
-            break;
-        case 3:
-            orders.add(Terms.Order.aggregation(randomAsciiOfLengthBetween(3, 20), randomAsciiOfLengthBetween(3, 20), randomBoolean()));
-            break;
-        case 4:
-            int numOrders = randomIntBetween(1, 3);
-            for (int i = 0; i < numOrders; i++) {
-                orders.addAll(randomOrder());
-            }
-            break;
-        default:
-            fail();
-        }
-        return orders;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
index 4ffa860..cd7dadd 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParserTests.java
@@ -18,52 +18,40 @@
  */
 package org.elasticsearch.search.aggregations.bucket.geogrid;
 
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.search.SearchParseException;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.TestSearchContext;
 
 public class GeoHashGridParserTests extends ESTestCase {
     public void testParseValidFromInts() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         int precision = randomIntBetween(1, 12);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(
                 "{\"field\":\"my_loc\", \"precision\":" + precision + ", \"size\": 500, \"shard_size\": 550}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         // can create a factory
-        assertNotNull(parser.parse("geohash_grid", stParser, parseContext));
+        assertNotNull(parser.parse("geohash_grid", stParser, searchContext));
     }
 
     public void testParseValidFromStrings() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         int precision = randomIntBetween(1, 12);
         XContentParser stParser = JsonXContent.jsonXContent.createParser(
                 "{\"field\":\"my_loc\", \"precision\":\"" + precision + "\", \"size\": \"500\", \"shard_size\": \"550\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         // can create a factory
-        assertNotNull(parser.parse("geohash_grid", stParser, parseContext));
+        assertNotNull(parser.parse("geohash_grid", stParser, searchContext));
     }
 
     public void testParseErrorOnNonIntPrecision() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":\"2.0\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
         } catch (NumberFormatException ex) {
             assertEquals("For input string: \"2.0\"", ex.getMessage());
@@ -71,31 +59,23 @@ public class GeoHashGridParserTests extends ESTestCase {
     }
 
     public void testParseErrorOnBooleanPrecision() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":false}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
-        } catch (ParsingException ex) {
-            assertEquals("Unexpected token VALUE_BOOLEAN [precision] in [geohash_grid].", ex.getMessage());
+        } catch (SearchParseException ex) {
+            assertEquals("Unexpected token VALUE_BOOLEAN in [geohash_grid].", ex.getMessage());
         }
     }
 
     public void testParseErrorOnPrecisionOutOfRange() throws Exception {
+        SearchContext searchContext = new TestSearchContext();
         XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"my_loc\", \"precision\":\"13\"}");
-        QueryParseContext parseContext = new QueryParseContext(null);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        XContentParser.Token token = stParser.nextToken();
-        assertSame(XContentParser.Token.START_OBJECT, token);
         GeoHashGridParser parser = new GeoHashGridParser();
         try {
-            parser.parse("geohash_grid", stParser, parseContext);
+            parser.parse("geohash_grid", stParser, searchContext);
             fail();
         } catch (IllegalArgumentException ex) {
             assertEquals("Invalid geohash aggregation precision of 13. Must be between 1 and 12.", ex.getMessage());
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
index 3a90c8e..0cb799d 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregatorTests.java
@@ -124,12 +124,10 @@ public class NestedAggregatorTests extends ESSingleNodeTestCase {
         AggregationContext context = new AggregationContext(searchContext);
 
         AggregatorFactories.Builder builder = AggregatorFactories.builder();
-        NestedAggregator.Factory factory = new NestedAggregator.Factory("test", "nested_field");
-        builder.addAggregator(factory);
+        builder.addAggregator(new NestedAggregator.Factory("test", "nested_field"));
         AggregatorFactories factories = builder.build();
         searchContext.aggregations(new SearchContextAggregations(factories));
-        factories.init(context);
-        Aggregator[] aggs = factories.createTopLevelAggregators();
+        Aggregator[] aggs = factories.createTopLevelAggregators(context);
         BucketCollector collector = BucketCollector.wrap(Arrays.asList(aggs));
         collector.preCollection();
         // A regular search always exclude nested docs, so we use NonNestedDocsFilter.INSTANCE here (otherwise MatchAllDocsQuery would be sufficient)
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java
deleted file mode 100644
index 59ceb4d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/NestedTests.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.nested.NestedAggregator.Factory;
-
-public class NestedTests extends BaseAggregationTestCase<NestedAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        return new Factory(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(3, 40));
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java
deleted file mode 100644
index 7feecd8..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedTests.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.bucket.nested;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregator.Factory;
-
-public class ReverseNestedTests extends BaseAggregationTestCase<ReverseNestedAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.path(randomAsciiOfLengthBetween(3, 40));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
index 865a351..0fe9113 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java
@@ -21,17 +21,12 @@ package org.elasticsearch.search.aggregations.bucket.significant;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.Version;
-import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.InputStreamStreamInput;
 import org.elasticsearch.common.io.stream.OutputStreamStreamOutput;
-import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
@@ -132,7 +127,7 @@ public class SignificanceHeuristicTests extends ESTestCase {
 
     SignificanceHeuristic getRandomSignificanceheuristic() {
         List<SignificanceHeuristic> heuristics = new ArrayList<>();
-        heuristics.add(JLHScore.PROTOTYPE);
+        heuristics.add(JLHScore.INSTANCE);
         heuristics.add(new MutualInformation(randomBoolean(), randomBoolean()));
         heuristics.add(new GND(randomBoolean()));
         heuristics.add(new ChiSquare(randomBoolean(), randomBoolean()));
@@ -195,7 +190,7 @@ public class SignificanceHeuristicTests extends ESTestCase {
     public void testBuilderAndParser() throws Exception {
 
         Set<SignificanceHeuristicParser> parsers = new HashSet<>();
-        SignificanceHeuristicParserMapper heuristicParserMapper = new SignificanceHeuristicParserMapper(parsers);
+        SignificanceHeuristicParserMapper heuristicParserMapper = new SignificanceHeuristicParserMapper(parsers, null);
         SearchContext searchContext = new SignificantTermsTestSearchContext();
 
         // test jlh with string
@@ -232,17 +227,11 @@ public class SignificanceHeuristicTests extends ESTestCase {
         checkParseException(heuristicParserMapper, searchContext, faultyHeuristicdefinition, expectedError);
     }
 
-    protected void checkParseException(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext,
-            String faultyHeuristicDefinition, String expectedError) throws IOException {
-
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, new HashMap<String, QueryParser<?>>());
+    protected void checkParseException(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, String faultyHeuristicDefinition, String expectedError) throws IOException {
         try {
             XContentParser stParser = JsonXContent.jsonXContent.createParser("{\"field\":\"text\", " + faultyHeuristicDefinition + ",\"min_doc_count\":200}");
-            QueryParseContext parseContext = new QueryParseContext(registry);
-            parseContext.reset(stParser);
-            parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
             stParser.nextToken();
-            new SignificantTermsParser(heuristicParserMapper, registry).parse("testagg", stParser, parseContext);
+            new SignificantTermsParser(heuristicParserMapper).parse("testagg", stParser, searchContext);
             fail();
         } catch (ElasticsearchParseException e) {
             assertTrue(e.getMessage().contains(expectedError));
@@ -258,15 +247,9 @@ public class SignificanceHeuristicTests extends ESTestCase {
         return parseSignificanceHeuristic(heuristicParserMapper, searchContext, stParser);
     }
 
-    private SignificanceHeuristic parseSignificanceHeuristic(SignificanceHeuristicParserMapper heuristicParserMapper,
-            SearchContext searchContext, XContentParser stParser) throws IOException {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, new HashMap<String, QueryParser<?>>());
-        QueryParseContext parseContext = new QueryParseContext(registry);
-        parseContext.reset(stParser);
-        parseContext.parseFieldMatcher(ParseFieldMatcher.STRICT);
+    private SignificanceHeuristic parseSignificanceHeuristic(SignificanceHeuristicParserMapper heuristicParserMapper, SearchContext searchContext, XContentParser stParser) throws IOException {
         stParser.nextToken();
-        SignificantTermsAggregatorFactory aggregatorFactory = (SignificantTermsAggregatorFactory) new SignificantTermsParser(
-                heuristicParserMapper, registry).parse("testagg", stParser, parseContext);
+        SignificantTermsAggregatorFactory aggregatorFactory = (SignificantTermsAggregatorFactory) new SignificantTermsParser(heuristicParserMapper).parse("testagg", stParser, searchContext);
         stParser.nextToken();
         assertThat(aggregatorFactory.getBucketCountThresholds().getMinDocCount(), equalTo(200l));
         assertThat(stParser.currentToken(), equalTo(null));
@@ -382,14 +365,14 @@ public class SignificanceHeuristicTests extends ESTestCase {
         testBackgroundAssertions(new MutualInformation(true, true), new MutualInformation(true, false));
         testBackgroundAssertions(new ChiSquare(true, true), new ChiSquare(true, false));
         testBackgroundAssertions(new GND(true), new GND(false));
-        testAssertions(PercentageScore.PROTOTYPE);
-        testAssertions(JLHScore.PROTOTYPE);
+        testAssertions(PercentageScore.INSTANCE);
+        testAssertions(JLHScore.INSTANCE);
     }
 
     public void testBasicScoreProperties() {
-        basicScoreProperties(JLHScore.PROTOTYPE, true);
+        basicScoreProperties(JLHScore.INSTANCE, true);
         basicScoreProperties(new GND(true), true);
-        basicScoreProperties(PercentageScore.PROTOTYPE, true);
+        basicScoreProperties(PercentageScore.INSTANCE, true);
         basicScoreProperties(new MutualInformation(true, true), false);
         basicScoreProperties(new ChiSquare(true, true), false);
     }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java
deleted file mode 100644
index f51a1a8..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AbstractNumericMetricTestCase.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-
-public abstract class AbstractNumericMetricTestCase<AF extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric, AF>>
-        extends BaseAggregationTestCase<AF> {
-
-    @Override
-    protected final AF createTestAggregatorFactory() {
-        AF factory = doCreateTestAggregatorFactory();
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-    protected abstract AF doCreateTestAggregatorFactory();
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java
deleted file mode 100644
index fa51fb2..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
-
-public class AvgTests extends AbstractNumericMetricTestCase<AvgAggregator.Factory> {
-
-    @Override
-    protected AvgAggregator.Factory doCreateTestAggregatorFactory() {
-        return new AvgAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java
deleted file mode 100644
index 504254d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.stats.extended.ExtendedStatsAggregator;
-
-public class ExtendedStatsTests extends AbstractNumericMetricTestCase<ExtendedStatsAggregator.Factory> {
-
-    @Override
-    protected ExtendedStatsAggregator.Factory doCreateTestAggregatorFactory() {
-        ExtendedStatsAggregator.Factory factory = new ExtendedStatsAggregator.Factory("foo");
-        if (randomBoolean()) {
-            factory.sigma(randomDoubleBetween(0.0, 10.0, true));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java
deleted file mode 100644
index 1072cdb..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FilterTests.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator;
-import org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.Factory;
-
-public class FilterTests extends BaseAggregationTestCase<FilterAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20),
-                QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        // NORELEASE make RandomQueryBuilder work outside of the
-        // AbstractQueryTestCase
-        // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java
deleted file mode 100644
index 4c67155..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/FiltersTests.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.Factory;
-import org.elasticsearch.search.aggregations.bucket.filters.FiltersAggregator.KeyedFilter;
-
-import java.util.ArrayList;
-import java.util.List;
-
-public class FiltersTests extends BaseAggregationTestCase<FiltersAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-
-        int size = randomIntBetween(1, 20);
-        Factory factory;
-        if (randomBoolean()) {
-            List<KeyedFilter> filters = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                // NORELEASE make RandomQueryBuilder work outside of the
-                // AbstractQueryTestCase
-                // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-                filters.add(new KeyedFilter(randomAsciiOfLengthBetween(1, 20),
-                        QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20))));
-            }
-            factory = new Factory(randomAsciiOfLengthBetween(1, 20), filters);
-        } else {
-            QueryBuilder<?>[] filters = new QueryBuilder<?>[size];
-            for (int i = 0; i < size; i++) {
-                // NORELEASE make RandomQueryBuilder work outside of the
-                // AbstractQueryTestCase
-                // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-                filters[i] = QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-            }
-            factory = new Factory(randomAsciiOfLengthBetween(1, 20), filters);
-        }
-        if (randomBoolean()) {
-            factory.otherBucket(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.otherBucketKey(randomAsciiOfLengthBetween(1, 20));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java
deleted file mode 100644
index e4bbffe..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoBoundsTests.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator;
-import org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator.Factory;
-
-public class GeoBoundsTests extends BaseAggregationTestCase<GeoBoundsAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        String field = randomAsciiOfLengthBetween(3, 20);
-        factory.field(field);
-        if (randomBoolean()) {
-            factory.wrapLongitude(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.missing("0,0");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java
deleted file mode 100644
index 28c426a..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/GeoCentroidTests.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator;
-import org.elasticsearch.search.aggregations.metrics.geocentroid.GeoCentroidAggregator.Factory;
-
-public class GeoCentroidTests extends BaseAggregationTestCase<GeoCentroidAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("0,0");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/HDRPercentileRanksTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/HDRPercentileRanksTests.java
deleted file mode 100644
index 8f65176..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/HDRPercentileRanksTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregator;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentileRanksAggregator.Factory;
-
-public class HDRPercentileRanksTests extends BaseAggregationTestCase<HDRPercentileRanksAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        int valuesSize = randomIntBetween(1, 20);
-        double[] values = new double[valuesSize];
-        for (int i = 0; i < valuesSize; i++) {
-            values[i] = randomDouble() * 100;
-        }
-        factory.values(values);
-        if (randomBoolean()) {
-            factory.numberOfSignificantValueDigits(randomIntBetween(0, 5));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/HDRPercentilesTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/HDRPercentilesTests.java
deleted file mode 100644
index c9785e7..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/HDRPercentilesTests.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentilesAggregator;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.HDRPercentilesAggregator.Factory;
-
-public class HDRPercentilesTests extends BaseAggregationTestCase<HDRPercentilesAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int percentsSize = randomIntBetween(1, 20);
-            double[] percents = new double[percentsSize];
-            for (int i = 0; i < percentsSize; i++) {
-                percents[i] = randomDouble() * 100;
-            }
-            factory.percents(percents);
-        }
-        if (randomBoolean()) {
-            factory.numberOfSignificantValueDigits(randomIntBetween(0, 5));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.format("###.00");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java
deleted file mode 100644
index cff4888..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
-
-public class MaxTests extends AbstractNumericMetricTestCase<MaxAggregator.Factory> {
-
-    @Override
-    protected MaxAggregator.Factory doCreateTestAggregatorFactory() {
-        return new MaxAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java
deleted file mode 100644
index b24ccca..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator.Factory;
-
-public class MinTests extends AbstractNumericMetricTestCase<MinAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory() {
-        return new MinAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java
deleted file mode 100644
index 16f5aaf..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/MissingTests.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.bucket.missing.MissingAggregator;
-
-public class MissingTests extends BaseAggregationTestCase<MissingAggregator.Factory> {
-
-    @Override
-    protected final MissingAggregator.Factory createTestAggregatorFactory() {
-        MissingAggregator.Factory factory = new MissingAggregator.Factory("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java
deleted file mode 100644
index be0aec6..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator;
-import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregator.Factory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class ScriptedMetricTests extends BaseAggregationTestCase<ScriptedMetricAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.initScript(randomScript("initScript"));
-        }
-        factory.mapScript(randomScript("mapScript"));
-        if (randomBoolean()) {
-            factory.combineScript(randomScript("combineScript"));
-        }
-        if (randomBoolean()) {
-            factory.reduceScript(randomScript("reduceScript"));
-        }
-        if (randomBoolean()) {
-            Map<String, Object> params = new HashMap<String, Object>();
-            params.put("foo", "bar");
-            factory.params(params);
-        }
-        return factory;
-    }
-
-    private Script randomScript(String script) {
-        if (randomBoolean()) {
-            return new Script(script);
-        } else {
-            return new Script(script, randomFrom(ScriptType.values()), randomFrom("my_lang", null), null);
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java
deleted file mode 100644
index a09958d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.stats.StatsAggregator;
-
-public class StatsTests extends AbstractNumericMetricTestCase<StatsAggregator.Factory> {
-
-    @Override
-    protected StatsAggregator.Factory doCreateTestAggregatorFactory() {
-        return new StatsAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java
deleted file mode 100644
index 0d8d61e..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-
-public class SumTests extends AbstractNumericMetricTestCase<SumAggregator.Factory> {
-
-    @Override
-    protected SumAggregator.Factory doCreateTestAggregatorFactory() {
-        return new SumAggregator.Factory("foo");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentileRanksTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentileRanksTests.java
deleted file mode 100644
index 6a43c91..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentileRanksTests.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregator;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentileRanksAggregator.Factory;
-
-public class TDigestPercentileRanksTests extends BaseAggregationTestCase<TDigestPercentileRanksAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        int valuesSize = randomIntBetween(1, 20);
-        double[] values = new double[valuesSize];
-        for (int i = 0; i < valuesSize; i++) {
-            values[i] = randomDouble() * 100;
-        }
-        factory.values(values);
-        if (randomBoolean()) {
-            factory.compression(randomDoubleBetween(10, 40000, true));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentilesTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentilesTests.java
deleted file mode 100644
index 898bda7..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentilesTests.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregator;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.TDigestPercentilesAggregator.Factory;
-
-public class TDigestPercentilesTests extends BaseAggregationTestCase<TDigestPercentilesAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        Factory factory = new Factory(randomAsciiOfLengthBetween(1, 20));
-        if (randomBoolean()) {
-            factory.keyed(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int percentsSize = randomIntBetween(1, 20);
-            double[] percents = new double[percentsSize];
-            for (int i = 0; i < percentsSize; i++) {
-                percents[i] = randomDouble() * 100;
-            }
-            factory.percents(percents);
-        }
-        if (randomBoolean()) {
-            factory.compression(randomDoubleBetween(10, 40000, true));
-        }
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        if (randomBoolean()) {
-            factory.format("###.00");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
index 1788213..65e71fe 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
@@ -256,7 +256,7 @@ public class TopHitsIT extends ESIntegTestCase {
                         .executionHint(randomExecutionHint())
                         .field(TERMS_AGGS_FIELD)
                         .subAggregation(
-                                topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
                         )
                 )
                 .get();
@@ -352,7 +352,7 @@ public class TopHitsIT extends ESIntegTestCase {
                         .executionHint(randomExecutionHint())
                         .collectMode(SubAggCollectionMode.BREADTH_FIRST)
                         .field(TERMS_AGGS_FIELD)
-                        .subAggregation(topHits("hits").size(3))
+                        .subAggregation(topHits("hits").setSize(3))
                 ).get();
 
         assertSearchResponse(response);
@@ -403,9 +403,9 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
-                                                .from(from)
-                                                .size(size)
+                                        topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                                .setFrom(from)
+                                                .setSize(size)
                                 )
                 )
                 .get();
@@ -447,7 +447,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .field(TERMS_AGGS_FIELD)
                                 .order(Terms.Order.aggregation("max_sort", false))
                                 .subAggregation(
-                                        topHits("hits").sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC)).trackScores(true)
+                                        topHits("hits").addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC)).setTrackScores(true)
                                 )
                                 .subAggregation(
                                         max("max_sort").field(SORT_FIELD)
@@ -487,7 +487,7 @@ public class TopHitsIT extends ESIntegTestCase {
                 .setQuery(matchQuery("text", "term rare"))
                 .addAggregation(
                         terms("terms").executionHint(randomExecutionHint()).field("group")
-                                .order(Terms.Order.aggregation("max_score", false)).subAggregation(topHits("hits").size(1))
+                                .order(Terms.Order.aggregation("max_score", false)).subAggregation(topHits("hits").setSize(1))
                                 .subAggregation(max("max_score").field("value"))).get();
         assertSearchResponse(response);
 
@@ -529,14 +529,14 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").size(1)
+                                        topHits("hits").setSize(1)
                                             .highlighter(new HighlightBuilder().field("text"))
-                                            .explain(true)
-                                            .field("text")
-                                            .fieldDataField("field1")
-                                            .scriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
-                                            .fetchSource("text", null)
-                                            .version(true)
+                                            .setExplain(true)
+                                            .addField("text")
+                                            .addFieldDataField("field1")
+                                            .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
+                                            .setFetchSource("text", null)
+                                            .setVersion(true)
                                 )
                 )
                 .get();
@@ -586,7 +586,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                     .executionHint(randomExecutionHint())
                                     .field(TERMS_AGGS_FIELD)
                                     .subAggregation(
-                                            topHits("hits").sort(SortBuilders.fieldSort("xyz").order(SortOrder.DESC))
+                                            topHits("hits").addSort(SortBuilders.fieldSort("xyz").order(SortOrder.DESC))
                                     )
                     ).get();
             fail();
@@ -650,9 +650,9 @@ public class TopHitsIT extends ESIntegTestCase {
                                     .field("group")
                                     .subAggregation(
                                             topHits("hits")
-                                                    .trackScores(trackScore)
-                                                    .size(1)
-                                                    .sort("_id", SortOrder.DESC)
+                                                    .setTrackScores(trackScore)
+                                                    .setSize(1)
+                                                    .addSort("_id", SortOrder.DESC)
                                     )
                     )
                     .get();
@@ -696,7 +696,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                         terms("users")
                                                 .field("comments.user")
                                                 .subAggregation(
-                                                        topHits("top-comments").sort("comments.date", SortOrder.ASC)
+                                                        topHits("top-comments").addSort("comments.date", SortOrder.ASC)
                                                 )
                                 )
                 )
@@ -746,10 +746,10 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .subAggregation(
                                     nested("to-reviewers").path("comments.reviewers").subAggregation(
                                             // Also need to sort on _doc because there are two reviewers with the same name
-                                            topHits("top-reviewers").sort("comments.reviewers.name", SortOrder.ASC).sort("_doc", SortOrder.DESC).size(7)
+                                            topHits("top-reviewers").addSort("comments.reviewers.name", SortOrder.ASC).addSort("_doc", SortOrder.DESC).setSize(7)
                                     )
                                 )
-                                .subAggregation(topHits("top-comments").sort("comments.date", SortOrder.DESC).size(4))
+                                .subAggregation(topHits("top-comments").addSort("comments.date", SortOrder.DESC).setSize(4))
                 ).get();
         assertNoFailures(searchResponse);
 
@@ -849,10 +849,10 @@ public class TopHitsIT extends ESIntegTestCase {
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "comment").queryName("test")))
                 .addAggregation(
                         nested("to-comments").path("comments").subAggregation(
-                                topHits("top-comments").size(1).highlighter(new HighlightBuilder().field(hlField)).explain(true)
-                                                .fieldDataField("comments.user")
-                                        .scriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).fetchSource("message", null)
-                                        .version(true).sort("comments.date", SortOrder.ASC))).get();
+                                topHits("top-comments").setSize(1).highlighter(new HighlightBuilder().field(hlField)).setExplain(true)
+                                                .addFieldDataField("comments.user")
+                                        .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).setFetchSource("message", null)
+                                        .setVersion(true).addSort("comments.date", SortOrder.ASC))).get();
         assertHitCount(searchResponse, 2);
         Nested nested = searchResponse.getAggregations().get("to-comments");
         assertThat(nested.getDocCount(), equalTo(4l));
@@ -901,7 +901,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                                 .path("comments")
                                                 .subAggregation(topHits("comments")
                                                         .highlighter(new HighlightBuilder().field(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text"))))
-                                                        .sort("comments.id", SortOrder.ASC))
+                                                        .addSort("comments.id", SortOrder.ASC))
                                 )
                 )
                 .get();
@@ -938,7 +938,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .executionHint(randomExecutionHint())
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
-                                        topHits("hits").size(ArrayUtil.MAX_ARRAY_LENGTH - 1).sort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
+                                        topHits("hits").setSize(ArrayUtil.MAX_ARRAY_LENGTH - 1).addSort(SortBuilders.fieldSort(SORT_FIELD).order(SortOrder.DESC))
                                 )
                 )
                 .get();
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java
deleted file mode 100644
index f53c196..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsTests.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.tophits.TopHitsAggregator;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilderTests;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-
-import java.util.ArrayList;
-import java.util.List;;
-
-public class TopHitsTests extends BaseAggregationTestCase<TopHitsAggregator.Factory> {
-
-    @Override
-    protected final TopHitsAggregator.Factory createTestAggregatorFactory() {
-        TopHitsAggregator.Factory factory = new TopHitsAggregator.Factory("foo");
-        if (randomBoolean()) {
-            factory.from(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            factory.size(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            factory.explain(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.version(randomBoolean());
-        }
-        if (randomBoolean()) {
-            factory.trackScores(randomBoolean());
-        }
-        if (randomBoolean()) {
-            int fieldsSize = randomInt(25);
-            List<String> fields = new ArrayList<>(fieldsSize);
-            for (int i = 0; i < fieldsSize; i++) {
-                fields.add(randomAsciiOfLengthBetween(5, 50));
-            }
-            factory.fields(fields);
-        }
-        if (randomBoolean()) {
-            int fieldDataFieldsSize = randomInt(25);
-            for (int i = 0; i < fieldDataFieldsSize; i++) {
-                factory.fieldDataField(randomAsciiOfLengthBetween(5, 50));
-            }
-        }
-        if (randomBoolean()) {
-            int scriptFieldsSize = randomInt(25);
-            for (int i = 0; i < scriptFieldsSize; i++) {
-                if (randomBoolean()) {
-                    factory.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"), randomBoolean());
-                } else {
-                    factory.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"));
-                }
-            }
-        }
-        if (randomBoolean()) {
-            FetchSourceContext fetchSourceContext;
-            int branch = randomInt(5);
-            String[] includes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < includes.length; i++) {
-                includes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            String[] excludes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < excludes.length; i++) {
-                excludes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            switch (branch) {
-            case 0:
-                fetchSourceContext = new FetchSourceContext(randomBoolean());
-                break;
-            case 1:
-                fetchSourceContext = new FetchSourceContext(includes, excludes);
-                break;
-            case 2:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-                break;
-            case 3:
-                fetchSourceContext = new FetchSourceContext(true, includes, excludes);
-                break;
-            case 4:
-                fetchSourceContext = new FetchSourceContext(includes);
-                break;
-            case 5:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20));
-                break;
-            default:
-                throw new IllegalStateException();
-            }
-            factory.fetchSource(fetchSourceContext);
-        }
-        if (randomBoolean()) {
-            int numSorts = randomIntBetween(1, 5);
-            for (int i = 0; i < numSorts; i++) {
-                int branch = randomInt(5);
-                switch (branch) {
-                case 0:
-                    factory.sort(SortBuilders.fieldSort(randomAsciiOfLengthBetween(5, 20)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 1:
-                    factory.sort(SortBuilders.geoDistanceSort(randomAsciiOfLengthBetween(5, 20))
-                            .geohashes(AbstractQueryTestCase.randomGeohash(1, 12)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 2:
-                    factory.sort(SortBuilders.scoreSort().order(randomFrom(SortOrder.values())));
-                    break;
-                case 3:
-                    factory.sort(SortBuilders.scriptSort(new Script("foo"), "number").order(randomFrom(SortOrder.values())));
-                    break;
-                case 4:
-                    factory.sort(randomAsciiOfLengthBetween(5, 20));
-                    break;
-                case 5:
-                    factory.sort(randomAsciiOfLengthBetween(5, 20), randomFrom(SortOrder.values()));
-                    break;
-                }
-            }
-        }
-        if (randomBoolean()) {
-            factory.highlighter(HighlightBuilderTests.randomHighlighterBuilder());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java
deleted file mode 100644
index b400bff..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-import org.elasticsearch.search.aggregations.metrics.valuecount.ValueCountAggregator;
-
-public class ValueCountTests extends BaseAggregationTestCase<ValueCountAggregator.Factory> {
-
-    @Override
-    protected final ValueCountAggregator.Factory createTestAggregatorFactory() {
-        ValueCountAggregator.Factory factory = new ValueCountAggregator.Factory("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java
deleted file mode 100644
index 5b2bbd1..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/cardinality/CardinalityTests.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.metrics.cardinality;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.BaseAggregationTestCase;
-
-public class CardinalityTests extends BaseAggregationTestCase<CardinalityAggregatorFactory> {
-
-    @Override
-    protected final CardinalityAggregatorFactory createTestAggregatorFactory() {
-        CardinalityAggregatorFactory factory = new CardinalityAggregatorFactory("foo", null);
-        String field = randomNumericField();
-        int randomFieldBranch = randomInt(3);
-        switch (randomFieldBranch) {
-        case 0:
-            factory.field(field);
-            break;
-        case 1:
-            factory.field(field);
-            factory.script(new Script("_value + 1"));
-            break;
-        case 2:
-            factory.script(new Script("doc[" + field + "] + 1"));
-            break;
-        }
-        if (randomBoolean()) {
-            factory.missing("MISSING");
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java
deleted file mode 100644
index d9b5496..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptTests.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketscript.BucketScriptPipelineAggregator.Factory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class BucketScriptTests extends BasePipelineAggregationTestCase<BucketScriptPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        Map<String, String> bucketsPaths = new HashMap<>();
-        int numBucketPaths = randomIntBetween(1, 10);
-        for (int i = 0; i < numBucketPaths; i++) {
-            bucketsPaths.put(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(1, 40));
-        }
-        Script script;
-        if (randomBoolean()) {
-            script = new Script("script");
-        } else {
-            Map<String, Object> params = null;
-            if (randomBoolean()) {
-                params = new HashMap<String, Object>();
-                params.put("foo", "bar");
-            }
-            script = new Script("script", randomFrom(ScriptType.values()), randomFrom("my_lang", null), params);
-        }
-        Factory factory = new Factory(name, bucketsPaths, script);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java
deleted file mode 100644
index 3d5f4d1..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorTests.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.having.BucketSelectorPipelineAggregator.Factory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-public class BucketSelectorTests extends BasePipelineAggregationTestCase<BucketSelectorPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        Map<String, String> bucketsPaths = new HashMap<>();
-        int numBucketPaths = randomIntBetween(1, 10);
-        for (int i = 0; i < numBucketPaths; i++) {
-            bucketsPaths.put(randomAsciiOfLengthBetween(1, 20), randomAsciiOfLengthBetween(1, 40));
-        }
-        Script script;
-        if (randomBoolean()) {
-            script = new Script("script");
-        } else {
-            Map<String, Object> params = null;
-            if (randomBoolean()) {
-                params = new HashMap<String, Object>();
-                params.put("foo", "bar");
-            }
-            script = new Script("script", randomFrom(ScriptType.values()), randomFrom("my_lang", null), params);
-        }
-        Factory factory = new Factory(name, bucketsPaths, script);
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java
deleted file mode 100644
index 793cd84..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumTests.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.cumulativesum.CumulativeSumPipelineAggregator.Factory;
-
-public class CumulativeSumTests extends BasePipelineAggregationTestCase<CumulativeSumPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String[] bucketsPaths = new String[1];
-        bucketsPaths[0] = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPaths);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java
deleted file mode 100644
index b2cd1d4..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeTests.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.derivative.DerivativePipelineAggregator.Factory;
-
-public class DerivativeTests extends BasePipelineAggregationTestCase<DerivativePipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String[] bucketsPaths = new String[1];
-        bucketsPaths[0] = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPaths);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                factory.units(String.valueOf(randomInt()));
-            } else {
-                factory.units(String.valueOf(randomIntBetween(1, 10) + randomFrom("s", "m", "h", "d", "w", "M", "y")));
-            }
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
index 9a6559a..e962e90 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregationHelperTests.java
@@ -20,11 +20,11 @@
 package org.elasticsearch.search.aggregations.pipeline;
 
 
-import org.elasticsearch.search.aggregations.metrics.avg.AvgAggregator;
-import org.elasticsearch.search.aggregations.metrics.max.MaxAggregator;
-import org.elasticsearch.search.aggregations.metrics.min.MinAggregator;
-import org.elasticsearch.search.aggregations.metrics.sum.SumAggregator;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
+import org.elasticsearch.search.aggregations.metrics.avg.AvgBuilder;
+import org.elasticsearch.search.aggregations.metrics.max.MaxBuilder;
+import org.elasticsearch.search.aggregations.metrics.min.MinBuilder;
+import org.elasticsearch.search.aggregations.metrics.sum.SumBuilder;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.ArrayList;
@@ -109,27 +109,27 @@ public class PipelineAggregationHelperTests extends ESTestCase {
      * @param values Array of values to compute metric for
      * @param metric A metric builder which defines what kind of metric should be returned for the values
      */
-    public static double calculateMetric(double[] values, ValuesSourceAggregatorFactory<?, ?> metric) {
+    public static double calculateMetric(double[] values, ValuesSourceMetricsAggregationBuilder metric) {
 
-        if (metric instanceof MinAggregator.Factory) {
+        if (metric instanceof MinBuilder) {
             double accumulator = Double.POSITIVE_INFINITY;
             for (double value : values) {
                 accumulator = Math.min(accumulator, value);
             }
             return accumulator;
-        } else if (metric instanceof MaxAggregator.Factory) {
+        } else if (metric instanceof MaxBuilder) {
             double accumulator = Double.NEGATIVE_INFINITY;
             for (double value : values) {
                 accumulator = Math.max(accumulator, value);
             }
             return accumulator;
-        } else if (metric instanceof SumAggregator.Factory) {
+        } else if (metric instanceof SumBuilder) {
             double accumulator = 0;
             for (double value : values) {
                 accumulator += value;
             }
             return accumulator;
-        } else if (metric instanceof AvgAggregator.Factory) {
+        } else if (metric instanceof AvgBuilder) {
             double accumulator = 0;
             for (double value : values) {
                 accumulator += value;
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java
deleted file mode 100644
index 03ec5b9..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SerialDifferenceTests.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.serialdiff.SerialDiffPipelineAggregator.Factory;
-
-public class SerialDifferenceTests extends BasePipelineAggregationTestCase<SerialDiffPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String[] bucketsPaths = new String[1];
-        bucketsPaths[0] = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPaths);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            factory.lag(randomIntBetween(1, 1000));
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java
deleted file mode 100644
index 8cfea91..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AbstractBucketMetricsTestCase.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-
-public abstract class AbstractBucketMetricsTestCase<PAF extends BucketMetricsFactory> extends BasePipelineAggregationTestCase<PAF> {
-
-    @Override
-    protected final PAF createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String[] bucketsPaths = new String[1];
-        bucketsPaths[0] = randomAsciiOfLengthBetween(3, 20);
-        PAF factory = doCreateTestAggregatorFactory(name, bucketsPaths);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        return factory;
-    }
-
-    protected abstract PAF doCreateTestAggregatorFactory(String name, String[] bucketsPaths);
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java
deleted file mode 100644
index f49c98d..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/AvgBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.avg.AvgBucketPipelineAggregator.Factory;
-
-public class AvgBucketTests extends AbstractBucketMetricsTestCase<AvgBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String[] bucketsPaths) {
-        return new Factory(name, bucketsPaths);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java
deleted file mode 100644
index 03d7c69..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/ExtendedStatsBucketTests.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.extended.ExtendedStatsBucketPipelineAggregator.Factory;
-
-public class ExtendedStatsBucketTests extends AbstractBucketMetricsTestCase<ExtendedStatsBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String[] bucketsPaths) {
-        Factory factory = new Factory(name, bucketsPaths);
-        if (randomBoolean()) {
-            factory.sigma(randomDoubleBetween(0.0, 10.0, false));
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java
deleted file mode 100644
index 74fc39e..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MaxBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.max.MaxBucketPipelineAggregator.Factory;
-
-public class MaxBucketTests extends AbstractBucketMetricsTestCase<MaxBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String[] bucketsPaths) {
-        return new Factory(name, bucketsPaths);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java
deleted file mode 100644
index bc8fd2a..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/MinBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.min.MinBucketPipelineAggregator.Factory;
-
-public class MinBucketTests extends AbstractBucketMetricsTestCase<MinBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String[] bucketsPaths) {
-        return new Factory(name, bucketsPaths);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java
deleted file mode 100644
index 6078584..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/PercentilesBucketTests.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.percentile.PercentilesBucketPipelineAggregator.Factory;
-
-public class PercentilesBucketTests extends AbstractBucketMetricsTestCase<PercentilesBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String[] bucketsPaths) {
-        Factory factory = new Factory(name, bucketsPaths);
-        if (randomBoolean()) {
-            int numPercents = randomIntBetween(1, 20);
-            double[] percents = new double[numPercents];
-            for (int i = 0; i < numPercents; i++) {
-                percents[i] = randomDoubleBetween(0.0, 100.0, false);
-            }
-            factory.percents(percents);
-        }
-        return factory;
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java
deleted file mode 100644
index 0aa8df0..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/StatsBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.stats.StatsBucketPipelineAggregator.Factory;
-
-public class StatsBucketTests extends AbstractBucketMetricsTestCase<StatsBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String[] bucketsPaths) {
-        return new Factory(name, bucketsPaths);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java
deleted file mode 100644
index a7d6b5a..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/SumBucketTests.java
+++ /dev/null
@@ -1,33 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.bucketmetrics;
-
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.bucketmetrics.sum.SumBucketPipelineAggregator.Factory;
-
-public class SumBucketTests extends AbstractBucketMetricsTestCase<SumBucketPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory doCreateTestAggregatorFactory(String name, String[] bucketsPaths) {
-        return new Factory(name, bucketsPaths);
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
index 35ecf95..90d4437 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java
@@ -26,6 +26,7 @@ import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.Bucket;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.metrics.avg.Avg;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests;
@@ -37,8 +38,6 @@ import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersM
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.LinearModel;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.MovAvgModelBuilder;
 import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -78,7 +77,7 @@ public class MovAvgIT extends ESIntegTestCase {
     static int period;
     static HoltWintersModel.SeasonalityType seasonalityType;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceAggregatorFactory<? extends ValuesSource, ? extends ValuesSourceAggregatorFactory<?, ?>> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -1356,8 +1355,7 @@ public class MovAvgIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceAggregatorFactory<? extends ValuesSource, ? extends ValuesSourceAggregatorFactory<?, ?>> randomMetric(String name,
-            String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java
deleted file mode 100644
index 6767a30..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgTests.java
+++ /dev/null
@@ -1,96 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.pipeline.moving.avg;
-
-import org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase;
-import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator;
-import org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgPipelineAggregator.Factory;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.EwmaModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltLinearModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.HoltWintersModel.SeasonalityType;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.LinearModel;
-import org.elasticsearch.search.aggregations.pipeline.movavg.models.SimpleModel;;
-
-public class MovAvgTests extends BasePipelineAggregationTestCase<MovAvgPipelineAggregator.Factory> {
-
-    @Override
-    protected Factory createTestAggregatorFactory() {
-        String name = randomAsciiOfLengthBetween(3, 20);
-        String[] bucketsPaths = new String[1];
-        bucketsPaths[0] = randomAsciiOfLengthBetween(3, 20);
-        Factory factory = new Factory(name, bucketsPaths);
-        if (randomBoolean()) {
-            factory.format(randomAsciiOfLengthBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            factory.gapPolicy(randomFrom(GapPolicy.values()));
-        }
-        if (randomBoolean()) {
-            switch (randomInt(4)) {
-            case 0:
-                factory.model(new SimpleModel());
-                factory.window(randomIntBetween(1, 100));
-                break;
-            case 1:
-                factory.model(new LinearModel());
-                factory.window(randomIntBetween(1, 100));
-                break;
-            case 2:
-                if (randomBoolean()) {
-                    factory.model(new EwmaModel());
-                    factory.window(randomIntBetween(1, 100));
-                } else {
-                    factory.model(new EwmaModel(randomDouble()));
-                    factory.window(randomIntBetween(1, 100));
-                }
-                break;
-            case 3:
-                if (randomBoolean()) {
-                    factory.model(new HoltLinearModel());
-                    factory.window(randomIntBetween(1, 100));
-                } else {
-                    factory.model(new HoltLinearModel(randomDouble(), randomDouble()));
-                    factory.window(randomIntBetween(1, 100));
-                }
-                break;
-            case 4:
-            default:
-                if (randomBoolean()) {
-                    factory.model(new HoltWintersModel());
-                    factory.window(randomIntBetween(2, 100));
-                } else {
-                    int period = randomIntBetween(1, 100);
-                    factory.model(new HoltWintersModel(randomDouble(), randomDouble(), randomDouble(), period,
-                            randomFrom(SeasonalityType.values()), randomBoolean()));
-                    factory.window(randomIntBetween(2 * period, 200 * period));
-                }
-                break;
-            }
-        }
-        factory.predict(randomIntBetween(1, 50));
-        if (factory.model().canBeMinimized() && randomBoolean()) {
-            factory.minimize(randomBoolean());
-        }
-        return factory;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
index d7825d2..aebd6a7 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java
@@ -25,11 +25,10 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.collect.EvictingQueue;
 import org.elasticsearch.search.aggregations.bucket.histogram.Histogram;
 import org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram;
+import org.elasticsearch.search.aggregations.metrics.ValuesSourceMetricsAggregationBuilder;
 import org.elasticsearch.search.aggregations.pipeline.BucketHelpers;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests;
 import org.elasticsearch.search.aggregations.pipeline.SimpleValue;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.hamcrest.Matchers;
 
@@ -61,7 +60,7 @@ public class SerialDiffIT extends ESIntegTestCase {
     static int numBuckets;
     static int lag;
     static BucketHelpers.GapPolicy gapPolicy;
-    static ValuesSourceAggregatorFactory<? extends ValuesSource, ? extends ValuesSourceAggregatorFactory<?, ?>> metric;
+    static ValuesSourceMetricsAggregationBuilder metric;
     static List<PipelineAggregationHelperTests.MockBucket> mockHisto;
 
     static Map<String, ArrayList<Double>> testValues;
@@ -81,7 +80,7 @@ public class SerialDiffIT extends ESIntegTestCase {
         }
     }
 
-    private ValuesSourceAggregatorFactory<? extends ValuesSource, ? extends ValuesSourceAggregatorFactory<?, ?>> randomMetric(String name, String field) {
+    private ValuesSourceMetricsAggregationBuilder randomMetric(String name, String field) {
         int rand = randomIntBetween(0,3);
 
         switch (rand) {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java b/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java
deleted file mode 100644
index a297181..0000000
--- a/core/src/test/java/org/elasticsearch/search/aggregations/support/ValuesSourceTypeTests.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.aggregations.support;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-
-public class ValuesSourceTypeTests extends ESTestCase {
-
-    public void testValidOrdinals() {
-        assertThat(ValuesSourceType.ANY.ordinal(), equalTo(0));
-        assertThat(ValuesSourceType.NUMERIC.ordinal(), equalTo(1));
-        assertThat(ValuesSourceType.BYTES.ordinal(), equalTo(2));
-        assertThat(ValuesSourceType.GEOPOINT.ordinal(), equalTo(3));
-    }
-
-    public void testwriteTo() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.ANY.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(0));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.NUMERIC.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(1));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.BYTES.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(2));
-            }
-        }
-
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            ValuesSourceType.GEOPOINT.writeTo(out);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(in.readVInt(), equalTo(3));
-            }
-        }
-    }
-
-    public void testReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(0);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.ANY));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(1);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.NUMERIC));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(2);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.BYTES));
-            }
-        }
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(3);
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                assertThat(ValuesSourceType.ANY.readFrom(in), equalTo(ValuesSourceType.GEOPOINT));
-            }
-        }
-    }
-
-    public void testInvalidReadFrom() throws Exception {
-        try (BytesStreamOutput out = new BytesStreamOutput()) {
-            out.writeVInt(randomIntBetween(4, Integer.MAX_VALUE));
-            try (StreamInput in = StreamInput.wrap(out.bytes())) {
-                ValuesSourceType.ANY.readFrom(in);
-                fail("Expected IOException");
-            } catch(IOException e) {
-                assertThat(e.getMessage(), containsString("Unknown ValuesSourceType ordinal ["));
-            }
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java b/core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java
index 28874d2..35dbde2 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/SearchWhileCreatingIndexIT.java
@@ -29,7 +29,6 @@ import org.elasticsearch.test.ESIntegTestCase;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 
-
 /**
  * This test basically verifies that search with a single shard active (cause we indexed to it) and other
  * shards possibly not active at all (cause they haven't allocated) will still work.
@@ -58,39 +57,44 @@ public class SearchWhileCreatingIndexIT extends ESIntegTestCase {
         int shardsNo = numberOfReplicas + 1;
         int neededNodes = shardsNo <= 2 ? 1 : shardsNo / 2 + 1;
         internalCluster().ensureAtLeastNumDataNodes(randomIntBetween(neededNodes, shardsNo));
-        for (int i = 0; i < 20; i++) {
-            logger.info("running iteration {}", i);
-            if (createIndex) {
-                createIndex("test");
-            }
-            client().prepareIndex("test", "type1", randomAsciiOfLength(5)).setSource("field", "test").execute().actionGet();
-            RefreshResponse refreshResponse = client().admin().indices().prepareRefresh("test").execute().actionGet();
-            assertThat(refreshResponse.getSuccessfulShards(), greaterThanOrEqualTo(1)); // at least one shard should be successful when refreshing
 
-            // we want to make sure that while recovery happens, and a replica gets recovered, its properly refreshed
-            ClusterHealthStatus status = ClusterHealthStatus.RED;
-            while (status != ClusterHealthStatus.GREEN) {
-                // first, verify that search on the primary search works
-                SearchResponse searchResponse = client().prepareSearch("test").setPreference("_primary").setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
-                assertHitCount(searchResponse, 1);
-                // now, let it go to primary or replica, though in a randomized re-creatable manner
-                String preference = randomAsciiOfLength(5);
-                Client client = client();
-                searchResponse = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
-                if (searchResponse.getHits().getTotalHits() != 1) {
-                    refresh();
-                    SearchResponse searchResponseAfterRefresh = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
-                    logger.info("hits count mismatch on any shard search failed, post explicit refresh hits are {}", searchResponseAfterRefresh.getHits().getTotalHits());
-                    ensureGreen();
-                    SearchResponse searchResponseAfterGreen = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
-                    logger.info("hits count mismatch on any shard search failed, post explicit wait for green hits are {}", searchResponseAfterGreen.getHits().getTotalHits());
-                    assertHitCount(searchResponse, 1);
-                }
+        String id = randomAsciiOfLength(5);
+        // we will go the primary or the replica, but in a
+        // randomized re-creatable manner
+        int counter = 0;
+        String preference = randomAsciiOfLength(5);
+
+        logger.info("running iteration for id {}, preference {}", id, preference);
+
+        if (createIndex) {
+            createIndex("test");
+        }
+        client().prepareIndex("test", "type1", id).setSource("field", "test").execute().actionGet();
+        RefreshResponse refreshResponse = client().admin().indices().prepareRefresh("test").execute().actionGet();
+        assertThat(refreshResponse.getSuccessfulShards(), greaterThanOrEqualTo(1)); // at least one shard should be successful when refreshing
+
+        logger.info("using preference {}", preference);
+        // we want to make sure that while recovery happens, and a replica gets recovered, its properly refreshed
+        ClusterHealthStatus status = ClusterHealthStatus.RED;
+        while (status != ClusterHealthStatus.GREEN) {
+            // first, verify that search on the primary search works
+            SearchResponse searchResponse = client().prepareSearch("test").setPreference("_primary").setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
+            assertHitCount(searchResponse, 1);
+            Client client = client();
+            searchResponse = client.prepareSearch("test").setPreference(preference + Integer.toString(counter++)).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
+            if (searchResponse.getHits().getTotalHits() != 1) {
+                refresh();
+                SearchResponse searchResponseAfterRefresh = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
+                logger.info("hits count mismatch on any shard search failed, post explicit refresh hits are {}", searchResponseAfterRefresh.getHits().getTotalHits());
+                ensureGreen();
+                SearchResponse searchResponseAfterGreen = client.prepareSearch("test").setPreference(preference).setQuery(QueryBuilders.termQuery("field", "test")).execute().actionGet();
+                logger.info("hits count mismatch on any shard search failed, post explicit wait for green hits are {}", searchResponseAfterGreen.getHits().getTotalHits());
                 assertHitCount(searchResponse, 1);
-                status = client().admin().cluster().prepareHealth("test").get().getStatus();
-                internalCluster().ensureAtLeastNumDataNodes(numberOfReplicas + 1);
             }
-            cluster().wipeIndices("test");
+            assertHitCount(searchResponse, 1);
+            status = client().admin().cluster().prepareHealth("test").get().getStatus();
+            internalCluster().ensureAtLeastNumDataNodes(numberOfReplicas + 1);
         }
+        cluster().wipeIndices("test");
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
index 62f3e22..9597af9 100644
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -94,7 +94,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
                         // skip me so we don't need transport
                     }
                     @Override
-                    protected void configureAggs(IndicesQueriesRegistry indicesQueriesRegistry) {
+                    protected void configureAggs() {
                         // skip me so we don't need scripting
                     }
                     @Override
@@ -291,7 +291,7 @@ public class SearchSourceBuilderTests extends ESTestCase {
             // NORELEASE need a random aggregation builder method
             builder.aggregation(AggregationBuilders.avg(randomAsciiOfLengthBetween(5, 20)));
         }
-        if (randomBoolean()) {
+        if (true) {
             // NORELEASE need a method to randomly build content for ext
             XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
             xContentBuilder.startObject();
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
index fac7f71..1543433 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
@@ -230,7 +230,7 @@ public class CompletionSuggestSearchIT extends ESIntegTestCase {
         SuggestResponse suggestResponse = client().suggest(request).get();
         assertThat(suggestResponse.getSuccessfulShards(), equalTo(0));
         for (ShardOperationFailedException exception : suggestResponse.getShardFailures()) {
-            assertThat(exception.reason(), containsString("ParsingException[[completion] failed to parse field [payload]]; nested: IllegalStateException[expected value but got [START_OBJECT]]"));
+            assertThat(exception.reason(), containsString("ParsingException[[completion] failed to parse field [payload]]; nested: IllegalStateException[Can't get text on a START_OBJECT"));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java b/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
index a120b63..1df9659 100644
--- a/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
+++ b/core/src/test/java/org/elasticsearch/test/search/aggregations/bucket/SharedSignificantTermsTestMethods.java
@@ -57,9 +57,13 @@ public class SharedSignificantTermsTestMethods {
     }
 
     private static void checkSignificantTermsAggregationCorrect(ESIntegTestCase testCase) {
-        SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE).addAggregation(
-                new TermsBuilder("class").field(CLASS_FIELD).subAggregation(new SignificantTermsBuilder("sig_terms").field(TEXT_FIELD)))
-                .execute().actionGet();
+
+        SearchResponse response = client().prepareSearch(INDEX_NAME).setTypes(DOC_TYPE)
+                .addAggregation(new TermsBuilder("class").field(CLASS_FIELD).subAggregation(
+                        new SignificantTermsBuilder("sig_terms")
+                                .field(TEXT_FIELD)))
+                .execute()
+                .actionGet();
         assertSearchResponse(response);
         StringTerms classes = response.getAggregations().get("class");
         Assert.assertThat(classes.getBuckets().size(), equalTo(2));
diff --git a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
index 09653c1..2fe11b5 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
@@ -29,6 +29,7 @@ import org.elasticsearch.threadpool.ThreadPool.Names;
 import java.lang.reflect.Field;
 import java.util.Arrays;
 import java.util.HashSet;
+import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.Executor;
@@ -47,6 +48,7 @@ import static org.hamcrest.Matchers.sameInstance;
 /**
  */
 public class UpdateThreadPoolSettingsTests extends ESTestCase {
+
     public void testCorrectThreadPoolTypePermittedInSettings() throws InterruptedException {
         String threadPoolName = randomThreadPoolName();
         ThreadPool.ThreadPoolType correctThreadPoolType = ThreadPool.THREAD_POOL_TYPES.get(threadPoolName);
@@ -452,11 +454,10 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         Set<ThreadPool.ThreadPoolType> set = new HashSet<>();
         set.addAll(Arrays.asList(ThreadPool.ThreadPoolType.values()));
         set.remove(ThreadPool.THREAD_POOL_TYPES.get(threadPoolName));
-        ThreadPool.ThreadPoolType invalidThreadPoolType = randomFrom(set.toArray(new ThreadPool.ThreadPoolType[set.size()]));
-        return invalidThreadPoolType;
+        return randomFrom(set.toArray(new ThreadPool.ThreadPoolType[set.size()]));
     }
 
     private String randomThreadPool(ThreadPool.ThreadPoolType type) {
-        return randomFrom(ThreadPool.THREAD_POOL_TYPES.entrySet().stream().filter(t -> t.getValue().equals(type)).map(t -> t.getKey()).collect(Collectors.toList()));
+        return randomFrom(ThreadPool.THREAD_POOL_TYPES.entrySet().stream().filter(t -> t.getValue().equals(type)).map(Map.Entry::getKey).collect(Collectors.toList()));
     }
 }
diff --git a/docs/plugins/ingest-geoip.asciidoc b/docs/plugins/ingest-geoip.asciidoc
new file mode 100644
index 0000000..539c299
--- /dev/null
+++ b/docs/plugins/ingest-geoip.asciidoc
@@ -0,0 +1,64 @@
+[[ingest-geoip]]
+== Ingest Geoip Processor Plugin
+
+The GeoIP processor adds information about the geographical location of IP addresses, based on data from the Maxmind databases.
+This processor adds this information by default under the `geoip` field.
+
+The ingest plugin ships by default with the GeoLite2 City and GeoLite2 Country geoip2 databases from Maxmind made available
+under the CCA-ShareAlike 3.0 license. For more details see, http://dev.maxmind.com/geoip/geoip2/geolite2/
+
+The GeoIP processor can run with other geoip2 databases from Maxmind. The files must be copied into the geoip config directory
+and the `database_file` option should be used to specify the filename of the custom database. The geoip config directory
+is located at `$ES_HOME/config/ingest/geoip` and holds the shipped databases too.
+
+[[geoip-options]]
+.Geoip options
+[options="header"]
+|======
+| Name                   | Required  | Default                                                                            | Description
+| `source_field`         | yes       | -                                                                                  | The field to get the ip address or hostname from for the geographical lookup.
+| `target_field`         | no        | geoip                                                                              | The field that will hold the geographical information looked up from the Maxmind database.
+| `database_file`        | no        | GeoLite2-City.mmdb                                                                 | The database filename in the geoip config directory. The ingest plugin ships with the GeoLite2-City.mmdb and GeoLite2-Country.mmdb files.
+| `fields`               | no        | [`continent_name`, `country_iso_code`, `region_name`, `city_name`, `location`] <1> | Controls what properties are added to the `target_field` based on the geoip lookup.
+|======
+
+<1> Depends on what is available in `database_field`:
+* If the GeoLite2 City database is used then the following fields may be added under the `target_field`: `ip`,
+`country_iso_code`, `country_name`, `continent_name`, `region_name`, `city_name`, `timezone`, `latitude`, `longitude`
+and `location`. The fields actually added depend on what has been found and which fields were configured in `fields`.
+* If the GeoLite2 Country database is used then the following fields may be added under the `target_field`: `ip`,
+`country_iso_code`, `country_name` and `continent_name`.The fields actually added depend on what has been found and which fields were configured in `fields`.
+
+An example that uses the default city database and adds the geographical information to the `geoip` field based on the `ip` field:
+
+[source,js]
+--------------------------------------------------
+{
+  "description" : "...",
+  "processors" : [
+    {
+      "geoip" : {
+        "source_field" : "ip"
+      }
+    }
+  ]
+}
+--------------------------------------------------
+
+An example that uses the default country database and add the geographical information to the `geo` field based on the `ip` field`:
+
+[source,js]
+--------------------------------------------------
+{
+  "description" : "...",
+  "processors" : [
+    {
+      "geoip" : {
+        "source_field" : "ip",
+        "target_field" : "geo",
+        "database_file" : "GeoLite2-Country.mmdb"
+      }
+    }
+  ]
+}
+--------------------------------------------------
diff --git a/docs/reference/aggregations/bucket.asciidoc b/docs/reference/aggregations/bucket.asciidoc
index 66ce2d8..2d185dd 100644
--- a/docs/reference/aggregations/bucket.asciidoc
+++ b/docs/reference/aggregations/bucket.asciidoc
@@ -19,8 +19,6 @@ include::bucket/datehistogram-aggregation.asciidoc[]
 
 include::bucket/daterange-aggregation.asciidoc[]
 
-include::bucket/diversified-sampler-aggregation.asciidoc[]
-
 include::bucket/filter-aggregation.asciidoc[]
 
 include::bucket/filters-aggregation.asciidoc[]
diff --git a/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc b/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc
deleted file mode 100644
index 92effce..0000000
--- a/docs/reference/aggregations/bucket/diversified-sampler-aggregation.asciidoc
+++ /dev/null
@@ -1,154 +0,0 @@
-[[search-aggregations-bucket-sampler-aggregation]]
-=== Sampler Aggregation
-
-experimental[]
-
-A filtering aggregation used to limit any sub aggregations' processing to a sample of the top-scoring documents. Diversity settings are 
-used to limit the number of matches that share a common value such as an "author".
-
-.Example use cases:
-* Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches
-* Removing bias from analytics by ensuring fair representation of content from different sources
-* Reducing the running cost of aggregations that can produce useful results using only samples e.g. `significant_terms`
- 
-
-Example:
-
-[source,js]
---------------------------------------------------
-{
-    "query": {
-        "match": {
-            "text": "iphone"
-        }
-    },
-    "aggs": {
-        "sample": {
-            "sampler": {
-                "shard_size": 200,
-                "field" : "user.id"   
-            },
-            "aggs": {
-                "keywords": {
-                    "significant_terms": {
-                        "field": "text"
-                    }
-                }
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Response:
-
-[source,js]
---------------------------------------------------
-{
-    ...
-        "aggregations": {
-        "sample": {
-            "doc_count": 1000,<1>
-            "keywords": {<2>
-                "doc_count": 1000,
-                "buckets": [
-                    ...
-                    {
-                        "key": "bend",
-                        "doc_count": 58,
-                        "score": 37.982536582524276,
-                        "bg_count": 103
-                    },
-                    ....
-}
---------------------------------------------------
-
-<1> 1000 documents were sampled in total becase we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
-<2> The results of the significant_terms aggregation are not skewed by any single over-active Twitter user because we asked for a maximum of one tweet from any one user in our sample.
-
-
-==== shard_size
-
-The `shard_size` parameter limits how many top-scoring documents are collected in the sample processed on each shard.
-The default value is 100.
-
-==== Controlling diversity
-=`field` or `script` and `max_docs_per_value` settings are used to control the maximum number of documents collected on any one shard which share a common value.
-The choice of value (e.g. `author`) is loaded from a regular `field` or derived dynamically by a `script`.
-
-The aggregation will throw an error if the choice of field or script produces multiple values for a document.
-It is currently not possible to offer this form of de-duplication using many values, primarily due to concerns over efficiency.
-
-NOTE: Any good market researcher will tell you that when working with samples of data it is important
-that the sample represents a healthy variety of opinions rather than being skewed by any single voice.
-The same is true with aggregations and sampling with these diversify settings can offer a way to remove the bias in your content (an over-populated geography, a large spike in a timeline or an over-active forum spammer).  
-
-==== Field
-
-Controlling diversity using a field:
-
-[source,js]
---------------------------------------------------
-{
-    "aggs" : {
-        "sample" : {
-            "diverisfied_sampler" : {
-                "field" : "author",
-                "max_docs_per_value" : 3
-            }
-        }
-    }
-}
---------------------------------------------------
-
-Note that the `max_docs_per_value` setting applies on a per-shard basis only for the purposes of shard-local sampling.
-It is not intended as a way of providing a global de-duplication feature on search results.
-
-
-
-==== Script
-
-Controlling diversity using a script:
-
-[source,js]
---------------------------------------------------
-{
-    "aggs" : {
-        "sample" : {
-            "diverisfied_sampler" : {
-                "script" : "doc['author'].value + '/' + doc['genre'].value"
-            }
-        }
-    }
-}
---------------------------------------------------
-Note in the above example we chose to use the default `max_docs_per_value` setting of 1 and combine author and genre fields to ensure 
-each shard sample has, at most, one match for an author/genre pair.
-
-
-==== execution_hint
-
-When using the settings to control diversity, the optional `execution_hint` setting can influence the management of the values used for de-duplication.
-Each option will hold up to `shard_size` values in memory while performing de-duplication but the type of value held can be controlled as follows:
- 
- - hold field values directly (`map`)
- - hold ordinals of the field as determined by the Lucene index (`global_ordinals`)
- - hold hashes of the field values - with potential for hash collisions (`bytes_hash`)
- 
-The default setting is to use `global_ordinals` if this information is available from the Lucene index and reverting to `map` if not.
-The `bytes_hash` setting may prove faster in some cases but introduces the possibility of false positives in de-duplication logic due to the possibility of hash collisions.
-Please note that Elasticsearch will ignore the choice of execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.
-
-==== Limitations
-
-===== Cannot be nested under `breadth_first` aggregations
-Being a quality-based filter the sampler aggregation needs access to the relevance score produced for each document.
-It therefore cannot be nested under a `terms` aggregation which has the `collect_mode` switched from the default `depth_first` mode to `breadth_first` as this discards scores.
-In this situation an error will be thrown.
-
-===== Limited de-dup logic.
-The de-duplication logic in the diversify settings applies only at a shard level so will not apply across shards.
-
-===== No specialized syntax for geo/date fields
-Currently the syntax for defining the diversifying values is defined by a choice of `field` or `script` - there is no added syntactical sugar for expressing geo or date units such as "1w" (1 week).
-This support may be added in a later release and users will currently have to create these sorts of values using a script.
\ No newline at end of file
diff --git a/docs/reference/aggregations/bucket/filters-aggregation.asciidoc b/docs/reference/aggregations/bucket/filters-aggregation.asciidoc
index 322dccb..a7e07ac 100644
--- a/docs/reference/aggregations/bucket/filters-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/filters-aggregation.asciidoc
@@ -146,7 +146,7 @@ The following snippet shows a response where the `other` bucket is requested to
   "aggs" : {
     "messages" : {
       "filters" : {
-        "other_bucket": "other_messages",
+        "other_bucket_key": "other_messages",
         "filters" : {
           "errors" :   { "term" : { "body" : "error"   }},
           "warnings" : { "term" : { "body" : "warning" }}
diff --git a/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc b/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
index 741edc8..2974270 100644
--- a/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
+++ b/docs/reference/aggregations/bucket/sampler-aggregation.asciidoc
@@ -4,9 +4,11 @@
 experimental[]
 
 A filtering aggregation used to limit any sub aggregations' processing to a sample of the top-scoring documents.
+Optionally, diversity settings can be used to limit the number of matches that share a common value such as an "author".
 
 .Example use cases:
 * Tightening the focus of analytics to high-relevance matches rather than the potentially very long tail of low-quality matches
+* Removing bias from analytics by ensuring fair representation of content from different sources
 * Reducing the running cost of aggregations that can produce useful results using only samples e.g. `significant_terms`
  
 
@@ -23,7 +25,8 @@ Example:
     "aggs": {
         "sample": {
             "sampler": {
-                "shard_size": 200
+                "shard_size": 200,
+                "field" : "user.id"   
             },
             "aggs": {
                 "keywords": {
@@ -60,7 +63,8 @@ Response:
 }
 --------------------------------------------------
 
-<1> 1000 documents were sampled in total because we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
+<1> 1000 documents were sampled in total becase we asked for a maximum of 200 from an index with 5 shards. The cost of performing the nested significant_terms aggregation was therefore limited rather than unbounded.
+<2> The results of the significant_terms aggregation are not skewed by any single over-active Twitter user because we asked for a maximum of one tweet from any one user in our sample.
 
 
 ==== shard_size
@@ -68,9 +72,83 @@ Response:
 The `shard_size` parameter limits how many top-scoring documents are collected in the sample processed on each shard.
 The default value is 100.
 
+==== Controlling diversity
+Optionally, you can use the `field` or `script` and `max_docs_per_value` settings to control the maximum number of documents collected on any one shard which share a common value.
+The choice of value (e.g. `author`) is loaded from a regular `field` or derived dynamically by a `script`.
+
+The aggregation will throw an error if the choice of field or script produces multiple values for a document.
+It is currently not possible to offer this form of de-duplication using many values, primarily due to concerns over efficiency.
+
+NOTE: Any good market researcher will tell you that when working with samples of data it is important
+that the sample represents a healthy variety of opinions rather than being skewed by any single voice.
+The same is true with aggregations and sampling with these diversify settings can offer a way to remove the bias in your content (an over-populated geography, a large spike in a timeline or an over-active forum spammer).  
+
+==== Field
+
+Controlling diversity using a field:
+
+[source,js]
+--------------------------------------------------
+{
+    "aggs" : {
+        "sample" : {
+            "sampler" : {
+                "field" : "author",
+                "max_docs_per_value" : 3
+            }
+        }
+    }
+}
+--------------------------------------------------
+
+Note that the `max_docs_per_value` setting applies on a per-shard basis only for the purposes of shard-local sampling.
+It is not intended as a way of providing a global de-duplication feature on search results.
+
+
+
+==== Script
+
+Controlling diversity using a script:
+
+[source,js]
+--------------------------------------------------
+{
+    "aggs" : {
+        "sample" : {
+            "sampler" : {
+                "script" : "doc['author'].value + '/' + doc['genre'].value"
+            }
+        }
+    }
+}
+--------------------------------------------------
+Note in the above example we chose to use the default `max_docs_per_value` setting of 1 and combine author and genre fields to ensure 
+each shard sample has, at most, one match for an author/genre pair.
+
+
+==== execution_hint
+
+When using the settings to control diversity, the optional `execution_hint` setting can influence the management of the values used for de-duplication.
+Each option will hold up to `shard_size` values in memory while performing de-duplication but the type of value held can be controlled as follows:
+ 
+ - hold field values directly (`map`)
+ - hold ordinals of the field as determined by the Lucene index (`global_ordinals`)
+ - hold hashes of the field values - with potential for hash collisions (`bytes_hash`)
+ 
+The default setting is to use `global_ordinals` if this information is available from the Lucene index and reverting to `map` if not.
+The `bytes_hash` setting may prove faster in some cases but introduces the possibility of false positives in de-duplication logic due to the possibility of hash collisions.
+Please note that Elasticsearch will ignore the choice of execution hint if it is not applicable and that there is no backward compatibility guarantee on these hints.
+
 ==== Limitations
 
 ===== Cannot be nested under `breadth_first` aggregations
 Being a quality-based filter the sampler aggregation needs access to the relevance score produced for each document.
 It therefore cannot be nested under a `terms` aggregation which has the `collect_mode` switched from the default `depth_first` mode to `breadth_first` as this discards scores.
-In this situation an error will be thrown.
\ No newline at end of file
+In this situation an error will be thrown.
+
+===== Limited de-dup logic.
+The de-duplication logic in the diversify settings applies only at a shard level so will not apply across shards.
+
+===== No specialized syntax for geo/date fields
+Currently the syntax for defining the diversifying values is defined by a choice of `field` or `script` - there is no added syntactical sugar for expressing geo or date units such as "1w" (1 week).
+This support may be added in a later release and users will currently have to create these sorts of values using a script.
\ No newline at end of file
diff --git a/docs/reference/cluster/stats.asciidoc b/docs/reference/cluster/stats.asciidoc
index 8093dd3..3f36ad6 100644
--- a/docs/reference/cluster/stats.asciidoc
+++ b/docs/reference/cluster/stats.asciidoc
@@ -57,15 +57,11 @@ Will return, for example:
          "memory_size_in_bytes": 0,
          "evictions": 0
       },
-      "filter_cache": {
+      "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
       },
-      "id_cache": {
-         "memory_size": "0b",
-         "memory_size_in_bytes": 0
-      },
       "completion": {
          "size": "0b",
          "size_in_bytes": 0
diff --git a/docs/reference/docs/bulk.asciidoc b/docs/reference/docs/bulk.asciidoc
index ef066eb..b9b7d47 100644
--- a/docs/reference/docs/bulk.asciidoc
+++ b/docs/reference/docs/bulk.asciidoc
@@ -131,6 +131,8 @@ operation based on the `_parent` / `_routing` mapping.
 [[bulk-timestamp]]
 === Timestamp
 
+deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
+
 Each bulk item can include the timestamp value using the
 `_timestamp`/`timestamp` field. It automatically follows the behavior of
 the index operation based on the `_timestamp` mapping.
@@ -139,6 +141,8 @@ the index operation based on the `_timestamp` mapping.
 [[bulk-ttl]]
 === TTL
 
+deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
+
 Each bulk item can include the ttl value using the `_ttl`/`ttl` field.
 It automatically follows the behavior of the index operation based on
 the `_ttl` mapping.
diff --git a/docs/reference/docs/index_.asciidoc b/docs/reference/docs/index_.asciidoc
index 5f79efb..27ac85b 100644
--- a/docs/reference/docs/index_.asciidoc
+++ b/docs/reference/docs/index_.asciidoc
@@ -258,6 +258,8 @@ specified using the `routing` parameter.
 [[index-timestamp]]
 === Timestamp
 
+deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
+
 A document can be indexed with a `timestamp` associated with it. The
 `timestamp` value of a document can be set using the `timestamp`
 parameter. For example:
@@ -280,6 +282,8 @@ page>>.
 [[index-ttl]]
 === TTL
 
+deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
+
 
 A document can be indexed with a `ttl` (time to live) associated with
 it. Expired documents will be expunged automatically. The expiration
diff --git a/docs/reference/ingest/ingest.asciidoc b/docs/reference/ingest/ingest.asciidoc
new file mode 100644
index 0000000..0c049f8
--- /dev/null
+++ b/docs/reference/ingest/ingest.asciidoc
@@ -0,0 +1,1103 @@
+[[ingest]]
+== Ingest Plugin
+
+The ingest plugin can be used to pre-process documents before the actual indexing takes place.
+This pre-processing happens by the ingest plugin that intercepts bulk and index requests, applies the
+transformations and then passes the documents back to the index or bulk APIs.
+
+The ingest plugin is disabled by default. In order to enable the ingest plugin the following
+setting should be configured in the elasticsearch.yml file:
+
+[source,yaml]
+--------------------------------------------------
+node.ingest: true
+--------------------------------------------------
+
+The ingest plugin can be installed and enabled on any node. It is possible to run ingest
+on an master and or data node or have dedicated client nodes that run with ingest.
+
+In order to pre-process document before indexing the `pipeline` parameter should be used
+on an index or bulk request to tell the ingest plugin what pipeline is going to be used.
+
+[source,js]
+--------------------------------------------------
+PUT /my-index/my-type/my-id?pipeline=my_pipeline_id
+{
+  ...
+}
+--------------------------------------------------
+// AUTOSENSE
+
+=== Pipeline Definition
+
+A pipeline is a definition of a series of processors that are to be 
+executed in the same sequential order as they are declared.
+[source,js]
+--------------------------------------------------
+{
+  "description" : "...",
+  "processors" : [ ... ]
+}
+--------------------------------------------------
+
+The `description` is a special field to store a helpful description of 
+what the pipeline attempts to achieve.
+
+The `processors` parameter defines a list of processors to be executed in 
+order.
+
+=== Processors
+
+All processors are defined in the following way within a pipeline definition:
+
+[source,js]
+--------------------------------------------------
+{
+  "PROCESSOR_NAME" : {
+    ... processor configuration options ...
+  }
+}
+--------------------------------------------------
+
+Each processor defines its own configuration parameters, but all processors have 
+the ability to declare `tag` and `on_failure` fields. These fields are optional.
+
+A `tag` is simply a string identifier of the specific instatiation of a certain
+processor in a pipeline. The `tag` field does not affect any processor's behavior,
+but is very useful for bookkeeping and tracing errors to specific processors.
+
+See <<handling-failure-in-pipelines>> to learn more about the `on_failure` field and error handling in pipelines.
+
+==== Set processor
+Sets one field and associates it with the specified value. If the field already exists,
+its value will be replaced with the provided one.
+
+[[set-options]]
+.Set Options
+[options="header"]
+|======
+| Name      | Required  | Default  | Description
+| `field`   | yes       | -        | The field to insert, upsert, or update
+| `value`   | yes       | -        | The value to be set for the field
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "set": {
+    "field": "field1",
+    "value": 582.1
+  }
+}
+--------------------------------------------------
+
+==== Append processor
+Appends one or more values to an existing array if the field already exists and it is an array.
+Converts a scalar to an array and appends one or more values to it if the field exists and it is a scalar.
+Creates an array containing the provided values if the fields doesn't exist.
+Accepts a single value or an array of values.
+
+[[append-options]]
+.Append Options
+[options="header"]
+|======
+| Name      | Required  | Default  | Description
+| `field`   | yes       | -        | The field to be appended to
+| `value`   | yes       | -        | The value to be appended
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "append": {
+    "field": "field1"
+    "value": ["item2", "item3", "item4"]
+  }
+}
+--------------------------------------------------
+
+==== Remove processor
+Removes an existing field. If the field doesn't exist, an exception will be thrown
+
+[[remove-options]]
+.Remove Options
+[options="header"]
+|======
+| Name      | Required  | Default  | Description
+| `field`   | yes       | -        | The field to be removed
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "remove": {
+    "field": "foo"
+  }
+}
+--------------------------------------------------
+
+==== Rename processor
+Renames an existing field. If the field doesn't exist, an exception will be thrown. Also, the new field
+name must not exist.
+
+[[rename-options]]
+.Rename Options
+[options="header"]
+|======
+| Name      | Required  | Default  | Description
+| `field`   | yes       | -        | The field to be renamed
+| `to`      | yes       | -        | The new name of the field
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "rename": {
+    "field": "foo",
+    "to": "foobar"
+  }
+}
+--------------------------------------------------
+
+
+==== Convert processor
+Converts an existing field's value to a different type, like turning a string to an integer.
+If the field value is an array, all members will be converted.
+
+The supported types include: `integer`, `float`, `string`, and `boolean`.
+
+`boolean` will set the field to true if its string value is equal to `true` (ignore case), to
+false if its string value is equal to `false` (ignore case) and it will throw exception otherwise.
+
+[[convert-options]]
+.Convert Options
+[options="header"]
+|======
+| Name      | Required  | Default  | Description
+| `field`   | yes       | -        | The field whose value is to be converted
+| `type`    | yes       | -        | The type to convert the existing value to
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "convert": {
+    "field" : "foo"
+    "type": "integer"
+  }
+}
+--------------------------------------------------
+
+==== Gsub processor
+Converts a string field by applying a regular expression and a replacement.
+If the field is not a string, the processor will throw an exception.
+
+[[gsub-options]]
+.Gsub Options
+[options="header"]
+|======
+| Name          | Required  | Default  | Description
+| `field`       | yes       | -        | The field apply the replacement for
+| `pattern`     | yes       | -        | The pattern to be replaced
+| `replacement` | yes       | -        | The string to replace the matching patterns with.
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "gsub": {
+    "field": "field1",
+    "pattern": "\.",
+    "replacement": "-"
+  }
+}
+--------------------------------------------------
+
+==== Join processor
+Joins each element of an array into a single string using a separator character between each element.
+Throws error when the field is not an array.
+
+[[join-options]]
+.Join Options
+[options="header"]
+|======
+| Name          | Required  | Default  | Description
+| `field`       | yes       | -        | The field to be separated
+| `separator`   | yes       | -        | The separator character
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "join": {
+    "field": "joined_array_field",
+    "separator": "-"
+  }
+}
+--------------------------------------------------
+
+==== Split processor
+Split a field to an array using a separator character. Only works on string fields.
+
+[[split-options]]
+.Split Options
+[options="header"]
+|======
+| Name     | Required  | Default  | Description
+| `field`  | yes       | -        | The field to split
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "split": {
+    "field": ","
+  }
+}
+--------------------------------------------------
+
+==== Lowercase processor
+Converts a string to its lowercase equivalent.
+
+[[lowercase-options]]
+.Lowercase Options
+[options="header"]
+|======
+| Name     | Required  | Default  | Description
+| `field`  | yes       | -        | The field to lowercase
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "lowercase": {
+    "field": "foo"
+  }
+}
+--------------------------------------------------
+
+==== Uppercase processor
+Converts a string to its uppercase equivalent.
+
+[[uppercase-options]]
+.Uppercase Options
+[options="header"]
+|======
+| Name     | Required  | Default  | Description
+| `field`  | yes       | -        | The field to uppercase
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "uppercase": {
+    "field": "foo"
+  }
+}
+--------------------------------------------------
+
+==== Trim processor
+Trims whitespace from field. NOTE: this only works on leading and trailing whitespaces.
+
+[[trim-options]]
+.Trim Options
+[options="header"]
+|======
+| Name     | Required  | Default  | Description
+| `field`  | yes       | -        | The string-valued field to trim whitespace from
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "trim": {
+    "field": "foo"
+  }
+}
+--------------------------------------------------
+
+==== Grok Processor
+
+The Grok Processor extracts structured fields out of a single text field within a document. You choose which field to
+extract matched fields from, as well as the Grok Pattern you expect will match. A Grok Pattern is like a regular
+expression that supports aliased expressions that can be reused.
+
+This tool is perfect for syslog logs, apache and other webserver logs, mysql logs, and in general, any log format
+that is generally written for humans and not computer consumption.
+
+The processor comes packaged with over 120 reusable patterns that are located at `$ES_HOME/config/ingest/grok/patterns`.
+Here, you can add your own custom grok pattern files with custom grok expressions to be used by the processor.
+
+If you need help building patterns to match your logs, you will find the <http://grokdebug.herokuapp.com> and
+<http://grokconstructor.appspot.com/> applications quite useful!
+
+===== Grok Basics
+
+Grok sits on top of regular expressions, so any regular expressions are valid in grok as well.
+The regular expression library is Oniguruma, and you can see the full supported regexp syntax
+https://github.com/kkos/oniguruma/blob/master/doc/RE[on the Onigiruma site].
+
+Grok works by leveraging this regular expression language to allow naming existing patterns and combining them into more
+complex patterns that match your fields.
+
+The syntax for re-using a grok pattern comes in three forms: `%{SYNTAX:SEMANTIC}`, `%{SYNTAX}`, `%{SYNTAX:SEMANTIC:TYPE}`.
+
+The `SYNTAX` is the name of the pattern that will match your text. For example, `3.44` will be matched by the `NUMBER`
+pattern and `55.3.244.1` will be matched by the `IP` pattern. The syntax is how you match. `NUMBER` and `IP` are both
+patterns that are provided within the default patterns set.
+
+The `SEMANTIC` is the identifier you give to the piece of text being matched. For example, `3.44` could be the
+duration of an event, so you could call it simply `duration`. Further, a string `55.3.244.1` might identify
+the `client` making a request.
+
+The `TYPE` is the type you wish to cast your named field. `int` and `float` are currently the only types supported for coercion.
+
+For example, here is a grok pattern that would match the above example given. We would like to match a text with the following
+contents:
+
+[source,js]
+--------------------------------------------------
+3.44 55.3.244.1
+--------------------------------------------------
+
+We may know that the above message is a number followed by an IP-address. We can match this text with the following
+Grok expression.
+
+[source,js]
+--------------------------------------------------
+%{NUMBER:duration} %{IP:client}
+--------------------------------------------------
+
+===== Custom Patterns and Pattern Files
+
+The Grok Processor comes pre-packaged with a base set of pattern files. These patterns may not always have
+what you are looking for. These pattern files have a very basic format. Each line describes a named pattern with
+the following format:
+
+[source,js]
+--------------------------------------------------
+NAME ' '+ PATTERN '\n'
+--------------------------------------------------
+
+You can add this pattern to an existing file, or add your own file in the patterns directory here: `$ES_HOME/config/ingest/grok/patterns`.
+The Ingest Plugin will pick up files in this directory to be loaded into the grok processor's known patterns. These patterns are loaded
+at startup, so you will need to do a restart your ingest node if you wish to update these files while running.
+
+Example snippet of pattern definitions found in the `grok-patterns` patterns file:
+
+[source,js]
+--------------------------------------------------
+YEAR (?>\d\d){1,2}
+HOUR (?:2[0123]|[01]?[0-9])
+MINUTE (?:[0-5][0-9])
+SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
+TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
+--------------------------------------------------
+
+===== Using Grok Processor in a Pipeline
+
+[[grok-options]]
+.Grok Options
+[options="header"]
+|======
+| Name                   | Required  | Default             | Description
+| `match_field`          | yes       | -                   | The field to use for grok expression parsing
+| `match_pattern`        | yes       | -                   | The grok expression to match and extract named captures with
+| `pattern_definitions`  | no        | -                   | A map of pattern-name and pattern tuples defining custom patterns to be used by the current processor. Patterns matching existing names will override the pre-existing definition.
+|======
+
+Here is an example of using the provided patterns to extract out and name structured fields from a string field in
+a document.
+
+[source,js]
+--------------------------------------------------
+{
+  "message": "55.3.244.1 GET /index.html 15824 0.043"
+}
+--------------------------------------------------
+
+The pattern for this could be
+
+[source]
+--------------------------------------------------
+%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
+--------------------------------------------------
+
+An example pipeline for processing the above document using Grok:
+
+[source,js]
+--------------------------------------------------
+{
+  "description" : "...",
+  "processors": [
+    {
+      "grok": {
+        "match_field": "message",
+        "match_pattern": "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}"
+      }
+    }
+  ]
+}
+--------------------------------------------------
+
+This pipeline will insert these named captures as new fields within the document, like so:
+
+[source,js]
+--------------------------------------------------
+{
+  "message": "55.3.244.1 GET /index.html 15824 0.043",
+  "client": "55.3.244.1",
+  "method": "GET",
+  "request": "/index.html",
+  "bytes": 15824,
+  "duration": "0.043"
+}
+--------------------------------------------------
+
+An example of a pipeline specifying custom pattern definitions:
+
+[source,js]
+--------------------------------------------------
+{
+  "description" : "...",
+  "processors": [
+    {
+      "grok": {
+        "match_field": "message",
+        "match_pattern": "my %{FAVORITE_DOG:dog} is colored %{RGB:color}"
+        "pattern_definitions" : {
+          "FAVORITE_DOG" : "beagle",
+          "RGB" : "RED|GREEN|BLUE"
+        }
+      }
+    }
+  ]
+}
+--------------------------------------------------
+
+==== Date processor
+
+The date processor is used for parsing dates from fields, and then using that date or timestamp as the timestamp for that document.
+The date processor adds by default the parsed date as a new field called `@timestamp`, configurable by setting the `target_field`
+configuration parameter. Multiple date formats are supported as part of the same date processor definition. They will be used
+sequentially to attempt parsing the date field, in the same order they were defined as part of the processor definition.
+
+[[date-options]]
+.Date options
+[options="header"]
+|======
+| Name                   | Required  | Default             | Description
+| `match_field`          | yes       | -                   | The field to get the date from.
+| `target_field`         | no        | @timestamp          | The field that will hold the parsed date.
+| `match_formats`        | yes       | -                   | Array of the expected date formats. Can be a joda pattern or one of the following formats: ISO8601, UNIX, UNIX_MS, TAI64N.
+| `timezone`             | no        | UTC                 | The timezone to use when parsing the date.
+| `locale`               | no        | ENGLISH             | The locale to use when parsing the date, relevant when parsing month names or week days.
+|======
+
+An example that adds the parsed date to the `timestamp` field based on the `initial_date` field:
+
+[source,js]
+--------------------------------------------------
+{
+  "description" : "...",
+  "processors" : [
+    {
+      "date" : {
+        "match_field" : "initial_date",
+        "target_field" : "timestamp",
+        "match_formats" : ["dd/MM/yyyy hh:mm:ss"],
+        "timezone" : "Europe/Amsterdam"
+      }
+    }
+  ]
+}
+--------------------------------------------------
+
+==== Fail processor
+The Fail Processor is used to raise an exception. This is useful for when
+a user expects a pipeline to fail and wishes to relay a specific message
+to the requester.
+
+[[fail-options]]
+.Fail Options
+[options="header"]
+|======
+| Name       | Required  | Default  | Description
+| `message`  | yes       | -        | The error message of the `FailException` thrown by the processor
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "fail": {
+    "message": "an error message"
+  }
+}
+--------------------------------------------------
+
+==== DeDot Processor
+The DeDot Processor is used to remove dots (".") from field names and
+replace them with a specific `separator` string.
+
+[[dedot-options]]
+.DeDot Options
+[options="header"]
+|======
+| Name         | Required  | Default  | Description
+| `separator`  | yes       | "_"      | The string to replace dots with in all field names
+|======
+
+[source,js]
+--------------------------------------------------
+{
+  "dedot": {
+    "separator": "_"
+  }
+}
+--------------------------------------------------
+
+
+=== Accessing data in pipelines
+
+Processors in pipelines have read and write access to documents that pass through the pipeline.
+The fields in the source of a document and its metadata fields are accessible.
+
+Accessing a field in the source is straightforward and one can refer to fields by
+their name. For example:
+
+[source,js]
+--------------------------------------------------
+{
+  "set": {
+    "field": "my_field"
+    "value": 582.1
+  }
+}
+--------------------------------------------------
+
+On top of this fields from the source are always accessible via the `_source` prefix:
+
+[source,js]
+--------------------------------------------------
+{
+  "set": {
+    "field": "_source.my_field"
+    "value": 582.1
+  }
+}
+--------------------------------------------------
+
+Metadata fields can also be accessed in the same way as fields from the source. This
+is possible because Elasticsearch doesn't allow fields in the source that have the
+same name as metadata fields.
+
+The following example sets the id of a document to `1`:
+
+[source,js]
+--------------------------------------------------
+{
+  "set": {
+    "field": "_id"
+    "value": "1"
+  }
+}
+--------------------------------------------------
+
+The following metadata fields are accessible by a processor: `_index`, `_type`, `_id`, `_routing`, `_parent`,
+`_timestamp` and `_ttl`.
+
+Beyond metadata fields and source fields, the ingest plugin also adds ingest metadata to documents being processed.
+These metadata properties are accessible under the `_ingest` key. Currently the ingest plugin adds the ingest timestamp
+under `_ingest.timestamp` key to the ingest metadata, which is the time the ingest plugin received the index or bulk
+request to pre-process. But any processor is free to add more ingest related metadata to it. Ingest metadata is transient
+and is lost after a document has been processed by the pipeline and thus ingest metadata won't be indexed.
+
+The following example adds a field with the name `received` and the value is the ingest timestamp:
+
+[source,js]
+--------------------------------------------------
+{
+  "set": {
+    "field": "received"
+    "value": "{{_ingest.timestamp}}"
+  }
+}
+--------------------------------------------------
+
+As opposed to Elasticsearch metadata fields, the ingest metadata field name _ingest can be used as a valid field name
+in the source of a document. Use _source._ingest to refer to it, otherwise _ingest will be interpreted as ingest
+metadata fields by the ingest plugin.
+
+A number of processor settings also support templating. Settings that support templating can have zero or more
+template snippets. A template snippet begins with `{{` and ends with `}}`.
+Accessing fields and metafields in templates is exactly the same as via regular processor field settings.
+
+In this example a field by the name `field_c` is added and its value is a concatenation of
+the values of `field_a` and `field_b`.
+
+[source,js]
+--------------------------------------------------
+{
+  "set": {
+    "field": "field_c"
+    "value": "{{field_a}} {{field_b}}"
+  }
+}
+--------------------------------------------------
+
+The following example changes the index a document is going to be indexed into. The index a document will be redirected
+to depends on the field in the source with name `geoip.country_iso_code`.
+
+[source,js]
+--------------------------------------------------
+{
+  "set": {
+    "field": "_index"
+    "value": "{{geoip.country_iso_code}}"
+  }
+}
+--------------------------------------------------
+
+==== Handling Failure in Pipelines
+
+In its simplest case, pipelines describe a list of processors which 
+are executed sequentially and processing halts at the first exception. This 
+may not be desirable when failures are expected. For example, not all your logs 
+may match a certain grok expression and you may wish to index such documents into 
+a separate index.
+
+To enable this behavior, you can utilize the `on_failure` parameter. `on_failure` 
+defines a list of processors to be executed immediately following the failed processor.
+This parameter can be supplied at the pipeline level, as well as at the processor 
+level. If a processor has an `on_failure` configuration option provided, whether 
+it is empty or not, any exceptions that are thrown by it will be caught and the 
+pipeline will continue executing the proceeding processors defined. Since further processors
+are defined within the scope of an `on_failure` statement, failure handling can be nested.
+
+Example: In the following example we define a pipeline that hopes to rename documents with 
+a field named `foo` to `bar`. If the document does not contain the `foo` field, we 
+go ahead and attach an error message within the document for later analysis within 
+Elasticsearch.
+
+[source,js]
+--------------------------------------------------
+{
+  "description" : "my first pipeline with handled exceptions",
+  "processors" : [
+    {
+      "rename" : {
+        "field" : "foo",
+        "to" : "bar",
+        "on_failure" : [
+          {
+            "set" : {
+              "field" : "error",
+              "value" : "field \"foo\" does not exist, cannot rename to \"bar\""
+            }
+          }
+        ]
+      }
+    }
+  ]
+}
+--------------------------------------------------
+
+Example: Here we define an `on_failure` block on a whole pipeline to change 
+the index for which failed documents get sent.
+
+[source,js]
+--------------------------------------------------
+{
+  "description" : "my first pipeline with handled exceptions",
+  "processors" : [ ... ],
+  "on_failure" : [
+    {
+      "set" : {
+        "field" : "_index",
+        "value" : "failed-{{ _index }}"
+      }
+    }
+  ]
+}
+--------------------------------------------------
+
+
+===== Accessing Error Metadata From Processors Handling Exceptions
+
+Sometimes you may want to retrieve the actual error message that was thrown 
+by a failed processor. To do so you can access metadata fields called 
+`on_failure_message` and `on_failure_processor`. These fields are only accessible 
+from within the context of an `on_failure` block. Here is an updated version of 
+our first example which leverages these fields to provide the error message instead 
+of manually setting it.
+
+[source,js]
+--------------------------------------------------
+{
+  "description" : "my first pipeline with handled exceptions",
+  "processors" : [
+    {
+      "rename" : {
+        "field" : "foo",
+        "to" : "bar",
+        "on_failure" : [
+          {
+            "set" : {
+              "field" : "error",
+              "value" : "{{ _ingest.on_failure_message }}"
+            }
+          }
+        ]
+      }
+    }
+  ]
+}
+--------------------------------------------------
+
+
+=== Ingest APIs
+
+==== Put pipeline API
+
+The put pipeline api adds pipelines and updates existing pipelines in the cluster.
+
+[source,js]
+--------------------------------------------------
+PUT _ingest/pipeline/my-pipeline-id
+{
+  "description" : "describe pipeline",
+  "processors" : [
+    {
+      "simple" : {
+        // settings
+      }
+    },
+    // other processors
+  ]
+}
+--------------------------------------------------
+// AUTOSENSE
+
+NOTE: The put pipeline api also instructs all ingest nodes to reload their in-memory representation of pipelines, so that
+      pipeline changes take immediately in effect.
+
+==== Get pipeline API
+
+The get pipeline api returns pipelines based on id. This api always returns a local reference of the pipeline.
+
+[source,js]
+--------------------------------------------------
+GET _ingest/pipeline/my-pipeline-id
+--------------------------------------------------
+// AUTOSENSE
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+   "my-pipeline-id": {
+      "_source" : {
+        "description": "describe pipeline",
+        "processors": [
+          {
+            "simple" : {
+              // settings
+            }
+          },
+          // other processors
+        ]
+      },
+      "_version" : 0
+   }
+}
+--------------------------------------------------
+
+For each returned pipeline the source and the version is returned.
+The version is useful for knowing what version of the pipeline the node has.
+Multiple ids can be provided at the same time. Also wildcards are supported.
+
+==== Delete pipeline API
+
+The delete pipeline api deletes pipelines by id.
+
+[source,js]
+--------------------------------------------------
+DELETE _ingest/pipeline/my-pipeline-id
+--------------------------------------------------
+// AUTOSENSE
+
+==== Simulate pipeline API
+
+The simulate pipeline api executes a specific pipeline against
+the set of documents provided in the body of the request.
+
+A simulate request may call upon an existing pipeline to be executed
+against the provided documents, or supply a pipeline definition in
+the body of the request.
+
+Here is the structure of a simulate request with a provided pipeline:
+
+[source,js]
+--------------------------------------------------
+POST _ingest/pipeline/_simulate
+{
+  "pipeline" : {
+    // pipeline definition here
+  },
+  "docs" : [
+    { /** first document **/ },
+    { /** second document **/ },
+    // ...
+  ]
+}
+--------------------------------------------------
+
+Here is the structure of a simulate request against a pre-existing pipeline:
+
+[source,js]
+--------------------------------------------------
+POST _ingest/pipeline/my-pipeline-id/_simulate
+{
+  "docs" : [
+    { /** first document **/ },
+    { /** second document **/ },
+    // ...
+  ]
+}
+--------------------------------------------------
+
+
+Here is an example simulate request with a provided pipeline and its response:
+
+[source,js]
+--------------------------------------------------
+POST _ingest/pipeline/_simulate
+{
+  "pipeline" :
+  {
+    "description": "_description",
+    "processors": [
+      {
+        "set" : {
+          "field" : "field2",
+          "value" : "_value"
+        }
+      }
+    ]
+  },
+  "docs": [
+    {
+      "_index": "index",
+      "_type": "type",
+      "_id": "id",
+      "_source": {
+        "foo": "bar"
+      }
+    },
+    {
+      "_index": "index",
+      "_type": "type",
+      "_id": "id",
+      "_source": {
+        "foo": "rab"
+      }
+    }
+  ]
+}
+--------------------------------------------------
+// AUTOSENSE
+
+response:
+
+[source,js]
+--------------------------------------------------
+{
+   "docs": [
+      {
+         "doc": {
+            "_id": "id",
+            "_ttl": null,
+            "_parent": null,
+            "_index": "index",
+            "_routing": null,
+            "_type": "type",
+            "_timestamp": null,
+            "_source": {
+               "field2": "_value",
+               "foo": "bar"
+            },
+            "_ingest": {
+               "timestamp": "2016-01-04T23:53:27.186+0000"
+            }
+         }
+      },
+      {
+         "doc": {
+            "_id": "id",
+            "_ttl": null,
+            "_parent": null,
+            "_index": "index",
+            "_routing": null,
+            "_type": "type",
+            "_timestamp": null,
+            "_source": {
+               "field2": "_value",
+               "foo": "rab"
+            },
+            "_ingest": {
+               "timestamp": "2016-01-04T23:53:27.186+0000"
+            }
+         }
+      }
+   ]
+}
+--------------------------------------------------
+
+It is often useful to see how each processor affects the ingest document
+as it is passed through the pipeline. To see the intermediate results of
+each processor in the simulat request, a `verbose` parameter may be added
+to the request
+
+Here is an example verbose request and its response:
+
+
+[source,js]
+--------------------------------------------------
+POST _ingest/pipeline/_simulate?verbose
+{
+  "pipeline" :
+  {
+    "description": "_description",
+    "processors": [
+      {
+        "set" : {
+          "field" : "field2",
+          "value" : "_value2"
+        }
+      },
+      {
+        "set" : {
+          "field" : "field3",
+          "value" : "_value3"
+        }
+      }
+    ]
+  },
+  "docs": [
+    {
+      "_index": "index",
+      "_type": "type",
+      "_id": "id",
+      "_source": {
+        "foo": "bar"
+      }
+    },
+    {
+      "_index": "index",
+      "_type": "type",
+      "_id": "id",
+      "_source": {
+        "foo": "rab"
+      }
+    }
+  ]
+}
+--------------------------------------------------
+// AUTOSENSE
+
+response:
+
+[source,js]
+--------------------------------------------------
+{
+   "docs": [
+      {
+         "processor_results": [
+            {
+               "processor_id": "processor[set]-0",
+               "doc": {
+                  "_id": "id",
+                  "_ttl": null,
+                  "_parent": null,
+                  "_index": "index",
+                  "_routing": null,
+                  "_type": "type",
+                  "_timestamp": null,
+                  "_source": {
+                     "field2": "_value2",
+                     "foo": "bar"
+                  },
+                  "_ingest": {
+                     "timestamp": "2016-01-05T00:02:51.383+0000"
+                  }
+               }
+            },
+            {
+               "processor_id": "processor[set]-1",
+               "doc": {
+                  "_id": "id",
+                  "_ttl": null,
+                  "_parent": null,
+                  "_index": "index",
+                  "_routing": null,
+                  "_type": "type",
+                  "_timestamp": null,
+                  "_source": {
+                     "field3": "_value3",
+                     "field2": "_value2",
+                     "foo": "bar"
+                  },
+                  "_ingest": {
+                     "timestamp": "2016-01-05T00:02:51.383+0000"
+                  }
+               }
+            }
+         ]
+      },
+      {
+         "processor_results": [
+            {
+               "processor_id": "processor[set]-0",
+               "doc": {
+                  "_id": "id",
+                  "_ttl": null,
+                  "_parent": null,
+                  "_index": "index",
+                  "_routing": null,
+                  "_type": "type",
+                  "_timestamp": null,
+                  "_source": {
+                     "field2": "_value2",
+                     "foo": "rab"
+                  },
+                  "_ingest": {
+                     "timestamp": "2016-01-05T00:02:51.384+0000"
+                  }
+               }
+            },
+            {
+               "processor_id": "processor[set]-1",
+               "doc": {
+                  "_id": "id",
+                  "_ttl": null,
+                  "_parent": null,
+                  "_index": "index",
+                  "_routing": null,
+                  "_type": "type",
+                  "_timestamp": null,
+                  "_source": {
+                     "field3": "_value3",
+                     "field2": "_value2",
+                     "foo": "rab"
+                  },
+                  "_ingest": {
+                     "timestamp": "2016-01-05T00:02:51.384+0000"
+                  }
+               }
+            }
+         ]
+      }
+   ]
+}
+--------------------------------------------------
diff --git a/docs/reference/mapping/fields/timestamp-field.asciidoc b/docs/reference/mapping/fields/timestamp-field.asciidoc
index 5971a02..3f4bf8a 100644
--- a/docs/reference/mapping/fields/timestamp-field.asciidoc
+++ b/docs/reference/mapping/fields/timestamp-field.asciidoc
@@ -1,6 +1,8 @@
 [[mapping-timestamp-field]]
 === `_timestamp` field
 
+deprecated[2.0.0,The `_timestamp` field is deprecated.  Instead, use a normal <<date,`date`>> field and set its value explicitly]
+
 The `_timestamp` field, when enabled, allows a timestamp to be indexed and
 stored with a document. The timestamp may be specified manually, generated
 automatically, or set to a default value:
diff --git a/docs/reference/mapping/fields/ttl-field.asciidoc b/docs/reference/mapping/fields/ttl-field.asciidoc
index d81582c..9bfdc72 100644
--- a/docs/reference/mapping/fields/ttl-field.asciidoc
+++ b/docs/reference/mapping/fields/ttl-field.asciidoc
@@ -1,6 +1,8 @@
 [[mapping-ttl-field]]
 === `_ttl` field
 
+deprecated[2.0.0,The current `_ttl` implementation is deprecated and will be replaced with a different implementation in a future version]
+
 Some types of documents, such as session data or special offers, come with an
 expiration date. The `_ttl` field allows you to specify the minimum time a
 document should live, after which time the document is deleted automatically.
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index 76b1ddb..b4aa2d6 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -644,6 +644,10 @@ The percolate api can no longer accept documents that have fields that don't exi
 When percolating an existing document then specifying a document in the source of the percolate request is not allowed
 any more.
 
+The percolate api no longer modifies the mappings. Before the percolate api could be used to dynamically introduce new
+fields to the mappings based on the fields in the document being percolated. This no longer works, because these
+unmapped fields are not persisted in the mapping.
+
 Percolator documents are no longer excluded from the search response.
 
 [[breaking_30_packaging]]
diff --git a/docs/reference/search/percolate.asciidoc b/docs/reference/search/percolate.asciidoc
index 7f160d1..4ac1b6b 100644
--- a/docs/reference/search/percolate.asciidoc
+++ b/docs/reference/search/percolate.asciidoc
@@ -20,14 +20,8 @@ in a request to the percolate API.
 =====================================
 
 Fields referred to in a percolator query must *already* exist in the mapping
-associated with the index used for percolation.
-There are two ways to make sure that a field mapping exist:
-
-* Add or update a mapping via the <<indices-create-index,create index>> or
-  <<indices-put-mapping,put mapping>> APIs.
-* Percolate a document before registering a query. Percolating a document can
-  add field mappings dynamically, in the same way as happens when indexing a
-  document.
+associated with the index used for percolation. In order to make sure these fields exist,
+add or update a mapping via the <<indices-create-index,create index>> or <<indices-put-mapping,put mapping>> APIs.
 
 =====================================
 
diff --git a/modules/ingest-grok/build.gradle b/modules/ingest-grok/build.gradle
new file mode 100644
index 0000000..2672234
--- /dev/null
+++ b/modules/ingest-grok/build.gradle
@@ -0,0 +1,39 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+esplugin {
+    description 'Ingest processor that uses grok patterns to split text'
+    classname 'org.elasticsearch.ingest.grok.IngestGrokPlugin'
+}
+
+dependencies {
+    compile 'org.jruby.joni:joni:2.1.6'
+    // joni dependencies:
+    compile 'org.jruby.jcodings:jcodings:1.0.12'
+}
+
+compileJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked,-serial"
+compileTestJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked"
+
+thirdPartyAudit.excludes = [
+        // joni has AsmCompilerSupport, but that isn't being used:
+        'org.objectweb.asm.ClassWriter',
+        'org.objectweb.asm.MethodVisitor',
+        'org.objectweb.asm.Opcodes',
+]
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1 b/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1
new file mode 100644
index 0000000..b097e32
--- /dev/null
+++ b/modules/ingest-grok/licenses/jcodings-1.0.12.jar.sha1
@@ -0,0 +1 @@
+6bc17079fcaa8823ea8cd0d4c66516335b558db8
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-LICENSE.txt b/modules/ingest-grok/licenses/jcodings-LICENSE.txt
new file mode 100644
index 0000000..a3fdf73
--- /dev/null
+++ b/modules/ingest-grok/licenses/jcodings-LICENSE.txt
@@ -0,0 +1,17 @@
+Permission is hereby granted, free of charge, to any person obtaining a copy of
+this software and associated documentation files (the "Software"), to deal in
+the Software without restriction, including without limitation the rights to
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
+of the Software, and to permit persons to whom the Software is furnished to do
+so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/jcodings-NOTICE.txt b/modules/ingest-grok/licenses/jcodings-NOTICE.txt
new file mode 100644
index 0000000..f6c4948
--- /dev/null
+++ b/modules/ingest-grok/licenses/jcodings-NOTICE.txt
@@ -0,0 +1 @@
+JCodings is released under the MIT License.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1 b/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1
new file mode 100644
index 0000000..48abe13
--- /dev/null
+++ b/modules/ingest-grok/licenses/joni-2.1.6.jar.sha1
@@ -0,0 +1 @@
+0f23c95a06eaecbc8c74c7458a8bfd13e4fd2d3a
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-LICENSE.txt b/modules/ingest-grok/licenses/joni-LICENSE.txt
new file mode 100644
index 0000000..a3fdf73
--- /dev/null
+++ b/modules/ingest-grok/licenses/joni-LICENSE.txt
@@ -0,0 +1,17 @@
+Permission is hereby granted, free of charge, to any person obtaining a copy of
+this software and associated documentation files (the "Software"), to deal in
+the Software without restriction, including without limitation the rights to
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
+of the Software, and to permit persons to whom the Software is furnished to do
+so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
\ No newline at end of file
diff --git a/modules/ingest-grok/licenses/joni-NOTICE.txt b/modules/ingest-grok/licenses/joni-NOTICE.txt
new file mode 100644
index 0000000..45bc517
--- /dev/null
+++ b/modules/ingest-grok/licenses/joni-NOTICE.txt
@@ -0,0 +1 @@
+Joni is released under the MIT License.
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java
new file mode 100644
index 0000000..abed841
--- /dev/null
+++ b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/Grok.java
@@ -0,0 +1,158 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.grok;
+
+import org.jcodings.specific.UTF8Encoding;
+import org.joni.Matcher;
+import org.joni.NameEntry;
+import org.joni.Option;
+import org.joni.Regex;
+import org.joni.Region;
+import org.joni.Syntax;
+import org.joni.exception.ValueException;
+
+import java.nio.charset.StandardCharsets;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Locale;
+import java.util.Map;
+
+final class Grok {
+
+    private static final String NAME_GROUP = "name";
+    private static final String SUBNAME_GROUP = "subname";
+    private static final String PATTERN_GROUP = "pattern";
+    private static final String DEFINITION_GROUP = "definition";
+    private static final String GROK_PATTERN =
+            "%\\{" +
+            "(?<name>" +
+            "(?<pattern>[A-z0-9]+)" +
+            "(?::(?<subname>[A-z0-9_:.-]+))?" +
+            ")" +
+            "(?:=(?<definition>" +
+            "(?:" +
+            "(?:[^{}]+|\\.+)+" +
+            ")+" +
+            ")" +
+            ")?" + "\\}";
+    private static final Regex GROK_PATTERN_REGEX = new Regex(GROK_PATTERN.getBytes(StandardCharsets.UTF_8), 0, GROK_PATTERN.getBytes(StandardCharsets.UTF_8).length, Option.NONE, UTF8Encoding.INSTANCE, Syntax.DEFAULT);
+    private final Map<String, String> patternBank;
+    private final boolean namedCaptures;
+    private final Regex compiledExpression;
+    private final String expression;
+
+
+    public Grok(Map<String, String> patternBank, String grokPattern) {
+        this(patternBank, grokPattern, true);
+    }
+
+    @SuppressWarnings("unchecked")
+    public Grok(Map<String, String> patternBank, String grokPattern, boolean namedCaptures) {
+        this.patternBank = patternBank;
+        this.namedCaptures = namedCaptures;
+
+        this.expression = toRegex(grokPattern);
+        byte[] expressionBytes = expression.getBytes(StandardCharsets.UTF_8);
+        this.compiledExpression = new Regex(expressionBytes, 0, expressionBytes.length, Option.DEFAULT, UTF8Encoding.INSTANCE);
+    }
+
+
+    public String groupMatch(String name, Region region, String pattern) {
+        try {
+            int number = GROK_PATTERN_REGEX.nameToBackrefNumber(name.getBytes(StandardCharsets.UTF_8), 0, name.getBytes(StandardCharsets.UTF_8).length, region);
+            int begin = region.beg[number];
+            int end = region.end[number];
+            return new String(pattern.getBytes(StandardCharsets.UTF_8), begin, end - begin, StandardCharsets.UTF_8);
+        } catch (StringIndexOutOfBoundsException e) {
+            return null;
+        } catch (ValueException e) {
+            return null;
+        }
+    }
+
+    /**
+     * converts a grok expression into a named regex expression
+     *
+     * @return named regex expression
+     */
+    public String toRegex(String grokPattern) {
+        byte[] grokPatternBytes = grokPattern.getBytes(StandardCharsets.UTF_8);
+        Matcher matcher = GROK_PATTERN_REGEX.matcher(grokPatternBytes);
+
+        int result = matcher.search(0, grokPatternBytes.length, Option.NONE);
+        if (result != -1) {
+            Region region = matcher.getEagerRegion();
+            String namedPatternRef = groupMatch(NAME_GROUP, region, grokPattern);
+            String subName = groupMatch(SUBNAME_GROUP, region, grokPattern);
+            // TODO(tal): Support definitions
+            String definition = groupMatch(DEFINITION_GROUP, region, grokPattern);
+            String patternName = groupMatch(PATTERN_GROUP, region, grokPattern);
+            String pattern = patternBank.get(patternName);
+
+            String grokPart;
+            if (namedCaptures && subName != null) {
+                grokPart = String.format(Locale.US, "(?<%s>%s)", namedPatternRef, pattern);
+            } else if (!namedCaptures) {
+                grokPart = String.format(Locale.US, "(?<%s>%s)", patternName + "_" + String.valueOf(result), pattern);
+            } else {
+                grokPart = String.format(Locale.US, "(?:%s)", pattern);
+            }
+
+            String start = new String(grokPatternBytes, 0, result, StandardCharsets.UTF_8);
+            String rest = new String(grokPatternBytes, region.end[0], grokPatternBytes.length - region.end[0], StandardCharsets.UTF_8);
+            return start + toRegex(grokPart + rest);
+        }
+
+        return grokPattern;
+    }
+
+    public boolean match(String text) {
+        Matcher matcher = compiledExpression.matcher(text.getBytes(StandardCharsets.UTF_8));
+        int result = matcher.search(0, text.length(), Option.DEFAULT);
+        return (result != -1);
+    }
+
+    public Map<String, Object> captures(String text) {
+        byte[] textAsBytes = text.getBytes(StandardCharsets.UTF_8);
+        Map<String, Object> fields = new HashMap<>();
+        Matcher matcher = compiledExpression.matcher(textAsBytes);
+        int result = matcher.search(0, textAsBytes.length, Option.DEFAULT);
+        if (result != -1 && compiledExpression.numberOfNames() > 0) {
+            Region region = matcher.getEagerRegion();
+            for (Iterator<NameEntry> entry = compiledExpression.namedBackrefIterator(); entry.hasNext();) {
+                NameEntry e = entry.next();
+                int number = e.getBackRefs()[0];
+
+                String groupName = new String(e.name, e.nameP, e.nameEnd - e.nameP, StandardCharsets.UTF_8);
+                String matchValue = null;
+                if (region.beg[number] >= 0) {
+                    matchValue = new String(textAsBytes, region.beg[number], region.end[number] - region.beg[number], StandardCharsets.UTF_8);
+                }
+                GrokMatchGroup match = new GrokMatchGroup(groupName, matchValue);
+                fields.put(match.getName(), match.getValue());
+            }
+            return fields;
+        } else if (result != -1) {
+            return fields;
+        }
+        return null;
+    }
+}
+
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java
new file mode 100644
index 0000000..2cebf62
--- /dev/null
+++ b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokMatchGroup.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.grok;
+
+final class GrokMatchGroup {
+    private static final String DEFAULT_TYPE = "string";
+    private final String patternName;
+    private final String fieldName;
+    private final String type;
+    private final String groupValue;
+
+    public GrokMatchGroup(String groupName, String groupValue) {
+        String[] parts = groupName.split(":");
+        patternName = parts[0];
+        if (parts.length >= 2) {
+            fieldName = parts[1];
+        } else {
+            fieldName = null;
+        }
+
+        if (parts.length == 3) {
+            type = parts[2];
+        } else {
+            type = DEFAULT_TYPE;
+        }
+        this.groupValue = groupValue;
+    }
+
+    public String getName() {
+        return (fieldName == null) ? patternName : fieldName;
+    }
+
+    public Object getValue() {
+        if (groupValue == null) { return null; }
+
+        switch(type) {
+            case "int":
+                return Integer.parseInt(groupValue);
+            case "float":
+                return Float.parseFloat(groupValue);
+            default:
+                return groupValue;
+        }
+    }
+}
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java
new file mode 100644
index 0000000..4df8d67
--- /dev/null
+++ b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/GrokProcessor.java
@@ -0,0 +1,91 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.grok;
+
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.ConfigurationUtils;
+import org.elasticsearch.ingest.core.IngestDocument;
+
+import java.util.HashMap;
+import java.util.Map;
+
+public final class GrokProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "grok";
+
+    private final String matchField;
+    private final Grok grok;
+
+    public GrokProcessor(String tag, Grok grok, String matchField) {
+        super(tag);
+        this.matchField = matchField;
+        this.grok = grok;
+    }
+
+    @Override
+    public void execute(IngestDocument ingestDocument) throws Exception {
+        String fieldValue = ingestDocument.getFieldValue(matchField, String.class);
+        Map<String, Object> matches = grok.captures(fieldValue);
+        if (matches != null) {
+            matches.forEach((k, v) -> ingestDocument.setFieldValue(k, v));
+        } else {
+            throw new IllegalArgumentException("Grok expression does not match field value: [" + fieldValue + "]");
+        }
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    String getMatchField() {
+        return matchField;
+    }
+
+    Grok getGrok() {
+        return grok;
+    }
+
+    public final static class Factory extends AbstractProcessorFactory<GrokProcessor> {
+
+        private final Map<String, String> builtinPatterns;
+
+        public Factory(Map<String, String> builtinPatterns) {
+            this.builtinPatterns = builtinPatterns;
+        }
+
+        @Override
+        public GrokProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String matchField = ConfigurationUtils.readStringProperty(config, "field");
+            String matchPattern = ConfigurationUtils.readStringProperty(config, "pattern");
+            Map<String, String> customPatternBank = ConfigurationUtils.readOptionalMap(config, "pattern_definitions");
+            Map<String, String> patternBank = new HashMap<>(builtinPatterns);
+            if (customPatternBank != null) {
+                patternBank.putAll(customPatternBank);
+            }
+
+            Grok grok = new Grok(patternBank, matchPattern);
+            return new GrokProcessor(processorTag, grok, matchField);
+        }
+
+    }
+
+}
diff --git a/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
new file mode 100644
index 0000000..54800ac
--- /dev/null
+++ b/modules/ingest-grok/src/main/java/org/elasticsearch/ingest/grok/IngestGrokPlugin.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.grok;
+
+import org.elasticsearch.node.NodeModule;
+import org.elasticsearch.plugins.Plugin;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+public class IngestGrokPlugin extends Plugin {
+
+    private static final String[] PATTERN_NAMES = new String[] {
+        "aws", "bacula", "bro", "exim", "firewalls", "grok-patterns", "haproxy",
+        "java", "junos", "linux-syslog", "mcollective-patterns", "mongodb", "nagios",
+        "postgresql", "rails", "redis", "ruby"
+    };
+
+    private final Map<String, String> builtinPatterns;
+
+    public IngestGrokPlugin() throws IOException {
+        this.builtinPatterns = loadBuiltinPatterns();
+    }
+
+    @Override
+    public String name() {
+        return "ingest-grok";
+    }
+
+    @Override
+    public String description() {
+        return "Ingest processor that uses grok patterns to split text";
+    }
+
+    public void onModule(NodeModule nodeModule) {
+        nodeModule.registerProcessor(GrokProcessor.TYPE, (templateService) -> new GrokProcessor.Factory(builtinPatterns));
+    }
+
+    static Map<String, String> loadBuiltinPatterns() throws IOException {
+        Map<String, String> builtinPatterns = new HashMap<>();
+        for (String pattern : PATTERN_NAMES) {
+            try(InputStream is = IngestGrokPlugin.class.getResourceAsStream("/patterns/" + pattern)) {
+                loadPatterns(builtinPatterns, is);
+            }
+        }
+        return Collections.unmodifiableMap(builtinPatterns);
+    }
+
+    private static void loadPatterns(Map<String, String> patternBank, InputStream inputStream) throws IOException {
+        String line;
+        BufferedReader br = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));
+        while ((line = br.readLine()) != null) {
+            String trimmedLine = line.replaceAll("^\\s+", "");
+            if (trimmedLine.startsWith("#") || trimmedLine.length() == 0) {
+                continue;
+            }
+
+            String[] parts = trimmedLine.split("\\s+", 2);
+            if (parts.length == 2) {
+                patternBank.put(parts[0], parts[1]);
+            }
+        }
+    }
+}
diff --git a/modules/ingest-grok/src/main/resources/patterns/aws b/modules/ingest-grok/src/main/resources/patterns/aws
new file mode 100644
index 0000000..71edbc9
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/aws
@@ -0,0 +1,11 @@
+S3_REQUEST_LINE (?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})
+
+S3_ACCESS_LOG %{WORD:owner} %{NOTSPACE:bucket} \[%{HTTPDATE:timestamp}\] %{IP:clientip} %{NOTSPACE:requester} %{NOTSPACE:request_id} %{NOTSPACE:operation} %{NOTSPACE:key} (?:"%{S3_REQUEST_LINE}"|-) (?:%{INT:response:int}|-) (?:-|%{NOTSPACE:error_code}) (?:%{INT:bytes:int}|-) (?:%{INT:object_size:int}|-) (?:%{INT:request_time_ms:int}|-) (?:%{INT:turnaround_time_ms:int}|-) (?:%{QS:referrer}|-) (?:"?%{QS:agent}"?|-) (?:-|%{NOTSPACE:version_id})
+
+ELB_URIPATHPARAM %{URIPATH:path}(?:%{URIPARAM:params})?
+
+ELB_URI %{URIPROTO:proto}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST:urihost})?(?:%{ELB_URIPATHPARAM})?
+
+ELB_REQUEST_LINE (?:%{WORD:verb} %{ELB_URI:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})
+
+ELB_ACCESS_LOG %{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:elb} %{IP:clientip}:%{INT:clientport:int} (?:(%{IP:backendip}:?:%{INT:backendport:int})|-) %{NUMBER:request_processing_time:float} %{NUMBER:backend_processing_time:float} %{NUMBER:response_processing_time:float} %{INT:response:int} %{INT:backend_response:int} %{INT:received_bytes:int} %{INT:bytes:int} "%{ELB_REQUEST_LINE}"
diff --git a/modules/ingest-grok/src/main/resources/patterns/bacula b/modules/ingest-grok/src/main/resources/patterns/bacula
new file mode 100644
index 0000000..d80dfe5
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/bacula
@@ -0,0 +1,50 @@
+BACULA_TIMESTAMP %{MONTHDAY}-%{MONTH} %{HOUR}:%{MINUTE}
+BACULA_HOST [a-zA-Z0-9-]+
+BACULA_VOLUME %{USER}
+BACULA_DEVICE %{USER}
+BACULA_DEVICEPATH %{UNIXPATH}
+BACULA_CAPACITY %{INT}{1,3}(,%{INT}{3})*
+BACULA_VERSION %{USER}
+BACULA_JOB %{USER}
+
+BACULA_LOG_MAX_CAPACITY User defined maximum volume capacity %{BACULA_CAPACITY} exceeded on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\)
+BACULA_LOG_END_VOLUME End of medium on Volume \"%{BACULA_VOLUME:volume}\" Bytes=%{BACULA_CAPACITY} Blocks=%{BACULA_CAPACITY} at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}.
+BACULA_LOG_NEW_VOLUME Created new Volume \"%{BACULA_VOLUME:volume}\" in catalog.
+BACULA_LOG_NEW_LABEL Labeled new Volume \"%{BACULA_VOLUME:volume}\" on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\).
+BACULA_LOG_WROTE_LABEL Wrote label to prelabeled Volume \"%{BACULA_VOLUME:volume}\" on device \"%{BACULA_DEVICE}\" \(%{BACULA_DEVICEPATH}\)
+BACULA_LOG_NEW_MOUNT New volume \"%{BACULA_VOLUME:volume}\" mounted on device \"%{BACULA_DEVICE:device}\" \(%{BACULA_DEVICEPATH}\) at %{MONTHDAY}-%{MONTH}-%{YEAR} %{HOUR}:%{MINUTE}.
+BACULA_LOG_NOOPEN \s+Cannot open %{DATA}: ERR=%{GREEDYDATA:berror}
+BACULA_LOG_NOOPENDIR \s+Could not open directory %{DATA}: ERR=%{GREEDYDATA:berror}
+BACULA_LOG_NOSTAT \s+Could not stat %{DATA}: ERR=%{GREEDYDATA:berror}
+BACULA_LOG_NOJOBS There are no more Jobs associated with Volume \"%{BACULA_VOLUME:volume}\". Marking it purged.
+BACULA_LOG_ALL_RECORDS_PRUNED All records pruned from Volume \"%{BACULA_VOLUME:volume}\"; marking it \"Purged\"
+BACULA_LOG_BEGIN_PRUNE_JOBS Begin pruning Jobs older than %{INT} month %{INT} days .
+BACULA_LOG_BEGIN_PRUNE_FILES Begin pruning Files.
+BACULA_LOG_PRUNED_JOBS Pruned %{INT} Jobs* for client %{BACULA_HOST:client} from catalog.
+BACULA_LOG_PRUNED_FILES Pruned Files from %{INT} Jobs* for client %{BACULA_HOST:client} from catalog.
+BACULA_LOG_ENDPRUNE End auto prune.
+BACULA_LOG_STARTJOB Start Backup JobId %{INT}, Job=%{BACULA_JOB:job}
+BACULA_LOG_STARTRESTORE Start Restore Job %{BACULA_JOB:job}
+BACULA_LOG_USEDEVICE Using Device \"%{BACULA_DEVICE:device}\"
+BACULA_LOG_DIFF_FS \s+%{UNIXPATH} is a different filesystem. Will not descend from %{UNIXPATH} into it.
+BACULA_LOG_JOBEND Job write elapsed time = %{DATA:elapsed}, Transfer rate = %{NUMBER} (K|M|G)? Bytes/second
+BACULA_LOG_NOPRUNE_JOBS No Jobs found to prune.
+BACULA_LOG_NOPRUNE_FILES No Files found to prune.
+BACULA_LOG_VOLUME_PREVWRITTEN Volume \"%{BACULA_VOLUME:volume}\" previously written, moving to end of data.
+BACULA_LOG_READYAPPEND Ready to append to end of Volume \"%{BACULA_VOLUME:volume}\" size=%{INT}
+BACULA_LOG_CANCELLING Cancelling duplicate JobId=%{INT}.
+BACULA_LOG_MARKCANCEL JobId %{INT}, Job %{BACULA_JOB:job} marked to be canceled.
+BACULA_LOG_CLIENT_RBJ shell command: run ClientRunBeforeJob \"%{GREEDYDATA:runjob}\"
+BACULA_LOG_VSS (Generate )?VSS (Writer)?
+BACULA_LOG_MAXSTART Fatal error: Job canceled because max start delay time exceeded.
+BACULA_LOG_DUPLICATE Fatal error: JobId %{INT:duplicate} already running. Duplicate job not allowed.
+BACULA_LOG_NOJOBSTAT Fatal error: No Job status returned from FD.
+BACULA_LOG_FATAL_CONN Fatal error: bsock.c:133 Unable to connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})
+BACULA_LOG_NO_CONNECT Warning: bsock.c:127 Could not connect to (Client: %{BACULA_HOST:client}|Storage daemon) on %{HOSTNAME}:%{POSINT}. ERR=(?<berror>%{GREEDYDATA})
+BACULA_LOG_NO_AUTH Fatal error: Unable to authenticate with File daemon at %{HOSTNAME}. Possible causes:
+BACULA_LOG_NOSUIT No prior or suitable Full backup found in catalog. Doing FULL backup.
+BACULA_LOG_NOPRIOR No prior Full backup Job record found.
+
+BACULA_LOG_JOB (Error: )?Bacula %{BACULA_HOST} %{BACULA_VERSION} \(%{BACULA_VERSION}\):
+
+BACULA_LOGLINE %{BACULA_TIMESTAMP:bts} %{BACULA_HOST:hostname} JobId %{INT:jobid}: (%{BACULA_LOG_MAX_CAPACITY}|%{BACULA_LOG_END_VOLUME}|%{BACULA_LOG_NEW_VOLUME}|%{BACULA_LOG_NEW_LABEL}|%{BACULA_LOG_WROTE_LABEL}|%{BACULA_LOG_NEW_MOUNT}|%{BACULA_LOG_NOOPEN}|%{BACULA_LOG_NOOPENDIR}|%{BACULA_LOG_NOSTAT}|%{BACULA_LOG_NOJOBS}|%{BACULA_LOG_ALL_RECORDS_PRUNED}|%{BACULA_LOG_BEGIN_PRUNE_JOBS}|%{BACULA_LOG_BEGIN_PRUNE_FILES}|%{BACULA_LOG_PRUNED_JOBS}|%{BACULA_LOG_PRUNED_FILES}|%{BACULA_LOG_ENDPRUNE}|%{BACULA_LOG_STARTJOB}|%{BACULA_LOG_STARTRESTORE}|%{BACULA_LOG_USEDEVICE}|%{BACULA_LOG_DIFF_FS}|%{BACULA_LOG_JOBEND}|%{BACULA_LOG_NOPRUNE_JOBS}|%{BACULA_LOG_NOPRUNE_FILES}|%{BACULA_LOG_VOLUME_PREVWRITTEN}|%{BACULA_LOG_READYAPPEND}|%{BACULA_LOG_CANCELLING}|%{BACULA_LOG_MARKCANCEL}|%{BACULA_LOG_CLIENT_RBJ}|%{BACULA_LOG_VSS}|%{BACULA_LOG_MAXSTART}|%{BACULA_LOG_DUPLICATE}|%{BACULA_LOG_NOJOBSTAT}|%{BACULA_LOG_FATAL_CONN}|%{BACULA_LOG_NO_CONNECT}|%{BACULA_LOG_NO_AUTH}|%{BACULA_LOG_NOSUIT}|%{BACULA_LOG_JOB}|%{BACULA_LOG_NOPRIOR})
diff --git a/modules/ingest-grok/src/main/resources/patterns/bro b/modules/ingest-grok/src/main/resources/patterns/bro
new file mode 100644
index 0000000..31b138b
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/bro
@@ -0,0 +1,13 @@
+# https://www.bro.org/sphinx/script-reference/log-files.html
+
+# http.log
+BRO_HTTP %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{INT:trans_depth}\t%{GREEDYDATA:method}\t%{GREEDYDATA:domain}\t%{GREEDYDATA:uri}\t%{GREEDYDATA:referrer}\t%{GREEDYDATA:user_agent}\t%{NUMBER:request_body_len}\t%{NUMBER:response_body_len}\t%{GREEDYDATA:status_code}\t%{GREEDYDATA:status_msg}\t%{GREEDYDATA:info_code}\t%{GREEDYDATA:info_msg}\t%{GREEDYDATA:filename}\t%{GREEDYDATA:bro_tags}\t%{GREEDYDATA:username}\t%{GREEDYDATA:password}\t%{GREEDYDATA:proxied}\t%{GREEDYDATA:orig_fuids}\t%{GREEDYDATA:orig_mime_types}\t%{GREEDYDATA:resp_fuids}\t%{GREEDYDATA:resp_mime_types}
+
+# dns.log
+BRO_DNS %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{WORD:proto}\t%{INT:trans_id}\t%{GREEDYDATA:query}\t%{GREEDYDATA:qclass}\t%{GREEDYDATA:qclass_name}\t%{GREEDYDATA:qtype}\t%{GREEDYDATA:qtype_name}\t%{GREEDYDATA:rcode}\t%{GREEDYDATA:rcode_name}\t%{GREEDYDATA:AA}\t%{GREEDYDATA:TC}\t%{GREEDYDATA:RD}\t%{GREEDYDATA:RA}\t%{GREEDYDATA:Z}\t%{GREEDYDATA:answers}\t%{GREEDYDATA:TTLs}\t%{GREEDYDATA:rejected}
+
+# conn.log
+BRO_CONN %{NUMBER:ts}\t%{NOTSPACE:uid}\t%{IP:orig_h}\t%{INT:orig_p}\t%{IP:resp_h}\t%{INT:resp_p}\t%{WORD:proto}\t%{GREEDYDATA:service}\t%{NUMBER:duration}\t%{NUMBER:orig_bytes}\t%{NUMBER:resp_bytes}\t%{GREEDYDATA:conn_state}\t%{GREEDYDATA:local_orig}\t%{GREEDYDATA:missed_bytes}\t%{GREEDYDATA:history}\t%{GREEDYDATA:orig_pkts}\t%{GREEDYDATA:orig_ip_bytes}\t%{GREEDYDATA:resp_pkts}\t%{GREEDYDATA:resp_ip_bytes}\t%{GREEDYDATA:tunnel_parents}
+
+# files.log
+BRO_FILES %{NUMBER:ts}\t%{NOTSPACE:fuid}\t%{IP:tx_hosts}\t%{IP:rx_hosts}\t%{NOTSPACE:conn_uids}\t%{GREEDYDATA:source}\t%{GREEDYDATA:depth}\t%{GREEDYDATA:analyzers}\t%{GREEDYDATA:mime_type}\t%{GREEDYDATA:filename}\t%{GREEDYDATA:duration}\t%{GREEDYDATA:local_orig}\t%{GREEDYDATA:is_orig}\t%{GREEDYDATA:seen_bytes}\t%{GREEDYDATA:total_bytes}\t%{GREEDYDATA:missing_bytes}\t%{GREEDYDATA:overflow_bytes}\t%{GREEDYDATA:timedout}\t%{GREEDYDATA:parent_fuid}\t%{GREEDYDATA:md5}\t%{GREEDYDATA:sha1}\t%{GREEDYDATA:sha256}\t%{GREEDYDATA:extracted}
diff --git a/modules/ingest-grok/src/main/resources/patterns/exim b/modules/ingest-grok/src/main/resources/patterns/exim
new file mode 100644
index 0000000..68c4e5c
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/exim
@@ -0,0 +1,13 @@
+EXIM_MSGID [0-9A-Za-z]{6}-[0-9A-Za-z]{6}-[0-9A-Za-z]{2}
+EXIM_FLAGS (<=|[-=>*]>|[*]{2}|==)
+EXIM_DATE %{YEAR:exim_year}-%{MONTHNUM:exim_month}-%{MONTHDAY:exim_day} %{TIME:exim_time}
+EXIM_PID \[%{POSINT}\]
+EXIM_QT ((\d+y)?(\d+w)?(\d+d)?(\d+h)?(\d+m)?(\d+s)?)
+EXIM_EXCLUDE_TERMS (Message is frozen|(Start|End) queue run| Warning: | retry time not reached | no (IP address|host name) found for (IP address|host) | unexpected disconnection while reading SMTP command | no immediate delivery: |another process is handling this message)
+EXIM_REMOTE_HOST (H=(%{NOTSPACE:remote_hostname} )?(\(%{NOTSPACE:remote_heloname}\) )?\[%{IP:remote_host}\])
+EXIM_INTERFACE (I=\[%{IP:exim_interface}\](:%{NUMBER:exim_interface_port}))
+EXIM_PROTOCOL (P=%{NOTSPACE:protocol})
+EXIM_MSG_SIZE (S=%{NUMBER:exim_msg_size})
+EXIM_HEADER_ID (id=%{NOTSPACE:exim_header_id})
+EXIM_SUBJECT (T=%{QS:exim_subject})
+
diff --git a/modules/ingest-grok/src/main/resources/patterns/firewalls b/modules/ingest-grok/src/main/resources/patterns/firewalls
new file mode 100644
index 0000000..03c3e5a
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/firewalls
@@ -0,0 +1,86 @@
+# NetScreen firewall logs
+NETSCREENSESSIONLOG %{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}
+
+#== Cisco ASA ==
+CISCO_TAGGED_SYSLOG ^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})? ?: %%{CISCOTAG:ciscotag}:
+CISCOTIMESTAMP %{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}
+CISCOTAG [A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)
+# Common Particles
+CISCO_ACTION Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted
+CISCO_REASON Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\s*)*
+CISCO_DIRECTION Inbound|inbound|Outbound|outbound
+CISCO_INTERVAL first hit|%{INT}-second interval
+CISCO_XLATE_TYPE static|dynamic
+# ASA-1-104001
+CISCOFW104001 \((?:Primary|Secondary)\) Switching to ACTIVE - %{GREEDYDATA:switch_reason}
+# ASA-1-104002
+CISCOFW104002 \((?:Primary|Secondary)\) Switching to STANDBY - %{GREEDYDATA:switch_reason}
+# ASA-1-104003
+CISCOFW104003 \((?:Primary|Secondary)\) Switching to FAILED\.
+# ASA-1-104004
+CISCOFW104004 \((?:Primary|Secondary)\) Switching to OK\.
+# ASA-1-105003
+CISCOFW105003 \((?:Primary|Secondary)\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} waiting
+# ASA-1-105004
+CISCOFW105004 \((?:Primary|Secondary)\) Monitoring on [Ii]nterface %{GREEDYDATA:interface_name} normal
+# ASA-1-105005
+CISCOFW105005 \((?:Primary|Secondary)\) Lost Failover communications with mate on [Ii]nterface %{GREEDYDATA:interface_name}
+# ASA-1-105008
+CISCOFW105008 \((?:Primary|Secondary)\) Testing [Ii]nterface %{GREEDYDATA:interface_name}
+# ASA-1-105009
+CISCOFW105009 \((?:Primary|Secondary)\) Testing on [Ii]nterface %{GREEDYDATA:interface_name} (?:Passed|Failed)
+# ASA-2-106001
+CISCOFW106001 %{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}
+# ASA-2-106006, ASA-2-106007, ASA-2-106010
+CISCOFW106006_106007_106010 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\(%{DATA:src_fwuser}\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\(%{DATA:dst_fwuser}\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})
+# ASA-3-106014
+CISCOFW106014 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\(%{DATA:dst_fwuser}\))? \(type %{INT:icmp_type}, code %{INT:icmp_code}\)
+# ASA-6-106015
+CISCOFW106015 %{CISCO_ACTION:action} %{WORD:protocol} \(%{DATA:policy_id}\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags}  on interface %{GREEDYDATA:interface}
+# ASA-1-106021
+CISCOFW106021 %{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}
+# ASA-4-106023
+CISCOFW106023 %{CISCO_ACTION:action}( protocol)? %{WORD:protocol} src %{DATA:src_interface}:%{DATA:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{DATA:dst_ip}(/%{INT:dst_port})?(\(%{DATA:dst_fwuser}\))?( \(type %{INT:icmp_type}, code %{INT:icmp_code}\))? by access-group "?%{DATA:policy_id}"? \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
+# ASA-4-106100, ASA-4-106102, ASA-4-106103
+CISCOFW106100_2_3 access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} for user '%{DATA:src_fwuser}' %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\) -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\) hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
+# ASA-5-106100
+CISCOFW106100 access-list %{NOTSPACE:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\)(\(%{DATA:src_fwuser}\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\)(\(%{DATA:src_fwuser}\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
+# ASA-6-110002
+CISCOFW110002 %{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
+# ASA-6-302010
+CISCOFW302010 %{INT:connection_count} in use, %{INT:connection_count_max} most used
+# ASA-6-302013, ASA-6-302014, ASA-6-302015, ASA-6-302016
+CISCOFW302013_302014_302015_302016 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\))?(\(%{DATA:src_fwuser}\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\))?(\(%{DATA:dst_fwuser}\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \(%{DATA:user}\))?
+# ASA-6-302020, ASA-6-302021
+CISCOFW302020_302021 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\(%{DATA:fwuser}\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \(%{DATA:user}\))?
+# ASA-6-305011
+CISCOFW305011 %{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}
+# ASA-3-313001, ASA-3-313004, ASA-3-313008
+CISCOFW313001_313004_313008 %{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?
+# ASA-4-313005
+CISCOFW313005 %{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\(%{DATA:err_src_fwuser}\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\(%{DATA:err_dst_fwuser}\))? \(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\) on %{DATA:interface} interface\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\(%{DATA:orig_src_fwuser}\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\(%{DATA:orig_dst_fwuser}\))?
+# ASA-5-321001
+CISCOFW321001 Resource '%{WORD:resource_name}' limit of %{POSINT:resource_limit} reached for system
+# ASA-4-402117
+CISCOFW402117 %{WORD:protocol}: Received a non-IPSec packet \(protocol= %{WORD:orig_protocol}\) from %{IP:src_ip} to %{IP:dst_ip}
+# ASA-4-402119
+CISCOFW402119 %{WORD:protocol}: Received an %{WORD:orig_protocol} packet \(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\) from %{IP:src_ip} \(user= %{DATA:user}\) to %{IP:dst_ip} that failed anti-replay checking
+# ASA-4-419001
+CISCOFW419001 %{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}
+# ASA-4-419002
+CISCOFW419002 %{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number
+# ASA-4-500004
+CISCOFW500004 %{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
+# ASA-6-602303, ASA-6-602304
+CISCOFW602303_602304 %{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \(SPI= %{DATA:spi}\) between %{IP:src_ip} and %{IP:dst_ip} \(user= %{DATA:user}\) has been %{CISCO_ACTION:action}
+# ASA-7-710001, ASA-7-710002, ASA-7-710003, ASA-7-710005, ASA-7-710006
+CISCOFW710001_710002_710003_710005_710006 %{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}
+# ASA-6-713172
+CISCOFW713172 Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\s+Remote end\s*%{DATA:is_remote_natted}\s*behind a NAT device\s+This\s+end\s*%{DATA:is_local_natted}\s*behind a NAT device
+# ASA-4-733100
+CISCOFW733100 \[\s*%{DATA:drop_type}\s*\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}
+#== End Cisco ASA ==
+
+# Shorewall firewall logs
+SHOREWALL (%{SYSLOGTIMESTAMP:timestamp}) (%{WORD:nf_host}) kernel:.*Shorewall:(%{WORD:nf_action1})?:(%{WORD:nf_action2})?.*IN=(%{USERNAME:nf_in_interface})?.*(OUT= *MAC=(%{COMMONMAC:nf_dst_mac}):(%{COMMONMAC:nf_src_mac})?|OUT=%{USERNAME:nf_out_interface}).*SRC=(%{IPV4:nf_src_ip}).*DST=(%{IPV4:nf_dst_ip}).*LEN=(%{WORD:nf_len}).?*TOS=(%{WORD:nf_tos}).?*PREC=(%{WORD:nf_prec}).?*TTL=(%{INT:nf_ttl}).?*ID=(%{INT:nf_id}).?*PROTO=(%{WORD:nf_protocol}).?*SPT=(%{INT:nf_src_port}?.*DPT=%{INT:nf_dst_port}?.*)
+#== End Shorewall
diff --git a/modules/ingest-grok/src/main/resources/patterns/grok-patterns b/modules/ingest-grok/src/main/resources/patterns/grok-patterns
new file mode 100644
index 0000000..cb4c3ff
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/grok-patterns
@@ -0,0 +1,102 @@
+USERNAME [a-zA-Z0-9._-]+
+USER %{USERNAME}
+EMAILLOCALPART [a-zA-Z][a-zA-Z0-9_.+-=:]+
+EMAILADDRESS %{EMAILLOCALPART}@%{HOSTNAME}
+HTTPDUSER %{EMAILADDRESS}|%{USER}
+INT (?:[+-]?(?:[0-9]+))
+BASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+)))
+NUMBER (?:%{BASE10NUM})
+BASE16NUM (?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))
+BASE16FLOAT \b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\.[0-9A-Fa-f]*)?)|(?:\.[0-9A-Fa-f]+)))\b
+
+POSINT \b(?:[1-9][0-9]*)\b
+NONNEGINT \b(?:[0-9]+)\b
+WORD \b\w+\b
+NOTSPACE \S+
+SPACE \s*
+DATA .*?
+GREEDYDATA .*
+QUOTEDSTRING (?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``))
+UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}
+
+# Networking
+MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})
+CISCOMAC (?:(?:[A-Fa-f0-9]{4}\.){2}[A-Fa-f0-9]{4})
+WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})
+COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})
+IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?
+IPV4 (?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])
+IP (?:%{IPV6}|%{IPV4})
+HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)
+IPORHOST (?:%{IP}|%{HOSTNAME})
+HOSTPORT %{IPORHOST}:%{POSINT}
+
+# paths
+PATH (?:%{UNIXPATH}|%{WINPATH})
+UNIXPATH (/([\w_%!$@:.,~-]+|\\.)*)+
+TTY (?:/dev/(pts|tty([pq])?)(\w+)?/?(?:[0-9]+))
+WINPATH (?>[A-Za-z]+:|\\)(?:\\[^\\?*]*)+
+URIPROTO [A-Za-z]+(\+[A-Za-z+]+)?
+URIHOST %{IPORHOST}(?::%{POSINT:port})?
+# uripath comes loosely from RFC1738, but mostly from what Firefox
+# doesn't turn into %XX
+URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%_\-]*)+
+#URIPARAM \?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)?
+URIPARAM \?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\-\[\]<>]*
+URIPATHPARAM %{URIPATH}(?:%{URIPARAM})?
+URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?
+
+# Months: January, Feb, 3, 03, 12, December
+MONTH \b(?:Jan(?:uary|uar)?|Feb(?:ruary|ruar)?|M(?:a|)?r(?:ch|z)?|Apr(?:il)?|Ma(?:y|i)?|Jun(?:e|i)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|O(?:c|k)?t(?:ober)?|Nov(?:ember)?|De(?:c|z)(?:ember)?)\b
+MONTHNUM (?:0?[1-9]|1[0-2])
+MONTHNUM2 (?:0[1-9]|1[0-2])
+MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])
+
+# Days: Monday, Tue, Thu, etc...
+DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)
+
+# Years?
+YEAR (?>\d\d){1,2}
+HOUR (?:2[0123]|[01]?[0-9])
+MINUTE (?:[0-5][0-9])
+# '60' is a leap second in most time standards and thus is valid.
+SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
+TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
+# datestamp is YYYY/MM/DD-HH:MM:SS.UUUU (or something like it)
+DATE_US %{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}
+DATE_EU %{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}
+ISO8601_TIMEZONE (?:Z|[+-]%{HOUR}(?::?%{MINUTE}))
+ISO8601_SECOND (?:%{SECOND}|60)
+ISO8601_HOUR (?:2[0123]|[01][0-9])
+TIMESTAMP_ISO8601 %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{ISO8601_HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?
+DATE %{DATE_US}|%{DATE_EU}
+DATESTAMP %{DATE}[- ]%{TIME}
+TZ (?:[PMCE][SD]T|UTC)
+DATESTAMP_RFC822 %{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}
+DATESTAMP_RFC2822 %{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}
+DATESTAMP_OTHER %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}
+DATESTAMP_EVENTLOG %{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}
+HTTPDERROR_DATE %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}
+
+# Syslog Dates: Month Day HH:MM:SS
+SYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME}
+PROG [\x21-\x5a\x5c\x5e-\x7e]+
+SYSLOGPROG %{PROG:program}(?:\[%{POSINT:pid}\])?
+SYSLOGHOST %{IPORHOST}
+SYSLOGFACILITY <%{NONNEGINT:facility}.%{NONNEGINT:priority}>
+HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}
+
+# Shortcuts
+QS %{QUOTEDSTRING}
+
+# Log formats
+SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
+COMMONAPACHELOG %{IPORHOST:clientip} %{HTTPDUSER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
+COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
+HTTPD20_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{LOGLEVEL:loglevel}\] (?:\[client %{IPORHOST:clientip}\] ){0,1}%{GREEDYDATA:errormsg}
+HTTPD24_ERRORLOG \[%{HTTPDERROR_DATE:timestamp}\] \[%{WORD:module}:%{LOGLEVEL:loglevel}\] \[pid %{POSINT:pid}:tid %{NUMBER:tid}\]( \(%{POSINT:proxy_errorcode}\)%{DATA:proxy_errormessage}:)?( \[client %{IPORHOST:client}:%{POSINT:clientport}\])? %{DATA:errorcode}: %{GREEDYDATA:message}
+HTTPD_ERRORLOG %{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG}
+
+
+# Log Levels
+LOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)
diff --git a/modules/ingest-grok/src/main/resources/patterns/haproxy b/modules/ingest-grok/src/main/resources/patterns/haproxy
new file mode 100644
index 0000000..ddabd19
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/haproxy
@@ -0,0 +1,39 @@
+## These patterns were tested w/ haproxy-1.4.15
+
+## Documentation of the haproxy log formats can be found at the following links:
+## http://code.google.com/p/haproxy-docs/wiki/HTTPLogFormat
+## http://code.google.com/p/haproxy-docs/wiki/TCPLogFormat
+
+HAPROXYTIME (?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])
+HAPROXYDATE %{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}
+
+# Override these default patterns to parse out what is captured in your haproxy.cfg
+HAPROXYCAPTUREDREQUESTHEADERS %{DATA:captured_request_headers}
+HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:captured_response_headers}
+
+# Example:
+#  These haproxy config lines will add data to the logs that are captured
+#  by the patterns below. Place them in your custom patterns directory to
+#  override the defaults.
+#
+#  capture request header Host len 40
+#  capture request header X-Forwarded-For len 50
+#  capture request header Accept-Language len 50
+#  capture request header Referer len 200
+#  capture request header User-Agent len 200
+#
+#  capture response header Content-Type len 30
+#  capture response header Content-Encoding len 10
+#  capture response header Cache-Control len 200
+#  capture response header Last-Modified len 200
+#
+# HAPROXYCAPTUREDREQUESTHEADERS %{DATA:request_header_host}\|%{DATA:request_header_x_forwarded_for}\|%{DATA:request_header_accept_language}\|%{DATA:request_header_referer}\|%{DATA:request_header_user_agent}
+# HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:response_header_content_type}\|%{DATA:response_header_content_encoding}\|%{DATA:response_header_cache_control}\|%{DATA:response_header_last_modified}
+
+# parse a haproxy 'httplog' line
+HAPROXYHTTPBASE %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
+
+HAPROXYHTTP (?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{HAPROXYHTTPBASE}
+
+# parse a haproxy 'tcplog' line
+HAPROXYTCP (?:%{SYSLOGTIMESTAMP:syslog_timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}
diff --git a/modules/ingest-grok/src/main/resources/patterns/java b/modules/ingest-grok/src/main/resources/patterns/java
new file mode 100644
index 0000000..e968006
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/java
@@ -0,0 +1,20 @@
+JAVACLASS (?:[a-zA-Z$_][a-zA-Z$_0-9]*\.)*[a-zA-Z$_][a-zA-Z$_0-9]*
+#Space is an allowed character to match special cases like 'Native Method' or 'Unknown Source'
+JAVAFILE (?:[A-Za-z0-9_. -]+)
+#Allow special <init> method
+JAVAMETHOD (?:(<init>)|[a-zA-Z$_][a-zA-Z$_0-9]*)
+#Line number is optional in special cases 'Native method' or 'Unknown source'
+JAVASTACKTRACEPART %{SPACE}at %{JAVACLASS:class}\.%{JAVAMETHOD:method}\(%{JAVAFILE:file}(?::%{NUMBER:line})?\)
+# Java Logs
+JAVATHREAD (?:[A-Z]{2}-Processor[\d]+)
+JAVACLASS (?:[a-zA-Z0-9-]+\.)+[A-Za-z0-9$]+
+JAVAFILE (?:[A-Za-z0-9_.-]+)
+JAVASTACKTRACEPART at %{JAVACLASS:class}\.%{WORD:method}\(%{JAVAFILE:file}:%{NUMBER:line}\)
+JAVALOGMESSAGE (.*)
+# MMM dd, yyyy HH:mm:ss eg: Jan 9, 2014 7:13:13 AM
+CATALINA_DATESTAMP %{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM)
+# yyyy-MM-dd HH:mm:ss,SSS ZZZ eg: 2014-01-09 17:32:25,527 -0800
+TOMCAT_DATESTAMP 20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE}
+CATALINALOG %{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage}
+# 2014-01-09 20:03:28,269 -0800 | ERROR | com.example.service.ExampleService - something compeletely unexpected happened...
+TOMCATLOG %{TOMCAT_DATESTAMP:timestamp} \| %{LOGLEVEL:level} \| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage}
diff --git a/modules/ingest-grok/src/main/resources/patterns/junos b/modules/ingest-grok/src/main/resources/patterns/junos
new file mode 100644
index 0000000..4eea59d
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/junos
@@ -0,0 +1,9 @@
+# JUNOS 11.4 RT_FLOW patterns
+RT_FLOW_EVENT (RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)
+
+RT_FLOW1 %{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \d+\(%{DATA:sent}\) \d+\(%{DATA:received}\) %{INT:elapsed-time} .*
+
+RT_FLOW2 %{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{INT:nat-src-port}->%{IP:nat-dst-ip}/%{INT:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*
+
+RT_FLOW3 %{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{INT:src-port}->%{IP:dst-ip}/%{INT:dst-port} %{DATA:service} %{INT:protocol-id}\(\d\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*
+
diff --git a/modules/ingest-grok/src/main/resources/patterns/linux-syslog b/modules/ingest-grok/src/main/resources/patterns/linux-syslog
new file mode 100644
index 0000000..dcffb41
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/linux-syslog
@@ -0,0 +1,16 @@
+SYSLOG5424PRINTASCII [!-~]+
+
+SYSLOGBASE2 (?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource}+(?: %{SYSLOGPROG}:|)
+SYSLOGPAMSESSION %{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\(%{DATA:pam_caller}\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?
+
+CRON_ACTION [A-Z ]+
+CRONLOG %{SYSLOGBASE} \(%{USER:user}\) %{CRON_ACTION:action} \(%{DATA:message}\)
+
+SYSLOGLINE %{SYSLOGBASE2} %{GREEDYDATA:message}
+
+# IETF 5424 syslog(8) format (see http://www.rfc-editor.org/info/rfc5424)
+SYSLOG5424PRI <%{NONNEGINT:syslog5424_pri}>
+SYSLOG5424SD \[%{DATA}\]+
+SYSLOG5424BASE %{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{HOSTNAME:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)
+
+SYSLOG5424LINE %{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}
diff --git a/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns b/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns
new file mode 100644
index 0000000..bb2f7f9
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/mcollective-patterns
@@ -0,0 +1,4 @@
+# Remember, these can be multi-line events.
+MCOLLECTIVE ., \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\]%{SPACE}%{LOGLEVEL:event_level}
+
+MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
diff --git a/modules/ingest-grok/src/main/resources/patterns/mongodb b/modules/ingest-grok/src/main/resources/patterns/mongodb
new file mode 100644
index 0000000..78a4300
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/mongodb
@@ -0,0 +1,7 @@
+MONGO_LOG %{SYSLOGTIMESTAMP:timestamp} \[%{WORD:component}\] %{GREEDYDATA:message}
+MONGO_QUERY \{ (?<={ ).*(?= } ntoreturn:) \}
+MONGO_SLOWQUERY %{WORD} %{MONGO_WORDDASH:database}\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms
+MONGO_WORDDASH \b[\w-]+\b
+MONGO3_SEVERITY \w
+MONGO3_COMPONENT %{WORD}|-
+MONGO3_LOG %{TIMESTAMP_ISO8601:timestamp} %{MONGO3_SEVERITY:severity} %{MONGO3_COMPONENT:component}%{SPACE}(?:\[%{DATA:context}\])? %{GREEDYDATA:message}
diff --git a/modules/ingest-grok/src/main/resources/patterns/nagios b/modules/ingest-grok/src/main/resources/patterns/nagios
new file mode 100644
index 0000000..f4a98bf
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/nagios
@@ -0,0 +1,124 @@
+##################################################################################
+##################################################################################
+# Chop Nagios log files to smithereens!
+#
+# A set of GROK filters to process logfiles generated by Nagios.
+# While it does not, this set intends to cover all possible Nagios logs.
+#
+# Some more work needs to be done to cover all External Commands:
+# http://old.nagios.org/developerinfo/externalcommands/commandlist.php
+#
+# If you need some support on these rules please contact:
+# Jelle Smet http://smetj.net
+#
+#################################################################################
+#################################################################################
+
+NAGIOSTIME \[%{NUMBER:nagios_epoch}\]
+
+###############################################
+######## Begin nagios log types
+###############################################
+NAGIOS_TYPE_CURRENT_SERVICE_STATE CURRENT SERVICE STATE
+NAGIOS_TYPE_CURRENT_HOST_STATE CURRENT HOST STATE
+
+NAGIOS_TYPE_SERVICE_NOTIFICATION SERVICE NOTIFICATION
+NAGIOS_TYPE_HOST_NOTIFICATION HOST NOTIFICATION
+
+NAGIOS_TYPE_SERVICE_ALERT SERVICE ALERT
+NAGIOS_TYPE_HOST_ALERT HOST ALERT
+
+NAGIOS_TYPE_SERVICE_FLAPPING_ALERT SERVICE FLAPPING ALERT
+NAGIOS_TYPE_HOST_FLAPPING_ALERT HOST FLAPPING ALERT
+
+NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT SERVICE DOWNTIME ALERT
+NAGIOS_TYPE_HOST_DOWNTIME_ALERT HOST DOWNTIME ALERT
+
+NAGIOS_TYPE_PASSIVE_SERVICE_CHECK PASSIVE SERVICE CHECK
+NAGIOS_TYPE_PASSIVE_HOST_CHECK PASSIVE HOST CHECK
+
+NAGIOS_TYPE_SERVICE_EVENT_HANDLER SERVICE EVENT HANDLER
+NAGIOS_TYPE_HOST_EVENT_HANDLER HOST EVENT HANDLER
+
+NAGIOS_TYPE_EXTERNAL_COMMAND EXTERNAL COMMAND
+NAGIOS_TYPE_TIMEPERIOD_TRANSITION TIMEPERIOD TRANSITION
+###############################################
+######## End nagios log types
+###############################################
+
+###############################################
+######## Begin external check types
+###############################################
+NAGIOS_EC_DISABLE_SVC_CHECK DISABLE_SVC_CHECK
+NAGIOS_EC_ENABLE_SVC_CHECK ENABLE_SVC_CHECK
+NAGIOS_EC_DISABLE_HOST_CHECK DISABLE_HOST_CHECK
+NAGIOS_EC_ENABLE_HOST_CHECK ENABLE_HOST_CHECK
+NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT PROCESS_SERVICE_CHECK_RESULT
+NAGIOS_EC_PROCESS_HOST_CHECK_RESULT PROCESS_HOST_CHECK_RESULT
+NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME SCHEDULE_SERVICE_DOWNTIME
+NAGIOS_EC_SCHEDULE_HOST_DOWNTIME SCHEDULE_HOST_DOWNTIME
+NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS DISABLE_HOST_SVC_NOTIFICATIONS
+NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS ENABLE_HOST_SVC_NOTIFICATIONS
+NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS DISABLE_HOST_NOTIFICATIONS
+NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS ENABLE_HOST_NOTIFICATIONS
+NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS DISABLE_SVC_NOTIFICATIONS
+NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS ENABLE_SVC_NOTIFICATIONS
+###############################################
+######## End external check types
+###############################################
+NAGIOS_WARNING Warning:%{SPACE}%{GREEDYDATA:nagios_message}
+
+NAGIOS_CURRENT_SERVICE_STATE %{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
+NAGIOS_CURRENT_HOST_STATE %{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
+
+NAGIOS_SERVICE_NOTIFICATION %{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
+NAGIOS_HOST_NOTIFICATION %{NAGIOS_TYPE_HOST_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
+
+NAGIOS_SERVICE_ALERT %{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
+NAGIOS_HOST_ALERT %{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
+
+NAGIOS_SERVICE_FLAPPING_ALERT %{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
+NAGIOS_HOST_FLAPPING_ALERT %{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
+
+NAGIOS_SERVICE_DOWNTIME_ALERT %{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
+NAGIOS_HOST_DOWNTIME_ALERT %{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
+
+NAGIOS_PASSIVE_SERVICE_CHECK %{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
+NAGIOS_PASSIVE_HOST_CHECK %{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
+
+NAGIOS_SERVICE_EVENT_HANDLER %{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
+NAGIOS_HOST_EVENT_HANDLER %{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
+
+NAGIOS_TIMEPERIOD_TRANSITION %{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2}
+
+####################
+#### External checks
+####################
+
+#Disable host & service check
+NAGIOS_EC_LINE_DISABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
+NAGIOS_EC_LINE_DISABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
+
+#Enable host & service check
+NAGIOS_EC_LINE_ENABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
+NAGIOS_EC_LINE_ENABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
+
+#Process host & service check
+NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
+NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
+
+#Disable host & service notifications
+NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
+NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
+NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}
+
+#Enable host & service notifications
+NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_SVC_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
+NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_NOTIFICATIONS:nagios_command};%{GREEDYDATA:nagios_hostname}
+NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_NOTIFICATIONS:nagios_command};%{DATA:nagios_hostname};%{GREEDYDATA:nagios_service}
+
+#Schedule host & service downtime
+NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}
+
+#End matching line
+NAGIOSLOGLINE %{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME}|%{NAGIOS_EC_LINE_DISABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_HOST_NOTIFICATIONS}|%{NAGIOS_EC_LINE_DISABLE_SVC_NOTIFICATIONS}|%{NAGIOS_EC_LINE_ENABLE_SVC_NOTIFICATIONS})
diff --git a/modules/ingest-grok/src/main/resources/patterns/postgresql b/modules/ingest-grok/src/main/resources/patterns/postgresql
new file mode 100644
index 0000000..c5b3e90
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/postgresql
@@ -0,0 +1,3 @@
+# Default postgresql pg_log format pattern
+POSTGRESQL %{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}
+
diff --git a/modules/ingest-grok/src/main/resources/patterns/rails b/modules/ingest-grok/src/main/resources/patterns/rails
new file mode 100644
index 0000000..68a50c7
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/rails
@@ -0,0 +1,13 @@
+RUUID \h{32}
+# rails controller with action
+RCONTROLLER (?<controller>[^#]+)#(?<action>\w+)
+
+# this will often be the only line:
+RAILS3HEAD (?m)Started %{WORD:verb} "%{URIPATHPARAM:request}" for %{IPORHOST:clientip} at (?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} %{ISO8601_TIMEZONE})
+# for some a strange reason, params are stripped of {} - not sure that's a good idea.
+RPROCESSING \W*Processing by %{RCONTROLLER} as (?<format>\S+)(?:\W*Parameters: {%{DATA:params}}\W*)?
+RAILS3FOOT Completed %{NUMBER:response}%{DATA} in %{NUMBER:totalms}ms %{RAILS3PROFILE}%{GREEDYDATA}
+RAILS3PROFILE (?:\(Views: %{NUMBER:viewms}ms \| ActiveRecord: %{NUMBER:activerecordms}ms|\(ActiveRecord: %{NUMBER:activerecordms}ms)?
+
+# putting it all together
+RAILS3 %{RAILS3HEAD}(?:%{RPROCESSING})?(?<context>(?:%{DATA}\n)*)(?:%{RAILS3FOOT})?
diff --git a/modules/ingest-grok/src/main/resources/patterns/redis b/modules/ingest-grok/src/main/resources/patterns/redis
new file mode 100644
index 0000000..8655c4f
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/redis
@@ -0,0 +1,3 @@
+REDISTIMESTAMP %{MONTHDAY} %{MONTH} %{TIME}
+REDISLOG \[%{POSINT:pid}\] %{REDISTIMESTAMP:timestamp} \* 
+
diff --git a/modules/ingest-grok/src/main/resources/patterns/ruby b/modules/ingest-grok/src/main/resources/patterns/ruby
new file mode 100644
index 0000000..b1729cd
--- /dev/null
+++ b/modules/ingest-grok/src/main/resources/patterns/ruby
@@ -0,0 +1,2 @@
+RUBY_LOGLEVEL (?:DEBUG|FATAL|ERROR|WARN|INFO)
+RUBY_LOGGER [DFEWI], \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
new file mode 100644
index 0000000..f6bed13
--- /dev/null
+++ b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorFactoryTests.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.grok;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.notNullValue;
+
+public class GrokProcessorFactoryTests extends ESTestCase {
+
+    public void testBuild() throws Exception {
+        GrokProcessor.Factory factory = new GrokProcessor.Factory(Collections.emptyMap());
+
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "_field");
+        config.put("pattern", "(?<foo>\\w+)");
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+        GrokProcessor processor = factory.create(config);
+        assertThat(processor.getTag(), equalTo(processorTag));
+        assertThat(processor.getMatchField(), equalTo("_field"));
+        assertThat(processor.getGrok(), notNullValue());
+    }
+
+    public void testCreateWithCustomPatterns() throws Exception {
+        GrokProcessor.Factory factory = new GrokProcessor.Factory(Collections.emptyMap());
+
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "_field");
+        config.put("pattern", "%{MY_PATTERN:name}!");
+        config.put("pattern_definitions", Collections.singletonMap("MY_PATTERN", "foo"));
+        GrokProcessor processor = factory.create(config);
+        assertThat(processor.getMatchField(), equalTo("_field"));
+        assertThat(processor.getGrok(), notNullValue());
+        assertThat(processor.getGrok().match("foo!"), equalTo(true));
+    }
+}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java
new file mode 100644
index 0000000..840cf95
--- /dev/null
+++ b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokProcessorTests.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.grok;
+
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.grok.Grok;
+import org.elasticsearch.ingest.grok.GrokProcessor;
+import org.elasticsearch.test.ESTestCase;
+
+import java.util.Collections;
+import java.util.HashMap;
+
+import static org.hamcrest.Matchers.equalTo;
+
+
+public class GrokProcessorTests extends ESTestCase {
+
+    public void testMatch() throws Exception {
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        doc.setFieldValue(fieldName, "1");
+        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
+        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
+        processor.execute(doc);
+        assertThat(doc.getFieldValue("one", String.class), equalTo("1"));
+    }
+
+    public void testNoMatch() {
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        doc.setFieldValue(fieldName, "23");
+        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
+        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
+        try {
+            processor.execute(doc);
+            fail();
+        } catch (Exception e) {
+            assertThat(e.getMessage(), equalTo("Grok expression does not match field value: [23]"));
+        }
+    }
+
+    public void testMatchWithoutCaptures() throws Exception {
+        String fieldName = "value";
+        IngestDocument originalDoc = new IngestDocument(new HashMap<>(), new HashMap<>());
+        originalDoc.setFieldValue(fieldName, fieldName);
+        IngestDocument doc = new IngestDocument(originalDoc);
+        Grok grok = new Grok(Collections.emptyMap(), fieldName);
+        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
+        processor.execute(doc);
+        assertThat(doc, equalTo(originalDoc));
+    }
+
+    public void testNotStringField() {
+        String fieldName = RandomDocumentPicks.randomFieldName(random());
+        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        doc.setFieldValue(fieldName, 1);
+        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
+        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
+        try {
+            processor.execute(doc);
+            fail();
+        } catch (Exception e) {
+            assertThat(e.getMessage(), equalTo("field [" + fieldName + "] of type [java.lang.Integer] cannot be cast to [java.lang.String]"));
+        }
+    }
+
+    public void testMissingField() {
+        String fieldName = "foo.bar";
+        IngestDocument doc = RandomDocumentPicks.randomIngestDocument(random(), new HashMap<>());
+        Grok grok = new Grok(Collections.singletonMap("ONE", "1"), "%{ONE:one}");
+        GrokProcessor processor = new GrokProcessor(randomAsciiOfLength(10), grok, fieldName);
+        try {
+            processor.execute(doc);
+            fail();
+        } catch (Exception e) {
+            assertThat(e.getMessage(), equalTo("field [foo] not present as part of path [foo.bar]"));
+        }
+    }
+}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java
new file mode 100644
index 0000000..21ca17a
--- /dev/null
+++ b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/GrokTests.java
@@ -0,0 +1,285 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.grok;
+
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.nullValue;
+
+
+public class GrokTests extends ESTestCase {
+    private Map<String, String> basePatterns;
+
+    @Before
+    public void setup() throws IOException {
+        basePatterns = IngestGrokPlugin.loadBuiltinPatterns();
+    }
+
+    public void testMatchWithoutCaptures() {
+        String line = "value";
+        Grok grok = new Grok(basePatterns, "value");
+        Map<String, Object> matches = grok.captures(line);
+        assertEquals(0, matches.size());
+    }
+
+    public void testSimpleSyslogLine() {
+        String line = "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]";
+        Grok grok = new Grok(basePatterns, "%{SYSLOGLINE}");
+        Map<String, Object> matches = grok.captures(line);
+        assertEquals("evita", matches.get("logsource"));
+        assertEquals("Mar 16 00:01:25", matches.get("timestamp"));
+        assertEquals("connect from camomile.cloud9.net[168.100.1.3]", matches.get("message"));
+        assertEquals("postfix/smtpd", matches.get("program"));
+        assertEquals("1713", matches.get("pid"));
+    }
+
+    public void testSyslog5424Line() {
+        String line = "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 - [id1 foo=\\\"bar\\\"][id2 baz=\\\"something\\\"] Hello, syslog.";
+        Grok grok = new Grok(basePatterns, "%{SYSLOG5424LINE}");
+        Map<String, Object> matches = grok.captures(line);
+        assertEquals("191", matches.get("syslog5424_pri"));
+        assertEquals("1", matches.get("syslog5424_ver"));
+        assertEquals("2009-06-30T18:30:00+02:00", matches.get("syslog5424_ts"));
+        assertEquals("paxton.local", matches.get("syslog5424_host"));
+        assertEquals("grokdebug", matches.get("syslog5424_app"));
+        assertEquals("4123", matches.get("syslog5424_proc"));
+        assertEquals(null, matches.get("syslog5424_msgid"));
+        assertEquals("[id1 foo=\\\"bar\\\"][id2 baz=\\\"something\\\"]", matches.get("syslog5424_sd"));
+        assertEquals("Hello, syslog.", matches.get("syslog5424_msg"));
+    }
+
+    public void testDatePattern() {
+        String line = "fancy 12-12-12 12:12:12";
+        Grok grok = new Grok(basePatterns, "(?<timestamp>%{DATE_EU} %{TIME})");
+        Map<String, Object> matches = grok.captures(line);
+        assertEquals("12-12-12 12:12:12", matches.get("timestamp"));
+    }
+
+    public void testNilCoercedValues() {
+        Grok grok = new Grok(basePatterns, "test (N/A|%{BASE10NUM:duration:float}ms)");
+        Map<String, Object> matches = grok.captures("test 28.4ms");
+        assertEquals(28.4f, matches.get("duration"));
+        matches = grok.captures("test N/A");
+        assertEquals(null, matches.get("duration"));
+    }
+
+    public void testNilWithNoCoercion() {
+        Grok grok = new Grok(basePatterns, "test (N/A|%{BASE10NUM:duration}ms)");
+        Map<String, Object> matches = grok.captures("test 28.4ms");
+        assertEquals("28.4", matches.get("duration"));
+        matches = grok.captures("test N/A");
+        assertEquals(null, matches.get("duration"));
+    }
+
+    public void testUnicodeSyslog() {
+        Grok grok = new Grok(basePatterns, "<%{POSINT:syslog_pri}>%{SPACE}%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(:?)(?:\\[%{GREEDYDATA:syslog_pid}\\])?(:?) %{GREEDYDATA:syslog_message}");
+        Map<String, Object> matches = grok.captures("<22>Jan  4 07:50:46 mailmaster postfix/policy-spf[9454]: : SPF permerror (Junk encountered in record 'v=spf1 mx a:mail.domain.no ip4:192.168.0.4 all'): Envelope-from: email@domain.no");
+        assertThat(matches.get("syslog_pri"), equalTo("22"));
+        assertThat(matches.get("syslog_program"), equalTo("postfix/policy-spf"));
+        assertThat(matches.get("tags"), nullValue());
+    }
+
+    public void testNamedFieldsWithWholeTextMatch() {
+        Grok grok = new Grok(basePatterns, "%{DATE_EU:stimestamp}");
+        Map<String, Object> matches = grok.captures("11/01/01");
+        assertThat(matches.get("stimestamp"), equalTo("11/01/01"));
+    }
+
+    public void testWithOniguramaNamedCaptures() {
+        Grok grok = new Grok(basePatterns, "(?<foo>\\w+)");
+        Map<String, Object> matches = grok.captures("hello world");
+        assertThat(matches.get("foo"), equalTo("hello"));
+    }
+
+    public void testISO8601() {
+        Grok grok = new Grok(basePatterns, "^%{TIMESTAMP_ISO8601}$");
+        List<String> timeMessages = Arrays.asList(
+                "2001-01-01T00:00:00",
+                "1974-03-02T04:09:09",
+                "2010-05-03T08:18:18+00:00",
+                "2004-07-04T12:27:27-00:00",
+                "2001-09-05T16:36:36+0000",
+                "2001-11-06T20:45:45-0000",
+                "2001-12-07T23:54:54Z",
+                "2001-01-01T00:00:00.123456",
+                "1974-03-02T04:09:09.123456",
+                "2010-05-03T08:18:18.123456+00:00",
+                "2004-07-04T12:27:27.123456-00:00",
+                "2001-09-05T16:36:36.123456+0000",
+                "2001-11-06T20:45:45.123456-0000",
+                "2001-12-07T23:54:54.123456Z",
+                "2001-12-07T23:54:60.123456Z" // '60' second is a leap second.
+        );
+        for (String msg : timeMessages) {
+            assertThat(grok.match(msg), is(true));
+        }
+    }
+
+    public void testNotISO8601() {
+        Grok grok = new Grok(basePatterns, "^%{TIMESTAMP_ISO8601}$");
+        List<String> timeMessages = Arrays.asList(
+                "2001-13-01T00:00:00", // invalid month
+                "2001-00-01T00:00:00", // invalid month
+                "2001-01-00T00:00:00", // invalid day
+                "2001-01-32T00:00:00", // invalid day
+                "2001-01-aT00:00:00", // invalid day
+                "2001-01-1aT00:00:00", // invalid day
+                "2001-01-01Ta0:00:00", // invalid hour
+                "2001-01-01T0:00:00", // invalid hour
+                "2001-01-01T25:00:00", // invalid hour
+                "2001-01-01T01:60:00", // invalid minute
+                "2001-01-01T00:aa:00", // invalid minute
+                "2001-01-01T00:00:aa", // invalid second
+                "2001-01-01T00:00:-1", // invalid second
+                "2001-01-01T00:00:61", // invalid second
+                "2001-01-01T00:00:00A", // invalid timezone
+                "2001-01-01T00:00:00+", // invalid timezone
+                "2001-01-01T00:00:00+25", // invalid timezone
+                "2001-01-01T00:00:00+2500", // invalid timezone
+                "2001-01-01T00:00:00+25:00", // invalid timezone
+                "2001-01-01T00:00:00-25", // invalid timezone
+                "2001-01-01T00:00:00-2500", // invalid timezone
+                "2001-01-01T00:00:00-00:61" // invalid timezone
+        );
+        for (String msg : timeMessages) {
+            assertThat(grok.match(msg), is(false));
+        }
+    }
+
+    public void testNoNamedCaptures() {
+        Map<String, String> bank = new HashMap<>();
+
+        bank.put("NAME", "Tal");
+        bank.put("EXCITED_NAME", "!!!%{NAME:name}!!!");
+        bank.put("TEST", "hello world");
+
+        String text = "wowza !!!Tal!!! - Tal";
+        String pattern = "%{EXCITED_NAME} - %{NAME}";
+        Grok g = new Grok(bank, pattern, false);
+
+        assertEquals("(?<EXCITED_NAME_0>!!!(?<NAME_21>Tal)!!!) - (?<NAME_22>Tal)", g.toRegex(pattern));
+        assertEquals(true, g.match(text));
+
+        Object actual = g.captures(text);
+        Map<String, Object> expected = new HashMap<>();
+        expected.put("EXCITED_NAME_0", "!!!Tal!!!");
+        expected.put("NAME_21", "Tal");
+        expected.put("NAME_22", "Tal");
+        assertEquals(expected, actual);
+    }
+
+    public void testNumericCapturesCoercion() {
+        Map<String, String> bank = new HashMap<>();
+        bank.put("BASE10NUM", "(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))");
+        bank.put("NUMBER", "(?:%{BASE10NUM})");
+
+        String pattern = "%{NUMBER:bytes:float} %{NUMBER:status} %{NUMBER}";
+        Grok g = new Grok(bank, pattern);
+
+        String text = "12009.34 200 9032";
+        Map<String, Object> expected = new HashMap<>();
+        expected.put("bytes", 12009.34f);
+        expected.put("status", "200");
+        Map<String, Object> actual = g.captures(text);
+
+        assertEquals(expected, actual);
+    }
+
+    public void testApacheLog() {
+        String logLine = "31.184.238.164 - - [24/Jul/2014:05:35:37 +0530] \"GET /logs/access.log HTTP/1.0\" 200 69849 \"http://8rursodiol.enjin.com\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.12785 YaBrowser/13.12.1599.12785 Safari/537.36\" \"www.dlwindianrailways.com\"";
+        Grok grok = new Grok(basePatterns, "%{COMBINEDAPACHELOG}");
+        Map<String, Object> matches = grok.captures(logLine);
+
+        assertEquals("31.184.238.164", matches.get("clientip"));
+        assertEquals("-", matches.get("ident"));
+        assertEquals("-", matches.get("auth"));
+        assertEquals("24/Jul/2014:05:35:37 +0530", matches.get("timestamp"));
+        assertEquals("GET", matches.get("verb"));
+        assertEquals("/logs/access.log", matches.get("request"));
+        assertEquals("1.0", matches.get("httpversion"));
+        assertEquals("200", matches.get("response"));
+        assertEquals("69849", matches.get("bytes"));
+        assertEquals("\"http://8rursodiol.enjin.com\"", matches.get("referrer"));
+        assertEquals(null, matches.get("port"));
+        assertEquals("\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.12785 YaBrowser/13.12.1599.12785 Safari/537.36\"", matches.get("agent"));
+    }
+
+    public void testComplete() {
+        Map<String, String> bank = new HashMap<>();
+        bank.put("MONTHDAY", "(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])");
+        bank.put("MONTH", "\\b(?:Jan(?:uary|uar)?|Feb(?:ruary|ruar)?|M(?:a|)?r(?:ch|z)?|Apr(?:il)?|Ma(?:y|i)?|Jun(?:e|i)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|O(?:c|k)?t(?:ober)?|Nov(?:ember)?|De(?:c|z)(?:ember)?)\\b");
+        bank.put("MINUTE", "(?:[0-5][0-9])");
+        bank.put("YEAR", "(?>\\d\\d){1,2}");
+        bank.put("HOUR", "(?:2[0123]|[01]?[0-9])");
+        bank.put("SECOND", "(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)");
+        bank.put("TIME", "(?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])");
+        bank.put("INT", "(?:[+-]?(?:[0-9]+))");
+        bank.put("HTTPDATE", "%{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}");
+        bank.put("WORD", "\\b\\w+\\b");
+        bank.put("BASE10NUM", "(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))");
+        bank.put("NUMBER", "(?:%{BASE10NUM})");
+        bank.put("IPV6", "((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?");
+        bank.put("IPV4", "(?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])");
+        bank.put("IP", "(?:%{IPV6}|%{IPV4})");
+        bank.put("HOSTNAME", "\\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)");
+        bank.put("IPORHOST", "(?:%{IP}|%{HOSTNAME})");
+        bank.put("USER", "[a-zA-Z0-9._-]+");
+        bank.put("DATA", ".*?");
+        bank.put("QS", "(?>(?<!\\\\)(?>\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``))");
+
+        String text = "83.149.9.216 - - [19/Jul/2015:08:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1\" 200 171717 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"";
+        String pattern = "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}";
+
+        Grok grok = new Grok(bank, pattern);
+
+        Map<String, Object> expected = new HashMap<>();
+        expected.put("clientip", "83.149.9.216");
+        expected.put("ident", "-");
+        expected.put("auth", "-");
+        expected.put("timestamp", "19/Jul/2015:08:13:42 +0000");
+        expected.put("verb", "GET");
+        expected.put("request", "/presentations/logstash-monitorama-2013/images/kibana-dashboard3.png");
+        expected.put("httpversion", "1.1");
+        expected.put("response", 200);
+        expected.put("bytes", 171717);
+        expected.put("referrer", "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"");
+        expected.put("agent", "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"");
+
+        Map<String, Object> actual = grok.captures(text);
+
+        assertEquals(expected, actual);
+    }
+
+    public void testNoMatch() {
+        Map<String, String> bank = new HashMap<>();
+        bank.put("MONTHDAY", "(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])");
+        Grok grok = new Grok(bank, "%{MONTHDAY:greatday}");
+        assertThat(grok.captures("nomatch"), nullValue());
+    }
+}
diff --git a/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java
new file mode 100644
index 0000000..3f4bdf1
--- /dev/null
+++ b/modules/ingest-grok/src/test/java/org/elasticsearch/ingest/grok/IngestGrokRestIT.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.grok;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.ingest.grok.IngestGrokPlugin;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class IngestGrokRestIT extends ESRestTestCase {
+
+    public IngestGrokRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml b/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml
new file mode 100644
index 0000000..5c0cca3
--- /dev/null
+++ b/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/10_basic.yaml
@@ -0,0 +1,11 @@
+"Ingest grok installed":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.modules.0.name: ingest-grok  }
diff --git a/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml b/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml
new file mode 100644
index 0000000..f88136d
--- /dev/null
+++ b/modules/ingest-grok/src/test/resources/rest-api-spec/test/ingest_grok/20_grok.yaml
@@ -0,0 +1,109 @@
+---
+"Test Grok Pipeline":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "grok" : {
+                  "field" : "field1",
+                  "pattern" : "%{NUMBER:val:float} %{NUMBER:status:int} <%{WORD:msg}>"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field1: "123.42 400 <foo>"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.val: 123.42 }
+  - match: { _source.status: 400 }
+  - match: { _source.msg: "foo" }
+
+---
+"Test Grok Pipeline With Custom Pattern":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "grok" : {
+                  "field" : "field1",
+                  "pattern" : "<%{MY_PATTERN:msg}>",
+                  "pattern_definitions" : {
+                    "MY_PATTERN" : "foo"
+                  }
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field1: "<foo>"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.msg: "foo" }
+
+---
+"Test Grok Pipeline With Custom Pattern Sharing Same Name As Another":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "grok" : {
+                  "field" : "field1",
+                  "pattern" : "<%{NUMBER:msg}>",
+                  "pattern_definitions" : {
+                    "NUMBER" : "foo"
+                  }
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field1: "<foo>"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.msg: "foo" }
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
index 7ea5c67..b8c6f6d 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/HistogramTests.java
@@ -1029,7 +1029,7 @@ public class HistogramTests extends ESIntegTestCase {
                     .addAggregation(histogram("histo").field(SINGLE_VALUED_FIELD_NAME).interval(-1).minDocCount(0)).execute().actionGet();
             fail();
         } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString(), containsString("[interval] must be 1 or greater for histogram aggregation [histo]"));
+            assertThat(e.toString(), containsString("Missing required field [interval]"));
         }
     }
 }
diff --git a/plugins/ingest-geoip/build.gradle b/plugins/ingest-geoip/build.gradle
new file mode 100644
index 0000000..7eee668
--- /dev/null
+++ b/plugins/ingest-geoip/build.gradle
@@ -0,0 +1,63 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+esplugin {
+  description 'Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database'
+  classname 'org.elasticsearch.ingest.geoip.IngestGeoIpPlugin'
+}
+
+dependencies {
+  compile ('com.maxmind.geoip2:geoip2:2.4.0')
+  // geoip2 dependencies:
+  compile('com.fasterxml.jackson.core:jackson-annotations:2.5.0')
+  compile('com.fasterxml.jackson.core:jackson-databind:2.5.3')
+  compile('com.maxmind.db:maxmind-db:1.0.1')
+
+  testCompile 'org.elasticsearch:geolite2-databases:20151029'
+}
+
+task copyDefaultGeoIp2DatabaseFiles(type: Copy) {
+  from { zipTree(configurations.testCompile.files.find { it.name.contains('geolite2-databases')}) }
+  into "${project.buildDir}/ingest-geoip"
+  include "*.mmdb"
+}
+
+project.bundlePlugin.dependsOn(copyDefaultGeoIp2DatabaseFiles)
+
+compileJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked,-serial"
+compileTestJava.options.compilerArgs << "-Xlint:-rawtypes,-unchecked"
+
+bundlePlugin {
+  from("${project.buildDir}/ingest-geoip") {
+    into 'config/'
+  }
+}
+
+thirdPartyAudit.excludes = [
+  // geoip WebServiceClient needs Google http client, but we're not using WebServiceClient:
+  'com.google.api.client.http.HttpTransport',
+  'com.google.api.client.http.GenericUrl',
+  'com.google.api.client.http.HttpResponse',
+  'com.google.api.client.http.HttpRequestFactory',
+  'com.google.api.client.http.HttpRequest',
+  'com.google.api.client.http.HttpHeaders',
+  'com.google.api.client.http.HttpResponseException',
+  'com.google.api.client.http.javanet.NetHttpTransport',
+  'com.google.api.client.http.javanet.NetHttpTransport',
+]
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1 b/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1
new file mode 100644
index 0000000..485286f
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/geoip2-2.4.0.jar.sha1
@@ -0,0 +1 @@
+ad40667ae87138e0aed075d2c15884497fa64acc
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt b/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt
new file mode 100644
index 0000000..7a4a3ea
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/geoip2-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt b/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt
new file mode 100644
index 0000000..448b71d
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/geoip2-NOTICE.txt
@@ -0,0 +1,3 @@
+This software is Copyright (c) 2013 by MaxMind, Inc.
+
+This is free software, licensed under the Apache License, Version 2.0.
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1 b/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1
new file mode 100644
index 0000000..862ac6f
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/jackson-annotations-2.5.0.jar.sha1
@@ -0,0 +1 @@
+a2a55a3375bc1cef830ca426d68d2ea22961190e
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE b/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE
new file mode 100644
index 0000000..f5f45d2
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/jackson-annotations-LICENSE
@@ -0,0 +1,8 @@
+This copy of Jackson JSON processor streaming parser/generator is licensed under the
+Apache (Software) License, version 2.0 ("the License").
+See the License for details about distribution rights, and the
+specific rights regarding derivate works.
+
+You may obtain a copy of the License at:
+
+http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE b/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE
new file mode 100644
index 0000000..4c976b7
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/jackson-annotations-NOTICE
@@ -0,0 +1,20 @@
+# Jackson JSON processor
+
+Jackson is a high-performance, Free/Open Source JSON processing library.
+It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
+been in development since 2007.
+It is currently developed by a community of developers, as well as supported
+commercially by FasterXML.com.
+
+## Licensing
+
+Jackson core and extension components may licensed under different licenses.
+To find the details that apply to this artifact see the accompanying LICENSE file.
+For more information, including possible other licensing options, contact
+FasterXML.com (http://fasterxml.com).
+
+## Credits
+
+A list of contributors may be found from CREDITS file, which is included
+in some artifacts (usually source distributions); but is always available
+from the source code management (SCM) system project uses.
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1 b/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1
new file mode 100644
index 0000000..cdc6695
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/jackson-databind-2.5.3.jar.sha1
@@ -0,0 +1 @@
+c37875ff66127d93e5f672708cb2dcc14c8232ab
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-LICENSE b/plugins/ingest-geoip/licenses/jackson-databind-LICENSE
new file mode 100644
index 0000000..f5f45d2
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/jackson-databind-LICENSE
@@ -0,0 +1,8 @@
+This copy of Jackson JSON processor streaming parser/generator is licensed under the
+Apache (Software) License, version 2.0 ("the License").
+See the License for details about distribution rights, and the
+specific rights regarding derivate works.
+
+You may obtain a copy of the License at:
+
+http://www.apache.org/licenses/LICENSE-2.0
diff --git a/plugins/ingest-geoip/licenses/jackson-databind-NOTICE b/plugins/ingest-geoip/licenses/jackson-databind-NOTICE
new file mode 100644
index 0000000..4c976b7
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/jackson-databind-NOTICE
@@ -0,0 +1,20 @@
+# Jackson JSON processor
+
+Jackson is a high-performance, Free/Open Source JSON processing library.
+It was originally written by Tatu Saloranta (tatu.saloranta@iki.fi), and has
+been in development since 2007.
+It is currently developed by a community of developers, as well as supported
+commercially by FasterXML.com.
+
+## Licensing
+
+Jackson core and extension components may licensed under different licenses.
+To find the details that apply to this artifact see the accompanying LICENSE file.
+For more information, including possible other licensing options, contact
+FasterXML.com (http://fasterxml.com).
+
+## Credits
+
+A list of contributors may be found from CREDITS file, which is included
+in some artifacts (usually source distributions); but is always available
+from the source code management (SCM) system project uses.
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1 b/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1
new file mode 100644
index 0000000..6cb749e
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/maxmind-db-1.0.1.jar.sha1
@@ -0,0 +1 @@
+305429b84dbcd1cc3d393686f412cdcaec9cdbe6
\ No newline at end of file
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt b/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/maxmind-db-LICENSE.txt
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt b/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt
new file mode 100644
index 0000000..1ebe2b0
--- /dev/null
+++ b/plugins/ingest-geoip/licenses/maxmind-db-NOTICE.txt
@@ -0,0 +1,3 @@
+This software is Copyright (c) 2014 by MaxMind, Inc.
+
+This is free software, licensed under the Apache License, Version 2.0.
diff --git a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
new file mode 100644
index 0000000..b1c25f5
--- /dev/null
+++ b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/GeoIpProcessor.java
@@ -0,0 +1,289 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.geoip;
+
+import com.maxmind.geoip2.DatabaseReader;
+import com.maxmind.geoip2.exception.AddressNotFoundException;
+import com.maxmind.geoip2.model.CityResponse;
+import com.maxmind.geoip2.model.CountryResponse;
+import com.maxmind.geoip2.record.City;
+import com.maxmind.geoip2.record.Continent;
+import com.maxmind.geoip2.record.Country;
+import com.maxmind.geoip2.record.Location;
+import com.maxmind.geoip2.record.Subdivision;
+import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.SpecialPermission;
+import org.elasticsearch.common.network.InetAddresses;
+import org.elasticsearch.common.network.NetworkAddress;
+import org.elasticsearch.ingest.core.AbstractProcessor;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.net.InetAddress;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
+
+import static org.elasticsearch.ingest.core.ConfigurationUtils.readOptionalList;
+import static org.elasticsearch.ingest.core.ConfigurationUtils.readStringProperty;
+
+public final class GeoIpProcessor extends AbstractProcessor {
+
+    public static final String TYPE = "geoip";
+
+    private final String sourceField;
+    private final String targetField;
+    private final DatabaseReader dbReader;
+    private final Set<Field> fields;
+
+    GeoIpProcessor(String tag, String sourceField, DatabaseReader dbReader, String targetField, Set<Field> fields) throws IOException {
+        super(tag);
+        this.sourceField = sourceField;
+        this.targetField = targetField;
+        this.dbReader = dbReader;
+        this.fields = fields;
+    }
+
+    @Override
+    public void execute(IngestDocument ingestDocument) {
+        String ip = ingestDocument.getFieldValue(sourceField, String.class);
+        final InetAddress ipAddress = InetAddresses.forString(ip);
+
+        Map<String, Object> geoData;
+        switch (dbReader.getMetadata().getDatabaseType()) {
+            case "GeoLite2-City":
+                try {
+                    geoData = retrieveCityGeoData(ipAddress);
+                } catch (AddressNotFoundRuntimeException e) {
+                    geoData = Collections.emptyMap();
+                }
+                break;
+            case "GeoLite2-Country":
+                try {
+                    geoData = retrieveCountryGeoData(ipAddress);
+                } catch (AddressNotFoundRuntimeException e) {
+                    geoData = Collections.emptyMap();
+                }
+                break;
+            default:
+                throw new IllegalStateException("Unsupported database type [" + dbReader.getMetadata().getDatabaseType() + "]");
+        }
+        ingestDocument.setFieldValue(targetField, geoData);
+    }
+
+    @Override
+    public String getType() {
+        return TYPE;
+    }
+
+    String getSourceField() {
+        return sourceField;
+    }
+
+    String getTargetField() {
+        return targetField;
+    }
+
+    DatabaseReader getDbReader() {
+        return dbReader;
+    }
+
+    Set<Field> getFields() {
+        return fields;
+    }
+
+    private Map<String, Object> retrieveCityGeoData(InetAddress ipAddress) {
+        SecurityManager sm = System.getSecurityManager();
+        if (sm != null) {
+            sm.checkPermission(new SpecialPermission());
+        }
+        CityResponse response = AccessController.doPrivileged((PrivilegedAction<CityResponse>) () -> {
+            try {
+                return dbReader.city(ipAddress);
+            } catch (AddressNotFoundException e) {
+                throw new AddressNotFoundRuntimeException(e);
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+        });
+
+        Country country = response.getCountry();
+        City city = response.getCity();
+        Location location = response.getLocation();
+        Continent continent = response.getContinent();
+        Subdivision subdivision = response.getMostSpecificSubdivision();
+
+        Map<String, Object> geoData = new HashMap<>();
+        for (Field field : fields) {
+            switch (field) {
+                case IP:
+                    geoData.put("ip", NetworkAddress.formatAddress(ipAddress));
+                    break;
+                case COUNTRY_ISO_CODE:
+                    geoData.put("country_iso_code", country.getIsoCode());
+                    break;
+                case COUNTRY_NAME:
+                    geoData.put("country_name", country.getName());
+                    break;
+                case CONTINENT_NAME:
+                    geoData.put("continent_name", continent.getName());
+                    break;
+                case REGION_NAME:
+                    geoData.put("region_name", subdivision.getName());
+                    break;
+                case CITY_NAME:
+                    geoData.put("city_name", city.getName());
+                    break;
+                case TIMEZONE:
+                    geoData.put("timezone", location.getTimeZone());
+                    break;
+                case LOCATION:
+                    Map<String, Object> locationObject = new HashMap<>();
+                    locationObject.put("lat", location.getLatitude());
+                    locationObject.put("lon", location.getLongitude());
+                    geoData.put("location", locationObject);
+                    break;
+            }
+        }
+        return geoData;
+    }
+
+    private Map<String, Object> retrieveCountryGeoData(InetAddress ipAddress) {
+        SecurityManager sm = System.getSecurityManager();
+        if (sm != null) {
+            sm.checkPermission(new SpecialPermission());
+        }
+        CountryResponse response = AccessController.doPrivileged((PrivilegedAction<CountryResponse>) () -> {
+            try {
+                return dbReader.country(ipAddress);
+            } catch (AddressNotFoundException e) {
+                throw new AddressNotFoundRuntimeException(e);
+            } catch (Exception e) {
+                throw new RuntimeException(e);
+            }
+        });
+
+        Country country = response.getCountry();
+        Continent continent = response.getContinent();
+
+        Map<String, Object> geoData = new HashMap<>();
+        for (Field field : fields) {
+            switch (field) {
+                case IP:
+                    geoData.put("ip", NetworkAddress.formatAddress(ipAddress));
+                    break;
+                case COUNTRY_ISO_CODE:
+                    geoData.put("country_iso_code", country.getIsoCode());
+                    break;
+                case COUNTRY_NAME:
+                    geoData.put("country_name", country.getName());
+                    break;
+                case CONTINENT_NAME:
+                    geoData.put("continent_name", continent.getName());
+                    break;
+            }
+        }
+        return geoData;
+    }
+
+    public static final class Factory extends AbstractProcessorFactory<GeoIpProcessor> implements Closeable {
+
+        static final Set<Field> DEFAULT_FIELDS = EnumSet.of(
+                Field.CONTINENT_NAME, Field.COUNTRY_ISO_CODE, Field.REGION_NAME, Field.CITY_NAME, Field.LOCATION
+        );
+
+        private final Map<String, DatabaseReader> databaseReaders;
+
+        public Factory(Map<String, DatabaseReader> databaseReaders) {
+            this.databaseReaders = databaseReaders;
+        }
+
+        @Override
+        public GeoIpProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            String ipField = readStringProperty(config, "source_field");
+            String targetField = readStringProperty(config, "target_field", "geoip");
+            String databaseFile = readStringProperty(config, "database_file", "GeoLite2-City.mmdb");
+            List<String> fieldNames = readOptionalList(config, "fields");
+
+            final Set<Field> fields;
+            if (fieldNames != null) {
+                fields = EnumSet.noneOf(Field.class);
+                for (String fieldName : fieldNames) {
+                    try {
+                        fields.add(Field.parse(fieldName));
+                    } catch (Exception e) {
+                        throw new IllegalArgumentException("illegal field option [" + fieldName +"]. valid values are [" + Arrays.toString(Field.values()) +"]", e);
+                    }
+                }
+            } else {
+                fields = DEFAULT_FIELDS;
+            }
+
+            DatabaseReader databaseReader = databaseReaders.get(databaseFile);
+            if (databaseReader == null) {
+                throw new IllegalArgumentException("database file [" + databaseFile + "] doesn't exist");
+            }
+            return new GeoIpProcessor(processorTag, ipField, databaseReader, targetField, fields);
+        }
+
+        @Override
+        public void close() throws IOException {
+            IOUtils.close(databaseReaders.values());
+        }
+    }
+
+    // Geoip2's AddressNotFoundException is checked and due to the fact that we need run their code
+    // inside a PrivilegedAction code block, we are forced to catch any checked exception and rethrow
+    // it with an unchecked exception.
+    private final static class AddressNotFoundRuntimeException extends RuntimeException {
+
+        public AddressNotFoundRuntimeException(Throwable cause) {
+            super(cause);
+        }
+    }
+
+    public enum Field {
+
+        IP,
+        COUNTRY_ISO_CODE,
+        COUNTRY_NAME,
+        CONTINENT_NAME,
+        REGION_NAME,
+        CITY_NAME,
+        TIMEZONE,
+        LATITUDE,
+        LONGITUDE,
+        LOCATION;
+
+        public static Field parse(String value) {
+            return valueOf(value.toUpperCase(Locale.ROOT));
+        }
+    }
+
+}
diff --git a/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
new file mode 100644
index 0000000..f92cb7b
--- /dev/null
+++ b/plugins/ingest-geoip/src/main/java/org/elasticsearch/ingest/geoip/IngestGeoIpPlugin.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.geoip;
+
+import com.maxmind.geoip2.DatabaseReader;
+import org.elasticsearch.node.NodeModule;
+import org.elasticsearch.plugins.Plugin;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.PathMatcher;
+import java.nio.file.StandardOpenOption;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.stream.Stream;
+
+public class IngestGeoIpPlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "ingest-geoip";
+    }
+
+    @Override
+    public String description() {
+        return "Ingest processor that adds information about the geographical location of ip addresses";
+    }
+
+    public void onModule(NodeModule nodeModule) throws IOException {
+        Path geoIpConfigDirectory = nodeModule.getNode().getEnvironment().configFile().resolve("ingest-geoip");
+        Map<String, DatabaseReader> databaseReaders = loadDatabaseReaders(geoIpConfigDirectory);
+        nodeModule.registerProcessor(GeoIpProcessor.TYPE, (templateService) -> new GeoIpProcessor.Factory(databaseReaders));
+    }
+
+    static Map<String, DatabaseReader> loadDatabaseReaders(Path geoIpConfigDirectory) throws IOException {
+        if (Files.exists(geoIpConfigDirectory) == false && Files.isDirectory(geoIpConfigDirectory)) {
+            throw new IllegalStateException("the geoip directory [" + geoIpConfigDirectory  + "] containing databases doesn't exist");
+        }
+
+        Map<String, DatabaseReader> databaseReaders = new HashMap<>();
+        try (Stream<Path> databaseFiles = Files.list(geoIpConfigDirectory)) {
+            PathMatcher pathMatcher = geoIpConfigDirectory.getFileSystem().getPathMatcher("glob:**.mmdb");
+            // Use iterator instead of forEach otherwise IOException needs to be caught twice...
+            Iterator<Path> iterator = databaseFiles.iterator();
+            while (iterator.hasNext()) {
+                Path databasePath = iterator.next();
+                if (Files.isRegularFile(databasePath) && pathMatcher.matches(databasePath)) {
+                    try (InputStream inputStream = Files.newInputStream(databasePath, StandardOpenOption.READ)) {
+                        databaseReaders.put(databasePath.getFileName().toString(), new DatabaseReader.Builder(inputStream).build());
+                    }
+                }
+            }
+        }
+        return Collections.unmodifiableMap(databaseReaders);
+    }
+}
diff --git a/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy b/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy
new file mode 100644
index 0000000..f49d15d
--- /dev/null
+++ b/plugins/ingest-geoip/src/main/plugin-metadata/plugin-security.policy
@@ -0,0 +1,27 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+grant {
+  // needed because jackson-databind is using Class#getDeclaredConstructors(), Class#getDeclaredMethods() and
+  // Class#getDeclaredAnnotations() to find all public, private, protected, package protected and
+  // private constructors, methods or annotations. Just locating all public constructors, methods and annotations
+  // should be enough, so this permission wouldn't then be needed. Unfortunately this is not what jackson-databind does
+  // or can be configured to do.
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
+};
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
new file mode 100644
index 0000000..b59242e
--- /dev/null
+++ b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorFactoryTests.java
@@ -0,0 +1,152 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.geoip;
+
+import com.maxmind.geoip2.DatabaseReader;
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.StreamsUtils;
+import org.junit.BeforeClass;
+
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.sameInstance;
+
+public class GeoIpProcessorFactoryTests extends ESTestCase {
+
+    private static Map<String, DatabaseReader> databaseReaders;
+
+    @BeforeClass
+    public static void loadDatabaseReaders() throws IOException {
+        Path configDir = createTempDir();
+        Path geoIpConfigDir = configDir.resolve("ingest-geoip");
+        Files.createDirectories(geoIpConfigDir);
+        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-City.mmdb")), geoIpConfigDir.resolve("GeoLite2-City.mmdb"));
+        Files.copy(new ByteArrayInputStream(StreamsUtils.copyToBytesFromClasspath("/GeoLite2-Country.mmdb")), geoIpConfigDir.resolve("GeoLite2-Country.mmdb"));
+        databaseReaders = IngestGeoIpPlugin.loadDatabaseReaders(geoIpConfigDir);
+    }
+
+    public void testBuildDefaults() throws Exception {
+        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
+
+        Map<String, Object> config = new HashMap<>();
+        config.put("source_field", "_field");
+
+        String processorTag = randomAsciiOfLength(10);
+        config.put(AbstractProcessorFactory.TAG_KEY, processorTag);
+
+        GeoIpProcessor processor = factory.create(config);
+        assertThat(processor.getTag(), equalTo(processorTag));
+        assertThat(processor.getSourceField(), equalTo("_field"));
+        assertThat(processor.getTargetField(), equalTo("geoip"));
+        assertThat(processor.getDbReader().getMetadata().getDatabaseType(), equalTo("GeoLite2-City"));
+        assertThat(processor.getFields(), sameInstance(GeoIpProcessor.Factory.DEFAULT_FIELDS));
+    }
+
+    public void testBuildTargetField() throws Exception {
+        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
+        Map<String, Object> config = new HashMap<>();
+        config.put("source_field", "_field");
+        config.put("target_field", "_field");
+        GeoIpProcessor processor = factory.create(config);
+        assertThat(processor.getSourceField(), equalTo("_field"));
+        assertThat(processor.getTargetField(), equalTo("_field"));
+    }
+
+    public void testBuildDbFile() throws Exception {
+        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
+        Map<String, Object> config = new HashMap<>();
+        config.put("source_field", "_field");
+        config.put("database_file", "GeoLite2-Country.mmdb");
+        GeoIpProcessor processor = factory.create(config);
+        assertThat(processor.getSourceField(), equalTo("_field"));
+        assertThat(processor.getTargetField(), equalTo("geoip"));
+        assertThat(processor.getDbReader().getMetadata().getDatabaseType(), equalTo("GeoLite2-Country"));
+    }
+
+    public void testBuildNonExistingDbFile() throws Exception {
+        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
+
+        Map<String, Object> config = new HashMap<>();
+        config.put("source_field", "_field");
+        config.put("database_file", "does-not-exist.mmdb");
+        try {
+            factory.create(config);
+            fail("Exception expected");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("database file [does-not-exist.mmdb] doesn't exist"));
+        }
+    }
+
+    public void testBuildFields() throws Exception {
+        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
+
+        Set<GeoIpProcessor.Field> fields = EnumSet.noneOf(GeoIpProcessor.Field.class);
+        List<String> fieldNames = new ArrayList<>();
+        int numFields = scaledRandomIntBetween(1, GeoIpProcessor.Field.values().length);
+        for (int i = 0; i < numFields; i++) {
+            GeoIpProcessor.Field field = GeoIpProcessor.Field.values()[i];
+            fields.add(field);
+            fieldNames.add(field.name().toLowerCase(Locale.ROOT));
+        }
+        Map<String, Object> config = new HashMap<>();
+        config.put("source_field", "_field");
+        config.put("fields", fieldNames);
+        GeoIpProcessor processor = factory.create(config);
+        assertThat(processor.getSourceField(), equalTo("_field"));
+        assertThat(processor.getFields(), equalTo(fields));
+    }
+
+    public void testBuildIllegalFieldOption() throws Exception {
+        GeoIpProcessor.Factory factory = new GeoIpProcessor.Factory(databaseReaders);
+
+        Map<String, Object> config = new HashMap<>();
+        config.put("source_field", "_field");
+        config.put("fields", Collections.singletonList("invalid"));
+        try {
+            factory.create(config);
+            fail("exception expected");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("illegal field option [invalid]. valid values are [[IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_NAME, CITY_NAME, TIMEZONE, LATITUDE, LONGITUDE, LOCATION]]"));
+        }
+
+        config = new HashMap<>();
+        config.put("source_field", "_field");
+        config.put("fields", "invalid");
+        try {
+            factory.create(config);
+            fail("exception expected");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("property [fields] isn't a list, but of type [java.lang.String]"));
+        }
+    }
+}
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java
new file mode 100644
index 0000000..4351798
--- /dev/null
+++ b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/GeoIpProcessorTests.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.geoip;
+
+import com.maxmind.geoip2.DatabaseReader;
+import org.elasticsearch.ingest.RandomDocumentPicks;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.InputStream;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+
+public class GeoIpProcessorTests extends ESTestCase {
+
+    public void testCity() throws Exception {
+        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
+        GeoIpProcessor processor = new GeoIpProcessor(randomAsciiOfLength(10), "source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
+
+        Map<String, Object> document = new HashMap<>();
+        document.put("source_field", "82.170.213.79");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        processor.execute(ingestDocument);
+
+        assertThat(ingestDocument.getSourceAndMetadata().get("source_field"), equalTo("82.170.213.79"));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
+        assertThat(geoData.size(), equalTo(8));
+        assertThat(geoData.get("ip"), equalTo("82.170.213.79"));
+        assertThat(geoData.get("country_iso_code"), equalTo("NL"));
+        assertThat(geoData.get("country_name"), equalTo("Netherlands"));
+        assertThat(geoData.get("continent_name"), equalTo("Europe"));
+        assertThat(geoData.get("region_name"), equalTo("North Holland"));
+        assertThat(geoData.get("city_name"), equalTo("Amsterdam"));
+        assertThat(geoData.get("timezone"), equalTo("Europe/Amsterdam"));
+        Map<String, Object> location = new HashMap<>();
+        location.put("lat", 52.374d);
+        location.put("lon", 4.8897d);
+        assertThat(geoData.get("location"), equalTo(location));
+    }
+
+    public void testCountry() throws Exception {
+        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-Country.mmdb");
+        GeoIpProcessor processor = new GeoIpProcessor(randomAsciiOfLength(10), "source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
+
+        Map<String, Object> document = new HashMap<>();
+        document.put("source_field", "82.170.213.79");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        processor.execute(ingestDocument);
+
+        assertThat(ingestDocument.getSourceAndMetadata().get("source_field"), equalTo("82.170.213.79"));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
+        assertThat(geoData.size(), equalTo(4));
+        assertThat(geoData.get("ip"), equalTo("82.170.213.79"));
+        assertThat(geoData.get("country_iso_code"), equalTo("NL"));
+        assertThat(geoData.get("country_name"), equalTo("Netherlands"));
+        assertThat(geoData.get("continent_name"), equalTo("Europe"));
+    }
+
+    public void testAddressIsNotInTheDatabase() throws Exception {
+        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
+        GeoIpProcessor processor = new GeoIpProcessor(randomAsciiOfLength(10), "source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
+
+        Map<String, Object> document = new HashMap<>();
+        document.put("source_field", "202.45.11.11");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        processor.execute(ingestDocument);
+        @SuppressWarnings("unchecked")
+        Map<String, Object> geoData = (Map<String, Object>) ingestDocument.getSourceAndMetadata().get("target_field");
+        assertThat(geoData.size(), equalTo(0));
+    }
+
+    /** Don't silently do DNS lookups or anything trappy on bogus data */
+    public void testInvalid() throws Exception {
+        InputStream database = GeoIpProcessor.class.getResourceAsStream("/GeoLite2-City.mmdb");
+        GeoIpProcessor processor = new GeoIpProcessor(randomAsciiOfLength(10), "source_field", new DatabaseReader.Builder(database).build(), "target_field", EnumSet.allOf(GeoIpProcessor.Field.class));
+
+        Map<String, Object> document = new HashMap<>();
+        document.put("source_field", "www.google.com");
+        IngestDocument ingestDocument = RandomDocumentPicks.randomIngestDocument(random(), document);
+        try {
+            processor.execute(ingestDocument);
+            fail("did not get expected exception");
+        } catch (IllegalArgumentException expected) {
+            assertNotNull(expected.getMessage());
+            assertThat(expected.getMessage(), containsString("not an IP string literal"));
+        }
+    }
+
+}
diff --git a/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java
new file mode 100644
index 0000000..0e4d1ee
--- /dev/null
+++ b/plugins/ingest-geoip/src/test/java/org/elasticsearch/ingest/geoip/IngestGeoIpRestIT.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest.geoip;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class IngestGeoIpRestIT extends ESRestTestCase {
+
+    public IngestGeoIpRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml b/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml
new file mode 100644
index 0000000..b522cb7
--- /dev/null
+++ b/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/10_basic.yaml
@@ -0,0 +1,5 @@
+"Ingest plugin installed":
+    - do:
+        cluster.stats: {}
+
+    - match:  { nodes.plugins.0.name: ingest-geoip  }
diff --git a/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml b/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml
new file mode 100644
index 0000000..704f288
--- /dev/null
+++ b/plugins/ingest-geoip/src/test/resources/rest-api-spec/test/ingest_geoip/20_geoip_processor.yaml
@@ -0,0 +1,124 @@
+---
+"Test geoip processor with defaults":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "geoip" : {
+                  "source_field" : "field1"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field1: "128.101.101.101"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.field1: "128.101.101.101" }
+  - length: { _source.geoip: 5 }
+  - match: { _source.geoip.city_name: "Minneapolis" }
+  - match: { _source.geoip.country_iso_code: "US" }
+  - match: { _source.geoip.location.lon: -93.2166 }
+  - match: { _source.geoip.location.lat: 44.9759 }
+  - match: { _source.geoip.region_name: "Minnesota" }
+  - match: { _source.geoip.continent_name: "North America" }
+
+---
+"Test geoip processor with fields":
+  - do:
+      cluster.health:
+          wait_for_status: green
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "geoip" : {
+                  "source_field" : "field1",
+                  "fields" : ["city_name", "country_iso_code", "ip", "latitude", "longitude", "location", "timezone", "country_name", "region_name", "continent_name"]
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field1: "128.101.101.101"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.field1: "128.101.101.101" }
+  - length: { _source.geoip: 8 }
+  - match: { _source.geoip.city_name: "Minneapolis" }
+  - match: { _source.geoip.country_iso_code: "US" }
+  - match: { _source.geoip.ip: "128.101.101.101" }
+  - match: { _source.geoip.location.lon: -93.2166 }
+  - match: { _source.geoip.location.lat: 44.9759 }
+  - match: { _source.geoip.timezone: "America/Chicago" }
+  - match: { _source.geoip.country_name: "United States" }
+  - match: { _source.geoip.region_name: "Minnesota" }
+  - match: { _source.geoip.continent_name: "North America" }
+
+---
+"Test geoip processor with different database file":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "geoip" : {
+                  "source_field" : "field1",
+                  "database_file" : "GeoLite2-Country.mmdb"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field1: "128.101.101.101"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.field1: "128.101.101.101" }
+  - length: { _source.geoip: 2 }
+  - match: { _source.geoip.country_iso_code: "US" }
+  - match: { _source.geoip.continent_name: "North America" }
diff --git a/qa/ingest-disabled/build.gradle b/qa/ingest-disabled/build.gradle
new file mode 100644
index 0000000..ca71697
--- /dev/null
+++ b/qa/ingest-disabled/build.gradle
@@ -0,0 +1,26 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+apply plugin: 'elasticsearch.rest-test'
+
+integTest {
+    cluster {
+        systemProperty 'es.node.ingest', 'false'
+    }
+}
diff --git a/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java b/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
new file mode 100644
index 0000000..e162807
--- /dev/null
+++ b/qa/ingest-disabled/src/test/java/org/elasticsearch/smoketest/IngestDisabledIT.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.smoketest;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+
+public class IngestDisabledIT extends ESRestTestCase {
+
+    public IngestDisabledIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+
+}
diff --git a/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml b/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
new file mode 100644
index 0000000..01d6740
--- /dev/null
+++ b/qa/ingest-disabled/src/test/resources/rest-api-spec/test/ingest_mustache/10_ingest_disabled.yaml
@@ -0,0 +1,122 @@
+---
+"Test ingest CRUD APIS work fine when node.ingest is set to false":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field2",
+                  "value": "_value"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.get_pipeline:
+        id: "my_pipeline"
+  - match: { pipelines.0.id: "my_pipeline" }
+  - match: { pipelines.0.config.description: "_description" }
+
+  - do:
+      ingest.delete_pipeline:
+        id: "my_pipeline"
+  - match: { acknowledged: true }
+
+---
+"Test ingest simulate API works fine when node.ingest is set to false":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field2",
+                  "value" : "_value"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.simulate:
+        id: "my_pipeline"
+        body: >
+          {
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { docs: 1 }
+  - match: { docs.0.doc._source.foo: "bar" }
+  - match: { docs.0.doc._source.field2: "_value" }
+  - length: { docs.0.doc._ingest: 1 }
+  - is_true: docs.0.doc._ingest.timestamp
+
+---
+"Test index api with pipeline id fails when node.ingest is set to false":
+  - do:
+      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_1"
+        body: {
+          field1: "1",
+          field2: "2",
+          field3: "3"
+        }
+
+---
+"Test bulk api with pipeline id fails when node.ingest is set to false":
+  - do:
+      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
+      bulk:
+        pipeline: "my_pipeline_1"
+        body:
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id
+          - f1: v1
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id2
+          - f1: v2
+
+---
+"Test bulk api that contains a single index call with pipeline id fails when node.ingest is set to false":
+  - do:
+      catch: /There are no ingest nodes in this cluster, unable to forward request to an ingest node./
+      bulk:
+        body:
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id
+          - f1: v1
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id2
+              pipeline: my_pipeline_1
+          - f1: v2
+
diff --git a/qa/ingest-with-mustache/build.gradle b/qa/ingest-with-mustache/build.gradle
new file mode 100644
index 0000000..e5ca482
--- /dev/null
+++ b/qa/ingest-with-mustache/build.gradle
@@ -0,0 +1,24 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+apply plugin: 'elasticsearch.rest-test'
+
+dependencies {
+    testCompile project(path: ':modules:lang-mustache', configuration: 'runtime')
+}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
new file mode 100644
index 0000000..57165e6
--- /dev/null
+++ b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/AbstractMustacheTests.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.ingest.InternalTemplateService;
+import org.elasticsearch.ingest.core.TemplateService;
+import org.elasticsearch.script.ScriptContextRegistry;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Collections;
+
+public abstract class AbstractMustacheTests extends ESTestCase {
+
+    protected TemplateService templateService;
+
+    @Before
+    public void init() throws Exception {
+        Settings settings = Settings.builder()
+            .put("path.home", createTempDir())
+            .put(ScriptService.SCRIPT_AUTO_RELOAD_ENABLED_SETTING, false)
+            .build();
+        MustacheScriptEngineService mustache = new MustacheScriptEngineService(settings);
+        ScriptContextRegistry registry = new ScriptContextRegistry(Collections.emptyList());
+        ScriptService scriptService = new ScriptService(
+            settings, new Environment(settings), Collections.singleton(mustache), null, registry
+        );
+        templateService = new InternalTemplateService(scriptService);
+    }
+
+}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
new file mode 100644
index 0000000..f27a8e4
--- /dev/null
+++ b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestDocumentMustacheIT.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ValueSource;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class IngestDocumentMustacheIT extends AbstractMustacheTests {
+
+    public void testAccessMetaDataViaTemplate() {
+        Map<String, Object> document = new HashMap<>();
+        document.put("foo", "bar");
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 bar"));
+
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 bar"));
+    }
+
+    public void testAccessMapMetaDataViaTemplate() {
+        Map<String, Object> document = new HashMap<>();
+        Map<String, Object> innerObject = new HashMap<>();
+        innerObject.put("bar", "hello bar");
+        innerObject.put("baz", "hello baz");
+        innerObject.put("qux", Collections.singletonMap("fubar", "hello qux and fubar"));
+        document.put("foo", innerObject);
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{foo.bar}} {{foo.baz}} {{foo.qux.fubar}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 hello bar hello baz hello qux and fubar"));
+
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("2 {{_source.foo.bar}} {{_source.foo.baz}} {{_source.foo.qux.fubar}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("2 hello bar hello baz hello qux and fubar"));
+    }
+
+    public void testAccessListMetaDataViaTemplate() {
+        Map<String, Object> document = new HashMap<>();
+        document.put("list1", Arrays.asList("foo", "bar", null));
+        List<Map<String, Object>> list = new ArrayList<>();
+        Map<String, Object> value = new HashMap<>();
+        value.put("field", "value");
+        list.add(value);
+        list.add(null);
+        document.put("list2", list);
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+        ingestDocument.setFieldValue(templateService.compile("field1"), ValueSource.wrap("1 {{list1.0}} {{list2.0}}", templateService));
+        assertThat(ingestDocument.getFieldValue("field1", String.class), equalTo("1 foo {field=value}"));
+    }
+
+    public void testAccessIngestMetadataViaTemplate() {
+        Map<String, Object> document = new HashMap<>();
+        Map<String, Object> ingestMap = new HashMap<>();
+        ingestMap.put("timestamp", "bogus_timestamp");
+        document.put("_ingest", ingestMap);
+        IngestDocument ingestDocument = new IngestDocument("index", "type", "id", null, null, null, null, document);
+        ingestDocument.setFieldValue(templateService.compile("ingest_timestamp"), ValueSource.wrap("{{_ingest.timestamp}} and {{_source._ingest.timestamp}}", templateService));
+        assertThat(ingestDocument.getFieldValue("ingest_timestamp", String.class), equalTo(ingestDocument.getIngestMetadata().get("timestamp") + " and bogus_timestamp"));
+    }
+}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
new file mode 100644
index 0000000..e94765a
--- /dev/null
+++ b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheRemoveProcessorIT.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.processor.RemoveProcessor;
+import org.hamcrest.CoreMatchers;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+public class IngestMustacheRemoveProcessorIT extends AbstractMustacheTests {
+
+    public void testRemoveProcessorMustacheExpression() throws Exception {
+        RemoveProcessor.Factory factory = new RemoveProcessor.Factory(templateService);
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", "field{{var}}");
+        RemoveProcessor processor = factory.create(config);
+        assertThat(processor.getField().execute(Collections.singletonMap("var", "_value")), CoreMatchers.equalTo("field_value"));
+    }
+}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
new file mode 100644
index 0000000..6846679
--- /dev/null
+++ b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/IngestMustacheSetProcessorIT.java
@@ -0,0 +1,72 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ValueSource;
+import org.elasticsearch.ingest.core.Processor;
+import org.elasticsearch.ingest.processor.SetProcessor;
+import org.hamcrest.Matchers;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.instanceOf;
+
+public class IngestMustacheSetProcessorIT extends AbstractMustacheTests {
+
+    public void testExpression() throws Exception {
+        SetProcessor processor = createSetProcessor("_index", "text {{var}}");
+        assertThat(processor.getValue(), instanceOf(ValueSource.TemplatedValue.class));
+        assertThat(processor.getValue().copyAndResolve(Collections.singletonMap("var", "_value")), equalTo("text _value"));
+    }
+
+    public void testSetMetadataWithTemplates() throws Exception {
+        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.values());
+        Processor processor = createSetProcessor(randomMetaData.getFieldName(), "_value {{field}}");
+        IngestDocument ingestDocument = createIngestDocument(Collections.singletonMap("field", "value"));
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class), Matchers.equalTo("_value value"));
+    }
+
+    public void testSetWithTemplates() throws Exception {
+        IngestDocument.MetaData randomMetaData = randomFrom(IngestDocument.MetaData.INDEX, IngestDocument.MetaData.TYPE, IngestDocument.MetaData.ID);
+        Processor processor = createSetProcessor("field{{_type}}", "_value {{" + randomMetaData.getFieldName() + "}}");
+        IngestDocument ingestDocument = createIngestDocument(new HashMap<>());
+        processor.execute(ingestDocument);
+        assertThat(ingestDocument.getFieldValue("field_type", String.class), Matchers.equalTo("_value " + ingestDocument.getFieldValue(randomMetaData.getFieldName(), String.class)));
+    }
+
+    private SetProcessor createSetProcessor(String fieldName, Object fieldValue) throws Exception {
+        SetProcessor.Factory factory = new SetProcessor.Factory(templateService);
+        Map<String, Object> config = new HashMap<>();
+        config.put("field", fieldName);
+        config.put("value", fieldValue);
+        return factory.create(config);
+    }
+
+    private IngestDocument createIngestDocument(Map<String, Object> source) {
+        return new IngestDocument("_index", "_type", "_id", null, null, null, null, source);
+    }
+
+}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
new file mode 100644
index 0000000..1d1579f
--- /dev/null
+++ b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/TemplateServiceIT.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.TemplateService;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+public class TemplateServiceIT extends AbstractMustacheTests {
+
+    public void testTemplates() {
+        Map<String, Object> model = new HashMap<>();
+        model.put("fielda", "value1");
+        model.put("fieldb", Collections.singletonMap("fieldc", "value3"));
+
+        TemplateService.Template template = templateService.compile("{{fielda}}/{{fieldb}}/{{fieldb.fieldc}}");
+        assertThat(template.execute(model), equalTo("value1/{fieldc=value3}/value3"));
+    }
+
+    public void testWrongTemplateUsage() {
+        Map<String, Object> model = Collections.emptyMap();
+        TemplateService.Template template = templateService.compile("value");
+        assertThat(template.execute(model), equalTo("value"));
+
+        template = templateService.compile("value {{");
+        assertThat(template.execute(model), equalTo("value {{"));
+        template = templateService.compile("value {{abc");
+        assertThat(template.execute(model), equalTo("value {{abc"));
+        template = templateService.compile("value }}");
+        assertThat(template.execute(model), equalTo("value }}"));
+        template = templateService.compile("value }} {{");
+        assertThat(template.execute(model), equalTo("value }} {{"));
+    }
+
+}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
new file mode 100644
index 0000000..18085b94
--- /dev/null
+++ b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/ingest/ValueSourceMustacheIT.java
@@ -0,0 +1,76 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.ValueSource;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.is;
+
+public class ValueSourceMustacheIT extends AbstractMustacheTests {
+
+    public void testValueSourceWithTemplates() {
+        Map<String, Object> model = new HashMap<>();
+        model.put("field1", "value1");
+        model.put("field2", Collections.singletonMap("field3", "value3"));
+
+        ValueSource valueSource = ValueSource.wrap("{{field1}}/{{field2}}/{{field2.field3}}", templateService);
+        assertThat(valueSource, instanceOf(ValueSource.TemplatedValue.class));
+        assertThat(valueSource.copyAndResolve(model), equalTo("value1/{field3=value3}/value3"));
+
+        valueSource = ValueSource.wrap(Arrays.asList("_value", "{{field1}}"), templateService);
+        assertThat(valueSource, instanceOf(ValueSource.ListValue.class));
+        List<String> result = (List<String>) valueSource.copyAndResolve(model);
+        assertThat(result.size(), equalTo(2));
+        assertThat(result.get(0), equalTo("_value"));
+        assertThat(result.get(1), equalTo("value1"));
+
+        Map<String, Object> map = new HashMap<>();
+        map.put("field1", "{{field1}}");
+        map.put("field2", Collections.singletonMap("field3", "{{field2.field3}}"));
+        map.put("field4", "_value");
+        valueSource = ValueSource.wrap(map, templateService);
+        assertThat(valueSource, instanceOf(ValueSource.MapValue.class));
+        Map<String, Object> resultMap = (Map<String, Object>) valueSource.copyAndResolve(model);
+        assertThat(resultMap.size(), equalTo(3));
+        assertThat(resultMap.get("field1"), equalTo("value1"));
+        assertThat(((Map) resultMap.get("field2")).size(), equalTo(1));
+        assertThat(((Map) resultMap.get("field2")).get("field3"), equalTo("value3"));
+        assertThat(resultMap.get("field4"), equalTo("_value"));
+    }
+
+    public void testAccessSourceViaTemplate() {
+        IngestDocument ingestDocument = new IngestDocument("marvel", "type", "id", null, null, null, null, new HashMap<>());
+        assertThat(ingestDocument.hasField("marvel"), is(false));
+        ingestDocument.setFieldValue(templateService.compile("{{_index}}"), ValueSource.wrap("{{_index}}", templateService));
+        assertThat(ingestDocument.getFieldValue("marvel", String.class), equalTo("marvel"));
+        ingestDocument.removeField(templateService.compile("{{marvel}}"));
+        assertThat(ingestDocument.hasField("index"), is(false));
+    }
+
+}
diff --git a/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
new file mode 100644
index 0000000..73f64d4
--- /dev/null
+++ b/qa/ingest-with-mustache/src/test/java/org/elasticsearch/smoketest/IngestWithMustacheIT.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.smoketest;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+
+public class IngestWithMustacheIT extends ESRestTestCase {
+
+    public IngestWithMustacheIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+
+}
diff --git a/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml b/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
new file mode 100644
index 0000000..9e64477
--- /dev/null
+++ b/qa/ingest-with-mustache/src/test/resources/rest-api-spec/test/ingest_mustache/10_pipeline_with_mustache_templates.yaml
@@ -0,0 +1,220 @@
+---
+"Test metadata templating":
+  - do:
+      cluster.health:
+          wait_for_status: green
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline_1"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "index_type_id",
+                  "value": "{{_index}}/{{_type}}/{{_id}}"
+                }
+              },
+              {
+                "append" : {
+                  "field" : "metadata",
+                  "value": ["{{_index}}", "{{_type}}", "{{_id}}"]
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_1"
+        body: {}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 2 }
+  - match: { _source.index_type_id: "test/test/1" }
+  - match: { _source.metadata: ["test", "test", "1"] }
+
+---
+"Test templating":
+  - do:
+      cluster.health:
+          wait_for_status: green
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline_1"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field4",
+                  "value": "{{field1}}/{{field2}}/{{field3}}"
+                }
+              },
+              {
+                "append" : {
+                  "field" : "metadata",
+                  "value": ["{{field1}}", "{{field2}}", "{{field3}}"]
+                }
+              }
+
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline_2"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "{{field1}}",
+                  "value": "value"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline_3"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "remove" : {
+                  "field" : "{{field_to_remove}}"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_1"
+        body: {
+          metadata: "0",
+          field1: "1",
+          field2: "2",
+          field3: "3"
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 5 }
+  - match: { _source.field1: "1" }
+  - match: { _source.field2: "2" }
+  - match: { _source.field3: "3" }
+  - match: { _source.field4: "1/2/3" }
+  - match: { _source.metadata: ["0","1","2","3"] }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_2"
+        body: {
+          field1: "field2"
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 2 }
+  - match: { _source.field1: "field2" }
+  - match: { _source.field2: "value" }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline_3"
+        body: {
+          field_to_remove: "field2",
+          field2: "2",
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 1 }
+  - match: { _source.field_to_remove: "field2" }
+
+---
+"Test on_failure metadata context templating":
+  - do:
+      cluster.health:
+          wait_for_status: green
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_handled_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "remove" : {
+                  "field" : "field_to_remove",
+                  "on_failure" : [
+                    {
+                      "set" : {
+                        "field" : "error",
+                        "value" : "processor [{{ _ingest.on_failure_processor }}]: {{ _ingest.on_failure_message }}"
+                      }
+                    }
+                  ]
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_handled_pipeline"
+        body: {
+          do_nothing: "foo",
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - length: { _source: 2 }
+  - match: { _source.do_nothing: "foo" }
+  - match: { _source.error: "processor [remove]: field [field_to_remove] not present as part of path [field_to_remove]" }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json b/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
index 577a03f..590054b 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/bulk.json
@@ -40,6 +40,10 @@
         "fields": {
           "type": "list",
           "description" : "Default comma-separated list of fields to return in the response for updates"
+        },
+        "pipeline" : {
+          "type" : "string",
+          "description" : "The pipeline id to preprocess incoming documents with"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/index.json b/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
index 1b8f714..5c13f67 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/index.json
@@ -65,6 +65,10 @@
           "type" : "enum",
           "options" : ["internal", "external", "external_gte", "force"],
           "description" : "Specific version type"
+        },
+        "pipeline" : {
+          "type" : "string",
+          "description" : "The pipeline id to preprocess incoming documents with"
         }
       }
     },
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json
new file mode 100644
index 0000000..1c515e4
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.delete_pipeline.json
@@ -0,0 +1,28 @@
+{
+  "ingest.delete_pipeline": {
+    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
+    "methods": [ "DELETE" ],
+    "url": {
+      "path": "/_ingest/pipeline/{id}",
+      "paths": [ "/_ingest/pipeline/{id}" ],
+      "parts": {
+        "id": {
+          "type" : "string",
+          "description" : "Pipeline ID",
+          "required" : true
+        }
+      },
+      "params": {
+        "master_timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout for connection to master node"
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
+        }
+      }
+    },
+    "body": null
+  }
+}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json
new file mode 100644
index 0000000..6c50657
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.get_pipeline.json
@@ -0,0 +1,24 @@
+{
+  "ingest.get_pipeline": {
+    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
+    "methods": [ "GET" ],
+    "url": {
+      "path": "/_ingest/pipeline/{id}",
+      "paths": [ "/_ingest/pipeline/{id}" ],
+      "parts": {
+        "id": {
+          "type" : "string",
+          "description" : "Comma separated list of pipeline ids. Wildcards supported",
+          "required" : true
+        }
+      },
+      "params": {
+        "master_timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout for connection to master node"
+        }
+      }
+    },
+    "body": null
+  }
+}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json
new file mode 100644
index 0000000..e4c3c2e
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.put_pipeline.json
@@ -0,0 +1,31 @@
+{
+  "ingest.put_pipeline": {
+    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
+    "methods": [ "PUT" ],
+    "url": {
+      "path": "/_ingest/pipeline/{id}",
+      "paths": [ "/_ingest/pipeline/{id}" ],
+      "parts": {
+        "id": {
+          "type" : "string",
+          "description" : "Pipeline ID",
+          "required" : true
+        }
+      },
+      "params": {
+        "master_timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout for connection to master node"
+        },
+        "timeout": {
+          "type" : "time",
+          "description" : "Explicit operation timeout"
+        }
+      }
+    },
+    "body": {
+      "description" : "The ingest definition",
+      "required" : true
+    }    
+  }
+}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json
new file mode 100644
index 0000000..a4904ce
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/ingest.simulate.json
@@ -0,0 +1,28 @@
+{
+  "ingest.simulate": {
+    "documentation": "https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest.html",
+    "methods": [ "GET", "POST" ],
+    "url": {
+      "path": "/_ingest/pipeline/_simulate",
+      "paths": [ "/_ingest/pipeline/_simulate", "/_ingest/pipeline/{id}/_simulate/" ],
+      "parts": {
+        "id": {
+          "type" : "string",
+          "description" : "Pipeline ID",
+          "required" : false
+        }
+      },
+      "params": {
+        "verbose": {
+          "type" : "boolean",
+          "description" : "Verbose mode. Display data output for each processor in executed pipeline",
+          "default" : false
+        }
+      }
+    },
+    "body": {
+      "description" : "The simulate definition",
+      "required" : true
+    }    
+  }
+}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
new file mode 100644
index 0000000..bf0817f
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/10_crud.yaml
@@ -0,0 +1,94 @@
+---
+"Test basic pipeline crud":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field2",
+                  "value": "_value"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.get_pipeline:
+        id: "my_pipeline"
+  - match: { pipelines.0.id: "my_pipeline" }
+  - match: { pipelines.0.config.description: "_description" }
+
+  - do:
+      ingest.delete_pipeline:
+        id: "my_pipeline"
+  - match: { acknowledged: true }
+
+  - do:
+      catch: missing
+      ingest.get_pipeline:
+        id: "my_pipeline"
+
+---
+"Test invalid config":
+  - do:
+      catch: param
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                }
+              }
+            ]
+          }
+
+---
+"Test basic pipeline with on_failure in processor":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field2",
+                  "value": "_value",
+                  "on_failure": [
+                    {
+                      "set" : {
+                        "field" : "field2",
+                        "value" : "_failed_value"
+                      }
+                    }
+                  ]
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.get_pipeline:
+        id: "my_pipeline"
+  - match: { pipelines.0.id: "my_pipeline" }
+  - match: { pipelines.0.config.description: "_description" }
+
+  - do:
+      ingest.delete_pipeline:
+        id: "my_pipeline"
+  - match: { acknowledged: true }
+
+  - do:
+      catch: missing
+      ingest.get_pipeline:
+        id: "my_pipeline"
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml
new file mode 100644
index 0000000..71c5c40
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/20_date_processor.yaml
@@ -0,0 +1,37 @@
+---
+"Test date processor":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "date" : {
+                  "match_field" : "date_source_field",
+                  "target_field" : "date_target_field",
+                  "match_formats" : ["dd/MM/yyyy"],
+                  "timezone" : "Europe/Amsterdam"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {date_source_field: "12/06/2010"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.date_source_field: "12/06/2010" }
+  - match: { _source.date_target_field: "2010-06-12T00:00:00.000+02:00" }
+
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml
new file mode 100644
index 0000000..1e7911e
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/30_mutate.yaml
@@ -0,0 +1,150 @@
+---
+"Test mutate processors":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "new_field",
+                  "value": "new_value"
+                }
+              },
+              {
+                "append" : {
+                  "field" : "new_field",
+                  "value": ["item2", "item3", "item4"]
+                }
+              },
+              {
+                "rename" : {
+                  "field" : "field_to_rename",
+                  "to": "renamed_field"
+                }
+              },
+              {
+                "remove" : {
+                  "field" : "field_to_remove"
+                }
+              },
+              {
+                "lowercase" : {
+                  "field" : "field_to_lowercase"
+                }
+              },
+              {
+                "uppercase" : {
+                  "field" : "field_to_uppercase"
+                }
+              },
+              {
+                "trim" : {
+                  "field" : "field_to_trim"
+                }
+              },
+              {
+                "split" : {
+                  "field" : "field_to_split",
+                  "separator": "-"
+                }
+              },
+              {
+                "join" : {
+                  "field" : "field_to_join",
+                  "separator": "-"
+                }
+              },
+              {
+                "convert" : {
+                  "field" : "field_to_convert",
+                  "type": "integer"
+                }
+              },
+              {
+                "gsub" : {
+                  "field": "field_to_gsub",
+                  "pattern" : "-",
+                  "replacement" : "."
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {
+          field_to_rename: "value",
+          field_to_remove: "old_value",
+          field_to_lowercase: "LOWERCASE",
+          field_to_uppercase: "uppercase",
+          field_to_trim: "   trimmed   ",
+          field_to_split: "127-0-0-1",
+          field_to_join: ["127","0","0","1"],
+          field_to_convert: ["127","0","0","1"],
+          field_to_gsub: "127-0-0-1"
+        }
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - is_false: _source.field_to_rename
+  - is_false: _source.field_to_remove
+  - match: { _source.new_field: ["new_value", "item2", "item3", "item4"] }
+  - match: { _source.renamed_field: "value" }
+  - match: { _source.field_to_lowercase: "lowercase" }
+  - match: { _source.field_to_uppercase: "UPPERCASE" }
+  - match: { _source.field_to_trim: "trimmed" }
+  - match: { _source.field_to_split: ["127","0","0","1"] }
+  - match: { _source.field_to_join: "127-0-0-1" }
+  - match: { _source.field_to_convert: [127,0,0,1] }
+  - match: { _source.field_to_gsub: "127.0.0.1" }
+
+---
+"Test metadata":
+  - do:
+      cluster.health:
+          wait_for_status: green
+
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "_index",
+                  "value" : "surprise"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field: "value"}
+
+  - do:
+      get:
+        index: surprise
+        type: test
+        id: 1
+  - length: { _source: 1 }
+  - match: { _source.field: "value" }
+
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
new file mode 100644
index 0000000..3153ba8
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/40_simulate.yaml
@@ -0,0 +1,421 @@
+---
+"Test simulate with stored ingest pipeline":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field2",
+                  "value" : "_value"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      ingest.simulate:
+        id: "my_pipeline"
+        body: >
+          {
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { docs: 1 }
+  - match: { docs.0.doc._source.foo: "bar" }
+  - match: { docs.0.doc._source.field2: "_value" }
+  - length: { docs.0.doc._ingest: 1 }
+  - is_true: docs.0.doc._ingest.timestamp
+
+---
+"Test simulate with provided pipeline definition":
+  - do:
+      ingest.simulate:
+        body: >
+          {
+            "pipeline": {
+              "description": "_description",
+              "processors": [
+                {
+                  "set" : {
+                    "field" : "field2",
+                    "value" : "_value"
+                  }
+                }
+              ]
+            },
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { docs: 1 }
+
+---
+"Test simulate with provided invalid pipeline definition":
+  - do:
+      catch: request
+      ingest.simulate:
+        body: >
+          {
+            "pipeline": {
+              "description": "_description",
+              "processors": [
+                {
+                  "set" : {
+                    "value" : "_value"
+                  }
+                }
+              ]
+            },
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { error: 3 }
+  - match: { status: 400 }
+  - match: { error.type: "illegal_argument_exception" }
+  - match: { error.reason: "required property [field] is missing" }
+
+---
+"Test simulate without index type and id":
+  - do:
+      ingest.simulate:
+        body: >
+          {
+            "pipeline": {
+              "description": "_description",
+              "processors": [
+                {
+                  "set" : {
+                    "field" : "field2",
+                    "value" : "_value"
+                  }
+                }
+              ]
+            },
+            "docs": [
+              {
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { docs: 1 }
+
+---
+"Test simulate with provided pipeline definition with on_failure block":
+  - do:
+      ingest.simulate:
+        body: >
+          {
+            "pipeline": {
+              "description": "_description",
+              "processors": [
+                {
+                  "rename" : {
+                    "field" : "does_not_exist",
+                    "to" : "field2",
+                    "on_failure" : [
+                      {
+                        "set" : {
+                          "field" : "field2",
+                          "value" : "_value"
+                        }
+                      }
+                    ]
+                  }
+                }
+              ]
+            },
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { docs: 1 }
+  - match: { docs.0.doc._source.foo: "bar" }
+  - match: { docs.0.doc._source.field2: "_value" }
+  - length: { docs.0.doc._ingest: 1 }
+  - is_true: docs.0.doc._ingest.timestamp
+
+---
+"Test simulate with no provided pipeline or pipeline_id":
+  - do:
+      catch: request
+      ingest.simulate:
+        body: >
+          {
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { error: 3 }
+  - match: { status: 400 }
+  - match: { error.type: "illegal_argument_exception" }
+  - match: { error.reason: "required property [pipeline] is missing" }
+
+---
+"Test simulate with verbose flag":
+  - do:
+      ingest.simulate:
+        verbose: true
+        body: >
+          {
+            "pipeline": {
+              "description": "_description",
+              "processors": [
+                {
+                  "set" : {
+                    "tag" : "processor[set]-0",
+                    "field" : "field2",
+                    "value" : "_value"
+                  }
+                },
+                {
+                  "set" : {
+                    "field" : "field3",
+                    "value" : "third_val"
+                  }
+                }
+              ]
+            },
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { docs: 1 }
+  - length: { docs.0.processor_results: 2 }
+  - match: { docs.0.processor_results.0.tag: "processor[set]-0" }
+  - length: { docs.0.processor_results.0.doc._source: 2 }
+  - match: { docs.0.processor_results.0.doc._source.foo: "bar" }
+  - match: { docs.0.processor_results.0.doc._source.field2: "_value" }
+  - length: { docs.0.processor_results.0.doc._ingest: 1 }
+  - is_true: docs.0.processor_results.0.doc._ingest.timestamp
+  - length: { docs.0.processor_results.1.doc._source: 3 }
+  - match: { docs.0.processor_results.1.doc._source.foo: "bar" }
+  - match: { docs.0.processor_results.1.doc._source.field2: "_value" }
+  - match: { docs.0.processor_results.1.doc._source.field3: "third_val" }
+  - length: { docs.0.processor_results.1.doc._ingest: 1 }
+  - is_true: docs.0.processor_results.1.doc._ingest.timestamp
+
+---
+"Test simulate with exception thrown":
+  - do:
+      ingest.simulate:
+        body: >
+          {
+            "pipeline": {
+              "description": "_description",
+              "processors": [
+                {
+                  "uppercase" : {
+                    "field" : "foo"
+                  }
+                }
+              ]
+            },
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "not_foo": "bar"
+                }
+              },
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id2",
+                "_source": {
+                  "foo": "bar"
+                }
+              }
+            ]
+          }
+  - length: { docs: 2 }
+  - match: { docs.0.error.type: "illegal_argument_exception" }
+  - match: { docs.1.doc._source.foo: "BAR" }
+  - length: { docs.1.doc._ingest: 1 }
+  - is_true: docs.1.doc._ingest.timestamp
+
+---
+"Test verbose simulate with exception thrown":
+  - do:
+      ingest.simulate:
+        verbose: true
+        body: >
+          {
+            "pipeline": {
+              "description": "_description",
+              "processors": [
+                {
+                  "convert" : {
+                    "field" : "foo",
+                    "type" : "integer"
+                  }
+                },
+                {
+                  "uppercase" : {
+                    "field" : "bar"
+                  }
+                }
+              ]
+            },
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "foo": "bar",
+                  "bar": "hello"
+                }
+              },
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id2",
+                "_source": {
+                  "foo": "5",
+                  "bar": "hello"
+                }
+              }
+            ]
+          }
+  - length: { docs: 2 }
+  - length: { docs.0.processor_results: 1 }
+  - match: { docs.0.processor_results.0.error.type: "illegal_argument_exception" }
+  - length: { docs.1.processor_results: 2 }
+  - match: { docs.1.processor_results.0.doc._index: "index" }
+  - match: { docs.1.processor_results.0.doc._source.foo: 5 }
+  - match: { docs.1.processor_results.0.doc._source.bar: "hello" }
+  - length: { docs.1.processor_results.0.doc._ingest: 1 }
+  - is_true: docs.1.processor_results.0.doc._ingest.timestamp
+  - match: { docs.1.processor_results.1.doc._source.foo: 5 }
+  - match: { docs.1.processor_results.1.doc._source.bar: "HELLO" }
+  - length: { docs.1.processor_results.1.doc._ingest: 1 }
+  - is_true: docs.1.processor_results.1.doc._ingest.timestamp
+
+---
+"Test verbose simulate with on_failure":
+  - do:
+      ingest.simulate:
+        verbose: true
+        body: >
+          {
+            "pipeline" : {
+              "description": "_description",
+              "processors": [
+                {
+                  "set" : {
+                    "tag" : "setstatus-1",
+                    "field" : "status",
+                    "value" : 200
+                  }
+                },
+                {
+                  "rename" : {
+                    "tag" : "rename-1",
+                    "field" : "foofield",
+                    "to" : "field1",
+                    "on_failure" : [
+                      {
+                        "set" : {
+                          "tag" : "set on_failure rename",
+                          "field" : "foofield",
+                          "value" : "exists"
+                        }
+                      },
+                      {
+                        "rename" : {
+                          "field" : "foofield2",
+                          "to" : "field1",
+                          "on_failure" : [
+                            {
+                              "set" : {
+                                "field" : "foofield2",
+                                "value" : "ran"
+                              }
+                            }
+                          ]
+                        }
+                      }
+                    ]
+                  }
+                }
+              ]
+            },
+            "docs": [
+              {
+                "_index": "index",
+                "_type": "type",
+                "_id": "id",
+                "_source": {
+                  "field1": "123.42 400 <foo>"
+                }
+              }
+            ]
+          }
+  - length: { docs: 1 }
+  - length: { docs.0.processor_results: 5 }
+  - match: { docs.0.processor_results.0.tag: "setstatus-1" }
+  - match: { docs.0.processor_results.0.doc._source.field1: "123.42 400 <foo>" }
+  - match: { docs.0.processor_results.0.doc._source.status: 200 }
+  - match: { docs.0.processor_results.1.tag: "rename-1" }
+  - match: { docs.0.processor_results.1.error.type: "illegal_argument_exception" }
+  - match: { docs.0.processor_results.1.error.reason: "field [foofield] doesn't exist" }
+  - match: { docs.0.processor_results.2.tag: "set on_failure rename" }
+  - is_false: docs.0.processor_results.3.tag
+  - is_false: docs.0.processor_results.4.tag
+  - match: { docs.0.processor_results.4.doc._source.foofield: "exists" }
+  - match: { docs.0.processor_results.4.doc._source.foofield2: "ran" }
+  - match: { docs.0.processor_results.4.doc._source.field1: "123.42 400 <foo>" }
+  - match: { docs.0.processor_results.4.doc._source.status: 200 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml
new file mode 100644
index 0000000..7bce12d
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/50_on_failure.yaml
@@ -0,0 +1,108 @@
+---
+"Test Pipeline With On Failure Block":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "_executed",
+                  "value" : true
+                }
+              },
+              {
+                "date" : {
+                  "match_field" : "date",
+                  "target_field" : "date",
+                  "match_formats" : ["yyyy"]
+                }
+              }
+            ],
+            "on_failure" : [
+              {
+                "set" : {
+                  "field" : "_failed",
+                  "value" : true
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field1: "value1"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.field1: "value1" }
+  - match: { _source._executed: true }
+  - match: { _source._failed: true }
+
+---
+"Test Pipeline With Nested Processor On Failures":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "rename" : {
+                  "field" : "foofield",
+                  "to" : "field1",
+                  "on_failure" : [
+                    {
+                      "set" : {
+                        "field" : "foofield",
+                        "value" : "exists"
+                      }
+                    },
+                    {
+                      "rename" : {
+                        "field" : "foofield2",
+                        "to" : "field1",
+                        "on_failure" : [
+                          {
+                            "set" : {
+                            "field" : "foofield2",
+                            "value" : "ran"
+                            }
+                          }
+                        ]
+                      }
+                    }
+                  ]
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {field1: "value1"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.field1: "value1" }
+  - match: { _source.foofield: "exists" }
+  - match: { _source.foofield2: "ran" }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml
new file mode 100644
index 0000000..019c229
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/60_fail.yaml
@@ -0,0 +1,68 @@
+---
+"Test Fail Processor":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "fail" : {
+                  "message" : "error_message"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      catch: request
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {}
+
+---
+"Test fail with on_failure":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "fail" : {
+                  "message" : "error",
+                  "on_failure" : [
+                    {
+                      "set" : {
+                        "field" : "error_message",
+                        "value" : "fail_processor_ran"
+                      }
+                    }
+                  ]
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.error_message: "fail_processor_ran" }
+
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml
new file mode 100644
index 0000000..b70f05a
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/70_bulk.yaml
@@ -0,0 +1,105 @@
+setup:
+  - do:
+      ingest.put_pipeline:
+        id: "pipeline1"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field1",
+                  "value": "value1"
+                }
+              }
+            ]
+          }
+
+  - do:
+      ingest.put_pipeline:
+        id: "pipeline2"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "set" : {
+                  "field" : "field2",
+                  "value": "value2"
+                }
+              }
+            ]
+          }
+
+---
+"Test bulk request without default pipeline":
+
+  - do:
+      bulk:
+        body:
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id1
+              pipeline: pipeline1
+          - f1: v1
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id2
+          - f1: v2
+
+  - do:
+      get:
+        index: test_index
+        type: test_type
+        id: test_id1
+
+  - match: {_source.field1: value1}
+  - is_false: _source.field2
+
+  - do:
+      get:
+        index: test_index
+        type: test_type
+        id: test_id2
+
+  - is_false: _source.field1
+  - is_false: _source.field2
+
+---
+"Test bulk request with default pipeline":
+
+  - do:
+      bulk:
+        pipeline: pipeline1
+        body:
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id1
+          - f1: v1
+          - index:
+              _index: test_index
+              _type:  test_type
+              _id:    test_id2
+              pipeline: pipeline2
+          - f1: v2
+  - do:
+      get:
+        index: test_index
+        type: test_type
+        id: test_id1
+
+  - match: {_source.field1: value1}
+  - is_false: _source.field2
+
+  - do:
+      get:
+        index: test_index
+        type: test_type
+        id: test_id2
+
+  - is_false: _source.field1
+  - match: {_source.field2: value2}
+
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml
new file mode 100644
index 0000000..bdc6457
--- /dev/null
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/ingest/80_dedot_processor.yaml
@@ -0,0 +1,64 @@
+---
+"Test De-Dot Processor With Provided Separator":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "dedot" : {
+                  "separator" : "3"
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {"a.b.c": "hello world"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.a3b3c: "hello world" }
+
+---
+"Test De-Dot Processor With Default Separator":
+  - do:
+      ingest.put_pipeline:
+        id: "my_pipeline"
+        body:  >
+          {
+            "description": "_description",
+            "processors": [
+              {
+                "dedot" : {
+                }
+              }
+            ]
+          }
+  - match: { acknowledged: true }
+
+  - do:
+      index:
+        index: test
+        type: test
+        id: 1
+        pipeline: "my_pipeline"
+        body: {"a.b.c": "hello world"}
+
+  - do:
+      get:
+        index: test
+        type: test
+        id: 1
+  - match: { _source.a_b_c: "hello world" }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/percolate/18_highligh_with_query.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/percolate/18_highligh_with_query.yaml
index d7e1fbd..97d6523 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/percolate/18_highligh_with_query.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/percolate/18_highligh_with_query.yaml
@@ -27,7 +27,7 @@
   - do:
       percolate:
         index: test_index
-        type:  test_type
+        type:  type_1
         body:
           doc:
               foo: "bar foo"
diff --git a/settings.gradle b/settings.gradle
index 682bfec..f6dab0f 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -11,6 +11,7 @@ List projects = [
   'test:framework',
   'test:fixtures:example-fixture',
   'test:fixtures:hdfs-fixture',
+  'modules:ingest-grok',
   'modules:lang-expression',
   'modules:lang-groovy',
   'modules:lang-mustache',
@@ -24,6 +25,7 @@ List projects = [
   'plugins:discovery-ec2',
   'plugins:discovery-gce',
   'plugins:discovery-multicast',
+  'plugins:ingest-geoip',
   'plugins:lang-javascript',
   'plugins:lang-plan-a',
   'plugins:lang-python',
@@ -39,6 +41,8 @@ List projects = [
   'qa:smoke-test-client',
   'qa:smoke-test-multinode',
   'qa:smoke-test-plugins',
+  'qa:ingest-with-mustache',
+  'qa:ingest-disabled',
   'qa:vagrant',
 ]
 
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java b/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java
new file mode 100644
index 0000000..3f350cf
--- /dev/null
+++ b/test/framework/src/main/java/org/elasticsearch/ingest/RandomDocumentPicks.java
@@ -0,0 +1,238 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import com.carrotsearch.randomizedtesting.generators.RandomInts;
+import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+import com.carrotsearch.randomizedtesting.generators.RandomStrings;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.ingest.core.IngestDocument;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.TreeMap;
+
+public final class RandomDocumentPicks {
+
+    private RandomDocumentPicks() {
+
+    }
+
+    /**
+     * Returns a random field name. Can be a leaf field name or the
+     * path to refer to a field name using the dot notation.
+     */
+    public static String randomFieldName(Random random) {
+        int numLevels = RandomInts.randomIntBetween(random, 1, 5);
+        String fieldName = "";
+        for (int i = 0; i < numLevels; i++) {
+            if (i > 0) {
+                fieldName += ".";
+            }
+            fieldName += randomString(random);
+        }
+        return fieldName;
+    }
+
+    /**
+     * Returns a random leaf field name.
+     */
+    public static String randomLeafFieldName(Random random) {
+        String fieldName;
+        do {
+            fieldName = randomString(random);
+        } while (fieldName.contains("."));
+        return fieldName;
+    }
+
+    /**
+     * Returns a randomly selected existing field name out of the fields that are contained
+     * in the document provided as an argument.
+     */
+    public static String randomExistingFieldName(Random random, IngestDocument ingestDocument) {
+        Map<String, Object> source = new TreeMap<>(ingestDocument.getSourceAndMetadata());
+        Map.Entry<String, Object> randomEntry = RandomPicks.randomFrom(random, source.entrySet());
+        String key = randomEntry.getKey();
+        while (randomEntry.getValue() instanceof Map) {
+            @SuppressWarnings("unchecked")
+            Map<String, Object> map = (Map<String, Object>) randomEntry.getValue();
+            Map<String, Object> treeMap = new TreeMap<>(map);
+            randomEntry = RandomPicks.randomFrom(random, treeMap.entrySet());
+            key += "." + randomEntry.getKey();
+        }
+        assert ingestDocument.getFieldValue(key, Object.class) != null;
+        return key;
+    }
+
+    /**
+     * Adds a random non existing field to the provided document and associates it
+     * with the provided value. The field will be added at a random position within the document,
+     * not necessarily at the top level using a leaf field name.
+     */
+    public static String addRandomField(Random random, IngestDocument ingestDocument, Object value) {
+        String fieldName;
+        do {
+            fieldName = randomFieldName(random);
+        } while (canAddField(fieldName, ingestDocument) == false);
+        ingestDocument.setFieldValue(fieldName, value);
+        return fieldName;
+    }
+
+    /**
+     * Checks whether the provided field name can be safely added to the provided document.
+     * When the provided field name holds the path using the dot notation, we have to make sure
+     * that each node of the tree either doesn't exist or is a map, otherwise new fields cannot be added.
+     */
+    public static boolean canAddField(String path, IngestDocument ingestDocument) {
+        String[] pathElements = Strings.splitStringToArray(path, '.');
+        Map<String, Object> innerMap = ingestDocument.getSourceAndMetadata();
+        if (pathElements.length > 1) {
+            for (int i = 0; i < pathElements.length - 1; i++) {
+                Object currentLevel = innerMap.get(pathElements[i]);
+                if (currentLevel == null) {
+                    return true;
+                }
+                if (currentLevel instanceof Map == false) {
+                    return false;
+                }
+                @SuppressWarnings("unchecked")
+                Map<String, Object> map = (Map<String, Object>) currentLevel;
+                innerMap = map;
+            }
+        }
+        String leafKey = pathElements[pathElements.length - 1];
+        return innerMap.containsKey(leafKey) == false;
+    }
+
+    /**
+     * Generates a random document and random metadata
+     */
+    public static IngestDocument randomIngestDocument(Random random) {
+        return randomIngestDocument(random, randomSource(random));
+    }
+
+    /**
+     * Generates a document that holds random metadata and the document provided as a map argument
+     */
+    public static IngestDocument randomIngestDocument(Random random, Map<String, Object> source) {
+        String index = randomString(random);
+        String type = randomString(random);
+        String id = randomString(random);
+        String routing = null;
+        if (random.nextBoolean()) {
+            routing = randomString(random);
+        }
+        String parent = null;
+        if (random.nextBoolean()) {
+            parent = randomString(random);
+        }
+        String timestamp = null;
+        if (random.nextBoolean()) {
+            timestamp = randomString(random);
+        }
+        String ttl = null;
+        if (random.nextBoolean()) {
+            ttl = randomString(random);
+        }
+        return new IngestDocument(index, type, id, routing, parent, timestamp, ttl, source);
+    }
+
+    public static Map<String, Object> randomSource(Random random) {
+        Map<String, Object> document = new HashMap<>();
+        addRandomFields(random, document, 0);
+        return document;
+    }
+
+    /**
+     * Generates a random field value, can be a string, a number, a list of an object itself.
+     */
+    public static Object randomFieldValue(Random random) {
+        return randomFieldValue(random, 0);
+    }
+
+    private static Object randomFieldValue(Random random, int currentDepth) {
+        switch(RandomInts.randomIntBetween(random, 0, 8)) {
+            case 0:
+                return randomString(random);
+            case 1:
+                return random.nextInt();
+            case 2:
+                return random.nextBoolean();
+            case 3:
+                return random.nextDouble();
+            case 4:
+                List<String> stringList = new ArrayList<>();
+                int numStringItems = RandomInts.randomIntBetween(random, 1, 10);
+                for (int j = 0; j < numStringItems; j++) {
+                    stringList.add(randomString(random));
+                }
+                return stringList;
+            case 5:
+                List<Integer> intList = new ArrayList<>();
+                int numIntItems = RandomInts.randomIntBetween(random, 1, 10);
+                for (int j = 0; j < numIntItems; j++) {
+                    intList.add(random.nextInt());
+                }
+                return intList;
+            case 6:
+                List<Boolean> booleanList = new ArrayList<>();
+                int numBooleanItems = RandomInts.randomIntBetween(random, 1, 10);
+                for (int j = 0; j < numBooleanItems; j++) {
+                    booleanList.add(random.nextBoolean());
+                }
+                return booleanList;
+            case 7:
+                List<Double> doubleList = new ArrayList<>();
+                int numDoubleItems = RandomInts.randomIntBetween(random, 1, 10);
+                for (int j = 0; j < numDoubleItems; j++) {
+                    doubleList.add(random.nextDouble());
+                }
+                return doubleList;
+            case 8:
+                Map<String, Object> newNode = new HashMap<>();
+                addRandomFields(random, newNode, ++currentDepth);
+                return newNode;
+            default:
+                throw new UnsupportedOperationException();
+        }
+    }
+
+    public static String randomString(Random random) {
+        if (random.nextBoolean()) {
+            return RandomStrings.randomAsciiOfLengthBetween(random, 1, 10);
+        }
+        return RandomStrings.randomUnicodeOfCodepointLengthBetween(random, 1, 10);
+    }
+
+    private static void addRandomFields(Random random, Map<String, Object> parentNode, int currentDepth) {
+        if (currentDepth > 5) {
+            return;
+        }
+        int numFields = RandomInts.randomIntBetween(random, 1, 10);
+        for (int i = 0; i < numFields; i++) {
+            String fieldName = randomLeafFieldName(random);
+            Object fieldValue = randomFieldValue(random, currentDepth);
+            parentNode.put(fieldName, fieldValue);
+        }
+    }
+}
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java b/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java
new file mode 100644
index 0000000..ae13174
--- /dev/null
+++ b/test/framework/src/main/java/org/elasticsearch/ingest/TestProcessor.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.AbstractProcessorFactory;
+import org.elasticsearch.ingest.core.IngestDocument;
+import org.elasticsearch.ingest.core.Processor;
+
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.function.Consumer;
+
+/**
+ * Processor used for testing, keeps track of how many times it is invoked and
+ * accepts a {@link Consumer} of {@link IngestDocument} to be called when executed.
+ */
+public class TestProcessor implements Processor {
+
+    private final String type;
+    private final String tag;
+    private final Consumer<IngestDocument> ingestDocumentConsumer;
+    private final AtomicInteger invokedCounter = new AtomicInteger();
+
+    public TestProcessor(Consumer<IngestDocument> ingestDocumentConsumer) {
+        this(null, "test-processor", ingestDocumentConsumer);
+    }
+
+    public TestProcessor(String tag, String type, Consumer<IngestDocument> ingestDocumentConsumer) {
+        this.ingestDocumentConsumer = ingestDocumentConsumer;
+        this.type = type;
+        this.tag = tag;
+    }
+
+    @Override
+    public void execute(IngestDocument ingestDocument) throws Exception {
+        invokedCounter.incrementAndGet();
+        ingestDocumentConsumer.accept(ingestDocument);
+    }
+
+    @Override
+    public String getType() {
+        return type;
+    }
+
+    @Override
+    public String getTag() {
+        return tag;
+    }
+
+    public int getInvokedCounter() {
+        return invokedCounter.get();
+    }
+
+    public static final class Factory extends AbstractProcessorFactory<TestProcessor> {
+        @Override
+        public TestProcessor doCreate(String processorTag, Map<String, Object> config) throws Exception {
+            return new TestProcessor(processorTag, "test-processor", ingestDocument -> {});
+        }
+    }
+}
diff --git a/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java b/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java
new file mode 100644
index 0000000..9330db1
--- /dev/null
+++ b/test/framework/src/main/java/org/elasticsearch/ingest/TestTemplateService.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.ingest;
+
+import org.elasticsearch.ingest.core.TemplateService;
+
+import java.util.Map;
+
+public class TestTemplateService implements TemplateService {
+
+    public static TemplateService instance() {
+        return new TestTemplateService();
+    }
+
+    private TestTemplateService() {
+    }
+
+    @Override
+    public Template compile(String template) {
+        return new MockTemplate(template);
+    }
+
+    public static class MockTemplate implements TemplateService.Template {
+
+        private final String expected;
+
+        public MockTemplate(String expected) {
+            this.expected = expected;
+        }
+
+        @Override
+        public String execute(Map<String, Object> model) {
+            return expected;
+        }
+
+        @Override
+        public String getKey() {
+            return expected;
+        }
+    }
+}
diff --git a/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java b/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
index 1bc2ca0..796872b 100644
--- a/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
+++ b/test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java
@@ -18,12 +18,6 @@
  */
 package org.elasticsearch.test;
 
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Query;
@@ -52,7 +46,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.aggregations.SearchContextAggregations;
 import org.elasticsearch.search.dfs.DfsSearchResult;
-import org.elasticsearch.search.fetch.FetchPhase;
 import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.fetch.FetchSubPhaseContext;
@@ -71,7 +64,11 @@ import org.elasticsearch.search.rescore.RescoreSearchContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
 
 public class TestSearchContext extends SearchContext {
 
@@ -551,10 +548,6 @@ public class TestSearchContext extends SearchContext {
         return null;
     }
 
-    @Override
-    public FetchPhase fetchPhase() {
-        return null;
-    }
 
     @Override
     public MappedFieldType smartNameFieldType(String name) {
