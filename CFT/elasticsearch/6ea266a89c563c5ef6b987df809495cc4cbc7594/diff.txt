diff --git a/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/RandomizedTestingTask.groovy b/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/RandomizedTestingTask.groovy
index 33e16fe..ccb5d59 100644
--- a/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/RandomizedTestingTask.groovy
+++ b/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/RandomizedTestingTask.groovy
@@ -78,7 +78,7 @@ class RandomizedTestingTask extends DefaultTask {
     @Input
     String argLine = null
 
-    Map<String, String> systemProperties = new HashMap<>()
+    Map<String, Object> systemProperties = new HashMap<>()
     PatternFilterable patternSet = new PatternSet()
 
     RandomizedTestingTask() {
@@ -100,7 +100,7 @@ class RandomizedTestingTask extends DefaultTask {
         jvmArgs.add(argument)
     }
 
-    void systemProperty(String property, String value) {
+    void systemProperty(String property, Object value) {
         systemProperties.put(property, value)
     }
 
@@ -245,8 +245,8 @@ class RandomizedTestingTask extends DefaultTask {
                         exclude(name: excludePattern)
                     }
                 }
-                for (Map.Entry<String, String> prop : systemProperties) {
-                    sysproperty key: prop.getKey(), value: prop.getValue()
+                for (Map.Entry<String, Object> prop : systemProperties) {
+                    sysproperty key: prop.getKey(), value: prop.getValue().toString()
                 }
                 makeListeners()
             }
diff --git a/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestReportLogger.groovy b/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestReportLogger.groovy
index b56a22e..0813713 100644
--- a/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestReportLogger.groovy
+++ b/buildSrc/src/main/groovy/com/carrotsearch/gradle/junit4/TestReportLogger.groovy
@@ -15,8 +15,15 @@ import org.gradle.api.logging.LogLevel
 import org.gradle.api.logging.Logger
 import org.junit.runner.Description
 
+import java.util.concurrent.atomic.AtomicBoolean
 import java.util.concurrent.atomic.AtomicInteger
 
+import javax.sound.sampled.AudioSystem;
+import javax.sound.sampled.Clip;
+import javax.sound.sampled.Line;
+import javax.sound.sampled.LineEvent;
+import javax.sound.sampled.LineListener;
+
 import static com.carrotsearch.ant.tasks.junit4.FormattingUtils.*
 import static com.carrotsearch.gradle.junit4.TestLoggingConfiguration.OutputMode
 
@@ -102,9 +109,36 @@ class TestReportLogger extends TestsSummaryEventListener implements AggregatedEv
                 formatTime(e.getCurrentTime()) + ", stalled for " +
                 formatDurationInSeconds(e.getNoEventDuration()) + " at: " +
                 (e.getDescription() == null ? "<unknown>" : formatDescription(e.getDescription())))
+        try {
+            playBeat();
+        } catch (Exception nosound) { /* handling exceptions with style */ }
         slowTestsFound = true
     }
 
+    void playBeat() throws Exception {
+        Clip clip = (Clip)AudioSystem.getLine(new Line.Info(Clip.class));
+        final AtomicBoolean stop = new AtomicBoolean();
+        clip.addLineListener(new LineListener() {
+            @Override
+            public void update(LineEvent event) {
+                if (event.getType() == LineEvent.Type.STOP) {
+                    stop.set(true);
+                }
+            }
+        });
+        InputStream stream = getClass().getResourceAsStream("/beat.wav");
+        try {
+            clip.open(AudioSystem.getAudioInputStream(stream));
+            clip.start();
+            while (!stop.get()) {
+                Thread.sleep(20);
+            }
+            clip.close();
+        } finally {
+            stream.close();
+        }
+    }
+
     @Subscribe
     void onQuit(AggregatedQuitEvent e) throws IOException {
         if (config.showNumFailuresAtEnd > 0 && !failedTests.isEmpty()) {
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
index eea2041..0d936ab 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy
@@ -65,7 +65,6 @@ public class PluginBuildPlugin extends BuildPlugin {
             // with a full elasticsearch server that includes optional deps
             provided "com.spatial4j:spatial4j:${project.versions.spatial4j}"
             provided "com.vividsolutions:jts:${project.versions.jts}"
-            provided "com.github.spullara.mustache.java:compiler:${project.versions.mustache}"
             provided "log4j:log4j:${project.versions.log4j}"
             provided "log4j:apache-log4j-extras:${project.versions.log4j}"
             provided "org.slf4j:slf4j-api:${project.versions.slf4j}"
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/precommit/ForbiddenPatternsTask.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/precommit/ForbiddenPatternsTask.groovy
index 5fa6395..6809adc 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/precommit/ForbiddenPatternsTask.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/precommit/ForbiddenPatternsTask.groovy
@@ -61,11 +61,14 @@ public class ForbiddenPatternsTask extends DefaultTask {
         // add mandatory rules
         patterns.put('nocommit', /nocommit/)
         patterns.put('tab', /\t/)
+
+        inputs.property("excludes", filesFilter.excludes)
+        inputs.property("rules", patterns)
     }
 
     /** Adds a file glob pattern to be excluded */
     public void exclude(String... excludes) {
-        this.filesFilter.exclude(excludes)
+        filesFilter.exclude(excludes)
     }
 
     /** Adds a pattern to forbid. T */
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterConfiguration.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterConfiguration.groovy
index 8bc80da..fa23299 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterConfiguration.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterConfiguration.groovy
@@ -33,10 +33,10 @@ class ClusterConfiguration {
     int numNodes = 1
 
     @Input
-    int baseHttpPort = 9400
+    int httpPort = 0
 
     @Input
-    int baseTransportPort = 9500
+    int transportPort = 0
 
     @Input
     boolean daemonize = true
@@ -55,7 +55,7 @@ class ClusterConfiguration {
     @Input
     Closure waitCondition = { NodeInfo node, AntBuilder ant ->
         File tmpFile = new File(node.cwd, 'wait.success')
-        ant.get(src: "http://localhost:${node.httpPort()}",
+        ant.get(src: "http://${node.httpUri()}",
                 dest: tmpFile.toString(),
                 ignoreerrors: true, // do not fail on error, so logging buffers can be flushed by the wait task
                 retries: 10)
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterFormationTasks.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterFormationTasks.groovy
index fff6082..08976db 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterFormationTasks.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterFormationTasks.groovy
@@ -38,8 +38,10 @@ class ClusterFormationTasks {
 
     /**
      * Adds dependent tasks to the given task to start and stop a cluster with the given configuration.
+     *
+     * Returns an object that will resolve at execution time of the given task to a uri for the cluster.
      */
-    static void setup(Project project, Task task, ClusterConfiguration config) {
+    static Object setup(Project project, Task task, ClusterConfiguration config) {
         if (task.getEnabled() == false) {
             // no need to add cluster formation tasks if the task won't run!
             return
@@ -55,6 +57,9 @@ class ClusterFormationTasks {
 
         Task wait = configureWaitTask("${task.name}#wait", project, nodes, startTasks)
         task.dependsOn(wait)
+
+        // delay the resolution of the uri by wrapping in a closure, so it is not used until read for tests
+        return "${-> nodes[0].transportUri()}"
     }
 
     /** Adds a dependency on the given distribution */
@@ -200,17 +205,24 @@ class ClusterFormationTasks {
     /** Adds a task to write elasticsearch.yml for the given node configuration */
     static Task configureWriteConfigTask(String name, Project project, Task setup, NodeInfo node) {
         Map esConfig = [
-            'cluster.name'                    : node.clusterName,
-            'http.port'                       : node.httpPort(),
-            'transport.tcp.port'              : node.transportPort(),
-            'pidfile'                         : node.pidFile,
-            'discovery.zen.ping.unicast.hosts': (0..<node.config.numNodes).collect{"127.0.0.1:${node.config.baseTransportPort + it}"}.join(','),
-            'path.repo'                       : "${node.homeDir}/repo",
-            'path.shared_data'                : "${node.homeDir}/../",
-            // Define a node attribute so we can test that it exists
-            'node.testattr'                   : 'test',
-            'repositories.url.allowed_urls'   : 'http://snapshot.test*'
+                'cluster.name'                 : node.clusterName,
+                'pidfile'                      : node.pidFile,
+                'path.repo'                    : "${node.homeDir}/repo",
+                'path.shared_data'             : "${node.homeDir}/../",
+                // Define a node attribute so we can test that it exists
+                'node.testattr'                : 'test',
+                'repositories.url.allowed_urls': 'http://snapshot.test*'
         ]
+        if (node.config.numNodes == 1) {
+            esConfig['http.port'] = node.config.httpPort
+            esConfig['transport.tcp.port'] =  node.config.transportPort
+        } else {
+            // TODO: fix multi node so it doesn't use hardcoded prots
+            esConfig['http.port'] = 9400 + node.nodeNum
+            esConfig['transport.tcp.port'] =  9500 + node.nodeNum
+            esConfig['discovery.zen.ping.unicast.hosts'] = (0..<node.config.numNodes).collect{"localhost:${9500 + it}"}.join(',')
+
+        }
         esConfig.putAll(node.config.settings)
 
         Task writeConfig = project.tasks.create(name: name, type: DefaultTask, dependsOn: setup)
@@ -400,7 +412,12 @@ class ClusterFormationTasks {
                             resourceexists {
                                 file(file: node.pidFile.toString())
                             }
-                            socket(server: '127.0.0.1', port: node.httpPort())
+                            resourceexists {
+                                file(file: node.httpPortsFile.toString())
+                            }
+                            resourceexists {
+                                file(file: node.transportPortsFile.toString())
+                            }
                         }
                     }
                 }
@@ -444,6 +461,8 @@ class ClusterFormationTasks {
             logger.error("|-----------------------------------------")
             logger.error("|  failure marker exists: ${node.failedMarker.exists()}")
             logger.error("|  pid file exists: ${node.pidFile.exists()}")
+            logger.error("|  http ports file exists: ${node.httpPortsFile.exists()}")
+            logger.error("|  transport ports file exists: ${node.transportPortsFile.exists()}")
             // the waitfor failed, so dump any output we got (if info logging this goes directly to stdout)
             logger.error("|\n|  [ant output]")
             node.buffer.toString('UTF-8').eachLine { line -> logger.error("|    ${line}") }
@@ -525,7 +544,7 @@ class ClusterFormationTasks {
         }
     }
 
-    static String pluginTaskName(String action, String name, String suffix) {
+    public static String pluginTaskName(String action, String name, String suffix) {
         // replace every dash followed by a character with just the uppercase character
         String camelName = name.replaceAll(/-(\w)/) { _, c -> c.toUpperCase(Locale.ROOT) }
         return action + camelName[0].toUpperCase(Locale.ROOT) + camelName.substring(1) + suffix
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/MessyTestPlugin.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/MessyTestPlugin.groovy
new file mode 100644
index 0000000..1cca2c5
--- /dev/null
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/MessyTestPlugin.groovy
@@ -0,0 +1,63 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.gradle.test
+
+import org.elasticsearch.gradle.plugin.PluginBuildPlugin
+import org.gradle.api.Project
+import org.gradle.api.artifacts.Dependency
+import org.gradle.api.artifacts.ProjectDependency
+import org.gradle.api.tasks.Copy
+
+/**
+ * A plugin to run messy tests, which are generally tests that depend on plugins.
+ *
+ * This plugin will add the same test configuration as standalone tests, except
+ * also add the plugin-metadata and properties files for each plugin project
+ * dependency.
+ */
+class MessyTestPlugin extends StandaloneTestPlugin {
+    @Override
+    public void apply(Project project) {
+        super.apply(project)
+
+        project.configurations.testCompile.dependencies.all { Dependency dep ->
+            // this closure is run every time a compile dependency is added
+            if (dep instanceof ProjectDependency && dep.dependencyProject.plugins.hasPlugin(PluginBuildPlugin)) {
+                project.gradle.projectsEvaluated {
+                    addPluginResources(project, dep.dependencyProject)
+                }
+            }
+        }
+    }
+
+    private static addPluginResources(Project project, Project pluginProject) {
+        String outputDir = "generated-resources/${pluginProject.name}"
+        String taskName = ClusterFormationTasks.pluginTaskName("copy", pluginProject.name, "Metadata")
+        Copy copyPluginMetadata = project.tasks.create(taskName, Copy.class)
+        copyPluginMetadata.into(outputDir)
+        copyPluginMetadata.from(pluginProject.tasks.pluginProperties)
+        copyPluginMetadata.from(pluginProject.file('src/main/plugin-metadata'))
+        project.sourceSets.test.output.dir(outputDir, builtBy: taskName)
+
+        // add each generated dir to the test classpath in IDEs
+        //project.eclipse.classpath.sourceSets = [project.sourceSets.test]
+        project.idea.module.singleEntryLibraries= ['TEST': [project.file(outputDir)]]
+    }
+}
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy
index 337eea3..b369d35 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy
@@ -43,6 +43,12 @@ class NodeInfo {
     /** the pid file the node will use */
     File pidFile
 
+    /** a file written by elasticsearch containing the ports of each bound address for http */
+    File httpPortsFile
+
+    /** a file written by elasticsearch containing the ports of each bound address for transport */
+    File transportPortsFile
+
     /** elasticsearch home dir */
     File homeDir
 
@@ -92,6 +98,10 @@ class NodeInfo {
         homeDir = homeDir(baseDir, config.distribution)
         confDir = confDir(baseDir, config.distribution)
         configFile = new File(confDir, 'elasticsearch.yml')
+        // even for rpm/deb, the logs are under home because we dont start with real services
+        File logsDir = new File(homeDir, 'logs')
+        httpPortsFile = new File(logsDir, 'http.ports')
+        transportPortsFile = new File(logsDir, 'transport.ports')
         cwd = new File(baseDir, "cwd")
         failedMarker = new File(cwd, 'run.failed')
         startLog = new File(cwd, 'run.log')
@@ -119,6 +129,7 @@ class NodeInfo {
             'JAVA_HOME' : project.javaHome,
             'ES_GC_OPTS': config.jvmArgs // we pass these with the undocumented gc opts so the argline can set gc, etc
         ]
+        args.add("-Des.tests.portsfile=true")
         args.addAll(config.systemProperties.collect { key, value -> "-D${key}=${value}" })
         for (Map.Entry<String, String> property : System.properties.entrySet()) {
             if (property.getKey().startsWith('es.')) {
@@ -159,14 +170,14 @@ class NodeInfo {
         wrapperScript.setText("\"${esScript}\" ${argsPasser} > run.log 2>&1 ${exitMarker}", 'UTF-8')
     }
 
-    /** Returns the http port for this node */
-    int httpPort() {
-        return config.baseHttpPort + nodeNum
+    /** Returns an address and port suitable for a uri to connect to this node over http */
+    String httpUri() {
+        return httpPortsFile.readLines("UTF-8").get(0)
     }
 
-    /** Returns the transport port for this node */
-    int transportPort() {
-        return config.baseTransportPort + nodeNum
+    /** Returns an address and port suitable for a uri to connect to this node over transport protocol */
+    String transportUri() {
+        return transportPortsFile.readLines("UTF-8").get(0)
     }
 
     /** Returns the directory elasticsearch home is contained in for the given distribution */
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy
index cd43cd2..24bd57a 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RestIntegTestTask.groovy
@@ -57,12 +57,12 @@ public class RestIntegTestTask extends RandomizedTestingTask {
         RestSpecHack.configureDependencies(project)
         project.afterEvaluate {
             dependsOn(RestSpecHack.configureTask(project, includePackaged))
-            systemProperty('tests.cluster', "localhost:${clusterConfig.baseTransportPort}")
         }
         // this must run after all projects have been configured, so we know any project
         // references can be accessed as a fully configured
         project.gradle.projectsEvaluated {
-            ClusterFormationTasks.setup(project, this, clusterConfig)
+            Object clusterUri = ClusterFormationTasks.setup(project, this, clusterConfig)
+            systemProperty('tests.cluster', clusterUri)
         }
     }
 
diff --git a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RunTask.groovy b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RunTask.groovy
index 3246921..842ef8c 100644
--- a/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RunTask.groovy
+++ b/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/RunTask.groovy
@@ -8,7 +8,7 @@ import org.gradle.util.ConfigureUtil
 
 public class RunTask extends DefaultTask {
 
-    ClusterConfiguration clusterConfig = new ClusterConfiguration(baseHttpPort: 9200, baseTransportPort: 9300, daemonize: false)
+    ClusterConfiguration clusterConfig = new ClusterConfiguration(httpPort: 9200, transportPort: 9300, daemonize: false)
 
     public RunTask() {
         description = "Runs elasticsearch with '${project.path}'"
diff --git a/buildSrc/src/main/resources/META-INF/gradle-plugins/elasticsearch.messy-test.properties b/buildSrc/src/main/resources/META-INF/gradle-plugins/elasticsearch.messy-test.properties
new file mode 100644
index 0000000..507a0f8
--- /dev/null
+++ b/buildSrc/src/main/resources/META-INF/gradle-plugins/elasticsearch.messy-test.properties
@@ -0,0 +1,20 @@
+#
+# Licensed to Elasticsearch under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Elasticsearch licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+implementation-class=org.elasticsearch.gradle.test.MessyTestPlugin
diff --git a/buildSrc/src/main/resources/beat.wav b/buildSrc/src/main/resources/beat.wav
new file mode 100644
index 0000000..4083a4c
Binary files /dev/null and b/buildSrc/src/main/resources/beat.wav differ
diff --git a/buildSrc/src/main/resources/forbidden/all-signatures.txt b/buildSrc/src/main/resources/forbidden/all-signatures.txt
index 5a91807..c1e65cb 100644
--- a/buildSrc/src/main/resources/forbidden/all-signatures.txt
+++ b/buildSrc/src/main/resources/forbidden/all-signatures.txt
@@ -92,6 +92,13 @@ java.net.InetAddress#getCanonicalHostName()
 java.net.InetSocketAddress#getHostName() @ Use getHostString() instead, which avoids a DNS lookup
 
 @defaultMessage Do not violate java's access system
+java.lang.Class#getDeclaredClasses() @ Do not violate java's access system: Use getClasses() instead
+java.lang.Class#getDeclaredConstructor(java.lang.Class[]) @ Do not violate java's access system: Use getConstructor() instead
+java.lang.Class#getDeclaredConstructors() @ Do not violate java's access system: Use getConstructors() instead
+java.lang.Class#getDeclaredField(java.lang.String) @ Do not violate java's access system: Use getField() instead
+java.lang.Class#getDeclaredFields() @ Do not violate java's access system: Use getFields() instead
+java.lang.Class#getDeclaredMethod(java.lang.String, java.lang.Class[]) @ Do not violate java's access system: Use getMethod() instead
+java.lang.Class#getDeclaredMethods() @ Do not violate java's access system: Use getMethods() instead
 java.lang.reflect.AccessibleObject#setAccessible(boolean)
 java.lang.reflect.AccessibleObject#setAccessible(java.lang.reflect.AccessibleObject[], boolean)
 
@@ -116,3 +123,6 @@ java.lang.System#getProperties() @ Use BootstrapInfo.getSystemProperties for a r
 java.util.Collections#EMPTY_LIST
 java.util.Collections#EMPTY_MAP
 java.util.Collections#EMPTY_SET
+
+java.util.Collections#shuffle(java.util.List) @ Use java.util.Collections#shuffle(java.util.List, java.util.Random) with a reproducible source of randomness
+java.util.Random#<init>() @ Use org.elasticsearch.common.random.Randomness#create for reproducible sources of randomness
diff --git a/buildSrc/version.properties b/buildSrc/version.properties
index 1a982d3..e33383a 100644
--- a/buildSrc/version.properties
+++ b/buildSrc/version.properties
@@ -1,10 +1,9 @@
 elasticsearch     = 3.0.0-SNAPSHOT
-lucene            = 5.4.0-snapshot-1715952
+lucene            = 5.5.0-snapshot-1719088
 
 # optional dependencies
 spatial4j         = 0.5
 jts               = 1.13
-mustache          = 0.9.1
 jackson           = 2.6.2
 log4j             = 1.2.17
 slf4j             = 1.6.2
@@ -12,7 +11,7 @@ jna               = 4.1.0
 
 
 # test dependencies
-randomizedrunner  = 2.2.0
+randomizedrunner  = 2.3.2
 junit             = 4.11
 httpclient        = 4.3.6
 httpcore          = 4.3.3
diff --git a/core/build.gradle b/core/build.gradle
index 3db5097..fd8a0c1 100644
--- a/core/build.gradle
+++ b/core/build.gradle
@@ -74,9 +74,6 @@ dependencies {
   compile "com.spatial4j:spatial4j:${versions.spatial4j}", optional
   compile "com.vividsolutions:jts:${versions.jts}", optional
 
-  // templating
-  compile "com.github.spullara.mustache.java:compiler:${versions.mustache}", optional
-
   // logging
   compile "log4j:log4j:${versions.log4j}", optional
   compile "log4j:apache-log4j-extras:${versions.log4j}", optional
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
index fce58d2..9f2b1b6 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
@@ -54,7 +54,6 @@ public class MapperQueryParser extends QueryParser {
     static {
         Map<String, FieldQueryExtension> fieldQueryExtensions = new HashMap<>();
         fieldQueryExtensions.put(ExistsFieldQueryExtension.NAME, new ExistsFieldQueryExtension());
-        fieldQueryExtensions.put(MissingFieldQueryExtension.NAME, new MissingFieldQueryExtension());
         FIELD_QUERY_EXTENSIONS = unmodifiableMap(fieldQueryExtensions);
     }
 
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
deleted file mode 100644
index f9fc8c9..0000000
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.lucene.queryparser.classic;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.MissingQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
-
-/**
- *
- */
-public class MissingFieldQueryExtension implements FieldQueryExtension {
-
-    public static final String NAME = "_missing_";
-
-    @Override
-    public Query query(QueryShardContext context, String queryText) {
-        Query query = MissingQueryBuilder.newFilter(context, queryText, MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE, MissingQueryBuilder.DEFAULT_NULL_VALUE);
-        if (query != null) {
-            return new ConstantScoreQuery(query);
-        }
-        return null;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/Version.java b/core/src/main/java/org/elasticsearch/Version.java
index 4e4ec3b..a5e2e38 100644
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -276,7 +276,7 @@ public class Version {
     public static final int V_2_2_0_ID = 2020099;
     public static final Version V_2_2_0 = new Version(V_2_2_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_4_0);
     public static final int V_3_0_0_ID = 3000099;
-    public static final Version V_3_0_0 = new Version(V_3_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_4_0);
+    public static final Version V_3_0_0 = new Version(V_3_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_5_0);
     public static final Version CURRENT = V_3_0_0;
 
     static {
diff --git a/core/src/main/java/org/elasticsearch/action/ActionWriteResponse.java b/core/src/main/java/org/elasticsearch/action/ActionWriteResponse.java
deleted file mode 100644
index f4152ac..0000000
--- a/core/src/main/java/org/elasticsearch/action/ActionWriteResponse.java
+++ /dev/null
@@ -1,306 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.bootstrap.Elasticsearch;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-import java.util.Collections;
-
-/**
- * Base class for write action responses.
- */
-public class ActionWriteResponse extends ActionResponse {
-
-    public final static ActionWriteResponse.ShardInfo.Failure[] EMPTY = new ActionWriteResponse.ShardInfo.Failure[0];
-
-    private ShardInfo shardInfo;
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        shardInfo = ActionWriteResponse.ShardInfo.readShardInfo(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        shardInfo.writeTo(out);
-    }
-
-    public ShardInfo getShardInfo() {
-        return shardInfo;
-    }
-
-    public void setShardInfo(ShardInfo shardInfo) {
-        this.shardInfo = shardInfo;
-    }
-
-    public static class ShardInfo implements Streamable, ToXContent {
-
-        private int total;
-        private int successful;
-        private Failure[] failures = EMPTY;
-
-        public ShardInfo() {
-        }
-
-        public ShardInfo(int total, int successful, Failure... failures) {
-            assert total >= 0 && successful >= 0;
-            this.total = total;
-            this.successful = successful;
-            this.failures = failures;
-        }
-
-        /**
-         * @return the total number of shards the write should go to (replicas and primaries). This includes relocating shards, so this number can be higher than the number of shards.
-         */
-        public int getTotal() {
-            return total;
-        }
-
-        /**
-         * @return the total number of shards the write succeeded on (replicas and primaries). This includes relocating shards, so this number can be higher than the number of shards.
-         */
-        public int getSuccessful() {
-            return successful;
-        }
-
-        /**
-         * @return The total number of replication failures.
-         */
-        public int getFailed() {
-            return failures.length;
-        }
-
-        /**
-         * @return The replication failures that have been captured in the case writes have failed on replica shards.
-         */
-        public Failure[] getFailures() {
-            return failures;
-        }
-
-        public RestStatus status() {
-            RestStatus status = RestStatus.OK;
-            for (Failure failure : failures) {
-                if (failure.primary() && failure.status().getStatus() > status.getStatus()) {
-                    status = failure.status();
-                }
-            }
-            return status;
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            total = in.readVInt();
-            successful = in.readVInt();
-            int size = in.readVInt();
-            failures = new Failure[size];
-            for (int i = 0; i < size; i++) {
-                Failure failure = new Failure();
-                failure.readFrom(in);
-                failures[i] = failure;
-            }
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(total);
-            out.writeVInt(successful);
-            out.writeVInt(failures.length);
-            for (Failure failure : failures) {
-                failure.writeTo(out);
-            }
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(Fields._SHARDS);
-            builder.field(Fields.TOTAL, total);
-            builder.field(Fields.SUCCESSFUL, successful);
-            builder.field(Fields.FAILED, getFailed());
-            if (failures.length > 0) {
-                builder.startArray(Fields.FAILURES);
-                for (Failure failure : failures) {
-                    failure.toXContent(builder, params);
-                }
-                builder.endArray();
-            }
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public String toString() {
-            return Strings.toString(this);
-        }
-
-        public static ShardInfo readShardInfo(StreamInput in) throws IOException {
-            ShardInfo shardInfo = new ShardInfo();
-            shardInfo.readFrom(in);
-            return shardInfo;
-        }
-
-        public static class Failure implements ShardOperationFailedException, ToXContent {
-
-            private String index;
-            private int shardId;
-            private String nodeId;
-            private Throwable cause;
-            private RestStatus status;
-            private boolean primary;
-
-            public Failure(String index, int shardId, @Nullable String nodeId, Throwable cause, RestStatus status, boolean primary) {
-                this.index = index;
-                this.shardId = shardId;
-                this.nodeId = nodeId;
-                this.cause = cause;
-                this.status = status;
-                this.primary = primary;
-            }
-
-            Failure() {
-            }
-
-            /**
-             * @return On what index the failure occurred.
-             */
-            @Override
-            public String index() {
-                return index;
-            }
-
-            /**
-             * @return On what shard id the failure occurred.
-             */
-            @Override
-            public int shardId() {
-                return shardId;
-            }
-
-            /**
-             * @return On what node the failure occurred.
-             */
-            @Nullable
-            public String nodeId() {
-                return nodeId;
-            }
-
-            /**
-             * @return A text description of the failure
-             */
-            @Override
-            public String reason() {
-                return ExceptionsHelper.detailedMessage(cause);
-            }
-
-            /**
-             * @return The status to report if this failure was a primary failure.
-             */
-            @Override
-            public RestStatus status() {
-                return status;
-            }
-
-            @Override
-            public Throwable getCause() {
-                return cause;
-            }
-
-            /**
-             * @return Whether this failure occurred on a primary shard.
-             * (this only reports true for delete by query)
-             */
-            public boolean primary() {
-                return primary;
-            }
-
-            @Override
-            public void readFrom(StreamInput in) throws IOException {
-                index = in.readString();
-                shardId = in.readVInt();
-                nodeId = in.readOptionalString();
-                cause = in.readThrowable();
-                status = RestStatus.readFrom(in);
-                primary = in.readBoolean();
-            }
-
-            @Override
-            public void writeTo(StreamOutput out) throws IOException {
-                out.writeString(index);
-                out.writeVInt(shardId);
-                out.writeOptionalString(nodeId);
-                out.writeThrowable(cause);
-                RestStatus.writeTo(out, status);
-                out.writeBoolean(primary);
-            }
-
-            @Override
-            public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-                builder.startObject();
-                builder.field(Fields._INDEX, index);
-                builder.field(Fields._SHARD, shardId);
-                builder.field(Fields._NODE, nodeId);
-                builder.field(Fields.REASON);
-                builder.startObject();
-                ElasticsearchException.toXContent(builder, params, cause);
-                builder.endObject();
-                builder.field(Fields.STATUS, status);
-                builder.field(Fields.PRIMARY, primary);
-                builder.endObject();
-                return builder;
-            }
-
-            private static class Fields {
-
-                private static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-                private static final XContentBuilderString _SHARD = new XContentBuilderString("_shard");
-                private static final XContentBuilderString _NODE = new XContentBuilderString("_node");
-                private static final XContentBuilderString REASON = new XContentBuilderString("reason");
-                private static final XContentBuilderString STATUS = new XContentBuilderString("status");
-                private static final XContentBuilderString PRIMARY = new XContentBuilderString("primary");
-
-            }
-        }
-
-        private static class Fields {
-
-            private static final XContentBuilderString _SHARDS = new XContentBuilderString("_shards");
-            private static final XContentBuilderString TOTAL = new XContentBuilderString("total");
-            private static final XContentBuilderString SUCCESSFUL = new XContentBuilderString("successful");
-            private static final XContentBuilderString PENDING = new XContentBuilderString("pending");
-            private static final XContentBuilderString FAILED = new XContentBuilderString("failed");
-            private static final XContentBuilderString FAILURES = new XContentBuilderString("failures");
-
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/DocWriteResponse.java b/core/src/main/java/org/elasticsearch/action/DocWriteResponse.java
new file mode 100644
index 0000000..009d3fc
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/DocWriteResponse.java
@@ -0,0 +1,130 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.action;
+
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.StatusToXContent;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
+
+import java.io.IOException;
+
+/**
+ * A base class for the response of a write operation that involves a single doc
+ */
+public abstract class DocWriteResponse extends ReplicationResponse implements StatusToXContent {
+
+    private ShardId shardId;
+    private String id;
+    private String type;
+    private long version;
+
+    public DocWriteResponse(ShardId shardId, String type, String id, long version) {
+        this.shardId = shardId;
+        this.type = type;
+        this.id = id;
+        this.version = version;
+    }
+
+    // needed for deserialization
+    protected DocWriteResponse() {
+    }
+
+    /**
+     * The index the document was changed in.
+     */
+    public String getIndex() {
+        return this.shardId.getIndex();
+    }
+
+
+    /**
+     * The exact shard the document was changed in.
+     */
+    public ShardId getShardId() {
+        return this.shardId;
+    }
+
+    /**
+     * The type of the document changed.
+     */
+    public String getType() {
+        return this.type;
+    }
+
+    /**
+     * The id of the document changed.
+     */
+    public String getId() {
+        return this.id;
+    }
+
+    /**
+     * Returns the current version of the doc.
+     */
+    public long getVersion() {
+        return this.version;
+    }
+
+    /** returns the rest status for this response (based on {@link ShardInfo#status()} */
+    public RestStatus status() {
+        return getShardInfo().status();
+    }
+
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        shardId = ShardId.readShardId(in);
+        type = in.readString();
+        id = in.readString();
+        version = in.readZLong();
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        shardId.writeTo(out);
+        out.writeString(type);
+        out.writeString(id);
+        out.writeZLong(version);
+    }
+
+    static final class Fields {
+        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
+        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
+        static final XContentBuilderString _ID = new XContentBuilderString("_id");
+        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        ReplicationResponse.ShardInfo shardInfo = getShardInfo();
+        builder.field(Fields._INDEX, shardId.getIndex())
+            .field(Fields._TYPE, type)
+            .field(Fields._ID, id)
+            .field(Fields._VERSION, version);
+        shardInfo.toXContent(builder, params);
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/ReplicationResponse.java b/core/src/main/java/org/elasticsearch/action/ReplicationResponse.java
new file mode 100644
index 0000000..4e358c8
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/ReplicationResponse.java
@@ -0,0 +1,303 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action;
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.rest.RestStatus;
+
+import java.io.IOException;
+
+/**
+ * Base class for write action responses.
+ */
+public class ReplicationResponse extends ActionResponse {
+
+    public final static ReplicationResponse.ShardInfo.Failure[] EMPTY = new ReplicationResponse.ShardInfo.Failure[0];
+
+    private ShardInfo shardInfo;
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        shardInfo = ReplicationResponse.ShardInfo.readShardInfo(in);
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        shardInfo.writeTo(out);
+    }
+
+    public ShardInfo getShardInfo() {
+        return shardInfo;
+    }
+
+    public void setShardInfo(ShardInfo shardInfo) {
+        this.shardInfo = shardInfo;
+    }
+
+    public static class ShardInfo implements Streamable, ToXContent {
+
+        private int total;
+        private int successful;
+        private Failure[] failures = EMPTY;
+
+        public ShardInfo() {
+        }
+
+        public ShardInfo(int total, int successful, Failure... failures) {
+            assert total >= 0 && successful >= 0;
+            this.total = total;
+            this.successful = successful;
+            this.failures = failures;
+        }
+
+        /**
+         * @return the total number of shards the write should go to (replicas and primaries). This includes relocating shards, so this number can be higher than the number of shards.
+         */
+        public int getTotal() {
+            return total;
+        }
+
+        /**
+         * @return the total number of shards the write succeeded on (replicas and primaries). This includes relocating shards, so this number can be higher than the number of shards.
+         */
+        public int getSuccessful() {
+            return successful;
+        }
+
+        /**
+         * @return The total number of replication failures.
+         */
+        public int getFailed() {
+            return failures.length;
+        }
+
+        /**
+         * @return The replication failures that have been captured in the case writes have failed on replica shards.
+         */
+        public Failure[] getFailures() {
+            return failures;
+        }
+
+        public RestStatus status() {
+            RestStatus status = RestStatus.OK;
+            for (Failure failure : failures) {
+                if (failure.primary() && failure.status().getStatus() > status.getStatus()) {
+                    status = failure.status();
+                }
+            }
+            return status;
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            total = in.readVInt();
+            successful = in.readVInt();
+            int size = in.readVInt();
+            failures = new Failure[size];
+            for (int i = 0; i < size; i++) {
+                Failure failure = new Failure();
+                failure.readFrom(in);
+                failures[i] = failure;
+            }
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeVInt(total);
+            out.writeVInt(successful);
+            out.writeVInt(failures.length);
+            for (Failure failure : failures) {
+                failure.writeTo(out);
+            }
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject(Fields._SHARDS);
+            builder.field(Fields.TOTAL, total);
+            builder.field(Fields.SUCCESSFUL, successful);
+            builder.field(Fields.FAILED, getFailed());
+            if (failures.length > 0) {
+                builder.startArray(Fields.FAILURES);
+                for (Failure failure : failures) {
+                    failure.toXContent(builder, params);
+                }
+                builder.endArray();
+            }
+            builder.endObject();
+            return builder;
+        }
+
+        @Override
+        public String toString() {
+            return Strings.toString(this);
+        }
+
+        public static ShardInfo readShardInfo(StreamInput in) throws IOException {
+            ShardInfo shardInfo = new ShardInfo();
+            shardInfo.readFrom(in);
+            return shardInfo;
+        }
+
+        public static class Failure implements ShardOperationFailedException, ToXContent {
+
+            private String index;
+            private int shardId;
+            private String nodeId;
+            private Throwable cause;
+            private RestStatus status;
+            private boolean primary;
+
+            public Failure(String index, int shardId, @Nullable String nodeId, Throwable cause, RestStatus status, boolean primary) {
+                this.index = index;
+                this.shardId = shardId;
+                this.nodeId = nodeId;
+                this.cause = cause;
+                this.status = status;
+                this.primary = primary;
+            }
+
+            Failure() {
+            }
+
+            /**
+             * @return On what index the failure occurred.
+             */
+            @Override
+            public String index() {
+                return index;
+            }
+
+            /**
+             * @return On what shard id the failure occurred.
+             */
+            @Override
+            public int shardId() {
+                return shardId;
+            }
+
+            /**
+             * @return On what node the failure occurred.
+             */
+            @Nullable
+            public String nodeId() {
+                return nodeId;
+            }
+
+            /**
+             * @return A text description of the failure
+             */
+            @Override
+            public String reason() {
+                return ExceptionsHelper.detailedMessage(cause);
+            }
+
+            /**
+             * @return The status to report if this failure was a primary failure.
+             */
+            @Override
+            public RestStatus status() {
+                return status;
+            }
+
+            @Override
+            public Throwable getCause() {
+                return cause;
+            }
+
+            /**
+             * @return Whether this failure occurred on a primary shard.
+             * (this only reports true for delete by query)
+             */
+            public boolean primary() {
+                return primary;
+            }
+
+            @Override
+            public void readFrom(StreamInput in) throws IOException {
+                index = in.readString();
+                shardId = in.readVInt();
+                nodeId = in.readOptionalString();
+                cause = in.readThrowable();
+                status = RestStatus.readFrom(in);
+                primary = in.readBoolean();
+            }
+
+            @Override
+            public void writeTo(StreamOutput out) throws IOException {
+                out.writeString(index);
+                out.writeVInt(shardId);
+                out.writeOptionalString(nodeId);
+                out.writeThrowable(cause);
+                RestStatus.writeTo(out, status);
+                out.writeBoolean(primary);
+            }
+
+            @Override
+            public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+                builder.startObject();
+                builder.field(Fields._INDEX, index);
+                builder.field(Fields._SHARD, shardId);
+                builder.field(Fields._NODE, nodeId);
+                builder.field(Fields.REASON);
+                builder.startObject();
+                ElasticsearchException.toXContent(builder, params, cause);
+                builder.endObject();
+                builder.field(Fields.STATUS, status);
+                builder.field(Fields.PRIMARY, primary);
+                builder.endObject();
+                return builder;
+            }
+
+            private static class Fields {
+
+                private static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
+                private static final XContentBuilderString _SHARD = new XContentBuilderString("_shard");
+                private static final XContentBuilderString _NODE = new XContentBuilderString("_node");
+                private static final XContentBuilderString REASON = new XContentBuilderString("reason");
+                private static final XContentBuilderString STATUS = new XContentBuilderString("status");
+                private static final XContentBuilderString PRIMARY = new XContentBuilderString("primary");
+
+            }
+        }
+
+        private static class Fields {
+
+            private static final XContentBuilderString _SHARDS = new XContentBuilderString("_shards");
+            private static final XContentBuilderString TOTAL = new XContentBuilderString("total");
+            private static final XContentBuilderString SUCCESSFUL = new XContentBuilderString("successful");
+            private static final XContentBuilderString PENDING = new XContentBuilderString("pending");
+            private static final XContentBuilderString FAILED = new XContentBuilderString("failed");
+            private static final XContentBuilderString FAILURES = new XContentBuilderString("failures");
+
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
index d09c73e..39d9cac 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java
@@ -31,6 +31,7 @@ import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.metadata.RepositoriesMetaData;
 import org.elasticsearch.cluster.metadata.RepositoryMetaData;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.repositories.RepositoryMissingException;
 import org.elasticsearch.threadpool.ThreadPool;
@@ -38,7 +39,9 @@ import org.elasticsearch.transport.TransportService;
 
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.LinkedHashSet;
 import java.util.List;
+import java.util.Set;
 
 /**
  * Transport action for get repositories operation
@@ -78,8 +81,20 @@ public class TransportGetRepositoriesAction extends TransportMasterNodeReadActio
             }
         } else {
             if (repositories != null) {
+                Set<String> repositoriesToGet = new LinkedHashSet<>(); // to keep insertion order
+                for (String repositoryOrPattern : request.repositories()) {
+                    if (Regex.isSimpleMatchPattern(repositoryOrPattern) == false) {
+                        repositoriesToGet.add(repositoryOrPattern);
+                    } else {
+                        for (RepositoryMetaData repository : repositories.repositories()) {
+                            if (Regex.simpleMatch(repositoryOrPattern, repository.name())) {
+                                repositoriesToGet.add(repository.name());
+                            }
+                        }
+                    }
+                }
                 List<RepositoryMetaData> repositoryListBuilder = new ArrayList<>();
-                for (String repository : request.repositories()) {
+                for (String repository : repositoriesToGet) {
                     RepositoryMetaData repositoryMetaData = repositories.repository(repository);
                     if (repositoryMetaData == null) {
                         listener.onFailure(new RepositoryMissingException(repository));
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdater.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdater.java
deleted file mode 100644
index f5020a4..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdater.java
+++ /dev/null
@@ -1,127 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.cluster.settings;
-
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.block.ClusterBlocks;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.regex.Regex;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Settings;
-
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-
-import static org.elasticsearch.cluster.ClusterState.builder;
-
-/**
- * Updates transient and persistent cluster state settings if there are any changes
- * due to the update.
- */
-final class SettingsUpdater {
-    final Settings.Builder transientUpdates = Settings.settingsBuilder();
-    final Settings.Builder persistentUpdates = Settings.settingsBuilder();
-    private final ClusterSettings clusterSettings;
-
-    SettingsUpdater(ClusterSettings clusterSettings) {
-        this.clusterSettings = clusterSettings;
-    }
-
-    synchronized Settings getTransientUpdates() {
-        return transientUpdates.build();
-    }
-
-    synchronized Settings getPersistentUpdate() {
-        return persistentUpdates.build();
-    }
-
-    synchronized ClusterState updateSettings(final ClusterState currentState, Settings transientToApply, Settings persistentToApply) {
-        boolean changed = false;
-        Settings.Builder transientSettings = Settings.settingsBuilder();
-        transientSettings.put(currentState.metaData().transientSettings());
-        changed |= apply(transientToApply, transientSettings, transientUpdates, "transient");
-
-        Settings.Builder persistentSettings = Settings.settingsBuilder();
-        persistentSettings.put(currentState.metaData().persistentSettings());
-        changed |= apply(persistentToApply, persistentSettings, persistentUpdates, "persistent");
-
-        if (!changed) {
-            return currentState;
-        }
-
-        MetaData.Builder metaData = MetaData.builder(currentState.metaData())
-            .persistentSettings(persistentSettings.build())
-            .transientSettings(transientSettings.build());
-
-        ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks());
-        boolean updatedReadOnly = MetaData.SETTING_READ_ONLY_SETTING.get(metaData.persistentSettings()) || MetaData.SETTING_READ_ONLY_SETTING.get(metaData.transientSettings());
-        if (updatedReadOnly) {
-            blocks.addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK);
-        } else {
-            blocks.removeGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK);
-        }
-        ClusterState build = builder(currentState).metaData(metaData).blocks(blocks).build();
-        Settings settings = build.metaData().settings();
-        // now we try to apply things and if they are invalid we fail
-        // this dryRun will validate & parse settings but won't actually apply them.
-        clusterSettings.dryRun(settings);
-        return build;
-    }
-
-    private boolean apply(Settings toApply, Settings.Builder target, Settings.Builder updates, String type) {
-        boolean changed = false;
-        final Set<String> toRemove = new HashSet<>();
-        Settings.Builder settingsBuilder = Settings.settingsBuilder();
-        for (Map.Entry<String, String> entry : toApply.getAsMap().entrySet()) {
-            if (entry.getValue() == null) {
-                toRemove.add(entry.getKey());
-            } else if (clusterSettings.isLoggerSetting(entry.getKey()) || clusterSettings.hasDynamicSetting(entry.getKey())) {
-                settingsBuilder.put(entry.getKey(), entry.getValue());
-                updates.put(entry.getKey(), entry.getValue());
-                changed = true;
-            } else {
-                throw new IllegalArgumentException(type + " setting [" + entry.getKey() + "], not dynamically updateable");
-            }
-
-        }
-        changed |= applyDeletes(toRemove, target);
-        target.put(settingsBuilder.build());
-        return changed;
-    }
-
-    private final boolean applyDeletes(Set<String> deletes, Settings.Builder builder) {
-        boolean changed = false;
-        for (String entry : deletes) {
-            Set<String> keysToRemove = new HashSet<>();
-            Set<String> keySet = builder.internalMap().keySet();
-            for (String key : keySet) {
-                if (Regex.simpleMatch(entry, key)) {
-                    keysToRemove.add(key);
-                }
-            }
-            for (String key : keysToRemove) {
-                builder.remove(key);
-                changed = true;
-            }
-        }
-        return changed;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java
index 99815b7..73d14a2 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java
@@ -28,19 +28,23 @@ import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
+import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
+import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
+import org.elasticsearch.cluster.settings.DynamicSettings;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Priority;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
+import java.util.Map;
+
 import static org.elasticsearch.cluster.ClusterState.builder;
 
 /**
@@ -50,14 +54,15 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
 
     private final AllocationService allocationService;
 
-    private final ClusterSettings clusterSettings;
+    private final DynamicSettings dynamicSettings;
 
     @Inject
     public TransportClusterUpdateSettingsAction(Settings settings, TransportService transportService, ClusterService clusterService, ThreadPool threadPool,
-                                                AllocationService allocationService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, ClusterSettings clusterSettings) {
+                                                AllocationService allocationService, @ClusterDynamicSettings DynamicSettings dynamicSettings,
+                                                ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
         super(settings, ClusterUpdateSettingsAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, ClusterUpdateSettingsRequest::new);
         this.allocationService = allocationService;
-        this.clusterSettings = clusterSettings;
+        this.dynamicSettings = dynamicSettings;
     }
 
     @Override
@@ -68,8 +73,8 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
     @Override
     protected ClusterBlockException checkBlock(ClusterUpdateSettingsRequest request, ClusterState state) {
         // allow for dedicated changes to the metadata blocks, so we don't block those to allow to "re-enable" it
-        if ((request.transientSettings().getAsMap().isEmpty() && request.persistentSettings().getAsMap().size() == 1 && MetaData.SETTING_READ_ONLY_SETTING.exists(request.persistentSettings())) ||
-                request.persistentSettings().getAsMap().isEmpty() && request.transientSettings().getAsMap().size() == 1 && MetaData.SETTING_READ_ONLY_SETTING.exists(request.transientSettings())) {
+        if ((request.transientSettings().getAsMap().isEmpty() && request.persistentSettings().getAsMap().size() == 1 && request.persistentSettings().get(MetaData.SETTING_READ_ONLY) != null) ||
+                request.persistentSettings().getAsMap().isEmpty() && request.transientSettings().getAsMap().size() == 1 && request.transientSettings().get(MetaData.SETTING_READ_ONLY) != null) {
             return null;
         }
         return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
@@ -83,7 +88,9 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
 
     @Override
     protected void masterOperation(final ClusterUpdateSettingsRequest request, final ClusterState state, final ActionListener<ClusterUpdateSettingsResponse> listener) {
-        final SettingsUpdater updater = new SettingsUpdater(clusterSettings);
+        final Settings.Builder transientUpdates = Settings.settingsBuilder();
+        final Settings.Builder persistentUpdates = Settings.settingsBuilder();
+
         clusterService.submitStateUpdateTask("cluster_update_settings",
                 new AckedClusterStateUpdateTask<ClusterUpdateSettingsResponse>(Priority.IMMEDIATE, request, listener) {
 
@@ -91,7 +98,7 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
 
             @Override
             protected ClusterUpdateSettingsResponse newResponse(boolean acknowledged) {
-                return new ClusterUpdateSettingsResponse(acknowledged, updater.getTransientUpdates(), updater.getPersistentUpdate());
+                return new ClusterUpdateSettingsResponse(acknowledged, transientUpdates.build(), persistentUpdates.build());
             }
 
             @Override
@@ -118,7 +125,7 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
                 // so we should *not* execute the reroute.
                 if (!clusterService.state().nodes().localNodeMaster()) {
                     logger.debug("Skipping reroute after cluster update settings, because node is no longer master");
-                    listener.onResponse(new ClusterUpdateSettingsResponse(updateSettingsAcked, updater.getTransientUpdates(), updater.getPersistentUpdate()));
+                    listener.onResponse(new ClusterUpdateSettingsResponse(updateSettingsAcked, transientUpdates.build(), persistentUpdates.build()));
                     return;
                 }
 
@@ -138,13 +145,13 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
                     @Override
                     //we return when the cluster reroute is acked or it times out but the acknowledged flag depends on whether the update settings was acknowledged
                     protected ClusterUpdateSettingsResponse newResponse(boolean acknowledged) {
-                        return new ClusterUpdateSettingsResponse(updateSettingsAcked && acknowledged, updater.getTransientUpdates(), updater.getPersistentUpdate());
+                        return new ClusterUpdateSettingsResponse(updateSettingsAcked && acknowledged, transientUpdates.build(), persistentUpdates.build());
                     }
 
                     @Override
                     public void onNoLongerMaster(String source) {
                         logger.debug("failed to preform reroute after cluster settings were updated - current node is no longer a master");
-                        listener.onResponse(new ClusterUpdateSettingsResponse(updateSettingsAcked, updater.getTransientUpdates(), updater.getPersistentUpdate()));
+                        listener.onResponse(new ClusterUpdateSettingsResponse(updateSettingsAcked, transientUpdates.build(), persistentUpdates.build()));
                     }
 
                     @Override
@@ -174,11 +181,58 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
 
             @Override
             public ClusterState execute(final ClusterState currentState) {
-                ClusterState clusterState = updater.updateSettings(currentState, request.transientSettings(), request.persistentSettings());
-                changed = clusterState != currentState;
-                return clusterState;
+                Settings.Builder transientSettings = Settings.settingsBuilder();
+                transientSettings.put(currentState.metaData().transientSettings());
+                for (Map.Entry<String, String> entry : request.transientSettings().getAsMap().entrySet()) {
+                    if (dynamicSettings.isDynamicOrLoggingSetting(entry.getKey())) {
+                        String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue(), clusterService.state());
+                        if (error == null) {
+                            transientSettings.put(entry.getKey(), entry.getValue());
+                            transientUpdates.put(entry.getKey(), entry.getValue());
+                            changed = true;
+                        } else {
+                            logger.warn("ignoring transient setting [{}], [{}]", entry.getKey(), error);
+                        }
+                    } else {
+                        logger.warn("ignoring transient setting [{}], not dynamically updateable", entry.getKey());
+                    }
+                }
+
+                Settings.Builder persistentSettings = Settings.settingsBuilder();
+                persistentSettings.put(currentState.metaData().persistentSettings());
+                for (Map.Entry<String, String> entry : request.persistentSettings().getAsMap().entrySet()) {
+                    if (dynamicSettings.isDynamicOrLoggingSetting(entry.getKey())) {
+                        String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue(), clusterService.state());
+                        if (error == null) {
+                            persistentSettings.put(entry.getKey(), entry.getValue());
+                            persistentUpdates.put(entry.getKey(), entry.getValue());
+                            changed = true;
+                        } else {
+                            logger.warn("ignoring persistent setting [{}], [{}]", entry.getKey(), error);
+                        }
+                    } else {
+                        logger.warn("ignoring persistent setting [{}], not dynamically updateable", entry.getKey());
+                    }
+                }
+
+                if (!changed) {
+                    return currentState;
+                }
+
+                MetaData.Builder metaData = MetaData.builder(currentState.metaData())
+                        .persistentSettings(persistentSettings.build())
+                        .transientSettings(transientSettings.build());
+
+                ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks());
+                boolean updatedReadOnly = metaData.persistentSettings().getAsBoolean(MetaData.SETTING_READ_ONLY, false) || metaData.transientSettings().getAsBoolean(MetaData.SETTING_READ_ONLY, false);
+                if (updatedReadOnly) {
+                    blocks.addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK);
+                } else {
+                    blocks.removeGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK);
+                }
+
+                return builder(currentState).metaData(metaData).blocks(blocks).build();
             }
         });
     }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
index 7bfb0e8..478146d 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java
@@ -29,6 +29,7 @@ import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.snapshots.Snapshot;
 import org.elasticsearch.snapshots.SnapshotInfo;
@@ -38,7 +39,9 @@ import org.elasticsearch.transport.TransportService;
 
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.LinkedHashSet;
 import java.util.List;
+import java.util.Set;
 
 /**
  * Transport Action for get snapshots operation
@@ -84,8 +87,24 @@ public class TransportGetSnapshotsAction extends TransportMasterNodeAction<GetSn
                     snapshotInfoBuilder.add(new SnapshotInfo(snapshot));
                 }
             } else {
-                for (int i = 0; i < request.snapshots().length; i++) {
-                    SnapshotId snapshotId = new SnapshotId(request.repository(), request.snapshots()[i]);
+                Set<String> snapshotsToGet = new LinkedHashSet<>(); // to keep insertion order
+                List<Snapshot> snapshots = null;
+                for (String snapshotOrPattern : request.snapshots()) {
+                    if (Regex.isSimpleMatchPattern(snapshotOrPattern) == false) {
+                        snapshotsToGet.add(snapshotOrPattern);
+                    } else {
+                        if (snapshots == null) { // lazily load snapshots
+                            snapshots = snapshotsService.snapshots(request.repository(), request.ignoreUnavailable());
+                        }
+                        for (Snapshot snapshot : snapshots) {
+                            if (Regex.simpleMatch(snapshotOrPattern, snapshot.name())) {
+                                snapshotsToGet.add(snapshot.name());
+                            }
+                        }
+                    }
+                }
+                for (String snapshot : snapshotsToGet) {
+                    SnapshotId snapshotId = new SnapshotId(request.repository(), snapshot);
                     snapshotInfoBuilder.add(new SnapshotInfo(snapshotsService.snapshot(snapshotId)));
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
index 13b7ee9..1da2662 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java
@@ -78,19 +78,19 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             indices(indices);
             aliases(aliases);
         }
-        
+
         public AliasActions(AliasAction.Type type, String index, String alias) {
             aliasAction = new AliasAction(type);
             indices(index);
             aliases(alias);
         }
-        
+
         AliasActions(AliasAction.Type type, String[] index, String alias) {
             aliasAction = new AliasAction(type);
             indices(index);
             aliases(alias);
         }
-        
+
         public AliasActions(AliasAction action) {
             this.aliasAction = action;
             indices(action.index());
@@ -110,7 +110,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             aliasAction.filter(filter);
             return this;
         }
-        
+
         public AliasActions filter(QueryBuilder filter) {
             aliasAction.filter(filter);
             return this;
@@ -197,7 +197,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             aliasAction = readAliasAction(in);
             return this;
         }
-        
+
         public void writeTo(StreamOutput out) throws IOException {
             out.writeStringArray(indices);
             out.writeStringArray(aliases);
@@ -225,7 +225,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
         addAliasAction(new AliasActions(action));
         return this;
     }
-    
+
     /**
      * Adds an alias to the index.
      * @param alias  The alias
@@ -247,8 +247,8 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
         addAliasAction(new AliasActions(AliasAction.Type.ADD, indices, alias).filter(filterBuilder));
         return this;
     }
-    
-    
+
+
     /**
      * Removes an alias to the index.
      *
@@ -259,7 +259,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
         addAliasAction(new AliasActions(AliasAction.Type.REMOVE, indices, aliases));
         return this;
     }
-    
+
     /**
      * Removes an alias to the index.
      *
@@ -286,25 +286,14 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
             return addValidationError("Must specify at least one alias action", validationException);
         }
         for (AliasActions aliasAction : allAliasActions) {
-            if (aliasAction.actionType() == AliasAction.Type.ADD) {
-                if (aliasAction.aliases.length != 1) {
-                    validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
-                            + "] requires exactly one [alias] to be set", validationException);
-                }
-                if (!Strings.hasText(aliasAction.aliases[0])) {
-                    validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
-                            + "] requires an [alias] to be set", validationException);
-                }
-            } else {
-                if (aliasAction.aliases.length == 0) {
+            if (aliasAction.aliases.length == 0) {
+                validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
+                        + "]: aliases may not be empty", validationException);
+            }
+            for (String alias : aliasAction.aliases) {
+                if (!Strings.hasText(alias)) {
                     validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
-                            + "]: aliases may not be empty", validationException);
-                }
-                for (String alias : aliasAction.aliases) {
-                    if (!Strings.hasText(alias)) {
-                        validationException = addValidationError("Alias action [" + aliasAction.actionType().name().toLowerCase(Locale.ENGLISH)
-                                + "]: [alias] may not be empty string", validationException);
-                    }
+                            + "]: [alias] may not be empty string", validationException);
                 }
             }
             if (CollectionUtils.isEmpty(aliasAction.indices)) {
@@ -345,7 +334,7 @@ public class IndicesAliasesRequest extends AcknowledgedRequest<IndicesAliasesReq
     public IndicesOptions indicesOptions() {
         return INDICES_OPTIONS;
     }
-    
+
     private static AliasActions readAliasActions(StreamInput in) throws IOException {
         AliasActions actions = new AliasActions();
         return actions.readFrom(in);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java
index 6482e34..db1a03e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.action.admin.indices.analyze;
 
+import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.support.single.shard.SingleShardRequest;
 import org.elasticsearch.common.Strings;
@@ -46,6 +47,10 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
 
     private String field;
 
+    private boolean explain = false;
+
+    private String[] attributes = Strings.EMPTY_ARRAY;
+
     public AnalyzeRequest() {
     }
 
@@ -86,6 +91,9 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
     }
 
     public AnalyzeRequest tokenFilters(String... tokenFilters) {
+        if (tokenFilters == null) {
+            throw new IllegalArgumentException("token filters must not be null");
+        }
         this.tokenFilters = tokenFilters;
         return this;
     }
@@ -95,6 +103,9 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
     }
 
     public AnalyzeRequest charFilters(String... charFilters) {
+        if (charFilters == null) {
+            throw new IllegalArgumentException("char filters must not be null");
+        }
         this.charFilters = charFilters;
         return this;
     }
@@ -112,18 +123,33 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
         return this.field;
     }
 
+    public AnalyzeRequest explain(boolean explain) {
+        this.explain = explain;
+        return this;
+    }
+
+    public boolean explain() {
+        return this.explain;
+    }
+
+    public AnalyzeRequest attributes(String... attributes) {
+        if (attributes == null) {
+            throw new IllegalArgumentException("attributes must not be null");
+        }
+        this.attributes = attributes;
+        return this;
+    }
+
+    public String[] attributes() {
+        return this.attributes;
+    }
+
     @Override
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
         if (text == null || text.length == 0) {
             validationException = addValidationError("text is missing", validationException);
         }
-        if (tokenFilters == null) {
-            validationException = addValidationError("token filters must not be null", validationException);
-        }
-        if (charFilters == null) {
-            validationException = addValidationError("char filters must not be null", validationException);
-        }
         return validationException;
     }
 
@@ -136,6 +162,10 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
         tokenFilters = in.readStringArray();
         charFilters = in.readStringArray();
         field = in.readOptionalString();
+        if (in.getVersion().onOrAfter(Version.V_2_2_0)) {
+            explain = in.readBoolean();
+            attributes = in.readStringArray();
+        }
     }
 
     @Override
@@ -147,5 +177,9 @@ public class AnalyzeRequest extends SingleShardRequest<AnalyzeRequest> {
         out.writeStringArray(tokenFilters);
         out.writeStringArray(charFilters);
         out.writeOptionalString(field);
+        if (out.getVersion().onOrAfter(Version.V_2_2_0)) {
+            out.writeBoolean(explain);
+            out.writeStringArray(attributes);
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java
index 9ed02e6..23c1739 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java
@@ -79,6 +79,22 @@ public class AnalyzeRequestBuilder extends SingleShardOperationRequestBuilder<An
     }
 
     /**
+     * Sets explain
+     */
+    public AnalyzeRequestBuilder setExplain(boolean explain) {
+        request.explain(explain);
+        return this;
+    }
+
+    /**
+     * Sets attributes that will include results
+     */
+    public AnalyzeRequestBuilder setAttributes(String attributes){
+        request.attributes(attributes);
+        return this;
+    }
+
+    /**
      * Sets texts to analyze
      */
     public AnalyzeRequestBuilder setText(String... texts) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java
index 720d19c..f867c2e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.action.admin.indices.analyze;
 
+import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -30,28 +31,32 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Map;
 
 /**
  *
  */
 public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeResponse.AnalyzeToken>, ToXContent {
 
-    public static class AnalyzeToken implements Streamable {
+    public static class AnalyzeToken implements Streamable, ToXContent {
         private String term;
         private int startOffset;
         private int endOffset;
         private int position;
+        private Map<String, Object> attributes;
         private String type;
 
         AnalyzeToken() {
         }
 
-        public AnalyzeToken(String term, int position, int startOffset, int endOffset, String type) {
+        public AnalyzeToken(String term, int position, int startOffset, int endOffset, String type,
+                            Map<String, Object> attributes) {
             this.term = term;
             this.position = position;
             this.startOffset = startOffset;
             this.endOffset = endOffset;
             this.type = type;
+            this.attributes = attributes;
         }
 
         public String getTerm() {
@@ -74,6 +79,27 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
             return this.type;
         }
 
+        public Map<String, Object> getAttributes(){
+            return this.attributes;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            builder.field(Fields.TOKEN, term);
+            builder.field(Fields.START_OFFSET, startOffset);
+            builder.field(Fields.END_OFFSET, endOffset);
+            builder.field(Fields.TYPE, type);
+            builder.field(Fields.POSITION, position);
+            if (attributes != null && !attributes.isEmpty()) {
+                for (Map.Entry<String, Object> entity : attributes.entrySet()) {
+                    builder.field(entity.getKey(), entity.getValue());
+                }
+            }
+            builder.endObject();
+            return builder;
+        }
+
         public static AnalyzeToken readAnalyzeToken(StreamInput in) throws IOException {
             AnalyzeToken analyzeToken = new AnalyzeToken();
             analyzeToken.readFrom(in);
@@ -87,6 +113,9 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
             endOffset = in.readInt();
             position = in.readVInt();
             type = in.readOptionalString();
+            if (in.getVersion().onOrAfter(Version.V_2_2_0)) {
+                attributes = (Map<String, Object>) in.readGenericValue();
+            }
         }
 
         @Override
@@ -96,22 +125,32 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
             out.writeInt(endOffset);
             out.writeVInt(position);
             out.writeOptionalString(type);
+            if (out.getVersion().onOrAfter(Version.V_2_2_0)) {
+                out.writeGenericValue(attributes);
+            }
         }
     }
 
+    private DetailAnalyzeResponse detail;
+
     private List<AnalyzeToken> tokens;
 
     AnalyzeResponse() {
     }
 
-    public AnalyzeResponse(List<AnalyzeToken> tokens) {
+    public AnalyzeResponse(List<AnalyzeToken> tokens, DetailAnalyzeResponse detail) {
         this.tokens = tokens;
+        this.detail = detail;
     }
 
     public List<AnalyzeToken> getTokens() {
         return this.tokens;
     }
 
+    public DetailAnalyzeResponse detail() {
+        return this.detail;
+    }
+
     @Override
     public Iterator<AnalyzeToken> iterator() {
         return tokens.iterator();
@@ -119,17 +158,19 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startArray(Fields.TOKENS);
-        for (AnalyzeToken token : tokens) {
-            builder.startObject();
-            builder.field(Fields.TOKEN, token.getTerm());
-            builder.field(Fields.START_OFFSET, token.getStartOffset());
-            builder.field(Fields.END_OFFSET, token.getEndOffset());
-            builder.field(Fields.TYPE, token.getType());
-            builder.field(Fields.POSITION, token.getPosition());
+        if (tokens != null) {
+            builder.startArray(Fields.TOKENS);
+            for (AnalyzeToken token : tokens) {
+                token.toXContent(builder, params);
+            }
+            builder.endArray();
+        }
+
+        if (detail != null) {
+            builder.startObject(Fields.DETAIL);
+            detail.toXContent(builder, params);
             builder.endObject();
         }
-        builder.endArray();
         return builder;
     }
 
@@ -141,14 +182,24 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
         for (int i = 0; i < size; i++) {
             tokens.add(AnalyzeToken.readAnalyzeToken(in));
         }
+        if (in.getVersion().onOrAfter(Version.V_2_2_0)) {
+            detail = in.readOptionalStreamable(DetailAnalyzeResponse::new);
+        }
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeVInt(tokens.size());
-        for (AnalyzeToken token : tokens) {
-            token.writeTo(out);
+        if (tokens != null) {
+            out.writeVInt(tokens.size());
+            for (AnalyzeToken token : tokens) {
+                token.writeTo(out);
+            }
+        } else {
+            out.writeVInt(0);
+        }
+        if (out.getVersion().onOrAfter(Version.V_2_2_0)) {
+            out.writeOptionalStreamable(detail);
         }
     }
 
@@ -159,5 +210,6 @@ public class AnalyzeResponse extends ActionResponse implements Iterable<AnalyzeR
         static final XContentBuilderString END_OFFSET = new XContentBuilderString("end_offset");
         static final XContentBuilderString TYPE = new XContentBuilderString("type");
         static final XContentBuilderString POSITION = new XContentBuilderString("position");
+        static final XContentBuilderString DETAIL = new XContentBuilderString("detail");
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/DetailAnalyzeResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/DetailAnalyzeResponse.java
new file mode 100644
index 0000000..08d18ff
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/DetailAnalyzeResponse.java
@@ -0,0 +1,319 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.indices.analyze;
+
+
+import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+
+import java.io.IOException;
+
+public class DetailAnalyzeResponse implements Streamable, ToXContent {
+
+    DetailAnalyzeResponse() {
+    }
+
+    private boolean customAnalyzer = false;
+    private AnalyzeTokenList analyzer;
+    private CharFilteredText[] charfilters;
+    private AnalyzeTokenList tokenizer;
+    private AnalyzeTokenList[] tokenfilters;
+
+    public DetailAnalyzeResponse(AnalyzeTokenList analyzer) {
+        this(false, analyzer, null, null, null);
+    }
+
+    public DetailAnalyzeResponse(CharFilteredText[] charfilters, AnalyzeTokenList tokenizer, AnalyzeTokenList[] tokenfilters) {
+        this(true, null, charfilters, tokenizer, tokenfilters);
+    }
+
+    public DetailAnalyzeResponse(boolean customAnalyzer,
+                                 AnalyzeTokenList analyzer,
+                                 CharFilteredText[] charfilters,
+                                 AnalyzeTokenList tokenizer,
+                                 AnalyzeTokenList[] tokenfilters) {
+        this.customAnalyzer = customAnalyzer;
+        this.analyzer = analyzer;
+        this.charfilters = charfilters;
+        this.tokenizer = tokenizer;
+        this.tokenfilters = tokenfilters;
+    }
+
+    public AnalyzeTokenList analyzer() {
+        return this.analyzer;
+    }
+
+    public DetailAnalyzeResponse analyzer(AnalyzeTokenList analyzer) {
+        this.analyzer = analyzer;
+        return this;
+    }
+
+    public CharFilteredText[] charfilters() {
+        return this.charfilters;
+    }
+
+    public DetailAnalyzeResponse charfilters(CharFilteredText[] charfilters) {
+        this.charfilters = charfilters;
+        return this;
+    }
+
+    public AnalyzeTokenList tokenizer() {
+        return tokenizer;
+    }
+
+    public DetailAnalyzeResponse tokenizer(AnalyzeTokenList tokenizer) {
+        this.tokenizer = tokenizer;
+        return this;
+    }
+
+    public AnalyzeTokenList[] tokenfilters() {
+        return tokenfilters;
+    }
+
+    public DetailAnalyzeResponse tokenfilters(AnalyzeTokenList[] tokenfilters) {
+        this.tokenfilters = tokenfilters;
+        return this;
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.field(Fields.CUSTOM_ANALYZER, customAnalyzer);
+
+        if (analyzer != null) {
+            builder.startObject(Fields.ANALYZER);
+            analyzer.toXContentWithoutObject(builder, params);
+            builder.endObject();
+        }
+
+        if (charfilters != null) {
+            builder.startArray(Fields.CHARFILTERS);
+            for (CharFilteredText charfilter : charfilters) {
+                charfilter.toXContent(builder, params);
+            }
+            builder.endArray();
+        }
+
+        if (tokenizer != null) {
+            builder.startObject(Fields.TOKENIZER);
+            tokenizer.toXContentWithoutObject(builder, params);
+            builder.endObject();
+        }
+
+        if (tokenfilters != null) {
+            builder.startArray(Fields.TOKENFILTERS);
+            for (AnalyzeTokenList tokenfilter : tokenfilters) {
+                tokenfilter.toXContent(builder, params);
+            }
+            builder.endArray();
+        }
+        return builder;
+    }
+
+    static final class Fields {
+        static final XContentBuilderString NAME = new XContentBuilderString("name");
+        static final XContentBuilderString FILTERED_TEXT = new XContentBuilderString("filtered_text");
+        static final XContentBuilderString CUSTOM_ANALYZER = new XContentBuilderString("custom_analyzer");
+        static final XContentBuilderString ANALYZER = new XContentBuilderString("analyzer");
+        static final XContentBuilderString CHARFILTERS = new XContentBuilderString("charfilters");
+        static final XContentBuilderString TOKENIZER = new XContentBuilderString("tokenizer");
+        static final XContentBuilderString TOKENFILTERS = new XContentBuilderString("tokenfilters");
+    }
+
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        this.customAnalyzer = in.readBoolean();
+        if (customAnalyzer) {
+            tokenizer = AnalyzeTokenList.readAnalyzeTokenList(in);
+            int size = in.readVInt();
+            if (size > 0) {
+                charfilters = new CharFilteredText[size];
+                for (int i = 0; i < size; i++) {
+                    charfilters[i] = CharFilteredText.readCharFilteredText(in);
+                }
+            }
+            size = in.readVInt();
+            if (size > 0) {
+                tokenfilters = new AnalyzeTokenList[size];
+                for (int i = 0; i < size; i++) {
+                    tokenfilters[i] = AnalyzeTokenList.readAnalyzeTokenList(in);
+                }
+            }
+        } else {
+            analyzer = AnalyzeTokenList.readAnalyzeTokenList(in);
+        }
+    }
+
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        out.writeBoolean(customAnalyzer);
+        if (customAnalyzer) {
+            tokenizer.writeTo(out);
+            if (charfilters != null) {
+                out.writeVInt(charfilters.length);
+                for (CharFilteredText charfilter : charfilters) {
+                    charfilter.writeTo(out);
+                }
+            } else {
+                out.writeVInt(0);
+            }
+            if (tokenfilters != null) {
+                out.writeVInt(tokenfilters.length);
+                for (AnalyzeTokenList tokenfilter : tokenfilters) {
+                    tokenfilter.writeTo(out);
+                }
+            } else {
+                out.writeVInt(0);
+            }
+        } else {
+            analyzer.writeTo(out);
+        }
+    }
+
+    public static class AnalyzeTokenList implements Streamable, ToXContent {
+        private String name;
+        private AnalyzeResponse.AnalyzeToken[] tokens;
+
+        AnalyzeTokenList() {
+        }
+
+        public AnalyzeTokenList(String name, AnalyzeResponse.AnalyzeToken[] tokens) {
+            this.name = name;
+            this.tokens = tokens;
+        }
+
+        public String getName() {
+            return name;
+        }
+
+        public AnalyzeResponse.AnalyzeToken[] getTokens() {
+            return tokens;
+        }
+
+        public static AnalyzeTokenList readAnalyzeTokenList(StreamInput in) throws IOException {
+            AnalyzeTokenList list = new AnalyzeTokenList();
+            list.readFrom(in);
+            return list;
+        }
+
+        public XContentBuilder toXContentWithoutObject(XContentBuilder builder, Params params) throws IOException {
+            builder.field(Fields.NAME, this.name);
+            builder.startArray(AnalyzeResponse.Fields.TOKENS);
+            for (AnalyzeResponse.AnalyzeToken token : tokens) {
+                token.toXContent(builder, params);
+            }
+            builder.endArray();
+            return builder;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            builder.field(Fields.NAME, this.name);
+            builder.startArray(AnalyzeResponse.Fields.TOKENS);
+            for (AnalyzeResponse.AnalyzeToken token : tokens) {
+                token.toXContent(builder, params);
+            }
+            builder.endArray();
+            builder.endObject();
+            return builder;
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            name = in.readString();
+            int size = in.readVInt();
+            if (size > 0) {
+                tokens = new AnalyzeResponse.AnalyzeToken[size];
+                for (int i = 0; i < size; i++) {
+                    tokens[i] = AnalyzeResponse.AnalyzeToken.readAnalyzeToken(in);
+                }
+            }
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeString(name);
+            if (tokens != null) {
+                out.writeVInt(tokens.length);
+                for (AnalyzeResponse.AnalyzeToken token : tokens) {
+                    token.writeTo(out);
+                }
+            } else {
+                out.writeVInt(0);
+            }
+        }
+    }
+
+    public static class CharFilteredText implements Streamable, ToXContent {
+        private String name;
+        private String[] texts;
+        CharFilteredText() {
+        }
+
+        public CharFilteredText(String name, String[] texts) {
+            this.name = name;
+            if (texts != null) {
+                this.texts = texts;
+            } else {
+                this.texts = Strings.EMPTY_ARRAY;
+            }
+        }
+
+        public String getName() {
+            return name;
+        }
+
+        public String[] getTexts() {
+            return texts;
+        }
+
+        @Override
+        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+            builder.startObject();
+            builder.field(Fields.NAME, name);
+            builder.field(Fields.FILTERED_TEXT, texts);
+            builder.endObject();
+            return builder;
+        }
+
+        public static CharFilteredText readCharFilteredText(StreamInput in) throws IOException {
+            CharFilteredText text = new CharFilteredText();
+            text.readFrom(in);
+            return text;
+        }
+
+        @Override
+        public void readFrom(StreamInput in) throws IOException {
+            name = in.readString();
+            texts = in.readStringArray();
+        }
+
+        @Override
+        public void writeTo(StreamOutput out) throws IOException {
+            out.writeString(name);
+            out.writeStringArray(texts);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java
index ba49c33..ecdf977 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java
@@ -20,10 +20,15 @@ package org.elasticsearch.action.admin.indices.analyze;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeReflector;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.single.shard.TransportSingleShardAction;
@@ -33,6 +38,7 @@ import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.routing.ShardsIterator;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.io.FastStringReader;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.IndexService;
@@ -46,8 +52,8 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
+import java.io.Reader;
+import java.util.*;
 
 /**
  * Transport action used to execute analyze requests
@@ -222,6 +228,23 @@ public class TransportAnalyzeAction extends TransportSingleShardAction<AnalyzeRe
             throw new IllegalArgumentException("failed to find analyzer");
         }
 
+        List<AnalyzeResponse.AnalyzeToken> tokens = null;
+        DetailAnalyzeResponse detail = null;
+
+        if (request.explain()) {
+            detail = detailAnalyze(request, analyzer, field);
+        } else {
+            tokens = simpleAnalyze(request, analyzer, field);
+        }
+
+        if (closeAnalyzer) {
+            analyzer.close();
+        }
+
+        return new AnalyzeResponse(tokens, detail);
+    }
+
+    private static List<AnalyzeResponse.AnalyzeToken> simpleAnalyze(AnalyzeRequest request, Analyzer analyzer, String field) {
         List<AnalyzeResponse.AnalyzeToken> tokens = new ArrayList<>();
         int lastPosition = -1;
         int lastOffset = 0;
@@ -238,7 +261,7 @@ public class TransportAnalyzeAction extends TransportSingleShardAction<AnalyzeRe
                     if (increment > 0) {
                         lastPosition = lastPosition + increment;
                     }
-                    tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(), lastPosition, lastOffset + offset.startOffset(), lastOffset + offset.endOffset(), type.type()));
+                    tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(), lastPosition, lastOffset + offset.startOffset(), lastOffset + offset.endOffset(), type.type(), null));
 
                 }
                 stream.end();
@@ -251,11 +274,211 @@ public class TransportAnalyzeAction extends TransportSingleShardAction<AnalyzeRe
                 throw new ElasticsearchException("failed to analyze", e);
             }
         }
+        return tokens;
+    }
 
-        if (closeAnalyzer) {
-            analyzer.close();
+    private static DetailAnalyzeResponse detailAnalyze(AnalyzeRequest request, Analyzer analyzer, String field) {
+        DetailAnalyzeResponse detailResponse;
+        final Set<String> includeAttributes = new HashSet<>();
+        if (request.attributes() != null) {
+            for (String attribute : request.attributes()) {
+                includeAttributes.add(attribute.toLowerCase(Locale.ROOT));
+            }
+        }
+
+        CustomAnalyzer customAnalyzer = null;
+        if (analyzer instanceof CustomAnalyzer) {
+            customAnalyzer = (CustomAnalyzer) analyzer;
+        } else if (analyzer instanceof NamedAnalyzer && ((NamedAnalyzer) analyzer).analyzer() instanceof CustomAnalyzer) {
+            customAnalyzer = (CustomAnalyzer) ((NamedAnalyzer) analyzer).analyzer();
+        }
+
+        if (customAnalyzer != null) {
+            // customAnalyzer = divide charfilter, tokenizer tokenfilters
+            CharFilterFactory[] charFilterFactories = customAnalyzer.charFilters();
+            TokenizerFactory tokenizerFactory = customAnalyzer.tokenizerFactory();
+            TokenFilterFactory[] tokenFilterFactories = customAnalyzer.tokenFilters();
+
+            String[][] charFiltersTexts = new String[charFilterFactories != null ? charFilterFactories.length : 0][request.text().length];
+            TokenListCreator[] tokenFiltersTokenListCreator = new TokenListCreator[tokenFilterFactories != null ? tokenFilterFactories.length : 0];
+
+            TokenListCreator tokenizerTokenListCreator = new TokenListCreator();
+
+            for (int textIndex = 0; textIndex < request.text().length; textIndex++) {
+                String charFilteredSource = request.text()[textIndex];
+
+                Reader reader = new FastStringReader(charFilteredSource);
+                if (charFilterFactories != null) {
+
+                    for (int charFilterIndex = 0; charFilterIndex < charFilterFactories.length; charFilterIndex++) {
+                        reader = charFilterFactories[charFilterIndex].create(reader);
+                        Reader readerForWriteOut = new FastStringReader(charFilteredSource);
+                        readerForWriteOut = charFilterFactories[charFilterIndex].create(readerForWriteOut);
+                        charFilteredSource = writeCharStream(readerForWriteOut);
+                        charFiltersTexts[charFilterIndex][textIndex] = charFilteredSource;
+                    }
+                }
+
+                // analyzing only tokenizer
+                Tokenizer tokenizer = tokenizerFactory.create();
+                tokenizer.setReader(reader);
+                tokenizerTokenListCreator.analyze(tokenizer, customAnalyzer, field, includeAttributes);
+
+                // analyzing each tokenfilter
+                if (tokenFilterFactories != null) {
+                    for (int tokenFilterIndex = 0; tokenFilterIndex < tokenFilterFactories.length; tokenFilterIndex++) {
+                        if (tokenFiltersTokenListCreator[tokenFilterIndex] == null) {
+                            tokenFiltersTokenListCreator[tokenFilterIndex] = new TokenListCreator();
+                        }
+                        TokenStream stream = createStackedTokenStream(request.text()[textIndex],
+                            charFilterFactories, tokenizerFactory, tokenFilterFactories, tokenFilterIndex + 1);
+                        tokenFiltersTokenListCreator[tokenFilterIndex].analyze(stream, customAnalyzer, field, includeAttributes);
+                    }
+                }
+            }
+
+            DetailAnalyzeResponse.CharFilteredText[] charFilteredLists = new DetailAnalyzeResponse.CharFilteredText[charFiltersTexts.length];
+            if (charFilterFactories != null) {
+                for (int charFilterIndex = 0; charFilterIndex < charFiltersTexts.length; charFilterIndex++) {
+                    charFilteredLists[charFilterIndex] = new DetailAnalyzeResponse.CharFilteredText(
+                        charFilterFactories[charFilterIndex].name(), charFiltersTexts[charFilterIndex]);
+                }
+            }
+            DetailAnalyzeResponse.AnalyzeTokenList[] tokenFilterLists = new DetailAnalyzeResponse.AnalyzeTokenList[tokenFiltersTokenListCreator.length];
+            if (tokenFilterFactories != null) {
+                for (int tokenFilterIndex = 0; tokenFilterIndex < tokenFiltersTokenListCreator.length; tokenFilterIndex++) {
+                    tokenFilterLists[tokenFilterIndex] = new DetailAnalyzeResponse.AnalyzeTokenList(
+                        tokenFilterFactories[tokenFilterIndex].name(), tokenFiltersTokenListCreator[tokenFilterIndex].getArrayTokens());
+                }
+            }
+            detailResponse = new DetailAnalyzeResponse(charFilteredLists, new DetailAnalyzeResponse.AnalyzeTokenList(tokenizerFactory.name(), tokenizerTokenListCreator.getArrayTokens()), tokenFilterLists);
+        } else {
+            String name;
+            if (analyzer instanceof NamedAnalyzer) {
+                name = ((NamedAnalyzer) analyzer).name();
+            } else {
+                name = analyzer.getClass().getName();
+            }
+
+            TokenListCreator tokenListCreator = new TokenListCreator();
+            for (String text : request.text()) {
+                tokenListCreator.analyze(analyzer.tokenStream(field, text), analyzer, field,
+                        includeAttributes);
+            }
+            detailResponse = new DetailAnalyzeResponse(new DetailAnalyzeResponse.AnalyzeTokenList(name, tokenListCreator.getArrayTokens()));
         }
+        return detailResponse;
+    }
+
+    private static TokenStream createStackedTokenStream(String source, CharFilterFactory[] charFilterFactories, TokenizerFactory tokenizerFactory, TokenFilterFactory[] tokenFilterFactories, int current) {
+        Reader reader = new FastStringReader(source);
+        for (CharFilterFactory charFilterFactory : charFilterFactories) {
+            reader = charFilterFactory.create(reader);
+        }
+        Tokenizer tokenizer = tokenizerFactory.create();
+        tokenizer.setReader(reader);
+        TokenStream tokenStream = tokenizer;
+        for (int i = 0; i < current; i++) {
+            tokenStream = tokenFilterFactories[i].create(tokenStream);
+        }
+        return tokenStream;
+    }
+
+    private static String writeCharStream(Reader input) {
+        final int BUFFER_SIZE = 1024;
+        char[] buf = new char[BUFFER_SIZE];
+        int len;
+        StringBuilder sb = new StringBuilder();
+        do {
+            try {
+                len = input.read(buf, 0, BUFFER_SIZE);
+            } catch (IOException e) {
+                throw new ElasticsearchException("failed to analyze (charFiltering)", e);
+            }
+            if (len > 0)
+                sb.append(buf, 0, len);
+        } while (len == BUFFER_SIZE);
+        return sb.toString();
+    }
+
+    private static class TokenListCreator {
+        int lastPosition = -1;
+        int lastOffset = 0;
+        List<AnalyzeResponse.AnalyzeToken> tokens;
+
+        TokenListCreator() {
+            tokens = new ArrayList<>();
+        }
+
+        private void analyze(TokenStream stream, Analyzer analyzer, String field, Set<String> includeAttributes) {
+            try {
+                stream.reset();
+                CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);
+                PositionIncrementAttribute posIncr = stream.addAttribute(PositionIncrementAttribute.class);
+                OffsetAttribute offset = stream.addAttribute(OffsetAttribute.class);
+                TypeAttribute type = stream.addAttribute(TypeAttribute.class);
+
+                while (stream.incrementToken()) {
+                    int increment = posIncr.getPositionIncrement();
+                    if (increment > 0) {
+                        lastPosition = lastPosition + increment;
+                    }
+                    tokens.add(new AnalyzeResponse.AnalyzeToken(term.toString(), lastPosition, lastOffset + offset.startOffset(),
+                        lastOffset +offset.endOffset(), type.type(), extractExtendedAttributes(stream, includeAttributes)));
+
+                }
+                stream.end();
+                lastOffset += offset.endOffset();
+                lastPosition += posIncr.getPositionIncrement();
+
+                lastPosition += analyzer.getPositionIncrementGap(field);
+                lastOffset += analyzer.getOffsetGap(field);
+
+            } catch (IOException e) {
+                throw new ElasticsearchException("failed to analyze", e);
+            } finally {
+                IOUtils.closeWhileHandlingException(stream);
+            }
+        }
+
+        private AnalyzeResponse.AnalyzeToken[] getArrayTokens() {
+            return tokens.toArray(new AnalyzeResponse.AnalyzeToken[tokens.size()]);
+        }
+
+    }
+
+    /**
+     * other attribute extract object.
+     * Extracted object group by AttributeClassName
+     *
+     * @param stream current TokenStream
+     * @param includeAttributes filtering attributes
+     * @return Map&lt;key value&gt;
+     */
+    private static Map<String, Object> extractExtendedAttributes(TokenStream stream, final Set<String> includeAttributes) {
+        final Map<String, Object> extendedAttributes = new TreeMap<>();
+
+        stream.reflectWith(new AttributeReflector() {
+            @Override
+            public void reflect(Class<? extends Attribute> attClass, String key, Object value) {
+                if (CharTermAttribute.class.isAssignableFrom(attClass))
+                    return;
+                if (PositionIncrementAttribute.class.isAssignableFrom(attClass))
+                    return;
+                if (OffsetAttribute.class.isAssignableFrom(attClass))
+                    return;
+                if (TypeAttribute.class.isAssignableFrom(attClass))
+                    return;
+                if (includeAttributes == null || includeAttributes.isEmpty() || includeAttributes.contains(key.toLowerCase(Locale.ROOT))) {
+                    if (value instanceof BytesRef) {
+                        final BytesRef p = (BytesRef) value;
+                        value = p.toString();
+                    }
+                    extendedAttributes.put(key, value);
+                }
+            }
+        });
 
-        return new AnalyzeResponse(tokens);
+        return extendedAttributes;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java
index e454fca..2c25ee3 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java
@@ -31,36 +31,31 @@ import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MetaDataIndexStateService;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
 /**
  * Close index action
  */
-public class TransportCloseIndexAction extends TransportMasterNodeAction<CloseIndexRequest, CloseIndexResponse> {
+public class TransportCloseIndexAction extends TransportMasterNodeAction<CloseIndexRequest, CloseIndexResponse> implements NodeSettingsService.Listener {
 
     private final MetaDataIndexStateService indexStateService;
     private final DestructiveOperations destructiveOperations;
     private volatile boolean closeIndexEnabled;
-    public static final Setting<Boolean> CLUSTER_INDICES_CLOSE_ENABLE_SETTING = Setting.boolSetting("cluster.indices.close.enable", true, true, Setting.Scope.CLUSTER);
+    public static final String SETTING_CLUSTER_INDICES_CLOSE_ENABLE = "cluster.indices.close.enable";
 
     @Inject
     public TransportCloseIndexAction(Settings settings, TransportService transportService, ClusterService clusterService,
                                      ThreadPool threadPool, MetaDataIndexStateService indexStateService,
-                                     ClusterSettings clusterSettings, ActionFilters actionFilters,
+                                     NodeSettingsService nodeSettingsService, ActionFilters actionFilters,
                                      IndexNameExpressionResolver indexNameExpressionResolver, DestructiveOperations destructiveOperations) {
         super(settings, CloseIndexAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, CloseIndexRequest::new);
         this.indexStateService = indexStateService;
         this.destructiveOperations = destructiveOperations;
-        this.closeIndexEnabled = CLUSTER_INDICES_CLOSE_ENABLE_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_INDICES_CLOSE_ENABLE_SETTING, this::setCloseIndexEnabled);
-    }
-
-    private void setCloseIndexEnabled(boolean closeIndexEnabled) {
-        this.closeIndexEnabled = closeIndexEnabled;
+        this.closeIndexEnabled = settings.getAsBoolean(SETTING_CLUSTER_INDICES_CLOSE_ENABLE, true);
+        nodeSettingsService.addListener(this);
     }
 
     @Override
@@ -78,7 +73,7 @@ public class TransportCloseIndexAction extends TransportMasterNodeAction<CloseIn
     protected void doExecute(CloseIndexRequest request, ActionListener<CloseIndexResponse> listener) {
         destructiveOperations.failDestructive(request.indices());
         if (closeIndexEnabled == false) {
-            throw new IllegalStateException("closing indices is disabled - set [" + CLUSTER_INDICES_CLOSE_ENABLE_SETTING.getKey() + ": true] to enable it. NOTE: closed indices still consume a significant amount of diskspace");
+            throw new IllegalStateException("closing indices is disabled - set [" + SETTING_CLUSTER_INDICES_CLOSE_ENABLE + ": true] to enable it. NOTE: closed indices still consume a significant amount of diskspace");
         }
         super.doExecute(request, listener);
     }
@@ -109,4 +104,13 @@ public class TransportCloseIndexAction extends TransportMasterNodeAction<CloseIn
             }
         });
     }
+
+    @Override
+    public void onRefreshSettings(Settings settings) {
+        final boolean enable = settings.getAsBoolean(SETTING_CLUSTER_INDICES_CLOSE_ENABLE, this.closeIndexEnabled);
+        if (enable != this.closeIndexEnabled) {
+            logger.info("updating [{}] from [{}] to [{}]", SETTING_CLUSTER_INDICES_CLOSE_ENABLE, this.closeIndexEnabled, enable);
+            this.closeIndexEnabled = enable;
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java
index 82176da..c02e2ad 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java
@@ -31,6 +31,7 @@ import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
@@ -44,7 +45,8 @@ public class TransportDeleteIndexAction extends TransportMasterNodeAction<Delete
 
     @Inject
     public TransportDeleteIndexAction(Settings settings, TransportService transportService, ClusterService clusterService,
-                                      ThreadPool threadPool, MetaDataDeleteIndexService deleteIndexService, ActionFilters actionFilters,
+                                      ThreadPool threadPool, MetaDataDeleteIndexService deleteIndexService,
+                                      NodeSettingsService nodeSettingsService, ActionFilters actionFilters,
                                       IndexNameExpressionResolver indexNameExpressionResolver, DestructiveOperations destructiveOperations) {
         super(settings, DeleteIndexAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, DeleteIndexRequest::new);
         this.deleteIndexService = deleteIndexService;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java
index 10db46c..ccf06be 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java
@@ -22,6 +22,7 @@ package org.elasticsearch.action.admin.indices.flush;
 import org.elasticsearch.action.support.replication.ReplicationRequest;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.index.shard.ShardId;
 
 import java.io.IOException;
 
@@ -29,8 +30,8 @@ public class ShardFlushRequest extends ReplicationRequest<ShardFlushRequest> {
 
     private FlushRequest request = new FlushRequest();
 
-    public ShardFlushRequest(FlushRequest request) {
-        super(request);
+    public ShardFlushRequest(FlushRequest request, ShardId shardId) {
+        super(request, shardId);
         this.request = request;
     }
 
@@ -53,5 +54,8 @@ public class ShardFlushRequest extends ReplicationRequest<ShardFlushRequest> {
         request.writeTo(out);
     }
 
-
+    @Override
+    public String toString() {
+        return "flush {" + super.toString() + "}";
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java
index ac15962..d2a8f1a 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.flush;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction;
@@ -36,7 +36,7 @@ import java.util.List;
 /**
  * Flush Action.
  */
-public class TransportFlushAction extends TransportBroadcastReplicationAction<FlushRequest, FlushResponse, ShardFlushRequest, ActionWriteResponse> {
+public class TransportFlushAction extends TransportBroadcastReplicationAction<FlushRequest, FlushResponse, ShardFlushRequest, ReplicationResponse> {
 
     @Inject
     public TransportFlushAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
@@ -47,13 +47,13 @@ public class TransportFlushAction extends TransportBroadcastReplicationAction<Fl
     }
 
     @Override
-    protected ActionWriteResponse newShardResponse() {
-        return new ActionWriteResponse();
+    protected ReplicationResponse newShardResponse() {
+        return new ReplicationResponse();
     }
 
     @Override
     protected ShardFlushRequest newShardRequest(FlushRequest request, ShardId shardId) {
-        return new ShardFlushRequest(request).setShardId(shardId);
+        return new ShardFlushRequest(request, shardId);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java
index f768cfe..8f7fce8 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java
@@ -19,22 +19,19 @@
 
 package org.elasticsearch.action.admin.indices.flush;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.replication.TransportReplicationAction;
 import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
-import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.shard.IndexShard;
-import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
@@ -42,7 +39,7 @@ import org.elasticsearch.transport.TransportService;
 /**
  *
  */
-public class TransportShardFlushAction extends TransportReplicationAction<ShardFlushRequest, ShardFlushRequest, ActionWriteResponse> {
+public class TransportShardFlushAction extends TransportReplicationAction<ShardFlushRequest, ShardFlushRequest, ReplicationResponse> {
 
     public static final String NAME = FlushAction.NAME + "[s]";
 
@@ -56,20 +53,20 @@ public class TransportShardFlushAction extends TransportReplicationAction<ShardF
     }
 
     @Override
-    protected ActionWriteResponse newResponseInstance() {
-        return new ActionWriteResponse();
+    protected ReplicationResponse newResponseInstance() {
+        return new ReplicationResponse();
     }
 
     @Override
-    protected Tuple<ActionWriteResponse, ShardFlushRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId.getIndex()).getShard(shardRequest.shardId.id());
-        indexShard.flush(shardRequest.request.getRequest());
+    protected Tuple<ReplicationResponse, ShardFlushRequest> shardOperationOnPrimary(MetaData metaData, ShardFlushRequest shardRequest) throws Throwable {
+        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId().getIndex()).getShard(shardRequest.shardId().id());
+        indexShard.flush(shardRequest.getRequest());
         logger.trace("{} flush request executed on primary", indexShard.shardId());
-        return new Tuple<>(new ActionWriteResponse(), shardRequest.request);
+        return new Tuple<>(new ReplicationResponse(), shardRequest);
     }
 
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, ShardFlushRequest request) {
+    protected void shardOperationOnReplica(ShardFlushRequest request) {
         IndexShard indexShard = indicesService.indexServiceSafe(request.shardId().getIndex()).getShard(request.shardId().id());
         indexShard.flush(request.getRequest());
         logger.trace("{} flush request executed on replica", indexShard.shardId());
@@ -81,18 +78,13 @@ public class TransportShardFlushAction extends TransportReplicationAction<ShardF
     }
 
     @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterState.getRoutingTable().indicesRouting().get(request.concreteIndex()).getShards().get(request.request().shardId().getId()).shardsIt();
+    protected ClusterBlockLevel globalBlockLevel() {
+        return ClusterBlockLevel.METADATA_WRITE;
     }
 
     @Override
-    protected ClusterBlockException checkGlobalBlock(ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
-    }
-
-    @Override
-    protected ClusterBlockException checkRequestBlock(ClusterState state, InternalRequest request) {
-        return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_WRITE, new String[]{request.concreteIndex()});
+    protected ClusterBlockLevel indexBlockLevel() {
+        return ClusterBlockLevel.METADATA_WRITE;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java
index 2717a23..cab1047 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java
@@ -32,9 +32,12 @@ import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MetaDataIndexStateService;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
+import java.util.Arrays;
+
 /**
  * Open index action
  */
@@ -46,7 +49,7 @@ public class TransportOpenIndexAction extends TransportMasterNodeAction<OpenInde
     @Inject
     public TransportOpenIndexAction(Settings settings, TransportService transportService, ClusterService clusterService,
                                     ThreadPool threadPool, MetaDataIndexStateService indexStateService,
-                                    ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver,
+                                    NodeSettingsService nodeSettingsService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver,
                                     DestructiveOperations destructiveOperations) {
         super(settings, OpenIndexAction.NAME, transportService, clusterService, threadPool, actionFilters, indexNameExpressionResolver, OpenIndexRequest::new);
         this.indexStateService = indexStateService;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java
index e2d978d..a76b714 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.refresh;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.replication.ReplicationRequest;
@@ -37,7 +37,7 @@ import java.util.List;
 /**
  * Refresh action.
  */
-public class TransportRefreshAction extends TransportBroadcastReplicationAction<RefreshRequest, RefreshResponse, ReplicationRequest, ActionWriteResponse> {
+public class TransportRefreshAction extends TransportBroadcastReplicationAction<RefreshRequest, RefreshResponse, ReplicationRequest, ReplicationResponse> {
 
     @Inject
     public TransportRefreshAction(Settings settings, ThreadPool threadPool, ClusterService clusterService,
@@ -48,13 +48,13 @@ public class TransportRefreshAction extends TransportBroadcastReplicationAction<
     }
 
     @Override
-    protected ActionWriteResponse newShardResponse() {
-        return new ActionWriteResponse();
+    protected ReplicationResponse newShardResponse() {
+        return new ReplicationResponse();
     }
 
     @Override
     protected ReplicationRequest newShardRequest(RefreshRequest request, ShardId shardId) {
-        return new ReplicationRequest(request).setShardId(shardId);
+        return new ReplicationRequest(request, shardId);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java
index a06483a..c78977f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java
@@ -19,18 +19,16 @@
 
 package org.elasticsearch.action.admin.indices.refresh;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.replication.ReplicationRequest;
 import org.elasticsearch.action.support.replication.TransportReplicationAction;
 import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
-import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -43,7 +41,7 @@ import org.elasticsearch.transport.TransportService;
 /**
  *
  */
-public class TransportShardRefreshAction extends TransportReplicationAction<ReplicationRequest, ReplicationRequest, ActionWriteResponse> {
+public class TransportShardRefreshAction extends TransportReplicationAction<ReplicationRequest, ReplicationRequest, ReplicationResponse> {
 
     public static final String NAME = RefreshAction.NAME + "[s]";
 
@@ -57,20 +55,21 @@ public class TransportShardRefreshAction extends TransportReplicationAction<Repl
     }
 
     @Override
-    protected ActionWriteResponse newResponseInstance() {
-        return new ActionWriteResponse();
+    protected ReplicationResponse newResponseInstance() {
+        return new ReplicationResponse();
     }
 
     @Override
-    protected Tuple<ActionWriteResponse, ReplicationRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId.getIndex()).getShard(shardRequest.shardId.id());
+    protected Tuple<ReplicationResponse, ReplicationRequest> shardOperationOnPrimary(MetaData metaData, ReplicationRequest shardRequest) throws Throwable {
+        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId().getIndex()).getShard(shardRequest.shardId().id());
         indexShard.refresh("api");
         logger.trace("{} refresh request executed on primary", indexShard.shardId());
-        return new Tuple<>(new ActionWriteResponse(), shardRequest.request);
+        return new Tuple<>(new ReplicationResponse(), shardRequest);
     }
 
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, ReplicationRequest request) {
+    protected void shardOperationOnReplica(ReplicationRequest request) {
+        final ShardId shardId = request.shardId();
         IndexShard indexShard = indicesService.indexServiceSafe(shardId.getIndex()).getShard(shardId.id());
         indexShard.refresh("api");
         logger.trace("{} refresh request executed on replica", indexShard.shardId());
@@ -82,18 +81,13 @@ public class TransportShardRefreshAction extends TransportReplicationAction<Repl
     }
 
     @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterState.getRoutingTable().indicesRouting().get(request.concreteIndex()).getShards().get(request.request().shardId().getId()).shardsIt();
+    protected ClusterBlockLevel globalBlockLevel() {
+        return ClusterBlockLevel.METADATA_WRITE;
     }
 
     @Override
-    protected ClusterBlockException checkGlobalBlock(ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.METADATA_WRITE);
-    }
-
-    @Override
-    protected ClusterBlockException checkRequestBlock(ClusterState state, InternalRequest request) {
-        return state.blocks().indicesBlockedException(ClusterBlockLevel.METADATA_WRITE, new String[]{request.concreteIndex()});
+    protected ClusterBlockLevel indexBlockLevel() {
+        return ClusterBlockLevel.METADATA_WRITE;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java
index 80e86ea..b8d0958 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java
@@ -19,14 +19,18 @@
 
 package org.elasticsearch.action.bulk;
 
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.DocWriteResponse;
 import org.elasticsearch.action.delete.DeleteResponse;
 import org.elasticsearch.action.index.IndexResponse;
 import org.elasticsearch.action.update.UpdateResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.xcontent.StatusToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
@@ -35,7 +39,39 @@ import java.io.IOException;
  * Represents a single item response for an action executed as part of the bulk API. Holds the index/type/id
  * of the relevant action, and if it has failed or not (with the failure message incase it failed).
  */
-public class BulkItemResponse implements Streamable {
+public class BulkItemResponse implements Streamable, StatusToXContent {
+
+    @Override
+    public RestStatus status() {
+        return failure == null ? response.status() : failure.getStatus();
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(opType);
+        if (failure == null) {
+            response.toXContent(builder, params);
+            builder.field(Fields.STATUS, response.status().getStatus());
+        } else {
+            builder.field(Fields._INDEX, failure.getIndex());
+            builder.field(Fields._TYPE, failure.getType());
+            builder.field(Fields._ID, failure.getId());
+            builder.field(Fields.STATUS, failure.getStatus().getStatus());
+            builder.startObject(Fields.ERROR);
+            ElasticsearchException.toXContent(builder, params, failure.getCause());
+            builder.endObject();
+        }
+        builder.endObject();
+        return builder;
+    }
+
+    static final class Fields {
+        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
+        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
+        static final XContentBuilderString _ID = new XContentBuilderString("_id");
+        static final XContentBuilderString STATUS = new XContentBuilderString("status");
+        static final XContentBuilderString ERROR = new XContentBuilderString("error");
+    }
 
     /**
      * Represents a failure.
@@ -99,7 +135,7 @@ public class BulkItemResponse implements Streamable {
 
     private String opType;
 
-    private ActionWriteResponse response;
+    private DocWriteResponse response;
 
     private Failure failure;
 
@@ -107,7 +143,7 @@ public class BulkItemResponse implements Streamable {
 
     }
 
-    public BulkItemResponse(int id, String opType, ActionWriteResponse response) {
+    public BulkItemResponse(int id, String opType, DocWriteResponse response) {
         this.id = id;
         this.opType = opType;
         this.response = response;
@@ -140,14 +176,7 @@ public class BulkItemResponse implements Streamable {
         if (failure != null) {
             return failure.getIndex();
         }
-        if (response instanceof IndexResponse) {
-            return ((IndexResponse) response).getIndex();
-        } else if (response instanceof DeleteResponse) {
-            return ((DeleteResponse) response).getIndex();
-        } else if (response instanceof UpdateResponse) {
-            return ((UpdateResponse) response).getIndex();
-        }
-        return null;
+        return response.getIndex();
     }
 
     /**
@@ -157,14 +186,7 @@ public class BulkItemResponse implements Streamable {
         if (failure != null) {
             return failure.getType();
         }
-        if (response instanceof IndexResponse) {
-            return ((IndexResponse) response).getType();
-        } else if (response instanceof DeleteResponse) {
-            return ((DeleteResponse) response).getType();
-        } else if (response instanceof UpdateResponse) {
-            return ((UpdateResponse) response).getType();
-        }
-        return null;
+        return response.getType();
     }
 
     /**
@@ -174,14 +196,7 @@ public class BulkItemResponse implements Streamable {
         if (failure != null) {
             return failure.getId();
         }
-        if (response instanceof IndexResponse) {
-            return ((IndexResponse) response).getId();
-        } else if (response instanceof DeleteResponse) {
-            return ((DeleteResponse) response).getId();
-        } else if (response instanceof UpdateResponse) {
-            return ((UpdateResponse) response).getId();
-        }
-        return null;
+        return response.getId();
     }
 
     /**
@@ -191,21 +206,14 @@ public class BulkItemResponse implements Streamable {
         if (failure != null) {
             return -1;
         }
-        if (response instanceof IndexResponse) {
-            return ((IndexResponse) response).getVersion();
-        } else if (response instanceof DeleteResponse) {
-            return ((DeleteResponse) response).getVersion();
-        } else if (response instanceof UpdateResponse) {
-            return ((UpdateResponse) response).getVersion();
-        }
-        return -1;
+        return response.getVersion();
     }
 
     /**
      * The actual response ({@link IndexResponse} or {@link DeleteResponse}). <tt>null</tt> in
      * case of failure.
      */
-    public <T extends ActionWriteResponse> T getResponse() {
+    public <T extends DocWriteResponse> T getResponse() {
         return (T) response;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
index ec15038..1edba16 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java
@@ -40,10 +40,8 @@ public class BulkShardRequest extends ReplicationRequest<BulkShardRequest> {
     public BulkShardRequest() {
     }
 
-    BulkShardRequest(BulkRequest bulkRequest, String index, int shardId, boolean refresh, BulkItemRequest[] items) {
-        super(bulkRequest);
-        this.index = index;
-        this.setShardId(new ShardId(index, shardId));
+    BulkShardRequest(BulkRequest bulkRequest, ShardId shardId, boolean refresh, BulkItemRequest[] items) {
+        super(bulkRequest, shardId);
         this.items = items;
         this.refresh = refresh;
     }
@@ -93,4 +91,9 @@ public class BulkShardRequest extends ReplicationRequest<BulkShardRequest> {
         }
         refresh = in.readBoolean();
     }
+
+    @Override
+    public String toString() {
+        return "shard bulk {" + super.toString() + "}";
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java
index 6b08627..76c80a9 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.action.bulk;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.index.shard.ShardId;
@@ -29,7 +29,7 @@ import java.io.IOException;
 /**
  *
  */
-public class BulkShardResponse extends ActionWriteResponse {
+public class BulkShardResponse extends ReplicationResponse {
 
     private ShardId shardId;
     private BulkItemResponse[] responses;
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
index 51d32e3..9b18d03 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java
@@ -275,7 +275,7 @@ public class TransportBulkAction extends HandledTransportAction<BulkRequest, Bul
                         list.add(new BulkItemRequest(i, new DeleteRequest(deleteRequest)));
                     }
                 } else {
-                    ShardId shardId = clusterService.operationRouting().deleteShards(clusterState, concreteIndex, deleteRequest.type(), deleteRequest.id(), deleteRequest.routing()).shardId();
+                    ShardId shardId = clusterService.operationRouting().indexShards(clusterState, concreteIndex, deleteRequest.type(), deleteRequest.id(), deleteRequest.routing()).shardId();
                     List<BulkItemRequest> list = requestsByShard.get(shardId);
                     if (list == null) {
                         list = new ArrayList<>();
@@ -312,7 +312,7 @@ public class TransportBulkAction extends HandledTransportAction<BulkRequest, Bul
         for (Map.Entry<ShardId, List<BulkItemRequest>> entry : requestsByShard.entrySet()) {
             final ShardId shardId = entry.getKey();
             final List<BulkItemRequest> requests = entry.getValue();
-            BulkShardRequest bulkShardRequest = new BulkShardRequest(bulkRequest, shardId.index().name(), shardId.id(), bulkRequest.refresh(), requests.toArray(new BulkItemRequest[requests.size()]));
+            BulkShardRequest bulkShardRequest = new BulkShardRequest(bulkRequest, shardId, bulkRequest.refresh(), requests.toArray(new BulkItemRequest[requests.size()]));
             bulkShardRequest.consistencyLevel(bulkRequest.consistencyLevel());
             bulkShardRequest.timeout(bulkRequest.timeout());
             shardBulkAction.execute(bulkShardRequest, new ActionListener<BulkShardResponse>() {
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
index e51a1b9..2597695 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java
@@ -35,12 +35,11 @@ import org.elasticsearch.action.update.UpdateHelper;
 import org.elasticsearch.action.update.UpdateRequest;
 import org.elasticsearch.action.update.UpdateResponse;
 import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
-import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
@@ -88,11 +87,6 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
     }
 
     @Override
-    protected boolean checkWriteConsistency() {
-        return true;
-    }
-
-    @Override
     protected TransportRequestOptions transportOptions() {
         return BulkAction.INSTANCE.transportOptions(settings);
     }
@@ -108,15 +102,9 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
     }
 
     @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterState.routingTable().index(request.concreteIndex()).shard(request.request().shardId().id()).shardsIt();
-    }
-
-    @Override
-    protected Tuple<BulkShardResponse, BulkShardRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) {
-        final BulkShardRequest request = shardRequest.request;
+    protected Tuple<BulkShardResponse, BulkShardRequest> shardOperationOnPrimary(MetaData metaData, BulkShardRequest request) {
         final IndexService indexService = indicesService.indexServiceSafe(request.index());
-        final IndexShard indexShard = indexService.getShard(shardRequest.shardId.id());
+        final IndexShard indexShard = indexService.getShard(request.shardId().id());
 
         long[] preVersions = new long[request.items().length];
         VersionType[] preVersionTypes = new VersionType[request.items().length];
@@ -128,7 +116,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                 preVersions[requestIndex] = indexRequest.version();
                 preVersionTypes[requestIndex] = indexRequest.versionType();
                 try {
-                    WriteResult<IndexResponse> result = shardIndexOperation(request, indexRequest, clusterState, indexShard, true);
+                    WriteResult<IndexResponse> result = shardIndexOperation(request, indexRequest, metaData, indexShard, true);
                     location = locationToSync(location, result.location);
                     // add the response
                     IndexResponse indexResponse = result.response();
@@ -143,9 +131,9 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                         throw (ElasticsearchException) e;
                     }
                     if (ExceptionsHelper.status(e) == RestStatus.CONFLICT) {
-                        logger.trace("{} failed to execute bulk item (index) {}", e, shardRequest.shardId, indexRequest);
+                        logger.trace("{} failed to execute bulk item (index) {}", e, request.shardId(), indexRequest);
                     } else {
-                        logger.debug("{} failed to execute bulk item (index) {}", e, shardRequest.shardId, indexRequest);
+                        logger.debug("{} failed to execute bulk item (index) {}", e, request.shardId(), indexRequest);
                     }
                     // if its a conflict failure, and we already executed the request on a primary (and we execute it
                     // again, due to primary relocation and only processing up to N bulk items when the shard gets closed)
@@ -178,9 +166,9 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                         throw (ElasticsearchException) e;
                     }
                     if (ExceptionsHelper.status(e) == RestStatus.CONFLICT) {
-                        logger.trace("{} failed to execute bulk item (delete) {}", e, shardRequest.shardId, deleteRequest);
+                        logger.trace("{} failed to execute bulk item (delete) {}", e, request.shardId(), deleteRequest);
                     } else {
-                        logger.debug("{} failed to execute bulk item (delete) {}", e, shardRequest.shardId, deleteRequest);
+                        logger.debug("{} failed to execute bulk item (delete) {}", e, request.shardId(), deleteRequest);
                     }
                     // if its a conflict failure, and we already executed the request on a primary (and we execute it
                     // again, due to primary relocation and only processing up to N bulk items when the shard gets closed)
@@ -200,7 +188,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                 for (int updateAttemptsCount = 0; updateAttemptsCount <= updateRequest.retryOnConflict(); updateAttemptsCount++) {
                     UpdateResult updateResult;
                     try {
-                        updateResult = shardUpdateOperation(clusterState, request, updateRequest, indexShard);
+                        updateResult = shardUpdateOperation(metaData, request, updateRequest, indexShard);
                     } catch (Throwable t) {
                         updateResult = new UpdateResult(null, null, false, t, null);
                     }
@@ -216,10 +204,10 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                                 BytesReference indexSourceAsBytes = indexRequest.source();
                                 // add the response
                                 IndexResponse indexResponse = result.response();
-                                UpdateResponse updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getIndex(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.isCreated());
+                                UpdateResponse updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getShardId(), indexResponse.getType(), indexResponse.getId(), indexResponse.getVersion(), indexResponse.isCreated());
                                 if (updateRequest.fields() != null && updateRequest.fields().length > 0) {
                                     Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(indexSourceAsBytes, true);
-                                    updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, shardRequest.request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes));
+                                    updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes));
                                 }
                                 item = request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), indexRequest);
                                 setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE, updateResponse));
@@ -228,8 +216,8 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                                 WriteResult<DeleteResponse> writeResult = updateResult.writeResult;
                                 DeleteResponse response = writeResult.response();
                                 DeleteRequest deleteRequest = updateResult.request();
-                                updateResponse = new UpdateResponse(response.getShardInfo(), response.getIndex(), response.getType(), response.getId(), response.getVersion(), false);
-                                updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, shardRequest.request.index(), response.getVersion(), updateResult.result.updatedSourceAsMap(), updateResult.result.updateSourceContentType(), null));
+                                updateResponse = new UpdateResponse(response.getShardInfo(), response.getShardId(), response.getType(), response.getId(), response.getVersion(), false);
+                                updateResponse.setGetResult(updateHelper.extractGetResult(updateRequest, request.index(), response.getVersion(), updateResult.result.updatedSourceAsMap(), updateResult.result.updateSourceContentType(), null));
                                 // Replace the update request to the translated delete request to execute on the replica.
                                 item = request.items()[requestIndex] = new BulkItemRequest(request.items()[requestIndex].id(), deleteRequest);
                                 setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE, updateResponse));
@@ -264,16 +252,16 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                             if (item.getPrimaryResponse() != null && isConflictException(t)) {
                                 setResponse(item, item.getPrimaryResponse());
                             } else if (updateResult.result == null) {
-                                setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE, new BulkItemResponse.Failure(shardRequest.request.index(), updateRequest.type(), updateRequest.id(), t)));
+                                setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE, new BulkItemResponse.Failure(request.index(), updateRequest.type(), updateRequest.id(), t)));
                             } else {
                                 switch (updateResult.result.operation()) {
                                     case UPSERT:
                                     case INDEX:
                                         IndexRequest indexRequest = updateResult.request();
                                         if (ExceptionsHelper.status(t) == RestStatus.CONFLICT) {
-                                            logger.trace("{} failed to execute bulk item (index) {}", t, shardRequest.shardId, indexRequest);
+                                            logger.trace("{} failed to execute bulk item (index) {}", t, request.shardId(), indexRequest);
                                         } else {
-                                            logger.debug("{} failed to execute bulk item (index) {}", t, shardRequest.shardId, indexRequest);
+                                            logger.debug("{} failed to execute bulk item (index) {}", t, request.shardId(), indexRequest);
                                         }
                                         setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_UPDATE,
                                                 new BulkItemResponse.Failure(request.index(), indexRequest.type(), indexRequest.id(), t)));
@@ -281,9 +269,9 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
                                     case DELETE:
                                         DeleteRequest deleteRequest = updateResult.request();
                                         if (ExceptionsHelper.status(t) == RestStatus.CONFLICT) {
-                                            logger.trace("{} failed to execute bulk item (delete) {}", t, shardRequest.shardId, deleteRequest);
+                                            logger.trace("{} failed to execute bulk item (delete) {}", t, request.shardId(), deleteRequest);
                                         } else {
-                                            logger.debug("{} failed to execute bulk item (delete) {}", t, shardRequest.shardId, deleteRequest);
+                                            logger.debug("{} failed to execute bulk item (delete) {}", t, request.shardId(), deleteRequest);
                                         }
                                         setResponse(item, new BulkItemResponse(item.id(), OP_TYPE_DELETE,
                                                 new BulkItemResponse.Failure(request.index(), deleteRequest.type(), deleteRequest.id(), t)));
@@ -310,7 +298,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
         for (int i = 0; i < items.length; i++) {
             responses[i] = items[i].getPrimaryResponse();
         }
-        return new Tuple<>(new BulkShardResponse(shardRequest.shardId, responses), shardRequest.request);
+        return new Tuple<>(new BulkShardResponse(request.shardId(), responses), request);
     }
 
     private void setResponse(BulkItemRequest request, BulkItemResponse response) {
@@ -320,11 +308,11 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
         }
     }
 
-    private WriteResult<IndexResponse> shardIndexOperation(BulkShardRequest request, IndexRequest indexRequest, ClusterState clusterState,
+    private WriteResult shardIndexOperation(BulkShardRequest request, IndexRequest indexRequest, MetaData metaData,
                                             IndexShard indexShard, boolean processed) throws Throwable {
 
         // validate, if routing is required, that we got routing
-        MappingMetaData mappingMd = clusterState.metaData().index(request.index()).mappingOrDefault(indexRequest.type());
+        MappingMetaData mappingMd = metaData.index(request.index()).mappingOrDefault(indexRequest.type());
         if (mappingMd != null && mappingMd.routing().required()) {
             if (indexRequest.routing() == null) {
                 throw new RoutingMissingException(request.index(), indexRequest.type(), indexRequest.id());
@@ -332,7 +320,7 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
         }
 
         if (!processed) {
-            indexRequest.process(clusterState.metaData(), mappingMd, allowIdGeneration, request.index());
+            indexRequest.process(metaData, mappingMd, allowIdGeneration, request.index());
         }
         return TransportIndexAction.executeIndexRequestOnPrimary(indexRequest, indexShard, mappingUpdatedAction);
     }
@@ -390,14 +378,14 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
 
     }
 
-    private UpdateResult shardUpdateOperation(ClusterState clusterState, BulkShardRequest bulkShardRequest, UpdateRequest updateRequest, IndexShard indexShard) {
+    private UpdateResult shardUpdateOperation(MetaData metaData, BulkShardRequest bulkShardRequest, UpdateRequest updateRequest, IndexShard indexShard) {
         UpdateHelper.Result translate = updateHelper.prepare(updateRequest, indexShard);
         switch (translate.operation()) {
             case UPSERT:
             case INDEX:
                 IndexRequest indexRequest = translate.action();
                 try {
-                    WriteResult result = shardIndexOperation(bulkShardRequest, indexRequest, clusterState, indexShard, false);
+                    WriteResult result = shardIndexOperation(bulkShardRequest, indexRequest, metaData, indexShard, false);
                     return new UpdateResult(translate, indexRequest, result);
                 } catch (Throwable t) {
                     t = ExceptionsHelper.unwrapCause(t);
@@ -431,7 +419,8 @@ public class TransportShardBulkAction extends TransportReplicationAction<BulkSha
 
 
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, BulkShardRequest request) {
+    protected void shardOperationOnReplica(BulkShardRequest request) {
+        final ShardId shardId = request.shardId();
         IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());
         IndexShard indexShard = indexService.getShard(shardId.id());
         Translog.Location location = null;
diff --git a/core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java b/core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java
index 26cfa57..5778154 100644
--- a/core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/delete/DeleteResponse.java
@@ -19,9 +19,13 @@
 
 package org.elasticsearch.action.delete;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.DocWriteResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
 
@@ -31,53 +35,19 @@ import java.io.IOException;
  * @see org.elasticsearch.action.delete.DeleteRequest
  * @see org.elasticsearch.client.Client#delete(DeleteRequest)
  */
-public class DeleteResponse extends ActionWriteResponse {
+public class DeleteResponse extends DocWriteResponse {
 
-    private String index;
-    private String id;
-    private String type;
-    private long version;
     private boolean found;
 
     public DeleteResponse() {
 
     }
 
-    public DeleteResponse(String index, String type, String id, long version, boolean found) {
-        this.index = index;
-        this.id = id;
-        this.type = type;
-        this.version = version;
+    public DeleteResponse(ShardId shardId, String type, String id, long version, boolean found) {
+        super(shardId, type, id, version);
         this.found = found;
     }
 
-    /**
-     * The index the document was deleted from.
-     */
-    public String getIndex() {
-        return this.index;
-    }
-
-    /**
-     * The type of the document deleted.
-     */
-    public String getType() {
-        return this.type;
-    }
-
-    /**
-     * The id of the document deleted.
-     */
-    public String getId() {
-        return this.id;
-    }
-
-    /**
-     * The version of the delete operation.
-     */
-    public long getVersion() {
-        return this.version;
-    }
 
     /**
      * Returns <tt>true</tt> if a doc was found to delete.
@@ -89,20 +59,44 @@ public class DeleteResponse extends ActionWriteResponse {
     @Override
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
-        index = in.readString();
-        type = in.readString();
-        id = in.readString();
-        version = in.readLong();
         found = in.readBoolean();
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeString(index);
-        out.writeString(type);
-        out.writeString(id);
-        out.writeLong(version);
         out.writeBoolean(found);
     }
+
+    @Override
+    public RestStatus status() {
+        if (found == false) {
+            return RestStatus.NOT_FOUND;
+        }
+        return super.status();
+    }
+
+    static final class Fields {
+        static final XContentBuilderString FOUND = new XContentBuilderString("found");
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.field(Fields.FOUND, isFound());
+        super.toXContent(builder, params);
+        return builder;
+    }
+
+    @Override
+    public String toString() {
+        StringBuilder builder = new StringBuilder();
+        builder.append("DeleteResponse[");
+        builder.append("index=").append(getIndex());
+        builder.append(",type=").append(getType());
+        builder.append(",id=").append(getId());
+        builder.append(",version=").append(getVersion());
+        builder.append(",found=").append(found);
+        builder.append(",shards=").append(getShardInfo());
+        return builder.append("]").toString();
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java b/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
index dc81992..ca66b28 100644
--- a/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
+++ b/core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java
@@ -34,7 +34,7 @@ import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
-import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -94,22 +94,24 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
     }
 
     @Override
-    protected void resolveRequest(final ClusterState state, final InternalRequest request, final ActionListener<DeleteResponse> listener) {
-        request.request().routing(state.metaData().resolveIndexRouting(request.request().routing(), request.request().index()));
-        if (state.metaData().hasIndex(request.concreteIndex())) {
+    protected void resolveRequest(final MetaData metaData, String concreteIndex, DeleteRequest request) {
+        request.routing(metaData.resolveIndexRouting(request.routing(), request.index()));
+        if (metaData.hasIndex(concreteIndex)) {
             // check if routing is required, if so, do a broadcast delete
-            MappingMetaData mappingMd = state.metaData().index(request.concreteIndex()).mappingOrDefault(request.request().type());
+            MappingMetaData mappingMd = metaData.index(concreteIndex).mappingOrDefault(request.type());
             if (mappingMd != null && mappingMd.routing().required()) {
-                if (request.request().routing() == null) {
-                    if (request.request().versionType() != VersionType.INTERNAL) {
+                if (request.routing() == null) {
+                    if (request.versionType() != VersionType.INTERNAL) {
                         // TODO: implement this feature
-                        throw new IllegalArgumentException("routing value is required for deleting documents of type [" + request.request().type()
-                                + "] while using version_type [" + request.request().versionType() + "]");
+                        throw new IllegalArgumentException("routing value is required for deleting documents of type [" + request.type()
+                                + "] while using version_type [" + request.versionType() + "]");
                     }
-                    throw new RoutingMissingException(request.concreteIndex(), request.request().type(), request.request().id());
+                    throw new RoutingMissingException(concreteIndex, request.type(), request.id());
                 }
             }
         }
+        ShardId shardId = clusterService.operationRouting().shardId(clusterService.state(), concreteIndex, request.id(), request.routing());
+        request.setShardId(shardId);
     }
 
     private void innerExecute(final DeleteRequest request, final ActionListener<DeleteResponse> listener) {
@@ -117,22 +119,16 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
     }
 
     @Override
-    protected boolean checkWriteConsistency() {
-        return true;
-    }
-
-    @Override
     protected DeleteResponse newResponseInstance() {
         return new DeleteResponse();
     }
 
     @Override
-    protected Tuple<DeleteResponse, DeleteRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) {
-        DeleteRequest request = shardRequest.request;
-        IndexShard indexShard = indicesService.indexServiceSafe(shardRequest.shardId.getIndex()).getShard(shardRequest.shardId.id());
+    protected Tuple<DeleteResponse, DeleteRequest> shardOperationOnPrimary(MetaData metaData, DeleteRequest request) {
+        IndexShard indexShard = indicesService.indexServiceSafe(request.shardId().getIndex()).getShard(request.shardId().id());
         final WriteResult<DeleteResponse> result = executeDeleteRequestOnPrimary(request, indexShard);
         processAfterWrite(request.refresh(), indexShard, result.location);
-        return new Tuple<>(result.response, shardRequest.request);
+        return new Tuple<>(result.response, request);
     }
 
     public static WriteResult<DeleteResponse> executeDeleteRequestOnPrimary(DeleteRequest request, IndexShard indexShard) {
@@ -144,7 +140,7 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
 
         assert request.versionType().validateVersionForWrites(request.version());
         return new WriteResult<>(
-            new DeleteResponse(indexShard.shardId().getIndex(), request.type(), request.id(), delete.version(), delete.found()),
+            new DeleteResponse(indexShard.shardId(), request.type(), request.id(), delete.version(), delete.found()),
             delete.getTranslogLocation());
     }
 
@@ -154,17 +150,12 @@ public class TransportDeleteAction extends TransportReplicationAction<DeleteRequ
         return delete;
     }
 
-
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, DeleteRequest request) {
+    protected void shardOperationOnReplica(DeleteRequest request) {
+        final ShardId shardId = request.shardId();
         IndexShard indexShard = indicesService.indexServiceSafe(shardId.getIndex()).getShard(shardId.id());
         Engine.Delete delete = executeDeleteRequestOnReplica(request, indexShard);
         processAfterWrite(request.refresh(), indexShard, delete.getTranslogLocation());
     }
 
-    @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterService.operationRouting()
-                .deleteShards(clusterService.state(), request.concreteIndex(), request.request().type(), request.request().id(), request.request().routing());
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/index/IndexResponse.java b/core/src/main/java/org/elasticsearch/action/index/IndexResponse.java
index 5727b2b..665327a 100644
--- a/core/src/main/java/org/elasticsearch/action/index/IndexResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/index/IndexResponse.java
@@ -19,9 +19,13 @@
 
 package org.elasticsearch.action.index;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.DocWriteResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
 
@@ -31,55 +35,20 @@ import java.io.IOException;
  * @see org.elasticsearch.action.index.IndexRequest
  * @see org.elasticsearch.client.Client#index(IndexRequest)
  */
-public class IndexResponse extends ActionWriteResponse {
+public class IndexResponse extends DocWriteResponse {
 
-    private String index;
-    private String id;
-    private String type;
-    private long version;
     private boolean created;
 
     public IndexResponse() {
 
     }
 
-    public IndexResponse(String index, String type, String id, long version, boolean created) {
-        this.index = index;
-        this.id = id;
-        this.type = type;
-        this.version = version;
+    public IndexResponse(ShardId shardId, String type, String id, long version, boolean created) {
+        super(shardId, type, id, version);
         this.created = created;
     }
 
     /**
-     * The index the document was indexed into.
-     */
-    public String getIndex() {
-        return this.index;
-    }
-
-    /**
-     * The type of the document indexed.
-     */
-    public String getType() {
-        return this.type;
-    }
-
-    /**
-     * The id of the document indexed.
-     */
-    public String getId() {
-        return this.id;
-    }
-
-    /**
-     * Returns the current version of the doc indexed.
-     */
-    public long getVersion() {
-        return this.version;
-    }
-
-    /**
      * Returns true if the document was created, false if updated.
      */
     public boolean isCreated() {
@@ -87,22 +56,22 @@ public class IndexResponse extends ActionWriteResponse {
     }
 
     @Override
+    public RestStatus status() {
+        if (created) {
+            return RestStatus.CREATED;
+        }
+        return super.status();
+    }
+
+    @Override
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
-        index = in.readString();
-        type = in.readString();
-        id = in.readString();
-        version = in.readLong();
         created = in.readBoolean();
     }
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeString(index);
-        out.writeString(type);
-        out.writeString(id);
-        out.writeLong(version);
         out.writeBoolean(created);
     }
 
@@ -110,12 +79,23 @@ public class IndexResponse extends ActionWriteResponse {
     public String toString() {
         StringBuilder builder = new StringBuilder();
         builder.append("IndexResponse[");
-        builder.append("index=").append(index);
-        builder.append(",type=").append(type);
-        builder.append(",id=").append(id);
-        builder.append(",version=").append(version);
+        builder.append("index=").append(getIndex());
+        builder.append(",type=").append(getType());
+        builder.append(",id=").append(getId());
+        builder.append(",version=").append(getVersion());
         builder.append(",created=").append(created);
         builder.append(",shards=").append(getShardInfo());
         return builder.append("]").toString();
     }
+
+    static final class Fields {
+        static final XContentBuilderString CREATED = new XContentBuilderString("created");
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        super.toXContent(builder, params);
+        builder.field(Fields.CREATED, isCreated());
+        return builder;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
index 6fb7ab6..620056d 100644
--- a/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java
@@ -36,7 +36,6 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.ShardIterator;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -120,14 +119,14 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
     }
 
     @Override
-    protected void resolveRequest(ClusterState state, InternalRequest request, ActionListener<IndexResponse> indexResponseActionListener) {
-        MetaData metaData = clusterService.state().metaData();
-
+    protected void resolveRequest(MetaData metaData, String concreteIndex, IndexRequest request) {
         MappingMetaData mappingMd = null;
-        if (metaData.hasIndex(request.concreteIndex())) {
-            mappingMd = metaData.index(request.concreteIndex()).mappingOrDefault(request.request().type());
+        if (metaData.hasIndex(concreteIndex)) {
+            mappingMd = metaData.index(concreteIndex).mappingOrDefault(request.type());
         }
-        request.request().process(metaData, mappingMd, allowIdGeneration, request.concreteIndex());
+        request.process(metaData, mappingMd, allowIdGeneration, concreteIndex);
+        ShardId shardId = clusterService.operationRouting().shardId(clusterService.state(), concreteIndex, request.id(), request.routing());
+        request.setShardId(shardId);
     }
 
     private void innerExecute(final IndexRequest request, final ActionListener<IndexResponse> listener) {
@@ -135,47 +134,36 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
     }
 
     @Override
-    protected boolean checkWriteConsistency() {
-        return true;
-    }
-
-    @Override
     protected IndexResponse newResponseInstance() {
         return new IndexResponse();
     }
 
     @Override
-    protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-        return clusterService.operationRouting()
-                .indexShards(clusterService.state(), request.concreteIndex(), request.request().type(), request.request().id(), request.request().routing());
-    }
-
-    @Override
-    protected Tuple<IndexResponse, IndexRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-        final IndexRequest request = shardRequest.request;
+    protected Tuple<IndexResponse, IndexRequest> shardOperationOnPrimary(MetaData metaData, IndexRequest request) throws Throwable {
 
         // validate, if routing is required, that we got routing
-        IndexMetaData indexMetaData = clusterState.metaData().index(shardRequest.shardId.getIndex());
+        IndexMetaData indexMetaData = metaData.index(request.shardId().getIndex());
         MappingMetaData mappingMd = indexMetaData.mappingOrDefault(request.type());
         if (mappingMd != null && mappingMd.routing().required()) {
             if (request.routing() == null) {
-                throw new RoutingMissingException(shardRequest.shardId.getIndex(), request.type(), request.id());
+                throw new RoutingMissingException(request.shardId().getIndex(), request.type(), request.id());
             }
         }
 
-        IndexService indexService = indicesService.indexServiceSafe(shardRequest.shardId.getIndex());
-        IndexShard indexShard = indexService.getShard(shardRequest.shardId.id());
+        IndexService indexService = indicesService.indexServiceSafe(request.shardId().getIndex());
+        IndexShard indexShard = indexService.getShard(request.shardId().id());
 
         final WriteResult<IndexResponse> result = executeIndexRequestOnPrimary(request, indexShard, mappingUpdatedAction);
 
         final IndexResponse response = result.response;
         final Translog.Location location = result.location;
         processAfterWrite(request.refresh(), indexShard, location);
-        return new Tuple<>(response, shardRequest.request);
+        return new Tuple<>(response, request);
     }
 
     @Override
-    protected void shardOperationOnReplica(ShardId shardId, IndexRequest request) {
+    protected void shardOperationOnReplica(IndexRequest request) {
+        final ShardId shardId = request.shardId();
         IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());
         IndexShard indexShard = indexService.getShard(shardId.id());
         final Engine.Index operation = executeIndexRequestOnReplica(request, indexShard);
@@ -234,7 +222,7 @@ public class TransportIndexAction extends TransportReplicationAction<IndexReques
 
         assert request.versionType().validateVersionForWrites(request.version());
 
-        return new WriteResult<>(new IndexResponse(shardId.getIndex(), request.type(), request.id(), request.version(), created), operation.getTranslogLocation());
+        return new WriteResult<>(new IndexResponse(shardId, request.type(), request.id(), request.version(), created), operation.getTranslogLocation());
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
index 967d5a0..79f51db 100644
--- a/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java
@@ -238,17 +238,7 @@ public class PercolateSourceBuilder extends ToXContentToBytes {
 
         @Override
         public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            XContentType contentType = XContentFactory.xContentType(doc);
-            if (contentType == builder.contentType()) {
-                builder.rawField("doc", doc);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(doc)) {
-                    parser.nextToken();
-                    builder.field("doc");
-                    builder.copyCurrentStructure(parser);
-                }
-            }
-            return builder;
+            return builder.rawField("doc", doc);
         }
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/support/DestructiveOperations.java b/core/src/main/java/org/elasticsearch/action/support/DestructiveOperations.java
index 5f2fb33..b73ee8a 100644
--- a/core/src/main/java/org/elasticsearch/action/support/DestructiveOperations.java
+++ b/core/src/main/java/org/elasticsearch/action/support/DestructiveOperations.java
@@ -21,30 +21,25 @@ package org.elasticsearch.action.support;
 
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 /**
  * Helper for dealing with destructive operations and wildcard usage.
  */
-public final class DestructiveOperations extends AbstractComponent {
+public final class DestructiveOperations extends AbstractComponent implements NodeSettingsService.Listener {
 
     /**
      * Setting which controls whether wildcard usage (*, prefix*, _all) is allowed.
      */
-    public static final Setting<Boolean> REQUIRES_NAME_SETTING = Setting.boolSetting("action.destructive_requires_name", false, true, Setting.Scope.CLUSTER);
+    public static final String REQUIRES_NAME = "action.destructive_requires_name";
     private volatile boolean destructiveRequiresName;
 
     @Inject
-    public DestructiveOperations(Settings settings, ClusterSettings clusterSettings) {
+    public DestructiveOperations(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        destructiveRequiresName = REQUIRES_NAME_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(REQUIRES_NAME_SETTING, this::setDestructiveRequiresName);
-    }
-
-    private void setDestructiveRequiresName(boolean destructiveRequiresName) {
-        this.destructiveRequiresName = destructiveRequiresName;
+        destructiveRequiresName = settings.getAsBoolean(DestructiveOperations.REQUIRES_NAME, false);
+        nodeSettingsService.addListener(this);
     }
 
     /**
@@ -70,6 +65,15 @@ public final class DestructiveOperations extends AbstractComponent {
         }
     }
 
+    @Override
+    public void onRefreshSettings(Settings settings) {
+        boolean newValue = settings.getAsBoolean(DestructiveOperations.REQUIRES_NAME, destructiveRequiresName);
+        if (destructiveRequiresName != newValue) {
+            logger.info("updating [action.operate_all_indices] from [{}] to [{}]", destructiveRequiresName, newValue);
+            this.destructiveRequiresName = newValue;
+        }
+    }
+
     private static boolean hasWildcardUsage(String aliasOrIndex) {
         return "_all".equals(aliasOrIndex) || aliasOrIndex.indexOf('*') != -1;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java b/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
index c629a70..adbe199 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java
@@ -42,7 +42,12 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
 
     public static final TimeValue DEFAULT_TIMEOUT = new TimeValue(1, TimeUnit.MINUTES);
 
-    ShardId internalShardId;
+    /**
+     * Target shard the request should execute on. In case of index and delete requests,
+     * shard id gets resolved by the transport action before performing request operation
+     * and at request creation time for shard-level bulk, refresh and flush requests.
+     */
+    protected ShardId shardId;
 
     protected TimeValue timeout = DEFAULT_TIMEOUT;
     protected String index;
@@ -61,6 +66,15 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
     }
 
     /**
+     * Creates a new request with resolved shard id
+     */
+    public ReplicationRequest(ActionRequest request, ShardId shardId) {
+        super(request);
+        this.index = shardId.getIndex();
+        this.shardId = shardId;
+    }
+
+    /**
      * Copy constructor that creates a new request that is a copy of the one provided as an argument.
      */
     protected ReplicationRequest(T request) {
@@ -124,12 +138,12 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
 
     /**
      * @return the shardId of the shard where this operation should be executed on.
-     * can be null in case the shardId is determined by a single document (index, type, id) for example for index or delete request.
+     * can be null if the shardID has not yet been resolved
      */
     public
     @Nullable
     ShardId shardId() {
-        return internalShardId;
+        return shardId;
     }
 
     /**
@@ -154,9 +168,9 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
         if (in.readBoolean()) {
-            internalShardId = ShardId.readShardId(in);
+            shardId = ShardId.readShardId(in);
         } else {
-            internalShardId = null;
+            shardId = null;
         }
         consistencyLevel = WriteConsistencyLevel.fromId(in.readByte());
         timeout = TimeValue.readTimeValue(in);
@@ -166,9 +180,9 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        if (internalShardId != null) {
+        if (shardId != null) {
             out.writeBoolean(true);
-            internalShardId.writeTo(out);
+            shardId.writeTo(out);
         } else {
             out.writeBoolean(false);
         }
@@ -177,9 +191,21 @@ public class ReplicationRequest<T extends ReplicationRequest> extends ActionRequ
         out.writeString(index);
     }
 
+    /**
+     * Sets the target shard id for the request. The shard id is set when a
+     * index/delete request is resolved by the transport action
+     */
     public T setShardId(ShardId shardId) {
-        this.internalShardId = shardId;
-        this.index = shardId.getIndex();
+        this.shardId = shardId;
         return (T) this;
     }
+
+    @Override
+    public String toString() {
+        if (shardId != null) {
+            return shardId.toString();
+        } else {
+            return index;
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java
index ddd4d42..33a9d34 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java
@@ -22,9 +22,8 @@ package org.elasticsearch.action.support.replication;
 import com.carrotsearch.hppc.cursors.IntObjectCursor;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.ShardOperationFailedException;
-import org.elasticsearch.action.UnavailableShardsException;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.DefaultShardOperationFailedException;
 import org.elasticsearch.action.support.HandledTransportAction;
@@ -53,7 +52,7 @@ import java.util.function.Supplier;
  * Base class for requests that should be executed on all shards of an index or several indices.
  * This action sends shard requests to all primary shards of the indices and they are then replicated like write requests
  */
-public abstract class TransportBroadcastReplicationAction<Request extends BroadcastRequest, Response extends BroadcastResponse, ShardRequest extends ReplicationRequest, ShardResponse extends ActionWriteResponse> extends HandledTransportAction<Request, Response> {
+public abstract class TransportBroadcastReplicationAction<Request extends BroadcastRequest, Response extends BroadcastResponse, ShardRequest extends ReplicationRequest, ShardResponse extends ReplicationResponse> extends HandledTransportAction<Request, Response> {
 
     private final TransportReplicationAction replicatedBroadcastShardAction;
     private final ClusterService clusterService;
@@ -91,15 +90,15 @@ public abstract class TransportBroadcastReplicationAction<Request extends Broadc
                     logger.trace("{}: got failure from {}", actionName, shardId);
                     int totalNumCopies = clusterState.getMetaData().index(shardId.index().getName()).getNumberOfReplicas() + 1;
                     ShardResponse shardResponse = newShardResponse();
-                    ActionWriteResponse.ShardInfo.Failure[] failures;
+                    ReplicationResponse.ShardInfo.Failure[] failures;
                     if (TransportActions.isShardNotAvailableException(e)) {
-                        failures = new ActionWriteResponse.ShardInfo.Failure[0];
+                        failures = new ReplicationResponse.ShardInfo.Failure[0];
                     } else {
-                        ActionWriteResponse.ShardInfo.Failure failure = new ActionWriteResponse.ShardInfo.Failure(shardId.index().name(), shardId.id(), null, e, ExceptionsHelper.status(e), true);
-                        failures = new ActionWriteResponse.ShardInfo.Failure[totalNumCopies];
+                        ReplicationResponse.ShardInfo.Failure failure = new ReplicationResponse.ShardInfo.Failure(shardId.index().name(), shardId.id(), null, e, ExceptionsHelper.status(e), true);
+                        failures = new ReplicationResponse.ShardInfo.Failure[totalNumCopies];
                         Arrays.fill(failures, failure);
                     }
-                    shardResponse.setShardInfo(new ActionWriteResponse.ShardInfo(totalNumCopies, 0, failures));
+                    shardResponse.setShardInfo(new ReplicationResponse.ShardInfo(totalNumCopies, 0, failures));
                     shardsResponses.add(shardResponse);
                     if (responsesCountDown.countDown()) {
                         finishAndNotifyListener(listener, shardsResponses);
@@ -142,7 +141,7 @@ public abstract class TransportBroadcastReplicationAction<Request extends Broadc
         int totalNumCopies = 0;
         List<ShardOperationFailedException> shardFailures = null;
         for (int i = 0; i < shardsResponses.size(); i++) {
-            ActionWriteResponse shardResponse = shardsResponses.get(i);
+            ReplicationResponse shardResponse = shardsResponses.get(i);
             if (shardResponse == null) {
                 // non active shard, ignore
             } else {
@@ -152,7 +151,7 @@ public abstract class TransportBroadcastReplicationAction<Request extends Broadc
                 if (shardFailures == null) {
                     shardFailures = new ArrayList<>();
                 }
-                for (ActionWriteResponse.ShardInfo.Failure failure : shardResponse.getShardInfo().getFailures()) {
+                for (ReplicationResponse.ShardInfo.Failure failure : shardResponse.getShardInfo().getFailures()) {
                     shardFailures.add(new DefaultShardOperationFailedException(new BroadcastShardOperationFailedException(new ShardId(failure.index(), failure.shardId()), failure.getCause())));
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
index 3b4d860..26c439c 100644
--- a/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
+++ b/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java
@@ -22,7 +22,7 @@ package org.elasticsearch.action.support.replication;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.UnavailableShardsException;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.support.ActionFilters;
@@ -37,11 +37,10 @@ import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.IndexRoutingTable;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.ShardIterator;
-import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.node.DiscoveryNodes;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -63,6 +62,8 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
 
 import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -70,8 +71,14 @@ import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Supplier;
 
 /**
+ * Base class for requests that should be executed on a primary copy followed by replica copies.
+ * Subclasses can resolve the target shard and provide implementation for primary and replica operations.
+ *
+ * The action samples cluster state on the receiving node to reroute to node with primary copy and on the
+ * primary node to validate request before primary operation followed by sampling state again for resolving
+ * nodes with replica copies to perform replication.
  */
-public abstract class TransportReplicationAction<Request extends ReplicationRequest, ReplicaRequest extends ReplicationRequest, Response extends ActionWriteResponse> extends TransportAction<Request, Response> {
+public abstract class TransportReplicationAction<Request extends ReplicationRequest, ReplicaRequest extends ReplicationRequest, Response extends ReplicationResponse> extends TransportAction<Request, Response> {
 
     public static final String SHARD_FAILURE_TIMEOUT = "action.support.replication.shard.failure_timeout";
 
@@ -85,6 +92,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
     private final TimeValue shardFailedTimeout;
 
     final String transportReplicaAction;
+    final String transportPrimaryAction;
     final String executor;
     final boolean checkWriteConsistency;
 
@@ -101,11 +109,12 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         this.shardStateAction = shardStateAction;
         this.mappingUpdatedAction = mappingUpdatedAction;
 
+        this.transportPrimaryAction = actionName + "[p]";
         this.transportReplicaAction = actionName + "[r]";
         this.executor = executor;
         this.checkWriteConsistency = checkWriteConsistency();
-
         transportService.registerRequestHandler(actionName, request, ThreadPool.Names.SAME, new OperationTransportHandler());
+        transportService.registerRequestHandler(transportPrimaryAction, request, executor, new PrimaryOperationTransportHandler());
         // we must never reject on because of thread pool capacity on replicas
         transportService.registerRequestHandler(transportReplicaAction, replicaRequest, executor, true, new ReplicaOperationTransportHandler());
 
@@ -118,40 +127,57 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
     @Override
     protected void doExecute(Request request, ActionListener<Response> listener) {
-        new PrimaryPhase(request, listener).run();
+        new ReroutePhase(request, listener).run();
     }
 
     protected abstract Response newResponseInstance();
 
     /**
+     * Resolves the target shard id of the incoming request.
+     * Additional processing or validation of the request should be done here.
+     */
+    protected void resolveRequest(MetaData metaData, String concreteIndex, Request request) {
+        // implementation should be provided if request shardID is not already resolved at request construction
+    }
+
+    /**
+     * Primary operation on node with primary copy, the provided metadata should be used for request validation if needed
      * @return A tuple containing not null values, as first value the result of the primary operation and as second value
      * the request to be executed on the replica shards.
      */
-    protected abstract Tuple<Response, ReplicaRequest> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable;
-
-    protected abstract void shardOperationOnReplica(ShardId shardId, ReplicaRequest shardRequest);
-
-    protected abstract ShardIterator shards(ClusterState clusterState, InternalRequest request);
+    protected abstract Tuple<Response, ReplicaRequest> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable;
 
-    protected abstract boolean checkWriteConsistency();
+    /**
+     * Replica operation on nodes with replica copies
+     */
+    protected abstract void shardOperationOnReplica(ReplicaRequest shardRequest);
 
-    protected ClusterBlockException checkGlobalBlock(ClusterState state) {
-        return state.blocks().globalBlockedException(ClusterBlockLevel.WRITE);
+    /**
+     * True if write consistency should be checked for an implementation
+     */
+    protected boolean checkWriteConsistency() {
+        return true;
     }
 
-    protected ClusterBlockException checkRequestBlock(ClusterState state, InternalRequest request) {
-        return state.blocks().indexBlockedException(ClusterBlockLevel.WRITE, request.concreteIndex());
+    /**
+     * Cluster level block to check before request execution
+     */
+    protected ClusterBlockLevel globalBlockLevel() {
+        return ClusterBlockLevel.WRITE;
     }
 
-    protected boolean resolveIndex() {
-        return true;
+    /**
+     * Index level block to check before request execution
+     */
+    protected ClusterBlockLevel indexBlockLevel() {
+        return ClusterBlockLevel.WRITE;
     }
 
     /**
-     * Resolves the request, by default doing nothing. Can be subclassed to do
-     * additional processing or validation depending on the incoming request
+     * True if provided index should be resolved when resolving request
      */
-    protected void resolveRequest(ClusterState state, InternalRequest request, ActionListener<Response> listener) {
+    protected boolean resolveIndex() {
+        return true;
     }
 
     protected TransportRequestOptions transportOptions() {
@@ -188,7 +214,7 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         return false;
     }
 
-    protected static class WriteResult<T extends ActionWriteResponse> {
+    protected static class WriteResult<T extends ReplicationResponse> {
 
         public final T response;
         public final Translog.Location location;
@@ -199,10 +225,10 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         }
 
         @SuppressWarnings("unchecked")
-        public <T extends ActionWriteResponse> T response() {
+        public <T extends ReplicationResponse> T response() {
             // this sets total, pending and failed to 0 and this is ok, because we will embed this into the replica
             // request and not use it
-            response.setShardInfo(new ActionWriteResponse.ShardInfo());
+            response.setShardInfo(new ReplicationResponse.ShardInfo());
             return (T) response;
         }
 
@@ -233,6 +259,13 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         }
     }
 
+    class PrimaryOperationTransportHandler implements TransportRequestHandler<Request> {
+        @Override
+        public void messageReceived(final Request request, final TransportChannel channel) throws Exception {
+            new PrimaryPhase(request, channel).run();
+        }
+    }
+
     class ReplicaOperationTransportHandler implements TransportRequestHandler<ReplicaRequest> {
         @Override
         public void messageReceived(final ReplicaRequest request, final TransportChannel channel) throws Exception {
@@ -259,7 +292,6 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         // something we want to avoid at all costs
         private final ClusterStateObserver observer = new ClusterStateObserver(clusterService, null, logger);
 
-
         AsyncReplicaAction(ReplicaRequest request, TransportChannel channel) {
             this.request = request;
             this.channel = channel;
@@ -287,14 +319,32 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
                 });
             } else {
                 try {
-                    failReplicaIfNeeded(request.internalShardId.getIndex(), request.internalShardId.id(), t, request);
+                    failReplicaIfNeeded(t);
                 } catch (Throwable unexpected) {
-                    logger.error("{} unexpected error while failing replica", request.internalShardId.id(), unexpected);
+                    logger.error("{} unexpected error while failing replica", unexpected, request.shardId().id());
                 } finally {
                     responseWithFailure(t);
                 }
             }
         }
+        private void failReplicaIfNeeded(Throwable t) {
+            String index = request.shardId().getIndex();
+            int shardId = request.shardId().id();
+            logger.trace("failure on replica [{}][{}], action [{}], request [{}]", t, index, shardId, actionName, request);
+            if (ignoreReplicaException(t) == false) {
+                IndexService indexService = indicesService.indexService(index);
+                if (indexService == null) {
+                    logger.debug("ignoring failed replica [{}][{}] because index was already removed.", index, shardId);
+                    return;
+                }
+                IndexShard indexShard = indexService.getShardOrNull(shardId);
+                if (indexShard == null) {
+                    logger.debug("ignoring failed replica [{}][{}] because index was already removed.", index, shardId);
+                    return;
+                }
+                indexShard.failShard(actionName + " failed on replica", t);
+            }
+        }
 
         protected void responseWithFailure(Throwable t) {
             try {
@@ -307,23 +357,17 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
         @Override
         protected void doRun() throws Exception {
-            try (Releasable shardReference = getIndexShardOperationsCounter(request.internalShardId)) {
-                shardOperationOnReplica(request.internalShardId, request);
+            assert request.shardId() != null : "request shardId must be set";
+            try (Releasable ignored = getIndexShardOperationsCounter(request.shardId())) {
+                shardOperationOnReplica(request);
+                if (logger.isTraceEnabled()) {
+                    logger.trace("action [{}] completed on shard [{}] for request [{}]", transportReplicaAction, request.shardId(), request);
+                }
             }
             channel.sendResponse(TransportResponse.Empty.INSTANCE);
         }
     }
 
-    protected class PrimaryOperationRequest {
-        public final ShardId shardId;
-        public final Request request;
-
-        public PrimaryOperationRequest(int shardId, String index, Request request) {
-            this.shardId = new ShardId(index, shardId);
-            this.request = request;
-        }
-    }
-
     public static class RetryOnPrimaryException extends ElasticsearchException {
         public RetryOnPrimaryException(ShardId shardId, String msg) {
             super(msg);
@@ -336,22 +380,22 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
     }
 
     /**
-     * Responsible for performing all operations up to the point we start starting sending requests to replica shards.
-     * Including forwarding the request to another node if the primary is not assigned locally.
-     * <p>
-     * Note that as soon as we start sending request to replicas, state responsibility is transferred to {@link ReplicationPhase}
+     * Responsible for routing and retrying failed operations on the primary.
+     * The actual primary operation is done in {@link PrimaryPhase} on the
+     * node with primary copy.
+     *
+     * Resolves index and shard id for the request before routing it to target node
      */
-    final class PrimaryPhase extends AbstractRunnable {
+    final class ReroutePhase extends AbstractRunnable {
         private final ActionListener<Response> listener;
-        private final InternalRequest internalRequest;
+        private final Request request;
         private final ClusterStateObserver observer;
-        private final AtomicBoolean finished = new AtomicBoolean(false);
-        private volatile Releasable indexShardReference;
+        private final AtomicBoolean finished = new AtomicBoolean();
 
-        PrimaryPhase(Request request, ActionListener<Response> listener) {
-            this.internalRequest = new InternalRequest(request);
+        ReroutePhase(Request request, ActionListener<Response> listener) {
+            this.request = request;
             this.listener = listener;
-            this.observer = new ClusterStateObserver(clusterService, internalRequest.request().timeout(), logger);
+            this.observer = new ClusterStateObserver(clusterService, request.timeout(), logger);
         }
 
         @Override
@@ -361,135 +405,91 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
         @Override
         protected void doRun() {
-            if (checkBlocks() == false) {
+            final ClusterState state = observer.observedState();
+            ClusterBlockException blockException = state.blocks().globalBlockedException(globalBlockLevel());
+            if (blockException != null) {
+                handleBlockException(blockException);
                 return;
             }
-            final ShardIterator shardIt = shards(observer.observedState(), internalRequest);
-            final ShardRouting primary = resolvePrimary(shardIt);
-            if (primary == null) {
-                retryBecauseUnavailable(shardIt.shardId(), "No active shards.");
+            final String concreteIndex = resolveIndex() ? indexNameExpressionResolver.concreteSingleIndex(state, request) : request.index();
+            blockException = state.blocks().indexBlockedException(indexBlockLevel(), concreteIndex);
+            if (blockException != null) {
+                handleBlockException(blockException);
                 return;
             }
-            if (primary.active() == false) {
-                logger.trace("primary shard [{}] is not yet active, scheduling a retry. action [{}], request [{}]", primary.shardId(), actionName, internalRequest.request);
-                retryBecauseUnavailable(shardIt.shardId(), "Primary shard is not active or isn't assigned to a known node.");
+            // request does not have a shardId yet, we need to pass the concrete index to resolve shardId
+            resolveRequest(state.metaData(), concreteIndex, request);
+            assert request.shardId() != null : "request shardId must be set in resolveRequest";
+
+            IndexShardRoutingTable indexShard = state.getRoutingTable().shardRoutingTable(request.shardId().getIndex(), request.shardId().id());
+            final ShardRouting primary = indexShard.primaryShard();
+            if (primary == null || primary.active() == false) {
+                logger.trace("primary shard [{}] is not yet active, scheduling a retry: action [{}], request [{}], cluster state version [{}]", request.shardId(), actionName, request, state.version());
+                retryBecauseUnavailable(request.shardId(), "primary shard is not active");
                 return;
             }
-            if (observer.observedState().nodes().nodeExists(primary.currentNodeId()) == false) {
-                logger.trace("primary shard [{}] is assigned to anode we do not know the node, scheduling a retry.", primary.shardId(), primary.currentNodeId());
-                retryBecauseUnavailable(shardIt.shardId(), "Primary shard is not active or isn't assigned to a known node.");
+            if (state.nodes().nodeExists(primary.currentNodeId()) == false) {
+                logger.trace("primary shard [{}] is assigned to an unknown node [{}], scheduling a retry: action [{}], request [{}], cluster state version [{}]", request.shardId(), primary.currentNodeId(), actionName, request, state.version());
+                retryBecauseUnavailable(request.shardId(), "primary shard isn't assigned to a known node.");
                 return;
             }
-            routeRequestOrPerformLocally(primary, shardIt);
-        }
-
-        /**
-         * checks for any cluster state blocks. Returns true if operation is OK to proceeded.
-         * if false is return, no further action is needed. The method takes care of any continuation, by either
-         * responding to the listener or scheduling a retry
-         */
-        protected boolean checkBlocks() {
-            ClusterBlockException blockException = checkGlobalBlock(observer.observedState());
-            if (blockException != null) {
-                if (blockException.retryable()) {
-                    logger.trace("cluster is blocked ({}), scheduling a retry", blockException.getMessage());
-                    retry(blockException);
-                } else {
-                    finishAsFailed(blockException);
+            final DiscoveryNode node = state.nodes().get(primary.currentNodeId());
+            if (primary.currentNodeId().equals(state.nodes().localNodeId())) {
+                if (logger.isTraceEnabled()) {
+                    logger.trace("send action [{}] on primary [{}] for request [{}] with cluster state version [{}] to [{}] ", transportPrimaryAction, request.shardId(), request, state.version(), primary.currentNodeId());
                 }
-                return false;
-            }
-            if (resolveIndex()) {
-                internalRequest.concreteIndex(indexNameExpressionResolver.concreteSingleIndex(observer.observedState(), internalRequest.request()));
+                performAction(node, transportPrimaryAction, true);
             } else {
-                internalRequest.concreteIndex(internalRequest.request().index());
-            }
-
-            resolveRequest(observer.observedState(), internalRequest, listener);
-
-            blockException = checkRequestBlock(observer.observedState(), internalRequest);
-            if (blockException != null) {
-                if (blockException.retryable()) {
-                    logger.trace("cluster is blocked ({}), scheduling a retry", blockException.getMessage());
-                    retry(blockException);
-                } else {
-                    finishAsFailed(blockException);
+                if (logger.isTraceEnabled()) {
+                    logger.trace("send action [{}] on primary [{}] for request [{}] with cluster state version [{}] to [{}]", actionName, request.shardId(), request, state.version(), primary.currentNodeId());
                 }
-                return false;
+                performAction(node, actionName, false);
             }
-            return true;
         }
 
-        protected ShardRouting resolvePrimary(ShardIterator shardIt) {
-            // no shardIt, might be in the case between index gateway recovery and shardIt initialization
-            ShardRouting shard;
-            while ((shard = shardIt.nextOrNull()) != null) {
-                // we only deal with primary shardIt here...
-                if (shard.primary()) {
-                    return shard;
-                }
+        private void handleBlockException(ClusterBlockException blockException) {
+            if (blockException.retryable()) {
+                logger.trace("cluster is blocked ({}), scheduling a retry", blockException.getMessage());
+                retry(blockException);
+            } else {
+                finishAsFailed(blockException);
             }
-            return null;
         }
 
-        /**
-         * send the request to the node holding the primary or execute if local
-         */
-        protected void routeRequestOrPerformLocally(final ShardRouting primary, final ShardIterator shardsIt) {
-            if (primary.currentNodeId().equals(observer.observedState().nodes().localNodeId())) {
-                try {
-                    threadPool.executor(executor).execute(new AbstractRunnable() {
-                        @Override
-                        public void onFailure(Throwable t) {
-                            finishAsFailed(t);
-                        }
+        private void performAction(final DiscoveryNode node, final String action, final boolean isPrimaryAction) {
+            transportService.sendRequest(node, action, request, transportOptions, new BaseTransportResponseHandler<Response>() {
 
-                        @Override
-                        protected void doRun() throws Exception {
-                            performOnPrimary(primary, shardsIt);
-                        }
-                    });
-                } catch (Throwable t) {
-                    finishAsFailed(t);
+                @Override
+                public Response newInstance() {
+                    return newResponseInstance();
                 }
-            } else {
-                DiscoveryNode node = observer.observedState().nodes().get(primary.currentNodeId());
-                transportService.sendRequest(node, actionName, internalRequest.request(), transportOptions, new BaseTransportResponseHandler<Response>() {
 
-                    @Override
-                    public Response newInstance() {
-                        return newResponseInstance();
-                    }
-
-                    @Override
-                    public String executor() {
-                        return ThreadPool.Names.SAME;
-                    }
+                @Override
+                public String executor() {
+                    return ThreadPool.Names.SAME;
+                }
 
-                    @Override
-                    public void handleResponse(Response response) {
-                        finishOnRemoteSuccess(response);
-                    }
+                @Override
+                public void handleResponse(Response response) {
+                    finishOnSuccess(response);
+                }
 
-                    @Override
-                    public void handleException(TransportException exp) {
-                        try {
-                            // if we got disconnected from the node, or the node / shard is not in the right state (being closed)
-                            if (exp.unwrapCause() instanceof ConnectTransportException || exp.unwrapCause() instanceof NodeClosedException ||
-                                    retryPrimaryException(exp)) {
-                                // we already marked it as started when we executed it (removed the listener) so pass false
-                                // to re-add to the cluster listener
-                                logger.trace("received an error from node the primary was assigned to ({}), scheduling a retry", exp.getMessage());
-                                retry(exp);
-                            } else {
-                                finishAsFailed(exp);
-                            }
-                        } catch (Throwable t) {
-                            finishWithUnexpectedFailure(t);
+                @Override
+                public void handleException(TransportException exp) {
+                    try {
+                        // if we got disconnected from the node, or the node / shard is not in the right state (being closed)
+                        if (exp.unwrapCause() instanceof ConnectTransportException || exp.unwrapCause() instanceof NodeClosedException ||
+                                (isPrimaryAction && retryPrimaryException(exp.unwrapCause()))) {
+                            logger.trace("received an error from node [{}] for request [{}], scheduling a retry", exp, node.id(), request);
+                            retry(exp);
+                        } else {
+                            finishAsFailed(exp);
                         }
+                    } catch (Throwable t) {
+                        finishWithUnexpectedFailure(t);
                     }
-                });
-            }
+                }
+            });
         }
 
         void retry(Throwable failure) {
@@ -518,22 +518,9 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
             });
         }
 
-        /**
-         * upon success, finish the first phase and transfer responsibility to the {@link ReplicationPhase}
-         */
-        void finishAndMoveToReplication(ReplicationPhase replicationPhase) {
-            if (finished.compareAndSet(false, true)) {
-                replicationPhase.run();
-            } else {
-                assert false : "finishAndMoveToReplication called but operation is already finished";
-            }
-        }
-
-
         void finishAsFailed(Throwable failure) {
             if (finished.compareAndSet(false, true)) {
-                Releasables.close(indexShardReference);
-                logger.trace("operation failed. action [{}], request [{}]", failure, actionName, internalRequest.request);
+                logger.trace("operation failed. action [{}], request [{}]", failure, actionName, request);
                 listener.onFailure(failure);
             } else {
                 assert false : "finishAsFailed called but operation is already finished";
@@ -541,64 +528,79 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         }
 
         void finishWithUnexpectedFailure(Throwable failure) {
-            logger.warn("unexpected error during the primary phase for action [{}], request [{}]", failure, actionName, internalRequest.request);
+            logger.warn("unexpected error during the primary phase for action [{}], request [{}]", failure, actionName, request);
             if (finished.compareAndSet(false, true)) {
-                Releasables.close(indexShardReference);
                 listener.onFailure(failure);
             } else {
                 assert false : "finishWithUnexpectedFailure called but operation is already finished";
             }
         }
 
-        void finishOnRemoteSuccess(Response response) {
+        void finishOnSuccess(Response response) {
             if (finished.compareAndSet(false, true)) {
                 if (logger.isTraceEnabled()) {
-                    logger.trace("operation succeeded. action [{}],request [{}]", actionName, internalRequest.request);
+                    logger.trace("operation succeeded. action [{}],request [{}]", actionName, request);
                 }
                 listener.onResponse(response);
             } else {
-                assert false : "finishOnRemoteSuccess called but operation is already finished";
+                assert false : "finishOnSuccess called but operation is already finished";
             }
         }
 
-        /**
-         * perform the operation on the node holding the primary
-         */
-        void performOnPrimary(final ShardRouting primary, final ShardIterator shardsIt) {
-            final String writeConsistencyFailure = checkWriteConsistency(primary);
+        void retryBecauseUnavailable(ShardId shardId, String message) {
+            retry(new UnavailableShardsException(shardId, "{} Timeout: [{}], request: [{}]", message, request.timeout(), request));
+        }
+    }
+
+    /**
+     * Responsible for performing primary operation locally and delegating to replication action once successful
+     * <p>
+     * Note that as soon as we move to replication action, state responsibility is transferred to {@link ReplicationPhase}.
+     */
+    final class PrimaryPhase extends AbstractRunnable {
+        private final Request request;
+        private final TransportChannel channel;
+        private final ClusterState state;
+        private final AtomicBoolean finished = new AtomicBoolean();
+        private Releasable indexShardReference;
+
+        PrimaryPhase(Request request, TransportChannel channel) {
+            this.state = clusterService.state();
+            this.request = request;
+            this.channel = channel;
+        }
+
+        @Override
+        public void onFailure(Throwable e) {
+            finishAsFailed(e);
+        }
+
+        @Override
+        protected void doRun() throws Exception {
+            // request shardID was set in ReroutePhase
+            assert request.shardId() != null : "request shardID must be set prior to primary phase";
+            final ShardId shardId = request.shardId();
+            final String writeConsistencyFailure = checkWriteConsistency(shardId);
             if (writeConsistencyFailure != null) {
-                retryBecauseUnavailable(primary.shardId(), writeConsistencyFailure);
+                finishBecauseUnavailable(shardId, writeConsistencyFailure);
                 return;
             }
             final ReplicationPhase replicationPhase;
             try {
-                indexShardReference = getIndexShardOperationsCounter(primary.shardId());
-                PrimaryOperationRequest por = new PrimaryOperationRequest(primary.id(), internalRequest.concreteIndex(), internalRequest.request());
-                Tuple<Response, ReplicaRequest> primaryResponse = shardOperationOnPrimary(observer.observedState(), por);
+                indexShardReference = getIndexShardOperationsCounter(shardId);
+                Tuple<Response, ReplicaRequest> primaryResponse = shardOperationOnPrimary(state.metaData(), request);
                 if (logger.isTraceEnabled()) {
-                    logger.trace("operation completed on primary [{}], action [{}], request [{}], cluster state version [{}]", primary, actionName, por.request, observer.observedState().version());
+                    logger.trace("action [{}] completed on shard [{}] for request [{}] with cluster state version [{}]", transportPrimaryAction, shardId, request, state.version());
                 }
-                replicationPhase = new ReplicationPhase(shardsIt, primaryResponse.v2(), primaryResponse.v1(), observer, primary, internalRequest, listener, indexShardReference, shardFailedTimeout);
+                replicationPhase = new ReplicationPhase(primaryResponse.v2(), primaryResponse.v1(), shardId, channel, indexShardReference, shardFailedTimeout);
             } catch (Throwable e) {
-                // shard has not been allocated yet, retry it here
-                if (retryPrimaryException(e)) {
-                    logger.trace("had an error while performing operation on primary ({}, action [{}], request [{}]), scheduling a retry.", e, primary, actionName, internalRequest.request);
-                    // We have to close here because when we retry we will increment get a new reference on index shard again and we do not want to
-                    // increment twice.
-                    Releasables.close(indexShardReference);
-                    // We have to reset to null here because whe we retry it might be that we never get to the point where we assign a new reference
-                    // (for example, in case the operation was rejected because queue is full). In this case we would release again once one of the finish methods is called.
-                    indexShardReference = null;
-                    retry(e);
-                    return;
-                }
                 if (ExceptionsHelper.status(e) == RestStatus.CONFLICT) {
                     if (logger.isTraceEnabled()) {
-                        logger.trace(primary.shortSummary() + ": Failed to execute [" + internalRequest.request() + "]", e);
+                        logger.trace("failed to execute [{}] on [{}]", e, request, shardId);
                     }
                 } else {
                     if (logger.isDebugEnabled()) {
-                        logger.debug(primary.shortSummary() + ": Failed to execute [" + internalRequest.request() + "]", e);
+                        logger.debug("failed to execute [{}] on [{}]", e, request, shardId);
                     }
                 }
                 finishAsFailed(e);
@@ -611,22 +613,22 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
          * checks whether we can perform a write based on the write consistency setting
          * returns **null* if OK to proceed, or a string describing the reason to stop
          */
-        String checkWriteConsistency(ShardRouting shard) {
+        String checkWriteConsistency(ShardId shardId) {
             if (checkWriteConsistency == false) {
                 return null;
             }
 
             final WriteConsistencyLevel consistencyLevel;
-            if (internalRequest.request().consistencyLevel() != WriteConsistencyLevel.DEFAULT) {
-                consistencyLevel = internalRequest.request().consistencyLevel();
+            if (request.consistencyLevel() != WriteConsistencyLevel.DEFAULT) {
+                consistencyLevel = request.consistencyLevel();
             } else {
                 consistencyLevel = defaultWriteConsistencyLevel;
             }
             final int sizeActive;
             final int requiredNumber;
-            IndexRoutingTable indexRoutingTable = observer.observedState().getRoutingTable().index(shard.index());
+            IndexRoutingTable indexRoutingTable = state.getRoutingTable().index(shardId.getIndex());
             if (indexRoutingTable != null) {
-                IndexShardRoutingTable shardRoutingTable = indexRoutingTable.shard(shard.getId());
+                IndexShardRoutingTable shardRoutingTable = indexRoutingTable.shard(shardId.getId());
                 if (shardRoutingTable != null) {
                     sizeActive = shardRoutingTable.activeShards().size();
                     if (consistencyLevel == WriteConsistencyLevel.QUORUM && shardRoutingTable.getSize() > 2) {
@@ -648,17 +650,44 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
             if (sizeActive < requiredNumber) {
                 logger.trace("not enough active copies of shard [{}] to meet write consistency of [{}] (have {}, needed {}), scheduling a retry. action [{}], request [{}]",
-                        shard.shardId(), consistencyLevel, sizeActive, requiredNumber, actionName, internalRequest.request);
+                        shardId, consistencyLevel, sizeActive, requiredNumber, transportPrimaryAction, request);
                 return "Not enough active copies to meet write consistency of [" + consistencyLevel + "] (have " + sizeActive + ", needed " + requiredNumber + ").";
             } else {
                 return null;
             }
         }
 
-        void retryBecauseUnavailable(ShardId shardId, String message) {
-            retry(new UnavailableShardsException(shardId, message + " Timeout: [" + internalRequest.request().timeout() + "], request: " + internalRequest.request().toString()));
+        /**
+         * upon success, finish the first phase and transfer responsibility to the {@link ReplicationPhase}
+         */
+        void finishAndMoveToReplication(ReplicationPhase replicationPhase) {
+            if (finished.compareAndSet(false, true)) {
+                replicationPhase.run();
+            } else {
+                assert false : "finishAndMoveToReplication called but operation is already finished";
+            }
+        }
+
+        /**
+         * upon failure, send failure back to the {@link ReroutePhase} for retrying if appropriate
+         */
+        void finishAsFailed(Throwable failure) {
+            if (finished.compareAndSet(false, true)) {
+                Releasables.close(indexShardReference);
+                logger.trace("operation failed", failure);
+                try {
+                    channel.sendResponse(failure);
+                } catch (IOException responseException) {
+                    logger.warn("failed to send error message back to client for action [{}]", responseException, transportPrimaryAction);
+                }
+            } else {
+                assert false : "finishAsFailed called but operation is already finished";
+            }
         }
 
+        void finishBecauseUnavailable(ShardId shardId, String message) {
+            finishAsFailed(new UnavailableShardsException(shardId, "{} Timeout: [{}], request: [{}]", message, request.timeout(), request));
+        }
     }
 
     protected Releasable getIndexShardOperationsCounter(ShardId shardId) {
@@ -667,135 +696,78 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         return new IndexShardReference(indexShard);
     }
 
-    private void failReplicaIfNeeded(String index, int shardId, Throwable t, ReplicaRequest request) {
-        logger.trace("failure on replica [{}][{}], action [{}], request [{}]", t, index, shardId, actionName, request);
-        if (ignoreReplicaException(t) == false) {
-            IndexService indexService = indicesService.indexService(index);
-            if (indexService == null) {
-                logger.debug("ignoring failed replica [{}][{}] because index was already removed.", index, shardId);
-                return;
-            }
-            IndexShard indexShard = indexService.getShardOrNull(shardId);
-            if (indexShard == null) {
-                logger.debug("ignoring failed replica [{}][{}] because index was already removed.", index, shardId);
-                return;
-            }
-            indexShard.failShard(actionName + " failed on replica", t);
-        }
-    }
-
     /**
-     * inner class is responsible for send the requests to all replica shards and manage the responses
+     * Responsible for sending replica requests (see {@link AsyncReplicaAction}) to nodes with replica copy, including
+     * relocating copies
      */
     final class ReplicationPhase extends AbstractRunnable {
 
         private final ReplicaRequest replicaRequest;
         private final Response finalResponse;
-        private final ShardIterator shardIt;
-        private final ActionListener<Response> listener;
-        private final AtomicBoolean finished = new AtomicBoolean(false);
+        private final TransportChannel channel;
+        private final ShardId shardId;
+        private final List<ShardRouting> shards;
+        private final DiscoveryNodes nodes;
+        private final boolean executeOnReplica;
+        private final String indexUUID;
+        private final AtomicBoolean finished = new AtomicBoolean();
         private final AtomicInteger success = new AtomicInteger(1); // We already wrote into the primary shard
         private final ConcurrentMap<String, Throwable> shardReplicaFailures = ConcurrentCollections.newConcurrentMap();
-        private final IndexMetaData indexMetaData;
-        private final ShardRouting originalPrimaryShard;
         private final AtomicInteger pending;
         private final int totalShards;
-        private final ClusterStateObserver observer;
         private final Releasable indexShardReference;
         private final TimeValue shardFailedTimeout;
 
-        /**
-         * the constructor doesn't take any action, just calculates state. Call {@link #run()} to start
-         * replicating.
-         */
-        public ReplicationPhase(ShardIterator originalShardIt, ReplicaRequest replicaRequest, Response finalResponse,
-                                ClusterStateObserver observer, ShardRouting originalPrimaryShard,
-                                InternalRequest internalRequest, ActionListener<Response> listener, Releasable indexShardReference,
-                                TimeValue shardFailedTimeout) {
+        public ReplicationPhase(ReplicaRequest replicaRequest, Response finalResponse, ShardId shardId,
+                                TransportChannel channel, Releasable indexShardReference, TimeValue shardFailedTimeout) {
             this.replicaRequest = replicaRequest;
-            this.listener = listener;
+            this.channel = channel;
             this.finalResponse = finalResponse;
-            this.originalPrimaryShard = originalPrimaryShard;
-            this.observer = observer;
-            indexMetaData = observer.observedState().metaData().index(internalRequest.concreteIndex());
             this.indexShardReference = indexShardReference;
             this.shardFailedTimeout = shardFailedTimeout;
-
-            ShardRouting shard;
-            // we double check on the state, if it got changed we need to make sure we take the latest one cause
-            // maybe a replica shard started its recovery process and we need to apply it there...
-
-            // we also need to make sure if the new state has a new primary shard (that we indexed to before) started
-            // and assigned to another node (while the indexing happened). In that case, we want to apply it on the
-            // new primary shard as well...
-            ClusterState newState = clusterService.state();
-
-            int numberOfUnassignedOrIgnoredReplicas = 0;
+            this.shardId = shardId;
+
+            // we have to get a new state after successfully indexing into the primary in order to honour recovery semantics.
+            // we have to make sure that every operation indexed into the primary after recovery start will also be replicated
+            // to the recovery target. If we use an old cluster state, we may miss a relocation that has started since then.
+            // If the index gets deleted after primary operation, we skip replication
+            final ClusterState state = clusterService.state();
+            final IndexRoutingTable index = state.getRoutingTable().index(shardId.getIndex());
+            final IndexShardRoutingTable shardRoutingTable = (index != null) ? index.shard(shardId.id()) : null;
+            final IndexMetaData indexMetaData = state.getMetaData().index(shardId.getIndex());
+            this.shards = (shardRoutingTable != null) ? shardRoutingTable.shards() : Collections.emptyList();
+            this.executeOnReplica = (indexMetaData == null) || shouldExecuteReplication(indexMetaData.getSettings());
+            this.indexUUID = (indexMetaData != null) ? indexMetaData.getIndexUUID() : null;
+            this.nodes = state.getNodes();
+
+            if (shards.isEmpty()) {
+                logger.debug("replication phase for request [{}] on [{}] is skipped due to index deletion after primary operation", replicaRequest, shardId);
+            }
+
+            // we calculate number of target nodes to send replication operations, including nodes with relocating shards
+            int numberOfIgnoredShardInstances = 0;
             int numberOfPendingShardInstances = 0;
-            if (observer.observedState() != newState) {
-                observer.reset(newState);
-                shardIt = shards(newState, internalRequest);
-                while ((shard = shardIt.nextOrNull()) != null) {
-                    if (shard.primary()) {
-                        if (originalPrimaryShard.currentNodeId().equals(shard.currentNodeId()) == false) {
-                            // there is a new primary, we'll have to replicate to it.
-                            numberOfPendingShardInstances++;
-                        }
-                        if (shard.relocating()) {
-                            numberOfPendingShardInstances++;
-                        }
-                    } else if (shouldExecuteReplication(indexMetaData.getSettings()) == false) {
-                        // If the replicas use shadow replicas, there is no reason to
-                        // perform the action on the replica, so skip it and
-                        // immediately return
-
-                        // this delays mapping updates on replicas because they have
-                        // to wait until they get the new mapping through the cluster
-                        // state, which is why we recommend pre-defined mappings for
-                        // indices using shadow replicas
-                        numberOfUnassignedOrIgnoredReplicas++;
-                    } else if (shard.unassigned()) {
-                        numberOfUnassignedOrIgnoredReplicas++;
-                    } else if (shard.relocating()) {
-                        // we need to send to two copies
-                        numberOfPendingShardInstances += 2;
-                    } else {
+            for (ShardRouting shard : shards) {
+                if (shard.primary() == false && executeOnReplica == false) {
+                    numberOfIgnoredShardInstances++;
+                } else if (shard.unassigned()) {
+                    numberOfIgnoredShardInstances++;
+                } else {
+                    if (shard.currentNodeId().equals(nodes.localNodeId()) == false) {
                         numberOfPendingShardInstances++;
                     }
-                }
-            } else {
-                shardIt = originalShardIt;
-                shardIt.reset();
-                while ((shard = shardIt.nextOrNull()) != null) {
-                    if (shard.unassigned()) {
-                        numberOfUnassignedOrIgnoredReplicas++;
-                    } else if (shard.primary()) {
-                        if (shard.relocating()) {
-                            // we have to replicate to the other copy
-                            numberOfPendingShardInstances += 1;
-                        }
-                    } else if (shouldExecuteReplication(indexMetaData.getSettings()) == false) {
-                        // If the replicas use shadow replicas, there is no reason to
-                        // perform the action on the replica, so skip it and
-                        // immediately return
-
-                        // this delays mapping updates on replicas because they have
-                        // to wait until they get the new mapping through the cluster
-                        // state, which is why we recommend pre-defined mappings for
-                        // indices using shadow replicas
-                        numberOfUnassignedOrIgnoredReplicas++;
-                    } else if (shard.relocating()) {
-                        // we need to send to two copies
-                        numberOfPendingShardInstances += 2;
-                    } else {
+                    if (shard.relocating()) {
                         numberOfPendingShardInstances++;
                     }
                 }
             }
-
-            // one for the primary already done
-            this.totalShards = 1 + numberOfPendingShardInstances + numberOfUnassignedOrIgnoredReplicas;
+            // one for the local primary copy
+            this.totalShards = 1 + numberOfPendingShardInstances + numberOfIgnoredShardInstances;
             this.pending = new AtomicInteger(numberOfPendingShardInstances);
+            if (logger.isTraceEnabled()) {
+                logger.trace("replication phase started. pending [{}], action [{}], request [{}], cluster state version used [{}]", pending.get(),
+                    transportReplicaAction, replicaRequest, state.version());
+            }
         }
 
         /**
@@ -821,114 +793,84 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
 
         @Override
         public void onFailure(Throwable t) {
-            logger.error("unexpected error while replicating for action [{}]. shard [{}]. ", t, actionName, shardIt.shardId());
+            logger.error("unexpected error while replicating for action [{}]. shard [{}]. ", t, actionName, shardId);
             forceFinishAsFailed(t);
         }
 
         /**
-         * start sending current requests to replicas
+         * start sending replica requests to target nodes
          */
         @Override
         protected void doRun() {
-            if (logger.isTraceEnabled()) {
-                logger.trace("replication phase started. pending [{}], action [{}], request [{}], cluster state version used [{}]", pending.get(),
-                        actionName, replicaRequest, observer.observedState().version());
-            }
             if (pending.get() == 0) {
                 doFinish();
                 return;
             }
-            ShardRouting shard;
-            shardIt.reset(); // reset the iterator
-            while ((shard = shardIt.nextOrNull()) != null) {
-                // if its unassigned, nothing to do here...
+            for (ShardRouting shard : shards) {
+                if (shard.primary() == false && executeOnReplica == false) {
+                    // If the replicas use shadow replicas, there is no reason to
+                    // perform the action on the replica, so skip it and
+                    // immediately return
+
+                    // this delays mapping updates on replicas because they have
+                    // to wait until they get the new mapping through the cluster
+                    // state, which is why we recommend pre-defined mappings for
+                    // indices using shadow replicas
+                    continue;
+                }
                 if (shard.unassigned()) {
                     continue;
                 }
-
                 // we index on a replica that is initializing as well since we might not have got the event
                 // yet that it was started. We will get an exception IllegalShardState exception if its not started
                 // and that's fine, we will ignore it
-                if (shard.primary()) {
-                    if (originalPrimaryShard.currentNodeId().equals(shard.currentNodeId()) == false) {
-                        // there is a new primary, we'll have to replicate to it.
-                        performOnReplica(shard, shard.currentNodeId());
-                    }
-                    if (shard.relocating()) {
-                        performOnReplica(shard, shard.relocatingNodeId());
-                    }
-                } else if (shouldExecuteReplication(indexMetaData.getSettings())) {
+
+                // we never execute replication operation locally as primary operation has already completed locally
+                // hence, we ignore any local shard for replication
+                if (nodes.localNodeId().equals(shard.currentNodeId()) == false) {
                     performOnReplica(shard, shard.currentNodeId());
-                    if (shard.relocating()) {
-                        performOnReplica(shard, shard.relocatingNodeId());
-                    }
+                }
+                // send operation to relocating shard
+                if (shard.relocating()) {
+                    performOnReplica(shard, shard.relocatingNodeId());
                 }
             }
         }
 
         /**
-         * send operation to the given node or perform it if local
+         * send replica operation to target node
          */
         void performOnReplica(final ShardRouting shard, final String nodeId) {
             // if we don't have that node, it means that it might have failed and will be created again, in
             // this case, we don't have to do the operation, and just let it failover
-            if (!observer.observedState().nodes().nodeExists(nodeId)) {
+            if (!nodes.nodeExists(nodeId)) {
+                logger.trace("failed to send action [{}] on replica [{}] for request [{}] due to unknown node [{}]", transportReplicaAction, shard.shardId(), replicaRequest, nodeId);
                 onReplicaFailure(nodeId, null);
                 return;
             }
+            if (logger.isTraceEnabled()) {
+                logger.trace("send action [{}] on replica [{}] for request [{}] to [{}]", transportReplicaAction, shard.shardId(), replicaRequest, nodeId);
+            }
 
-            replicaRequest.internalShardId = shardIt.shardId();
-
-            if (!nodeId.equals(observer.observedState().nodes().localNodeId())) {
-                final DiscoveryNode node = observer.observedState().nodes().get(nodeId);
-                transportService.sendRequest(node, transportReplicaAction, replicaRequest,
-                        transportOptions, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
-                            @Override
-                            public void handleResponse(TransportResponse.Empty vResponse) {
-                                onReplicaSuccess();
-                            }
-
-                            @Override
-                            public void handleException(TransportException exp) {
-                                logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, actionName);
-                                if (ignoreReplicaException(exp)) {
-                                    onReplicaFailure(nodeId, exp);
-                                } else {
-                                    logger.warn("{} failed to perform {} on node {}", exp, shardIt.shardId(), actionName, node);
-                                    shardStateAction.shardFailed(shard, indexMetaData.getIndexUUID(), "failed to perform " + actionName + " on replica on node " + node, exp, shardFailedTimeout, new ReplicationFailedShardStateListener(nodeId, exp));
-                                }
-                            }
-                        });
-            } else {
-                try {
-                    threadPool.executor(executor).execute(new AbstractRunnable() {
-                        @Override
-                        protected void doRun() {
-                            try {
-                                shardOperationOnReplica(shard.shardId(), replicaRequest);
-                                onReplicaSuccess();
-                            } catch (Throwable e) {
-                                onReplicaFailure(nodeId, e);
-                                failReplicaIfNeeded(shard.index(), shard.id(), e, replicaRequest);
-                            }
-                        }
-
-                        // we must never reject on because of thread pool capacity on replicas
+            final DiscoveryNode node = nodes.get(nodeId);
+            transportService.sendRequest(node, transportReplicaAction, replicaRequest, transportOptions, new EmptyTransportResponseHandler(ThreadPool.Names.SAME) {
                         @Override
-                        public boolean isForceExecution() {
-                            return true;
+                        public void handleResponse(TransportResponse.Empty vResponse) {
+                            onReplicaSuccess();
                         }
 
                         @Override
-                        public void onFailure(Throwable t) {
-                            onReplicaFailure(nodeId, t);
+                        public void handleException(TransportException exp) {
+                            logger.trace("[{}] transport failure during replica request [{}], action [{}]", exp, node, replicaRequest, transportReplicaAction);
+                            if (ignoreReplicaException(exp)) {
+                                onReplicaFailure(nodeId, exp);
+                            } else {
+                                logger.warn("{} failed to perform {} on node {}", exp, shardId, transportReplicaAction, node);
+                                shardStateAction.shardFailed(shard, indexUUID, "failed to perform " + transportReplicaAction + " on replica on node " + node, exp, shardFailedTimeout, new ReplicationFailedShardStateListener(nodeId, exp));
+                            }
                         }
-                    });
-                } catch (Throwable e) {
-                    failReplicaIfNeeded(shard.index(), shard.id(), e, replicaRequest);
-                    onReplicaFailure(nodeId, e);
-                }
-            }
+                    }
+            );
         }
 
 
@@ -954,35 +896,46 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         private void forceFinishAsFailed(Throwable t) {
             if (finished.compareAndSet(false, true)) {
                 Releasables.close(indexShardReference);
-                listener.onFailure(t);
+                try {
+                    channel.sendResponse(t);
+                } catch (IOException responseException) {
+                    logger.warn("failed to send error message back to client for action [{}]", responseException, transportReplicaAction);
+                    logger.warn("actual Exception", t);
+                }
             }
         }
 
         private void doFinish() {
             if (finished.compareAndSet(false, true)) {
                 Releasables.close(indexShardReference);
-                final ShardId shardId = shardIt.shardId();
-                final ActionWriteResponse.ShardInfo.Failure[] failuresArray;
+                final ReplicationResponse.ShardInfo.Failure[] failuresArray;
                 if (!shardReplicaFailures.isEmpty()) {
                     int slot = 0;
-                    failuresArray = new ActionWriteResponse.ShardInfo.Failure[shardReplicaFailures.size()];
+                    failuresArray = new ReplicationResponse.ShardInfo.Failure[shardReplicaFailures.size()];
                     for (Map.Entry<String, Throwable> entry : shardReplicaFailures.entrySet()) {
                         RestStatus restStatus = ExceptionsHelper.status(entry.getValue());
-                        failuresArray[slot++] = new ActionWriteResponse.ShardInfo.Failure(
+                        failuresArray[slot++] = new ReplicationResponse.ShardInfo.Failure(
                                 shardId.getIndex(), shardId.getId(), entry.getKey(), entry.getValue(), restStatus, false
                         );
                     }
                 } else {
-                    failuresArray = ActionWriteResponse.EMPTY;
+                    failuresArray = ReplicationResponse.EMPTY;
                 }
-                finalResponse.setShardInfo(new ActionWriteResponse.ShardInfo(
+                finalResponse.setShardInfo(new ReplicationResponse.ShardInfo(
                                 totalShards,
                                 success.get(),
                                 failuresArray
 
                         )
                 );
-                listener.onResponse(finalResponse);
+                try {
+                    channel.sendResponse(finalResponse);
+                } catch (IOException responseException) {
+                    logger.warn("failed to send error message back to client for action [" + transportReplicaAction + "]", responseException);
+                }
+                if (logger.isTraceEnabled()) {
+                    logger.trace("action [{}] completed on all replicas [{}] for request [{}]", transportReplicaAction, shardId, replicaRequest);
+                }
             }
         }
 
@@ -1023,34 +976,10 @@ public abstract class TransportReplicationAction<Request extends ReplicationRequ
         return IndexMetaData.isIndexUsingShadowReplicas(settings) == false;
     }
 
-    /**
-     * Internal request class that gets built on each node. Holds the original request plus additional info.
-     */
-    protected class InternalRequest {
-        final Request request;
-        String concreteIndex;
-
-        InternalRequest(Request request) {
-            this.request = request;
-        }
-
-        public Request request() {
-            return request;
-        }
-
-        void concreteIndex(String concreteIndex) {
-            this.concreteIndex = concreteIndex;
-        }
-
-        public String concreteIndex() {
-            return concreteIndex;
-        }
-    }
-
     static class IndexShardReference implements Releasable {
 
         final private IndexShard counter;
-        private final AtomicBoolean closed = new AtomicBoolean(false);
+        private final AtomicBoolean closed = new AtomicBoolean();
 
         IndexShardReference(IndexShard counter) {
             counter.incrementOperationCounter();
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java b/core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java
index e654eda..dd78d7a 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.termvectors;
 
-import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.DocumentRequest;
 import org.elasticsearch.action.support.ActionFilters;
@@ -79,8 +78,8 @@ public class TransportMultiTermVectorsAction extends HandledTransportAction<Mult
                         new IllegalArgumentException("routing is required for [" + concreteSingleIndex + "]/[" + termVectorsRequest.type() + "]/[" + termVectorsRequest.id() + "]"))));
                 continue;
             }
-            ShardId shardId = clusterService.operationRouting().getShards(clusterState, concreteSingleIndex,
-                    termVectorsRequest.type(), termVectorsRequest.id(), termVectorsRequest.routing(), null).shardId();
+            ShardId shardId = clusterService.operationRouting().shardId(clusterState, concreteSingleIndex,
+                    termVectorsRequest.id(), termVectorsRequest.routing());
             MultiTermVectorsShardRequest shardRequest = shardRequests.get(shardId);
             if (shardRequest == null) {
                 shardRequest = new MultiTermVectorsShardRequest(request, shardId.index().name(), shardId.id());
diff --git a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
index b2d24fe..e5edc1a 100644
--- a/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java
@@ -175,7 +175,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 indexAction.execute(upsertRequest, new ActionListener<IndexResponse>() {
                     @Override
                     public void onResponse(IndexResponse response) {
-                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getIndex(), response.getType(), response.getId(), response.getVersion(), response.isCreated());
+                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getShardId(), response.getType(), response.getId(), response.getVersion(), response.isCreated());
                         if (request.fields() != null && request.fields().length > 0) {
                             Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(upsertSourceBytes, true);
                             update.setGetResult(updateHelper.extractGetResult(request, request.concreteIndex(), response.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), upsertSourceBytes));
@@ -212,7 +212,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 indexAction.execute(indexRequest, new ActionListener<IndexResponse>() {
                     @Override
                     public void onResponse(IndexResponse response) {
-                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getIndex(), response.getType(), response.getId(), response.getVersion(), response.isCreated());
+                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getShardId(), response.getType(), response.getId(), response.getVersion(), response.isCreated());
                         update.setGetResult(updateHelper.extractGetResult(request, request.concreteIndex(), response.getVersion(), result.updatedSourceAsMap(), result.updateSourceContentType(), indexSourceBytes));
                         listener.onResponse(update);
                     }
@@ -240,7 +240,7 @@ public class TransportUpdateAction extends TransportInstanceSingleOperationActio
                 deleteAction.execute(deleteRequest, new ActionListener<DeleteResponse>() {
                     @Override
                     public void onResponse(DeleteResponse response) {
-                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getIndex(), response.getType(), response.getId(), response.getVersion(), false);
+                        UpdateResponse update = new UpdateResponse(response.getShardInfo(), response.getShardId(), response.getType(), response.getId(), response.getVersion(), false);
                         update.setGetResult(updateHelper.extractGetResult(request, request.concreteIndex(), response.getVersion(), result.updatedSourceAsMap(), result.updateSourceContentType(), null));
                         listener.onResponse(update);
                     }
diff --git a/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java b/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java
index 4bdcd43..9f8b2a2 100644
--- a/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java
+++ b/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java
@@ -83,9 +83,10 @@ public class UpdateHelper extends AbstractComponent {
     @SuppressWarnings("unchecked")
     protected Result prepare(UpdateRequest request, final GetResult getResult) {
         long getDateNS = System.nanoTime();
+        final ShardId shardId = new ShardId(getResult.getIndex(), request.shardId());
         if (!getResult.isExists()) {
             if (request.upsertRequest() == null && !request.docAsUpsert()) {
-                throw new DocumentMissingException(new ShardId(request.index(), request.shardId()), request.type(), request.id());
+                throw new DocumentMissingException(shardId, request.type(), request.id());
             }
             IndexRequest indexRequest = request.docAsUpsert() ? request.doc() : request.upsertRequest();
             TimeValue ttl = indexRequest.ttl();
@@ -113,7 +114,7 @@ public class UpdateHelper extends AbstractComponent {
                         logger.warn("Used upsert operation [{}] for script [{}], doing nothing...", scriptOpChoice,
                                 request.script.getScript());
                     }
-                    UpdateResponse update = new UpdateResponse(getResult.getIndex(), getResult.getType(), getResult.getId(),
+                    UpdateResponse update = new UpdateResponse(shardId, getResult.getType(), getResult.getId(),
                             getResult.getVersion(), false);
                     update.setGetResult(getResult);
                     return new Result(update, Operation.NONE, upsertDoc, XContentType.JSON);
@@ -145,7 +146,7 @@ public class UpdateHelper extends AbstractComponent {
 
         if (getResult.internalSourceRef() == null) {
             // no source, we can't do nothing, through a failure...
-            throw new DocumentSourceMissingException(new ShardId(request.index(), request.shardId()), request.type(), request.id());
+            throw new DocumentSourceMissingException(shardId, request.type(), request.id());
         }
 
         Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(getResult.internalSourceRef(), true);
@@ -231,12 +232,12 @@ public class UpdateHelper extends AbstractComponent {
                     .consistencyLevel(request.consistencyLevel());
             return new Result(deleteRequest, Operation.DELETE, updatedSourceAsMap, updateSourceContentType);
         } else if ("none".equals(operation)) {
-            UpdateResponse update = new UpdateResponse(getResult.getIndex(), getResult.getType(), getResult.getId(), getResult.getVersion(), false);
+            UpdateResponse update = new UpdateResponse(shardId, getResult.getType(), getResult.getId(), getResult.getVersion(), false);
             update.setGetResult(extractGetResult(request, request.index(), getResult.getVersion(), updatedSourceAsMap, updateSourceContentType, getResult.internalSourceRef()));
             return new Result(update, Operation.NONE, updatedSourceAsMap, updateSourceContentType);
         } else {
             logger.warn("Used update operation [{}] for script [{}], doing nothing...", operation, request.script.getScript());
-            UpdateResponse update = new UpdateResponse(getResult.getIndex(), getResult.getType(), getResult.getId(), getResult.getVersion(), false);
+            UpdateResponse update = new UpdateResponse(shardId, getResult.getType(), getResult.getId(), getResult.getVersion(), false);
             return new Result(update, Operation.NONE, updatedSourceAsMap, updateSourceContentType);
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java b/core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java
index af64380..2f3146b 100644
--- a/core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/update/UpdateResponse.java
@@ -19,21 +19,21 @@
 
 package org.elasticsearch.action.update;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.DocWriteResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.get.GetResult;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
 
 /**
  */
-public class UpdateResponse extends ActionWriteResponse {
+public class UpdateResponse extends DocWriteResponse {
 
-    private String index;
-    private String id;
-    private String type;
-    private long version;
     private boolean created;
     private GetResult getResult;
 
@@ -44,47 +44,16 @@ public class UpdateResponse extends ActionWriteResponse {
      * Constructor to be used when a update didn't translate in a write.
      * For example: update script with operation set to none
      */
-    public UpdateResponse(String index, String type, String id, long version, boolean created) {
-        this(new ShardInfo(0, 0), index, type, id, version, created);
+    public UpdateResponse(ShardId shardId, String type, String id, long version, boolean created) {
+        this(new ShardInfo(0, 0), shardId, type, id, version, created);
     }
 
-    public UpdateResponse(ShardInfo shardInfo, String index, String type, String id, long version, boolean created) {
+    public UpdateResponse(ShardInfo shardInfo, ShardId shardId, String type, String id, long version, boolean created) {
+        super(shardId, type, id, version);
         setShardInfo(shardInfo);
-        this.index = index;
-        this.id = id;
-        this.type = type;
-        this.version = version;
         this.created = created;
     }
 
-    /**
-     * The index the document was indexed into.
-     */
-    public String getIndex() {
-        return this.index;
-    }
-
-    /**
-     * The type of the document indexed.
-     */
-    public String getType() {
-        return this.type;
-    }
-
-    /**
-     * The id of the document indexed.
-     */
-    public String getId() {
-        return this.id;
-    }
-
-    /**
-     * Returns the current version of the doc indexed.
-     */
-    public long getVersion() {
-        return this.version;
-    }
-
     public void setGetResult(GetResult getResult) {
         this.getResult = getResult;
     }
@@ -102,12 +71,16 @@ public class UpdateResponse extends ActionWriteResponse {
     }
 
     @Override
+    public RestStatus status() {
+        if (created) {
+            return RestStatus.CREATED;
+        }
+        return super.status();
+    }
+
+    @Override
     public void readFrom(StreamInput in) throws IOException {
         super.readFrom(in);
-        index = in.readString();
-        type = in.readString();
-        id = in.readString();
-        version = in.readLong();
         created = in.readBoolean();
         if (in.readBoolean()) {
             getResult = GetResult.readGetResult(in);
@@ -117,10 +90,6 @@ public class UpdateResponse extends ActionWriteResponse {
     @Override
     public void writeTo(StreamOutput out) throws IOException {
         super.writeTo(out);
-        out.writeString(index);
-        out.writeString(type);
-        out.writeString(id);
-        out.writeLong(version);
         out.writeBoolean(created);
         if (getResult == null) {
             out.writeBoolean(false);
@@ -129,4 +98,34 @@ public class UpdateResponse extends ActionWriteResponse {
             getResult.writeTo(out);
         }
     }
+
+
+    static final class Fields {
+        static final XContentBuilderString GET = new XContentBuilderString("get");
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        super.toXContent(builder, params);
+        if (getGetResult() != null) {
+            builder.startObject(Fields.GET);
+            getGetResult().toXContentEmbedded(builder, params);
+            builder.endObject();
+        }
+        return builder;
+    }
+
+    @Override
+    public String toString() {
+        StringBuilder builder = new StringBuilder();
+        builder.append("UpdateResponse[");
+        builder.append("index=").append(getIndex());
+        builder.append(",type=").append(getType());
+        builder.append(",id=").append(getId());
+        builder.append(",version=").append(getVersion());
+        builder.append(",created=").append(created);
+        builder.append(",shards=").append(getShardInfo());
+        return builder.append("]").toString();
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
index 3a0ddf1..053f4ae 100644
--- a/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
+++ b/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
@@ -37,7 +37,6 @@ import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.monitor.os.OsProbe;
 import org.elasticsearch.monitor.process.ProcessProbe;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 
 import java.io.ByteArrayOutputStream;
@@ -80,11 +79,11 @@ final class Bootstrap {
             }
         });
     }
-    
+
     /** initialize native resources */
     public static void initializeNatives(Path tmpFile, boolean mlockAll, boolean seccomp, boolean ctrlHandler) {
         final ESLogger logger = Loggers.getLogger(Bootstrap.class);
-        
+
         // check if the user is running as root, and bail
         if (Natives.definitelyRunningAsRoot()) {
             if (Boolean.parseBoolean(System.getProperty("es.insecure.allow.root"))) {
@@ -93,12 +92,12 @@ final class Bootstrap {
                 throw new RuntimeException("don't run elasticsearch as root.");
             }
         }
-        
+
         // enable secure computing mode
         if (seccomp) {
             Natives.trySeccomp(tmpFile);
         }
-        
+
         // mlockall if requested
         if (mlockAll) {
             if (Constants.WINDOWS) {
@@ -175,11 +174,10 @@ final class Bootstrap {
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true)
                 .build();
 
-        NodeBuilder nodeBuilder = NodeBuilder.nodeBuilder().settings(nodeSettings);
-        node = nodeBuilder.build();
+        node = new Node(nodeSettings);
     }
-    
-    /** 
+
+    /**
      * option for elasticsearch.yml etc to turn off our security manager completely,
      * for example if you want to have your own configuration or just disable.
      */
@@ -322,7 +320,7 @@ final class Bootstrap {
             if (foreground) {
                 Loggers.enableConsoleLogging();
             }
-            
+
             throw e;
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
index eb06399..33cf347 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/TransportClient.java
@@ -32,6 +32,7 @@ import org.elasticsearch.client.support.Headers;
 import org.elasticsearch.client.transport.support.TransportProxyClient;
 import org.elasticsearch.cluster.ClusterNameModule;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.LifecycleComponent;
 import org.elasticsearch.common.inject.Injector;
 import org.elasticsearch.common.inject.Module;
@@ -42,9 +43,12 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.SettingsFilter;
 import org.elasticsearch.common.settings.SettingsModule;
 import org.elasticsearch.common.transport.TransportAddress;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.env.EnvironmentModule;
 import org.elasticsearch.indices.breaker.CircuitBreakerModule;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.plugins.PluginsModule;
 import org.elasticsearch.plugins.PluginsService;
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
index ca4de19..2b7786f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.cluster;
 
+import org.elasticsearch.action.admin.indices.close.TransportCloseIndexAction;
+import org.elasticsearch.action.support.DestructiveOperations;
+import org.elasticsearch.action.support.replication.TransportReplicationAction;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.index.NodeIndexDeletedAction;
 import org.elasticsearch.cluster.action.index.NodeMappingRefreshAction;
@@ -26,6 +29,7 @@ import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.IndexTemplateFilter;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.metadata.MetaDataCreateIndexService;
 import org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService;
 import org.elasticsearch.cluster.metadata.MetaDataIndexAliasesService;
@@ -56,15 +60,17 @@ import org.elasticsearch.cluster.routing.allocation.decider.ShardsLimitAllocatio
 import org.elasticsearch.cluster.routing.allocation.decider.SnapshotInProgressAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;
 import org.elasticsearch.cluster.service.InternalClusterService;
-import org.elasticsearch.common.settings.ClusterSettings;
+import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
 import org.elasticsearch.cluster.settings.DynamicSettings;
 import org.elasticsearch.cluster.settings.Validator;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.ExtensionPoint;
+import org.elasticsearch.discovery.DiscoverySettings;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
+import org.elasticsearch.discovery.zen.elect.ElectMasterService;
 import org.elasticsearch.gateway.GatewayAllocator;
 import org.elasticsearch.gateway.PrimaryShardAllocator;
 import org.elasticsearch.index.engine.EngineConfig;
@@ -75,13 +81,21 @@ import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.MergePolicyConfig;
 import org.elasticsearch.index.shard.MergeSchedulerConfig;
 import org.elasticsearch.index.store.IndexStore;
+import org.elasticsearch.index.store.IndexStoreConfig;
 import org.elasticsearch.index.translog.TranslogConfig;
 import org.elasticsearch.indices.IndicesWarmer;
+import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
 import org.elasticsearch.indices.cache.request.IndicesRequestCache;
+import org.elasticsearch.indices.recovery.RecoverySettings;
 import org.elasticsearch.indices.ttl.IndicesTTLService;
+import org.elasticsearch.search.SearchService;
 import org.elasticsearch.search.internal.DefaultSearchContext;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
 
-import java.util.*;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
 
 /**
  * Configures classes and services that affect the entire cluster.
@@ -108,6 +122,7 @@ public class ClusterModule extends AbstractModule {
             SnapshotInProgressAllocationDecider.class));
 
     private final Settings settings;
+    private final DynamicSettings.Builder clusterDynamicSettings = new DynamicSettings.Builder();
     private final DynamicSettings.Builder indexDynamicSettings = new DynamicSettings.Builder();
     private final ExtensionPoint.SelectedType<ShardsAllocator> shardsAllocators = new ExtensionPoint.SelectedType<>("shards_allocator", ShardsAllocator.class);
     private final ExtensionPoint.ClassSet<AllocationDecider> allocationDeciders = new ExtensionPoint.ClassSet<>("allocation_decider", AllocationDecider.class, AllocationDeciders.class);
@@ -119,6 +134,7 @@ public class ClusterModule extends AbstractModule {
     public ClusterModule(Settings settings) {
         this.settings = settings;
 
+        registerBuiltinClusterSettings();
         registerBuiltinIndexSettings();
 
         for (Class<? extends AllocationDecider> decider : ClusterModule.DEFAULT_ALLOCATION_DECIDERS) {
@@ -128,10 +144,70 @@ public class ClusterModule extends AbstractModule {
         registerShardsAllocator(ClusterModule.EVEN_SHARD_COUNT_ALLOCATOR, BalancedShardsAllocator.class);
     }
 
+    private void registerBuiltinClusterSettings() {
+        registerClusterDynamicSetting(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTES, Validator.EMPTY);
+        registerClusterDynamicSetting(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(BalancedShardsAllocator.SETTING_INDEX_BALANCE_FACTOR, Validator.FLOAT);
+        registerClusterDynamicSetting(BalancedShardsAllocator.SETTING_SHARD_BALANCE_FACTOR, Validator.FLOAT);
+        registerClusterDynamicSetting(BalancedShardsAllocator.SETTING_THRESHOLD, Validator.NON_NEGATIVE_FLOAT);
+        registerClusterDynamicSetting(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ALLOCATION_ALLOW_REBALANCE_VALIDATOR);
+        registerClusterDynamicSetting(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, Validator.INTEGER);
+        registerClusterDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, Validator.EMPTY);
+        registerClusterDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, Validator.EMPTY);
+        registerClusterDynamicSetting(ZenDiscovery.SETTING_REJOIN_ON_MASTER_GONE, Validator.BOOLEAN);
+        registerClusterDynamicSetting(DiscoverySettings.NO_MASTER_BLOCK, Validator.EMPTY);
+        registerClusterDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE, Validator.EMPTY);
+        registerClusterDynamicSetting(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
+        registerClusterDynamicSetting(IndicesTTLService.INDICES_TTL_INTERVAL, Validator.TIME);
+        registerClusterDynamicSetting(MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT, Validator.TIME);
+        registerClusterDynamicSetting(MetaData.SETTING_READ_ONLY, Validator.EMPTY);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, Validator.POSITIVE_INTEGER);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, Validator.POSITIVE_INTEGER);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(ThreadPool.THREADPOOL_GROUP + "*", ThreadPool.THREAD_POOL_TYPE_SETTINGS_VALIDATOR);
+        registerClusterDynamicSetting(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES, Validator.INTEGER);
+        registerClusterDynamicSetting(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, Validator.INTEGER);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, Validator.EMPTY);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, Validator.EMPTY);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, Validator.BOOLEAN);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, Validator.BOOLEAN);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(SnapshotInProgressAllocationDecider.CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED, Validator.EMPTY);
+        registerClusterDynamicSetting(DestructiveOperations.REQUIRES_NAME, Validator.EMPTY);
+        registerClusterDynamicSetting(DiscoverySettings.PUBLISH_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(DiscoverySettings.PUBLISH_DIFF_ENABLE, Validator.BOOLEAN);
+        registerClusterDynamicSetting(DiscoverySettings.COMMIT_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, Validator.NON_NEGATIVE_DOUBLE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING, Validator.NON_NEGATIVE_DOUBLE);
+        registerClusterDynamicSetting(InternalClusterService.SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(SearchService.DEFAULT_SEARCH_TIMEOUT, Validator.TIMEOUT);
+        registerClusterDynamicSetting(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_VALIDATOR);
+        registerClusterDynamicSetting(TransportService.SETTING_TRACE_LOG_INCLUDE, Validator.EMPTY);
+        registerClusterDynamicSetting(TransportService.SETTING_TRACE_LOG_INCLUDE + ".*", Validator.EMPTY);
+        registerClusterDynamicSetting(TransportService.SETTING_TRACE_LOG_EXCLUDE, Validator.EMPTY);
+        registerClusterDynamicSetting(TransportService.SETTING_TRACE_LOG_EXCLUDE + ".*", Validator.EMPTY);
+        registerClusterDynamicSetting(TransportCloseIndexAction.SETTING_CLUSTER_INDICES_CLOSE_ENABLE, Validator.BOOLEAN);
+        registerClusterDynamicSetting(ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE, Validator.INTEGER);
+        registerClusterDynamicSetting(TransportReplicationAction.SHARD_FAILURE_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+    }
+
     private void registerBuiltinIndexSettings() {
         registerIndexDynamicSetting(IndexStore.INDEX_STORE_THROTTLE_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
         registerIndexDynamicSetting(IndexStore.INDEX_STORE_THROTTLE_TYPE, Validator.EMPTY);
-        registerIndexDynamicSetting(MergeSchedulerConfig.MAX_THREAD_COUNT, Validator.NON_NEGATIVE_INTEGER);
+        registerIndexDynamicSetting(MergeSchedulerConfig.MAX_THREAD_COUNT, Validator.EMPTY);
         registerIndexDynamicSetting(MergeSchedulerConfig.MAX_MERGE_COUNT, Validator.EMPTY);
         registerIndexDynamicSetting(MergeSchedulerConfig.AUTO_THROTTLE, Validator.EMPTY);
         registerIndexDynamicSetting(FilterAllocationDecider.INDEX_ROUTING_REQUIRE_GROUP + "*", Validator.EMPTY);
@@ -196,6 +272,9 @@ public class ClusterModule extends AbstractModule {
         indexDynamicSettings.addSetting(setting, validator);
     }
 
+    public void registerClusterDynamicSetting(String setting, Validator validator) {
+        clusterDynamicSettings.addSetting(setting, validator);
+    }
 
     public void registerAllocationDecider(Class<? extends AllocationDecider> allocationDecider) {
         allocationDeciders.registerExtension(allocationDecider);
@@ -211,6 +290,7 @@ public class ClusterModule extends AbstractModule {
 
     @Override
     protected void configure() {
+        bind(DynamicSettings.class).annotatedWith(ClusterDynamicSettings.class).toInstance(clusterDynamicSettings.build());
         bind(DynamicSettings.class).annotatedWith(IndexDynamicSettings.class).toInstance(indexDynamicSettings.build());
 
         // bind ShardsAllocator
diff --git a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
index 925a5a1..039868d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
@@ -37,12 +37,11 @@ import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
 import org.elasticsearch.monitor.fs.FsInfo;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.ReceiveTimeoutTransportException;
 
@@ -64,8 +63,8 @@ import java.util.concurrent.TimeUnit;
  */
 public class InternalClusterInfoService extends AbstractComponent implements ClusterInfoService, LocalNodeMasterListener, ClusterStateListener {
 
-    public static final Setting<TimeValue> INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING = Setting.timeSetting("cluster.info.update.interval", TimeValue.timeValueSeconds(30), TimeValue.timeValueSeconds(10), true, Setting.Scope.CLUSTER);
-    public static final Setting<TimeValue> INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING = Setting.positiveTimeSetting("cluster.info.update.timeout", TimeValue.timeValueSeconds(15), true, Setting.Scope.CLUSTER);
+    public static final String INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL = "cluster.info.update.interval";
+    public static final String INTERNAL_CLUSTER_INFO_TIMEOUT = "cluster.info.update.timeout";
 
     private volatile TimeValue updateFrequency;
 
@@ -83,7 +82,7 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
     private final List<Listener> listeners = new CopyOnWriteArrayList<>();
 
     @Inject
-    public InternalClusterInfoService(Settings settings, ClusterSettings clusterSettings,
+    public InternalClusterInfoService(Settings settings, NodeSettingsService nodeSettingsService,
                                       TransportNodesStatsAction transportNodesStatsAction,
                                       TransportIndicesStatsAction transportIndicesStatsAction, ClusterService clusterService,
                                       ThreadPool threadPool) {
@@ -96,12 +95,10 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
         this.transportIndicesStatsAction = transportIndicesStatsAction;
         this.clusterService = clusterService;
         this.threadPool = threadPool;
-        this.updateFrequency = INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING.get(settings);
-        this.fetchTimeout = INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING.get(settings);
-        this.enabled = DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING, this::setFetchTimeout);
-        clusterSettings.addSettingsUpdateConsumer(INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING, this::setUpdateFrequency);
-        clusterSettings.addSettingsUpdateConsumer(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING, this::setEnabled);
+        this.updateFrequency = settings.getAsTime(INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, TimeValue.timeValueSeconds(30));
+        this.fetchTimeout = settings.getAsTime(INTERNAL_CLUSTER_INFO_TIMEOUT, TimeValue.timeValueSeconds(15));
+        this.enabled = settings.getAsBoolean(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true);
+        nodeSettingsService.addListener(new ApplySettings());
 
         // Add InternalClusterInfoService to listen for Master changes
         this.clusterService.add((LocalNodeMasterListener)this);
@@ -109,16 +106,35 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
         this.clusterService.add((ClusterStateListener)this);
     }
 
-    private void setEnabled(boolean enabled) {
-        this.enabled = enabled;
-    }
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            TimeValue newUpdateFrequency = settings.getAsTime(INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, null);
+            // ClusterInfoService is only enabled if the DiskThresholdDecider is enabled
+            Boolean newEnabled = settings.getAsBoolean(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, null);
+
+            if (newUpdateFrequency != null) {
+                if (newUpdateFrequency.getMillis() < TimeValue.timeValueSeconds(10).getMillis()) {
+                    logger.warn("[{}] set too low [{}] (< 10s)", INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, newUpdateFrequency);
+                    throw new IllegalStateException("Unable to set " + INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL + " less than 10 seconds");
+                } else {
+                    logger.info("updating [{}] from [{}] to [{}]", INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, updateFrequency, newUpdateFrequency);
+                    InternalClusterInfoService.this.updateFrequency = newUpdateFrequency;
+                }
+            }
+
+            TimeValue newFetchTimeout = settings.getAsTime(INTERNAL_CLUSTER_INFO_TIMEOUT, null);
+            if (newFetchTimeout != null) {
+                logger.info("updating fetch timeout [{}] from [{}] to [{}]", INTERNAL_CLUSTER_INFO_TIMEOUT, fetchTimeout, newFetchTimeout);
+                InternalClusterInfoService.this.fetchTimeout = newFetchTimeout;
+            }
 
-    private void setFetchTimeout(TimeValue fetchTimeout) {
-        this.fetchTimeout = fetchTimeout;
-    }
 
-    void setUpdateFrequency(TimeValue updateFrequency) {
-        this.updateFrequency = updateFrequency;
+            // We don't log about enabling it here, because the DiskThresholdDecider will already be logging about enable/disable
+            if (newEnabled != null) {
+                InternalClusterInfoService.this.enabled = newEnabled;
+            }
+        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
index 9e57fe3..e3925aa 100644
--- a/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java
@@ -26,12 +26,11 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.client.IndicesAdminClient;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.Mapping;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.util.concurrent.TimeoutException;
 
@@ -41,23 +40,30 @@ import java.util.concurrent.TimeoutException;
  */
 public class MappingUpdatedAction extends AbstractComponent {
 
-    public static final Setting<TimeValue> INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING = Setting.positiveTimeSetting("indices.mapping.dynamic_timeout", TimeValue.timeValueSeconds(30), true, Setting.Scope.CLUSTER);
+    public static final String INDICES_MAPPING_DYNAMIC_TIMEOUT = "indices.mapping.dynamic_timeout";
 
     private IndicesAdminClient client;
     private volatile TimeValue dynamicMappingUpdateTimeout;
 
-    @Inject
-    public MappingUpdatedAction(Settings settings, ClusterSettings clusterSettings) {
-        super(settings);
-        this.dynamicMappingUpdateTimeout = INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING, this::setDynamicMappingUpdateTimeout);
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            TimeValue current = MappingUpdatedAction.this.dynamicMappingUpdateTimeout;
+            TimeValue newValue = settings.getAsTime(INDICES_MAPPING_DYNAMIC_TIMEOUT, current);
+            if (!current.equals(newValue)) {
+                logger.info("updating " + INDICES_MAPPING_DYNAMIC_TIMEOUT + " from [{}] to [{}]", current, newValue);
+                MappingUpdatedAction.this.dynamicMappingUpdateTimeout = newValue;
+            }
+        }
     }
 
-    private void setDynamicMappingUpdateTimeout(TimeValue dynamicMappingUpdateTimeout) {
-        this.dynamicMappingUpdateTimeout = dynamicMappingUpdateTimeout;
+    @Inject
+    public MappingUpdatedAction(Settings settings, NodeSettingsService nodeSettingsService) {
+        super(settings);
+        this.dynamicMappingUpdateTimeout = settings.getAsTime(INDICES_MAPPING_DYNAMIC_TIMEOUT, TimeValue.timeValueSeconds(30));
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
-
     public void setClient(Client client) {
         this.client = client.admin().indices();
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
index 0bcebfb..751f8a0 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java
@@ -40,8 +40,8 @@ import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.support.LoggerMessageFormat;
 import org.elasticsearch.common.regex.Regex;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.loader.SettingsLoader;
 import org.elasticsearch.common.xcontent.FromXContentBuilder;
@@ -140,7 +140,7 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
     }
 
 
-    public static final Setting<Boolean> SETTING_READ_ONLY_SETTING = Setting.boolSetting("cluster.blocks.read_only", false, true, Setting.Scope.CLUSTER);
+    public static final String SETTING_READ_ONLY = "cluster.blocks.read_only";
 
     public static final ClusterBlock CLUSTER_READ_ONLY_BLOCK = new ClusterBlock(6, "cluster read-only (api)", false, false, RestStatus.FORBIDDEN, EnumSet.of(ClusterBlockLevel.WRITE, ClusterBlockLevel.METADATA_WRITE));
 
@@ -745,25 +745,23 @@ public class MetaData implements Iterable<IndexMetaData>, Diffable<MetaData>, Fr
 
     /** All known byte-sized cluster settings. */
     public static final Set<String> CLUSTER_BYTES_SIZE_SETTINGS = unmodifiableSet(newHashSet(
-        IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING.getKey(),
-        RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING.getKey(),
-        RecoverySettings.INDICES_RECOVERY_TRANSLOG_SIZE_SETTING.getKey(),
-        RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey()));
+        IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC,
+        RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC));
 
 
     /** All known time cluster settings. */
     public static final Set<String> CLUSTER_TIME_SETTINGS = unmodifiableSet(newHashSet(
-                                    IndicesTTLService.INDICES_TTL_INTERVAL_SETTING.getKey(),
-                                    RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING.getKey(),
-                                    RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.getKey(),
-                                    RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING.getKey(),
-                                    RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.getKey(),
-                                    RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING.getKey(),
-                                    DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(),
-                                    InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING.getKey(),
-                                    InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING.getKey(),
-                                    DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(),
-                                    InternalClusterService.CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING.getKey()));
+                                    IndicesTTLService.INDICES_TTL_INTERVAL,
+                                    RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC,
+                                    RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK,
+                                    RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT,
+                                    RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT,
+                                    RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT,
+                                    DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL,
+                                    InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL,
+                                    InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT,
+                                    DiscoverySettings.PUBLISH_TIMEOUT,
+                                    InternalClusterService.SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD));
 
     /** As of 2.0 we require units for time and byte-sized settings.  This methods adds default units to any cluster settings that don't
      *  specify a unit. */
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java b/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
index 44542b5..267dae8 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
@@ -25,7 +25,6 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.math.MathUtils;
@@ -55,19 +54,16 @@ public class OperationRouting extends AbstractComponent {
     }
 
     public ShardIterator indexShards(ClusterState clusterState, String index, String type, String id, @Nullable String routing) {
-        return shards(clusterState, index, type, id, routing).shardsIt();
-    }
-
-    public ShardIterator deleteShards(ClusterState clusterState, String index, String type, String id, @Nullable String routing) {
-        return shards(clusterState, index, type, id, routing).shardsIt();
+        return shards(clusterState, index, id, routing).shardsIt();
     }
 
     public ShardIterator getShards(ClusterState clusterState, String index, String type, String id, @Nullable String routing, @Nullable String preference) {
-        return preferenceActiveShardIterator(shards(clusterState, index, type, id, routing), clusterState.nodes().localNodeId(), clusterState.nodes(), preference);
+        return preferenceActiveShardIterator(shards(clusterState, index, id, routing), clusterState.nodes().localNodeId(), clusterState.nodes(), preference);
     }
 
     public ShardIterator getShards(ClusterState clusterState, String index, int shardId, @Nullable String preference) {
-        return preferenceActiveShardIterator(shards(clusterState, index, shardId), clusterState.nodes().localNodeId(), clusterState.nodes(), preference);
+        final IndexShardRoutingTable indexShard = clusterState.getRoutingTable().shardRoutingTable(index, shardId);
+        return preferenceActiveShardIterator(indexShard, clusterState.nodes().localNodeId(), clusterState.nodes(), preference);
     }
 
     public GroupShardsIterator broadcastDeleteShards(ClusterState clusterState, String index) {
@@ -102,7 +98,7 @@ public class OperationRouting extends AbstractComponent {
             final Set<String> effectiveRouting = routing.get(index);
             if (effectiveRouting != null) {
                 for (String r : effectiveRouting) {
-                    int shardId = shardId(clusterState, index, null, null, r);
+                    int shardId = generateShardId(clusterState, index, null, r);
                     IndexShardRoutingTable indexShard = indexRouting.shard(shardId);
                     if (indexShard == null) {
                         throw new ShardNotFoundException(new ShardId(index, shardId));
@@ -200,14 +196,6 @@ public class OperationRouting extends AbstractComponent {
         }
     }
 
-    public IndexMetaData indexMetaData(ClusterState clusterState, String index) {
-        IndexMetaData indexMetaData = clusterState.metaData().index(index);
-        if (indexMetaData == null) {
-            throw new IndexNotFoundException(index);
-        }
-        return indexMetaData;
-    }
-
     protected IndexRoutingTable indexRoutingTable(ClusterState clusterState, String index) {
         IndexRoutingTable indexRouting = clusterState.routingTable().index(index);
         if (indexRouting == null) {
@@ -216,25 +204,20 @@ public class OperationRouting extends AbstractComponent {
         return indexRouting;
     }
 
-
-    // either routing is set, or type/id are set
-
-    protected IndexShardRoutingTable shards(ClusterState clusterState, String index, String type, String id, String routing) {
-        int shardId = shardId(clusterState, index, type, id, routing);
-        return shards(clusterState, index, shardId);
+    protected IndexShardRoutingTable shards(ClusterState clusterState, String index, String id, String routing) {
+        int shardId = generateShardId(clusterState, index, id, routing);
+        return clusterState.getRoutingTable().shardRoutingTable(index, shardId);
     }
 
-    protected IndexShardRoutingTable shards(ClusterState clusterState, String index, int shardId) {
-        IndexShardRoutingTable indexShard = indexRoutingTable(clusterState, index).shard(shardId);
-        if (indexShard == null) {
-            throw new ShardNotFoundException(new ShardId(index, shardId));
-        }
-        return indexShard;
+    public ShardId shardId(ClusterState clusterState, String index, String id, @Nullable String routing) {
+        return new ShardId(index, generateShardId(clusterState, index, id, routing));
     }
 
-    @SuppressForbidden(reason = "Math#abs is trappy")
-    private int shardId(ClusterState clusterState, String index, String type, String id, @Nullable String routing) {
-        final IndexMetaData indexMetaData = indexMetaData(clusterState, index);
+    private int generateShardId(ClusterState clusterState, String index, String id, @Nullable String routing) {
+        IndexMetaData indexMetaData = clusterState.metaData().index(index);
+        if (indexMetaData == null) {
+            throw new IndexNotFoundException(index);
+        }
         final int hash;
         if (routing == null) {
             hash = Murmur3HashFunction.hash(id);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
index badf70a..5d17a59 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
@@ -27,18 +27,11 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.index.shard.ShardId;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 import java.util.function.Predicate;
 
 /**
@@ -671,7 +664,7 @@ public class RoutingNodes implements Iterable<RoutingNode> {
         }
 
         public void shuffle() {
-            Collections.shuffle(unassigned);
+            Randomness.shuffle(unassigned);
         }
 
         /**
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
index c210539..fbabacd 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
@@ -33,6 +33,8 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.index.IndexNotFoundException;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.shard.ShardNotFoundException;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -95,6 +97,24 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         return indicesRouting();
     }
 
+    /**
+     * All shards for the provided index and shard id
+     * @return All the shard routing entries for the given index and shard id
+     * @throws IndexNotFoundException if provided index does not exist
+     * @throws ShardNotFoundException if provided shard id is unknown
+     */
+    public IndexShardRoutingTable shardRoutingTable(String index, int shardId) {
+        IndexRoutingTable indexRouting = index(index);
+        if (indexRouting == null) {
+            throw new IndexNotFoundException(index);
+        }
+        IndexShardRoutingTable shard = indexRouting.shard(shardId);
+        if (shard == null) {
+            throw new ShardNotFoundException(new ShardId(index, shardId));
+        }
+        return shard;
+    }
+
     public RoutingTable validateRaiseException(MetaData metaData) throws RoutingValidationException {
         RoutingTableValidation validation = validate(metaData);
         if (!validation.valid()) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
index feafb76..2268bf1 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java
@@ -39,15 +39,12 @@ import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
 import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;
 import org.elasticsearch.cluster.routing.allocation.decider.Decision;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Objects;
-import java.util.Set;
+import java.util.*;
 import java.util.function.Function;
 import java.util.stream.Collectors;
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
index e6dc9a6..b9ce532 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
@@ -34,13 +34,12 @@ import org.elasticsearch.cluster.routing.allocation.StartedRerouteAllocation;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;
 import org.elasticsearch.cluster.routing.allocation.decider.Decision;
 import org.elasticsearch.cluster.routing.allocation.decider.Decision.Type;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.gateway.PriorityComparator;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.util.ArrayList;
 import java.util.Collection;
@@ -73,32 +72,42 @@ import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
  */
 public class BalancedShardsAllocator extends AbstractComponent implements ShardsAllocator {
 
-    public static final Setting<Float> INDEX_BALANCE_FACTOR_SETTING = Setting.floatSetting("cluster.routing.allocation.balance.index", 0.55f, true, Setting.Scope.CLUSTER);
-    public static final Setting<Float> SHARD_BALANCE_FACTOR_SETTING = Setting.floatSetting("cluster.routing.allocation.balance.shard", 0.45f, true, Setting.Scope.CLUSTER);
-    public static final Setting<Float> THRESHOLD_SETTING = Setting.floatSetting("cluster.routing.allocation.balance.threshold", 1.0f, 0.0f, true, Setting.Scope.CLUSTER);
+    public static final String SETTING_THRESHOLD = "cluster.routing.allocation.balance.threshold";
+    public static final String SETTING_INDEX_BALANCE_FACTOR = "cluster.routing.allocation.balance.index";
+    public static final String SETTING_SHARD_BALANCE_FACTOR = "cluster.routing.allocation.balance.shard";
 
-    private volatile WeightFunction weightFunction;
-    private volatile float threshold;
+    private static final float DEFAULT_INDEX_BALANCE_FACTOR = 0.55f;
+    private static final float DEFAULT_SHARD_BALANCE_FACTOR = 0.45f;
 
-    public BalancedShardsAllocator(Settings settings) {
-        this(settings, new ClusterSettings(settings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS));
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            final float indexBalance = settings.getAsFloat(SETTING_INDEX_BALANCE_FACTOR, weightFunction.indexBalance);
+            final float shardBalance = settings.getAsFloat(SETTING_SHARD_BALANCE_FACTOR, weightFunction.shardBalance);
+            float threshold = settings.getAsFloat(SETTING_THRESHOLD, BalancedShardsAllocator.this.threshold);
+            if (threshold <= 0.0f) {
+                throw new IllegalArgumentException("threshold must be greater than 0.0f but was: " + threshold);
+            }
+            BalancedShardsAllocator.this.threshold = threshold;
+            BalancedShardsAllocator.this.weightFunction = new WeightFunction(indexBalance, shardBalance);
+        }
     }
 
-    @Inject
-    public BalancedShardsAllocator(Settings settings, ClusterSettings clusterSettings) {
-        super(settings);
-        setWeightFunction(INDEX_BALANCE_FACTOR_SETTING.get(settings), SHARD_BALANCE_FACTOR_SETTING.get(settings));
-        setThreshold(THRESHOLD_SETTING.get(settings));
-        clusterSettings.addSettingsUpdateConsumer(INDEX_BALANCE_FACTOR_SETTING, SHARD_BALANCE_FACTOR_SETTING, this::setWeightFunction);
-        clusterSettings.addSettingsUpdateConsumer(THRESHOLD_SETTING, this::setThreshold);
-    }
+    private volatile WeightFunction weightFunction = new WeightFunction(DEFAULT_INDEX_BALANCE_FACTOR, DEFAULT_SHARD_BALANCE_FACTOR);
+
+    private volatile float threshold = 1.0f;
+
 
-    private void setWeightFunction(float indexBalance, float shardBalanceFactor) {
-        weightFunction = new WeightFunction(indexBalance, shardBalanceFactor);
+    public BalancedShardsAllocator(Settings settings) {
+        this(settings, new NodeSettingsService(settings));
     }
 
-    private void setThreshold(float threshold) {
-        this.threshold = threshold;
+    @Inject
+    public BalancedShardsAllocator(Settings settings, NodeSettingsService nodeSettingsService) {
+        super(settings);
+        ApplySettings applySettings = new ApplySettings();
+        applySettings.onRefreshSettings(settings);
+        nodeSettingsService.addListener(applySettings);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
index a66c8dd..6f7bbac 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java
@@ -24,11 +24,10 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.util.HashMap;
 import java.util.Map;
@@ -77,12 +76,37 @@ public class AwarenessAllocationDecider extends AllocationDecider {
 
     public static final String NAME = "awareness";
 
-    public static final Setting<String[]> CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING = new Setting<>("cluster.routing.allocation.awareness.attributes", "", Strings::splitStringByCommaToArray , true, Setting.Scope.CLUSTER);
-    public static final Setting<Settings> CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING = Setting.groupSetting("cluster.routing.allocation.awareness.force.", true, Setting.Scope.CLUSTER);
+    public static final String CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTES = "cluster.routing.allocation.awareness.attributes";
+    public static final String CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP = "cluster.routing.allocation.awareness.force.";
+
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            String[] awarenessAttributes = settings.getAsArray(CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTES, null);
+            if (awarenessAttributes == null && "".equals(settings.get(CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTES, null))) {
+                awarenessAttributes = Strings.EMPTY_ARRAY; // the empty string resets this
+            }
+            if (awarenessAttributes != null) {
+                logger.info("updating [cluster.routing.allocation.awareness.attributes] from [{}] to [{}]", AwarenessAllocationDecider.this.awarenessAttributes, awarenessAttributes);
+                AwarenessAllocationDecider.this.awarenessAttributes = awarenessAttributes;
+            }
+            Map<String, String[]> forcedAwarenessAttributes = new HashMap<>(AwarenessAllocationDecider.this.forcedAwarenessAttributes);
+            Map<String, Settings> forceGroups = settings.getGroups(CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP);
+            if (!forceGroups.isEmpty()) {
+                for (Map.Entry<String, Settings> entry : forceGroups.entrySet()) {
+                    String[] aValues = entry.getValue().getAsArray("values");
+                    if (aValues.length > 0) {
+                        forcedAwarenessAttributes.put(entry.getKey(), aValues);
+                    }
+                }
+            }
+            AwarenessAllocationDecider.this.forcedAwarenessAttributes = forcedAwarenessAttributes;
+        }
+    }
 
     private String[] awarenessAttributes;
 
-    private volatile Map<String, String[]> forcedAwarenessAttributes;
+    private Map<String, String[]> forcedAwarenessAttributes;
 
     /**
      * Creates a new {@link AwarenessAllocationDecider} instance
@@ -97,28 +121,24 @@ public class AwarenessAllocationDecider extends AllocationDecider {
      * @param settings {@link Settings} to use
      */
     public AwarenessAllocationDecider(Settings settings) {
-        this(settings, new ClusterSettings(settings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS));
+        this(settings, new NodeSettingsService(settings));
     }
 
     @Inject
-    public AwarenessAllocationDecider(Settings settings, ClusterSettings clusterSettings) {
+    public AwarenessAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        this.awarenessAttributes = CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING, this::setAwarenessAttributes);
-        setForcedAwarenessAttributes(CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING.get(settings));
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING, this::setForcedAwarenessAttributes);
-    }
+        this.awarenessAttributes = settings.getAsArray(CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTES);
 
-    private void setForcedAwarenessAttributes(Settings forceSettings) {
-        Map<String, String[]> forcedAwarenessAttributes = new HashMap<>();
-        Map<String, Settings> forceGroups = forceSettings.getAsGroups();
+        forcedAwarenessAttributes = new HashMap<>();
+        Map<String, Settings> forceGroups = settings.getGroups(CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP);
         for (Map.Entry<String, Settings> entry : forceGroups.entrySet()) {
             String[] aValues = entry.getValue().getAsArray("values");
             if (aValues.length > 0) {
                 forcedAwarenessAttributes.put(entry.getKey(), aValues);
             }
         }
-        this.forcedAwarenessAttributes = forcedAwarenessAttributes;
+
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
     /**
@@ -130,10 +150,6 @@ public class AwarenessAllocationDecider extends AllocationDecider {
         return this.awarenessAttributes;
     }
 
-    private void setAwarenessAttributes(String[] awarenessAttributes) {
-        this.awarenessAttributes = awarenessAttributes;
-    }
-
     @Override
     public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {
         return underCapacity(shardRouting, node, allocation, true);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java
index b1be2a6..7638c7a 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java
@@ -19,12 +19,13 @@
 
 package org.elasticsearch.cluster.routing.allocation.decider;
 
+import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
+import org.elasticsearch.cluster.settings.Validator;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.util.Locale;
 
@@ -37,10 +38,10 @@ import java.util.Locale;
  * <ul>
  * <li><tt>indices_primaries_active</tt> - Re-balancing is allowed only once all
  * primary shards on all indices are active.</li>
- *
+ * 
  * <li><tt>indices_all_active</tt> - Re-balancing is allowed only once all
  * shards on all indices are active.</li>
- *
+ * 
  * <li><tt>always</tt> - Re-balancing is allowed once a shard replication group
  * is active</li>
  * </ul>
@@ -48,10 +49,19 @@ import java.util.Locale;
 public class ClusterRebalanceAllocationDecider extends AllocationDecider {
 
     public static final String NAME = "cluster_rebalance";
-    public static final Setting<ClusterRebalanceType> CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING = new Setting<>("cluster.routing.allocation.allow_rebalance", ClusterRebalanceType.INDICES_ALL_ACTIVE.name().toLowerCase(Locale.ROOT), ClusterRebalanceType::parseString, true, Setting.Scope.CLUSTER);
+
+    public static final String CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE = "cluster.routing.allocation.allow_rebalance";
+    public static final Validator ALLOCATION_ALLOW_REBALANCE_VALIDATOR = (setting, value, clusterState) -> {
+        try {
+            ClusterRebalanceType.parseString(value);
+            return null;
+        } catch (IllegalArgumentException e) {
+            return "the value of " + setting + " must be one of: [always, indices_primaries_active, indices_all_active]";
+        }
+    };
 
     /**
-     * An enum representation for the configured re-balance type.
+     * An enum representation for the configured re-balance type. 
      */
     public static enum ClusterRebalanceType {
         /**
@@ -63,7 +73,7 @@ public class ClusterRebalanceAllocationDecider extends AllocationDecider {
          */
         INDICES_PRIMARIES_ACTIVE,
         /**
-         * Re-balancing is allowed only once all shards on all indices are active.
+         * Re-balancing is allowed only once all shards on all indices are active. 
          */
         INDICES_ALL_ACTIVE;
 
@@ -75,28 +85,48 @@ public class ClusterRebalanceAllocationDecider extends AllocationDecider {
             } else if ("indices_all_active".equalsIgnoreCase(typeString) || "indicesAllActive".equalsIgnoreCase(typeString)) {
                 return ClusterRebalanceType.INDICES_ALL_ACTIVE;
             }
-            throw new IllegalArgumentException("Illegal value for " + CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING + ": " + typeString);
+            throw new IllegalArgumentException("Illegal value for " + CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE + ": " + typeString);
         }
     }
 
-    private volatile ClusterRebalanceType type;
+    private ClusterRebalanceType type;
 
     @Inject
-    public ClusterRebalanceAllocationDecider(Settings settings, ClusterSettings clusterSettings) {
+    public ClusterRebalanceAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
+        String allowRebalance = settings.get(CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "indices_all_active");
         try {
-            type = CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.get(settings);
+            type = ClusterRebalanceType.parseString(allowRebalance);
         } catch (IllegalStateException e) {
-            logger.warn("[{}] has a wrong value {}, defaulting to 'indices_all_active'", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING, CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getRaw(settings));
+            logger.warn("[{}] has a wrong value {}, defaulting to 'indices_all_active'", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, allowRebalance);
             type = ClusterRebalanceType.INDICES_ALL_ACTIVE;
         }
-        logger.debug("using [{}] with [{}]", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING, type.toString().toLowerCase(Locale.ROOT));
+        logger.debug("using [{}] with [{}]", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, type.toString().toLowerCase(Locale.ROOT));
 
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING, this::setType);
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
-    private void setType(ClusterRebalanceType type) {
-        this.type = type;
+    class ApplySettings implements NodeSettingsService.Listener {
+
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            String newAllowRebalance = settings.get(CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, null);
+            if (newAllowRebalance != null) {
+                ClusterRebalanceType newType = null;
+                try {
+                    newType = ClusterRebalanceType.parseString(newAllowRebalance);
+                } catch (IllegalArgumentException e) {
+                    // ignore
+                }
+
+                if (newType != null && newType != ClusterRebalanceAllocationDecider.this.type) {
+                    logger.info("updating [{}] from [{}] to [{}]", CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
+                            ClusterRebalanceAllocationDecider.this.type.toString().toLowerCase(Locale.ROOT),
+                            newType.toString().toLowerCase(Locale.ROOT));
+                    ClusterRebalanceAllocationDecider.this.type = newType;
+                }
+            }
+        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
index 504ea5d..6bd1b43 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java
@@ -22,9 +22,8 @@ package org.elasticsearch.cluster.routing.allocation.decider;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 /**
  * Similar to the {@link ClusterRebalanceAllocationDecider} this
@@ -42,19 +41,27 @@ public class ConcurrentRebalanceAllocationDecider extends AllocationDecider {
 
     public static final String NAME = "concurrent_rebalance";
 
-    public static final Setting<Integer> CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING = Setting.intSetting("cluster.routing.allocation.cluster_concurrent_rebalance", 2, 0, true, Setting.Scope.CLUSTER);
+    public static final String CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE = "cluster.routing.allocation.cluster_concurrent_rebalance";
+
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            int clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance);
+            if (clusterConcurrentRebalance != ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance) {
+                logger.info("updating [cluster.routing.allocation.cluster_concurrent_rebalance] from [{}], to [{}]", ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance, clusterConcurrentRebalance);
+                ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance = clusterConcurrentRebalance;
+            }
+        }
+    }
+
     private volatile int clusterConcurrentRebalance;
 
     @Inject
-    public ConcurrentRebalanceAllocationDecider(Settings settings, ClusterSettings clusterSettings) {
+    public ConcurrentRebalanceAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        this.clusterConcurrentRebalance = CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING.get(settings);
+        this.clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, 2);
         logger.debug("using [cluster_concurrent_rebalance] with [{}]", clusterConcurrentRebalance);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING, this::setClusterConcurrentRebalance);
-    }
-
-    private void setClusterConcurrentRebalance(int concurrentRebalance) {
-        clusterConcurrentRebalance = concurrentRebalance;
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
index 400ed70..a02c72c 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
@@ -33,16 +33,15 @@ import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.RatioValue;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.set.Sets;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.util.Set;
 
@@ -81,11 +80,53 @@ public class DiskThresholdDecider extends AllocationDecider {
     private volatile boolean enabled;
     private volatile TimeValue rerouteInterval;
 
-    public static final Setting<Boolean> CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING = Setting.boolSetting("cluster.routing.allocation.disk.threshold_enabled", true, true, Setting.Scope.CLUSTER);
-    public static final Setting<String> CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING = new Setting<>("cluster.routing.allocation.disk.watermark.low", "85%", (s) -> validWatermarkSetting(s, "cluster.routing.allocation.disk.watermark.low"), true, Setting.Scope.CLUSTER);
-    public static final Setting<String> CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING = new Setting<>("cluster.routing.allocation.disk.watermark.high", "90%", (s) -> validWatermarkSetting(s, "cluster.routing.allocation.disk.watermark.high"), true, Setting.Scope.CLUSTER);
-    public static final Setting<Boolean> CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING = Setting.boolSetting("cluster.routing.allocation.disk.include_relocations", true, true, Setting.Scope.CLUSTER);;
-    public static final Setting<TimeValue> CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING = Setting.positiveTimeSetting("cluster.routing.allocation.disk.reroute_interval", TimeValue.timeValueSeconds(60), true, Setting.Scope.CLUSTER);
+    public static final String CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED = "cluster.routing.allocation.disk.threshold_enabled";
+    public static final String CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK = "cluster.routing.allocation.disk.watermark.low";
+    public static final String CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK = "cluster.routing.allocation.disk.watermark.high";
+    public static final String CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS = "cluster.routing.allocation.disk.include_relocations";
+    public static final String CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL = "cluster.routing.allocation.disk.reroute_interval";
+
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            String newLowWatermark = settings.get(CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, null);
+            String newHighWatermark = settings.get(CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, null);
+            Boolean newRelocationsSetting = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, null);
+            Boolean newEnableSetting =  settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, null);
+            TimeValue newRerouteInterval = settings.getAsTime(CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL, null);
+
+            if (newEnableSetting != null) {
+                logger.info("updating [{}] from [{}] to [{}]", CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED,
+                        DiskThresholdDecider.this.enabled, newEnableSetting);
+                DiskThresholdDecider.this.enabled = newEnableSetting;
+            }
+            if (newRelocationsSetting != null) {
+                logger.info("updating [{}] from [{}] to [{}]", CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS,
+                        DiskThresholdDecider.this.includeRelocations, newRelocationsSetting);
+                DiskThresholdDecider.this.includeRelocations = newRelocationsSetting;
+            }
+            if (newLowWatermark != null) {
+                if (!validWatermarkSetting(newLowWatermark, CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK)) {
+                    throw new ElasticsearchParseException("unable to parse low watermark [{}]", newLowWatermark);
+                }
+                logger.info("updating [{}] to [{}]", CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, newLowWatermark);
+                DiskThresholdDecider.this.freeDiskThresholdLow = 100.0 - thresholdPercentageFromWatermark(newLowWatermark);
+                DiskThresholdDecider.this.freeBytesThresholdLow = thresholdBytesFromWatermark(newLowWatermark, CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK);
+            }
+            if (newHighWatermark != null) {
+                if (!validWatermarkSetting(newHighWatermark, CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK)) {
+                    throw new ElasticsearchParseException("unable to parse high watermark [{}]", newHighWatermark);
+                }
+                logger.info("updating [{}] to [{}]", CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, newHighWatermark);
+                DiskThresholdDecider.this.freeDiskThresholdHigh = 100.0 - thresholdPercentageFromWatermark(newHighWatermark);
+                DiskThresholdDecider.this.freeBytesThresholdHigh = thresholdBytesFromWatermark(newHighWatermark, CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK);
+            }
+            if (newRerouteInterval != null) {
+                logger.info("updating [{}] to [{}]", CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL, newRerouteInterval);
+                DiskThresholdDecider.this.rerouteInterval = newRerouteInterval;
+            }
+        }
+    }
 
     /**
      * Listens for a node to go over the high watermark and kicks off an empty
@@ -190,49 +231,38 @@ public class DiskThresholdDecider extends AllocationDecider {
         // It's okay the Client is null here, because the empty cluster info
         // service will never actually call the listener where the client is
         // needed. Also this constructor is only used for tests
-        this(settings, new ClusterSettings(settings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), EmptyClusterInfoService.INSTANCE, null);
+        this(settings, new NodeSettingsService(settings), EmptyClusterInfoService.INSTANCE, null);
     }
 
     @Inject
-    public DiskThresholdDecider(Settings settings, ClusterSettings clusterSettings, ClusterInfoService infoService, Client client) {
+    public DiskThresholdDecider(Settings settings, NodeSettingsService nodeSettingsService, ClusterInfoService infoService, Client client) {
         super(settings);
-        final String lowWatermark = CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.get(settings);
-        final String highWatermark = CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.get(settings);
-        setHighWatermark(highWatermark);
-        setLowWatermark(lowWatermark);
-        this.includeRelocations = CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING.get(settings);
-        this.rerouteInterval = CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.get(settings);
-        this.enabled = CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING, this::setLowWatermark);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING, this::setHighWatermark);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING, this::setIncludeRelocations);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING, this::setRerouteInterval);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING, this::setEnabled);
-        infoService.addListener(new DiskListener(client));
-    }
-
-    private void setIncludeRelocations(boolean includeRelocations) {
-        this.includeRelocations = includeRelocations;
-    }
+        String lowWatermark = settings.get(CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "85%");
+        String highWatermark = settings.get(CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "90%");
 
-    private void setRerouteInterval(TimeValue rerouteInterval) {
-        this.rerouteInterval = rerouteInterval;
-    }
-
-    private void setEnabled(boolean enabled) {
-        this.enabled = enabled;
-    }
-
-    private void setLowWatermark(String lowWatermark) {
+        if (!validWatermarkSetting(lowWatermark, CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK)) {
+            throw new ElasticsearchParseException("unable to parse low watermark [{}]", lowWatermark);
+        }
+        if (!validWatermarkSetting(highWatermark, CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK)) {
+            throw new ElasticsearchParseException("unable to parse high watermark [{}]", highWatermark);
+        }
         // Watermark is expressed in terms of used data, but we need "free" data watermark
         this.freeDiskThresholdLow = 100.0 - thresholdPercentageFromWatermark(lowWatermark);
-        this.freeBytesThresholdLow = thresholdBytesFromWatermark(lowWatermark, CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey());
+        this.freeDiskThresholdHigh = 100.0 - thresholdPercentageFromWatermark(highWatermark);
+
+        this.freeBytesThresholdLow = thresholdBytesFromWatermark(lowWatermark, CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK);
+        this.freeBytesThresholdHigh = thresholdBytesFromWatermark(highWatermark, CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK);
+        this.includeRelocations = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, true);
+        this.rerouteInterval = settings.getAsTime(CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL, TimeValue.timeValueSeconds(60));
+
+        this.enabled = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true);
+        nodeSettingsService.addListener(new ApplySettings());
+        infoService.addListener(new DiskListener(client));
     }
 
-    private void setHighWatermark(String highWatermark) {
-        // Watermark is expressed in terms of used data, but we need "free" data watermark
-        this.freeDiskThresholdHigh = 100.0 - thresholdPercentageFromWatermark(highWatermark);
-        this.freeBytesThresholdHigh = thresholdBytesFromWatermark(highWatermark, CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey());
+    // For Testing
+    ApplySettings newApplySettings() {
+        return new ApplySettings();
     }
 
     // For Testing
@@ -550,21 +580,20 @@ public class DiskThresholdDecider extends AllocationDecider {
 
     /**
      * Checks if a watermark string is a valid percentage or byte size value,
-     * @return the watermark value given
+     * returning true if valid, false if invalid.
      */
-    public static String validWatermarkSetting(String watermark, String settingName) {
+    public boolean validWatermarkSetting(String watermark, String settingName) {
         try {
             RatioValue.parseRatioValue(watermark);
+            return true;
         } catch (ElasticsearchParseException e) {
             try {
                 ByteSizeValue.parseBytesSizeValue(watermark, settingName);
+                return true;
             } catch (ElasticsearchParseException ex) {
-                ex.addSuppressed(e);
-                throw ex;
+                return false;
             }
         }
-        return watermark;
-
     }
 
     private Decision earlyTerminate(RoutingAllocation allocation, ImmutableOpenMap<String, DiskUsage> usages) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java
index 1df3623..0bbd493 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java
@@ -23,15 +23,14 @@ import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.util.Locale;
 
 /**
- * This allocation decider allows shard allocations / rebalancing via the cluster wide settings {@link #CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING} /
- * {@link #CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING} and the per index setting {@link #INDEX_ROUTING_ALLOCATION_ENABLE} / {@link #INDEX_ROUTING_REBALANCE_ENABLE}.
+ * This allocation decider allows shard allocations / rebalancing via the cluster wide settings {@link #CLUSTER_ROUTING_ALLOCATION_ENABLE} /
+ * {@link #CLUSTER_ROUTING_REBALANCE_ENABLE} and the per index setting {@link #INDEX_ROUTING_ALLOCATION_ENABLE} / {@link #INDEX_ROUTING_REBALANCE_ENABLE}.
  * The per index settings overrides the cluster wide setting.
  *
  * <p>
@@ -55,34 +54,26 @@ import java.util.Locale;
  * @see Rebalance
  * @see Allocation
  */
-public class EnableAllocationDecider extends AllocationDecider {
+public class EnableAllocationDecider extends AllocationDecider implements NodeSettingsService.Listener {
 
     public static final String NAME = "enable";
 
-    public static final Setting<Allocation> CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING = new Setting<>("cluster.routing.allocation.enable", Allocation.ALL.name(), Allocation::parse, true, Setting.Scope.CLUSTER);
-    public static final String INDEX_ROUTING_ALLOCATION_ENABLE= "index.routing.allocation.enable";
+    public static final String CLUSTER_ROUTING_ALLOCATION_ENABLE = "cluster.routing.allocation.enable";
+    public static final String INDEX_ROUTING_ALLOCATION_ENABLE = "index.routing.allocation.enable";
 
-    public static final Setting<Rebalance> CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING = new Setting<>("cluster.routing.rebalance.enable", Rebalance.ALL.name(), Rebalance::parse, true, Setting.Scope.CLUSTER);
+    public static final String CLUSTER_ROUTING_REBALANCE_ENABLE = "cluster.routing.rebalance.enable";
     public static final String INDEX_ROUTING_REBALANCE_ENABLE = "index.routing.rebalance.enable";
 
     private volatile Rebalance enableRebalance;
     private volatile Allocation enableAllocation;
 
+
     @Inject
-    public EnableAllocationDecider(Settings settings, ClusterSettings clusterSettings) {
+    public EnableAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        this.enableAllocation = CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.get(settings);
-        this.enableRebalance = CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING, this::setEnableAllocation);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING, this::setEnableRebalance);
-    }
-
-    public void setEnableRebalance(Rebalance enableRebalance) {
-        this.enableRebalance = enableRebalance;
-    }
-
-    public void setEnableAllocation(Allocation enableAllocation) {
-        this.enableAllocation = enableAllocation;
+        this.enableAllocation = Allocation.parse(settings.get(CLUSTER_ROUTING_ALLOCATION_ENABLE, Allocation.ALL.name()));
+        this.enableRebalance = Rebalance.parse(settings.get(CLUSTER_ROUTING_REBALANCE_ENABLE, Rebalance.ALL.name()));
+        nodeSettingsService.addListener(this);
     }
 
     @Override
@@ -157,9 +148,25 @@ public class EnableAllocationDecider extends AllocationDecider {
         }
     }
 
+    @Override
+    public void onRefreshSettings(Settings settings) {
+        final Allocation enable = Allocation.parse(settings.get(CLUSTER_ROUTING_ALLOCATION_ENABLE, this.enableAllocation.name()));
+        if (enable != this.enableAllocation) {
+            logger.info("updating [{}] from [{}] to [{}]", CLUSTER_ROUTING_ALLOCATION_ENABLE, this.enableAllocation, enable);
+            EnableAllocationDecider.this.enableAllocation = enable;
+        }
+
+        final Rebalance enableRebalance = Rebalance.parse(settings.get(CLUSTER_ROUTING_REBALANCE_ENABLE, this.enableRebalance.name()));
+        if (enableRebalance != this.enableRebalance) {
+            logger.info("updating [{}] from [{}] to [{}]", CLUSTER_ROUTING_REBALANCE_ENABLE, this.enableRebalance, enableRebalance);
+            EnableAllocationDecider.this.enableRebalance = enableRebalance;
+        }
+
+    }
+
     /**
      * Allocation values or rather their string representation to be used used with
-     * {@link EnableAllocationDecider#CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING} / {@link EnableAllocationDecider#INDEX_ROUTING_ALLOCATION_ENABLE}
+     * {@link EnableAllocationDecider#CLUSTER_ROUTING_ALLOCATION_ENABLE} / {@link EnableAllocationDecider#INDEX_ROUTING_ALLOCATION_ENABLE}
      * via cluster / index settings.
      */
     public enum Allocation {
@@ -185,7 +192,7 @@ public class EnableAllocationDecider extends AllocationDecider {
 
     /**
      * Rebalance values or rather their string representation to be used used with
-     * {@link EnableAllocationDecider#CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING} / {@link EnableAllocationDecider#INDEX_ROUTING_REBALANCE_ENABLE}
+     * {@link EnableAllocationDecider#CLUSTER_ROUTING_REBALANCE_ENABLE} / {@link EnableAllocationDecider#INDEX_ROUTING_REBALANCE_ENABLE}
      * via cluster / index settings.
      */
     public enum Rebalance {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
index 4c451e7..e0e2caa 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java
@@ -25,9 +25,10 @@ import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
+
+import java.util.Map;
 
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.AND;
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.OR;
@@ -64,23 +65,36 @@ public class FilterAllocationDecider extends AllocationDecider {
     public static final String INDEX_ROUTING_INCLUDE_GROUP = "index.routing.allocation.include.";
     public static final String INDEX_ROUTING_EXCLUDE_GROUP = "index.routing.allocation.exclude.";
 
-    public static final Setting<Settings> CLUSTER_ROUTING_REQUIRE_GROUP_SETTING = Setting.groupSetting("cluster.routing.allocation.require.", true, Setting.Scope.CLUSTER);
-    public static final Setting<Settings> CLUSTER_ROUTING_INCLUDE_GROUP_SETTING = Setting.groupSetting("cluster.routing.allocation.include.", true, Setting.Scope.CLUSTER);
-    public static final Setting<Settings> CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING = Setting.groupSetting("cluster.routing.allocation.exclude.", true, Setting.Scope.CLUSTER);
+    public static final String CLUSTER_ROUTING_REQUIRE_GROUP = "cluster.routing.allocation.require.";
+    public static final String CLUSTER_ROUTING_INCLUDE_GROUP = "cluster.routing.allocation.include.";
+    public static final String CLUSTER_ROUTING_EXCLUDE_GROUP = "cluster.routing.allocation.exclude.";
 
     private volatile DiscoveryNodeFilters clusterRequireFilters;
     private volatile DiscoveryNodeFilters clusterIncludeFilters;
     private volatile DiscoveryNodeFilters clusterExcludeFilters;
 
     @Inject
-    public FilterAllocationDecider(Settings settings, ClusterSettings clusterSettings) {
+    public FilterAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        setClusterRequireFilters(CLUSTER_ROUTING_REQUIRE_GROUP_SETTING.get(settings));
-        setClusterExcludeFilters(CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING.get(settings));
-        setClusterIncludeFilters(CLUSTER_ROUTING_INCLUDE_GROUP_SETTING.get(settings));
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_REQUIRE_GROUP_SETTING, this::setClusterRequireFilters);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING, this::setClusterExcludeFilters);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_INCLUDE_GROUP_SETTING, this::setClusterIncludeFilters);
+        Map<String, String> requireMap = settings.getByPrefix(CLUSTER_ROUTING_REQUIRE_GROUP).getAsMap();
+        if (requireMap.isEmpty()) {
+            clusterRequireFilters = null;
+        } else {
+            clusterRequireFilters = DiscoveryNodeFilters.buildFromKeyValue(AND, requireMap);
+        }
+        Map<String, String> includeMap = settings.getByPrefix(CLUSTER_ROUTING_INCLUDE_GROUP).getAsMap();
+        if (includeMap.isEmpty()) {
+            clusterIncludeFilters = null;
+        } else {
+            clusterIncludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, includeMap);
+        }
+        Map<String, String> excludeMap = settings.getByPrefix(CLUSTER_ROUTING_EXCLUDE_GROUP).getAsMap();
+        if (excludeMap.isEmpty()) {
+            clusterExcludeFilters = null;
+        } else {
+            clusterExcludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, excludeMap);
+        }
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
     @Override
@@ -130,13 +144,21 @@ public class FilterAllocationDecider extends AllocationDecider {
         return allocation.decision(Decision.YES, NAME, "node passes include/exclude/require filters");
     }
 
-    private void setClusterRequireFilters(Settings settings) {
-        clusterRequireFilters = DiscoveryNodeFilters.buildFromKeyValue(AND, settings.getAsMap());
-    }
-    private void setClusterIncludeFilters(Settings settings) {
-        clusterIncludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, settings.getAsMap());
-    }
-    private void setClusterExcludeFilters(Settings settings) {
-        clusterExcludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, settings.getAsMap());
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            Map<String, String> requireMap = settings.getByPrefix(CLUSTER_ROUTING_REQUIRE_GROUP).getAsMap();
+            if (!requireMap.isEmpty()) {
+                clusterRequireFilters = DiscoveryNodeFilters.buildFromKeyValue(AND, requireMap);
+            }
+            Map<String, String> includeMap = settings.getByPrefix(CLUSTER_ROUTING_INCLUDE_GROUP).getAsMap();
+            if (!includeMap.isEmpty()) {
+                clusterIncludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, includeMap);
+            }
+            Map<String, String> excludeMap = settings.getByPrefix(CLUSTER_ROUTING_EXCLUDE_GROUP).getAsMap();
+            if (!excludeMap.isEmpty()) {
+                clusterExcludeFilters = DiscoveryNodeFilters.buildFromKeyValue(OR, excludeMap);
+            }
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java
index 9149d04..3d68ed5 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java
@@ -24,16 +24,16 @@ import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 /**
  * This {@link AllocationDecider} limits the number of shards per node on a per
  * index or node-wide basis. The allocator prevents a single node to hold more
  * than {@value #INDEX_TOTAL_SHARDS_PER_NODE} per index and
- * <tt>cluster.routing.allocation.total_shards_per_node</tt> globally during the allocation
+ * {@value #CLUSTER_TOTAL_SHARDS_PER_NODE} globally during the allocation
  * process. The limits of this decider can be changed in real-time via a the
  * index settings API.
  * <p>
@@ -64,18 +64,26 @@ public class ShardsLimitAllocationDecider extends AllocationDecider {
      * Controls the maximum number of shards per node on a global level.
      * Negative values are interpreted as unlimited.
      */
-    public static final Setting<Integer> CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING = Setting.intSetting("cluster.routing.allocation.total_shards_per_node", -1, true, Setting.Scope.CLUSTER);
+    public static final String CLUSTER_TOTAL_SHARDS_PER_NODE = "cluster.routing.allocation.total_shards_per_node";
 
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            Integer newClusterLimit = settings.getAsInt(CLUSTER_TOTAL_SHARDS_PER_NODE, null);
 
-    @Inject
-    public ShardsLimitAllocationDecider(Settings settings, ClusterSettings clusterSettings) {
-        super(settings);
-        this.clusterShardLimit = CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING, this::setClusterShardLimit);
+            if (newClusterLimit != null) {
+                logger.info("updating [{}] from [{}] to [{}]", CLUSTER_TOTAL_SHARDS_PER_NODE,
+                        ShardsLimitAllocationDecider.this.clusterShardLimit, newClusterLimit);
+                ShardsLimitAllocationDecider.this.clusterShardLimit = newClusterLimit;
+            }
+        }
     }
 
-    private void setClusterShardLimit(int clusterShardLimit) {
-        this.clusterShardLimit = clusterShardLimit;
+    @Inject
+    public ShardsLimitAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
+        super(settings);
+        this.clusterShardLimit = settings.getAsInt(CLUSTER_TOTAL_SHARDS_PER_NODE, -1);
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java
index 597f0ad..37b9f9f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java
@@ -23,10 +23,9 @@ import org.elasticsearch.cluster.SnapshotsInProgress;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 /**
  * This {@link org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider} prevents shards that
@@ -39,7 +38,18 @@ public class SnapshotInProgressAllocationDecider extends AllocationDecider {
     /**
      * Disables relocation of shards that are currently being snapshotted.
      */
-    public static final Setting<Boolean> CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED_SETTING = Setting.boolSetting("cluster.routing.allocation.snapshot.relocation_enabled", false, true, Setting.Scope.CLUSTER);
+    public static final String CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED = "cluster.routing.allocation.snapshot.relocation_enabled";
+
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            boolean newEnableRelocation = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED, enableRelocation);
+            if (newEnableRelocation != enableRelocation) {
+                logger.info("updating [{}] from [{}], to [{}]", CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED, enableRelocation, newEnableRelocation);
+                enableRelocation = newEnableRelocation;
+            }
+        }
+    }
 
     private volatile boolean enableRelocation = false;
 
@@ -56,18 +66,14 @@ public class SnapshotInProgressAllocationDecider extends AllocationDecider {
      * @param settings {@link org.elasticsearch.common.settings.Settings} to use
      */
     public SnapshotInProgressAllocationDecider(Settings settings) {
-        this(settings, new ClusterSettings(settings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS));
+        this(settings, new NodeSettingsService(settings));
     }
 
     @Inject
-    public SnapshotInProgressAllocationDecider(Settings settings, ClusterSettings clusterSettings) {
+    public SnapshotInProgressAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        enableRelocation = CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED_SETTING, this::setEnableRelocation);
-    }
-
-    private void setEnableRelocation(boolean enableRelocation) {
-        this.enableRelocation = enableRelocation;
+        enableRelocation = settings.getAsBoolean(CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED, enableRelocation);
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java
index b97e613..ed6814d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java
@@ -19,14 +19,13 @@
 
 package org.elasticsearch.cluster.routing.allocation.decider;
 
-import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 /**
  * {@link ThrottlingAllocationDecider} controls the recovery process per node in
@@ -48,33 +47,27 @@ import org.elasticsearch.common.settings.Settings;
  */
 public class ThrottlingAllocationDecider extends AllocationDecider {
 
-    public static final int DEFAULT_CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES = 2;
-    public static final int DEFAULT_CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES = 4;
     public static final String NAME = "throttling";
+
+    public static final String CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES = "cluster.routing.allocation.node_initial_primaries_recoveries";
+    public static final String CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES = "cluster.routing.allocation.node_concurrent_recoveries";
     public static final String CLUSTER_ROUTING_ALLOCATION_CONCURRENT_RECOVERIES = "cluster.routing.allocation.concurrent_recoveries";
 
-    public static final Setting<Integer> CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING = Setting.intSetting("cluster.routing.allocation.node_initial_primaries_recoveries", DEFAULT_CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES, 0, true, Setting.Scope.CLUSTER);
-    public static final Setting<Integer> CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING = new Setting<>("cluster.routing.allocation.node_concurrent_recoveries", (s) -> s.get(CLUSTER_ROUTING_ALLOCATION_CONCURRENT_RECOVERIES,Integer.toString(DEFAULT_CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES)), (s) -> Setting.parseInt(s, 0, "cluster.routing.allocation.node_concurrent_recoveries"), true, Setting.Scope.CLUSTER);
+    public static final int DEFAULT_CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES = 2;
+    public static final int DEFAULT_CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES = 4;
 
     private volatile int primariesInitialRecoveries;
     private volatile int concurrentRecoveries;
 
     @Inject
-    public ThrottlingAllocationDecider(Settings settings, ClusterSettings clusterSettings) {
+    public ThrottlingAllocationDecider(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        this.primariesInitialRecoveries = CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING.get(settings);
-        this.concurrentRecoveries = CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.get(settings);
-        logger.debug("using node_concurrent_recoveries [{}], node_initial_primaries_recoveries [{}]", concurrentRecoveries, primariesInitialRecoveries);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING, this::setPrimariesInitialRecoveries);
-        clusterSettings.addSettingsUpdateConsumer(CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING, this::setConcurrentRecoveries);
-    }
 
-    private void setConcurrentRecoveries(int concurrentRecoveries) {
-        this.concurrentRecoveries = concurrentRecoveries;
-    }
+        this.primariesInitialRecoveries = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES, DEFAULT_CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES);
+        this.concurrentRecoveries = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CONCURRENT_RECOVERIES, settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, DEFAULT_CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES));
+        logger.debug("using node_concurrent_recoveries [{}], node_initial_primaries_recoveries [{}]", concurrentRecoveries, primariesInitialRecoveries);
 
-    private void setPrimariesInitialRecoveries(int primariesInitialRecoveries) {
-        this.primariesInitialRecoveries = primariesInitialRecoveries;
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
     @Override
@@ -122,4 +115,21 @@ public class ThrottlingAllocationDecider extends AllocationDecider {
             return allocation.decision(Decision.YES, NAME, "below shard recovery limit of [%d]", concurrentRecoveries);
         }
     }
+
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            int primariesInitialRecoveries = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES, ThrottlingAllocationDecider.this.primariesInitialRecoveries);
+            if (primariesInitialRecoveries != ThrottlingAllocationDecider.this.primariesInitialRecoveries) {
+                logger.info("updating [cluster.routing.allocation.node_initial_primaries_recoveries] from [{}] to [{}]", ThrottlingAllocationDecider.this.primariesInitialRecoveries, primariesInitialRecoveries);
+                ThrottlingAllocationDecider.this.primariesInitialRecoveries = primariesInitialRecoveries;
+            }
+
+            int concurrentRecoveries = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, ThrottlingAllocationDecider.this.concurrentRecoveries);
+            if (concurrentRecoveries != ThrottlingAllocationDecider.this.concurrentRecoveries) {
+                logger.info("updating [cluster.routing.allocation.node_concurrent_recoveries] from [{}] to [{}]", ThrottlingAllocationDecider.this.concurrentRecoveries, concurrentRecoveries);
+                ThrottlingAllocationDecider.this.concurrentRecoveries = concurrentRecoveries;
+            }
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
index 402df72..d4b1586 100644
--- a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
@@ -38,8 +38,6 @@ import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.text.StringText;
 import org.elasticsearch.common.transport.TransportAddress;
@@ -48,6 +46,7 @@ import org.elasticsearch.common.util.concurrent.*;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoveryService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
@@ -63,8 +62,8 @@ import static org.elasticsearch.common.util.concurrent.EsExecutors.daemonThreadF
  */
 public class InternalClusterService extends AbstractLifecycleComponent<ClusterService> implements ClusterService {
 
-    public static final Setting<TimeValue> CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING = Setting.positiveTimeSetting("cluster.service.slow_task_logging_threshold", TimeValue.timeValueSeconds(30), true, Setting.Scope.CLUSTER);
-    public static final Setting<TimeValue> CLUSTER_SERVICE_RECONNECT_INTERVAL_SETTING = Setting.positiveTimeSetting("cluster.service.reconnect_interval",  TimeValue.timeValueSeconds(10), false, Setting.Scope.CLUSTER);
+    public static final String SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD = "cluster.service.slow_task_logging_threshold";
+    public static final String SETTING_CLUSTER_SERVICE_RECONNECT_INTERVAL = "cluster.service.reconnect_interval";
 
     public static final String UPDATE_THREAD_NAME = "clusterService#updateTask";
     private final ThreadPool threadPool;
@@ -75,7 +74,7 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
 
     private final TransportService transportService;
 
-    private final ClusterSettings clusterSettings;
+    private final NodeSettingsService nodeSettingsService;
     private final DiscoveryNodeService discoveryNodeService;
     private final Version version;
 
@@ -108,32 +107,33 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
 
     @Inject
     public InternalClusterService(Settings settings, DiscoveryService discoveryService, OperationRouting operationRouting, TransportService transportService,
-                                  ClusterSettings clusterSettings, ThreadPool threadPool, ClusterName clusterName, DiscoveryNodeService discoveryNodeService, Version version) {
+                                  NodeSettingsService nodeSettingsService, ThreadPool threadPool, ClusterName clusterName, DiscoveryNodeService discoveryNodeService, Version version) {
         super(settings);
         this.operationRouting = operationRouting;
         this.transportService = transportService;
         this.discoveryService = discoveryService;
         this.threadPool = threadPool;
-        this.clusterSettings = clusterSettings;
+        this.nodeSettingsService = nodeSettingsService;
         this.discoveryNodeService = discoveryNodeService;
         this.version = version;
 
         // will be replaced on doStart.
         this.clusterState = ClusterState.builder(clusterName).build();
 
-        this.clusterSettings.addSettingsUpdateConsumer(CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING, this::setSlowTaskLoggingThreshold);
+        this.nodeSettingsService.setClusterService(this);
+        this.nodeSettingsService.addListener(new ApplySettings());
 
-        this.reconnectInterval = CLUSTER_SERVICE_RECONNECT_INTERVAL_SETTING.get(settings);
+        this.reconnectInterval = this.settings.getAsTime(SETTING_CLUSTER_SERVICE_RECONNECT_INTERVAL, TimeValue.timeValueSeconds(10));
 
-        this.slowTaskLoggingThreshold = CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING.get(settings);
+        this.slowTaskLoggingThreshold = this.settings.getAsTime(SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD, TimeValue.timeValueSeconds(30));
 
         localNodeMasterListeners = new LocalNodeMasterListeners(threadPool);
 
         initialBlocks = ClusterBlocks.builder().addGlobalBlock(discoveryService.getNoMasterBlock());
     }
 
-    private void setSlowTaskLoggingThreshold(TimeValue slowTaskLoggingThreshold) {
-        this.slowTaskLoggingThreshold = slowTaskLoggingThreshold;
+    public NodeSettingsService settingsService() {
+        return this.nodeSettingsService;
     }
 
     @Override
@@ -521,15 +521,6 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
             // update the current cluster state
             clusterState = newClusterState;
             logger.debug("set local cluster state to version {}", newClusterState.version());
-            try {
-                // nothing to do until we actually recover from the gateway or any other block indicates we need to disable persistency
-                if (clusterChangedEvent.state().blocks().disableStatePersistence() == false && clusterChangedEvent.metaDataChanged()) {
-                    final Settings incomingSettings = clusterChangedEvent.state().metaData().settings();
-                    clusterSettings.applySettings(incomingSettings);
-                }
-            } catch (Exception ex) {
-                logger.warn("failed to apply cluster settings", ex);
-            }
             for (ClusterStateListener listener : preAppliedListeners) {
                 try {
                     listener.clusterChanged(clusterChangedEvent);
@@ -855,4 +846,12 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
             }
         }
     }
+
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            final TimeValue slowTaskLoggingThreshold = settings.getAsTime(SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD, InternalClusterService.this.slowTaskLoggingThreshold);
+            InternalClusterService.this.slowTaskLoggingThreshold = slowTaskLoggingThreshold;
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettings.java b/core/src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettings.java
new file mode 100644
index 0000000..b537c44
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettings.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cluster.settings;
+
+import org.elasticsearch.common.inject.BindingAnnotation;
+
+import java.lang.annotation.Documented;
+import java.lang.annotation.Retention;
+import java.lang.annotation.Target;
+
+import static java.lang.annotation.ElementType.FIELD;
+import static java.lang.annotation.ElementType.PARAMETER;
+import static java.lang.annotation.RetentionPolicy.RUNTIME;
+
+
+@BindingAnnotation
+@Target({FIELD, PARAMETER})
+@Retention(RUNTIME)
+@Documented
+public @interface ClusterDynamicSettings {
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/Randomness.java b/core/src/main/java/org/elasticsearch/common/Randomness.java
new file mode 100644
index 0000000..dbfa803
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/Randomness.java
@@ -0,0 +1,120 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common;
+
+import org.elasticsearch.common.settings.Settings;
+
+import java.lang.reflect.Method;
+import java.util.Collections;
+import java.util.List;
+import java.util.Random;
+import java.util.concurrent.ThreadLocalRandom;
+
+/**
+ * Provides factory methods for producing reproducible sources of
+ * randomness. Reproducible sources of randomness contribute to
+ * reproducible tests. When running the Elasticsearch test suite, the
+ * test runner will establish a global random seed accessible via the
+ * system property "tests.seed". By seeding a random number generator
+ * with this global seed, we ensure that instances of Random produced
+ * with this class produce reproducible sources of randomness under
+ * when running under the Elasticsearch test suite. Alternatively,
+ * a reproducible source of randomness can be produced by providing a
+ * setting a reproducible seed. When running the Elasticsearch server
+ * process, non-reproducible sources of randomness are provided (unless
+ * a setting is provided for a module that exposes a seed setting (e.g.,
+ * DiscoveryService#SETTING_DISCOVERY_SEED)).
+ */
+public final class Randomness {
+    private static final Method currentMethod;
+    private static final Method getRandomMethod;
+
+    static {
+        Method maybeCurrentMethod;
+        Method maybeGetRandomMethod;
+        try {
+            Class<?> clazz = Class.forName("com.carrotsearch.randomizedtesting.RandomizedContext");
+            maybeCurrentMethod = clazz.getMethod("current");
+            maybeGetRandomMethod = clazz.getMethod("getRandom");
+        } catch (Throwable t) {
+            maybeCurrentMethod = null;
+            maybeGetRandomMethod = null;
+        }
+        currentMethod = maybeCurrentMethod;
+        getRandomMethod = maybeGetRandomMethod;
+    }
+
+    private Randomness() {}
+
+    /**
+     * Provides a reproducible source of randomness seeded by a long
+     * seed in the settings with the key setting.
+     *
+     * @param settings the settings containing the seed
+     * @param setting  the key to access the seed
+     * @return a reproducible source of randomness
+     */
+    public static Random get(Settings settings, String setting) {
+        Long maybeSeed = settings.getAsLong(setting, null);
+        if (maybeSeed != null) {
+            return new Random(maybeSeed);
+        } else {
+            return get();
+        }
+    }
+
+    /**
+     * Provides a source of randomness that is reproducible when
+     * running under the Elasticsearch test suite, and otherwise
+     * produces a non-reproducible source of randomness. Reproducible
+     * sources of randomness are created when the system property
+     * "tests.seed" is set and the security policy allows reading this
+     * system property. Otherwise, non-reproducible sources of
+     * randomness are created.
+     *
+     * @return a source of randomness
+     * @throws IllegalStateException if running tests but was not able
+     *                               to acquire an instance of Random from
+     *                               RandomizedContext or tests are
+     *                               running but tests.seed is not set
+     */
+    public static Random get() {
+        if (currentMethod != null && getRandomMethod != null) {
+            try {
+                Object randomizedContext = currentMethod.invoke(null);
+                return (Random) getRandomMethod.invoke(randomizedContext);
+            } catch (ReflectiveOperationException e) {
+                // unexpected, bail
+                throw new IllegalStateException("running tests but failed to invoke RandomizedContext#getRandom", e);
+            }
+        } else {
+            return getWithoutSeed();
+        }
+    }
+
+    private static Random getWithoutSeed() {
+        assert currentMethod == null && getRandomMethod == null : "running under tests but tried to create non-reproducible random";
+        return ThreadLocalRandom.current();
+    }
+
+    public static void shuffle(List<?> list) {
+        Collections.shuffle(list, get());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Key.java b/core/src/main/java/org/elasticsearch/common/inject/Key.java
index 3af3b4e..7344dfe 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Key.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Key.java
@@ -333,7 +333,7 @@ public class Key<T> {
      * Returns {@code true} if the given annotation type has no attributes.
      */
     static boolean isMarker(Class<? extends Annotation> annotationType) {
-        return annotationType.getDeclaredMethods().length == 0;
+        return annotationType.getMethods().length == 0;
     }
 
     /**
@@ -345,7 +345,7 @@ public class Key<T> {
         ensureRetainedAtRuntime(annotationType);
         ensureIsBindingAnnotation(annotationType);
 
-        if (annotationType.getDeclaredMethods().length == 0) {
+        if (annotationType.getMethods().length == 0) {
             return new AnnotationTypeStrategy(annotationType, annotation);
         }
 
diff --git a/core/src/main/java/org/elasticsearch/common/inject/Reflection.java b/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
index 22c542b..667466f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/Reflection.java
@@ -41,7 +41,7 @@ class Reflection {
     @SuppressWarnings("unchecked")
     static <T> Constructor<T> invalidConstructor() {
         try {
-            return (Constructor<T>) InvalidConstructor.class.getDeclaredConstructor();
+            return (Constructor<T>) InvalidConstructor.class.getConstructor();
         } catch (NoSuchMethodException e) {
             throw new AssertionError(e);
         }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
index 3837de8..0def65b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
@@ -212,7 +212,7 @@ public class FactoryProvider<F> implements Provider<F>, HasDependencies {
             TypeLiteral<?> factoryType, TypeLiteral<?> implementationType) {
         List<AssistedConstructor<?>> constructors = new ArrayList<>();
 
-        for (Constructor<?> constructor : implementationType.getRawType().getDeclaredConstructors()) {
+        for (Constructor<?> constructor : implementationType.getRawType().getConstructors()) {
             if (constructor.getAnnotation(AssistedInject.class) != null) {
                 @SuppressWarnings("unchecked") // the constructor type and implementation type agree
                         AssistedConstructor assistedConstructor = new AssistedConstructor(
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
index 1671b4a..5e45b49 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java
@@ -83,7 +83,7 @@ public final class ProviderMethodsModule implements Module {
     public List<ProviderMethod<?>> getProviderMethods(Binder binder) {
         List<ProviderMethod<?>> result = new ArrayList<>();
         for (Class<?> c = delegate.getClass(); c != Object.class; c = c.getSuperclass()) {
-            for (Method method : c.getDeclaredMethods()) {
+            for (Method method : c.getMethods()) {
                 if (method.getAnnotation(Provides.class) != null) {
                     result.add(createProviderMethod(binder, method));
                 }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
index 17497d5..286635b 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java
@@ -188,7 +188,7 @@ public final class InjectionPoint {
         Errors errors = new Errors(rawType);
 
         Constructor<?> injectableConstructor = null;
-        for (Constructor<?> constructor : rawType.getDeclaredConstructors()) {
+        for (Constructor<?> constructor : rawType.getConstructors()) {
             Inject inject = constructor.getAnnotation(Inject.class);
             if (inject != null) {
                 if (inject.optional()) {
@@ -212,7 +212,7 @@ public final class InjectionPoint {
 
         // If no annotated constructor is found, look for a no-arg constructor instead.
         try {
-            Constructor<?> noArgConstructor = rawType.getDeclaredConstructor();
+            Constructor<?> noArgConstructor = rawType.getConstructor();
 
             // Disallow private constructors on non-private classes (unless they have @Inject)
             if (Modifier.isPrivate(noArgConstructor.getModifiers())
@@ -334,7 +334,7 @@ public final class InjectionPoint {
         // name. In Scala, fields always get accessor methods (that we need to ignore). See bug 242.
         if (member instanceof Method) {
             try {
-                if (member.getDeclaringClass().getDeclaredField(member.getName()) != null) {
+                if (member.getDeclaringClass().getField(member.getName()) != null) {
                     return;
                 }
             } catch (NoSuchFieldException ignore) {
@@ -390,7 +390,7 @@ public final class InjectionPoint {
         Factory<Field> FIELDS = new Factory<Field>() {
             @Override
             public Field[] getMembers(Class<?> type) {
-                return type.getDeclaredFields();
+                return type.getFields();
             }
 
             @Override
@@ -402,7 +402,7 @@ public final class InjectionPoint {
         Factory<Method> METHODS = new Factory<Method>() {
             @Override
             public Method[] getMembers(Class<?> type) {
-                return type.getDeclaredMethods();
+                return type.getMethods();
             }
 
             @Override
diff --git a/core/src/main/java/org/elasticsearch/common/io/Streams.java b/core/src/main/java/org/elasticsearch/common/io/Streams.java
index 8adf091..36b1d94 100644
--- a/core/src/main/java/org/elasticsearch/common/io/Streams.java
+++ b/core/src/main/java/org/elasticsearch/common/io/Streams.java
@@ -20,6 +20,8 @@
 package org.elasticsearch.common.io;
 
 import java.nio.charset.StandardCharsets;
+
+import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.common.util.Callback;
 
 import java.io.BufferedReader;
@@ -68,6 +70,7 @@ public abstract class Streams {
     public static long copy(InputStream in, OutputStream out, byte[] buffer) throws IOException {
         Objects.requireNonNull(in, "No InputStream specified");
         Objects.requireNonNull(out, "No OutputStream specified");
+        boolean success = false;
         try {
             long byteCount = 0;
             int bytesRead;
@@ -76,17 +79,13 @@ public abstract class Streams {
                 byteCount += bytesRead;
             }
             out.flush();
+            success = true;
             return byteCount;
         } finally {
-            try {
-                in.close();
-            } catch (IOException ex) {
-                // do nothing
-            }
-            try {
-                out.close();
-            } catch (IOException ex) {
-                // do nothing
+            if (success) {
+                IOUtils.close(in, out);
+            } else {
+                IOUtils.closeWhileHandlingException(in, out);
             }
         }
     }
@@ -130,6 +129,7 @@ public abstract class Streams {
     public static int copy(Reader in, Writer out) throws IOException {
         Objects.requireNonNull(in, "No Reader specified");
         Objects.requireNonNull(out, "No Writer specified");
+        boolean success = false;
         try {
             int byteCount = 0;
             char[] buffer = new char[BUFFER_SIZE];
@@ -139,17 +139,13 @@ public abstract class Streams {
                 byteCount += bytesRead;
             }
             out.flush();
+            success = true;
             return byteCount;
         } finally {
-            try {
-                in.close();
-            } catch (IOException ex) {
-                // do nothing
-            }
-            try {
-                out.close();
-            } catch (IOException ex) {
-                // do nothing
+            if (success) {
+                IOUtils.close(in, out);
+            } else {
+                IOUtils.closeWhileHandlingException(in, out);
             }
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
index 05eaac1..835a35d 100644
--- a/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
+++ b/core/src/main/java/org/elasticsearch/common/network/NetworkService.java
@@ -137,8 +137,7 @@ public class NetworkService extends AbstractComponent {
      * Resolves {@code publishHosts} to a single publish address. The fact that it returns
      * only one address is just a current limitation.
      * <p>
-     * If {@code publishHosts} resolves to more than one address, <b>then one is selected with magic</b>,
-     * and the user is warned (they can always just be more specific).
+     * If {@code publishHosts} resolves to more than one address, <b>then one is selected with magic</b>
      * @param publishHosts list of hosts to publish as. this may contain special pseudo-hostnames
      *                     such as _local_ (see the documentation). if it is null, it will be populated
      *                     based on global default settings.
@@ -186,13 +185,12 @@ public class NetworkService extends AbstractComponent {
             }
         }
         
-        // 3. warn user if we end out with multiple publish addresses
+        // 3. if we end out with multiple publish addresses, select by preference.
+        // don't warn the user, or they will get confused by bind_host vs publish_host etc.
         if (addresses.length > 1) {
             List<InetAddress> sorted = new ArrayList<>(Arrays.asList(addresses));
             NetworkUtils.sortAddresses(sorted);
             addresses = new InetAddress[] { sorted.get(0) };
-            logger.warn("publish host: {} resolves to multiple addresses, auto-selecting {{}} as single publish address", 
-                    Arrays.toString(publishHosts), NetworkAddress.format(addresses[0]));
         }
         return addresses[0];
     }
diff --git a/core/src/main/java/org/elasticsearch/common/regex/Regex.java b/core/src/main/java/org/elasticsearch/common/regex/Regex.java
index f5c3094..061ad6c 100644
--- a/core/src/main/java/org/elasticsearch/common/regex/Regex.java
+++ b/core/src/main/java/org/elasticsearch/common/regex/Regex.java
@@ -150,7 +150,7 @@ public class Regex {
                 pFlags |= Pattern.LITERAL;
             } else if ("COMMENTS".equals(s)) {
                 pFlags |= Pattern.COMMENTS;
-            } else if ("UNICODE_CHAR_CLASS".equals(s)) {
+            } else if (("UNICODE_CHAR_CLASS".equals(s)) || ("UNICODE_CHARACTER_CLASS".equals(s))) {
                 pFlags |= UNICODE_CHARACTER_CLASS;
             } else {
                 throw new IllegalArgumentException("Unknown regex flag [" + s + "]");
diff --git a/core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java b/core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java
deleted file mode 100644
index c7361e7..0000000
--- a/core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java
+++ /dev/null
@@ -1,253 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.settings;
-
-import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-
-import java.util.*;
-import java.util.function.BiConsumer;
-import java.util.function.Consumer;
-
-/**
- * A basic setting service that can be used for per-index and per-cluster settings.
- * This service offers transactional application of updates settings.
- */
-public abstract class AbstractScopedSettings extends AbstractComponent {
-    private Settings lastSettingsApplied = Settings.EMPTY;
-    private final List<SettingUpdater> settingUpdaters = new ArrayList<>();
-    private final Map<String, Setting<?>> groupSettings = new HashMap<>();
-    private final Map<String, Setting<?>> keySettings = new HashMap<>();
-    private final Setting.Scope scope;
-
-    protected AbstractScopedSettings(Settings settings, Set<Setting<?>> settingsSet, Setting.Scope scope) {
-        super(settings);
-        for (Setting<?> entry : settingsSet) {
-            if (entry.getScope() != scope) {
-                throw new IllegalArgumentException("Setting must be a cluster setting but was: " + entry.getScope());
-            }
-            if (entry.isGroupSetting()) {
-                groupSettings.put(entry.getKey(), entry);
-            } else {
-                keySettings.put(entry.getKey(), entry);
-            }
-        }
-        this.scope = scope;
-    }
-
-    public Setting.Scope getScope() {
-        return this.scope;
-    }
-
-    /**
-     * Applies the given settings to all listeners and rolls back the result after application. This
-     * method will not change any settings but will fail if any of the settings can't be applied.
-     */
-    public synchronized Settings dryRun(Settings settings) {
-        final Settings current = Settings.builder().put(this.settings).put(settings).build();
-        final Settings previous = Settings.builder().put(this.settings).put(this.lastSettingsApplied).build();
-        List<RuntimeException> exceptions = new ArrayList<>();
-        for (SettingUpdater settingUpdater : settingUpdaters) {
-            try {
-                if (settingUpdater.hasChanged(current, previous)) {
-                    settingUpdater.getValue(current, previous);
-                }
-            } catch (RuntimeException ex) {
-                exceptions.add(ex);
-                logger.debug("failed to prepareCommit settings for [{}]", ex, settingUpdater);
-            }
-        }
-        // here we are exhaustive and record all settings that failed.
-        ExceptionsHelper.rethrowAndSuppress(exceptions);
-        return current;
-    }
-
-    /**
-     * Applies the given settings to all the settings consumers or to none of them. The settings
-     * will be merged with the node settings before they are applied while given settings override existing node
-     * settings.
-     * @param newSettings the settings to apply
-     * @return the unmerged applied settings
-    */
-    public synchronized Settings applySettings(Settings newSettings) {
-        if (lastSettingsApplied != null && newSettings.equals(lastSettingsApplied)) {
-            // nothing changed in the settings, ignore
-            return newSettings;
-        }
-        final Settings current = Settings.builder().put(this.settings).put(newSettings).build();
-        final Settings previous = Settings.builder().put(this.settings).put(this.lastSettingsApplied).build();
-        try {
-            List<Runnable> applyRunnables = new ArrayList<>();
-            for (SettingUpdater settingUpdater : settingUpdaters) {
-                try {
-                    applyRunnables.add(settingUpdater.updater(current, previous));
-                } catch (Exception ex) {
-                    logger.warn("failed to prepareCommit settings for [{}]", ex, settingUpdater);
-                    throw ex;
-                }
-            }
-            for (Runnable settingUpdater : applyRunnables) {
-                settingUpdater.run();
-            }
-        } catch (Exception ex) {
-            logger.warn("failed to apply settings", ex);
-            throw ex;
-        } finally {
-        }
-        return lastSettingsApplied = newSettings;
-    }
-
-    /**
-     * Adds a settings consumer with a predicate that is only evaluated at update time.
-     * <p>
-     * Note: Only settings registered in {@link SettingsModule} can be changed dynamically.
-     * </p>
-     * @param validator an additional validator that is only applied to updates of this setting.
-     *                  This is useful to add additional validation to settings at runtime compared to at startup time.
-     */
-    public synchronized <T> void addSettingsUpdateConsumer(Setting<T> setting, Consumer<T> consumer, Consumer<T> validator) {
-        if (setting != get(setting.getKey())) {
-            throw new IllegalArgumentException("Setting is not registered for key [" + setting.getKey() + "]");
-        }
-        this.settingUpdaters.add(setting.newUpdater(consumer, logger, validator));
-    }
-
-    /**
-     * Adds a settings consumer that accepts the values for two settings. The consumer if only notified if one or both settings change.
-     * <p>
-     * Note: Only settings registered in {@link SettingsModule} can be changed dynamically.
-     * </p>
-     * This method registers a compound updater that is useful if two settings are depending on each other. The consumer is always provided
-     * with both values even if only one of the two changes.
-     */
-    public synchronized <A, B> void addSettingsUpdateConsumer(Setting<A> a, Setting<B> b, BiConsumer<A, B> consumer) {
-        if (a != get(a.getKey())) {
-            throw new IllegalArgumentException("Setting is not registered for key [" + a.getKey() + "]");
-        }
-        if (b != get(b.getKey())) {
-            throw new IllegalArgumentException("Setting is not registered for key [" + b.getKey() + "]");
-        }
-        this.settingUpdaters.add(Setting.compoundUpdater(consumer, a, b, logger));
-    }
-
-    /**
-     * Adds a settings consumer.
-     * <p>
-     * Note: Only settings registered in {@link org.elasticsearch.cluster.ClusterModule} can be changed dynamically.
-     * </p>
-     */
-    public synchronized <T> void addSettingsUpdateConsumer(Setting<T> setting, Consumer<T> consumer) {
-       addSettingsUpdateConsumer(setting, consumer, (s) -> {});
-    }
-
-    /**
-     * Transactional interface to update settings.
-     * @see Setting
-     */
-    public interface SettingUpdater<T> {
-
-        /**
-         * Returns true if this updaters setting has changed with the current update
-         * @param current the current settings
-         * @param previous the previous setting
-         * @return true if this updaters setting has changed with the current update
-         */
-        boolean hasChanged(Settings current, Settings previous);
-
-        /**
-         * Returns the instance value for the current settings. This method is stateless and idempotent.
-         * This method will throw an exception if the source of this value is invalid.
-         */
-        T getValue(Settings current, Settings previous);
-
-        /**
-         * Applies the given value to the updater. This methods will actually run the update.
-         */
-        void apply(T value, Settings current, Settings previous);
-
-        /**
-         * Updates this updaters value if it has changed.
-         * @return <code>true</code> iff the value has been updated.
-         */
-        default boolean apply(Settings current, Settings previous) {
-            if (hasChanged(current, previous)) {
-                T value = getValue(current, previous);
-                apply(value, current, previous);
-                return true;
-            }
-            return false;
-        }
-
-        /**
-         * Returns a callable runnable that calls {@link #apply(Object, Settings, Settings)} if the settings
-         * actually changed. This allows to defer the update to a later point in time while keeping type safety.
-         * If the value didn't change the returned runnable is a noop.
-         */
-        default Runnable updater(Settings current, Settings previous) {
-            if (hasChanged(current, previous)) {
-                T value = getValue(current, previous);
-                return () -> { apply(value, current, previous);};
-            }
-            return () -> {};
-        }
-    }
-
-    /**
-     * Returns the {@link Setting} for the given key or <code>null</code> if the setting can not be found.
-     */
-    public Setting get(String key) {
-        Setting<?> setting = keySettings.get(key);
-        if (setting == null) {
-            for (Map.Entry<String, Setting<?>> entry : groupSettings.entrySet()) {
-                if (entry.getValue().match(key)) {
-                    return entry.getValue();
-                }
-            }
-        } else {
-            return setting;
-        }
-        return null;
-    }
-
-    /**
-     * Returns <code>true</code> if the setting for the given key is dynamically updateable. Otherwise <code>false</code>.
-     */
-    public boolean hasDynamicSetting(String key) {
-        final Setting setting = get(key);
-        return setting != null && setting.isDynamic();
-    }
-
-    /**
-     * Returns a settings object that contains all settings that are not
-     * already set in the given source. The diff contains either the default value for each
-     * setting or the settings value in the given default settings.
-     */
-    public Settings diff(Settings source, Settings defaultSettings) {
-        Settings.Builder builder = Settings.builder();
-        for (Setting<?> setting : keySettings.values()) {
-            if (setting.exists(source) == false) {
-                builder.put(setting.getKey(), setting.getRaw(defaultSettings));
-            }
-        }
-        return builder.build();
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java b/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
deleted file mode 100644
index 89cdd04..0000000
--- a/core/src/main/java/org/elasticsearch/common/settings/ClusterSettings.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.settings;
-
-import org.elasticsearch.action.admin.indices.close.TransportCloseIndexAction;
-import org.elasticsearch.action.support.DestructiveOperations;
-import org.elasticsearch.cluster.InternalClusterInfoService;
-import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
-import org.elasticsearch.cluster.routing.allocation.decider.*;
-import org.elasticsearch.cluster.service.InternalClusterService;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
-import org.elasticsearch.discovery.zen.elect.ElectMasterService;
-import org.elasticsearch.index.store.IndexStoreConfig;
-import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
-import org.elasticsearch.indices.recovery.RecoverySettings;
-import org.elasticsearch.indices.ttl.IndicesTTLService;
-import org.elasticsearch.search.SearchService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-import java.util.*;
-
-/**
- * Encapsulates all valid cluster level settings.
- */
-public final class ClusterSettings extends AbstractScopedSettings {
-
-    public ClusterSettings(Settings settings, Set<Setting<?>> settingsSet) {
-        super(settings, settingsSet, Setting.Scope.CLUSTER);
-    }
-
-
-    @Override
-    public synchronized Settings applySettings(Settings newSettings) {
-        Settings settings = super.applySettings(newSettings);
-        try {
-            for (Map.Entry<String, String> entry : settings.getAsMap().entrySet()) {
-                if (entry.getKey().startsWith("logger.")) {
-                    String component = entry.getKey().substring("logger.".length());
-                    if ("_root".equals(component)) {
-                        ESLoggerFactory.getRootLogger().setLevel(entry.getValue());
-                    } else {
-                        ESLoggerFactory.getLogger(component).setLevel(entry.getValue());
-                    }
-                }
-            }
-        } catch (Exception e) {
-            logger.warn("failed to refresh settings for [{}]", e, "logger");
-        }
-        return settings;
-    }
-
-    /**
-     * Returns <code>true</code> if the settings is a logger setting.
-     */
-    public boolean isLoggerSetting(String key) {
-        return key.startsWith("logger.");
-    }
-
-
-    public static Set<Setting<?>> BUILT_IN_CLUSTER_SETTINGS = Collections.unmodifiableSet(new HashSet<>(Arrays.asList(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING,
-        AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING,
-        BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING,
-        BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING,
-        BalancedShardsAllocator.THRESHOLD_SETTING,
-        ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING,
-        ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING,
-        EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING,
-        EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING,
-        ZenDiscovery.REJOIN_ON_MASTER_GONE_SETTING,
-        FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP_SETTING,
-        FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING,
-        FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP_SETTING,
-        IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING,
-        IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING,
-        IndicesTTLService.INDICES_TTL_INTERVAL_SETTING,
-        MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING,
-        MetaData.SETTING_READ_ONLY_SETTING,
-        RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING,
-        RecoverySettings.INDICES_RECOVERY_TRANSLOG_OPS_SETTING,
-        RecoverySettings.INDICES_RECOVERY_TRANSLOG_SIZE_SETTING,
-        RecoverySettings.INDICES_RECOVERY_COMPRESS_SETTING,
-        RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS_SETTING,
-        RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING,
-        RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING,
-        RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING,
-        RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING,
-        RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING,
-        RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING,
-        RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING,
-        ThreadPool.THREADPOOL_GROUP_SETTING,
-        ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING,
-        ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING,
-        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING,
-        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING,
-        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING,
-        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING,
-        DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING,
-        InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING,
-        InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING,
-        SnapshotInProgressAllocationDecider.CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED_SETTING,
-        DestructiveOperations.REQUIRES_NAME_SETTING,
-        DiscoverySettings.PUBLISH_TIMEOUT_SETTING,
-        DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING,
-        DiscoverySettings.COMMIT_TIMEOUT_SETTING,
-        DiscoverySettings.NO_MASTER_BLOCK_SETTING,
-        HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING,
-        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING,
-        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING,
-        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING,
-        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING,
-        InternalClusterService.CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING,
-        SearchService.DEFAULT_SEARCH_TIMEOUT_SETTING,
-        ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING,
-        TransportService.TRACE_LOG_EXCLUDE_SETTING,
-        TransportService.TRACE_LOG_INCLUDE_SETTING,
-        TransportCloseIndexAction.CLUSTER_INDICES_CLOSE_ENABLE_SETTING,
-        ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING,
-        InternalClusterService.CLUSTER_SERVICE_RECONNECT_INTERVAL_SETTING,
-        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING,
-        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING)));
-}
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Setting.java b/core/src/main/java/org/elasticsearch/common/settings/Setting.java
deleted file mode 100644
index b15634b..0000000
--- a/core/src/main/java/org/elasticsearch/common/settings/Setting.java
+++ /dev/null
@@ -1,391 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.settings;
-
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.Booleans;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.regex.Regex;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.MemorySizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.*;
-
-import java.io.IOException;
-import java.util.function.BiConsumer;
-import java.util.function.Consumer;
-import java.util.function.Function;
-
-/**
- */
-public class Setting<T> extends ToXContentToBytes {
-    private final String key;
-    private final Function<Settings, String> defaultValue;
-    private final Function<String, T> parser;
-    private final boolean dynamic;
-    private final Scope scope;
-
-    /**
-     * Creates a new Setting instance
-     * @param key the settings key for this setting.
-     * @param defaultValue a default value function that returns the default values string representation.
-     * @param parser a parser that parses the string rep into a complex datatype.
-     * @param dynamic true iff this setting can be dynamically updateable
-     * @param scope the scope of this setting
-     */
-    public Setting(String key, Function<Settings, String> defaultValue, Function<String, T> parser, boolean dynamic, Scope scope) {
-        assert parser.apply(defaultValue.apply(Settings.EMPTY)) != null || this.isGroupSetting(): "parser returned null";
-        this.key = key;
-        this.defaultValue = defaultValue;
-        this.parser = parser;
-        this.dynamic = dynamic;
-        this.scope = scope;
-    }
-
-    /**
-     * Returns the settings key or a prefix if this setting is a group setting
-     * @see #isGroupSetting()
-     */
-    public final String getKey() {
-        return key;
-    }
-
-    /**
-     * Returns <code>true</code> iff this setting is dynamically updateable, otherwise <code>false</code>
-     */
-    public final boolean isDynamic() {
-        return dynamic;
-    }
-
-    /**
-     * Returns the settings scope
-     */
-    public final Scope getScope() {
-        return scope;
-    }
-
-    /**
-     * Returns <code>true</code> iff this setting is a group setting. Group settings represent a set of settings
-     * rather than a single value. The key, see {@link #getKey()}, in contrast to non-group settings is a prefix like <tt>cluster.store.</tt>
-     * that matches all settings with this prefix.
-     */
-    public boolean isGroupSetting() {
-        return false;
-    }
-
-    /**
-     * Returns the default values string representation for this setting.
-     * @param settings a settings object for settings that has a default value depending on another setting if available
-     */
-    public final String getDefault(Settings settings) {
-        return defaultValue.apply(settings);
-    }
-
-    /**
-     * Returns <code>true</code> iff this setting is present in the given settings object. Otherwise <code>false</code>
-     */
-    public final boolean exists(Settings settings) {
-        return settings.get(key) != null;
-    }
-
-    /**
-     * Returns the settings value. If the setting is not present in the given settings object the default value is returned
-     * instead.
-     */
-    public T get(Settings settings) {
-        String value = getRaw(settings);
-        try {
-            return parser.apply(value);
-        } catch (ElasticsearchParseException ex) {
-            throw new IllegalArgumentException(ex.getMessage(), ex);
-        } catch (Exception t) {
-            throw new IllegalArgumentException("Failed to parse value [" + value + "] for setting [" + getKey() + "]", t);
-        }
-    }
-
-    /**
-     * Returns the raw (string) settings value. If the setting is not present in the given settings object the default value is returned
-     * instead. This is useful if the value can't be parsed due to an invalid value to access the actual value.
-     */
-    public final String getRaw(Settings settings) {
-        return settings.get(key, defaultValue.apply(settings));
-    }
-
-    /**
-     * Returns <code>true</code> iff the given key matches the settings key or if this setting is a group setting if the
-     * given key is part of the settings group.
-     * @see #isGroupSetting()
-     */
-    public boolean match(String toTest) {
-        return Regex.simpleMatch(key, toTest);
-    }
-
-    @Override
-    public final XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.field("key", key);
-        builder.field("type", scope.name());
-        builder.field("dynamic", dynamic);
-        builder.field("is_group_setting", isGroupSetting());
-        builder.field("default", defaultValue.apply(Settings.EMPTY));
-        builder.endObject();
-        return builder;
-    }
-
-    /**
-     * The settings scope - settings can either be cluster settings or per index settings.
-     */
-    public enum Scope {
-        CLUSTER,
-        INDEX;
-    }
-
-    final AbstractScopedSettings.SettingUpdater newUpdater(Consumer<T> consumer, ESLogger logger) {
-        return newUpdater(consumer, logger, (s) -> {});
-    }
-
-    AbstractScopedSettings.SettingUpdater newUpdater(Consumer<T> consumer, ESLogger logger, Consumer<T> validator) {
-        if (isDynamic()) {
-            return new Updater(consumer, logger, validator);
-        } else {
-            throw new IllegalStateException("setting [" + getKey() + "] is not dynamic");
-        }
-    }
-
-    /**
-     * this is used for settings that depend on each other... see {@link org.elasticsearch.common.settings.AbstractScopedSettings#addSettingsUpdateConsumer(Setting, Setting, BiConsumer)} and it's
-     * usage for details.
-     */
-    static <A, B> AbstractScopedSettings.SettingUpdater<Tuple<A, B>> compoundUpdater(final BiConsumer<A,B> consumer, final Setting<A> aSettting, final Setting<B> bSetting, ESLogger logger) {
-        final AbstractScopedSettings.SettingUpdater<A> aSettingUpdater = aSettting.newUpdater(null, logger);
-        final AbstractScopedSettings.SettingUpdater<B> bSettingUpdater = bSetting.newUpdater(null, logger);
-        return new AbstractScopedSettings.SettingUpdater<Tuple<A, B>>() {
-            @Override
-            public boolean hasChanged(Settings current, Settings previous) {
-                return aSettingUpdater.hasChanged(current, previous) || bSettingUpdater.hasChanged(current, previous);
-            }
-
-            @Override
-            public Tuple<A, B> getValue(Settings current, Settings previous) {
-                return new Tuple<>(aSettingUpdater.getValue(current, previous), bSettingUpdater.getValue(current, previous));
-            }
-
-            @Override
-            public void apply(Tuple<A, B> value, Settings current, Settings previous) {
-                consumer.accept(value.v1(), value.v2());
-            }
-
-            @Override
-            public String toString() {
-                return "CompoundUpdater for: " + aSettingUpdater + " and " + bSettingUpdater;
-            }
-        };
-    }
-
-
-    private class Updater implements AbstractScopedSettings.SettingUpdater<T> {
-        private final Consumer<T> consumer;
-        private final ESLogger logger;
-        private final Consumer<T> accept;
-
-        public Updater(Consumer<T> consumer, ESLogger logger, Consumer<T> accept) {
-            this.consumer = consumer;
-            this.logger = logger;
-            this.accept = accept;
-        }
-
-        @Override
-        public String toString() {
-            return "Updater for: " + Setting.this.toString();
-        }
-
-        @Override
-        public boolean hasChanged(Settings current, Settings previous) {
-            final String newValue = getRaw(current);
-            final String value = getRaw(previous);
-            assert isGroupSetting() == false : "group settings must override this method";
-            assert value != null : "value was null but can't be unless default is null which is invalid";
-
-            return value.equals(newValue) == false;
-        }
-
-        @Override
-        public T getValue(Settings current, Settings previous) {
-            final String newValue = getRaw(current);
-            final String value = getRaw(previous);
-            T inst = get(current);
-            try {
-                accept.accept(inst);
-            } catch (Exception | AssertionError e) {
-                throw new IllegalArgumentException("illegal value can't update [" + key + "] from [" + value + "] to [" + newValue + "]", e);
-            }
-            return inst;
-        }
-
-        @Override
-        public void apply(T value, Settings current, Settings previous) {
-            logger.info("update [{}] from [{}] to [{}]", key, getRaw(previous), getRaw(current));
-            consumer.accept(value);
-        }
-    }
-
-
-    public Setting(String key, String defaultValue, Function<String, T> parser, boolean dynamic, Scope scope) {
-        this(key, (s) -> defaultValue, parser, dynamic, scope);
-    }
-
-    public static Setting<Float> floatSetting(String key, float defaultValue, boolean dynamic, Scope scope) {
-        return new Setting<>(key, (s) -> Float.toString(defaultValue), Float::parseFloat, dynamic, scope);
-    }
-
-    public static Setting<Float> floatSetting(String key, float defaultValue, float minValue, boolean dynamic, Scope scope) {
-        return new Setting<>(key, (s) -> Float.toString(defaultValue), (s) -> {
-            float value = Float.parseFloat(s);
-            if (value < minValue) {
-                throw new IllegalArgumentException("Failed to parse value [" + s + "] for setting [" + key + "] must be >= " + minValue);
-            }
-            return value;
-        }, dynamic, scope);
-    }
-
-    public static Setting<Integer> intSetting(String key, int defaultValue, int minValue, boolean dynamic, Scope scope) {
-        return new Setting<>(key, (s) -> Integer.toString(defaultValue), (s) -> parseInt(s, minValue, key), dynamic, scope);
-    }
-
-    public static int parseInt(String s, int minValue, String key) {
-        int value = Integer.parseInt(s);
-        if (value < minValue) {
-            throw new IllegalArgumentException("Failed to parse value [" + s + "] for setting [" + key + "] must be >= " + minValue);
-        }
-        return value;
-    }
-
-    public static Setting<Integer> intSetting(String key, int defaultValue, boolean dynamic, Scope scope) {
-        return intSetting(key, defaultValue, Integer.MIN_VALUE, dynamic, scope);
-    }
-
-    public static Setting<Boolean> boolSetting(String key, boolean defaultValue, boolean dynamic, Scope scope) {
-        return new Setting<>(key, (s) -> Boolean.toString(defaultValue), Booleans::parseBooleanExact, dynamic, scope);
-    }
-
-    public static Setting<ByteSizeValue> byteSizeSetting(String key, String percentage, boolean dynamic, Scope scope) {
-        return new Setting<>(key, (s) -> percentage, (s) -> MemorySizeValue.parseBytesSizeValueOrHeapRatio(s, key), dynamic, scope);
-    }
-
-    public static Setting<ByteSizeValue> byteSizeSetting(String key, ByteSizeValue value, boolean dynamic, Scope scope) {
-        return new Setting<>(key, (s) -> value.toString(), (s) -> ByteSizeValue.parseBytesSizeValue(s, key), dynamic, scope);
-    }
-
-    public static Setting<TimeValue> positiveTimeSetting(String key, TimeValue defaultValue, boolean dynamic, Scope scope) {
-        return timeSetting(key, defaultValue, TimeValue.timeValueMillis(0), dynamic, scope);
-    }
-
-    public static Setting<Settings> groupSetting(String key, boolean dynamic, Scope scope) {
-        if (key.endsWith(".") == false) {
-            throw new IllegalArgumentException("key must end with a '.'");
-        }
-        return new Setting<Settings>(key, "", (s) -> null, dynamic, scope) {
-
-            @Override
-            public boolean isGroupSetting() {
-                return true;
-            }
-
-            @Override
-            public Settings get(Settings settings) {
-                return settings.getByPrefix(key);
-            }
-
-            @Override
-            public boolean match(String toTest) {
-                return Regex.simpleMatch(key + "*", toTest);
-            }
-
-            @Override
-            public AbstractScopedSettings.SettingUpdater<Settings> newUpdater(Consumer<Settings> consumer, ESLogger logger, Consumer<Settings> validator) {
-                if (isDynamic() == false) {
-                    throw new IllegalStateException("setting [" + getKey() + "] is not dynamic");
-                }
-                final Setting<?> setting = this;
-                return new AbstractScopedSettings.SettingUpdater<Settings>() {
-
-                    @Override
-                    public boolean hasChanged(Settings current, Settings previous) {
-                        Settings currentSettings = get(current);
-                        Settings previousSettings = get(previous);
-                        return currentSettings.equals(previousSettings) == false;
-                    }
-
-                    @Override
-                    public Settings getValue(Settings current, Settings previous) {
-                        Settings currentSettings = get(current);
-                        Settings previousSettings = get(previous);
-                        try {
-                            validator.accept(currentSettings);
-                        } catch (Exception | AssertionError e) {
-                            throw new IllegalArgumentException("illegal value can't update [" + key + "] from [" + previousSettings.getAsMap() + "] to [" + currentSettings.getAsMap() + "]", e);
-                        }
-                        return currentSettings;
-                    }
-
-                    @Override
-                    public void apply(Settings value, Settings current, Settings previous) {
-                        consumer.accept(value);
-                    }
-
-                    @Override
-                    public String toString() {
-                        return "Updater for: " + setting.toString();
-                    }
-                };
-            }
-        };
-    }
-
-    public static Setting<TimeValue> timeSetting(String key, Function<Settings, String> defaultValue, TimeValue minValue, boolean dynamic, Scope scope) {
-        return new Setting<>(key, defaultValue, (s) -> {
-            TimeValue timeValue = TimeValue.parseTimeValue(s, null, key);
-            if (timeValue.millis() < minValue.millis()) {
-                throw new IllegalArgumentException("Failed to parse value [" + s + "] for setting [" + key + "] must be >= " + minValue);
-            }
-            return timeValue;
-        }, dynamic, scope);
-    }
-
-    public static Setting<TimeValue> timeSetting(String key, TimeValue defaultValue, TimeValue minValue, boolean dynamic, Scope scope) {
-        return timeSetting(key, (s) -> defaultValue.getStringRep(), minValue, dynamic, scope);
-    }
-
-    public static Setting<TimeValue> timeSetting(String key, TimeValue defaultValue, boolean dynamic, Scope scope) {
-        return new Setting<>(key, (s) -> defaultValue.toString(), (s) -> TimeValue.parseTimeValue(s, defaultValue, key), dynamic, scope);
-    }
-
-    public static Setting<Double> doubleSetting(String key, double defaultValue, double minValue, boolean dynamic, Scope scope) {
-        return new Setting<>(key, (s) -> Double.toString(defaultValue), (s) -> {
-            final double d = Double.parseDouble(s);
-            if (d < minValue) {
-                throw new IllegalArgumentException("Failed to parse value [" + s + "] for setting [" + key + "] must be >= " + minValue);
-            }
-            return d;
-        }, dynamic, scope);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Settings.java b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
index 05f3cb1..5e083a9 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Settings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
@@ -614,9 +614,6 @@ public final class Settings implements ToXContent {
         if (settingPrefix.charAt(settingPrefix.length() - 1) != '.') {
             settingPrefix = settingPrefix + ".";
         }
-        return getGroupsInternal(settingPrefix, ignoreNonGrouped);
-    }
-    private Map<String, Settings> getGroupsInternal(String settingPrefix, boolean ignoreNonGrouped) throws SettingsException {
         // we don't really care that it might happen twice
         Map<String, Map<String, String>> map = new LinkedHashMap<>();
         for (Object o : settings.keySet()) {
@@ -646,12 +643,6 @@ public final class Settings implements ToXContent {
         }
         return Collections.unmodifiableMap(retVal);
     }
-    /**
-     * Returns group settings for the given setting prefix.
-     */
-    public Map<String, Settings> getAsGroups() throws SettingsException {
-        return getGroupsInternal("", false);
-    }
 
     /**
      * Returns a parsed version.
@@ -715,7 +706,7 @@ public final class Settings implements ToXContent {
         Builder builder = new Builder();
         int numberOfSettings = in.readVInt();
         for (int i = 0; i < numberOfSettings; i++) {
-            builder.put(in.readString(), in.readOptionalString());
+            builder.put(in.readString(), in.readString());
         }
         return builder.build();
     }
@@ -724,7 +715,7 @@ public final class Settings implements ToXContent {
         out.writeVInt(settings.getAsMap().size());
         for (Map.Entry<String, String> entry : settings.getAsMap().entrySet()) {
             out.writeString(entry.getKey());
-            out.writeOptionalString(entry.getValue());
+            out.writeString(entry.getValue());
         }
     }
 
@@ -827,10 +818,6 @@ public final class Settings implements ToXContent {
             return this;
         }
 
-        public Builder putNull(String key) {
-            return put(key, (String) null);
-        }
-
         /**
          * Sets a setting with the provided setting key and class as value.
          *
diff --git a/core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java b/core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java
index 8bc8ce1..2ae4799 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/SettingsModule.java
@@ -21,10 +21,6 @@ package org.elasticsearch.common.settings;
 
 import org.elasticsearch.common.inject.AbstractModule;
 
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-
 /**
  * A module that binds the provided settings to the {@link Settings} interface.
  *
@@ -34,36 +30,15 @@ public class SettingsModule extends AbstractModule {
 
     private final Settings settings;
     private final SettingsFilter settingsFilter;
-    private final Map<String, Setting<?>> clusterDynamicSettings = new HashMap<>();
-
 
     public SettingsModule(Settings settings, SettingsFilter settingsFilter) {
         this.settings = settings;
         this.settingsFilter = settingsFilter;
-        for (Setting<?> setting : ClusterSettings.BUILT_IN_CLUSTER_SETTINGS) {
-            registerSetting(setting);
-        }
     }
 
     @Override
     protected void configure() {
         bind(Settings.class).toInstance(settings);
         bind(SettingsFilter.class).toInstance(settingsFilter);
-        final ClusterSettings clusterSettings = new ClusterSettings(settings, new HashSet<>(clusterDynamicSettings.values()));
-        bind(ClusterSettings.class).toInstance(clusterSettings);
     }
-
-    public void registerSetting(Setting<?> setting) {
-        switch (setting.getScope()) {
-            case CLUSTER:
-                if (clusterDynamicSettings.containsKey(setting.getKey())) {
-                    throw new IllegalArgumentException("Cannot register setting [" + setting.getKey() + "] twice");
-                }
-                clusterDynamicSettings.put(setting.getKey(), setting);
-                break;
-            case INDEX:
-                throw new UnsupportedOperationException("not yet implemented");
-        }
-    }
-
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/settings/loader/XContentSettingsLoader.java b/core/src/main/java/org/elasticsearch/common/settings/loader/XContentSettingsLoader.java
index 9c2f973..725c7e5 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/loader/XContentSettingsLoader.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/loader/XContentSettingsLoader.java
@@ -103,9 +103,9 @@ public abstract class XContentSettingsLoader implements SettingsLoader {
             } else if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.VALUE_NULL) {
-                serializeValue(settings, sb, path, parser, currentFieldName, true);
+                // ignore this
             } else {
-                serializeValue(settings, sb, path, parser, currentFieldName, false);
+                serializeValue(settings, sb, path, parser, currentFieldName);
 
             }
         }
@@ -126,33 +126,31 @@ public abstract class XContentSettingsLoader implements SettingsLoader {
             } else if (token == XContentParser.Token.FIELD_NAME) {
                 fieldName = parser.currentName();
             } else if (token == XContentParser.Token.VALUE_NULL) {
-                serializeValue(settings, sb, path, parser, fieldName + '.' + (counter++), true);
                 // ignore
             } else {
-                serializeValue(settings, sb, path, parser, fieldName + '.' + (counter++), false);
+                serializeValue(settings, sb, path, parser, fieldName + '.' + (counter++));
             }
         }
     }
 
-    private void serializeValue(Map<String, String> settings, StringBuilder sb, List<String> path, XContentParser parser, String fieldName, boolean isNull) throws IOException {
+    private void serializeValue(Map<String, String> settings, StringBuilder sb, List<String> path, XContentParser parser, String fieldName) throws IOException {
         sb.setLength(0);
         for (String pathEle : path) {
             sb.append(pathEle).append('.');
         }
         sb.append(fieldName);
         String key = sb.toString();
-        String currentValue = isNull ? null : parser.text();
-
-        if (settings.containsKey(key)) {
+        String currentValue = parser.text();
+        String previousValue = settings.put(key, currentValue);
+        if (previousValue != null) {
             throw new ElasticsearchParseException(
                     "duplicate settings key [{}] found at line number [{}], column number [{}], previous value [{}], current value [{}]",
                     key,
                     parser.getTokenLocation().lineNumber,
                     parser.getTokenLocation().columnNumber,
-                    settings.get(key),
+                    previousValue,
                     currentValue
             );
         }
-        settings.put(key, currentValue);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/unit/TimeValue.java b/core/src/main/java/org/elasticsearch/common/unit/TimeValue.java
index fb44c7d..ee63716 100644
--- a/core/src/main/java/org/elasticsearch/common/unit/TimeValue.java
+++ b/core/src/main/java/org/elasticsearch/common/unit/TimeValue.java
@@ -229,30 +229,6 @@ public class TimeValue implements Streamable {
         return Strings.format1Decimals(value, suffix);
     }
 
-    public String getStringRep() {
-        if (duration < 0) {
-            return Long.toString(duration);
-        }
-        switch (timeUnit) {
-            case NANOSECONDS:
-                return Strings.format1Decimals(duration, "nanos");
-            case MICROSECONDS:
-                return Strings.format1Decimals(duration, "micros");
-            case MILLISECONDS:
-                return Strings.format1Decimals(duration, "ms");
-            case SECONDS:
-                return Strings.format1Decimals(duration, "s");
-            case MINUTES:
-                return Strings.format1Decimals(duration, "m");
-            case HOURS:
-                return Strings.format1Decimals(duration, "h");
-            case DAYS:
-                return Strings.format1Decimals(duration, "d");
-            default:
-                throw new IllegalArgumentException("unknown time unit: " + timeUnit.name());
-        }
-    }
-
     public static TimeValue parseTimeValue(String sValue, TimeValue defaultValue, String settingName) {
         settingName = Objects.requireNonNull(settingName);
         assert settingName.startsWith("index.") == false || MetaDataIndexUpgradeService.INDEX_TIME_SETTINGS.contains(settingName) : settingName;
diff --git a/core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java b/core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java
index b8c5ba0..a605d66 100644
--- a/core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java
+++ b/core/src/main/java/org/elasticsearch/common/util/CancellableThreads.java
@@ -18,6 +18,7 @@
  */
 package org.elasticsearch.common.util;
 
+import org.apache.lucene.util.ThreadInterruptedException;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.Nullable;
@@ -84,7 +85,7 @@ public class CancellableThreads {
         RuntimeException throwable = null;
         try {
             interruptable.run();
-        } catch (InterruptedException e) {
+        } catch (InterruptedException | ThreadInterruptedException e) {
             // assume this is us and ignore
         } catch (RuntimeException t) {
             throwable = t;
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContent.java
index 101098d..50c0493 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContent.java
@@ -46,11 +46,6 @@ public interface XContent {
     XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException;
 
     /**
-     * Creates a new generator using the provided writer.
-     */
-    XContentGenerator createGenerator(Writer writer) throws IOException;
-
-    /**
      * Creates a parser over the provided string content.
      */
     XContentParser createParser(String content) throws IOException;
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java
index 2bd4b9e..af8e753 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java
@@ -920,23 +920,18 @@ public final class XContentBuilder implements BytesStream, Releasable {
         return this;
     }
 
-    public XContentBuilder rawField(String fieldName, byte[] content) throws IOException {
-        generator.writeRawField(fieldName, content, bos);
+    public XContentBuilder rawField(String fieldName, InputStream content) throws IOException {
+        generator.writeRawField(fieldName, content);
         return this;
     }
 
-    public XContentBuilder rawField(String fieldName, byte[] content, int offset, int length) throws IOException {
-        generator.writeRawField(fieldName, content, offset, length, bos);
-        return this;
-    }
-
-    public XContentBuilder rawField(String fieldName, InputStream content, XContentType contentType) throws IOException {
-        generator.writeRawField(fieldName, content, bos, contentType);
+    public XContentBuilder rawField(String fieldName, BytesReference content) throws IOException {
+        generator.writeRawField(fieldName, content);
         return this;
     }
 
-    public XContentBuilder rawField(String fieldName, BytesReference content) throws IOException {
-        generator.writeRawField(fieldName, content, bos);
+    public XContentBuilder rawValue(BytesReference content) throws IOException {
+        generator.writeRawValue(content);
         return this;
     }
 
@@ -1202,10 +1197,6 @@ public final class XContentBuilder implements BytesStream, Releasable {
         return this.generator;
     }
 
-    public OutputStream stream() {
-        return this.bos;
-    }
-
     @Override
     public BytesReference bytes() {
         close();
@@ -1213,14 +1204,6 @@ public final class XContentBuilder implements BytesStream, Releasable {
     }
 
     /**
-     * Returns the actual stream used.
-     */
-    public BytesStream bytesStream() throws IOException {
-        close();
-        return (BytesStream) bos;
-    }
-
-    /**
      * Returns a string representation of the builder (only applicable for text based xcontent).
      */
     public String string() throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
index 9843a19..11a42e3 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
@@ -24,7 +24,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import java.io.Closeable;
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 
 /**
  *
@@ -112,13 +111,11 @@ public interface XContentGenerator extends Closeable {
 
     void writeObjectFieldStart(XContentString fieldName) throws IOException;
 
-    void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException;
+    void writeRawField(String fieldName, InputStream content) throws IOException;
 
-    void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException;
+    void writeRawField(String fieldName, BytesReference content) throws IOException;
 
-    void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException;
-
-    void writeRawField(String fieldName, BytesReference content, OutputStream bos) throws IOException;
+    void writeRawValue(BytesReference content) throws IOException;
 
     void copyCurrentStructure(XContentParser parser) throws IOException;
 
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java
index f16332f..4466d29 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.compress.Compressor;
 import org.elasticsearch.common.compress.CompressorFactory;
-import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.xcontent.ToXContent.Params;
 
 import java.io.BufferedInputStream;
@@ -102,9 +101,7 @@ public class XContentHelper {
             BytesArray bytesArray = bytes.toBytesArray();
             return new String(bytesArray.array(), bytesArray.arrayOffset(), bytesArray.length(), StandardCharsets.UTF_8);
         }
-        XContentParser parser = null;
-        try {
-            parser = XContentFactory.xContent(xContentType).createParser(bytes.streamInput());
+        try (XContentParser parser = XContentFactory.xContent(xContentType).createParser(bytes.streamInput())) {
             parser.nextToken();
             XContentBuilder builder = XContentFactory.jsonBuilder();
             if (prettyPrint) {
@@ -112,10 +109,6 @@ public class XContentHelper {
             }
             builder.copyCurrentStructure(parser);
             return builder.string();
-        } finally {
-            if (parser != null) {
-                parser.close();
-            }
         }
     }
 
@@ -128,9 +121,7 @@ public class XContentHelper {
         if (xContentType == XContentType.JSON && !reformatJson) {
             return new String(data, offset, length, StandardCharsets.UTF_8);
         }
-        XContentParser parser = null;
-        try {
-            parser = XContentFactory.xContent(xContentType).createParser(data, offset, length);
+        try (XContentParser parser = XContentFactory.xContent(xContentType).createParser(data, offset, length)) {
             parser.nextToken();
             XContentBuilder builder = XContentFactory.jsonBuilder();
             if (prettyPrint) {
@@ -138,10 +129,6 @@ public class XContentHelper {
             }
             builder.copyCurrentStructure(parser);
             return builder.string();
-        } finally {
-            if (parser != null) {
-                parser.close();
-            }
         }
     }
 
@@ -379,38 +366,6 @@ public class XContentHelper {
     }
 
     /**
-     * Directly writes the source to the output builder
-     */
-    public static void writeDirect(BytesReference source, XContentBuilder rawBuilder, ToXContent.Params params) throws IOException {
-        Compressor compressor = CompressorFactory.compressor(source);
-        if (compressor != null) {
-            InputStream compressedStreamInput = compressor.streamInput(source.streamInput());
-            if (compressedStreamInput.markSupported() == false) {
-                compressedStreamInput = new BufferedInputStream(compressedStreamInput);
-            }
-            XContentType contentType = XContentFactory.xContentType(compressedStreamInput);
-            if (contentType == rawBuilder.contentType()) {
-                Streams.copy(compressedStreamInput, rawBuilder.stream());
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(compressedStreamInput)) {
-                    parser.nextToken();
-                    rawBuilder.copyCurrentStructure(parser);
-                }
-            }
-        } else {
-            XContentType contentType = XContentFactory.xContentType(source);
-            if (contentType == rawBuilder.contentType()) {
-                source.writeTo(rawBuilder.stream());
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(source)) {
-                    parser.nextToken();
-                    rawBuilder.copyCurrentStructure(parser);
-                }
-            }
-        }
-    }
-
-    /**
      * Writes a "raw" (bytes) field, handling cases where the bytes are compressed, and tries to optimize writing using
      * {@link XContentBuilder#rawField(String, org.elasticsearch.common.bytes.BytesReference)}.
      */
@@ -418,30 +373,9 @@ public class XContentHelper {
         Compressor compressor = CompressorFactory.compressor(source);
         if (compressor != null) {
             InputStream compressedStreamInput = compressor.streamInput(source.streamInput());
-            if (compressedStreamInput.markSupported() == false) {
-                compressedStreamInput = new BufferedInputStream(compressedStreamInput);
-            }
-            XContentType contentType = XContentFactory.xContentType(compressedStreamInput);
-            if (contentType == builder.contentType()) {
-                builder.rawField(field, compressedStreamInput, contentType);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(compressedStreamInput)) {
-                    parser.nextToken();
-                    builder.field(field);
-                    builder.copyCurrentStructure(parser);
-                }
-            }
+            builder.rawField(field, compressedStreamInput);
         } else {
-            XContentType contentType = XContentFactory.xContentType(source);
-            if (contentType == builder.contentType()) {
-                builder.rawField(field, source);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(contentType).createParser(source)) {
-                    parser.nextToken();
-                    builder.field(field);
-                    builder.copyCurrentStructure(parser);
-                }
-            }
+            builder.rawField(field, source);
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
index b68d3e1..d647c5f 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentParser.java
@@ -178,12 +178,6 @@ public interface XContentParser extends Releasable {
 
     NumberType numberType() throws IOException;
 
-    /**
-     * Is the number type estimated or not (i.e. an int might actually be a long, its just low enough
-     * to be an int).
-     */
-    boolean estimatedNumberType();
-
     short shortValue(boolean coerce) throws IOException;
 
     int intValue(boolean coerce) throws IOException;
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java
index 5417675..5f8dddc 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContent.java
@@ -61,17 +61,12 @@ public class CborXContent implements XContent {
 
     @Override
     public XContentGenerator createGenerator(OutputStream os) throws IOException {
-        return new CborXContentGenerator(cborFactory.createGenerator(os, JsonEncoding.UTF8));
+        return new CborXContentGenerator(cborFactory.createGenerator(os, JsonEncoding.UTF8), os);
     }
 
     @Override
     public XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException {
-        return new CborXContentGenerator(cborFactory.createGenerator(os, JsonEncoding.UTF8), filters);
-    }
-
-    @Override
-    public XContentGenerator createGenerator(Writer writer) throws IOException {
-        return new CborXContentGenerator(cborFactory.createGenerator(writer));
+        return new CborXContentGenerator(cborFactory.createGenerator(os, JsonEncoding.UTF8), os, filters);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContentGenerator.java
index a3ca669..517266b 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/cbor/CborXContentGenerator.java
@@ -20,13 +20,9 @@
 package org.elasticsearch.common.xcontent.cbor;
 
 import com.fasterxml.jackson.core.JsonGenerator;
-import com.fasterxml.jackson.dataformat.cbor.CBORParser;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.common.xcontent.json.JsonXContentGenerator;
 
-import java.io.IOException;
-import java.io.InputStream;
 import java.io.OutputStream;
 
 /**
@@ -34,8 +30,8 @@ import java.io.OutputStream;
  */
 public class CborXContentGenerator extends JsonXContentGenerator {
 
-    public CborXContentGenerator(JsonGenerator jsonGenerator, String... filters) {
-        super(jsonGenerator, filters);
+    public CborXContentGenerator(JsonGenerator jsonGenerator, OutputStream os, String... filters) {
+        super(jsonGenerator, os, filters);
     }
 
     @Override
@@ -49,46 +45,7 @@ public class CborXContentGenerator extends JsonXContentGenerator {
     }
 
     @Override
-    public void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException {
-        writeFieldName(fieldName);
-        try (CBORParser parser = CborXContent.cborFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (CBORParser parser = CborXContent.cborFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    protected void writeObjectRaw(String fieldName, BytesReference content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        CBORParser parser;
-        if (content.hasArray()) {
-            parser = CborXContent.cborFactory.createParser(content.array(), content.arrayOffset(), content.length());
-        } else {
-            parser = CborXContent.cborFactory.createParser(content.streamInput());
-        }
-        try {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        } finally {
-            parser.close();
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (CBORParser parser = CborXContent.cborFactory.createParser(content, offset, length)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
+    protected boolean supportsRawWrites() {
+        return false;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java
index 671dee0..86f5bc2 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java
@@ -65,17 +65,12 @@ public class JsonXContent implements XContent {
 
     @Override
     public XContentGenerator createGenerator(OutputStream os) throws IOException {
-        return new JsonXContentGenerator(jsonFactory.createGenerator(os, JsonEncoding.UTF8));
+        return new JsonXContentGenerator(jsonFactory.createGenerator(os, JsonEncoding.UTF8), os);
     }
 
     @Override
     public XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException {
-        return new JsonXContentGenerator(jsonFactory.createGenerator(os, JsonEncoding.UTF8), filters);
-    }
-
-    @Override
-    public XContentGenerator createGenerator(Writer writer) throws IOException {
-        return new JsonXContentGenerator(jsonFactory.createGenerator(writer));
+        return new JsonXContentGenerator(jsonFactory.createGenerator(os, JsonEncoding.UTF8), os, filters);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java
index 7f3cc63..0854f7a 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java
@@ -26,13 +26,13 @@ import com.fasterxml.jackson.core.filter.FilteringGeneratorDelegate;
 import com.fasterxml.jackson.core.io.SerializedString;
 import com.fasterxml.jackson.core.util.DefaultIndenter;
 import com.fasterxml.jackson.core.util.DefaultPrettyPrinter;
-import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.common.xcontent.support.filtering.FilterPathBasedFilter;
 
+import java.io.BufferedInputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
@@ -58,11 +58,14 @@ public class JsonXContentGenerator implements XContentGenerator {
      */
     private final FilteringGeneratorDelegate filter;
 
+    private final OutputStream os;
+
     private boolean writeLineFeedAtEnd;
     private static final SerializedString LF = new SerializedString("\n");
     private static final DefaultPrettyPrinter.Indenter INDENTER = new DefaultIndenter("  ", LF.getValue());
+    private boolean prettyPrint = false;
 
-    public JsonXContentGenerator(JsonGenerator jsonGenerator, String... filters) {
+    public JsonXContentGenerator(JsonGenerator jsonGenerator, OutputStream os, String... filters) {
         if (jsonGenerator instanceof GeneratorBase) {
             this.base = (GeneratorBase) jsonGenerator;
         } else {
@@ -76,6 +79,8 @@ public class JsonXContentGenerator implements XContentGenerator {
             this.filter = new FilteringGeneratorDelegate(jsonGenerator, new FilterPathBasedFilter(filters), true, true);
             this.generator = this.filter;
         }
+
+        this.os = os;
     }
 
     @Override
@@ -86,6 +91,7 @@ public class JsonXContentGenerator implements XContentGenerator {
     @Override
     public final void usePrettyPrint() {
         generator.setPrettyPrinter(new DefaultPrettyPrinter().withObjectIndenter(INDENTER));
+        prettyPrint = true;
     }
 
     @Override
@@ -323,22 +329,16 @@ public class JsonXContentGenerator implements XContentGenerator {
     }
 
     @Override
-    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
-        writeRawField(fieldName, new BytesArray(content), bos);
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException {
-        writeRawField(fieldName, new BytesArray(content, offset, length), bos);
-    }
-
-    @Override
-    public void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException {
-        if (isFiltered() || (contentType != contentType())) {
-            // When the current generator is filtered (ie filter != null)
-            // or the content is in a different format than the current generator,
-            // we need to copy the whole structure so that it will be correctly
-            // filtered or converted
+    public void writeRawField(String fieldName, InputStream content) throws IOException {
+        if (content.markSupported() == false) {
+            // needed for the XContentFactory.xContentType call
+            content = new BufferedInputStream(content);
+        }
+        XContentType contentType = XContentFactory.xContentType(content);
+        if (contentType == null) {
+            throw new IllegalArgumentException("Can't write raw bytes whose xcontent-type can't be guessed");
+        }
+        if (mayWriteRawData(contentType) == false) {
             try (XContentParser parser = XContentFactory.xContent(contentType).createParser(content)) {
                 parser.nextToken();
                 writeFieldName(fieldName);
@@ -347,49 +347,59 @@ public class JsonXContentGenerator implements XContentGenerator {
         } else {
             writeStartRaw(fieldName);
             flush();
-            Streams.copy(content, bos);
+            Streams.copy(content, os);
             writeEndRaw();
         }
     }
 
     @Override
-    public final void writeRawField(String fieldName, BytesReference content, OutputStream bos) throws IOException {
+    public final void writeRawField(String fieldName, BytesReference content) throws IOException {
         XContentType contentType = XContentFactory.xContentType(content);
-        if (contentType != null) {
-            if (isFiltered() || (contentType != contentType())) {
-                // When the current generator is filtered (ie filter != null)
-                // or the content is in a different format than the current generator,
-                // we need to copy the whole structure so that it will be correctly
-                // filtered or converted
-                copyRawField(fieldName, content, contentType.xContent());
-            } else {
-                // Otherwise, the generator is not filtered and has the same type: we can potentially optimize the write
-                writeObjectRaw(fieldName, content, bos);
-            }
-        } else {
+        if (contentType == null) {
+            throw new IllegalArgumentException("Can't write raw bytes whose xcontent-type can't be guessed");
+        }
+        if (mayWriteRawData(contentType) == false) {
             writeFieldName(fieldName);
-            // we could potentially optimize this to not rely on exception logic...
-            String sValue = content.toUtf8();
-            try {
-                writeNumber(Long.parseLong(sValue));
-            } catch (NumberFormatException e) {
-                try {
-                    writeNumber(Double.parseDouble(sValue));
-                } catch (NumberFormatException e1) {
-                    writeString(sValue);
-                }
-            }
+            copyRawValue(content, contentType.xContent());
+        } else {
+            writeStartRaw(fieldName);
+            flush();
+            content.writeTo(os);
+            writeEndRaw();
+        }
+    }
+
+    public final void writeRawValue(BytesReference content) throws IOException {
+        XContentType contentType = XContentFactory.xContentType(content);
+        if (contentType == null) {
+            throw new IllegalArgumentException("Can't write raw bytes whose xcontent-type can't be guessed");
+        }
+        if (mayWriteRawData(contentType) == false) {
+            copyRawValue(content, contentType.xContent());
+        } else {
+            flush();
+            content.writeTo(os);
+            writeEndRaw();
         }
     }
 
-    protected void writeObjectRaw(String fieldName, BytesReference content, OutputStream bos) throws IOException {
-        writeStartRaw(fieldName);
-        flush();
-        content.writeTo(bos);
-        writeEndRaw();
+    private boolean mayWriteRawData(XContentType contentType) {
+        // When the current generator is filtered (ie filter != null)
+        // or the content is in a different format than the current generator,
+        // we need to copy the whole structure so that it will be correctly
+        // filtered or converted
+        return supportsRawWrites()
+                && isFiltered() == false
+                && contentType == contentType()
+                && prettyPrint == false;
     }
 
-    protected void copyRawField(String fieldName, BytesReference content, XContent xContent) throws IOException {
+    /** Whether this generator supports writing raw data directly */
+    protected boolean supportsRawWrites() {
+        return true;
+    }
+
+    protected void copyRawValue(BytesReference content, XContent xContent) throws IOException {
         XContentParser parser = null;
         try {
             if (content.hasArray()) {
@@ -397,9 +407,6 @@ public class JsonXContentGenerator implements XContentGenerator {
             } else {
                 parser = xContent.createParser(content.streamInput());
             }
-            if (fieldName != null) {
-                writeFieldName(fieldName);
-            }
             copyCurrentStructure(parser);
         } finally {
             if (parser != null) {
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java
index 787c283..c3aca76 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java
@@ -69,11 +69,6 @@ public class JsonXContentParser extends AbstractXContentParser {
     }
 
     @Override
-    public boolean estimatedNumberType() {
-        return true;
-    }
-
-    @Override
     public String currentName() throws IOException {
         return parser.getCurrentName();
     }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java
index cb1bdd3..51b8590 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java
@@ -62,17 +62,12 @@ public class SmileXContent implements XContent {
 
     @Override
     public XContentGenerator createGenerator(OutputStream os) throws IOException {
-        return new SmileXContentGenerator(smileFactory.createGenerator(os, JsonEncoding.UTF8));
+        return new SmileXContentGenerator(smileFactory.createGenerator(os, JsonEncoding.UTF8), os);
     }
 
     @Override
     public XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException {
-        return new SmileXContentGenerator(smileFactory.createGenerator(os, JsonEncoding.UTF8), filters);
-    }
-
-    @Override
-    public XContentGenerator createGenerator(Writer writer) throws IOException {
-        return new SmileXContentGenerator(smileFactory.createGenerator(writer));
+        return new SmileXContentGenerator(smileFactory.createGenerator(os, JsonEncoding.UTF8), os, filters);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java
index 5002cfa..451abab 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java
@@ -20,13 +20,9 @@
 package org.elasticsearch.common.xcontent.smile;
 
 import com.fasterxml.jackson.core.JsonGenerator;
-import com.fasterxml.jackson.dataformat.smile.SmileParser;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.common.xcontent.json.JsonXContentGenerator;
 
-import java.io.IOException;
-import java.io.InputStream;
 import java.io.OutputStream;
 
 /**
@@ -34,8 +30,8 @@ import java.io.OutputStream;
  */
 public class SmileXContentGenerator extends JsonXContentGenerator {
 
-    public SmileXContentGenerator(JsonGenerator jsonGenerator, String... filters) {
-        super(jsonGenerator, filters);
+    public SmileXContentGenerator(JsonGenerator jsonGenerator, OutputStream os, String... filters) {
+        super(jsonGenerator, os, filters);
     }
 
     @Override
@@ -49,46 +45,7 @@ public class SmileXContentGenerator extends JsonXContentGenerator {
     }
 
     @Override
-    public void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException {
-        writeFieldName(fieldName);
-        try (SmileParser parser = SmileXContent.smileFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (SmileParser parser = SmileXContent.smileFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    protected void writeObjectRaw(String fieldName, BytesReference content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        SmileParser parser;
-        if (content.hasArray()) {
-            parser = SmileXContent.smileFactory.createParser(content.array(), content.arrayOffset(), content.length());
-        } else {
-            parser = SmileXContent.smileFactory.createParser(content.streamInput());
-        }
-        try {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        } finally {
-            parser.close();
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (SmileParser parser = SmileXContent.smileFactory.createParser(content, offset, length)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
+    protected boolean supportsRawWrites() {
+        return false;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java b/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java
index d79a6a8..c24ddb7 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java
@@ -60,17 +60,12 @@ public class YamlXContent implements XContent {
 
     @Override
     public XContentGenerator createGenerator(OutputStream os) throws IOException {
-        return new YamlXContentGenerator(yamlFactory.createGenerator(os, JsonEncoding.UTF8));
+        return new YamlXContentGenerator(yamlFactory.createGenerator(os, JsonEncoding.UTF8), os);
     }
 
     @Override
     public XContentGenerator createGenerator(OutputStream os, String[] filters) throws IOException {
-        return new YamlXContentGenerator(yamlFactory.createGenerator(os, JsonEncoding.UTF8), filters);
-    }
-
-    @Override
-    public XContentGenerator createGenerator(Writer writer) throws IOException {
-        return new YamlXContentGenerator(yamlFactory.createGenerator(writer));
+        return new YamlXContentGenerator(yamlFactory.createGenerator(os, JsonEncoding.UTF8), os, filters);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java
index e1a71c1..dcb2155 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContentGenerator.java
@@ -20,13 +20,9 @@
 package org.elasticsearch.common.xcontent.yaml;
 
 import com.fasterxml.jackson.core.JsonGenerator;
-import com.fasterxml.jackson.dataformat.yaml.YAMLParser;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.common.xcontent.json.JsonXContentGenerator;
 
-import java.io.IOException;
-import java.io.InputStream;
 import java.io.OutputStream;
 
 /**
@@ -34,8 +30,8 @@ import java.io.OutputStream;
  */
 public class YamlXContentGenerator extends JsonXContentGenerator {
 
-    public YamlXContentGenerator(JsonGenerator jsonGenerator, String... filters) {
-        super(jsonGenerator, filters);
+    public YamlXContentGenerator(JsonGenerator jsonGenerator, OutputStream os, String... filters) {
+        super(jsonGenerator, os, filters);
     }
 
     @Override
@@ -49,46 +45,7 @@ public class YamlXContentGenerator extends JsonXContentGenerator {
     }
 
     @Override
-    public void writeRawField(String fieldName, InputStream content, OutputStream bos, XContentType contentType) throws IOException {
-        writeFieldName(fieldName);
-        try (YAMLParser parser = YamlXContent.yamlFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (YAMLParser parser = YamlXContent.yamlFactory.createParser(content)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
-    }
-
-    @Override
-    protected void writeObjectRaw(String fieldName, BytesReference content, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        YAMLParser parser;
-        if (content.hasArray()) {
-            parser = YamlXContent.yamlFactory.createParser(content.array(), content.arrayOffset(), content.length());
-        } else {
-            parser = YamlXContent.yamlFactory.createParser(content.streamInput());
-        }
-        try {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        } finally {
-            parser.close();
-        }
-    }
-
-    @Override
-    public void writeRawField(String fieldName, byte[] content, int offset, int length, OutputStream bos) throws IOException {
-        writeFieldName(fieldName);
-        try (YAMLParser parser = YamlXContent.yamlFactory.createParser(content, offset, length)) {
-            parser.nextToken();
-            generator.copyCurrentStructure(parser);
-        }
+    protected boolean supportsRawWrites() {
+        return false;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java b/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
index eeba9ba..a820996 100644
--- a/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
+++ b/core/src/main/java/org/elasticsearch/discovery/DiscoveryService.java
@@ -23,6 +23,7 @@ import org.elasticsearch.ElasticsearchTimeoutException;
 import org.elasticsearch.cluster.ClusterChangedEvent;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -41,7 +42,6 @@ public class DiscoveryService extends AbstractLifecycleComponent<DiscoveryServic
     public static final String SETTING_INITIAL_STATE_TIMEOUT = "discovery.initial_state_timeout";
     public static final String SETTING_DISCOVERY_SEED = "discovery.id.seed";
 
-
     private static class InitialStateListener implements InitialStateDiscoveryListener {
 
         private final CountDownLatch latch = new CountDownLatch(1);
@@ -132,10 +132,7 @@ public class DiscoveryService extends AbstractLifecycleComponent<DiscoveryServic
     }
 
     public static String generateNodeId(Settings settings) {
-        String seed = settings.get(DiscoveryService.SETTING_DISCOVERY_SEED);
-        if (seed != null) {
-            return Strings.randomBase64UUID(new Random(Long.parseLong(seed)));
-        }
-        return Strings.randomBase64UUID();
+        Random random = Randomness.get(settings, DiscoveryService.SETTING_DISCOVERY_SEED);
+        return Strings.randomBase64UUID(random);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java b/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
index 6689d9c..20f2c96 100644
--- a/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
+++ b/core/src/main/java/org/elasticsearch/discovery/DiscoverySettings.java
@@ -23,10 +23,9 @@ import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.rest.RestStatus;
 
 import java.util.EnumSet;
@@ -36,40 +35,42 @@ import java.util.EnumSet;
  */
 public class DiscoverySettings extends AbstractComponent {
 
-    public final static int NO_MASTER_BLOCK_ID = 2;
-    public final static ClusterBlock NO_MASTER_BLOCK_ALL = new ClusterBlock(NO_MASTER_BLOCK_ID, "no master", true, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL);
-    public final static ClusterBlock NO_MASTER_BLOCK_WRITES = new ClusterBlock(NO_MASTER_BLOCK_ID, "no master", true, false, RestStatus.SERVICE_UNAVAILABLE, EnumSet.of(ClusterBlockLevel.WRITE, ClusterBlockLevel.METADATA_WRITE));
     /**
      * sets the timeout for a complete publishing cycle, including both sending and committing. the master
      * will continute to process the next cluster state update after this time has elapsed
      **/
-    public static final Setting<TimeValue> PUBLISH_TIMEOUT_SETTING = Setting.positiveTimeSetting("discovery.zen.publish_timeout", TimeValue.timeValueSeconds(30), true, Setting.Scope.CLUSTER);
+    public static final String PUBLISH_TIMEOUT = "discovery.zen.publish_timeout";
 
     /**
      * sets the timeout for receiving enough acks for a specific cluster state and committing it. failing
      * to receive responses within this window will cause the cluster state change to be rejected.
      */
-    public static final Setting<TimeValue> COMMIT_TIMEOUT_SETTING = new Setting<>("discovery.zen.commit_timeout", (s) -> PUBLISH_TIMEOUT_SETTING.getRaw(s), (s) -> TimeValue.parseTimeValue(s, TimeValue.timeValueSeconds(30), "discovery.zen.commit_timeout"), true, Setting.Scope.CLUSTER);
-    public static final Setting<ClusterBlock> NO_MASTER_BLOCK_SETTING = new Setting<>("discovery.zen.no_master_block", "write", DiscoverySettings::parseNoMasterBlock, true, Setting.Scope.CLUSTER);
-    public static final Setting<Boolean> PUBLISH_DIFF_ENABLE_SETTING = Setting.boolSetting("discovery.zen.publish_diff.enable", true, true, Setting.Scope.CLUSTER);
+    public static final String COMMIT_TIMEOUT = "discovery.zen.commit_timeout";
+    public static final String NO_MASTER_BLOCK = "discovery.zen.no_master_block";
+    public static final String PUBLISH_DIFF_ENABLE = "discovery.zen.publish_diff.enable";
+
+    public static final TimeValue DEFAULT_PUBLISH_TIMEOUT = TimeValue.timeValueSeconds(30);
+    public static final TimeValue DEFAULT_COMMIT_TIMEOUT = TimeValue.timeValueSeconds(30);
+    public static final String DEFAULT_NO_MASTER_BLOCK = "write";
+    public final static int NO_MASTER_BLOCK_ID = 2;
+    public final static boolean DEFAULT_PUBLISH_DIFF_ENABLE = true;
+
+    public final static ClusterBlock NO_MASTER_BLOCK_ALL = new ClusterBlock(NO_MASTER_BLOCK_ID, "no master", true, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL);
+    public final static ClusterBlock NO_MASTER_BLOCK_WRITES = new ClusterBlock(NO_MASTER_BLOCK_ID, "no master", true, false, RestStatus.SERVICE_UNAVAILABLE, EnumSet.of(ClusterBlockLevel.WRITE, ClusterBlockLevel.METADATA_WRITE));
 
     private volatile ClusterBlock noMasterBlock;
     private volatile TimeValue publishTimeout;
-
     private volatile TimeValue commitTimeout;
     private volatile boolean publishDiff;
 
     @Inject
-    public DiscoverySettings(Settings settings, ClusterSettings clusterSettings) {
+    public DiscoverySettings(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        clusterSettings.addSettingsUpdateConsumer(NO_MASTER_BLOCK_SETTING, this::setNoMasterBlock);
-        clusterSettings.addSettingsUpdateConsumer(PUBLISH_DIFF_ENABLE_SETTING, this::setPublishDiff);
-        clusterSettings.addSettingsUpdateConsumer(COMMIT_TIMEOUT_SETTING, this::setCommitTimeout);
-        clusterSettings.addSettingsUpdateConsumer(PUBLISH_TIMEOUT_SETTING, this::setPublishTimeout);
-        this.noMasterBlock = NO_MASTER_BLOCK_SETTING.get(settings);
-        this.publishTimeout = PUBLISH_TIMEOUT_SETTING.get(settings);
-        this.commitTimeout = COMMIT_TIMEOUT_SETTING.get(settings);
-        this.publishDiff = PUBLISH_DIFF_ENABLE_SETTING.get(settings);
+        nodeSettingsService.addListener(new ApplySettings());
+        this.noMasterBlock = parseNoMasterBlock(settings.get(NO_MASTER_BLOCK, DEFAULT_NO_MASTER_BLOCK));
+        this.publishTimeout = settings.getAsTime(PUBLISH_TIMEOUT, DEFAULT_PUBLISH_TIMEOUT);
+        this.commitTimeout = settings.getAsTime(COMMIT_TIMEOUT, new TimeValue(Math.min(DEFAULT_COMMIT_TIMEOUT.millis(), publishTimeout.millis())));
+        this.publishDiff = settings.getAsBoolean(PUBLISH_DIFF_ENABLE, DEFAULT_PUBLISH_DIFF_ENABLE);
     }
 
     /**
@@ -87,25 +88,47 @@ public class DiscoverySettings extends AbstractComponent {
         return noMasterBlock;
     }
 
-    private void setNoMasterBlock(ClusterBlock noMasterBlock) {
-        this.noMasterBlock = noMasterBlock;
-    }
-
-    private void setPublishDiff(boolean publishDiff) {
-        this.publishDiff = publishDiff;
-    }
-
-    private void setPublishTimeout(TimeValue publishTimeout) {
-        this.publishTimeout = publishTimeout;
-    }
+    public boolean getPublishDiff() { return publishDiff;}
 
-    private void setCommitTimeout(TimeValue commitTimeout) {
-        this.commitTimeout = commitTimeout;
+    private class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            TimeValue newPublishTimeout = settings.getAsTime(PUBLISH_TIMEOUT, null);
+            if (newPublishTimeout != null) {
+                if (newPublishTimeout.millis() != publishTimeout.millis()) {
+                    logger.info("updating [{}] from [{}] to [{}]", PUBLISH_TIMEOUT, publishTimeout, newPublishTimeout);
+                    publishTimeout = newPublishTimeout;
+                    if (settings.getAsTime(COMMIT_TIMEOUT, null) == null && commitTimeout.millis() > publishTimeout.millis()) {
+                        logger.info("reducing default [{}] to [{}] due to publish timeout change", COMMIT_TIMEOUT, publishTimeout);
+                        commitTimeout = publishTimeout;
+                    }
+                }
+            }
+            TimeValue newCommitTimeout = settings.getAsTime(COMMIT_TIMEOUT, null);
+            if (newCommitTimeout != null) {
+                if (newCommitTimeout.millis() != commitTimeout.millis()) {
+                    logger.info("updating [{}] from [{}] to [{}]", COMMIT_TIMEOUT, commitTimeout, newCommitTimeout);
+                    commitTimeout = newCommitTimeout;
+                }
+            }
+            String newNoMasterBlockValue = settings.get(NO_MASTER_BLOCK);
+            if (newNoMasterBlockValue != null) {
+                ClusterBlock newNoMasterBlock = parseNoMasterBlock(newNoMasterBlockValue);
+                if (newNoMasterBlock != noMasterBlock) {
+                    noMasterBlock = newNoMasterBlock;
+                }
+            }
+            Boolean newPublishDiff = settings.getAsBoolean(PUBLISH_DIFF_ENABLE, null);
+            if (newPublishDiff != null) {
+                if (newPublishDiff != publishDiff) {
+                    logger.info("updating [{}] from [{}] to [{}]", PUBLISH_DIFF_ENABLE, publishDiff, newPublishDiff);
+                    publishDiff = newPublishDiff;
+                }
+            }
+        }
     }
 
-    public boolean getPublishDiff() { return publishDiff;}
-
-    private static ClusterBlock parseNoMasterBlock(String value) {
+    private ClusterBlock parseNoMasterBlock(String value) {
         switch (value) {
             case "all":
                 return NO_MASTER_BLOCK_ALL;
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
index d69227d..03111d1 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
@@ -39,8 +39,6 @@ import org.elasticsearch.common.inject.internal.Nullable;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.discovery.Discovery;
@@ -57,6 +55,7 @@ import org.elasticsearch.discovery.zen.ping.ZenPingService;
 import org.elasticsearch.discovery.zen.publish.PendingClusterStateStats;
 import org.elasticsearch.discovery.zen.publish.PublishClusterStateAction;
 import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.*;
 
@@ -75,7 +74,7 @@ import static org.elasticsearch.common.unit.TimeValue.timeValueSeconds;
  */
 public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implements Discovery, PingContextProvider {
 
-    public final static Setting<Boolean> REJOIN_ON_MASTER_GONE_SETTING = Setting.boolSetting("discovery.zen.rejoin_on_master_gone", true, true, Setting.Scope.CLUSTER);
+    public final static String SETTING_REJOIN_ON_MASTER_GONE = "discovery.zen.rejoin_on_master_gone";
     public final static String SETTING_PING_TIMEOUT = "discovery.zen.ping_timeout";
     public final static String SETTING_JOIN_TIMEOUT = "discovery.zen.join_timeout";
     public final static String SETTING_JOIN_RETRY_ATTEMPTS = "discovery.zen.join_retry_attempts";
@@ -140,7 +139,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
 
     @Inject
     public ZenDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool,
-                        TransportService transportService, final ClusterService clusterService, ClusterSettings clusterSettings,
+                        TransportService transportService, final ClusterService clusterService, NodeSettingsService nodeSettingsService,
                         ZenPingService pingService, ElectMasterService electMasterService,
                         DiscoverySettings discoverySettings) {
         super(settings);
@@ -161,7 +160,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         this.masterElectionFilterClientNodes = settings.getAsBoolean(SETTING_MASTER_ELECTION_FILTER_CLIENT, true);
         this.masterElectionFilterDataNodes = settings.getAsBoolean(SETTING_MASTER_ELECTION_FILTER_DATA, false);
         this.masterElectionWaitForJoinsTimeout = settings.getAsTime(SETTING_MASTER_ELECTION_WAIT_FOR_JOINS_TIMEOUT, TimeValue.timeValueMillis(joinTimeout.millis() / 2));
-        this.rejoinOnMasterGone = REJOIN_ON_MASTER_GONE_SETTING.get(settings);
+        this.rejoinOnMasterGone = settings.getAsBoolean(SETTING_REJOIN_ON_MASTER_GONE, true);
 
         if (this.joinRetryAttempts < 1) {
             throw new IllegalArgumentException("'" + SETTING_JOIN_RETRY_ATTEMPTS + "' must be a positive number. got [" + SETTING_JOIN_RETRY_ATTEMPTS + "]");
@@ -172,14 +171,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
 
         logger.debug("using ping_timeout [{}], join.timeout [{}], master_election.filter_client [{}], master_election.filter_data [{}]", this.pingTimeout, joinTimeout, masterElectionFilterClientNodes, masterElectionFilterDataNodes);
 
-        clusterSettings.addSettingsUpdateConsumer(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING, this::handleMinimumMasterNodesChanged, (value) -> {
-            final ClusterState clusterState = clusterService.state();
-            int masterNodes = clusterState.nodes().masterNodes().size();
-            if (value > masterNodes) {
-                throw new IllegalArgumentException("cannot set " + ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey() + " to more than the current master nodes count [" + masterNodes + "]");
-            }
-        });
-        clusterSettings.addSettingsUpdateConsumer(REJOIN_ON_MASTER_GONE_SETTING, this::setRejoingOnMasterGone);
+        nodeSettingsService.addListener(new ApplySettings());
 
         this.masterFD = new MasterFaultDetection(settings, threadPool, transportService, clusterName, clusterService);
         this.masterFD.addListener(new MasterNodeFailureListener());
@@ -314,10 +306,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         return clusterJoinsCounter.get() > 0;
     }
 
-    private void setRejoingOnMasterGone(boolean rejoin) {
-        this.rejoinOnMasterGone = rejoin;
-    }
-
     /** end of {@link org.elasticsearch.discovery.zen.ping.PingContextProvider } implementation */
 
 
@@ -1151,6 +1139,26 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         }
     }
 
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            int minimumMasterNodes = settings.getAsInt(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES,
+                    ZenDiscovery.this.electMaster.minimumMasterNodes());
+            if (minimumMasterNodes != ZenDiscovery.this.electMaster.minimumMasterNodes()) {
+                logger.info("updating {} from [{}] to [{}]", ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES,
+                        ZenDiscovery.this.electMaster.minimumMasterNodes(), minimumMasterNodes);
+                handleMinimumMasterNodesChanged(minimumMasterNodes);
+            }
+
+            boolean rejoinOnMasterGone = settings.getAsBoolean(SETTING_REJOIN_ON_MASTER_GONE, ZenDiscovery.this.rejoinOnMasterGone);
+            if (rejoinOnMasterGone != ZenDiscovery.this.rejoinOnMasterGone) {
+                logger.info("updating {} from [{}] to [{}]", SETTING_REJOIN_ON_MASTER_GONE, ZenDiscovery.this.rejoinOnMasterGone, rejoinOnMasterGone);
+                ZenDiscovery.this.rejoinOnMasterGone = rejoinOnMasterGone;
+            }
+        }
+    }
+
+
     /**
      * All control of the join thread should happen under the cluster state update task thread.
      * This is important to make sure that the background joining process is always in sync with any cluster state updates
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java b/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
index 9cca1ed..9164a85 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
@@ -22,10 +22,11 @@ package org.elasticsearch.discovery.zen.elect;
 import com.carrotsearch.hppc.ObjectContainer;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.settings.Validator;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.CollectionUtils;
 
@@ -40,7 +41,23 @@ import java.util.List;
  */
 public class ElectMasterService extends AbstractComponent {
 
-    public static final Setting<Integer> DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING = Setting.intSetting("discovery.zen.minimum_master_nodes", -1, true, Setting.Scope.CLUSTER);
+    public static final String DISCOVERY_ZEN_MINIMUM_MASTER_NODES = "discovery.zen.minimum_master_nodes";
+    public static final Validator DISCOVERY_ZEN_MINIMUM_MASTER_NODES_VALIDATOR = new Validator() {
+        @Override
+        public String validate(String setting, String value, ClusterState clusterState) {
+            int intValue;
+            try {
+                intValue = Integer.parseInt(value);
+            } catch (NumberFormatException ex) {
+                return "cannot parse value [" + value + "] as an integer";
+            }
+            int masterNodes = clusterState.nodes().masterNodes().size();
+            if (intValue > masterNodes) {
+                return "cannot set " + ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES + " to more than the current master nodes count [" + masterNodes + "]";
+            }
+            return null;
+        }
+    };
 
     // This is the minimum version a master needs to be on, otherwise it gets ignored
     // This is based on the minimum compatible version of the current version this node is on
@@ -53,7 +70,7 @@ public class ElectMasterService extends AbstractComponent {
     public ElectMasterService(Settings settings, Version version) {
         super(settings);
         this.minMasterVersion = version.minimumCompatibilityVersion();
-        this.minimumMasterNodes = DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.get(settings);
+        this.minimumMasterNodes = settings.getAsInt(DISCOVERY_ZEN_MINIMUM_MASTER_NODES, -1);
         logger.debug("using minimum_master_nodes [{}]", minimumMasterNodes);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
index 5e410fb..e83ec69 100644
--- a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
+++ b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
@@ -227,7 +227,7 @@ public class GatewayService extends AbstractLifecycleComponent<GatewayService> i
                     // automatically generate a UID for the metadata if we need to
                     metaDataBuilder.generateClusterUuidIfNeeded();
 
-                    if (MetaData.SETTING_READ_ONLY_SETTING.get(recoveredState.metaData().settings()) || MetaData.SETTING_READ_ONLY_SETTING.get(currentState.metaData().settings())) {
+                    if (recoveredState.metaData().settings().getAsBoolean(MetaData.SETTING_READ_ONLY, false) || currentState.metaData().settings().getAsBoolean(MetaData.SETTING_READ_ONLY, false)) {
                         blocks.addGlobalBlock(MetaData.CLUSTER_READ_ONLY_BLOCK);
                     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
index 1404b61..de13eb1 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
@@ -781,10 +781,14 @@ public class InternalEngine extends Engine {
             // we need to fail the engine. it might have already been failed before
             // but we are double-checking it's failed and closed
             if (indexWriter.isOpen() == false && indexWriter.getTragicException() != null) {
-                failEngine("already closed by tragic event", indexWriter.getTragicException());
+                failEngine("already closed by tragic event on the index writer", indexWriter.getTragicException());
+            } else if (translog.isOpen() == false && translog.getTragicException() != null) {
+                failEngine("already closed by tragic event on the translog", translog.getTragicException());
             }
             return true;
-        } else if (t != null && indexWriter.isOpen() == false && indexWriter.getTragicException() == t) {
+        } else if (t != null &&
+            ((indexWriter.isOpen() == false && indexWriter.getTragicException() == t)
+                || (translog.isOpen() == false && translog.getTragicException() == t))) {
             // this spot on - we are handling the tragic event exception here so we have to fail the engine
             // right away
             failEngine(source, t);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
index e5e3387..b0ad972 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java
@@ -560,44 +560,19 @@ class DocumentParser implements Closeable {
             return builder;
         } else if (token == XContentParser.Token.VALUE_NUMBER) {
             XContentParser.NumberType numberType = context.parser().numberType();
-            if (numberType == XContentParser.NumberType.INT) {
-                if (context.parser().estimatedNumberType()) {
-                    Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "long");
-                    if (builder == null) {
-                        builder = MapperBuilders.longField(currentFieldName);
-                    }
-                    return builder;
-                } else {
-                    Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "integer");
-                    if (builder == null) {
-                        builder = MapperBuilders.integerField(currentFieldName);
-                    }
-                    return builder;
-                }
-            } else if (numberType == XContentParser.NumberType.LONG) {
+            if (numberType == XContentParser.NumberType.INT || numberType == XContentParser.NumberType.LONG) {
                 Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "long");
                 if (builder == null) {
                     builder = MapperBuilders.longField(currentFieldName);
                 }
                 return builder;
-            } else if (numberType == XContentParser.NumberType.FLOAT) {
-                if (context.parser().estimatedNumberType()) {
-                    Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "double");
-                    if (builder == null) {
-                        builder = MapperBuilders.doubleField(currentFieldName);
-                    }
-                    return builder;
-                } else {
-                    Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "float");
-                    if (builder == null) {
-                        builder = MapperBuilders.floatField(currentFieldName);
-                    }
-                    return builder;
-                }
-            } else if (numberType == XContentParser.NumberType.DOUBLE) {
+            } else if (numberType == XContentParser.NumberType.FLOAT || numberType == XContentParser.NumberType.DOUBLE) {
                 Mapper.Builder builder = context.root().findTemplateBuilder(context, currentFieldName, "double");
                 if (builder == null) {
-                    builder = MapperBuilders.doubleField(currentFieldName);
+                    // no templates are defined, we use float by default instead of double
+                    // since this is much more space-efficient and should be enough most of
+                    // the time
+                    builder = MapperBuilders.floatField(currentFieldName);
                 }
                 return builder;
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java b/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
index a8d0c0a..33a4dab 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/Mapper.java
@@ -134,6 +134,26 @@ public abstract class Mapper implements ToXContent, Iterable<Mapper> {
             public ParseFieldMatcher parseFieldMatcher() {
                 return parseFieldMatcher;
             }
+
+            public boolean isWithinMultiField() { return false; }
+
+            protected Function<String, TypeParser> typeParsers() { return typeParsers; }
+
+            protected Function<String, SimilarityProvider> similarityLookupService() { return similarityLookupService; }
+
+            public ParserContext createMultiFieldContext(ParserContext in) {
+                return new MultiFieldParserContext(in) {
+                    @Override
+                    public boolean isWithinMultiField() { return true; }
+                };
+            }
+
+            static class MultiFieldParserContext extends ParserContext {
+                MultiFieldParserContext(ParserContext in) {
+                    super(in.type(), in.analysisService, in.similarityLookupService(), in.mapperService(), in.typeParsers(), in.indexVersionCreated(), in.parseFieldMatcher());
+                }
+            }
+
         }
 
         Mapper.Builder<?,?> parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException;
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java
index a5c681d..0a921ad 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java
@@ -46,7 +46,7 @@ import java.util.Map;
 
 import static org.apache.lucene.index.IndexOptions.NONE;
 import static org.elasticsearch.index.mapper.MapperBuilders.stringField;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parseField;
+import static org.elasticsearch.index.mapper.core.TypeParsers.parseTextField;
 import static org.elasticsearch.index.mapper.core.TypeParsers.parseMultiField;
 
 public class StringFieldMapper extends FieldMapper implements AllFieldMapper.IncludeInAll {
@@ -159,7 +159,7 @@ public class StringFieldMapper extends FieldMapper implements AllFieldMapper.Inc
         @Override
         public Mapper.Builder parse(String name, Map<String, Object> node, ParserContext parserContext) throws MapperParsingException {
             StringFieldMapper.Builder builder = stringField(name);
-            parseField(builder, name, node, parserContext);
+            parseTextField(builder, name, node, parserContext);
             for (Iterator<Map.Entry<String, Object>> iterator = node.entrySet().iterator(); iterator.hasNext();) {
                 Map.Entry<String, Object> entry = iterator.next();
                 String propName = Strings.toUnderscoreCase(entry.getKey());
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java b/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
index c0e0cef..e530243 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java
@@ -25,6 +25,7 @@ import org.elasticsearch.Version;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.joda.FormatDateTimeFormatter;
 import org.elasticsearch.common.joda.Joda;
+import org.elasticsearch.common.logging.ESLoggerFactory;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.loader.SettingsLoader;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
@@ -181,31 +182,17 @@ public class TypeParsers {
         }
     }
 
-    public static void parseField(FieldMapper.Builder builder, String name, Map<String, Object> fieldNode, Mapper.TypeParser.ParserContext parserContext) {
+    private static void parseAnalyzersAndTermVectors(FieldMapper.Builder builder, String name, Map<String, Object> fieldNode, Mapper.TypeParser.ParserContext parserContext) {
         NamedAnalyzer indexAnalyzer = builder.fieldType().indexAnalyzer();
         NamedAnalyzer searchAnalyzer = builder.fieldType().searchAnalyzer();
+
         for (Iterator<Map.Entry<String, Object>> iterator = fieldNode.entrySet().iterator(); iterator.hasNext();) {
             Map.Entry<String, Object> entry = iterator.next();
             final String propName = Strings.toUnderscoreCase(entry.getKey());
             final Object propNode = entry.getValue();
-            if (propName.equals("index_name") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
-                builder.indexName(propNode.toString());
-                iterator.remove();
-            } else if (propName.equals("store")) {
-                builder.store(parseStore(name, propNode.toString()));
-                iterator.remove();
-            } else if (propName.equals("index")) {
-                parseIndex(name, propNode.toString(), builder);
-                iterator.remove();
-            } else if (propName.equals(DOC_VALUES)) {
-                builder.docValues(nodeBooleanValue(propNode));
-                iterator.remove();
-            } else if (propName.equals("term_vector")) {
+            if (propName.equals("term_vector")) {
                 parseTermVector(name, propNode.toString(), builder);
                 iterator.remove();
-            } else if (propName.equals("boost")) {
-                builder.boost(nodeFloatValue(propNode));
-                iterator.remove();
             } else if (propName.equals("store_term_vectors")) {
                 builder.storeTermVectors(nodeBooleanValue(propNode));
                 iterator.remove();
@@ -218,6 +205,69 @@ public class TypeParsers {
             } else if (propName.equals("store_term_vector_payloads")) {
                 builder.storeTermVectorPayloads(nodeBooleanValue(propNode));
                 iterator.remove();
+            } else if (propName.equals("analyzer") || // for backcompat, reading old indexes, remove for v3.0
+                    propName.equals("index_analyzer") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
+
+                NamedAnalyzer analyzer = parserContext.analysisService().analyzer(propNode.toString());
+                if (analyzer == null) {
+                    throw new MapperParsingException("analyzer [" + propNode.toString() + "] not found for field [" + name + "]");
+                }
+                indexAnalyzer = analyzer;
+                iterator.remove();
+            } else if (propName.equals("search_analyzer")) {
+                NamedAnalyzer analyzer = parserContext.analysisService().analyzer(propNode.toString());
+                if (analyzer == null) {
+                    throw new MapperParsingException("analyzer [" + propNode.toString() + "] not found for field [" + name + "]");
+                }
+                searchAnalyzer = analyzer;
+                iterator.remove();
+            }
+        }
+
+        if (indexAnalyzer == null) {
+            if (searchAnalyzer != null) {
+                throw new MapperParsingException("analyzer on field [" + name + "] must be set when search_analyzer is set");
+            }
+        } else if (searchAnalyzer == null) {
+            searchAnalyzer = indexAnalyzer;
+        }
+        builder.indexAnalyzer(indexAnalyzer);
+        builder.searchAnalyzer(searchAnalyzer);
+    }
+
+    /**
+     * Parse text field attributes. In addition to {@link #parseField common attributes}
+     * this will parse analysis and term-vectors related settings.
+     */
+    public static void parseTextField(FieldMapper.Builder builder, String name, Map<String, Object> fieldNode, Mapper.TypeParser.ParserContext parserContext) {
+        parseField(builder, name, fieldNode, parserContext);
+        parseAnalyzersAndTermVectors(builder, name, fieldNode, parserContext);
+    }
+
+    /**
+     * Parse common field attributes such as {@code doc_values} or {@code store}.
+     */
+    public static void parseField(FieldMapper.Builder builder, String name, Map<String, Object> fieldNode, Mapper.TypeParser.ParserContext parserContext) {
+        Version indexVersionCreated = parserContext.indexVersionCreated();
+        for (Iterator<Map.Entry<String, Object>> iterator = fieldNode.entrySet().iterator(); iterator.hasNext();) {
+            Map.Entry<String, Object> entry = iterator.next();
+            final String propName = Strings.toUnderscoreCase(entry.getKey());
+            final Object propNode = entry.getValue();
+            if (propName.equals("index_name") && indexVersionCreated.before(Version.V_2_0_0_beta1)) {
+                builder.indexName(propNode.toString());
+                iterator.remove();
+            } else if (propName.equals("store")) {
+                builder.store(parseStore(name, propNode.toString()));
+                iterator.remove();
+            } else if (propName.equals("index")) {
+                parseIndex(name, propNode.toString(), builder);
+                iterator.remove();
+            } else if (propName.equals(DOC_VALUES)) {
+                builder.docValues(nodeBooleanValue(propNode));
+                iterator.remove();
+            } else if (propName.equals("boost")) {
+                builder.boost(nodeFloatValue(propNode));
+                iterator.remove();
             } else if (propName.equals("omit_norms")) {
                 builder.omitNorms(nodeBooleanValue(propNode));
                 iterator.remove();
@@ -239,7 +289,7 @@ public class TypeParsers {
                 iterator.remove();
             } else if (propName.equals("omit_term_freq_and_positions")) {
                 final IndexOptions op = nodeBooleanValue(propNode) ? IndexOptions.DOCS : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-                if (parserContext.indexVersionCreated().onOrAfter(Version.V_1_0_0_RC2)) {
+                if (indexVersionCreated.onOrAfter(Version.V_1_0_0_RC2)) {
                     throw new ElasticsearchParseException("'omit_term_freq_and_positions' is not supported anymore - use ['index_options' : 'docs']  instead");
                 }
                 // deprecated option for BW compat
@@ -248,29 +298,13 @@ public class TypeParsers {
             } else if (propName.equals("index_options")) {
                 builder.indexOptions(nodeIndexOptionValue(propNode));
                 iterator.remove();
-            } else if (propName.equals("analyzer") || // for backcompat, reading old indexes, remove for v3.0
-                       propName.equals("index_analyzer") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
-                
-                NamedAnalyzer analyzer = parserContext.analysisService().analyzer(propNode.toString());
-                if (analyzer == null) {
-                    throw new MapperParsingException("analyzer [" + propNode.toString() + "] not found for field [" + name + "]");
-                }
-                indexAnalyzer = analyzer;
-                iterator.remove();
-            } else if (propName.equals("search_analyzer")) {
-                NamedAnalyzer analyzer = parserContext.analysisService().analyzer(propNode.toString());
-                if (analyzer == null) {
-                    throw new MapperParsingException("analyzer [" + propNode.toString() + "] not found for field [" + name + "]");
-                }
-                searchAnalyzer = analyzer;
-                iterator.remove();
             } else if (propName.equals("include_in_all")) {
                 builder.includeInAll(nodeBooleanValue(propNode));
                 iterator.remove();
-            } else if (propName.equals("postings_format") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
+            } else if (propName.equals("postings_format") && indexVersionCreated.before(Version.V_2_0_0_beta1)) {
                 // ignore for old indexes
                 iterator.remove();
-            } else if (propName.equals("doc_values_format") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
+            } else if (propName.equals("doc_values_format") && indexVersionCreated.before(Version.V_2_0_0_beta1)) {
                 // ignore for old indexes
                 iterator.remove();
             } else if (propName.equals("similarity")) {
@@ -281,23 +315,28 @@ public class TypeParsers {
                 builder.fieldDataSettings(settings);
                 iterator.remove();
             } else if (propName.equals("copy_to")) {
-                parseCopyFields(propNode, builder);
+                if (parserContext.isWithinMultiField()) {
+                    if (indexVersionCreated.after(Version.V_2_1_0) ||
+                        (indexVersionCreated.after(Version.V_2_0_1) && indexVersionCreated.before(Version.V_2_1_0))) {
+                        throw new MapperParsingException("copy_to in multi fields is not allowed. Found the copy_to in field [" + name + "] which is within a multi field.");
+                    } else {
+                        ESLoggerFactory.getLogger("mapping [" + parserContext.type() + "]").warn("Found a copy_to in field [" + name + "] which is within a multi field. This feature has been removed and the copy_to will be removed from the mapping.");
+                    }
+                } else {
+                    parseCopyFields(propNode, builder);
+                }
                 iterator.remove();
             }
         }
-
-        if (indexAnalyzer == null) {
-            if (searchAnalyzer != null) {
-                throw new MapperParsingException("analyzer on field [" + name + "] must be set when search_analyzer is set");
-            }
-        } else if (searchAnalyzer == null) {
-            searchAnalyzer = indexAnalyzer;
+        if (indexVersionCreated.before(Version.V_2_2_0)) {
+            // analyzer, search_analyzer, term_vectors were accepted on all fields
+            // before 2.2, even though it made little sense
+            parseAnalyzersAndTermVectors(builder, name, fieldNode, parserContext);
         }
-        builder.indexAnalyzer(indexAnalyzer);
-        builder.searchAnalyzer(searchAnalyzer);
     }
 
     public static boolean parseMultiField(FieldMapper.Builder builder, String name, Mapper.TypeParser.ParserContext parserContext, String propName, Object propNode) {
+        parserContext = parserContext.createMultiFieldContext(parserContext);
         if (propName.equals("path") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
             builder.multiFieldPathType(parsePathType(name, propNode.toString()));
             return true;
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
index 7e78432..71b6d89 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java
@@ -193,7 +193,8 @@ public class GeoShapeFieldMapper extends FieldMapper {
                 } else if (Names.COERCE.equals(fieldName)) {
                     builder.coerce(nodeBooleanValue(fieldNode));
                     iterator.remove();
-                } else if (Names.STRATEGY_POINTS_ONLY.equals(fieldName)) {
+                } else if (Names.STRATEGY_POINTS_ONLY.equals(fieldName)
+                    && builder.fieldType().strategyName.equals(SpatialStrategy.TERM.getStrategyName()) == false) {
                     builder.fieldType().setPointsOnly(XContentMapValues.nodeBooleanValue(fieldNode));
                     iterator.remove();
                 }
@@ -284,6 +285,7 @@ public class GeoShapeFieldMapper extends FieldMapper {
             termStrategy = new TermQueryPrefixTreeStrategy(prefixTree, names().indexName());
             termStrategy.setDistErrPct(distanceErrorPct());
             defaultStrategy = resolveStrategy(strategyName);
+            defaultStrategy.setPointsOnly(pointsOnly);
         }
 
         @Override
@@ -347,6 +349,9 @@ public class GeoShapeFieldMapper extends FieldMapper {
         public void setStrategyName(String strategyName) {
             checkIfFrozen();
             this.strategyName = strategyName;
+            if (this.strategyName.equals(SpatialStrategy.TERM)) {
+                this.pointsOnly = true;
+            }
         }
 
         public boolean pointsOnly() {
@@ -406,7 +411,6 @@ public class GeoShapeFieldMapper extends FieldMapper {
 
         public PrefixTreeStrategy resolveStrategy(String strategyName) {
             if (SpatialStrategy.RECURSIVE.getStrategyName().equals(strategyName)) {
-                recursiveStrategy.setPointsOnly(pointsOnly());
                 return recursiveStrategy;
             }
             if (SpatialStrategy.TERM.getStrategyName().equals(strategyName)) {
@@ -446,7 +450,7 @@ public class GeoShapeFieldMapper extends FieldMapper {
                 }
                 shape = shapeBuilder.build();
             }
-            if (fieldType().defaultStrategy() instanceof RecursivePrefixTreeStrategy && fieldType().pointsOnly() && !(shape instanceof Point)) {
+            if (fieldType().pointsOnly() && !(shape instanceof Point)) {
                 throw new MapperParsingException("[{" + fieldType().names().fullName() + "}] is configured for points only but a " +
                         ((shape instanceof JtsGeometry) ? ((JtsGeometry)shape).getGeom().getGeometryType() : shape.getClass()) + " was found");
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
index 3166a68..645c36a 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
@@ -49,7 +49,7 @@ import java.util.Map;
 
 import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBooleanValue;
 import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeMapValue;
-import static org.elasticsearch.index.mapper.core.TypeParsers.parseField;
+import static org.elasticsearch.index.mapper.core.TypeParsers.parseTextField;
 
 /**
  *
@@ -134,7 +134,7 @@ public class AllFieldMapper extends MetadataFieldMapper {
                 }
             }
             
-            parseField(builder, builder.name, node, parserContext);
+            parseTextField(builder, builder.name, node, parserContext);
             for (Iterator<Map.Entry<String, Object>> iterator = node.entrySet().iterator(); iterator.hasNext();) {
                 Map.Entry<String, Object> entry = iterator.next();
                 String fieldName = Strings.toUnderscoreCase(entry.getKey());
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java
index da3b8db..f9bcb31 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java
@@ -29,13 +29,10 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.compress.Compressor;
 import org.elasticsearch.common.compress.CompressorFactory;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
@@ -48,17 +45,13 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 
-import java.io.BufferedInputStream;
 import java.io.IOException;
-import java.io.InputStream;
 import java.util.Arrays;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Objects;
 
 import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBooleanValue;
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeStringValue;
 
 /**
  *
@@ -72,8 +65,6 @@ public class SourceFieldMapper extends MetadataFieldMapper {
     public static class Defaults {
         public static final String NAME = SourceFieldMapper.NAME;
         public static final boolean ENABLED = true;
-        public static final long COMPRESS_THRESHOLD = -1;
-        public static final String FORMAT = null; // default format is to use the one provided
 
         public static final MappedFieldType FIELD_TYPE = new SourceFieldType();
 
@@ -93,12 +84,6 @@ public class SourceFieldMapper extends MetadataFieldMapper {
 
         private boolean enabled = Defaults.ENABLED;
 
-        private long compressThreshold = Defaults.COMPRESS_THRESHOLD;
-
-        private Boolean compress = null;
-
-        private String format = Defaults.FORMAT;
-
         private String[] includes = null;
         private String[] excludes = null;
 
@@ -111,21 +96,6 @@ public class SourceFieldMapper extends MetadataFieldMapper {
             return this;
         }
 
-        public Builder compress(boolean compress) {
-            this.compress = compress;
-            return this;
-        }
-
-        public Builder compressThreshold(long compressThreshold) {
-            this.compressThreshold = compressThreshold;
-            return this;
-        }
-
-        public Builder format(String format) {
-            this.format = format;
-            return this;
-        }
-
         public Builder includes(String[] includes) {
             this.includes = includes;
             return this;
@@ -138,7 +108,7 @@ public class SourceFieldMapper extends MetadataFieldMapper {
 
         @Override
         public SourceFieldMapper build(BuilderContext context) {
-            return new SourceFieldMapper(enabled, format, compress, compressThreshold, includes, excludes, context.indexSettings());
+            return new SourceFieldMapper(enabled, includes, excludes, context.indexSettings());
         }
     }
 
@@ -154,24 +124,8 @@ public class SourceFieldMapper extends MetadataFieldMapper {
                 if (fieldName.equals("enabled")) {
                     builder.enabled(nodeBooleanValue(fieldNode));
                     iterator.remove();
-                } else if (fieldName.equals("compress") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
-                    if (fieldNode != null) {
-                        builder.compress(nodeBooleanValue(fieldNode));
-                    }
-                    iterator.remove();
-                } else if (fieldName.equals("compress_threshold") && parserContext.indexVersionCreated().before(Version.V_2_0_0_beta1)) {
-                    if (fieldNode != null) {
-                        if (fieldNode instanceof Number) {
-                            builder.compressThreshold(((Number) fieldNode).longValue());
-                            builder.compress(true);
-                        } else {
-                            builder.compressThreshold(ByteSizeValue.parseBytesSizeValue(fieldNode.toString(), "compress_threshold").bytes());
-                            builder.compress(true);
-                        }
-                    }
-                    iterator.remove();
-                } else if ("format".equals(fieldName)) {
-                    builder.format(nodeStringValue(fieldNode, null));
+                } else if ("format".equals(fieldName) && parserContext.indexVersionCreated().before(Version.V_3_0_0)) {
+                    // ignore on old indices, reject on and after 3.0
                     iterator.remove();
                 } else if (fieldName.equals("includes")) {
                     List<Object> values = (List<Object>) fieldNode;
@@ -242,30 +196,18 @@ public class SourceFieldMapper extends MetadataFieldMapper {
     /** indicates whether the source will always exist and be complete, for use by features like the update API */
     private final boolean complete;
 
-    private Boolean compress;
-    private long compressThreshold;
-
     private final String[] includes;
     private final String[] excludes;
 
-    private String format;
-
-    private XContentType formatContentType;
-
     private SourceFieldMapper(Settings indexSettings) {
-        this(Defaults.ENABLED, Defaults.FORMAT, null, -1, null, null, indexSettings);
+        this(Defaults.ENABLED, null, null, indexSettings);
     }
 
-    private SourceFieldMapper(boolean enabled, String format, Boolean compress, long compressThreshold,
-                                String[] includes, String[] excludes, Settings indexSettings) {
+    private SourceFieldMapper(boolean enabled, String[] includes, String[] excludes, Settings indexSettings) {
         super(NAME, Defaults.FIELD_TYPE.clone(), Defaults.FIELD_TYPE, indexSettings); // Only stored.
         this.enabled = enabled;
-        this.compress = compress;
-        this.compressThreshold = compressThreshold;
         this.includes = includes;
         this.excludes = excludes;
-        this.format = format;
-        this.formatContentType = format == null ? null : XContentType.fromRestContentType(format);
         this.complete = enabled && includes == null && excludes == null;
     }
 
@@ -321,71 +263,11 @@ public class SourceFieldMapper extends MetadataFieldMapper {
             Tuple<XContentType, Map<String, Object>> mapTuple = XContentHelper.convertToMap(source, true);
             Map<String, Object> filteredSource = XContentMapValues.filter(mapTuple.v2(), includes, excludes);
             BytesStreamOutput bStream = new BytesStreamOutput();
-            StreamOutput streamOutput = bStream;
-            if (compress != null && compress && (compressThreshold == -1 || source.length() > compressThreshold)) {
-                streamOutput = CompressorFactory.defaultCompressor().streamOutput(bStream);
-            }
-            XContentType contentType = formatContentType;
-            if (contentType == null) {
-                contentType = mapTuple.v1();
-            }
-            XContentBuilder builder = XContentFactory.contentBuilder(contentType, streamOutput).map(filteredSource);
+            XContentType contentType = mapTuple.v1();
+            XContentBuilder builder = XContentFactory.contentBuilder(contentType, bStream).map(filteredSource);
             builder.close();
 
             source = bStream.bytes();
-        } else if (compress != null && compress && !CompressorFactory.isCompressed(source)) {
-            if (compressThreshold == -1 || source.length() > compressThreshold) {
-                BytesStreamOutput bStream = new BytesStreamOutput();
-                XContentType contentType = XContentFactory.xContentType(source);
-                if (formatContentType != null && formatContentType != contentType) {
-                    XContentBuilder builder = XContentFactory.contentBuilder(formatContentType, CompressorFactory.defaultCompressor().streamOutput(bStream));
-                    builder.copyCurrentStructure(XContentFactory.xContent(contentType).createParser(source));
-                    builder.close();
-                } else {
-                    StreamOutput streamOutput = CompressorFactory.defaultCompressor().streamOutput(bStream);
-                    source.writeTo(streamOutput);
-                    streamOutput.close();
-                }
-                source = bStream.bytes();
-                // update the data in the context, so it can be compressed and stored compressed outside...
-                context.source(source);
-            }
-        } else if (formatContentType != null) {
-            // see if we need to convert the content type
-            Compressor compressor = CompressorFactory.compressor(source);
-            if (compressor != null) {
-                InputStream compressedStreamInput = compressor.streamInput(source.streamInput());
-                if (compressedStreamInput.markSupported() == false) {
-                    compressedStreamInput = new BufferedInputStream(compressedStreamInput);
-                }
-                XContentType contentType = XContentFactory.xContentType(compressedStreamInput);
-                if (contentType != formatContentType) {
-                    // we need to reread and store back, compressed....
-                    BytesStreamOutput bStream = new BytesStreamOutput();
-                    StreamOutput streamOutput = CompressorFactory.defaultCompressor().streamOutput(bStream);
-                    XContentBuilder builder = XContentFactory.contentBuilder(formatContentType, streamOutput);
-                    builder.copyCurrentStructure(XContentFactory.xContent(contentType).createParser(compressedStreamInput));
-                    builder.close();
-                    source = bStream.bytes();
-                    // update the data in the context, so we store it in the translog in this format
-                    context.source(source);
-                } else {
-                    compressedStreamInput.close();
-                }
-            } else {
-                XContentType contentType = XContentFactory.xContentType(source);
-                if (contentType != formatContentType) {
-                    // we need to reread and store back
-                    // we need to reread and store back, compressed....
-                    BytesStreamOutput bStream = new BytesStreamOutput();
-                    XContentBuilder builder = XContentFactory.contentBuilder(formatContentType, bStream);
-                    builder.copyCurrentStructure(XContentFactory.xContent(contentType).createParser(source));
-                    builder.close();
-                    source = bStream.bytes();
-                    // update the data in the context, so we store it in the translog in this format
-                    context.source(source);
-                }
-            }
         }
         if (!source.hasArray()) {
             source = source.toBytesArray();
@@ -403,26 +285,13 @@ public class SourceFieldMapper extends MetadataFieldMapper {
         boolean includeDefaults = params.paramAsBoolean("include_defaults", false);
 
         // all are defaults, no need to write it at all
-        if (!includeDefaults && enabled == Defaults.ENABLED && compress == null && compressThreshold == -1 && includes == null && excludes == null) {
+        if (!includeDefaults && enabled == Defaults.ENABLED && includes == null && excludes == null) {
             return builder;
         }
         builder.startObject(contentType());
         if (includeDefaults || enabled != Defaults.ENABLED) {
             builder.field("enabled", enabled);
         }
-        if (includeDefaults || !Objects.equals(format, Defaults.FORMAT)) {
-            builder.field("format", format);
-        }
-        if (compress != null) {
-            builder.field("compress", compress);
-        } else if (includeDefaults) {
-            builder.field("compress", false);
-        }
-        if (compressThreshold != -1) {
-            builder.field("compress_threshold", new ByteSizeValue(compressThreshold).toString());
-        } else if (includeDefaults) {
-            builder.field("compress_threshold", -1);
-        }
 
         if (includes != null) {
             builder.field("includes", includes);
@@ -453,13 +322,6 @@ public class SourceFieldMapper extends MetadataFieldMapper {
             if (Arrays.equals(excludes(), sourceMergeWith.excludes()) == false) {
                 mergeResult.addConflict("Cannot update excludes setting for [_source]");
             }
-        } else {
-            if (sourceMergeWith.compress != null) {
-                this.compress = sourceMergeWith.compress;
-            }
-            if (sourceMergeWith.compressThreshold != -1) {
-                this.compressThreshold = sourceMergeWith.compressThreshold;
-            }
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
deleted file mode 100644
index 70d0bb9..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
+++ /dev/null
@@ -1,234 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
-
-/**
- * Constructs a filter that have only null values or no value in the original field.
- */
-public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilder> {
-
-    public static final String NAME = "missing";
-
-    public static final boolean DEFAULT_NULL_VALUE = false;
-
-    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
-
-    private final String fieldPattern;
-
-    private final boolean nullValue;
-
-    private final boolean existence;
-
-    static final MissingQueryBuilder PROTOTYPE = new MissingQueryBuilder("field", DEFAULT_NULL_VALUE, DEFAULT_EXISTENCE_VALUE);
-
-    /**
-     * Constructs a filter that returns documents with only null values or no value in the original field.
-     * @param fieldPattern the field to query
-     * @param nullValue should the missing filter automatically include fields with null value configured in the
-     * mappings. Defaults to <tt>false</tt>.
-     * @param existence should the missing filter include documents where the field doesn't exist in the docs.
-     * Defaults to <tt>true</tt>.
-     * @throws IllegalArgumentException when both <tt>existence</tt> and <tt>nullValue</tt> are set to false
-     */
-    public MissingQueryBuilder(String fieldPattern, boolean nullValue, boolean existence) {
-        if (Strings.isEmpty(fieldPattern)) {
-            throw new IllegalArgumentException("missing query must be provided with a [field]");
-        }
-        if (nullValue == false && existence == false) {
-            throw new IllegalArgumentException("missing query must have either 'existence', or 'null_value', or both set to true");
-        }
-        this.fieldPattern = fieldPattern;
-        this.nullValue = nullValue;
-        this.existence = existence;
-    }
-
-    public MissingQueryBuilder(String fieldPattern) {
-        this(fieldPattern, DEFAULT_NULL_VALUE, DEFAULT_EXISTENCE_VALUE);
-    }
-
-    public String fieldPattern() {
-        return this.fieldPattern;
-    }
-
-    /**
-     * Returns true if the missing filter will include documents where the field contains a null value, otherwise
-     * these documents will not be included.
-     */
-    public boolean nullValue() {
-        return this.nullValue;
-    }
-
-    /**
-     * Returns true if the missing filter will include documents where the field has no values, otherwise
-     * these documents will not be included.
-     */
-    public boolean existence() {
-        return this.existence;
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field(MissingQueryParser.FIELD_FIELD.getPreferredName(), fieldPattern);
-        builder.field(MissingQueryParser.NULL_VALUE_FIELD.getPreferredName(), nullValue);
-        builder.field(MissingQueryParser.EXISTENCE_FIELD.getPreferredName(), existence);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, fieldPattern, existence, nullValue);
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern, boolean existence, boolean nullValue) {
-        if (!existence && !nullValue) {
-            throw new QueryShardException(context, "missing must have either existence, or null_value, or both set to true");
-        }
-
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType) context.getMapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            if (existence) {
-                // if we ask for existence of fields, and we found none, then we should match on all
-                return Queries.newMatchAllQuery();
-            }
-            return null;
-        }
-
-        Query existenceFilter = null;
-        Query nullFilter = null;
-
-        if (existence) {
-            BooleanQuery.Builder boolFilter = new BooleanQuery.Builder();
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                Query filter = null;
-                if (fieldNamesFieldType.isEnabled()) {
-                    final String f;
-                    if (fieldType != null) {
-                        f = fieldType.names().indexName();
-                    } else {
-                        f = field;
-                    }
-                    filter = fieldNamesFieldType.termQuery(f, context);
-                }
-                // if _field_names are not indexed, we need to go the slow way
-                if (filter == null && fieldType != null) {
-                    filter = fieldType.rangeQuery(null, null, true, true);
-                }
-                if (filter == null) {
-                    filter = new TermRangeQuery(field, null, null, true, true);
-                }
-                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
-            }
-
-            existenceFilter = boolFilter.build();
-            existenceFilter = Queries.not(existenceFilter);;
-        }
-
-        if (nullValue) {
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                if (fieldType != null) {
-                    nullFilter = fieldType.nullValueQuery();
-                }
-            }
-        }
-
-        Query filter;
-        if (nullFilter != null) {
-            if (existenceFilter != null) {
-                filter = new BooleanQuery.Builder()
-                        .add(existenceFilter, BooleanClause.Occur.SHOULD)
-                        .add(nullFilter, BooleanClause.Occur.SHOULD)
-                        .build();
-            } else {
-                filter = nullFilter;
-            }
-        } else {
-            filter = existenceFilter;
-        }
-
-        if (filter == null) {
-            return null;
-        }
-
-        return new ConstantScoreQuery(filter);
-    }
-
-    @Override
-    protected MissingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new MissingQueryBuilder(in.readString(), in.readBoolean(), in.readBoolean());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldPattern);
-        out.writeBoolean(nullValue);
-        out.writeBoolean(existence);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldPattern, nullValue, existence);
-    }
-
-    @Override
-    protected boolean doEquals(MissingQueryBuilder other) {
-        return Objects.equals(fieldPattern, other.fieldPattern) &&
-                Objects.equals(nullValue, other.nullValue) &&
-                Objects.equals(existence, other.existence);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
deleted file mode 100644
index 467971b..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.xcontent.XContentParser;
-
-import java.io.IOException;
-
-/**
- * Parser for missing query
- */
-public class MissingQueryParser implements QueryParser<MissingQueryBuilder> {
-
-    public static final ParseField FIELD_FIELD = new ParseField("field");
-    public static final ParseField NULL_VALUE_FIELD = new ParseField("null_value");
-    public static final ParseField EXISTENCE_FIELD = new ParseField("existence");
-
-    @Override
-    public String[] names() {
-        return new String[]{MissingQueryBuilder.NAME};
-    }
-
-    @Override
-    public MissingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
-        XContentParser parser = parseContext.parser();
-
-        String fieldPattern = null;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        boolean nullValue = MissingQueryBuilder.DEFAULT_NULL_VALUE;
-        boolean existence = MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE;
-
-        XContentParser.Token token;
-        String currentFieldName = null;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if (parseContext.parseFieldMatcher().match(currentFieldName, FIELD_FIELD)) {
-                    fieldPattern = parser.text();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, NULL_VALUE_FIELD)) {
-                    nullValue = parser.booleanValue();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, EXISTENCE_FIELD)) {
-                    existence = parser.booleanValue();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, AbstractQueryBuilder.NAME_FIELD)) {
-                    queryName = parser.text();
-                } else if (parseContext.parseFieldMatcher().match(currentFieldName, AbstractQueryBuilder.BOOST_FIELD)) {
-                    boost = parser.floatValue();
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "[" + MissingQueryBuilder.NAME + "] query does not support [" + currentFieldName + "]");
-                }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(), "[" + MissingQueryBuilder.NAME + "] unknown token [" + token + "] after [" + currentFieldName + "]");
-            }
-        }
-
-        if (fieldPattern == null) {
-            throw new ParsingException(parser.getTokenLocation(), "missing must be provided with a [field]");
-        }
-        return new MissingQueryBuilder(fieldPattern, nullValue, existence)
-                .boost(boost)
-                .queryName(queryName);
-    }
-
-    @Override
-    public MissingQueryBuilder getBuilderPrototype() {
-        return MissingQueryBuilder.PROTOTYPE;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
index 45f97d6..3fb0967 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
@@ -810,27 +810,6 @@ public abstract class QueryBuilders {
         return new ExistsQueryBuilder(name);
     }
 
-    /**
-     * A filter to filter only documents where a field does not exists in them.
-     * @param name the field to query
-     */
-    public static MissingQueryBuilder missingQuery(String name) {
-        return missingQuery(name, MissingQueryBuilder.DEFAULT_NULL_VALUE, MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE);
-    }
-
-    /**
-     * A filter to filter only documents where a field does not exists in them.
-     * @param name the field to query
-     * @param nullValue should the missing filter automatically include fields with null value configured in the
-     * mappings. Defaults to <tt>false</tt>.
-     * @param existence should the missing filter include documents where the field doesn't exist in the docs.
-     * Defaults to <tt>true</tt>.
-     * @throws IllegalArgumentException when both <tt>existence</tt> and <tt>nullValue</tt> are set to false
-     */
-    public static MissingQueryBuilder missingQuery(String name, boolean nullValue, boolean existence) {
-        return new MissingQueryBuilder(name, nullValue, existence);
-    }
-
     private QueryBuilders() {
 
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index 3c72adf..7f64eb3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -18,25 +18,15 @@
  */
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.HasContextAndHeaders;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.script.*;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static org.elasticsearch.common.Strings.hasLength;
-
 /**
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index d7e4294..c0bf924 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -506,9 +506,6 @@ public class IndexShard extends AbstractIndexShardComponent {
     }
 
     public Engine.Delete prepareDeleteOnReplica(String type, String id, long version, VersionType versionType) {
-        if (shardRouting.primary() && shardRouting.isRelocationTarget() == false) {
-            throw new IllegalIndexShardStateException(shardId, state, "shard is not a replica");
-        }
         final DocumentMapper documentMapper = docMapper(type).getDocumentMapper();
         return prepareDelete(type, id, documentMapper.uidMapper().term(Uid.createUid(type, id)), version, versionType, Engine.Operation.Origin.REPLICA);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/store/IndexStoreConfig.java b/core/src/main/java/org/elasticsearch/index/store/IndexStoreConfig.java
index ed56187..1bd023a 100644
--- a/core/src/main/java/org/elasticsearch/index/store/IndexStoreConfig.java
+++ b/core/src/main/java/org/elasticsearch/index/store/IndexStoreConfig.java
@@ -21,36 +21,37 @@ package org.elasticsearch.index.store;
 import org.apache.lucene.store.StoreRateLimiting;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 /**
  * IndexStoreConfig encapsulates node / cluster level configuration for index level {@link IndexStore} instances.
  * For instance it maintains the node level rate limiter configuration: updates to the cluster that disable or enable
- * <tt>indices.store.throttle.type</tt> or <tt>indices.store.throttle.max_bytes_per_sec</tt> are reflected immediately
+ * {@value #INDICES_STORE_THROTTLE_TYPE} or {@value #INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC} are reflected immediately
  * on all referencing {@link IndexStore} instances
  */
-public class IndexStoreConfig{
+public class IndexStoreConfig implements NodeSettingsService.Listener {
 
     /**
      * Configures the node / cluster level throttle type. See {@link StoreRateLimiting.Type}.
      */
-    public static final Setting<StoreRateLimiting.Type> INDICES_STORE_THROTTLE_TYPE_SETTING = new Setting<>("indices.store.throttle.type", StoreRateLimiting.Type.NONE.name(),StoreRateLimiting.Type::fromString, true, Setting.Scope.CLUSTER);
+    public static final String INDICES_STORE_THROTTLE_TYPE = "indices.store.throttle.type";
     /**
      * Configures the node / cluster level throttle intensity. The default is <tt>10240 MB</tt>
      */
-    public static final Setting<ByteSizeValue> INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING = Setting.byteSizeSetting("indices.store.throttle.max_bytes_per_sec", new ByteSizeValue(0), true, Setting.Scope.CLUSTER);
-    private volatile StoreRateLimiting.Type rateLimitingType;
+    public static final String INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC = "indices.store.throttle.max_bytes_per_sec";
+    private volatile String rateLimitingType;
     private volatile ByteSizeValue rateLimitingThrottle;
     private final StoreRateLimiting rateLimiting = new StoreRateLimiting();
     private final ESLogger logger;
     public IndexStoreConfig(Settings settings) {
         logger = Loggers.getLogger(IndexStoreConfig.class, settings);
         // we don't limit by default (we default to CMS's auto throttle instead):
-        this.rateLimitingType = INDICES_STORE_THROTTLE_TYPE_SETTING.get(settings);
+        this.rateLimitingType = settings.get("indices.store.throttle.type", StoreRateLimiting.Type.NONE.name());
         rateLimiting.setType(rateLimitingType);
-        this.rateLimitingThrottle = INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING.get(settings);
+        this.rateLimitingThrottle = settings.getAsBytesSize("indices.store.throttle.max_bytes_per_sec", new ByteSizeValue(0));
         rateLimiting.setMaxRate(rateLimitingThrottle);
         logger.debug("using indices.store.throttle.type [{}], with index.store.throttle.max_bytes_per_sec [{}]", rateLimitingType, rateLimitingThrottle);
     }
@@ -62,12 +63,22 @@ public class IndexStoreConfig{
         return rateLimiting;
     }
 
-    public void setRateLimitingType(StoreRateLimiting.Type rateLimitingType) {
-        this.rateLimitingType = rateLimitingType;
-        rateLimiting.setType(rateLimitingType);
-    }
+    @Override
+    public void onRefreshSettings(Settings settings) {
+        String rateLimitingType = settings.get(INDICES_STORE_THROTTLE_TYPE, this.rateLimitingType);
+        // try and parse the type
+        StoreRateLimiting.Type.fromString(rateLimitingType);
+        if (!rateLimitingType.equals(this.rateLimitingType)) {
+            logger.info("updating indices.store.throttle.type from [{}] to [{}]", this.rateLimitingType, rateLimitingType);
+            this.rateLimitingType = rateLimitingType;
+            this.rateLimiting.setType(rateLimitingType);
+        }
 
-    public void setRateLimitingThrottle(ByteSizeValue rateLimitingThrottle) {
-        this.rateLimitingThrottle = rateLimitingThrottle;
+        ByteSizeValue rateLimitingThrottle = settings.getAsBytesSize(INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC, this.rateLimitingThrottle);
+        if (!rateLimitingThrottle.equals(this.rateLimitingThrottle)) {
+            logger.info("updating indices.store.throttle.max_bytes_per_sec from [{}] to [{}], note, type is [{}]", this.rateLimitingThrottle, rateLimitingThrottle, this.rateLimitingType);
+            this.rateLimitingThrottle = rateLimitingThrottle;
+            this.rateLimiting.setMaxRate(rateLimitingThrottle);
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java b/core/src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java
index 6026468..a2eb0bf 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/BufferingTranslogWriter.java
@@ -48,22 +48,27 @@ public final class BufferingTranslogWriter extends TranslogWriter {
     public Translog.Location add(BytesReference data) throws IOException {
         try (ReleasableLock lock = writeLock.acquire()) {
             ensureOpen();
-            operationCounter++;
             final long offset = totalOffset;
             if (data.length() >= buffer.length) {
                 flush();
                 // we use the channel to write, since on windows, writing to the RAF might not be reflected
                 // when reading through the channel
-                data.writeTo(channel);
+                try {
+                    data.writeTo(channel);
+                } catch (Throwable ex) {
+                    closeWithTragicEvent(ex);
+                    throw ex;
+                }
                 writtenOffset += data.length();
                 totalOffset += data.length();
-                return new Translog.Location(generation, offset, data.length());
-            }
-            if (data.length() > buffer.length - bufferCount) {
-                flush();
+            } else {
+                if (data.length() > buffer.length - bufferCount) {
+                    flush();
+                }
+                data.writeTo(bufferOs);
+                totalOffset += data.length();
             }
-            data.writeTo(bufferOs);
-            totalOffset += data.length();
+            operationCounter++;
             return new Translog.Location(generation, offset, data.length());
         }
     }
@@ -71,10 +76,17 @@ public final class BufferingTranslogWriter extends TranslogWriter {
     protected final void flush() throws IOException {
         assert writeLock.isHeldByCurrentThread();
         if (bufferCount > 0) {
+            ensureOpen();
             // we use the channel to write, since on windows, writing to the RAF might not be reflected
             // when reading through the channel
-            Channels.writeToChannel(buffer, 0, bufferCount, channel);
-            writtenOffset += bufferCount;
+            final int bufferSize = bufferCount;
+            try {
+                Channels.writeToChannel(buffer, 0, bufferSize, channel);
+            } catch (Throwable ex) {
+                closeWithTragicEvent(ex);
+                throw ex;
+            }
+            writtenOffset += bufferSize;
             bufferCount = 0;
         }
     }
@@ -102,20 +114,28 @@ public final class BufferingTranslogWriter extends TranslogWriter {
     }
 
     @Override
-    public void sync() throws IOException {
-        if (!syncNeeded()) {
-            return;
-        }
-        synchronized (this) {
+    public synchronized void sync() throws IOException {
+        if (syncNeeded()) {
+            ensureOpen(); // this call gives a better exception that the incRef if we are closed by a tragic event
             channelReference.incRef();
             try {
+                final long offsetToSync;
+                final int opsCounter;
                 try (ReleasableLock lock = writeLock.acquire()) {
                     flush();
-                    lastSyncedOffset = totalOffset;
+                    offsetToSync = totalOffset;
+                    opsCounter = operationCounter;
                 }
                 // we can do this outside of the write lock but we have to protect from
                 // concurrent syncs
-                checkpoint(lastSyncedOffset, operationCounter, channelReference);
+                ensureOpen(); // just for kicks - the checkpoint happens or not either way
+                try {
+                    checkpoint(offsetToSync, opsCounter, channelReference);
+                } catch (Throwable ex) {
+                    closeWithTragicEvent(ex);
+                    throw ex;
+                }
+                lastSyncedOffset = offsetToSync;
             } finally {
                 channelReference.decRef();
             }
diff --git a/core/src/main/java/org/elasticsearch/index/translog/Translog.java b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
index 35dd895..4016695 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/Translog.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
@@ -115,7 +115,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     private final Path location;
     private TranslogWriter current;
     private volatile ImmutableTranslogReader currentCommittingTranslog;
-    private long lastCommittedTranslogFileGeneration = -1; // -1 is safe as it will not cause an translog deletion.
+    private volatile long lastCommittedTranslogFileGeneration = -1; // -1 is safe as it will not cause an translog deletion.
     private final AtomicBoolean closed = new AtomicBoolean();
     private final TranslogConfig config;
     private final String translogUUID;
@@ -279,7 +279,8 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         }
     }
 
-    boolean isOpen() {
+    /** Returns {@code true} if this {@code Translog} is still open. */
+    public boolean isOpen() {
         return closed.get() == false;
     }
 
@@ -288,10 +289,14 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         if (closed.compareAndSet(false, true)) {
             try (ReleasableLock lock = writeLock.acquire()) {
                 try {
-                    IOUtils.close(current, currentCommittingTranslog);
+                    current.sync();
                 } finally {
-                    IOUtils.close(recoveredTranslogs);
-                    recoveredTranslogs.clear();
+                    try {
+                        IOUtils.close(current, currentCommittingTranslog);
+                    } finally {
+                        IOUtils.close(recoveredTranslogs);
+                        recoveredTranslogs.clear();
+                    }
                 }
             } finally {
                 FutureUtils.cancel(syncScheduler);
@@ -354,7 +359,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     TranslogWriter createWriter(long fileGeneration) throws IOException {
         TranslogWriter newFile;
         try {
-            newFile = TranslogWriter.create(config.getType(), shardId, translogUUID, fileGeneration, location.resolve(getFilename(fileGeneration)), new OnCloseRunnable(), config.getBufferSize());
+            newFile = TranslogWriter.create(config.getType(), shardId, translogUUID, fileGeneration, location.resolve(getFilename(fileGeneration)), new OnCloseRunnable(), config.getBufferSize(), getChannelFactory());
         } catch (IOException e) {
             throw new TranslogException(shardId, "failed to create new translog file", e);
         }
@@ -393,7 +398,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
      * @see Index
      * @see org.elasticsearch.index.translog.Translog.Delete
      */
-    public Location add(Operation operation) throws TranslogException {
+    public Location add(Operation operation) throws IOException {
         final ReleasableBytesStreamOutput out = new ReleasableBytesStreamOutput(bigArrays);
         try {
             final BufferedChecksumStreamOutput checksumStreamOutput = new BufferedChecksumStreamOutput(out);
@@ -415,7 +420,14 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
                 assert current.assertBytesAtLocation(location, bytes);
                 return location;
             }
-        } catch (AlreadyClosedException ex) {
+        } catch (AlreadyClosedException | IOException ex) {
+            if (current.getTragicException() != null) {
+                try {
+                    close();
+                } catch (Exception inner) {
+                    ex.addSuppressed(inner);
+                }
+            }
             throw ex;
         } catch (Throwable e) {
             throw new TranslogException(shardId, "Failed to write operation [" + operation + "]", e);
@@ -429,6 +441,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
      * Snapshots are fixed in time and will not be updated with future operations.
      */
     public Snapshot newSnapshot() {
+        ensureOpen();
         try (ReleasableLock lock = readLock.acquire()) {
             ArrayList<TranslogReader> toOpen = new ArrayList<>();
             toOpen.addAll(recoveredTranslogs);
@@ -493,6 +506,15 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
             if (closed.get() == false) {
                 current.sync();
             }
+        } catch (AlreadyClosedException | IOException ex) {
+            if (current.getTragicException() != null) {
+                try {
+                    close();
+                } catch (Exception inner) {
+                    ex.addSuppressed(inner);
+                }
+            }
+            throw ex;
         }
     }
 
@@ -520,6 +542,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     public boolean ensureSynced(Location location) throws IOException {
         try (ReleasableLock lock = readLock.acquire()) {
             if (location.generation == current.generation) { // if we have a new one it's already synced
+                ensureOpen();
                 return current.syncUpTo(location.translogLocation + location.size);
             }
         }
@@ -548,31 +571,29 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
     private final class OnCloseRunnable implements Callback<ChannelReference> {
         @Override
         public void handle(ChannelReference channelReference) {
-            try (ReleasableLock lock = writeLock.acquire()) {
-                if (isReferencedGeneration(channelReference.getGeneration()) == false) {
-                    Path translogPath = channelReference.getPath();
-                    assert channelReference.getPath().getParent().equals(location) : "translog files must be in the location folder: " + location + " but was: " + translogPath;
-                    // if the given translogPath is not the current we can safely delete the file since all references are released
-                    logger.trace("delete translog file - not referenced and not current anymore {}", translogPath);
-                    IOUtils.deleteFilesIgnoringExceptions(translogPath);
-                    IOUtils.deleteFilesIgnoringExceptions(translogPath.resolveSibling(getCommitCheckpointFileName(channelReference.getGeneration())));
+            if (isReferencedGeneration(channelReference.getGeneration()) == false) {
+                Path translogPath = channelReference.getPath();
+                assert channelReference.getPath().getParent().equals(location) : "translog files must be in the location folder: " + location + " but was: " + translogPath;
+                // if the given translogPath is not the current we can safely delete the file since all references are released
+                logger.trace("delete translog file - not referenced and not current anymore {}", translogPath);
+                IOUtils.deleteFilesIgnoringExceptions(translogPath);
+                IOUtils.deleteFilesIgnoringExceptions(translogPath.resolveSibling(getCommitCheckpointFileName(channelReference.getGeneration())));
 
-                }
-                try (DirectoryStream<Path> stream = Files.newDirectoryStream(location)) {
-                    for (Path path : stream) {
-                        Matcher matcher = PARSE_STRICT_ID_PATTERN.matcher(path.getFileName().toString());
-                        if (matcher.matches()) {
-                            long generation = Long.parseLong(matcher.group(1));
-                            if (isReferencedGeneration(generation) == false) {
-                                logger.trace("delete translog file - not referenced and not current anymore {}", path);
-                                IOUtils.deleteFilesIgnoringExceptions(path);
-                                IOUtils.deleteFilesIgnoringExceptions(path.resolveSibling(getCommitCheckpointFileName(channelReference.getGeneration())));
-                            }
+            }
+            try (DirectoryStream<Path> stream = Files.newDirectoryStream(location)) {
+                for (Path path : stream) {
+                    Matcher matcher = PARSE_STRICT_ID_PATTERN.matcher(path.getFileName().toString());
+                    if (matcher.matches()) {
+                        long generation = Long.parseLong(matcher.group(1));
+                        if (isReferencedGeneration(generation) == false) {
+                            logger.trace("delete translog file - not referenced and not current anymore {}", path);
+                            IOUtils.deleteFilesIgnoringExceptions(path);
+                            IOUtils.deleteFilesIgnoringExceptions(path.resolveSibling(getCommitCheckpointFileName(channelReference.getGeneration())));
                         }
                     }
-                } catch (IOException e) {
-                    logger.warn("failed to delete unreferenced translog files", e);
                 }
+            } catch (IOException e) {
+                logger.warn("failed to delete unreferenced translog files", e);
             }
         }
     }
@@ -1294,6 +1315,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
                 throw new IllegalStateException("already committing a translog with generation: " + currentCommittingTranslog.getGeneration());
             }
             final TranslogWriter oldCurrent = current;
+            oldCurrent.ensureOpen();
             oldCurrent.sync();
             currentCommittingTranslog = current.immutableReader();
             Path checkpoint = location.resolve(CHECKPOINT_FILE_NAME);
@@ -1389,7 +1411,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
 
     private void ensureOpen() {
         if (closed.get()) {
-            throw new AlreadyClosedException("translog is already closed");
+            throw new AlreadyClosedException("translog is already closed", current.getTragicException());
         }
     }
 
@@ -1400,4 +1422,15 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         return outstandingViews.size();
     }
 
+    TranslogWriter.ChannelFactory getChannelFactory() {
+        return TranslogWriter.ChannelFactory.DEFAULT;
+    }
+
+    /** If this {@code Translog} was closed as a side-effect of a tragic exception,
+     *  e.g. disk full while flushing a new segment, this returns the root cause exception.
+     *  Otherwise (no tragic exception has occurred) it returns null. */
+    public Throwable getTragicException() {
+        return current.getTragicException();
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java
index 590bc31..d7077fd 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogReader.java
@@ -140,16 +140,16 @@ public abstract class TranslogReader implements Closeable, Comparable<TranslogRe
     @Override
     public void close() throws IOException {
         if (closed.compareAndSet(false, true)) {
-            doClose();
+            channelReference.decRef();
         }
     }
 
-    protected void doClose() throws IOException {
-        channelReference.decRef();
+    protected final boolean isClosed() {
+        return closed.get();
     }
 
     protected void ensureOpen() {
-        if (closed.get()) {
+        if (isClosed()) {
             throw new AlreadyClosedException("translog [" + getGeneration() + "] is already closed");
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java b/core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java
index 045550c..9870bdd 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.translog;
 
 import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.OutputStreamDataOutput;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
@@ -54,6 +55,9 @@ public class TranslogWriter extends TranslogReader {
     protected volatile int operationCounter;
     /* the offset in bytes written to the file */
     protected volatile long writtenOffset;
+    /* if we hit an exception that we can't recover from we assign it to this var and ship it with every AlreadyClosedException we throw */
+    private volatile Throwable tragedy;
+
 
     public TranslogWriter(ShardId shardId, long generation, ChannelReference channelReference) throws IOException {
         super(generation, channelReference, channelReference.getChannel().position());
@@ -65,10 +69,10 @@ public class TranslogWriter extends TranslogReader {
         this.lastSyncedOffset = channelReference.getChannel().position();;
     }
 
-    public static TranslogWriter create(Type type, ShardId shardId, String translogUUID, long fileGeneration, Path file, Callback<ChannelReference> onClose, int bufferSize) throws IOException {
+    public static TranslogWriter create(Type type, ShardId shardId, String translogUUID, long fileGeneration, Path file, Callback<ChannelReference> onClose, int bufferSize, ChannelFactory channelFactory) throws IOException {
         final BytesRef ref = new BytesRef(translogUUID);
         final int headerLength = CodecUtil.headerLength(TRANSLOG_CODEC) + ref.length + RamUsageEstimator.NUM_BYTES_INT;
-        final FileChannel channel = FileChannel.open(file, StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE_NEW);
+        final FileChannel channel = channelFactory.open(file);
         try {
             // This OutputStreamDataOutput is intentionally not closed because
             // closing it will close the FileChannel
@@ -90,6 +94,12 @@ public class TranslogWriter extends TranslogReader {
             throw throwable;
         }
     }
+    /** If this {@code TranslogWriter} was closed as a side-effect of a tragic exception,
+     *  e.g. disk full while flushing a new segment, this returns the root cause exception.
+     *  Otherwise (no tragic exception has occurred) it returns null. */
+    public Throwable getTragicException() {
+        return tragedy;
+    }
 
     public enum Type {
 
@@ -118,6 +128,16 @@ public class TranslogWriter extends TranslogReader {
         }
     }
 
+    protected final void closeWithTragicEvent(Throwable throwable) throws IOException {
+        try (ReleasableLock lock = writeLock.acquire()) {
+            if (tragedy == null) {
+                tragedy = throwable;
+            } else {
+                tragedy.addSuppressed(throwable);
+            }
+            close();
+        }
+    }
 
     /**
      * add the given bytes to the translog and return the location they were written at
@@ -127,9 +147,14 @@ public class TranslogWriter extends TranslogReader {
         try (ReleasableLock lock = writeLock.acquire()) {
             ensureOpen();
             position = writtenOffset;
-            data.writeTo(channel);
+            try {
+                data.writeTo(channel);
+            } catch (Throwable e) {
+                closeWithTragicEvent(e);
+                throw e;
+            }
             writtenOffset = writtenOffset + data.length();
-            operationCounter = operationCounter + 1;
+            operationCounter++;;
         }
         return new Translog.Location(generation, position, data.length());
     }
@@ -143,12 +168,13 @@ public class TranslogWriter extends TranslogReader {
     /**
      * write all buffered ops to disk and fsync file
      */
-    public void sync() throws IOException {
+    public synchronized void sync() throws IOException { // synchronized to ensure only one sync happens a time
         // check if we really need to sync here...
         if (syncNeeded()) {
             try (ReleasableLock lock = writeLock.acquire()) {
+                ensureOpen();
+                checkpoint(writtenOffset, operationCounter, channelReference);
                 lastSyncedOffset = writtenOffset;
-                checkpoint(lastSyncedOffset, operationCounter, channelReference);
             }
         }
     }
@@ -263,15 +289,6 @@ public class TranslogWriter extends TranslogReader {
     }
 
     @Override
-    protected final void doClose() throws IOException {
-        try (ReleasableLock lock = writeLock.acquire()) {
-            sync();
-        } finally {
-            super.doClose();
-        }
-    }
-
-    @Override
     protected void readBytes(ByteBuffer buffer, long position) throws IOException {
         try (ReleasableLock lock = readLock.acquire()) {
             Channels.readFromFileChannelWithEofException(channel, position, buffer);
@@ -288,4 +305,20 @@ public class TranslogWriter extends TranslogReader {
         Checkpoint checkpoint = new Checkpoint(syncPosition, numOperations, generation);
         Checkpoint.write(checkpointFile, checkpoint, options);
     }
+
+    static class ChannelFactory {
+
+        static final ChannelFactory DEFAULT = new ChannelFactory();
+
+        // only for testing until we have a disk-full FileSystemt
+        public FileChannel open(Path file) throws IOException {
+            return FileChannel.open(file, StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE_NEW);
+        }
+    }
+
+    protected final void ensureOpen() {
+        if (isClosed()) {
+            throw new AlreadyClosedException("translog [" + getGeneration() + "] is already closed", tragedy);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
index cdd7f05..6878002 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesModule.java
@@ -120,7 +120,6 @@ public class IndicesModule extends AbstractModule {
         registerQueryParser(GeohashCellQuery.Parser.class);
         registerQueryParser(GeoPolygonQueryParser.class);
         registerQueryParser(ExistsQueryParser.class);
-        registerQueryParser(MissingQueryParser.class);
         registerQueryParser(MatchNoneQueryParser.class);
 
         if (ShapesAvailability.JTS_AVAILABLE) {
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesService.java b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
index d8c142f..dead72a 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesService.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
@@ -36,7 +36,6 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.FileSystemUtils;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
@@ -59,6 +58,7 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.store.IndexStoreConfig;
 import org.elasticsearch.indices.mapper.MapperRegistry;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.plugins.PluginsService;
 
 import java.io.IOException;
@@ -100,9 +100,9 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
 
     @Inject
     public IndicesService(Settings settings, PluginsService pluginsService, NodeEnvironment nodeEnv,
-                          ClusterSettings clusterSettings, AnalysisRegistry analysisRegistry,
-                          IndicesQueriesRegistry indicesQueriesRegistry, IndexNameExpressionResolver indexNameExpressionResolver,
-                          ClusterService clusterService, MapperRegistry mapperRegistry) {
+            NodeSettingsService nodeSettingsService, AnalysisRegistry analysisRegistry,
+            IndicesQueriesRegistry indicesQueriesRegistry, IndexNameExpressionResolver indexNameExpressionResolver,
+            ClusterService clusterService, MapperRegistry mapperRegistry) {
         super(settings);
         this.pluginsService = pluginsService;
         this.nodeEnv = nodeEnv;
@@ -113,9 +113,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         this.clusterService = clusterService;
         this.indexNameExpressionResolver = indexNameExpressionResolver;
         this.mapperRegistry = mapperRegistry;
-        clusterSettings.addSettingsUpdateConsumer(IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING, indexStoreConfig::setRateLimitingType);
-        clusterSettings.addSettingsUpdateConsumer(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING, indexStoreConfig::setRateLimitingThrottle);
-
+        nodeSettingsService.addListener(indexStoreConfig);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java b/core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java
index 0e1532b..33f3c12 100644
--- a/core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java
+++ b/core/src/main/java/org/elasticsearch/indices/breaker/HierarchyCircuitBreakerService.java
@@ -25,10 +25,9 @@ import org.elasticsearch.common.breaker.CircuitBreakingException;
 import org.elasticsearch.common.breaker.NoopCircuitBreaker;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.util.ArrayList;
 import java.util.List;
@@ -46,17 +45,25 @@ public class HierarchyCircuitBreakerService extends CircuitBreakerService {
 
     private final ConcurrentMap<String, CircuitBreaker> breakers = new ConcurrentHashMap();
 
-    public static final Setting<ByteSizeValue> TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING = Setting.byteSizeSetting("indices.breaker.total.limit", "70%", true, Setting.Scope.CLUSTER);
+    // Old pre-1.4.0 backwards compatible settings
+    public static final String OLD_CIRCUIT_BREAKER_MAX_BYTES_SETTING = "indices.fielddata.breaker.limit";
+    public static final String OLD_CIRCUIT_BREAKER_OVERHEAD_SETTING = "indices.fielddata.breaker.overhead";
 
-    public static final Setting<ByteSizeValue> FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING = Setting.byteSizeSetting("indices.breaker.fielddata.limit", "60%", true, Setting.Scope.CLUSTER);
-    public static final Setting<Double> FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING = Setting.doubleSetting("indices.breaker.fielddata.overhead", 1.03d, 0.0d, true, Setting.Scope.CLUSTER);
-    public static final Setting<CircuitBreaker.Type> FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING =  new Setting<>("indices.breaker.fielddata.type", "memory", CircuitBreaker.Type::parseValue, false, Setting.Scope.CLUSTER);
+    public static final String TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING = "indices.breaker.total.limit";
+    public static final String DEFAULT_TOTAL_CIRCUIT_BREAKER_LIMIT = "70%";
 
-    public static final Setting<ByteSizeValue> REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING = Setting.byteSizeSetting("indices.breaker.request.limit", "40%", true, Setting.Scope.CLUSTER);
-    public static final Setting<Double> REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING = Setting.doubleSetting("indices.breaker.request.overhead", 1.0d, 0.0d, true, Setting.Scope.CLUSTER);
-    public static final Setting<CircuitBreaker.Type> REQUEST_CIRCUIT_BREAKER_TYPE_SETTING = new Setting<>("indices.breaker.request.type", "memory", CircuitBreaker.Type::parseValue, false, Setting.Scope.CLUSTER);
+    public static final String FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING = "indices.breaker.fielddata.limit";
+    public static final String FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING = "indices.breaker.fielddata.overhead";
+    public static final String FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING = "indices.breaker.fielddata.type";
+    public static final String DEFAULT_FIELDDATA_BREAKER_LIMIT = "60%";
+    public static final double DEFAULT_FIELDDATA_OVERHEAD_CONSTANT = 1.03;
 
+    public static final String REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING = "indices.breaker.request.limit";
+    public static final String REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING = "indices.breaker.request.overhead";
+    public static final String REQUEST_CIRCUIT_BREAKER_TYPE_SETTING = "indices.breaker.request.type";
+    public static final String DEFAULT_REQUEST_BREAKER_LIMIT = "40%";
 
+    public static final String DEFAULT_BREAKER_TYPE = "memory";
 
     private volatile BreakerSettings parentSettings;
     private volatile BreakerSettings fielddataSettings;
@@ -66,21 +73,41 @@ public class HierarchyCircuitBreakerService extends CircuitBreakerService {
     private final AtomicLong parentTripCount = new AtomicLong(0);
 
     @Inject
-    public HierarchyCircuitBreakerService(Settings settings, ClusterSettings clusterSettings) {
+    public HierarchyCircuitBreakerService(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
+
+        // This uses the old InternalCircuitBreakerService.CIRCUIT_BREAKER_MAX_BYTES_SETTING
+        // setting to keep backwards compatibility with 1.3, it can be safely
+        // removed when compatibility with 1.3 is no longer needed
+        String compatibilityFielddataLimitDefault = DEFAULT_FIELDDATA_BREAKER_LIMIT;
+        ByteSizeValue compatibilityFielddataLimit = settings.getAsMemory(OLD_CIRCUIT_BREAKER_MAX_BYTES_SETTING, null);
+        if (compatibilityFielddataLimit != null) {
+            compatibilityFielddataLimitDefault = compatibilityFielddataLimit.toString();
+        }
+
+        // This uses the old InternalCircuitBreakerService.CIRCUIT_BREAKER_OVERHEAD_SETTING
+        // setting to keep backwards compatibility with 1.3, it can be safely
+        // removed when compatibility with 1.3 is no longer needed
+        double compatibilityFielddataOverheadDefault = DEFAULT_FIELDDATA_OVERHEAD_CONSTANT;
+        Double compatibilityFielddataOverhead = settings.getAsDouble(OLD_CIRCUIT_BREAKER_OVERHEAD_SETTING, null);
+        if (compatibilityFielddataOverhead != null) {
+            compatibilityFielddataOverheadDefault = compatibilityFielddataOverhead;
+        }
+
         this.fielddataSettings = new BreakerSettings(CircuitBreaker.FIELDDATA,
-                FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.get(settings).bytes(),
-                FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.get(settings),
-                FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING.get(settings)
+                settings.getAsMemory(FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, compatibilityFielddataLimitDefault).bytes(),
+                settings.getAsDouble(FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, compatibilityFielddataOverheadDefault),
+                CircuitBreaker.Type.parseValue(settings.get(FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING, DEFAULT_BREAKER_TYPE))
         );
 
         this.requestSettings = new BreakerSettings(CircuitBreaker.REQUEST,
-                REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.get(settings).bytes(),
-                REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING.get(settings),
-                REQUEST_CIRCUIT_BREAKER_TYPE_SETTING.get(settings)
+                settings.getAsMemory(REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, DEFAULT_REQUEST_BREAKER_LIMIT).bytes(),
+                settings.getAsDouble(REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING, 1.0),
+                CircuitBreaker.Type.parseValue(settings.get(REQUEST_CIRCUIT_BREAKER_TYPE_SETTING, DEFAULT_BREAKER_TYPE))
         );
 
-        this.parentSettings = new BreakerSettings(CircuitBreaker.PARENT, TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING.get(settings).bytes(), 1.0, CircuitBreaker.Type.PARENT);
+        this.parentSettings = new BreakerSettings(CircuitBreaker.PARENT,
+                settings.getAsMemory(TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING, DEFAULT_TOTAL_CIRCUIT_BREAKER_LIMIT).bytes(), 1.0, CircuitBreaker.Type.PARENT);
         if (logger.isTraceEnabled()) {
             logger.trace("parent circuit breaker with settings {}", this.parentSettings);
         }
@@ -88,38 +115,52 @@ public class HierarchyCircuitBreakerService extends CircuitBreakerService {
         registerBreaker(this.requestSettings);
         registerBreaker(this.fielddataSettings);
 
-        clusterSettings.addSettingsUpdateConsumer(TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING, this::setTotalCircuitBreakerLimit, this::validateTotalCircuitBreakerLimit);
-        clusterSettings.addSettingsUpdateConsumer(FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, this::setFieldDataBreakerLimit);
-        clusterSettings.addSettingsUpdateConsumer(REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING, this::setRequestBreakerLimit);
-    }
-    private void setRequestBreakerLimit(ByteSizeValue newRequestMax, Double newRequestOverhead) {
-        BreakerSettings newRequestSettings = new BreakerSettings(CircuitBreaker.REQUEST, newRequestMax.bytes(), newRequestOverhead,
-                HierarchyCircuitBreakerService.this.requestSettings.getType());
-        registerBreaker(newRequestSettings);
-        HierarchyCircuitBreakerService.this.requestSettings = newRequestSettings;
-        logger.info("Updated breaker settings request: {}", newRequestSettings);
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
-    private void setFieldDataBreakerLimit(ByteSizeValue newFielddataMax, Double newFielddataOverhead) {
-        long newFielddataLimitBytes = newFielddataMax == null ? HierarchyCircuitBreakerService.this.fielddataSettings.getLimit() : newFielddataMax.bytes();
-        newFielddataOverhead = newFielddataOverhead == null ? HierarchyCircuitBreakerService.this.fielddataSettings.getOverhead() : newFielddataOverhead;
-        BreakerSettings newFielddataSettings = new BreakerSettings(CircuitBreaker.FIELDDATA, newFielddataLimitBytes, newFielddataOverhead,
-                HierarchyCircuitBreakerService.this.fielddataSettings.getType());
-        registerBreaker(newFielddataSettings);
-        HierarchyCircuitBreakerService.this.fielddataSettings = newFielddataSettings;
-        logger.info("Updated breaker settings field data: {}", newFielddataSettings);
+    public class ApplySettings implements NodeSettingsService.Listener {
 
-    }
+        @Override
+        public void onRefreshSettings(Settings settings) {
 
-    private boolean validateTotalCircuitBreakerLimit(ByteSizeValue byteSizeValue) {
-        BreakerSettings newParentSettings = new BreakerSettings(CircuitBreaker.PARENT, byteSizeValue.bytes(), 1.0, CircuitBreaker.Type.PARENT);
-        validateSettings(new BreakerSettings[]{newParentSettings});
-        return true;
-    }
+            // Fielddata settings
+            ByteSizeValue newFielddataMax = settings.getAsMemory(FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, null);
+            Double newFielddataOverhead = settings.getAsDouble(FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, null);
+            if (newFielddataMax != null || newFielddataOverhead != null) {
+                long newFielddataLimitBytes = newFielddataMax == null ? HierarchyCircuitBreakerService.this.fielddataSettings.getLimit() : newFielddataMax.bytes();
+                newFielddataOverhead = newFielddataOverhead == null ? HierarchyCircuitBreakerService.this.fielddataSettings.getOverhead() : newFielddataOverhead;
+
+                BreakerSettings newFielddataSettings = new BreakerSettings(CircuitBreaker.FIELDDATA, newFielddataLimitBytes, newFielddataOverhead,
+                        HierarchyCircuitBreakerService.this.fielddataSettings.getType());
+                registerBreaker(newFielddataSettings);
+                HierarchyCircuitBreakerService.this.fielddataSettings = newFielddataSettings;
+                logger.info("Updated breaker settings fielddata: {}", newFielddataSettings);
+            }
 
-    private void setTotalCircuitBreakerLimit(ByteSizeValue byteSizeValue) {
-        BreakerSettings newParentSettings = new BreakerSettings(CircuitBreaker.PARENT, byteSizeValue.bytes(), 1.0, CircuitBreaker.Type.PARENT);
-        this.parentSettings = newParentSettings;
+            // Request settings
+            ByteSizeValue newRequestMax = settings.getAsMemory(REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, null);
+            Double newRequestOverhead = settings.getAsDouble(REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING, null);
+            if (newRequestMax != null || newRequestOverhead != null) {
+                long newRequestLimitBytes = newRequestMax == null ? HierarchyCircuitBreakerService.this.requestSettings.getLimit() : newRequestMax.bytes();
+                newRequestOverhead = newRequestOverhead == null ? HierarchyCircuitBreakerService.this.requestSettings.getOverhead() : newRequestOverhead;
+
+                BreakerSettings newRequestSettings = new BreakerSettings(CircuitBreaker.REQUEST, newRequestLimitBytes, newRequestOverhead,
+                        HierarchyCircuitBreakerService.this.requestSettings.getType());
+                registerBreaker(newRequestSettings);
+                HierarchyCircuitBreakerService.this.requestSettings = newRequestSettings;
+                logger.info("Updated breaker settings request: {}", newRequestSettings);
+            }
+
+            // Parent settings
+            long oldParentMax = HierarchyCircuitBreakerService.this.parentSettings.getLimit();
+            ByteSizeValue newParentMax = settings.getAsMemory(TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING, null);
+            if (newParentMax != null && (newParentMax.bytes() != oldParentMax)) {
+                BreakerSettings newParentSettings = new BreakerSettings(CircuitBreaker.PARENT, newParentMax.bytes(), 1.0, CircuitBreaker.Type.PARENT);
+                validateSettings(new BreakerSettings[]{newParentSettings});
+                HierarchyCircuitBreakerService.this.parentSettings = newParentSettings;
+                logger.info("Updated breaker settings parent: {}", newParentSettings);
+            }
+        }
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java
index c07ef57..6db38d5 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java
@@ -23,16 +23,16 @@ import org.apache.lucene.store.RateLimiter;
 import org.apache.lucene.store.RateLimiter.SimpleRateLimiter;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.Closeable;
+import java.util.Objects;
 import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
 
@@ -40,45 +40,38 @@ import java.util.concurrent.TimeUnit;
  */
 public class RecoverySettings extends AbstractComponent implements Closeable {
 
-    public static final Setting<ByteSizeValue> INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING = Setting.byteSizeSetting("indices.recovery.file_chunk_size", new ByteSizeValue(512, ByteSizeUnit.KB), true, Setting.Scope.CLUSTER);
-    public static final Setting<Integer> INDICES_RECOVERY_TRANSLOG_OPS_SETTING = Setting.intSetting("indices.recovery.translog_ops", 1000, true, Setting.Scope.CLUSTER);
-    public static final Setting<ByteSizeValue> INDICES_RECOVERY_TRANSLOG_SIZE_SETTING = Setting.byteSizeSetting("indices.recovery.translog_size", new ByteSizeValue(512, ByteSizeUnit.KB), true, Setting.Scope.CLUSTER);
-    public static final Setting<Boolean> INDICES_RECOVERY_COMPRESS_SETTING = Setting.boolSetting("indices.recovery.compress", true, true, Setting.Scope.CLUSTER);
-    public static final Setting<Integer> INDICES_RECOVERY_CONCURRENT_STREAMS_SETTING = Setting.intSetting("indices.recovery.concurrent_streams", 3, true, Setting.Scope.CLUSTER);
-    public static final Setting<Integer> INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING = Setting.intSetting("indices.recovery.concurrent_small_file_streams", 2, true, Setting.Scope.CLUSTER);
-    public static final Setting<ByteSizeValue> INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING = Setting.byteSizeSetting("indices.recovery.max_bytes_per_sec", new ByteSizeValue(40, ByteSizeUnit.MB), true, Setting.Scope.CLUSTER);
+    public static final String INDICES_RECOVERY_CONCURRENT_STREAMS = "indices.recovery.concurrent_streams";
+    public static final String INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS = "indices.recovery.concurrent_small_file_streams";
+    public static final String INDICES_RECOVERY_MAX_BYTES_PER_SEC = "indices.recovery.max_bytes_per_sec";
 
     /**
      * how long to wait before retrying after issues cause by cluster state syncing between nodes
      * i.e., local node is not yet known on remote node, remote shard not yet started etc.
      */
-    public static final Setting<TimeValue> INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING = Setting.positiveTimeSetting("indices.recovery.retry_delay_state_sync", TimeValue.timeValueMillis(500), true, Setting.Scope.CLUSTER);
+    public static final String INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC = "indices.recovery.retry_delay_state_sync";
 
     /** how long to wait before retrying after network related issues */
-    public static final Setting<TimeValue> INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING = Setting.positiveTimeSetting("indices.recovery.retry_delay_network", TimeValue.timeValueSeconds(5), true, Setting.Scope.CLUSTER);
+    public static final String INDICES_RECOVERY_RETRY_DELAY_NETWORK = "indices.recovery.retry_delay_network";
+
+    /**
+     * recoveries that don't show any activity for more then this interval will be failed.
+     * defaults to `indices.recovery.internal_action_long_timeout`
+     */
+    public static final String INDICES_RECOVERY_ACTIVITY_TIMEOUT = "indices.recovery.recovery_activity_timeout";
 
     /** timeout value to use for requests made as part of the recovery process */
-    public static final Setting<TimeValue> INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING = Setting.positiveTimeSetting("indices.recovery.internal_action_timeout", TimeValue.timeValueMinutes(15), true, Setting.Scope.CLUSTER);
+    public static final String INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT = "indices.recovery.internal_action_timeout";
 
     /**
      * timeout value to use for requests made as part of the recovery process that are expected to take long time.
      * defaults to twice `indices.recovery.internal_action_timeout`.
      */
-    public static final Setting<TimeValue> INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING = Setting.timeSetting("indices.recovery.internal_action_long_timeout", (s) -> TimeValue.timeValueMillis(INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.get(s).millis() * 2).toString(), TimeValue.timeValueSeconds(0), true,  Setting.Scope.CLUSTER);
+    public static final String INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT = "indices.recovery.internal_action_long_timeout";
 
-    /**
-     * recoveries that don't show any activity for more then this interval will be failed.
-     * defaults to `indices.recovery.internal_action_long_timeout`
-     */
-    public static final Setting<TimeValue> INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING = Setting.timeSetting("indices.recovery.recovery_activity_timeout", (s) -> INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING.getRaw(s) , TimeValue.timeValueSeconds(0), true,  Setting.Scope.CLUSTER);
 
     public static final long SMALL_FILE_CUTOFF_BYTES = ByteSizeValue.parseBytesSizeValue("5mb", "SMALL_FILE_CUTOFF_BYTES").bytes();
 
-    private volatile ByteSizeValue fileChunkSize;
-
-    private volatile boolean compress;
-    private volatile int translogOps;
-    private volatile ByteSizeValue translogSize;
+    public static final ByteSizeValue DEFAULT_CHUNK_SIZE = new ByteSizeValue(512, ByteSizeUnit.KB);
 
     private volatile int concurrentStreams;
     private volatile int concurrentSmallFileStreams;
@@ -93,55 +86,44 @@ public class RecoverySettings extends AbstractComponent implements Closeable {
     private volatile TimeValue internalActionTimeout;
     private volatile TimeValue internalActionLongTimeout;
 
+    private volatile ByteSizeValue chunkSize = DEFAULT_CHUNK_SIZE;
 
     @Inject
-    public RecoverySettings(Settings settings, ClusterSettings clusterSettings) {
+    public RecoverySettings(Settings settings, NodeSettingsService nodeSettingsService) {
         super(settings);
-        this.fileChunkSize = INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING.get(settings);
-        this.translogOps = INDICES_RECOVERY_TRANSLOG_OPS_SETTING.get(settings);
-        this.translogSize = INDICES_RECOVERY_TRANSLOG_SIZE_SETTING.get(settings);
-        this.compress = INDICES_RECOVERY_COMPRESS_SETTING.get(settings);
 
-        this.retryDelayStateSync = INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING.get(settings);
+        this.retryDelayStateSync = settings.getAsTime(INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC, TimeValue.timeValueMillis(500));
         // doesn't have to be fast as nodes are reconnected every 10s by default (see InternalClusterService.ReconnectToNodes)
         // and we want to give the master time to remove a faulty node
-        this.retryDelayNetwork = INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.get(settings);
+        this.retryDelayNetwork = settings.getAsTime(INDICES_RECOVERY_RETRY_DELAY_NETWORK, TimeValue.timeValueSeconds(5));
 
-        this.internalActionTimeout = INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.get(settings);
-        this.internalActionLongTimeout = INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING.get(settings);
+        this.internalActionTimeout = settings.getAsTime(INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT, TimeValue.timeValueMinutes(15));
+        this.internalActionLongTimeout = settings.getAsTime(INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT, new TimeValue(internalActionTimeout.millis() * 2));
 
-        this.activityTimeout = INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING.get(settings);
+        this.activityTimeout = settings.getAsTime(INDICES_RECOVERY_ACTIVITY_TIMEOUT,
+                // default to the internalActionLongTimeout used as timeouts on RecoverySource
+                internalActionLongTimeout
+        );
 
 
-        this.concurrentStreams = INDICES_RECOVERY_CONCURRENT_STREAMS_SETTING.get(settings);
+        this.concurrentStreams = settings.getAsInt(INDICES_RECOVERY_CONCURRENT_STREAMS, 3);
         this.concurrentStreamPool = EsExecutors.newScaling("recovery_stream", 0, concurrentStreams, 60, TimeUnit.SECONDS,
                 EsExecutors.daemonThreadFactory(settings, "[recovery_stream]"));
-        this.concurrentSmallFileStreams = INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING.get(settings);
+        this.concurrentSmallFileStreams = settings.getAsInt(INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, 2);
         this.concurrentSmallFileStreamPool = EsExecutors.newScaling("small_file_recovery_stream", 0, concurrentSmallFileStreams, 60,
                 TimeUnit.SECONDS, EsExecutors.daemonThreadFactory(settings, "[small_file_recovery_stream]"));
 
-        this.maxBytesPerSec = INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.get(settings);
+        this.maxBytesPerSec = settings.getAsBytesSize(INDICES_RECOVERY_MAX_BYTES_PER_SEC, new ByteSizeValue(40, ByteSizeUnit.MB));
         if (maxBytesPerSec.bytes() <= 0) {
             rateLimiter = null;
         } else {
             rateLimiter = new SimpleRateLimiter(maxBytesPerSec.mbFrac());
         }
 
-        logger.debug("using max_bytes_per_sec[{}], concurrent_streams [{}], file_chunk_size [{}], translog_size [{}], translog_ops [{}], and compress [{}]",
-                maxBytesPerSec, concurrentStreams, fileChunkSize, translogSize, translogOps, compress);
-
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING, this::setFileChunkSize);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_TRANSLOG_OPS_SETTING, this::setTranslogOps);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_TRANSLOG_SIZE_SETTING, this::setTranslogSize);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_COMPRESS_SETTING, this::setCompress);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_CONCURRENT_STREAMS_SETTING, this::setConcurrentStreams);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING, this::setConcurrentSmallFileStreams);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING, this::setMaxBytesPerSec);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING, this::setRetryDelayStateSync);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING, this::setRetryDelayNetwork);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING, this::setInternalActionTimeout);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING, this::setInternalActionLongTimeout);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING, this::setActivityTimeout);
+        logger.debug("using max_bytes_per_sec[{}], concurrent_streams [{}]",
+                maxBytesPerSec, concurrentStreams);
+
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
     @Override
@@ -150,26 +132,6 @@ public class RecoverySettings extends AbstractComponent implements Closeable {
         ThreadPool.terminate(concurrentSmallFileStreamPool, 1, TimeUnit.SECONDS);
     }
 
-    public ByteSizeValue fileChunkSize() {
-        return fileChunkSize;
-    }
-
-    public boolean compress() {
-        return compress;
-    }
-
-    public int translogOps() {
-        return translogOps;
-    }
-
-    public ByteSizeValue translogSize() {
-        return translogSize;
-    }
-
-    public int concurrentStreams() {
-        return concurrentStreams;
-    }
-
     public ThreadPoolExecutor concurrentStreamPool() {
         return concurrentStreamPool;
     }
@@ -202,60 +164,60 @@ public class RecoverySettings extends AbstractComponent implements Closeable {
         return internalActionLongTimeout;
     }
 
-    private void setFileChunkSize(ByteSizeValue fileChunkSize) {
-        this.fileChunkSize = fileChunkSize;
-    }
-
-    private void setCompress(boolean compress) {
-        this.compress = compress;
-    }
+    public ByteSizeValue getChunkSize() { return chunkSize; }
 
-    private void setTranslogOps(int translogOps) {
-        this.translogOps = translogOps;
-    }
-
-    private void setTranslogSize(ByteSizeValue translogSize) {
-        this.translogSize = translogSize;
-    }
-
-    private void setConcurrentStreams(int concurrentStreams) {
-        this.concurrentStreams = concurrentStreams;
-        concurrentStreamPool.setMaximumPoolSize(concurrentStreams);
-    }
-
-    public void setRetryDelayStateSync(TimeValue retryDelayStateSync) {
-        this.retryDelayStateSync = retryDelayStateSync;
-    }
-
-    public void setRetryDelayNetwork(TimeValue retryDelayNetwork) {
-        this.retryDelayNetwork = retryDelayNetwork;
-    }
-
-    public void setActivityTimeout(TimeValue activityTimeout) {
-        this.activityTimeout = activityTimeout;
-    }
-
-    public void setInternalActionTimeout(TimeValue internalActionTimeout) {
-        this.internalActionTimeout = internalActionTimeout;
-    }
-
-    public void setInternalActionLongTimeout(TimeValue internalActionLongTimeout) {
-        this.internalActionLongTimeout = internalActionLongTimeout;
-    }
-
-    private void setMaxBytesPerSec(ByteSizeValue maxBytesPerSec) {
-        this.maxBytesPerSec = maxBytesPerSec;
-        if (maxBytesPerSec.bytes() <= 0) {
-            rateLimiter = null;
-        } else if (rateLimiter != null) {
-            rateLimiter.setMBPerSec(maxBytesPerSec.mbFrac());
-        } else {
-            rateLimiter = new SimpleRateLimiter(maxBytesPerSec.mbFrac());
+    void setChunkSize(ByteSizeValue chunkSize) { // only settable for tests
+        if (chunkSize.bytesAsInt() <= 0) {
+            throw new IllegalArgumentException("chunkSize must be > 0");
+        }
+        this.chunkSize = chunkSize;
+    }
+
+
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            ByteSizeValue maxSizePerSec = settings.getAsBytesSize(INDICES_RECOVERY_MAX_BYTES_PER_SEC, RecoverySettings.this.maxBytesPerSec);
+            if (!Objects.equals(maxSizePerSec, RecoverySettings.this.maxBytesPerSec)) {
+                logger.info("updating [{}] from [{}] to [{}]", INDICES_RECOVERY_MAX_BYTES_PER_SEC, RecoverySettings.this.maxBytesPerSec, maxSizePerSec);
+                RecoverySettings.this.maxBytesPerSec = maxSizePerSec;
+                if (maxSizePerSec.bytes() <= 0) {
+                    rateLimiter = null;
+                } else if (rateLimiter != null) {
+                    rateLimiter.setMBPerSec(maxSizePerSec.mbFrac());
+                } else {
+                    rateLimiter = new SimpleRateLimiter(maxSizePerSec.mbFrac());
+                }
+            }
+
+            int concurrentStreams = settings.getAsInt(INDICES_RECOVERY_CONCURRENT_STREAMS, RecoverySettings.this.concurrentStreams);
+            if (concurrentStreams != RecoverySettings.this.concurrentStreams) {
+                logger.info("updating [indices.recovery.concurrent_streams] from [{}] to [{}]", RecoverySettings.this.concurrentStreams, concurrentStreams);
+                RecoverySettings.this.concurrentStreams = concurrentStreams;
+                RecoverySettings.this.concurrentStreamPool.setMaximumPoolSize(concurrentStreams);
+            }
+
+            int concurrentSmallFileStreams = settings.getAsInt(INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, RecoverySettings.this.concurrentSmallFileStreams);
+            if (concurrentSmallFileStreams != RecoverySettings.this.concurrentSmallFileStreams) {
+                logger.info("updating [indices.recovery.concurrent_small_file_streams] from [{}] to [{}]", RecoverySettings.this.concurrentSmallFileStreams, concurrentSmallFileStreams);
+                RecoverySettings.this.concurrentSmallFileStreams = concurrentSmallFileStreams;
+                RecoverySettings.this.concurrentSmallFileStreamPool.setMaximumPoolSize(concurrentSmallFileStreams);
+            }
+
+            RecoverySettings.this.retryDelayNetwork = maybeUpdate(RecoverySettings.this.retryDelayNetwork, settings, INDICES_RECOVERY_RETRY_DELAY_NETWORK);
+            RecoverySettings.this.retryDelayStateSync = maybeUpdate(RecoverySettings.this.retryDelayStateSync, settings, INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC);
+            RecoverySettings.this.activityTimeout = maybeUpdate(RecoverySettings.this.activityTimeout, settings, INDICES_RECOVERY_ACTIVITY_TIMEOUT);
+            RecoverySettings.this.internalActionTimeout = maybeUpdate(RecoverySettings.this.internalActionTimeout, settings, INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT);
+            RecoverySettings.this.internalActionLongTimeout = maybeUpdate(RecoverySettings.this.internalActionLongTimeout, settings, INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT);
         }
-    }
 
-    private void setConcurrentSmallFileStreams(int concurrentSmallFileStreams) {
-        this.concurrentSmallFileStreams = concurrentSmallFileStreams;
-        concurrentSmallFileStreamPool.setMaximumPoolSize(concurrentSmallFileStreams);
+        private TimeValue maybeUpdate(final TimeValue currentValue, final Settings settings, final String key) {
+            final TimeValue value = settings.getAsTime(key, currentValue);
+            if (value.equals(currentValue)) {
+                return currentValue;
+            }
+            logger.info("updating [] from [{}] to [{}]", key, currentValue, value);
+            return value;
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
index 7319316..4057af0 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
@@ -36,7 +36,9 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.store.InputStreamIndexInput;
+import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.CancellableThreads;
 import org.elasticsearch.common.util.CancellableThreads.Interruptable;
 import org.elasticsearch.index.engine.RecoveryEngineException;
@@ -49,6 +51,7 @@ import org.elasticsearch.transport.RemoteTransportException;
 import org.elasticsearch.transport.TransportRequestOptions;
 import org.elasticsearch.transport.TransportService;
 
+import java.io.BufferedOutputStream;
 import java.io.IOException;
 import java.io.OutputStream;
 import java.util.ArrayList;
@@ -57,6 +60,7 @@ import java.util.List;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Future;
 import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.function.Function;
 import java.util.stream.StreamSupport;
@@ -77,9 +81,9 @@ public class RecoverySourceHandler {
     private final StartRecoveryRequest request;
     private final RecoverySettings recoverySettings;
     private final TransportService transportService;
+    private final int chunkSizeInBytes;
 
     protected final RecoveryResponse response;
-    private final TransportRequestOptions requestOptions;
 
     private final CancellableThreads cancellableThreads = new CancellableThreads() {
         @Override
@@ -106,14 +110,8 @@ public class RecoverySourceHandler {
         this.transportService = transportService;
         this.indexName = this.request.shardId().index().name();
         this.shardId = this.request.shardId().id();
-
+        this.chunkSizeInBytes = recoverySettings.getChunkSize().bytesAsInt();
         this.response = new RecoveryResponse();
-        this.requestOptions = TransportRequestOptions.builder()
-                .withCompress(recoverySettings.compress())
-                .withType(TransportRequestOptions.Type.RECOVERY)
-                .withTimeout(recoverySettings.internalActionTimeout())
-                .build();
-
     }
 
     /**
@@ -218,7 +216,7 @@ public class RecoverySourceHandler {
                     totalSize += md.length();
                 }
                 List<StoreFileMetaData> phase1Files = new ArrayList<>(diff.different.size() + diff.missing.size());
-                phase1Files.addAll(diff.different); 
+                phase1Files.addAll(diff.different);
                 phase1Files.addAll(diff.missing);
                 for (StoreFileMetaData md : phase1Files) {
                     if (request.metadataSnapshot().asMap().containsKey(md.name())) {
@@ -249,7 +247,7 @@ public class RecoverySourceHandler {
                 });
                 // How many bytes we've copied since we last called RateLimiter.pause
                 final AtomicLong bytesSinceLastPause = new AtomicLong();
-                final Function<StoreFileMetaData, OutputStream> outputStreamFactories = (md) -> new RecoveryOutputStream(md, bytesSinceLastPause, translogView);
+                final Function<StoreFileMetaData, OutputStream> outputStreamFactories = (md) -> new BufferedOutputStream(new RecoveryOutputStream(md, bytesSinceLastPause, translogView), chunkSizeInBytes);
                 sendFiles(store, phase1Files.toArray(new StoreFileMetaData[phase1Files.size()]), outputStreamFactories);
                 cancellableThreads.execute(() -> {
                     // Send the CLEAN_FILES request, which takes all of the files that
@@ -432,7 +430,7 @@ public class RecoverySourceHandler {
         }
 
         final TransportRequestOptions recoveryOptions = TransportRequestOptions.builder()
-                .withCompress(recoverySettings.compress())
+                .withCompress(true)
                 .withType(TransportRequestOptions.Type.RECOVERY)
                 .withTimeout(recoverySettings.internalActionLongTimeout())
                 .build();
@@ -451,9 +449,9 @@ public class RecoverySourceHandler {
             size += operation.estimateSize();
             totalOperations++;
 
-            // Check if this request is past the size or bytes threshold, and
+            // Check if this request is past bytes threshold, and
             // if so, send it off
-            if (ops >= recoverySettings.translogOps() || size >= recoverySettings.translogSize().bytes()) {
+            if (size >= chunkSizeInBytes) {
 
                 // don't throttle translog, since we lock for phase3 indexing,
                 // so we need to move it as fast as possible. Note, since we
@@ -537,7 +535,7 @@ public class RecoverySourceHandler {
 
         @Override
         public final void write(int b) throws IOException {
-            write(new byte[]{(byte) b}, 0, 1);
+            throw new UnsupportedOperationException("we can't send single bytes over the wire");
         }
 
         @Override
@@ -548,6 +546,11 @@ public class RecoverySourceHandler {
         }
 
         private void sendNextChunk(long position, BytesArray content, boolean lastChunk) throws IOException {
+            final TransportRequestOptions chunkSendOptions = TransportRequestOptions.builder()
+                .withCompress(false)  // lucene files are already compressed and therefore compressing this won't really help much so we are safing the cpu for other things
+                .withType(TransportRequestOptions.Type.RECOVERY)
+                .withTimeout(recoverySettings.internalActionTimeout())
+                .build();
             cancellableThreads.execute(() -> {
                 // Pause using the rate limiter, if desired, to throttle the recovery
                 final long throttleTimeInNanos;
@@ -577,7 +580,7 @@ public class RecoverySourceHandler {
                                  * see how many translog ops we accumulate while copying files across the network. A future optimization
                                  * would be in to restart file copy again (new deltas) if we have too many translog ops are piling up.
                                  */
-                                throttleTimeInNanos), requestOptions, EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
+                                throttleTimeInNanos), chunkSendOptions, EmptyTransportResponseHandler.INSTANCE_SAME).txGet();
             });
             if (shard.state() == IndexShardState.CLOSED) { // check if the shard got closed on us
                 throw new IndexShardClosedException(request.shardId());
@@ -670,9 +673,10 @@ public class RecoverySourceHandler {
                     pool = recoverySettings.concurrentSmallFileStreamPool();
                 }
                 Future<Void> future = pool.submit(() -> {
-                    try (final OutputStream outputStream = outputStreamFactory.apply(md);
-                         final IndexInput indexInput = store.directory().openInput(md.name(), IOContext.READONCE)) {
-                        Streams.copy(new InputStreamIndexInput(indexInput, md.length()), outputStream);
+                    try (final IndexInput indexInput = store.directory().openInput(md.name(), IOContext.READONCE)) {
+                        // it's fine that we are only having the indexInput int he try/with block. The copy methods handles
+                        // exceptions during close correctly and doesn't hide the original exception.
+                        Streams.copy(new InputStreamIndexInput(indexInput, md.length()), outputStreamFactory.apply(md));
                     }
                     return null;
                 });
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
index 2ccfbcb..32e644a 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java
@@ -134,12 +134,12 @@ public class RecoveryTarget extends AbstractComponent implements IndexEventListe
         logger.trace("will retry recovery with id [{}] in [{}]", reason, recoveryStatus.recoveryId(), retryAfter);
         retryRecovery(recoveryStatus, retryAfter, currentRequest);
     }
-    
+
     protected void retryRecovery(final RecoveryStatus recoveryStatus, final String reason, TimeValue retryAfter, final StartRecoveryRequest currentRequest) {
         logger.trace("will retry recovery with id [{}] in [{}] (reason [{}])", recoveryStatus.recoveryId(), retryAfter, reason);
         retryRecovery(recoveryStatus, retryAfter, currentRequest);
     }
-    
+
     private void retryRecovery(final RecoveryStatus recoveryStatus, TimeValue retryAfter, final StartRecoveryRequest currentRequest) {
         try {
             recoveryStatus.resetRecovery();
@@ -208,11 +208,15 @@ public class RecoveryTarget extends AbstractComponent implements IndexEventListe
         } catch (CancellableThreads.ExecutionCancelledException e) {
             logger.trace("recovery cancelled", e);
         } catch (Throwable e) {
-
             if (logger.isTraceEnabled()) {
                 logger.trace("[{}][{}] Got exception on recovery", e, request.shardId().index().name(), request.shardId().id());
             }
             Throwable cause = ExceptionsHelper.unwrapCause(e);
+            if (cause instanceof CancellableThreads.ExecutionCancelledException) {
+                // this can also come from the source wrapped in a RemoteTransportException
+                onGoingRecoveries.failRecovery(recoveryStatus.recoveryId(), new RecoveryFailedException(request, "source has canceled the recovery", cause), false);
+                return;
+            }
             if (cause instanceof RecoveryEngineException) {
                 // unwrap an exception that was thrown as part of the recovery
                 cause = cause.getCause();
diff --git a/core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java b/core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java
index 0eed825..f095cc3 100644
--- a/core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java
+++ b/core/src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java
@@ -36,8 +36,6 @@ import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
@@ -51,6 +49,7 @@ import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -67,7 +66,7 @@ import java.util.concurrent.locks.ReentrantLock;
  */
 public class IndicesTTLService extends AbstractLifecycleComponent<IndicesTTLService> {
 
-    public static final Setting<TimeValue> INDICES_TTL_INTERVAL_SETTING = Setting.positiveTimeSetting("indices.ttl.interval", TimeValue.timeValueSeconds(60), true, Setting.Scope.CLUSTER);
+    public static final String INDICES_TTL_INTERVAL = "indices.ttl.interval";
     public static final String INDEX_TTL_DISABLE_PURGE = "index.ttl.disable_purge";
 
     private final ClusterService clusterService;
@@ -78,15 +77,16 @@ public class IndicesTTLService extends AbstractLifecycleComponent<IndicesTTLServ
     private PurgerThread purgerThread;
 
     @Inject
-    public IndicesTTLService(Settings settings, ClusterService clusterService, IndicesService indicesService, ClusterSettings clusterSettings, TransportBulkAction bulkAction) {
+    public IndicesTTLService(Settings settings, ClusterService clusterService, IndicesService indicesService, NodeSettingsService nodeSettingsService, TransportBulkAction bulkAction) {
         super(settings);
         this.clusterService = clusterService;
         this.indicesService = indicesService;
-        TimeValue interval = INDICES_TTL_INTERVAL_SETTING.get(settings);
+        TimeValue interval = this.settings.getAsTime("indices.ttl.interval", TimeValue.timeValueSeconds(60));
         this.bulkAction = bulkAction;
         this.bulkSize = this.settings.getAsInt("indices.ttl.bulk_size", 10000);
         this.purgerThread = new PurgerThread(EsExecutors.threadName(settings, "[ttl_expire]"), interval);
-        clusterSettings.addSettingsUpdateConsumer(INDICES_TTL_INTERVAL_SETTING, this.purgerThread::resetInterval);
+
+        nodeSettingsService.addListener(new ApplySettings());
     }
 
     @Override
@@ -310,6 +310,20 @@ public class IndicesTTLService extends AbstractLifecycleComponent<IndicesTTLServ
         return bulkRequest;
     }
 
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            final TimeValue currentInterval = IndicesTTLService.this.purgerThread.getInterval();
+            final TimeValue interval = settings.getAsTime(INDICES_TTL_INTERVAL, currentInterval);
+            if (!interval.equals(currentInterval)) {
+                logger.info("updating indices.ttl.interval from [{}] to [{}]",currentInterval, interval);
+                IndicesTTLService.this.purgerThread.resetInterval(interval);
+
+            }
+        }
+    }
+
+
     private static final class Notifier {
 
         private final ReentrantLock lock = new ReentrantLock();
diff --git a/core/src/main/java/org/elasticsearch/node/Node.java b/core/src/main/java/org/elasticsearch/node/Node.java
index a28e532..d3f6367 100644
--- a/core/src/main/java/org/elasticsearch/node/Node.java
+++ b/core/src/main/java/org/elasticsearch/node/Node.java
@@ -23,6 +23,7 @@ import org.elasticsearch.Build;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionModule;
+import org.elasticsearch.bootstrap.Elasticsearch;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.node.NodeClientModule;
@@ -32,6 +33,7 @@ import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.routing.RoutingService;
 import org.elasticsearch.common.StopWatch;
+import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.component.Lifecycle;
 import org.elasticsearch.common.component.LifecycleComponent;
 import org.elasticsearch.common.inject.Injector;
@@ -41,12 +43,14 @@ import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
+import org.elasticsearch.common.network.NetworkAddress;
 import org.elasticsearch.common.network.NetworkModule;
 import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.SettingsFilter;
 import org.elasticsearch.common.settings.SettingsModule;
+import org.elasticsearch.common.transport.BoundTransportAddress;
+import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoveryModule;
 import org.elasticsearch.discovery.DiscoveryService;
@@ -59,6 +63,7 @@ import org.elasticsearch.gateway.GatewayModule;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.http.HttpServer;
 import org.elasticsearch.http.HttpServerModule;
+import org.elasticsearch.http.HttpServerTransport;
 import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.analysis.AnalysisModule;
@@ -73,6 +78,7 @@ import org.elasticsearch.indices.ttl.IndicesTTLService;
 import org.elasticsearch.monitor.MonitorService;
 import org.elasticsearch.monitor.jvm.JvmInfo;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.percolator.PercolatorModule;
 import org.elasticsearch.percolator.PercolatorService;
 import org.elasticsearch.plugins.Plugin;
@@ -96,7 +102,16 @@ import org.elasticsearch.tribe.TribeService;
 import org.elasticsearch.watcher.ResourceWatcherModule;
 import org.elasticsearch.watcher.ResourceWatcherService;
 
+import java.io.BufferedWriter;
 import java.io.IOException;
+import java.net.Inet6Address;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.nio.charset.Charset;
+import java.nio.file.CopyOption;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardCopyOption;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
@@ -107,8 +122,6 @@ import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 /**
  * A node represent a node within a cluster (<tt>cluster.name</tt>). The {@link #client()} can be used
  * in order to use a {@link Client} to perform actions/operations against the cluster.
- * <p>In order to create a node, the {@link NodeBuilder} can be used. When done with it, make sure to
- * call {@link #close()} on it.
  */
 public class Node implements Releasable {
 
@@ -127,14 +140,13 @@ public class Node implements Releasable {
      * @param preparedSettings Base settings to configure the node with
      */
     public Node(Settings preparedSettings) {
-        this(preparedSettings, Version.CURRENT, Collections.<Class<? extends Plugin>>emptyList());
+        this(InternalSettingsPreparer.prepareEnvironment(preparedSettings, null), Version.CURRENT, Collections.<Class<? extends Plugin>>emptyList());
     }
 
-    Node(Settings preparedSettings, Version version, Collection<Class<? extends Plugin>> classpathPlugins) {
-        final Settings pSettings = settingsBuilder().put(preparedSettings)
-                .put(Client.CLIENT_TYPE_SETTING, CLIENT_TYPE).build();
-        Environment tmpEnv = InternalSettingsPreparer.prepareEnvironment(pSettings, null);
-        Settings tmpSettings = TribeService.processSettings(tmpEnv.settings());
+    protected Node(Environment tmpEnv, Version version, Collection<Class<? extends Plugin>> classpathPlugins) {
+        Settings tmpSettings = settingsBuilder().put(tmpEnv.settings())
+            .put(Client.CLIENT_TYPE_SETTING, CLIENT_TYPE).build();
+        tmpSettings = TribeService.processSettings(tmpSettings);
 
         ESLogger logger = Loggers.getLogger(Node.class, tmpSettings.get("name"));
         logger.info("version[{}], pid[{}], build[{}/{}]", version, JvmInfo.jvmInfo().pid(), Build.CURRENT.shortHash(), Build.CURRENT.date());
@@ -158,6 +170,7 @@ public class Node implements Releasable {
             throw new IllegalStateException("Failed to created node environment", ex);
         }
         final NetworkService networkService = new NetworkService(settings);
+        final NodeSettingsService nodeSettingsService = new NodeSettingsService(settings);
         final SettingsFilter settingsFilter = new SettingsFilter(settings);
         final ThreadPool threadPool = new ThreadPool(settings);
         boolean success = false;
@@ -172,7 +185,7 @@ public class Node implements Releasable {
             }
             modules.add(new PluginsModule(pluginsService));
             modules.add(new SettingsModule(this.settings, settingsFilter));
-            modules.add(new NodeModule(this,monitorService));
+            modules.add(new NodeModule(this, nodeSettingsService, monitorService));
             modules.add(new NetworkModule(networkService));
             modules.add(new ScriptModule(this.settings));
             modules.add(new EnvironmentModule(environment));
@@ -202,7 +215,7 @@ public class Node implements Releasable {
             injector = modules.createInjector();
 
             client = injector.getInstance(Client.class);
-            threadPool.setClusterSettings(injector.getInstance(ClusterSettings.class));
+            threadPool.setNodeSettingsService(injector.getInstance(NodeSettingsService.class));
             success = true;
         } catch (IOException ex) {
             throw new ElasticsearchException("failed to bind service", ex);
@@ -275,6 +288,15 @@ public class Node implements Releasable {
         injector.getInstance(ResourceWatcherService.class).start();
         injector.getInstance(TribeService.class).start();
 
+        if (System.getProperty("es.tests.portsfile", "false").equals("true")) {
+            if (settings.getAsBoolean("http.enabled", true)) {
+                HttpServerTransport http = injector.getInstance(HttpServerTransport.class);
+                writePortsFile("http", http.boundAddress());
+            }
+            TransportService transport = injector.getInstance(TransportService.class);
+            writePortsFile("transport", transport.boundAddress());
+        }
+
         logger.info("started");
 
         return this;
@@ -426,4 +448,27 @@ public class Node implements Releasable {
     public Injector injector() {
         return this.injector;
     }
+
+    /** Writes a file to the logs dir containing the ports for the given transport type */
+    private void writePortsFile(String type, BoundTransportAddress boundAddress) {
+        Path tmpPortsFile = environment.logsFile().resolve(type + ".ports.tmp");
+        try (BufferedWriter writer = Files.newBufferedWriter(tmpPortsFile, Charset.forName("UTF-8"))) {
+            for (TransportAddress address : boundAddress.boundAddresses()) {
+                InetAddress inetAddress = InetAddress.getByName(address.getAddress());
+                if (inetAddress instanceof Inet6Address && inetAddress.isLinkLocalAddress()) {
+                    // no link local, just causes problems
+                    continue;
+                }
+                writer.write(NetworkAddress.formatAddress(new InetSocketAddress(inetAddress, address.getPort())) + "\n");
+            }
+        } catch (IOException e) {
+            throw new RuntimeException("Failed to write ports file", e);
+        }
+        Path portsFile = environment.logsFile().resolve(type + ".ports");
+        try {
+            Files.move(tmpPortsFile, portsFile, StandardCopyOption.ATOMIC_MOVE);
+        } catch (IOException e) {
+            throw new RuntimeException("Failed to rename ports file", e);
+        }
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/node/NodeBuilder.java b/core/src/main/java/org/elasticsearch/node/NodeBuilder.java
deleted file mode 100644
index 377c409..0000000
--- a/core/src/main/java/org/elasticsearch/node/NodeBuilder.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.node;
-
-import org.elasticsearch.common.settings.Settings;
-
-/**
- * A node builder is used to construct a {@link Node} instance.
- * <p>
- * Settings will be loaded relative to the ES home (with or without <tt>config/</tt> prefix) and if not found,
- * within the classpath (with or without <tt>config/</tt> prefix). The settings file loaded can either be named
- * <tt>elasticsearch.yml</tt> or <tt>elasticsearch.json</tt>).
- * <p>
- * Explicit settings can be passed by using the {@link #settings(org.elasticsearch.common.settings.Settings)} method.
- * <p>
- * In any case, settings will be resolved from system properties as well that are either prefixed with <tt>es.</tt>
- * or <tt>elasticsearch.</tt>.
- * <p>
- * An example for creating a simple node with optional settings loaded from the classpath:
- * <pre>
- * Node node = NodeBuilder.nodeBuilder().node();
- * </pre>
- * <p>
- * An example for creating a node with explicit settings (in this case, a node in the cluster that does not hold
- * data):
- * <pre>
- * Node node = NodeBuilder.nodeBuilder()
- *                      .settings(Settings.settingsBuilder().put("node.data", false)
- *                      .node();
- * </pre>
- * <p>
- * When done with the node, make sure you call {@link Node#close()} on it.
- *
- *
- */
-public class NodeBuilder {
-
-    private final Settings.Builder settings = Settings.settingsBuilder();
-
-    /**
-     * A convenient factory method to create a {@link NodeBuilder}.
-     */
-    public static NodeBuilder nodeBuilder() {
-        return new NodeBuilder();
-    }
-
-    /**
-     * Set addition settings simply by working directly against the settings builder.
-     */
-    public Settings.Builder settings() {
-        return settings;
-    }
-
-    /**
-     * Set addition settings simply by working directly against the settings builder.
-     */
-    public Settings.Builder getSettings() {
-        return settings;
-    }
-
-    /**
-     * Explicit node settings to set.
-     */
-    public NodeBuilder settings(Settings.Builder settings) {
-        return settings(settings.build());
-    }
-
-    /**
-     * Explicit node settings to set.
-     */
-    public NodeBuilder settings(Settings settings) {
-        this.settings.put(settings);
-        return this;
-    }
-
-    /**
-     * Is the node going to be a client node which means it will hold no data (<tt>node.data</tt> is
-     * set to <tt>false</tt>) and other optimizations by different modules.
-     *
-     * @param client Should the node be just a client node or not.
-     */
-    public NodeBuilder client(boolean client) {
-        settings.put("node.client", client);
-        return this;
-    }
-
-    /**
-     * Is the node going to be allowed to allocate data (shards) to it or not. This setting map to
-     * the <tt>node.data</tt> setting. Note, when setting {@link #client(boolean)}, the node will
-     * not hold any data by default.
-     *
-     * @param data Should the node be allocated data to or not.
-     */
-    public NodeBuilder data(boolean data) {
-        settings.put("node.data", data);
-        return this;
-    }
-
-    /**
-     * Is the node a local node. A local node is a node that uses a local (JVM level) discovery and
-     * transport. Other (local) nodes started within the same JVM (actually, class-loader) will be
-     * discovered and communicated with. Nodes outside of the JVM will not be discovered.
-     *
-     * @param local Should the node be local or not
-     */
-    public NodeBuilder local(boolean local) {
-        settings.put("node.local", local);
-        return this;
-    }
-
-    /**
-     * The cluster name this node is part of (maps to the <tt>cluster.name</tt> setting). Defaults
-     * to <tt>elasticsearch</tt>.
-     *
-     * @param clusterName The cluster name this node is part of.
-     */
-    public NodeBuilder clusterName(String clusterName) {
-        settings.put("cluster.name", clusterName);
-        return this;
-    }
-
-    /**
-     * Builds the node without starting it.
-     */
-    public Node build() {
-        return new Node(settings.build());
-    }
-
-    /**
-     * {@link #build()}s and starts the node.
-     */
-    public Node node() {
-        return build().start();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/node/NodeModule.java b/core/src/main/java/org/elasticsearch/node/NodeModule.java
index aa52d38..3641c32 100644
--- a/core/src/main/java/org/elasticsearch/node/NodeModule.java
+++ b/core/src/main/java/org/elasticsearch/node/NodeModule.java
@@ -23,7 +23,9 @@ import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.monitor.MonitorService;
+import org.elasticsearch.node.Node;
 import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 /**
  *
@@ -31,14 +33,16 @@ import org.elasticsearch.node.service.NodeService;
 public class NodeModule extends AbstractModule {
 
     private final Node node;
+    private final NodeSettingsService nodeSettingsService;
     private final MonitorService monitorService;
 
     // pkg private so tests can mock
     Class<? extends PageCacheRecycler> pageCacheRecyclerImpl = PageCacheRecycler.class;
     Class<? extends BigArrays> bigArraysImpl = BigArrays.class;
 
-    public NodeModule(Node node, MonitorService monitorService) {
+    public NodeModule(Node node, NodeSettingsService nodeSettingsService, MonitorService monitorService) {
         this.node = node;
+        this.nodeSettingsService = nodeSettingsService;
         this.monitorService = monitorService;
     }
 
@@ -56,6 +60,7 @@ public class NodeModule extends AbstractModule {
         }
 
         bind(Node.class).toInstance(node);
+        bind(NodeSettingsService.class).toInstance(nodeSettingsService);
         bind(MonitorService.class).toInstance(monitorService);
         bind(NodeService.class).asEagerSingleton();
     }
diff --git a/core/src/main/java/org/elasticsearch/node/package-info.java b/core/src/main/java/org/elasticsearch/node/package-info.java
index fa503a9..02538cd 100644
--- a/core/src/main/java/org/elasticsearch/node/package-info.java
+++ b/core/src/main/java/org/elasticsearch/node/package-info.java
@@ -18,7 +18,7 @@
  */
 
 /**
- * Allow to build a {@link org.elasticsearch.node.Node} using {@link org.elasticsearch.node.NodeBuilder} which is a
+ * Allow to build a {@link org.elasticsearch.node.Node} which is a
  * node within the cluster.
  */
-package org.elasticsearch.node;
\ No newline at end of file
+package org.elasticsearch.node;
diff --git a/core/src/main/java/org/elasticsearch/node/settings/NodeSettingsService.java b/core/src/main/java/org/elasticsearch/node/settings/NodeSettingsService.java
new file mode 100644
index 0000000..dbe6a33
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/node/settings/NodeSettingsService.java
@@ -0,0 +1,122 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.node.settings;
+
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterStateListener;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.logging.ESLoggerFactory;
+import org.elasticsearch.common.settings.Settings;
+
+import java.util.Map;
+import java.util.concurrent.CopyOnWriteArrayList;
+
+/**
+ * A service that allows to register for node settings change that can come from cluster
+ * events holding new settings.
+ */
+public class NodeSettingsService extends AbstractComponent implements ClusterStateListener {
+
+    private static volatile Settings globalSettings = Settings.Builder.EMPTY_SETTINGS;
+
+    /**
+     * Returns the global (static) settings last updated by a node. Note, if you have multiple
+     * nodes on the same JVM, it will just return the latest one set...
+     */
+    public static Settings getGlobalSettings() {
+        return globalSettings;
+    }
+
+    private volatile Settings lastSettingsApplied;
+
+    private final CopyOnWriteArrayList<Listener> listeners = new CopyOnWriteArrayList<>();
+
+    @Inject
+    public NodeSettingsService(Settings settings) {
+        super(settings);
+        globalSettings = settings;
+    }
+
+    // inject it as a member, so we won't get into possible cyclic problems
+    public void setClusterService(ClusterService clusterService) {
+        clusterService.add(this);
+    }
+
+    @Override
+    public void clusterChanged(ClusterChangedEvent event) {
+        // nothing to do until we actually recover from the gateway or any other block indicates we need to disable persistency
+        if (event.state().blocks().disableStatePersistence()) {
+            return;
+        }
+
+        if (!event.metaDataChanged()) {
+            // nothing changed in the metadata, no need to check
+            return;
+        }
+
+        if (lastSettingsApplied != null && event.state().metaData().settings().equals(lastSettingsApplied)) {
+            // nothing changed in the settings, ignore
+            return;
+        }
+
+        for (Listener listener : listeners) {
+            try {
+                listener.onRefreshSettings(event.state().metaData().settings());
+            } catch (Exception e) {
+                logger.warn("failed to refresh settings for [{}]", e, listener);
+            }
+        }
+
+        try {
+            for (Map.Entry<String, String> entry : event.state().metaData().settings().getAsMap().entrySet()) {
+                if (entry.getKey().startsWith("logger.")) {
+                    String component = entry.getKey().substring("logger.".length());
+                    if ("_root".equals(component)) {
+                        ESLoggerFactory.getRootLogger().setLevel(entry.getValue());
+                    } else {
+                        ESLoggerFactory.getLogger(component).setLevel(entry.getValue());
+                    }
+                }
+            }
+        } catch (Exception e) {
+            logger.warn("failed to refresh settings for [{}]", e, "logger");
+        }
+
+        lastSettingsApplied = event.state().metaData().settings();
+        globalSettings = lastSettingsApplied;
+    }
+
+    /**
+     * Only settings registered in {@link org.elasticsearch.cluster.ClusterModule} can be changed dynamically.
+     */
+    public void addListener(Listener listener) {
+        this.listeners.add(listener);
+    }
+
+    public void removeListener(Listener listener) {
+        this.listeners.remove(listener);
+    }
+
+    public interface Listener {
+        void onRefreshSettings(Settings settings);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 5993958..0a01667 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -22,6 +22,7 @@ package org.elasticsearch.plugins;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.*;
 import org.elasticsearch.bootstrap.JarHell;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.cli.Terminal;
 import org.elasticsearch.common.collect.Tuple;
@@ -66,7 +67,7 @@ public class PluginManager {
             "plugin",
             "plugin.bat",
             "service.bat"));
-    
+
     static final Set<String> MODULES = unmodifiableSet(newHashSet(
             "lang-expression",
             "lang-groovy"));
@@ -83,6 +84,7 @@ public class PluginManager {
             "discovery-gce",
             "discovery-multicast",
             "lang-javascript",
+            "lang-plan-a",
             "lang-python",
             "mapper-attachments",
             "mapper-murmur3",
@@ -123,7 +125,7 @@ public class PluginManager {
             checkForForbiddenName(pluginHandle.name);
         } else {
             // if we have no name but url, use temporary name that will be overwritten later
-            pluginHandle = new PluginHandle("temp_name" + new Random().nextInt(), null, null);
+            pluginHandle = new PluginHandle("temp_name" + Randomness.get().nextInt(), null, null);
         }
 
         Path pluginFile = download(pluginHandle, terminal);
@@ -223,7 +225,7 @@ public class PluginManager {
         PluginInfo info = PluginInfo.readFromProperties(root);
         terminal.println(VERBOSE, "%s", info);
 
-        // don't let luser install plugin as a module... 
+        // don't let luser install plugin as a module...
         // they might be unavoidably in maven central and are packaged up the same way)
         if (MODULES.contains(info.getName())) {
             throw new IOException("plugin '" + info.getName() + "' cannot be installed like this, it is a system module");
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
index 4cd5f11..5ebd43d 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginsService.java
@@ -316,7 +316,8 @@ public class PluginsService extends AbstractComponent {
                 // gather urls for jar files
                 try (DirectoryStream<Path> jarStream = Files.newDirectoryStream(module, "*.jar")) {
                     for (Path jar : jarStream) {
-                        bundle.urls.add(jar.toUri().toURL());
+                        // normalize with toRealPath to get symlinks out of our hair
+                        bundle.urls.add(jar.toRealPath().toUri().toURL());
                     }
                 }
                 bundles.add(bundle);
@@ -357,7 +358,8 @@ public class PluginsService extends AbstractComponent {
                     // a jvm plugin: gather urls for jar files
                     try (DirectoryStream<Path> jarStream = Files.newDirectoryStream(plugin, "*.jar")) {
                         for (Path jar : jarStream) {
-                            urls.add(jar.toUri().toURL());
+                            // normalize with toRealPath to get symlinks out of our hair
+                            urls.add(jar.toRealPath().toUri().toURL());
                         }
                     }
                 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java
index b7b5064..a1cfdb4 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java
@@ -23,27 +23,19 @@ import org.elasticsearch.action.admin.cluster.state.ClusterStateRequest;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
-import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
-import java.io.IOException;
-
 /**
  */
 public class RestClusterGetSettingsAction extends BaseRestHandler {
 
-    private final ClusterSettings clusterSettings;
-
     @Inject
-    public RestClusterGetSettingsAction(Settings settings, RestController controller, Client client, ClusterSettings clusterSettings) {
+    public RestClusterGetSettingsAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.clusterSettings = clusterSettings;
         controller.registerHandler(RestRequest.Method.GET, "/_cluster/settings", this);
     }
 
@@ -52,34 +44,24 @@ public class RestClusterGetSettingsAction extends BaseRestHandler {
         ClusterStateRequest clusterStateRequest = Requests.clusterStateRequest()
                 .routingTable(false)
                 .nodes(false);
-        final boolean renderDefaults = request.paramAsBoolean("defaults", false);
         clusterStateRequest.local(request.paramAsBoolean("local", clusterStateRequest.local()));
         client.admin().cluster().state(clusterStateRequest, new RestBuilderListener<ClusterStateResponse>(channel) {
             @Override
             public RestResponse buildResponse(ClusterStateResponse response, XContentBuilder builder) throws Exception {
-                return new BytesRestResponse(RestStatus.OK, renderResponse(response.getState(), renderDefaults, builder, request));
-            }
-        });
-    }
-
-    private XContentBuilder renderResponse(ClusterState state, boolean renderDefaults, XContentBuilder builder, ToXContent.Params params) throws IOException {
-        builder.startObject();
+                builder.startObject();
 
-        builder.startObject("persistent");
-        state.metaData().persistentSettings().toXContent(builder, params);
-        builder.endObject();
+                builder.startObject("persistent");
+                response.getState().metaData().persistentSettings().toXContent(builder, request);
+                builder.endObject();
 
-        builder.startObject("transient");
-        state.metaData().transientSettings().toXContent(builder, params);
-        builder.endObject();
+                builder.startObject("transient");
+                response.getState().metaData().transientSettings().toXContent(builder, request);
+                builder.endObject();
 
-        if (renderDefaults) {
-            builder.startObject("defaults");
-            clusterSettings.diff(state.metaData().settings(), this.settings).toXContent(builder, params);
-            builder.endObject();
-        }
+                builder.endObject();
 
-        builder.endObject();
-        return builder;
+                return new BytesRestResponse(RestStatus.OK, builder);
+            }
+        });
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java
index 4841500..5648abc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.rest.action.admin.indices.alias;
 
 import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest;
+import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequest.AliasActions;
 import org.elasticsearch.action.admin.indices.alias.IndicesAliasesResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.metadata.AliasAction;
@@ -30,9 +31,10 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
 
+import java.util.ArrayList;
+import java.util.List;
 import java.util.Map;
 
-import static org.elasticsearch.cluster.metadata.AliasAction.newAddAliasAction;
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 
 /**
@@ -75,8 +77,8 @@ public class RestIndicesAliasesAction extends BaseRestHandler {
                             } else {
                                 throw new IllegalArgumentException("Alias action [" + action + "] not supported");
                             }
-                            String index = null;
-                            String alias = null;
+                            String[] indices = null;
+                            String[] aliases = null;
                             Map<String, Object> filter = null;
                             String routing = null;
                             boolean routingSet = false;
@@ -90,9 +92,9 @@ public class RestIndicesAliasesAction extends BaseRestHandler {
                                     currentFieldName = parser.currentName();
                                 } else if (token.isValue()) {
                                     if ("index".equals(currentFieldName)) {
-                                        index = parser.text();
+                                        indices = new String[] { parser.text() };
                                     } else if ("alias".equals(currentFieldName)) {
-                                        alias = parser.text();
+                                        aliases = new String[] { parser.text() };
                                     } else if ("routing".equals(currentFieldName)) {
                                         routing = parser.textOrNull();
                                         routingSet = true;
@@ -103,6 +105,23 @@ public class RestIndicesAliasesAction extends BaseRestHandler {
                                         searchRouting = parser.textOrNull();
                                         searchRoutingSet = true;
                                     }
+                                } else if (token == XContentParser.Token.START_ARRAY) {
+                                    if ("indices".equals(currentFieldName)) {
+                                        List<String> indexNames = new ArrayList<>();
+                                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                                            String index = parser.text();
+                                            indexNames.add(index);
+                                        }
+                                        indices = indexNames.toArray(new String[indexNames.size()]);
+                                    }
+                                    if ("aliases".equals(currentFieldName)) {
+                                        List<String> aliasNames = new ArrayList<>();
+                                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                                            String alias = parser.text();
+                                            aliasNames.add(alias);
+                                        }
+                                        aliases = aliasNames.toArray(new String[aliasNames.size()]);
+                                    }
                                 } else if (token == XContentParser.Token.START_OBJECT) {
                                     if ("filter".equals(currentFieldName)) {
                                         filter = parser.mapOrdered();
@@ -111,19 +130,19 @@ public class RestIndicesAliasesAction extends BaseRestHandler {
                             }
 
                             if (type == AliasAction.Type.ADD) {
-                                AliasAction aliasAction = newAddAliasAction(index, alias).filter(filter);
+                                AliasActions aliasActions = new AliasActions(type, indices, aliases);
                                 if (routingSet) {
-                                    aliasAction.routing(routing);
+                                    aliasActions.routing(routing);
                                 }
                                 if (indexRoutingSet) {
-                                    aliasAction.indexRouting(indexRouting);
+                                    aliasActions.indexRouting(indexRouting);
                                 }
                                 if (searchRoutingSet) {
-                                    aliasAction.searchRouting(searchRouting);
+                                    aliasActions.searchRouting(searchRouting);
                                 }
-                                indicesAliasesRequest.addAliasAction(aliasAction);
+                                indicesAliasesRequest.addAliasAction(aliasActions);
                             } else if (type == AliasAction.Type.REMOVE) {
-                                indicesAliasesRequest.removeAlias(index, alias);
+                                indicesAliasesRequest.removeAlias(indices, aliases);
                             }
                         }
                     }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java
index 57ceb21..3a86911 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java
@@ -21,7 +21,8 @@ package org.elasticsearch.rest.action.admin.indices.analyze;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeResponse;
 import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -47,6 +48,17 @@ import static org.elasticsearch.rest.RestRequest.Method.POST;
  */
 public class RestAnalyzeAction extends BaseRestHandler {
 
+    public static class Fields {
+        public static final ParseField ANALYZER = new ParseField("analyzer");
+        public static final ParseField TEXT = new ParseField("text");
+        public static final ParseField FIELD = new ParseField("field");
+        public static final ParseField TOKENIZER = new ParseField("tokenizer");
+        public static final ParseField TOKEN_FILTERS = new ParseField("token_filters", "filters");
+        public static final ParseField CHAR_FILTERS = new ParseField("char_filters");
+        public static final ParseField EXPLAIN = new ParseField("explain");
+        public static final ParseField ATTRIBUTES = new ParseField("attributes");
+    }
+
     @Inject
     public RestAnalyzeAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
@@ -68,6 +80,8 @@ public class RestAnalyzeAction extends BaseRestHandler {
         analyzeRequest.tokenizer(request.param("tokenizer"));
         analyzeRequest.tokenFilters(request.paramAsStringArray("token_filters", request.paramAsStringArray("filters", analyzeRequest.tokenFilters())));
         analyzeRequest.charFilters(request.paramAsStringArray("char_filters", analyzeRequest.charFilters()));
+        analyzeRequest.explain(request.paramAsBoolean("explain", false));
+        analyzeRequest.attributes(request.paramAsStringArray("attributes", analyzeRequest.attributes()));
 
         if (RestActions.hasBodyContent(request)) {
             XContentType type = RestActions.guessBodyContentType(request);
@@ -78,14 +92,14 @@ public class RestAnalyzeAction extends BaseRestHandler {
                 }
             } else {
                 // NOTE: if rest request with xcontent body has request parameters, the parameters does not override xcontent values
-                buildFromContent(RestActions.getRestContent(request), analyzeRequest);
+                buildFromContent(RestActions.getRestContent(request), analyzeRequest, parseFieldMatcher);
             }
         }
 
         client.admin().indices().analyze(analyzeRequest, new RestToXContentListener<AnalyzeResponse>(channel));
     }
 
-    public static void buildFromContent(BytesReference content, AnalyzeRequest analyzeRequest) {
+    public static void buildFromContent(BytesReference content, AnalyzeRequest analyzeRequest, ParseFieldMatcher parseFieldMatcher) {
         try (XContentParser parser = XContentHelper.createParser(content)) {
             if (parser.nextToken() != XContentParser.Token.START_OBJECT) {
                 throw new IllegalArgumentException("Malforrmed content, must start with an object");
@@ -95,9 +109,9 @@ public class RestAnalyzeAction extends BaseRestHandler {
                 while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                     if (token == XContentParser.Token.FIELD_NAME) {
                         currentFieldName = parser.currentName();
-                    } else if ("text".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.TEXT) && token == XContentParser.Token.VALUE_STRING) {
                         analyzeRequest.text(parser.text());
-                    } else if ("text".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.TEXT) && token == XContentParser.Token.START_ARRAY) {
                         List<String> texts = new ArrayList<>();
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                             if (token.isValue() == false) {
@@ -105,14 +119,14 @@ public class RestAnalyzeAction extends BaseRestHandler {
                             }
                             texts.add(parser.text());
                         }
-                        analyzeRequest.text(texts.toArray(Strings.EMPTY_ARRAY));
-                    } else if ("analyzer".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {
+                        analyzeRequest.text(texts.toArray(new String[texts.size()]));
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.ANALYZER) && token == XContentParser.Token.VALUE_STRING) {
                         analyzeRequest.analyzer(parser.text());
-                    } else if ("field".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.FIELD) && token == XContentParser.Token.VALUE_STRING) {
                         analyzeRequest.field(parser.text());
-                    } else if ("tokenizer".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.TOKENIZER) && token == XContentParser.Token.VALUE_STRING) {
                         analyzeRequest.tokenizer(parser.text());
-                    } else if (("token_filters".equals(currentFieldName) || "filters".equals(currentFieldName)) && token == XContentParser.Token.START_ARRAY) {
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.TOKEN_FILTERS) && token == XContentParser.Token.START_ARRAY) {
                         List<String> filters = new ArrayList<>();
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                             if (token.isValue() == false) {
@@ -120,8 +134,8 @@ public class RestAnalyzeAction extends BaseRestHandler {
                             }
                             filters.add(parser.text());
                         }
-                        analyzeRequest.tokenFilters(filters.toArray(Strings.EMPTY_ARRAY));
-                    } else if ("char_filters".equals(currentFieldName) && token == XContentParser.Token.START_ARRAY) {
+                        analyzeRequest.tokenFilters(filters.toArray(new String[filters.size()]));
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.CHAR_FILTERS) && token == XContentParser.Token.START_ARRAY) {
                         List<String> charFilters = new ArrayList<>();
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                             if (token.isValue() == false) {
@@ -129,7 +143,18 @@ public class RestAnalyzeAction extends BaseRestHandler {
                             }
                             charFilters.add(parser.text());
                         }
-                        analyzeRequest.tokenFilters(charFilters.toArray(Strings.EMPTY_ARRAY));
+                        analyzeRequest.charFilters(charFilters.toArray(new String[charFilters.size()]));
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.EXPLAIN) && token == XContentParser.Token.VALUE_BOOLEAN) {
+                        analyzeRequest.explain(parser.booleanValue());
+                    } else if (parseFieldMatcher.match(currentFieldName, Fields.ATTRIBUTES) && token == XContentParser.Token.START_ARRAY){
+                        List<String> attributes = new ArrayList<>();
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                            if (token.isValue() == false) {
+                                throw new IllegalArgumentException(currentFieldName + " array element should only contain attribute name");
+                            }
+                            attributes.add(parser.text());
+                        }
+                        analyzeRequest.attributes(attributes.toArray(new String[attributes.size()]));
                     } else {
                         throw new IllegalArgumentException("Unknown parameter [" + currentFieldName + "] in request body or parameter is of the wrong type[" + token + "] ");
                     }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java
index bd7e62a..005b30e 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java
@@ -88,6 +88,6 @@ public class RestUpdateSettingsAction extends BaseRestHandler {
         }
         updateSettingsRequest.settings(updateSettings);
 
-        client.admin().indices().updateSettings(updateSettingsRequest, new AcknowledgedRestListener<>(channel));
+        client.admin().indices().updateSettings(updateSettingsRequest, new AcknowledgedRestListener<UpdateSettingsResponse>(channel));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
index a25754d..5ebec71 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
@@ -41,7 +41,6 @@ import org.elasticsearch.rest.action.support.RestBuilderListener;
 import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 
 import java.util.Map;
 
@@ -89,7 +88,7 @@ public class RestRenderSearchTemplateAction extends BaseRestHandler {
                     throw new ElasticsearchParseException("failed to parse request. unknown field [{}] of type [{}]", currentFieldName, token);
                 }
             }
-            template = new Template(templateId, ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, params);
+            template = new Template(templateId, ScriptType.INDEXED, Template.DEFAULT_LANG, null, params);
         }
         renderSearchTemplateRequest = new RenderSearchTemplateRequest();
         renderSearchTemplateRequest.template(template);
diff --git a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
index 9018435..536b73b 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java
@@ -19,16 +19,11 @@
 
 package org.elasticsearch.rest.action.bulk;
 
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.ActionWriteResponse;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.bulk.BulkItemResponse;
 import org.elasticsearch.action.bulk.BulkRequest;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.bulk.BulkShardRequest;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.action.update.UpdateResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Strings;
@@ -96,52 +91,7 @@ public class RestBulkAction extends BaseRestHandler {
                 builder.startArray(Fields.ITEMS);
                 for (BulkItemResponse itemResponse : response) {
                     builder.startObject();
-                    builder.startObject(itemResponse.getOpType());
-                    builder.field(Fields._INDEX, itemResponse.getIndex());
-                    builder.field(Fields._TYPE, itemResponse.getType());
-                    builder.field(Fields._ID, itemResponse.getId());
-                    long version = itemResponse.getVersion();
-                    if (version != -1) {
-                        builder.field(Fields._VERSION, itemResponse.getVersion());
-                    }
-                    if (itemResponse.isFailed()) {
-                        builder.field(Fields.STATUS, itemResponse.getFailure().getStatus().getStatus());
-                        builder.startObject(Fields.ERROR);
-                        ElasticsearchException.toXContent(builder, request, itemResponse.getFailure().getCause());
-                        builder.endObject();
-                    } else {
-                        ActionWriteResponse.ShardInfo shardInfo = itemResponse.getResponse().getShardInfo();
-                        shardInfo.toXContent(builder, request);
-                        if (itemResponse.getResponse() instanceof DeleteResponse) {
-                            DeleteResponse deleteResponse = itemResponse.getResponse();
-                            if (deleteResponse.isFound()) {
-                                builder.field(Fields.STATUS, shardInfo.status().getStatus());
-                            } else {
-                                builder.field(Fields.STATUS, RestStatus.NOT_FOUND.getStatus());
-                            }
-                            builder.field(Fields.FOUND, deleteResponse.isFound());
-                        } else if (itemResponse.getResponse() instanceof IndexResponse) {
-                            IndexResponse indexResponse = itemResponse.getResponse();
-                            if (indexResponse.isCreated()) {
-                                builder.field(Fields.STATUS, RestStatus.CREATED.getStatus());
-                            } else {
-                                builder.field(Fields.STATUS, shardInfo.status().getStatus());
-                            }
-                        } else if (itemResponse.getResponse() instanceof UpdateResponse) {
-                            UpdateResponse updateResponse = itemResponse.getResponse();
-                            if (updateResponse.isCreated()) {
-                                builder.field(Fields.STATUS, RestStatus.CREATED.getStatus());
-                            } else {
-                                builder.field(Fields.STATUS, shardInfo.status().getStatus());
-                            }
-                            if (updateResponse.getGetResult() != null) {
-                                builder.startObject(Fields.GET);
-                                updateResponse.getGetResult().toXContentEmbedded(builder, request);
-                                builder.endObject();
-                            }
-                        }
-                    }
-                    builder.endObject();
+                    itemResponse.toXContent(builder, request);
                     builder.endObject();
                 }
                 builder.endArray();
@@ -155,15 +105,7 @@ public class RestBulkAction extends BaseRestHandler {
     static final class Fields {
         static final XContentBuilderString ITEMS = new XContentBuilderString("items");
         static final XContentBuilderString ERRORS = new XContentBuilderString("errors");
-        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
-        static final XContentBuilderString _ID = new XContentBuilderString("_id");
-        static final XContentBuilderString STATUS = new XContentBuilderString("status");
-        static final XContentBuilderString ERROR = new XContentBuilderString("error");
         static final XContentBuilderString TOOK = new XContentBuilderString("took");
-        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
-        static final XContentBuilderString FOUND = new XContentBuilderString("found");
-        static final XContentBuilderString GET = new XContentBuilderString("get");
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java b/core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java
index 209ab68..e583ed3 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.rest.action.delete;
 
-import org.elasticsearch.action.ActionWriteResponse;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.delete.DeleteRequest;
 import org.elasticsearch.action.delete.DeleteResponse;
@@ -27,14 +26,13 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
+import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
 
 import static org.elasticsearch.rest.RestRequest.Method.DELETE;
-import static org.elasticsearch.rest.RestStatus.NOT_FOUND;
 
 /**
  *
@@ -62,31 +60,6 @@ public class RestDeleteAction extends BaseRestHandler {
             deleteRequest.consistencyLevel(WriteConsistencyLevel.fromString(consistencyLevel));
         }
 
-        client.delete(deleteRequest, new RestBuilderListener<DeleteResponse>(channel) {
-            @Override
-            public RestResponse buildResponse(DeleteResponse result, XContentBuilder builder) throws Exception {
-                ActionWriteResponse.ShardInfo shardInfo = result.getShardInfo();
-                builder.startObject().field(Fields.FOUND, result.isFound())
-                        .field(Fields._INDEX, result.getIndex())
-                        .field(Fields._TYPE, result.getType())
-                        .field(Fields._ID, result.getId())
-                        .field(Fields._VERSION, result.getVersion())
-                        .value(shardInfo)
-                        .endObject();
-                RestStatus status = shardInfo.status();
-                if (!result.isFound()) {
-                    status = NOT_FOUND;
-                }
-                return new BytesRestResponse(status, builder);
-            }
-        });
-    }
-
-    static final class Fields {
-        static final XContentBuilderString FOUND = new XContentBuilderString("found");
-        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
-        static final XContentBuilderString _ID = new XContentBuilderString("_id");
-        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
+        client.delete(deleteRequest, new RestStatusToXContentListener<>(channel));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java b/core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java
index 1fe0715..c0e45fc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java
@@ -26,7 +26,6 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestResponseListener;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
@@ -77,7 +76,7 @@ public class RestGetSourceAction extends BaseRestHandler {
                 if (!response.isExists()) {
                     return new BytesRestResponse(NOT_FOUND, builder);
                 } else {
-                    XContentHelper.writeDirect(response.getSourceInternal(), builder, request);
+                    builder.rawValue(response.getSourceInternal());
                     return new BytesRestResponse(OK, builder);
                 }
             }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
index c7fc291..310ce0a 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.rest.action.index;
 
-import org.elasticsearch.action.ActionWriteResponse;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.index.IndexRequest;
 import org.elasticsearch.action.index.IndexResponse;
@@ -27,11 +26,11 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
+import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
 
 import java.io.IOException;
 
@@ -99,33 +98,6 @@ public class RestIndexAction extends BaseRestHandler {
         if (consistencyLevel != null) {
             indexRequest.consistencyLevel(WriteConsistencyLevel.fromString(consistencyLevel));
         }
-        client.index(indexRequest, new RestBuilderListener<IndexResponse>(channel) {
-            @Override
-            public RestResponse buildResponse(IndexResponse response, XContentBuilder builder) throws Exception {
-                builder.startObject();
-                ActionWriteResponse.ShardInfo shardInfo = response.getShardInfo();
-                builder.field(Fields._INDEX, response.getIndex())
-                        .field(Fields._TYPE, response.getType())
-                        .field(Fields._ID, response.getId())
-                        .field(Fields._VERSION, response.getVersion());
-                shardInfo.toXContent(builder, request);
-                builder.field(Fields.CREATED, response.isCreated());
-                builder.endObject();
-                RestStatus status = shardInfo.status();
-                if (response.isCreated()) {
-                    status = CREATED;
-                }
-                return new BytesRestResponse(status, builder);
-            }
-        });
+        client.index(indexRequest, new RestStatusToXContentListener<>(channel));
     }
-
-    static final class Fields {
-        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
-        static final XContentBuilderString _ID = new XContentBuilderString("_id");
-        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
-        static final XContentBuilderString CREATED = new XContentBuilderString("created");
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestTable.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestTable.java
index e1c6204..3808e58 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestTable.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestTable.java
@@ -75,20 +75,26 @@ public class RestTable {
 
         BytesStreamOutput bytesOut = channel.bytesOutput();
         UTF8StreamWriter out = new UTF8StreamWriter().setOutput(bytesOut);
+        int lastHeader = headers.size() - 1;
         if (verbose) {
             for (int col = 0; col < headers.size(); col++) {
                 DisplayHeader header = headers.get(col);
-                pad(new Table.Cell(header.display, table.findHeaderByName(header.name)), width[col], request, out);
-                out.append(" ");
+                boolean isLastColumn = col == lastHeader;
+                pad(new Table.Cell(header.display, table.findHeaderByName(header.name)), width[col], request, out, isLastColumn);
+                if (!isLastColumn) {
+                    out.append(" ");
+                }
             }
             out.append("\n");
         }
-
         for (int row = 0; row < table.getRows().size(); row++) {
             for (int col = 0; col < headers.size(); col++) {
                 DisplayHeader header = headers.get(col);
-                pad(table.getAsMap().get(header.name).get(row), width[col], request, out);
-                out.append(" ");
+                boolean isLastColumn = col == lastHeader;
+                pad(table.getAsMap().get(header.name).get(row), width[col], request, out, isLastColumn);
+                if (!isLastColumn) {
+                    out.append(" ");
+                }
             }
             out.append("\n");
         }
@@ -236,6 +242,10 @@ public class RestTable {
     }
 
     public static void pad(Table.Cell cell, int width, RestRequest request, UTF8StreamWriter out) throws IOException {
+      pad(cell, width, request, out, false);
+    }
+
+    public static void pad(Table.Cell cell, int width, RestRequest request, UTF8StreamWriter out, boolean isLast) throws IOException {
         String sValue = renderValue(request, cell.value);
         int length = sValue == null ? 0 : sValue.length();
         byte leftOver = (byte) (width - length);
@@ -254,8 +264,11 @@ public class RestTable {
             if (sValue != null) {
                 out.append(sValue);
             }
-            for (byte i = 0; i < leftOver; i++) {
-                out.append(" ");
+            // Ignores the leftover spaces if the cell is the last of the column.
+            if (!isLast) {
+                for (byte i = 0; i < leftOver; i++) {
+                    out.append(" ");
+                }
             }
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/template/RestDeleteSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/template/RestDeleteSearchTemplateAction.java
index 9b205a8..3d0daf3 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/template/RestDeleteSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/template/RestDeleteSearchTemplateAction.java
@@ -24,7 +24,7 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.rest.RestController;
 import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.script.RestDeleteIndexedScriptAction;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.script.Template;
 
 import static org.elasticsearch.rest.RestRequest.Method.DELETE;
 
@@ -38,6 +38,6 @@ public class RestDeleteSearchTemplateAction extends RestDeleteIndexedScriptActio
 
     @Override
     protected String getScriptLang(RestRequest request) {
-        return MustacheScriptEngineService.NAME;
+        return Template.DEFAULT_LANG;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/rest/action/template/RestGetSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/template/RestGetSearchTemplateAction.java
index 39be6a5..0e8aa35 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/template/RestGetSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/template/RestGetSearchTemplateAction.java
@@ -25,7 +25,7 @@ import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.RestController;
 import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.script.RestGetIndexedScriptAction;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.script.Template;
 
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 
@@ -42,7 +42,7 @@ public class RestGetSearchTemplateAction extends RestGetIndexedScriptAction {
 
     @Override
     protected String getScriptLang(RestRequest request) {
-        return MustacheScriptEngineService.NAME;
+        return Template.DEFAULT_LANG;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/rest/action/template/RestPutSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/template/RestPutSearchTemplateAction.java
index a734ce3..0d23645 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/template/RestPutSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/template/RestPutSearchTemplateAction.java
@@ -23,7 +23,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.script.RestPutIndexedScriptAction;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.script.Template;
 
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.RestRequest.Method.PUT;
@@ -59,6 +59,6 @@ public class RestPutSearchTemplateAction extends RestPutIndexedScriptAction {
 
     @Override
     protected String getScriptLang(RestRequest request) {
-        return MustacheScriptEngineService.NAME;
+        return Template.DEFAULT_LANG;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java b/core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java
index 76e96ab..f59c329 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.rest.action.update;
 
-import org.elasticsearch.action.ActionWriteResponse;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.index.IndexRequest;
 import org.elasticsearch.action.update.UpdateRequest;
@@ -29,7 +28,6 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.BytesRestResponse;
@@ -40,6 +38,7 @@ import org.elasticsearch.rest.RestResponse;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
+import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
@@ -48,7 +47,6 @@ import java.util.HashMap;
 import java.util.Map;
 
 import static org.elasticsearch.rest.RestRequest.Method.POST;
-import static org.elasticsearch.rest.RestStatus.CREATED;
 
 /**
  */
@@ -123,38 +121,6 @@ public class RestUpdateAction extends BaseRestHandler {
             }
         }
 
-        client.update(updateRequest, new RestBuilderListener<UpdateResponse>(channel) {
-            @Override
-            public RestResponse buildResponse(UpdateResponse response, XContentBuilder builder) throws Exception {
-                builder.startObject();
-                ActionWriteResponse.ShardInfo shardInfo = response.getShardInfo();
-                builder.field(Fields._INDEX, response.getIndex())
-                        .field(Fields._TYPE, response.getType())
-                        .field(Fields._ID, response.getId())
-                        .field(Fields._VERSION, response.getVersion());
-
-                shardInfo.toXContent(builder, request);
-                if (response.getGetResult() != null) {
-                    builder.startObject(Fields.GET);
-                    response.getGetResult().toXContentEmbedded(builder, request);
-                    builder.endObject();
-                }
-
-                builder.endObject();
-                RestStatus status = shardInfo.status();
-                if (response.isCreated()) {
-                    status = CREATED;
-                }
-                return new BytesRestResponse(status, builder);
-            }
-        });
-    }
-
-    static final class Fields {
-        static final XContentBuilderString _INDEX = new XContentBuilderString("_index");
-        static final XContentBuilderString _TYPE = new XContentBuilderString("_type");
-        static final XContentBuilderString _ID = new XContentBuilderString("_id");
-        static final XContentBuilderString _VERSION = new XContentBuilderString("_version");
-        static final XContentBuilderString GET = new XContentBuilderString("get");
+        client.update(updateRequest, new RestStatusToXContentListener<>(channel));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptModule.java b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
index 3c19826..f3bdad6 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptModule.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptModule.java
@@ -22,9 +22,7 @@ package org.elasticsearch.script;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.multibindings.MapBinder;
 import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 
 import java.util.ArrayList;
 import java.util.HashMap;
@@ -75,13 +73,6 @@ public class ScriptModule extends AbstractModule {
 
         Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
         multibinder.addBinding().to(NativeScriptEngineService.class);
-        
-        try {
-            Class.forName("com.github.mustachejava.Mustache");
-            multibinder.addBinding().to(MustacheScriptEngineService.class).asEagerSingleton();
-        } catch (Throwable t) {
-            Loggers.getLogger(ScriptService.class, settings).debug("failed to load mustache", t);
-        }
 
         for (Class<? extends ScriptEngineService> scriptEngine : scriptEngines) {
             multibinder.addBinding().to(scriptEngine).asEagerSingleton();
diff --git a/core/src/main/java/org/elasticsearch/script/Template.java b/core/src/main/java/org/elasticsearch/script/Template.java
index 4419d6f..c9bb908 100644
--- a/core/src/main/java/org/elasticsearch/script/Template.java
+++ b/core/src/main/java/org/elasticsearch/script/Template.java
@@ -29,13 +29,15 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 
 import java.io.IOException;
 import java.util.Collections;
 import java.util.Map;
 
 public class Template extends Script {
+    
+    /** Default templating language */
+    public static final String DEFAULT_LANG = "mustache";
 
     private XContentType contentType;
 
@@ -51,7 +53,7 @@ public class Template extends Script {
      *            The inline template.
      */
     public Template(String template) {
-        super(template, MustacheScriptEngineService.NAME);
+        super(template, DEFAULT_LANG);
     }
 
     /**
@@ -73,7 +75,7 @@ public class Template extends Script {
      */
     public Template(String template, ScriptType type, @Nullable String lang, @Nullable XContentType xContentType,
             @Nullable Map<String, Object> params) {
-        super(template, type, lang == null ? MustacheScriptEngineService.NAME : lang, params);
+        super(template, type, lang == null ? DEFAULT_LANG : lang, params);
         this.contentType = xContentType;
     }
 
@@ -120,16 +122,16 @@ public class Template extends Script {
     }
 
     public static Script parse(Map<String, Object> config, boolean removeMatchedEntries, ParseFieldMatcher parseFieldMatcher) {
-        return new TemplateParser(Collections.emptyMap(), MustacheScriptEngineService.NAME).parse(config, removeMatchedEntries, parseFieldMatcher);
+        return new TemplateParser(Collections.emptyMap(), DEFAULT_LANG).parse(config, removeMatchedEntries, parseFieldMatcher);
     }
 
     public static Template parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return new TemplateParser(Collections.emptyMap(), MustacheScriptEngineService.NAME).parse(parser, parseFieldMatcher);
+        return new TemplateParser(Collections.emptyMap(), DEFAULT_LANG).parse(parser, parseFieldMatcher);
     }
 
     @Deprecated
     public static Template parse(XContentParser parser, Map<String, ScriptType> additionalTemplateFieldNames, ParseFieldMatcher parseFieldMatcher) throws IOException {
-        return new TemplateParser(additionalTemplateFieldNames, MustacheScriptEngineService.NAME).parse(parser, parseFieldMatcher);
+        return new TemplateParser(additionalTemplateFieldNames, DEFAULT_LANG).parse(parser, parseFieldMatcher);
     }
 
     @Deprecated
@@ -172,7 +174,7 @@ public class Template extends Script {
 
         @Override
         protected Template createSimpleScript(XContentParser parser) throws IOException {
-            return new Template(String.valueOf(parser.objectText()), ScriptType.INLINE, MustacheScriptEngineService.NAME, contentType, null);
+            return new Template(String.valueOf(parser.objectText()), ScriptType.INLINE, DEFAULT_LANG, contentType, null);
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java b/core/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
deleted file mode 100644
index 7734d03..0000000
--- a/core/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import com.fasterxml.jackson.core.io.JsonStringEncoder;
-import com.github.mustachejava.DefaultMustacheFactory;
-import com.github.mustachejava.MustacheException;
-
-import java.io.IOException;
-import java.io.Writer;
-
-/**
- * A MustacheFactory that does simple JSON escaping.
- */
-public final class JsonEscapingMustacheFactory extends DefaultMustacheFactory {
-    
-    @Override
-    public void encode(String value, Writer writer) {
-        try {
-            JsonStringEncoder utils = new JsonStringEncoder();
-            writer.write(utils.quoteAsString(value));;
-        } catch (IOException e) {
-            throw new MustacheException("Failed to encode value: " + value);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
deleted file mode 100644
index 3affd0c..0000000
--- a/core/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
+++ /dev/null
@@ -1,166 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import com.github.mustachejava.Mustache;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.component.AbstractComponent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.FastStringReader;
-import org.elasticsearch.common.io.UTF8StreamWriter;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptEngineService;
-import org.elasticsearch.script.ScriptException;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.lang.ref.SoftReference;
-import java.util.Collections;
-import java.util.Map;
-
-/**
- * Main entry point handling template registration, compilation and
- * execution.
- *
- * Template handling is based on Mustache. Template handling is a two step
- * process: First compile the string representing the template, the resulting
- * {@link Mustache} object can then be re-used for subsequent executions.
- */
-public class MustacheScriptEngineService extends AbstractComponent implements ScriptEngineService {
-
-    public static final String NAME = "mustache";
-
-    /** Thread local UTF8StreamWriter to store template execution results in, thread local to save object creation.*/
-    private static ThreadLocal<SoftReference<UTF8StreamWriter>> utf8StreamWriter = new ThreadLocal<>();
-
-    /** If exists, reset and return, otherwise create, reset and return a writer.*/
-    private static UTF8StreamWriter utf8StreamWriter() {
-        SoftReference<UTF8StreamWriter> ref = utf8StreamWriter.get();
-        UTF8StreamWriter writer = (ref == null) ? null : ref.get();
-        if (writer == null) {
-            writer = new UTF8StreamWriter(1024 * 4);
-            utf8StreamWriter.set(new SoftReference<>(writer));
-        }
-        writer.reset();
-        return writer;
-    }
-
-    /**
-     * @param settings automatically wired by Guice.
-     * */
-    @Inject
-    public MustacheScriptEngineService(Settings settings) {
-        super(settings);
-    }
-
-    /**
-     * Compile a template string to (in this case) a Mustache object than can
-     * later be re-used for execution to fill in missing parameter values.
-     *
-     * @param template
-     *            a string representing the template to compile.
-     * @return a compiled template object for later execution.
-     * */
-    @Override
-    public Object compile(String template) {
-        /** Factory to generate Mustache objects from. */
-        return (new JsonEscapingMustacheFactory()).compile(new FastStringReader(template), "query-template");
-    }
-
-    @Override
-    public String[] types() {
-        return new String[] {NAME};
-    }
-
-    @Override
-    public String[] extensions() {
-        return new String[] {NAME};
-    }
-
-    @Override
-    public boolean sandboxed() {
-        return true;
-    }
-
-    @Override
-    public ExecutableScript executable(CompiledScript compiledScript,
-            @Nullable Map<String, Object> vars) {
-        return new MustacheExecutableScript(compiledScript, vars);
-    }
-
-    @Override
-    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup,
-            @Nullable Map<String, Object> vars) {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void close() {
-        // Nothing to do here
-    }
-
-    @Override
-    public void scriptRemoved(CompiledScript script) {
-        // Nothing to do here
-    }
-
-    /**
-     * Used at query execution time by script service in order to execute a query template.
-     * */
-    private class MustacheExecutableScript implements ExecutableScript {
-        /** Compiled template object wrapper. */
-        private CompiledScript template;
-        /** Parameters to fill above object with. */
-        private Map<String, Object> vars;
-
-        /**
-         * @param template the compiled template object wrapper
-         * @param vars the parameters to fill above object with
-         **/
-        public MustacheExecutableScript(CompiledScript template, Map<String, Object> vars) {
-            this.template = template;
-            this.vars = vars == null ? Collections.<String, Object>emptyMap() : vars;
-        }
-
-        @Override
-        public void setNextVar(String name, Object value) {
-            this.vars.put(name, value);
-        }
-
-        @Override
-        public Object run() {
-            BytesStreamOutput result = new BytesStreamOutput();
-            try (UTF8StreamWriter writer = utf8StreamWriter().setOutput(result)) {
-                ((Mustache) template.compiled()).execute(writer, vars);
-            } catch (Exception e) {
-                logger.error("Error running " + template, e);
-                throw new ScriptException("Error running " + template, e);
-            }
-            return result.bytes();
-        }
-
-        @Override
-        public Object unwrap(Object value) {
-            return value;
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index c416ec4..9501099 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -38,8 +38,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.BigArrays;
@@ -72,6 +70,7 @@ import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.IndicesWarmer.TerminationHandle;
 import org.elasticsearch.indices.cache.request.IndicesRequestCache;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.script.ExecutableScript;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
@@ -110,10 +109,9 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
     public static final String NORMS_LOADING_KEY = "index.norms.loading";
     public static final String DEFAULT_KEEPALIVE_KEY = "search.default_keep_alive";
     public static final String KEEPALIVE_INTERVAL_KEY = "search.keep_alive_interval";
+    public static final String DEFAULT_SEARCH_TIMEOUT = "search.default_search_timeout";
 
     public static final TimeValue NO_TIMEOUT = timeValueMillis(-1);
-    public static final Setting<TimeValue> DEFAULT_SEARCH_TIMEOUT_SETTING = Setting.timeSetting("search.default_search_timeout", NO_TIMEOUT, true, Setting.Scope.CLUSTER);
-
 
     private final ThreadPool threadPool;
 
@@ -152,7 +150,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
     private final ParseFieldMatcher parseFieldMatcher;
 
     @Inject
-    public SearchService(Settings settings, ClusterSettings clusterSettings, ClusterService clusterService, IndicesService indicesService, IndicesWarmer indicesWarmer, ThreadPool threadPool,
+    public SearchService(Settings settings, NodeSettingsService nodeSettingsService, ClusterService clusterService, IndicesService indicesService,IndicesWarmer indicesWarmer, ThreadPool threadPool,
                          ScriptService scriptService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays, DfsPhase dfsPhase, QueryPhase queryPhase, FetchPhase fetchPhase,
                          IndicesRequestCache indicesQueryCache) {
         super(settings);
@@ -186,12 +184,19 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> imp
         this.indicesWarmer.addListener(new FieldDataWarmer(indicesWarmer));
         this.indicesWarmer.addListener(new SearchWarmer());
 
-        defaultSearchTimeout = DEFAULT_SEARCH_TIMEOUT_SETTING.get(settings);
-        clusterSettings.addSettingsUpdateConsumer(DEFAULT_SEARCH_TIMEOUT_SETTING, this::setDefaultSearchTimeout);
+        defaultSearchTimeout = settings.getAsTime(DEFAULT_SEARCH_TIMEOUT, NO_TIMEOUT);
+        nodeSettingsService.addListener(new SearchSettingsListener());
     }
 
-    private void setDefaultSearchTimeout(TimeValue defaultSearchTimeout) {
-        this.defaultSearchTimeout = defaultSearchTimeout;
+    class SearchSettingsListener implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            final TimeValue maybeNewDefaultSearchTimeout = settings.getAsTime(SearchService.DEFAULT_SEARCH_TIMEOUT, SearchService.this.defaultSearchTimeout);
+            if (!maybeNewDefaultSearchTimeout.equals(SearchService.this.defaultSearchTimeout)) {
+                logger.info("updating [{}] from [{}] to [{}]", SearchService.DEFAULT_SEARCH_TIMEOUT, SearchService.this.defaultSearchTimeout, maybeNewDefaultSearchTimeout);
+                SearchService.this.defaultSearchTimeout = maybeNewDefaultSearchTimeout;
+            }
+        }
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggegator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggegator.java
deleted file mode 100644
index 5cc7ddb..0000000
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggegator.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.search.aggregations.metrics.stats;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.util.BigArrays;
-import org.elasticsearch.common.util.DoubleArray;
-import org.elasticsearch.common.util.LongArray;
-import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
-import org.elasticsearch.search.aggregations.Aggregator;
-import org.elasticsearch.search.aggregations.InternalAggregation;
-import org.elasticsearch.search.aggregations.LeafBucketCollector;
-import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
-import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
-import org.elasticsearch.search.aggregations.support.AggregationContext;
-import org.elasticsearch.search.aggregations.support.ValuesSource;
-import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
-import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
-import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-/**
- *
- */
-public class StatsAggegator extends NumericMetricsAggregator.MultiValue {
-
-    final ValuesSource.Numeric valuesSource;
-    final ValueFormatter formatter;
-
-    LongArray counts;
-    DoubleArray sums;
-    DoubleArray mins;
-    DoubleArray maxes;
-
-
-    public StatsAggegator(String name, ValuesSource.Numeric valuesSource, ValueFormatter formatter,
- AggregationContext context,
-            Aggregator parent, List<PipelineAggregator> pipelineAggregators,
-            Map<String, Object> metaData) throws IOException {
-        super(name, context, parent, pipelineAggregators, metaData);
-        this.valuesSource = valuesSource;
-        if (valuesSource != null) {
-            final BigArrays bigArrays = context.bigArrays();
-            counts = bigArrays.newLongArray(1, true);
-            sums = bigArrays.newDoubleArray(1, true);
-            mins = bigArrays.newDoubleArray(1, false);
-            mins.fill(0, mins.size(), Double.POSITIVE_INFINITY);
-            maxes = bigArrays.newDoubleArray(1, false);
-            maxes.fill(0, maxes.size(), Double.NEGATIVE_INFINITY);
-        }
-        this.formatter = formatter;
-    }
-
-    @Override
-    public boolean needsScores() {
-        return valuesSource != null && valuesSource.needsScores();
-    }
-
-    @Override
-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,
-            final LeafBucketCollector sub) throws IOException {
-        if (valuesSource == null) {
-            return LeafBucketCollector.NO_OP_COLLECTOR;
-        }
-        final BigArrays bigArrays = context.bigArrays();
-        final SortedNumericDoubleValues values = valuesSource.doubleValues(ctx);
-        return new LeafBucketCollectorBase(sub, values) {
-            @Override
-            public void collect(int doc, long bucket) throws IOException {
-                if (bucket >= counts.size()) {
-                    final long from = counts.size();
-                    final long overSize = BigArrays.overSize(bucket + 1);
-                    counts = bigArrays.resize(counts, overSize);
-                    sums = bigArrays.resize(sums, overSize);
-                    mins = bigArrays.resize(mins, overSize);
-                    maxes = bigArrays.resize(maxes, overSize);
-                    mins.fill(from, overSize, Double.POSITIVE_INFINITY);
-                    maxes.fill(from, overSize, Double.NEGATIVE_INFINITY);
-                }
-
-                values.setDocument(doc);
-                final int valuesCount = values.count();
-                counts.increment(bucket, valuesCount);
-                double sum = 0;
-                double min = mins.get(bucket);
-                double max = maxes.get(bucket);
-                for (int i = 0; i < valuesCount; i++) {
-                    double value = values.valueAt(i);
-                    sum += value;
-                    min = Math.min(min, value);
-                    max = Math.max(max, value);
-                }
-                sums.increment(bucket, sum);
-                mins.set(bucket, min);
-                maxes.set(bucket, max);
-            }
-        };
-    }
-
-    @Override
-    public boolean hasMetric(String name) {
-        try {
-            InternalStats.Metrics.resolve(name);
-            return true;
-        } catch (IllegalArgumentException iae) {
-            return false;
-        }
-    }
-
-    @Override
-    public double metric(String name, long owningBucketOrd) {
-        switch(InternalStats.Metrics.resolve(name)) {
-            case count: return valuesSource == null ? 0 : counts.get(owningBucketOrd);
-            case sum: return valuesSource == null ? 0 : sums.get(owningBucketOrd);
-            case min: return valuesSource == null ? Double.POSITIVE_INFINITY : mins.get(owningBucketOrd);
-            case max: return valuesSource == null ? Double.NEGATIVE_INFINITY : maxes.get(owningBucketOrd);
-            case avg: return valuesSource == null ? Double.NaN : sums.get(owningBucketOrd) / counts.get(owningBucketOrd);
-            default:
-                throw new IllegalArgumentException("Unknown value [" + name + "] in common stats aggregation");
-        }
-    }
-
-    @Override
-    public InternalAggregation buildAggregation(long bucket) {
-        if (valuesSource == null || bucket >= sums.size()) {
-            return buildEmptyAggregation();
-        }
-        return new InternalStats(name, counts.get(bucket), sums.get(bucket), mins.get(bucket),
-                maxes.get(bucket), formatter, pipelineAggregators(), metaData());
-    }
-
-    @Override
-    public InternalAggregation buildEmptyAggregation() {
-        return new InternalStats(name, 0, 0, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
-    }
-
-    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
-
-        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
-            super(name, InternalStats.TYPE.name(), valuesSourceConfig);
-        }
-
-        @Override
-        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
-                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
-            return new StatsAggegator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-        }
-
-        @Override
-        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
-                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
-                throws IOException {
-            return new StatsAggegator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
-        }
-    }
-
-    @Override
-    public void doClose() {
-        Releasables.close(counts, maxes, mins, sums);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
new file mode 100644
index 0000000..6e648cb
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggregator.java
@@ -0,0 +1,182 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.search.aggregations.metrics.stats;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.elasticsearch.common.lease.Releasables;
+import org.elasticsearch.common.util.BigArrays;
+import org.elasticsearch.common.util.DoubleArray;
+import org.elasticsearch.common.util.LongArray;
+import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
+import org.elasticsearch.search.aggregations.Aggregator;
+import org.elasticsearch.search.aggregations.InternalAggregation;
+import org.elasticsearch.search.aggregations.LeafBucketCollector;
+import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;
+import org.elasticsearch.search.aggregations.metrics.NumericMetricsAggregator;
+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;
+import org.elasticsearch.search.aggregations.support.AggregationContext;
+import org.elasticsearch.search.aggregations.support.ValuesSource;
+import org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory;
+import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
+import org.elasticsearch.search.aggregations.support.format.ValueFormatter;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+/**
+ *
+ */
+public class StatsAggregator extends NumericMetricsAggregator.MultiValue {
+
+    final ValuesSource.Numeric valuesSource;
+    final ValueFormatter formatter;
+
+    LongArray counts;
+    DoubleArray sums;
+    DoubleArray mins;
+    DoubleArray maxes;
+
+
+    public StatsAggregator(String name, ValuesSource.Numeric valuesSource, ValueFormatter formatter,
+                           AggregationContext context,
+                           Aggregator parent, List<PipelineAggregator> pipelineAggregators,
+                           Map<String, Object> metaData) throws IOException {
+        super(name, context, parent, pipelineAggregators, metaData);
+        this.valuesSource = valuesSource;
+        if (valuesSource != null) {
+            final BigArrays bigArrays = context.bigArrays();
+            counts = bigArrays.newLongArray(1, true);
+            sums = bigArrays.newDoubleArray(1, true);
+            mins = bigArrays.newDoubleArray(1, false);
+            mins.fill(0, mins.size(), Double.POSITIVE_INFINITY);
+            maxes = bigArrays.newDoubleArray(1, false);
+            maxes.fill(0, maxes.size(), Double.NEGATIVE_INFINITY);
+        }
+        this.formatter = formatter;
+    }
+
+    @Override
+    public boolean needsScores() {
+        return valuesSource != null && valuesSource.needsScores();
+    }
+
+    @Override
+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,
+            final LeafBucketCollector sub) throws IOException {
+        if (valuesSource == null) {
+            return LeafBucketCollector.NO_OP_COLLECTOR;
+        }
+        final BigArrays bigArrays = context.bigArrays();
+        final SortedNumericDoubleValues values = valuesSource.doubleValues(ctx);
+        return new LeafBucketCollectorBase(sub, values) {
+            @Override
+            public void collect(int doc, long bucket) throws IOException {
+                if (bucket >= counts.size()) {
+                    final long from = counts.size();
+                    final long overSize = BigArrays.overSize(bucket + 1);
+                    counts = bigArrays.resize(counts, overSize);
+                    sums = bigArrays.resize(sums, overSize);
+                    mins = bigArrays.resize(mins, overSize);
+                    maxes = bigArrays.resize(maxes, overSize);
+                    mins.fill(from, overSize, Double.POSITIVE_INFINITY);
+                    maxes.fill(from, overSize, Double.NEGATIVE_INFINITY);
+                }
+
+                values.setDocument(doc);
+                final int valuesCount = values.count();
+                counts.increment(bucket, valuesCount);
+                double sum = 0;
+                double min = mins.get(bucket);
+                double max = maxes.get(bucket);
+                for (int i = 0; i < valuesCount; i++) {
+                    double value = values.valueAt(i);
+                    sum += value;
+                    min = Math.min(min, value);
+                    max = Math.max(max, value);
+                }
+                sums.increment(bucket, sum);
+                mins.set(bucket, min);
+                maxes.set(bucket, max);
+            }
+        };
+    }
+
+    @Override
+    public boolean hasMetric(String name) {
+        try {
+            InternalStats.Metrics.resolve(name);
+            return true;
+        } catch (IllegalArgumentException iae) {
+            return false;
+        }
+    }
+
+    @Override
+    public double metric(String name, long owningBucketOrd) {
+        switch(InternalStats.Metrics.resolve(name)) {
+            case count: return valuesSource == null ? 0 : counts.get(owningBucketOrd);
+            case sum: return valuesSource == null ? 0 : sums.get(owningBucketOrd);
+            case min: return valuesSource == null ? Double.POSITIVE_INFINITY : mins.get(owningBucketOrd);
+            case max: return valuesSource == null ? Double.NEGATIVE_INFINITY : maxes.get(owningBucketOrd);
+            case avg: return valuesSource == null ? Double.NaN : sums.get(owningBucketOrd) / counts.get(owningBucketOrd);
+            default:
+                throw new IllegalArgumentException("Unknown value [" + name + "] in common stats aggregation");
+        }
+    }
+
+    @Override
+    public InternalAggregation buildAggregation(long bucket) {
+        if (valuesSource == null || bucket >= sums.size()) {
+            return buildEmptyAggregation();
+        }
+        return new InternalStats(name, counts.get(bucket), sums.get(bucket), mins.get(bucket),
+                maxes.get(bucket), formatter, pipelineAggregators(), metaData());
+    }
+
+    @Override
+    public InternalAggregation buildEmptyAggregation() {
+        return new InternalStats(name, 0, 0, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, formatter, pipelineAggregators(), metaData());
+    }
+
+    public static class Factory extends ValuesSourceAggregatorFactory.LeafOnly<ValuesSource.Numeric> {
+
+        public Factory(String name, ValuesSourceConfig<ValuesSource.Numeric> valuesSourceConfig) {
+            super(name, InternalStats.TYPE.name(), valuesSourceConfig);
+        }
+
+        @Override
+        protected Aggregator createUnmapped(AggregationContext aggregationContext, Aggregator parent,
+                List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData) throws IOException {
+            return new StatsAggregator(name, null, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
+        }
+
+        @Override
+        protected Aggregator doCreateInternal(ValuesSource.Numeric valuesSource, AggregationContext aggregationContext, Aggregator parent,
+                boolean collectsFromSingleBucket, List<PipelineAggregator> pipelineAggregators, Map<String, Object> metaData)
+                throws IOException {
+            return new StatsAggregator(name, valuesSource, config.formatter(), aggregationContext, parent, pipelineAggregators, metaData);
+        }
+    }
+
+    @Override
+    public void doClose() {
+        Releasables.close(counts, maxes, mins, sums);
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
index 5ec9b2a..86c85e4 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java
@@ -34,6 +34,6 @@ public class StatsParser extends NumericValuesSourceMetricsAggregatorParser<Inte
 
     @Override
     protected AggregatorFactory createFactory(String aggregationName, ValuesSourceConfig<ValuesSource.Numeric> config) {
-        return new StatsAggegator.Factory(aggregationName, config);
+        return new StatsAggregator.Factory(aggregationName, config);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/AbstractHighlighterBuilder.java b/core/src/main/java/org/elasticsearch/search/highlight/AbstractHighlighterBuilder.java
index b10e2e8..12fb987 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/AbstractHighlighterBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/AbstractHighlighterBuilder.java
@@ -21,13 +21,18 @@ package org.elasticsearch.search.highlight;
 
 import org.apache.lucene.search.highlight.SimpleFragmenter;
 import org.apache.lucene.search.highlight.SimpleSpanFragmenter;
+import org.elasticsearch.common.ParseField;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryBuilder;
-
+import org.elasticsearch.index.query.QueryParseContext;
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.List;
 import java.util.Map;
 import java.util.Objects;
 
@@ -35,7 +40,29 @@ import java.util.Objects;
  * This abstract class holds parameters shared by {@link HighlightBuilder} and {@link HighlightBuilder.Field}
  * and provides the common setters, equality, hashCode calculation and common serialization
  */
-public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterBuilder> {
+public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterBuilder<?>> {
+
+    public static final ParseField PRE_TAGS_FIELD = new ParseField("pre_tags");
+    public static final ParseField POST_TAGS_FIELD = new ParseField("post_tags");
+    public static final ParseField FIELDS_FIELD = new ParseField("fields");
+    public static final ParseField ORDER_FIELD = new ParseField("order");
+    public static final ParseField TAGS_SCHEMA_FIELD = new ParseField("tags_schema");
+    public static final ParseField HIGHLIGHT_FILTER_FIELD = new ParseField("highlight_filter");
+    public static final ParseField FRAGMENT_SIZE_FIELD = new ParseField("fragment_size");
+    public static final ParseField FRAGMENT_OFFSET_FIELD = new ParseField("fragment_offset");
+    public static final ParseField NUMBER_OF_FRAGMENTS_FIELD = new ParseField("number_of_fragments");
+    public static final ParseField ENCODER_FIELD = new ParseField("encoder");
+    public static final ParseField REQUIRE_FIELD_MATCH_FIELD = new ParseField("require_field_match");
+    public static final ParseField BOUNDARY_MAX_SCAN_FIELD = new ParseField("boundary_max_scan");
+    public static final ParseField BOUNDARY_CHARS_FIELD = new ParseField("boundary_chars");
+    public static final ParseField TYPE_FIELD = new ParseField("type");
+    public static final ParseField FRAGMENTER_FIELD = new ParseField("fragmenter");
+    public static final ParseField NO_MATCH_SIZE_FIELD = new ParseField("no_match_size");
+    public static final ParseField FORCE_SOURCE_FIELD = new ParseField("force_source");
+    public static final ParseField PHRASE_LIMIT_FIELD = new ParseField("phrase_limit");
+    public static final ParseField OPTIONS_FIELD = new ParseField("options");
+    public static final ParseField HIGHLIGHT_QUERY_FIELD = new ParseField("highlight_query");
+    public static final ParseField MATCHED_FIELDS_FIELD = new ParseField("matched_fields");
 
     protected String[] preTags;
 
@@ -49,7 +76,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
 
     protected String fragmenter;
 
-    protected QueryBuilder highlightQuery;
+    protected QueryBuilder<?> highlightQuery;
 
     protected String order;
 
@@ -102,7 +129,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     }
 
     /**
-     * Set the fragment size in characters, defaults to {@link HighlighterParseElement#DEFAULT_FRAGMENT_CHAR_SIZE}
+     * Set the fragment size in characters, defaults to {@link HighlightBuilder#DEFAULT_FRAGMENT_CHAR_SIZE}
      */
     @SuppressWarnings("unchecked")
     public HB fragmentSize(Integer fragmentSize) {
@@ -118,7 +145,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     }
 
     /**
-     * Set the number of fragments, defaults to {@link HighlighterParseElement#DEFAULT_NUMBER_OF_FRAGMENTS}
+     * Set the number of fragments, defaults to {@link HighlightBuilder#DEFAULT_NUMBER_OF_FRAGMENTS}
      */
     @SuppressWarnings("unchecked")
     public HB numOfFragments(Integer numOfFragments) {
@@ -175,7 +202,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
      * Sets a query to be used for highlighting instead of the search query.
      */
     @SuppressWarnings("unchecked")
-    public HB highlightQuery(QueryBuilder highlightQuery) {
+    public HB highlightQuery(QueryBuilder<?> highlightQuery) {
         this.highlightQuery = highlightQuery;
         return (HB) this;
     }
@@ -183,7 +210,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     /**
      * @return the value set by {@link #highlightQuery(QueryBuilder)}
      */
-    public QueryBuilder highlightQuery() {
+    public QueryBuilder<?> highlightQuery() {
         return this.highlightQuery;
     }
 
@@ -347,55 +374,149 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
 
     void commonOptionsToXContent(XContentBuilder builder) throws IOException {
         if (preTags != null) {
-            builder.array("pre_tags", preTags);
+            builder.array(PRE_TAGS_FIELD.getPreferredName(), preTags);
         }
         if (postTags != null) {
-            builder.array("post_tags", postTags);
+            builder.array(POST_TAGS_FIELD.getPreferredName(), postTags);
         }
         if (fragmentSize != null) {
-            builder.field("fragment_size", fragmentSize);
+            builder.field(FRAGMENT_SIZE_FIELD.getPreferredName(), fragmentSize);
         }
         if (numOfFragments != null) {
-            builder.field("number_of_fragments", numOfFragments);
+            builder.field(NUMBER_OF_FRAGMENTS_FIELD.getPreferredName(), numOfFragments);
         }
         if (highlighterType != null) {
-            builder.field("type", highlighterType);
+            builder.field(TYPE_FIELD.getPreferredName(), highlighterType);
         }
         if (fragmenter != null) {
-            builder.field("fragmenter", fragmenter);
+            builder.field(FRAGMENTER_FIELD.getPreferredName(), fragmenter);
         }
         if (highlightQuery != null) {
-            builder.field("highlight_query", highlightQuery);
+            builder.field(HIGHLIGHT_QUERY_FIELD.getPreferredName(), highlightQuery);
         }
         if (order != null) {
-            builder.field("order", order);
+            builder.field(ORDER_FIELD.getPreferredName(), order);
         }
         if (highlightFilter != null) {
-            builder.field("highlight_filter", highlightFilter);
+            builder.field(HIGHLIGHT_FILTER_FIELD.getPreferredName(), highlightFilter);
         }
         if (boundaryMaxScan != null) {
-            builder.field("boundary_max_scan", boundaryMaxScan);
+            builder.field(BOUNDARY_MAX_SCAN_FIELD.getPreferredName(), boundaryMaxScan);
         }
         if (boundaryChars != null) {
-            builder.field("boundary_chars", boundaryChars);
+            builder.field(BOUNDARY_CHARS_FIELD.getPreferredName(), new String(boundaryChars));
         }
         if (options != null && options.size() > 0) {
-            builder.field("options", options);
+            builder.field(OPTIONS_FIELD.getPreferredName(), options);
         }
         if (forceSource != null) {
-            builder.field("force_source", forceSource);
+            builder.field(FORCE_SOURCE_FIELD.getPreferredName(), forceSource);
         }
         if (requireFieldMatch != null) {
-            builder.field("require_field_match", requireFieldMatch);
+            builder.field(REQUIRE_FIELD_MATCH_FIELD.getPreferredName(), requireFieldMatch);
         }
         if (noMatchSize != null) {
-            builder.field("no_match_size", noMatchSize);
+            builder.field(NO_MATCH_SIZE_FIELD.getPreferredName(), noMatchSize);
         }
         if (phraseLimit != null) {
-            builder.field("phrase_limit", phraseLimit);
+            builder.field(PHRASE_LIMIT_FIELD.getPreferredName(), phraseLimit);
+        }
+    }
+
+    /**
+     * Creates a new {@link HighlightBuilder} from the highlighter held by the {@link QueryParseContext}
+     * in {@link org.elasticsearch.common.xcontent.XContent} format
+     *
+     * @param parseContext containing the parser positioned at the structure to be parsed from.
+     * the state on the parser contained in this context will be changed as a side effect of this
+     * method call
+     * @return the new {@link AbstractHighlighterBuilder}
+     */
+    public HB fromXContent(QueryParseContext parseContext) throws IOException {
+        XContentParser parser = parseContext.parser();
+        XContentParser.Token token = parser.currentToken();
+        String currentFieldName = null;
+        HB highlightBuilder = createInstance(parser);
+        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+            if (token == XContentParser.Token.FIELD_NAME) {
+                currentFieldName = parser.currentName();
+            } else if (token == XContentParser.Token.START_ARRAY) {
+                if (parseContext.parseFieldMatcher().match(currentFieldName, PRE_TAGS_FIELD)) {
+                    List<String> preTagsList = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        preTagsList.add(parser.text());
+                    }
+                    highlightBuilder.preTags(preTagsList.toArray(new String[preTagsList.size()]));
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, POST_TAGS_FIELD)) {
+                    List<String> postTagsList = new ArrayList<>();
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                        postTagsList.add(parser.text());
+                    }
+                    highlightBuilder.postTags(postTagsList.toArray(new String[postTagsList.size()]));
+                } else if (false == highlightBuilder.doFromXContent(parseContext, currentFieldName, token)) {
+                    throw new ParsingException(parser.getTokenLocation(), "cannot parse array with name [{}]", currentFieldName);
+                }
+            } else if (token.isValue()) {
+                if (parseContext.parseFieldMatcher().match(currentFieldName, ORDER_FIELD)) {
+                    highlightBuilder.order(parser.text());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, HIGHLIGHT_FILTER_FIELD)) {
+                    highlightBuilder.highlightFilter(parser.booleanValue());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, FRAGMENT_SIZE_FIELD)) {
+                    highlightBuilder.fragmentSize(parser.intValue());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, NUMBER_OF_FRAGMENTS_FIELD)) {
+                    highlightBuilder.numOfFragments(parser.intValue());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, REQUIRE_FIELD_MATCH_FIELD)) {
+                    highlightBuilder.requireFieldMatch(parser.booleanValue());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, BOUNDARY_MAX_SCAN_FIELD)) {
+                    highlightBuilder.boundaryMaxScan(parser.intValue());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, BOUNDARY_CHARS_FIELD)) {
+                    highlightBuilder.boundaryChars(parser.text().toCharArray());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, TYPE_FIELD)) {
+                    highlightBuilder.highlighterType(parser.text());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, FRAGMENTER_FIELD)) {
+                    highlightBuilder.fragmenter(parser.text());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, NO_MATCH_SIZE_FIELD)) {
+                    highlightBuilder.noMatchSize(parser.intValue());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, FORCE_SOURCE_FIELD)) {
+                    highlightBuilder.forceSource(parser.booleanValue());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, PHRASE_LIMIT_FIELD)) {
+                    highlightBuilder.phraseLimit(parser.intValue());
+                } else if (false == highlightBuilder.doFromXContent(parseContext, currentFieldName, token)) {
+                    throw new ParsingException(parser.getTokenLocation(), "unexpected fieldname [{}]", currentFieldName);
+                }
+            } else if (token == XContentParser.Token.START_OBJECT && currentFieldName != null) {
+                if (parseContext.parseFieldMatcher().match(currentFieldName, OPTIONS_FIELD)) {
+                    highlightBuilder.options(parser.map());
+                } else if (parseContext.parseFieldMatcher().match(currentFieldName, HIGHLIGHT_QUERY_FIELD)) {
+                    highlightBuilder.highlightQuery(parseContext.parseInnerQueryBuilder());
+                } else if (false == highlightBuilder.doFromXContent(parseContext, currentFieldName, token)) {
+                    throw new ParsingException(parser.getTokenLocation(), "cannot parse object with name [{}]", currentFieldName);
+                }
+            } else if (currentFieldName != null) {
+                throw new ParsingException(parser.getTokenLocation(), "unexpected token [{}] after [{}]", token, currentFieldName);
+            }
+        }
+
+        if (highlightBuilder.preTags() != null && highlightBuilder.postTags() == null) {
+            throw new ParsingException(parser.getTokenLocation(), "Highlighter global preTags are set, but global postTags are not set");
         }
+        return highlightBuilder;
     }
 
+    /**
+     * @param parser the input parser. Implementing classes might advance the parser depending on the
+     * information they need to instantiate a new instance
+     * @return a new instance
+     */
+    protected abstract HB createInstance(XContentParser parser) throws IOException;
+
+    /**
+     * Implementing subclasses can handle parsing special options depending on the
+     * current token, field name and the parse context.
+     * @return <tt>true</tt> if an option was found and successfully parsed, otherwise <tt>false</tt>
+     */
+    protected abstract boolean doFromXContent(QueryParseContext parseContext, String currentFieldName, XContentParser.Token endMarkerToken) throws IOException;
+
     @Override
     public final int hashCode() {
         return Objects.hash(getClass(), Arrays.hashCode(preTags), Arrays.hashCode(postTags), fragmentSize,
@@ -405,7 +526,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     }
 
     /**
-     * internal hashCode calculation to overwrite for the implementing classes.
+     * fields only present in subclass should contribute to hashCode in the implementation
      */
     protected abstract int doHashCode();
 
@@ -439,7 +560,7 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
     }
 
     /**
-     * internal equals to overwrite for the implementing classes.
+     * fields only present in subclass should be checked for equality in the implementation
      */
     protected abstract boolean doEquals(HB other);
 
@@ -506,4 +627,4 @@ public abstract class AbstractHighlighterBuilder<HB extends AbstractHighlighterB
         }
         out.writeOptionalBoolean(requireFieldMatch);
     }
-}
\ No newline at end of file
+}
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
index dbae661..4b62734 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
@@ -19,19 +19,33 @@
 
 package org.elasticsearch.search.highlight;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.vectorhighlight.SimpleBoundaryScanner;
 import org.elasticsearch.ExceptionsHelper;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.search.highlight.SearchContextHighlight.FieldOptions;
+import org.elasticsearch.search.highlight.SearchContextHighlight.FieldOptions.Builder;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Objects;
+import java.util.Set;
 
 /**
  * A builder for search highlighting. Settings can control how large fields
@@ -43,6 +57,53 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
 
     public static final HighlightBuilder PROTOTYPE = new HighlightBuilder();
 
+    public static final String HIGHLIGHT_ELEMENT_NAME = "highlight";
+
+    /** default for whether to highlight fields based on the source even if stored separately */
+    public static final boolean DEFAULT_FORCE_SOURCE = false;
+    /** default for whether a field should be highlighted only if a query matches that field */
+    public static final boolean DEFAULT_REQUIRE_FIELD_MATCH = true;
+    /** default for whether <tt>fvh</tt> should provide highlighting on filter clauses */
+    public static final boolean DEFAULT_HIGHLIGHT_FILTER = false;
+    /** default for highlight fragments being ordered by score */
+    public static final boolean DEFAULT_SCORE_ORDERED = false;
+    /** the default encoder setting */
+    public static final String DEFAULT_ENCODER = "default";
+    /** default for the maximum number of phrases the fvh will consider */
+    public static final int DEFAULT_PHRASE_LIMIT = 256;
+    /** default for fragment size when there are no matches */
+    public static final int DEFAULT_NO_MATCH_SIZE = 0;
+    /** the default number of fragments for highlighting */
+    public static final int DEFAULT_NUMBER_OF_FRAGMENTS = 5;
+    /** the default number of fragments size in characters */
+    public static final int DEFAULT_FRAGMENT_CHAR_SIZE = 100;
+    /** the default opening tag  */
+    public static final String[] DEFAULT_PRE_TAGS = new String[]{"<em>"};
+    /** the default closing tag  */
+    public static final String[] DEFAULT_POST_TAGS = new String[]{"</em>"};
+
+    /** the default opening tags when <tt>tag_schema = "styled"</tt>  */
+    public static final String[] DEFAULT_STYLED_PRE_TAG = {
+            "<em class=\"hlt1\">", "<em class=\"hlt2\">", "<em class=\"hlt3\">",
+            "<em class=\"hlt4\">", "<em class=\"hlt5\">", "<em class=\"hlt6\">",
+            "<em class=\"hlt7\">", "<em class=\"hlt8\">", "<em class=\"hlt9\">",
+            "<em class=\"hlt10\">"
+    };
+    /** the default closing tags when <tt>tag_schema = "styled"</tt>  */
+    public static final String[] DEFAULT_STYLED_POST_TAGS = {"</em>"};
+
+    /**
+     * a {@link FieldOptions.Builder} with default settings
+     */
+    public final static Builder defaultFieldOptions() {
+        return new SearchContextHighlight.FieldOptions.Builder()
+                .preTags(DEFAULT_PRE_TAGS).postTags(DEFAULT_POST_TAGS).scoreOrdered(DEFAULT_SCORE_ORDERED).highlightFilter(DEFAULT_HIGHLIGHT_FILTER)
+                .requireFieldMatch(DEFAULT_REQUIRE_FIELD_MATCH).forceSource(DEFAULT_FORCE_SOURCE).fragmentCharSize(DEFAULT_FRAGMENT_CHAR_SIZE).numberOfFragments(DEFAULT_NUMBER_OF_FRAGMENTS)
+                .encoder(DEFAULT_ENCODER).boundaryMaxScan(SimpleBoundaryScanner.DEFAULT_MAX_SCAN)
+                .boundaryChars(SimpleBoundaryScanner.DEFAULT_BOUNDARY_CHARS)
+                .noMatchSize(DEFAULT_NO_MATCH_SIZE).phraseLimit(DEFAULT_PHRASE_LIMIT);
+    }
+
     private final List<Field> fields = new ArrayList<>();
 
     private String encoder;
@@ -115,12 +176,12 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
     public HighlightBuilder tagsSchema(String schemaName) {
         switch (schemaName) {
         case "default":
-            preTags(HighlighterParseElement.DEFAULT_PRE_TAGS);
-            postTags(HighlighterParseElement.DEFAULT_POST_TAGS);
+            preTags(DEFAULT_PRE_TAGS);
+            postTags(DEFAULT_POST_TAGS);
             break;
         case "styled":
-            preTags(HighlighterParseElement.STYLED_PRE_TAG);
-            postTags(HighlighterParseElement.STYLED_POST_TAGS);
+            preTags(DEFAULT_STYLED_PRE_TAG);
+            postTags(DEFAULT_STYLED_POST_TAGS);
             break;
         default:
             throw new IllegalArgumentException("Unknown tag schema ["+ schemaName +"]");
@@ -164,24 +225,148 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject("highlight");
+        builder.startObject(HIGHLIGHT_ELEMENT_NAME);
         innerXContent(builder);
         builder.endObject();
         return builder;
     }
 
+    /**
+     * parse options only present in top level highlight builder (`tags_schema`, `encoder` and nested `fields`)
+     */
+    @Override
+    protected boolean doFromXContent(QueryParseContext parseContext, String currentFieldName, Token currentToken) throws IOException {
+        XContentParser parser = parseContext.parser();
+        XContentParser.Token token;
+        boolean foundCurrentFieldMatch = false;
+        if (currentToken.isValue()) {
+            if (parseContext.parseFieldMatcher().match(currentFieldName, TAGS_SCHEMA_FIELD)) {
+                tagsSchema(parser.text());
+                foundCurrentFieldMatch = true;
+            } else if (parseContext.parseFieldMatcher().match(currentFieldName, ENCODER_FIELD)) {
+                encoder(parser.text());
+                foundCurrentFieldMatch = true;
+            }
+        } else if (currentToken == Token.START_ARRAY && parseContext.parseFieldMatcher().match(currentFieldName, FIELDS_FIELD)) {
+            useExplicitFieldOrder(true);
+            while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                if (token == XContentParser.Token.START_OBJECT) {
+                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                        if (token == XContentParser.Token.FIELD_NAME) {
+                            field(HighlightBuilder.Field.PROTOTYPE.fromXContent(parseContext));
+                        }
+                    }
+                    foundCurrentFieldMatch = true;
+                } else {
+                    throw new ParsingException(parser.getTokenLocation(),
+                            "If highlighter fields is an array it must contain objects containing a single field");
+                }
+            }
+        } else if (currentToken == Token.START_OBJECT && parseContext.parseFieldMatcher().match(currentFieldName, FIELDS_FIELD)) {
+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (token == XContentParser.Token.FIELD_NAME) {
+                    field(HighlightBuilder.Field.PROTOTYPE.fromXContent(parseContext));
+                }
+            }
+            foundCurrentFieldMatch = true;
+        }
+        return foundCurrentFieldMatch;
+    }
+
+    public SearchContextHighlight build(QueryShardContext context) throws IOException {
+        // create template global options that are later merged with any partial field options
+        final SearchContextHighlight.FieldOptions.Builder globalOptionsBuilder = new SearchContextHighlight.FieldOptions.Builder();
+        globalOptionsBuilder.encoder(this.encoder);
+        transferOptions(this, globalOptionsBuilder, context);
+
+        // overwrite unset global options by default values
+        globalOptionsBuilder.merge(defaultFieldOptions().build());
+
+        // create field options
+        Collection<org.elasticsearch.search.highlight.SearchContextHighlight.Field> fieldOptions = new ArrayList<>();
+        for (Field field : this.fields) {
+            final SearchContextHighlight.FieldOptions.Builder fieldOptionsBuilder = new SearchContextHighlight.FieldOptions.Builder();
+            fieldOptionsBuilder.fragmentOffset(field.fragmentOffset);
+            if (field.matchedFields != null) {
+                Set<String> matchedFields = new HashSet<String>(field.matchedFields.length);
+                Collections.addAll(matchedFields, field.matchedFields);
+                fieldOptionsBuilder.matchedFields(matchedFields);
+            }
+            transferOptions(field, fieldOptionsBuilder, context);
+            fieldOptions.add(new SearchContextHighlight.Field(field.name(), fieldOptionsBuilder.merge(globalOptionsBuilder.build()).build()));
+        }
+        return new SearchContextHighlight(fieldOptions);
+    }
+
+    /**
+     * Transfers field options present in the input {@link AbstractHighlighterBuilder} to the receiving
+     * {@link FieldOptions.Builder}, effectively overwriting existing settings
+     * @param targetOptionsBuilder the receiving options builder
+     * @param highlighterBuilder highlight builder with the input options
+     * @param context needed to convert {@link QueryBuilder} to {@link Query}
+     * @throws IOException on errors parsing any optional nested highlight query
+     */
+    @SuppressWarnings({ "rawtypes", "unchecked" })
+    private static void transferOptions(AbstractHighlighterBuilder highlighterBuilder, SearchContextHighlight.FieldOptions.Builder targetOptionsBuilder, QueryShardContext context) throws IOException {
+        targetOptionsBuilder.preTags(highlighterBuilder.preTags);
+        targetOptionsBuilder.postTags(highlighterBuilder.postTags);
+        targetOptionsBuilder.scoreOrdered("score".equals(highlighterBuilder.order));
+        if (highlighterBuilder.highlightFilter != null) {
+            targetOptionsBuilder.highlightFilter(highlighterBuilder.highlightFilter);
+        }
+        if (highlighterBuilder.fragmentSize != null) {
+            targetOptionsBuilder.fragmentCharSize(highlighterBuilder.fragmentSize);
+        }
+        if (highlighterBuilder.numOfFragments != null) {
+            targetOptionsBuilder.numberOfFragments(highlighterBuilder.numOfFragments);
+        }
+        if (highlighterBuilder.requireFieldMatch != null) {
+            targetOptionsBuilder.requireFieldMatch(highlighterBuilder.requireFieldMatch);
+        }
+        if (highlighterBuilder.boundaryMaxScan != null) {
+            targetOptionsBuilder.boundaryMaxScan(highlighterBuilder.boundaryMaxScan);
+        }
+        targetOptionsBuilder.boundaryChars(convertCharArray(highlighterBuilder.boundaryChars));
+        targetOptionsBuilder.highlighterType(highlighterBuilder.highlighterType);
+        targetOptionsBuilder.fragmenter(highlighterBuilder.fragmenter);
+        if (highlighterBuilder.noMatchSize != null) {
+            targetOptionsBuilder.noMatchSize(highlighterBuilder.noMatchSize);
+        }
+        if (highlighterBuilder.forceSource != null) {
+            targetOptionsBuilder.forceSource(highlighterBuilder.forceSource);
+        }
+        if (highlighterBuilder.phraseLimit != null) {
+            targetOptionsBuilder.phraseLimit(highlighterBuilder.phraseLimit);
+        }
+        targetOptionsBuilder.options(highlighterBuilder.options);
+        if (highlighterBuilder.highlightQuery != null) {
+            targetOptionsBuilder.highlightQuery(highlighterBuilder.highlightQuery.toQuery(context));
+        }
+    }
+
+    private static Character[] convertCharArray(char[] array) {
+        if (array == null) {
+            return null;
+        }
+        Character[] charArray = new Character[array.length];
+        for (int i = 0; i < array.length; i++) {
+            charArray[i] = array[i];
+        }
+        return charArray;
+    }
+
     public void innerXContent(XContentBuilder builder) throws IOException {
         // first write common options
         commonOptionsToXContent(builder);
         // special options for top-level highlighter
         if (encoder != null) {
-            builder.field("encoder", encoder);
+            builder.field(ENCODER_FIELD.getPreferredName(), encoder);
         }
         if (fields.size() > 0) {
             if (useExplicitFieldOrder) {
-                builder.startArray("fields");
+                builder.startArray(FIELDS_FIELD.getPreferredName());
             } else {
-                builder.startObject("fields");
+                builder.startObject(FIELDS_FIELD.getPreferredName());
             }
             for (Field field : fields) {
                 if (useExplicitFieldOrder) {
@@ -205,7 +390,7 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
         try {
             XContentBuilder builder = XContentFactory.jsonBuilder();
             builder.prettyPrint();
-            toXContent(builder, ToXContent.EMPTY_PARAMS);
+            toXContent(builder, EMPTY_PARAMS);
             return builder.string();
         } catch (Exception e) {
             return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}";
@@ -213,6 +398,11 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
     }
 
     @Override
+    protected HighlightBuilder createInstance(XContentParser parser) {
+        return new HighlightBuilder();
+    }
+
+    @Override
     protected int doHashCode() {
         return Objects.hash(encoder, useExplicitFieldOrder, fields);
     }
@@ -286,14 +476,46 @@ public class HighlightBuilder extends AbstractHighlighterBuilder<HighlightBuilde
             commonOptionsToXContent(builder);
             // write special field-highlighter options
             if (fragmentOffset != -1) {
-                builder.field("fragment_offset", fragmentOffset);
+                builder.field(FRAGMENT_OFFSET_FIELD.getPreferredName(), fragmentOffset);
             }
             if (matchedFields != null) {
-                builder.field("matched_fields", matchedFields);
+                builder.field(MATCHED_FIELDS_FIELD.getPreferredName(), matchedFields);
             }
             builder.endObject();
         }
 
+        /**
+         * parse options only present in field highlight builder (`fragment_offset`, `matched_fields`)
+         */
+        @Override
+        protected boolean doFromXContent(QueryParseContext parseContext, String currentFieldName, Token currentToken) throws IOException {
+            XContentParser parser = parseContext.parser();
+            boolean foundCurrentFieldMatch = false;
+            if (parseContext.parseFieldMatcher().match(currentFieldName, FRAGMENT_OFFSET_FIELD) && currentToken.isValue()) {
+                fragmentOffset(parser.intValue());
+                foundCurrentFieldMatch = true;
+            } else if (parseContext.parseFieldMatcher().match(currentFieldName, MATCHED_FIELDS_FIELD)
+                    && currentToken == XContentParser.Token.START_ARRAY) {
+                List<String> matchedFields = new ArrayList<>();
+                while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
+                    matchedFields.add(parser.text());
+                }
+                matchedFields(matchedFields.toArray(new String[matchedFields.size()]));
+                foundCurrentFieldMatch = true;
+            }
+            return foundCurrentFieldMatch;
+        }
+
+        @Override
+        protected Field createInstance(XContentParser parser) throws IOException {
+            if (parser.currentToken() == XContentParser.Token.FIELD_NAME) {
+                String fieldname = parser.currentName();
+                return new Field(fieldname);
+            } else {
+                throw new ParsingException(parser.getTokenLocation(), "unknown token type [{}], expected field name", parser.currentToken());
+            }
+        }
+
         @Override
         protected int doHashCode() {
             return Objects.hash(name, fragmentOffset, Arrays.hashCode(matchedFields));
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
index fdf9e2c..38534ba 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.highlight;
 
-import org.apache.lucene.search.vectorhighlight.SimpleBoundaryScanner;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryShardContext;
@@ -52,39 +51,6 @@ import java.util.Set;
  */
 public class HighlighterParseElement implements SearchParseElement {
 
-    /** default for whether to highlight fields based on the source even if stored separately */
-    public static final boolean DEFAULT_FORCE_SOURCE = false;
-    /** default for whether a field should be highlighted only if a query matches that field */
-    public static final boolean DEFAULT_REQUIRE_FIELD_MATCH = true;
-    /** default for whether <tt>fvh</tt> should provide highlighting on filter clauses */
-    public static final boolean DEFAULT_HIGHLIGHT_FILTER = false;
-    /** default for highlight fragments being ordered by score */ 
-    public static final boolean DEFAULT_SCORE_ORDERED = false;
-    /** the default encoder setting */
-    public static final String DEFAULT_ENCODER = "default";
-    /** default for the maximum number of phrases the fvh will consider */
-    public static final int DEFAULT_PHRASE_LIMIT = 256;
-    /** default for fragment size when there are no matches */
-    public static final int DEFAULT_NO_MATCH_SIZE = 0;
-    /** the default number of fragments for highlighting */
-    public static final int DEFAULT_NUMBER_OF_FRAGMENTS = 5;
-    /** the default number of fragments size in characters */
-    public static final int DEFAULT_FRAGMENT_CHAR_SIZE = 100;
-    /** the default opening tag  */
-    public static final String[] DEFAULT_PRE_TAGS = new String[]{"<em>"};
-    /** the default closing tag  */
-    public static final String[] DEFAULT_POST_TAGS = new String[]{"</em>"};
-    
-    /** the default opening tags when <tt>tag_schema = "styled"</tt>  */
-    public static final String[] STYLED_PRE_TAG = {
-            "<em class=\"hlt1\">", "<em class=\"hlt2\">", "<em class=\"hlt3\">",
-            "<em class=\"hlt4\">", "<em class=\"hlt5\">", "<em class=\"hlt6\">",
-            "<em class=\"hlt7\">", "<em class=\"hlt8\">", "<em class=\"hlt9\">",
-            "<em class=\"hlt10\">"
-    };
-    /** the default closing tags when <tt>tag_schema = "styled"</tt>  */
-    public static final String[] STYLED_POST_TAGS = {"</em>"};
-
     @Override
     public void parse(XContentParser parser, SearchContext context) throws Exception {
         try {
@@ -99,12 +65,7 @@ public class HighlighterParseElement implements SearchParseElement {
         String topLevelFieldName = null;
         final List<Tuple<String, SearchContextHighlight.FieldOptions.Builder>> fieldsOptions = new ArrayList<>();
 
-        final SearchContextHighlight.FieldOptions.Builder globalOptionsBuilder = new SearchContextHighlight.FieldOptions.Builder()
-                .preTags(DEFAULT_PRE_TAGS).postTags(DEFAULT_POST_TAGS).scoreOrdered(DEFAULT_SCORE_ORDERED).highlightFilter(DEFAULT_HIGHLIGHT_FILTER)
-                .requireFieldMatch(DEFAULT_REQUIRE_FIELD_MATCH).forceSource(DEFAULT_FORCE_SOURCE).fragmentCharSize(DEFAULT_FRAGMENT_CHAR_SIZE).numberOfFragments(DEFAULT_NUMBER_OF_FRAGMENTS)
-                .encoder(DEFAULT_ENCODER).boundaryMaxScan(SimpleBoundaryScanner.DEFAULT_MAX_SCAN)
-                .boundaryChars(SimpleBoundaryScanner.DEFAULT_BOUNDARY_CHARS)
-                .noMatchSize(DEFAULT_NO_MATCH_SIZE).phraseLimit(DEFAULT_PHRASE_LIMIT);
+        final SearchContextHighlight.FieldOptions.Builder globalOptionsBuilder = HighlightBuilder.defaultFieldOptions();
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -147,8 +108,8 @@ public class HighlighterParseElement implements SearchParseElement {
                 } else if ("tags_schema".equals(topLevelFieldName) || "tagsSchema".equals(topLevelFieldName)) {
                     String schema = parser.text();
                     if ("styled".equals(schema)) {
-                        globalOptionsBuilder.preTags(STYLED_PRE_TAG);
-                        globalOptionsBuilder.postTags(STYLED_POST_TAGS);
+                        globalOptionsBuilder.preTags(HighlightBuilder.DEFAULT_STYLED_PRE_TAG);
+                        globalOptionsBuilder.postTags(HighlightBuilder.DEFAULT_STYLED_POST_TAGS);
                     }
                 } else if ("highlight_filter".equals(topLevelFieldName) || "highlightFilter".equals(topLevelFieldName)) {
                     globalOptionsBuilder.highlightFilter(parser.booleanValue());
@@ -211,7 +172,7 @@ public class HighlighterParseElement implements SearchParseElement {
         return new SearchContextHighlight(fields);
     }
 
-    protected SearchContextHighlight.FieldOptions.Builder parseFields(XContentParser parser, QueryShardContext queryShardContext) throws IOException {
+    private static SearchContextHighlight.FieldOptions.Builder parseFields(XContentParser parser, QueryShardContext queryShardContext) throws IOException {
         XContentParser.Token token;
 
         final SearchContextHighlight.FieldOptions.Builder fieldOptionsBuilder = new SearchContextHighlight.FieldOptions.Builder();
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java b/core/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java
index 38a8147..293143f 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java
@@ -53,6 +53,10 @@ public class SearchContextHighlight {
         this.globalForceSource = globalForceSource;
     }
 
+    boolean globalForceSource() {
+        return this.globalForceSource;
+    }
+
     public boolean forceSource(Field field) {
         if (globalForceSource) {
             return true;
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
index 14b2680..cd710d5 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
@@ -33,7 +33,8 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.settings.ClusterSettings;
+import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
+import org.elasticsearch.cluster.settings.DynamicSettings;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
@@ -117,19 +118,18 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
     private final MetaDataCreateIndexService createIndexService;
 
-    private final ClusterSettings dynamicSettings;
+    private final DynamicSettings dynamicSettings;
 
     private final MetaDataIndexUpgradeService metaDataIndexUpgradeService;
 
     private final CopyOnWriteArrayList<ActionListener<RestoreCompletionResponse>> listeners = new CopyOnWriteArrayList<>();
 
     private final BlockingQueue<UpdateIndexShardRestoreStatusRequest> updatedSnapshotStateQueue = ConcurrentCollections.newBlockingQueue();
-    private final ClusterSettings clusterSettings;
 
     @Inject
     public RestoreService(Settings settings, ClusterService clusterService, RepositoriesService repositoriesService, TransportService transportService,
-                          AllocationService allocationService, MetaDataCreateIndexService createIndexService, ClusterSettings dynamicSettings,
-                          MetaDataIndexUpgradeService metaDataIndexUpgradeService, ClusterSettings clusterSettings) {
+                          AllocationService allocationService, MetaDataCreateIndexService createIndexService, @ClusterDynamicSettings DynamicSettings dynamicSettings,
+                          MetaDataIndexUpgradeService metaDataIndexUpgradeService) {
         super(settings);
         this.clusterService = clusterService;
         this.repositoriesService = repositoriesService;
@@ -140,7 +140,6 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
         this.metaDataIndexUpgradeService = metaDataIndexUpgradeService;
         transportService.registerRequestHandler(UPDATE_RESTORE_ACTION_NAME, UpdateIndexShardRestoreStatusRequest::new, ThreadPool.Names.SAME, new UpdateRestoreStateRequestHandler());
         clusterService.add(this);
-        this.clusterSettings = clusterSettings;
     }
 
     /**
@@ -390,9 +389,24 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                 private void restoreGlobalStateIfRequested(MetaData.Builder mdBuilder) {
                     if (request.includeGlobalState()) {
                         if (metaData.persistentSettings() != null) {
-                            Settings settings = metaData.persistentSettings();
-                            clusterSettings.dryRun(settings);
-                            mdBuilder.persistentSettings(settings);
+                            boolean changed = false;
+                            Settings.Builder persistentSettings = Settings.settingsBuilder().put();
+                            for (Map.Entry<String, String> entry : metaData.persistentSettings().getAsMap().entrySet()) {
+                                if (dynamicSettings.isDynamicOrLoggingSetting(entry.getKey())) {
+                                    String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue(), clusterService.state());
+                                    if (error == null) {
+                                        persistentSettings.put(entry.getKey(), entry.getValue());
+                                        changed = true;
+                                    } else {
+                                        logger.warn("ignoring persistent setting [{}], [{}]", entry.getKey(), error);
+                                    }
+                                } else {
+                                    logger.warn("ignoring persistent setting [{}], not dynamically updateable", entry.getKey());
+                                }
+                            }
+                            if (changed) {
+                                mdBuilder.persistentSettings(persistentSettings.build());
+                            }
                         }
                         if (metaData.templates() != null) {
                             // TODO: Should all existing templates be deleted first?
diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index 56e0292..b0d8127 100644
--- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -20,13 +20,13 @@
 package org.elasticsearch.threadpool;
 
 import org.apache.lucene.util.Counter;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.settings.Validator;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.common.unit.SizeValue;
@@ -38,11 +38,14 @@ import org.elasticsearch.common.util.concurrent.XRejectedExecutionHandler;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
+import org.elasticsearch.node.settings.NodeSettingsService;
 
 import java.io.IOException;
 import java.util.*;
 import java.util.concurrent.*;
-import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.function.Function;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 
 import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -169,7 +172,7 @@ public class ThreadPool extends AbstractComponent {
         }
     }
 
-    public static final Setting<Settings> THREADPOOL_GROUP_SETTING = Setting.groupSetting("threadpool.", true, Setting.Scope.CLUSTER);
+    public static final String THREADPOOL_GROUP = "threadpool.";
 
     private volatile Map<String, ExecutorHolder> executors;
 
@@ -181,7 +184,7 @@ public class ThreadPool extends AbstractComponent {
 
     private final EstimatedTimeThread estimatedTimeThread;
 
-    private final AtomicBoolean settingsListenerIsSet = new AtomicBoolean(false);
+    private boolean settingsListenerIsSet = false;
 
     static final Executor DIRECT_EXECUTOR = command -> command.run();
 
@@ -194,8 +197,7 @@ public class ThreadPool extends AbstractComponent {
 
         assert settings.get("name") != null : "ThreadPool's settings should contain a name";
 
-        Map<String, Settings> groupSettings = THREADPOOL_GROUP_SETTING.get(settings).getAsGroups();
-        validate(groupSettings);
+        Map<String, Settings> groupSettings = getThreadPoolSettingsGroup(settings);
 
         int availableProcessors = EsExecutors.boundedNumberOfProcessors(settings);
         int halfProcMaxAt5 = Math.min(((availableProcessors + 1) / 2), 5);
@@ -250,12 +252,18 @@ public class ThreadPool extends AbstractComponent {
         this.estimatedTimeThread.start();
     }
 
-    public void setClusterSettings(ClusterSettings clusterSettings) {
-        if(settingsListenerIsSet.compareAndSet(false, true)) {
-            clusterSettings.addSettingsUpdateConsumer(THREADPOOL_GROUP_SETTING, this::updateSettings, (s) -> validate(s.getAsGroups()));
-        } else {
+    private Map<String, Settings> getThreadPoolSettingsGroup(Settings settings) {
+        Map<String, Settings> groupSettings = settings.getGroups(THREADPOOL_GROUP);
+        validate(groupSettings);
+        return groupSettings;
+    }
+
+    public void setNodeSettingsService(NodeSettingsService nodeSettingsService) {
+        if(settingsListenerIsSet) {
             throw new IllegalStateException("the node settings listener was set more then once");
         }
+        nodeSettingsService.addListener(new ApplySettings());
+        settingsListenerIsSet = true;
     }
 
     public long estimatedTimeInMillis() {
@@ -518,8 +526,8 @@ public class ThreadPool extends AbstractComponent {
         throw new IllegalArgumentException("No type found [" + type + "], for [" + name + "]");
     }
 
-    private void updateSettings(Settings settings) {
-        Map<String, Settings> groupSettings = settings.getAsGroups();
+    public void updateSettings(Settings settings) {
+        Map<String, Settings> groupSettings = getThreadPoolSettingsGroup(settings);
         if (groupSettings.isEmpty()) {
             return;
         }
@@ -575,7 +583,7 @@ public class ThreadPool extends AbstractComponent {
             ThreadPoolType correctThreadPoolType = THREAD_POOL_TYPES.get(key);
             // TODO: the type equality check can be removed after #3760/#6732 are addressed
             if (type != null && !correctThreadPoolType.getType().equals(type)) {
-                throw new IllegalArgumentException("setting " + THREADPOOL_GROUP_SETTING.getKey() + key + ".type to " + type + " is not permitted; must be " + correctThreadPoolType.getType());
+                throw new IllegalArgumentException("setting " + THREADPOOL_GROUP + key + ".type to " + type + " is not permitted; must be " + correctThreadPoolType.getType());
             }
         }
     }
@@ -858,6 +866,13 @@ public class ThreadPool extends AbstractComponent {
 
     }
 
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            updateSettings(settings);
+        }
+    }
+
     /**
      * Returns <code>true</code> if the given service was terminated successfully. If the termination timed out,
      * the service is <code>null</code> this method will return <code>false</code>.
@@ -896,4 +911,38 @@ public class ThreadPool extends AbstractComponent {
         }
         return false;
     }
+
+    public static ThreadPoolTypeSettingsValidator THREAD_POOL_TYPE_SETTINGS_VALIDATOR = new ThreadPoolTypeSettingsValidator();
+    private static class ThreadPoolTypeSettingsValidator implements Validator {
+        @Override
+        public String validate(String setting, String value, ClusterState clusterState) {
+            // TODO: the type equality validation can be removed after #3760/#6732 are addressed
+            Matcher matcher = Pattern.compile("threadpool\\.(.*)\\.type").matcher(setting);
+            if (!matcher.matches()) {
+                return null;
+            } else {
+                String threadPool = matcher.group(1);
+                ThreadPool.ThreadPoolType defaultThreadPoolType = ThreadPool.THREAD_POOL_TYPES.get(threadPool);
+                ThreadPool.ThreadPoolType threadPoolType;
+                try {
+                    threadPoolType = ThreadPool.ThreadPoolType.fromType(value);
+                } catch (IllegalArgumentException e) {
+                    return e.getMessage();
+                }
+                if (defaultThreadPoolType.equals(threadPoolType)) {
+                    return null;
+                } else {
+                    return String.format(
+                            Locale.ROOT,
+                            "thread pool type for [%s] can only be updated to [%s] but was [%s]",
+                            threadPool,
+                            defaultThreadPoolType.getType(),
+                            threadPoolType.getType()
+                    );
+                }
+            }
+
+        }
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportService.java b/core/src/main/java/org/elasticsearch/transport/TransportService.java
index 05d5242..14fc902 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportService.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportService.java
@@ -29,8 +29,6 @@ import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.metrics.MeanMetric;
 import org.elasticsearch.common.regex.Regex;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Setting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.BoundTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
@@ -39,13 +37,16 @@ import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.common.util.concurrent.ConcurrentMapLong;
 import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Collections;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.Callable;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -87,13 +88,14 @@ public class TransportService extends AbstractLifecycleComponent<TransportServic
 
     // tracer log
 
-    public static final Setting<String[]> TRACE_LOG_INCLUDE_SETTING = new Setting<>("transport.tracer.include", "", Strings::splitStringByCommaToArray , true, Setting.Scope.CLUSTER);
-    public static final Setting<String[]> TRACE_LOG_EXCLUDE_SETTING = new Setting<>("transport.tracer.exclude", "internal:discovery/zen/fd*," + TransportLivenessAction.NAME, Strings::splitStringByCommaToArray , true, Setting.Scope.CLUSTER);;
+    public static final String SETTING_TRACE_LOG_INCLUDE = "transport.tracer.include";
+    public static final String SETTING_TRACE_LOG_EXCLUDE = "transport.tracer.exclude";
 
     private final ESLogger tracerLog;
 
     volatile String[] tracerLogInclude;
     volatile String[] tracelLogExclude;
+    private final ApplySettings settingsListener = new ApplySettings();
 
     /** if set will call requests sent to this id to shortcut and executed locally */
     volatile DiscoveryNode localNode = null;
@@ -107,8 +109,8 @@ public class TransportService extends AbstractLifecycleComponent<TransportServic
         super(settings);
         this.transport = transport;
         this.threadPool = threadPool;
-        this.tracerLogInclude = TRACE_LOG_INCLUDE_SETTING.get(settings);
-        this.tracelLogExclude = TRACE_LOG_EXCLUDE_SETTING.get(settings);
+        this.tracerLogInclude = settings.getAsArray(SETTING_TRACE_LOG_INCLUDE, Strings.EMPTY_ARRAY, true);
+        this.tracelLogExclude = settings.getAsArray(SETTING_TRACE_LOG_EXCLUDE, new String[]{"internal:discovery/zen/fd*", TransportLivenessAction.NAME}, true);
         tracerLog = Loggers.getLogger(logger, ".tracer");
         adapter = createAdapter();
     }
@@ -132,18 +134,34 @@ public class TransportService extends AbstractLifecycleComponent<TransportServic
 
     // These need to be optional as they don't exist in the context of a transport client
     @Inject(optional = true)
-    public void setDynamicSettings(ClusterSettings clusterSettings) {
-        clusterSettings.addSettingsUpdateConsumer(TRACE_LOG_INCLUDE_SETTING, this::setTracerLogInclude);
-        clusterSettings.addSettingsUpdateConsumer(TRACE_LOG_EXCLUDE_SETTING, this::setTracelLogExclude);
+    public void setDynamicSettings(NodeSettingsService nodeSettingsService) {
+        nodeSettingsService.addListener(settingsListener);
     }
 
-    void setTracerLogInclude(String[] tracerLogInclude) {
-        this.tracerLogInclude = tracerLogInclude;
+
+    class ApplySettings implements NodeSettingsService.Listener {
+        @Override
+        public void onRefreshSettings(Settings settings) {
+            String[] newTracerLogInclude = settings.getAsArray(SETTING_TRACE_LOG_INCLUDE, TransportService.this.tracerLogInclude, true);
+            String[] newTracerLogExclude = settings.getAsArray(SETTING_TRACE_LOG_EXCLUDE, TransportService.this.tracelLogExclude, true);
+            if (newTracerLogInclude == TransportService.this.tracerLogInclude && newTracerLogExclude == TransportService.this.tracelLogExclude) {
+                return;
+            }
+            if (Arrays.equals(newTracerLogInclude, TransportService.this.tracerLogInclude) &&
+                    Arrays.equals(newTracerLogExclude, TransportService.this.tracelLogExclude)) {
+                return;
+            }
+            TransportService.this.tracerLogInclude = newTracerLogInclude;
+            TransportService.this.tracelLogExclude = newTracerLogExclude;
+            logger.info("tracer log updated to use include: {}, exclude: {}", newTracerLogInclude, newTracerLogExclude);
+        }
     }
 
-    void setTracelLogExclude(String[] tracelLogExclude) {
-        this.tracelLogExclude = tracelLogExclude;
+    // used for testing
+    public void applySettings(Settings settings) {
+        settingsListener.onRefreshSettings(settings);
     }
+
     @Override
     protected void doStart() {
         adapter.rxMetric.clear();
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index ab39a35..2f1c52a 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -87,6 +87,7 @@ import org.jboss.netty.channel.socket.oio.OioServerSocketChannelFactory;
 import org.jboss.netty.util.HashedWheelTimer;
 
 import java.io.IOException;
+import java.net.BindException;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.net.SocketAddress;
@@ -763,6 +764,11 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             // close the channel as safe measure, which will cause a node to be disconnected if relevant
             ctx.getChannel().close();
             disconnectFromNodeChannel(ctx.getChannel(), e.getCause());
+        } else if (e.getCause() instanceof BindException) {
+            logger.trace("bind exception caught on transport layer [{}]", e.getCause(), ctx.getChannel());
+            // close the channel as safe measure, which will cause a node to be disconnected if relevant
+            ctx.getChannel().close();
+            disconnectFromNodeChannel(ctx.getChannel(), e.getCause());
         } else if (e.getCause() instanceof CancelledKeyException) {
             logger.trace("cancelled key exception caught on transport layer [{}], disconnecting from relevant node", e.getCause(), ctx.getChannel());
             // close the channel as safe measure, which will cause a node to be disconnected if relevant
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeClientNode.java b/core/src/main/java/org/elasticsearch/tribe/TribeClientNode.java
new file mode 100644
index 0000000..688dfe5
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeClientNode.java
@@ -0,0 +1,37 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.tribe;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.node.Node;
+import org.elasticsearch.plugins.Plugin;
+
+import java.util.Collections;
+
+/**
+ * An internal node that connects to a remove cluster, as part of a tribe node.
+ */
+class TribeClientNode extends Node {
+    TribeClientNode(Settings settings) {
+        super(new Environment(settings), Version.CURRENT, Collections.<Class<? extends Plugin>>emptyList());
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeService.java b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
index 87da13f..f577415 100644
--- a/core/src/main/java/org/elasticsearch/tribe/TribeService.java
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.tribe;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
 import org.elasticsearch.cluster.ClusterChangedEvent;
@@ -46,8 +45,6 @@ import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.discovery.DiscoveryService;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.rest.RestStatus;
 
 import java.util.EnumSet;
@@ -132,14 +129,14 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
         nodesSettings.remove("on_conflict"); // remove prefix settings that don't indicate a client
         for (Map.Entry<String, Settings> entry : nodesSettings.entrySet()) {
             Settings.Builder sb = Settings.builder().put(entry.getValue());
-            sb.put("node.name", settings.get("name") + "/" + entry.getKey());
+            sb.put("name", settings.get("name") + "/" + entry.getKey());
             sb.put("path.home", settings.get("path.home")); // pass through ES home dir
             sb.put(TRIBE_NAME, entry.getKey());
-            sb.put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true);
             if (sb.get("http.enabled") == null) {
                 sb.put("http.enabled", false);
             }
-            nodes.add(NodeBuilder.nodeBuilder().settings(sb).client(true).build());
+            sb.put("node.client", true);
+            nodes.add(new TribeClientNode(sb.build()));
         }
 
         String[] blockIndicesWrite = Strings.EMPTY_ARRAY;
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
index 7e060cd..2678501 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/security.policy
@@ -31,10 +31,12 @@ grant codeBase "${codebase.securesm-1.0.jar}" {
 //// Very special jar permissions:
 //// These are dangerous permissions that we don't want to grant to everything.
 
-grant codeBase "${codebase.lucene-core-5.4.0-snapshot-1715952.jar}" {
+grant codeBase "${codebase.lucene-core-5.5.0-snapshot-1719088.jar}" {
   // needed to allow MMapDirectory's "unmap hack"
   permission java.lang.RuntimePermission "accessClassInPackage.sun.misc";
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
+  // NOTE: also needed for RAMUsageEstimator size calculations
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
 };
 
 //// Everything else:
@@ -97,9 +99,6 @@ grant {
   // (TODO: clean this up?)
   permission java.lang.RuntimePermission "getProtectionDomain";
   
-  // likely not low hanging fruit...
-  permission java.lang.RuntimePermission "accessDeclaredMembers";
-
   // needed by HotThreads and potentially more
   // otherwise can be provided only to test libraries
   permission java.lang.RuntimePermission "getStackTrace";
diff --git a/core/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy b/core/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy
index cde6795..b5f9c24 100644
--- a/core/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy
+++ b/core/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy
@@ -21,30 +21,38 @@
 //// These are mock objects and test management that we allow test framework libs
 //// to provide on our behalf. But tests themselves cannot do this stuff!
 
-grant codeBase "${codebase.securemock-1.1.jar}" {
+grant codeBase "${codebase.securemock-1.2.jar}" {
   // needed to access ReflectionFactory (see below)
   permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
   // needed to support creation of mocks
   permission java.lang.RuntimePermission "reflectionFactoryAccess";
   // needed for spy interception, etc
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
-grant codeBase "${codebase.lucene-test-framework-5.4.0-snapshot-1715952.jar}" {
+grant codeBase "${codebase.lucene-test-framework-5.5.0-snapshot-1719088.jar}" {
   // needed by RamUsageTester
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
 
-grant codeBase "${codebase.randomizedtesting-runner-2.2.0.jar}" {
+grant codeBase "${codebase.randomizedtesting-runner-2.3.2.jar}" {
   // optionally needed for access to private test methods (e.g. beforeClass)
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
   // needed to fail tests on uncaught exceptions from other threads
   permission java.lang.RuntimePermission "setDefaultUncaughtExceptionHandler";
   // needed for top threads handling
   permission org.elasticsearch.ThreadPermission "modifyArbitraryThreadGroup";
+  // needed for TestClass creation
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
 };
 
-grant codeBase "${codebase.junit4-ant-2.2.0.jar}" {
+grant codeBase "${codebase.junit4-ant-2.3.2.jar}" {
   // needed for stream redirection
   permission java.lang.RuntimePermission "setIO";
 };
+
+grant codeBase "${codebase.junit-4.11.jar}" {
+  // needed for TestClass creation
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
+};
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index e6622e2..2a4e6a6 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -44,6 +44,7 @@ OFFICIAL PLUGINS
     - discovery-gce
     - discovery-multicast
     - lang-javascript
+    - lang-plan-a
     - lang-python
     - mapper-attachments
     - mapper-murmur3
diff --git a/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTests.java b/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTests.java
index 0f29ed5..a287ec1 100644
--- a/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTests.java
+++ b/core/src/test/java/org/apache/lucene/queries/BlendedTermQueryTests.java
@@ -23,19 +23,8 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexOptions;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.QueryUtils;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.index.*;
+import org.apache.lucene.search.*;
 import org.apache.lucene.search.similarities.BM25Similarity;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.search.similarities.Similarity;
@@ -44,11 +33,7 @@ import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
+import java.util.*;
 
 import static org.hamcrest.Matchers.containsInAnyOrder;
 import static org.hamcrest.Matchers.equalTo;
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 21ee6de..46cdea3 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -136,7 +136,7 @@ public class ExceptionSerializationTests extends ESTestCase {
                                 } else if (ElasticsearchException.isRegistered((Class<? extends Throwable>) clazz)) {
                                     registered.add(clazz);
                                     try {
-                                        if (clazz.getDeclaredMethod("writeTo", StreamOutput.class) != null) {
+                                        if (clazz.getMethod("writeTo", StreamOutput.class) != null) {
                                             hasDedicatedWrite.add(clazz);
                                         }
                                     } catch (Exception e) {
diff --git a/core/src/test/java/org/elasticsearch/VersionTests.java b/core/src/test/java/org/elasticsearch/VersionTests.java
index d79d1ee..52508f8 100644
--- a/core/src/test/java/org/elasticsearch/VersionTests.java
+++ b/core/src/test/java/org/elasticsearch/VersionTests.java
@@ -191,7 +191,7 @@ public class VersionTests extends ESTestCase {
 
     public void testAllVersionsMatchId() throws Exception {
         Map<String, Version> maxBranchVersions = new HashMap<>();
-        for (java.lang.reflect.Field field : Version.class.getDeclaredFields()) {
+        for (java.lang.reflect.Field field : Version.class.getFields()) {
             if (field.getName().endsWith("_ID")) {
                 assertTrue(field.getName() + " should be static", Modifier.isStatic(field.getModifiers()));
                 assertTrue(field.getName() + " should be final", Modifier.isFinal(field.getModifiers()));
diff --git a/core/src/test/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdaterTests.java b/core/src/test/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdaterTests.java
deleted file mode 100644
index fe4f8bb..0000000
--- a/core/src/test/java/org/elasticsearch/action/admin/cluster/settings/SettingsUpdaterTests.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.action.admin.cluster.settings;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
-import org.elasticsearch.common.settings.ClusterSettings;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.concurrent.atomic.AtomicReference;
-
-public class SettingsUpdaterTests extends ESTestCase {
-
-
-    public void testUpdateSetting() {
-        AtomicReference<Float> index = new AtomicReference<>();
-        AtomicReference<Float> shard = new AtomicReference<>();
-        ClusterState.Builder builder = ClusterState.builder(new ClusterName("foo"));
-        ClusterSettings settingsService = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-        settingsService.addSettingsUpdateConsumer(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING, index::set);
-        settingsService.addSettingsUpdateConsumer(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING, shard::set);
-        SettingsUpdater updater = new SettingsUpdater(settingsService);
-        MetaData.Builder metaData = MetaData.builder()
-            .persistentSettings(Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 1.5)
-                .put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 2.5).build())
-            .transientSettings(Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 3.5)
-                .put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 4.5).build());
-        ClusterState build = builder.metaData(metaData).build();
-        ClusterState clusterState = updater.updateSettings(build, Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 0.5).build(),
-            Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 0.4).build());
-        assertNotSame(clusterState, build);
-        assertEquals(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 0.4, 0.1);
-        assertEquals(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 2.5, 0.1);
-        assertEquals(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.get(clusterState.metaData().transientSettings()), 0.5, 0.1);
-        assertEquals(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.get(clusterState.metaData().transientSettings()), 4.5, 0.1);
-
-        clusterState = updater.updateSettings(clusterState, Settings.builder().putNull("cluster.routing.*").build(),
-            Settings.EMPTY);
-        assertEquals(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 0.4, 0.1);
-        assertEquals(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 2.5, 0.1);
-        assertFalse(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().transientSettings()));
-        assertFalse(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().transientSettings()));
-
-        clusterState = updater.updateSettings(clusterState,
-            Settings.EMPTY,  Settings.builder().putNull("cluster.routing.*").put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 10.0).build());
-
-        assertEquals(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.get(clusterState.metaData().persistentSettings()), 10.0, 0.1);
-        assertFalse(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().persistentSettings()));
-        assertFalse(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().transientSettings()));
-        assertFalse(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.exists(clusterState.metaData().transientSettings()));
-        assertNull("updater only does a dryRun", index.get());
-        assertNull("updater only does a dryRun", shard.get());
-    }
-
-    public void testAllOrNothing() {
-        ClusterState.Builder builder = ClusterState.builder(new ClusterName("foo"));
-        ClusterSettings settingsService = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-        AtomicReference<Float> index = new AtomicReference<>();
-        AtomicReference<Float> shard = new AtomicReference<>();
-        settingsService.addSettingsUpdateConsumer(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING, index::set);
-        settingsService.addSettingsUpdateConsumer(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING, shard::set);
-        SettingsUpdater updater = new SettingsUpdater(settingsService);
-        MetaData.Builder metaData = MetaData.builder()
-            .persistentSettings(Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 1.5)
-                .put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 2.5).build())
-            .transientSettings(Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 3.5)
-                .put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 4.5).build());
-        ClusterState build = builder.metaData(metaData).build();
-
-        try {
-            updater.updateSettings(build, Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), "not a float").build(),
-                Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), "not a float").put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 1.0f).build());
-            fail("all or nothing");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Failed to parse value [not a float] for setting [cluster.routing.allocation.balance.index]", ex.getMessage());
-        }
-        assertNull("updater only does a dryRun", index.get());
-        assertNull("updater only does a dryRun", shard.get());
-    }
-
-    public void testClusterBlock() {
-        ClusterState.Builder builder = ClusterState.builder(new ClusterName("foo"));
-        ClusterSettings settingsService = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-        AtomicReference<Float> index = new AtomicReference<>();
-        AtomicReference<Float> shard = new AtomicReference<>();
-        settingsService.addSettingsUpdateConsumer(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING, index::set);
-        settingsService.addSettingsUpdateConsumer(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING, shard::set);
-        SettingsUpdater updater = new SettingsUpdater(settingsService);
-        MetaData.Builder metaData = MetaData.builder()
-            .persistentSettings(Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 1.5)
-                .put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 2.5).build())
-            .transientSettings(Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 3.5)
-                .put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 4.5).build());
-        ClusterState build = builder.metaData(metaData).build();
-
-        ClusterState clusterState = updater.updateSettings(build, Settings.builder().put(MetaData.SETTING_READ_ONLY_SETTING.getKey(), true).build(),
-            Settings.builder().put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 1.6).put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 1.0f).build());
-        assertEquals(clusterState.blocks().global().size(), 1);
-        assertEquals(clusterState.blocks().global().iterator().next(), MetaData.CLUSTER_READ_ONLY_BLOCK);
-
-        clusterState = updater.updateSettings(build, Settings.EMPTY,
-            Settings.builder().put(MetaData.SETTING_READ_ONLY_SETTING.getKey(), false).build());
-        assertEquals(clusterState.blocks().global().size(), 0);
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTests.java b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTests.java
index c862583..cf197a2 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTests.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/shards/IndicesShardStoreResponseTests.java
@@ -26,20 +26,12 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.ImmutableOpenIntMap;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.transport.DummyTransportAddress;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.transport.NodeDisconnectedException;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 
 import static org.hamcrest.Matchers.equalTo;
 
@@ -119,7 +111,7 @@ public class IndicesShardStoreResponseTests extends ESTestCase {
         orderedStoreStatuses.add(new IndicesShardStoresResponse.StoreStatus(node1, 3, IndicesShardStoresResponse.StoreStatus.Allocation.REPLICA, new IOException("corrupted")));
 
         List<IndicesShardStoresResponse.StoreStatus> storeStatuses = new ArrayList<>(orderedStoreStatuses);
-        Collections.shuffle(storeStatuses);
+        Collections.shuffle(storeStatuses, random());
         CollectionUtil.timSort(storeStatuses);
         assertThat(storeStatuses, equalTo(orderedStoreStatuses));
     }
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeIT.java b/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeIT.java
index a38a46d..143300e 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeIT.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeIT.java
@@ -65,7 +65,7 @@ public class UpgradeIT extends ESBackcompatTestCase {
     public void testUpgrade() throws Exception {
         // allow the cluster to rebalance quickly - 2 concurrent rebalance are default we can do higher
         Settings.Builder builder = Settings.builder();
-        builder.put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING.getKey(), 100);
+        builder.put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, 100);
         client().admin().cluster().prepareUpdateSettings().setPersistentSettings(builder).get();
 
         int numIndexes = randomIntBetween(2, 4);
@@ -117,13 +117,13 @@ public class UpgradeIT extends ESBackcompatTestCase {
         ensureGreen();
         // disable allocation entirely until all nodes are upgraded
         builder = Settings.builder();
-        builder.put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), EnableAllocationDecider.Allocation.NONE);
+        builder.put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, EnableAllocationDecider.Allocation.NONE);
         client().admin().cluster().prepareUpdateSettings().setTransientSettings(builder).get();
         backwardsCluster().upgradeAllNodes();
         builder = Settings.builder();
         // disable rebalanceing entirely for the time being otherwise we might get relocations / rebalance from nodes with old segments
-        builder.put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), EnableAllocationDecider.Rebalance.NONE);
-        builder.put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), EnableAllocationDecider.Allocation.ALL);
+        builder.put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, EnableAllocationDecider.Rebalance.NONE);
+        builder.put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, EnableAllocationDecider.Allocation.ALL);
         client().admin().cluster().prepareUpdateSettings().setTransientSettings(builder).get();
         ensureGreen();
         logger.info("--> Nodes upgrade complete");
diff --git a/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java b/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
index ded2abb..237f3a2 100644
--- a/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
+++ b/core/src/test/java/org/elasticsearch/action/bulk/BulkProcessorIT.java
@@ -105,7 +105,7 @@ public class BulkProcessorIT extends ESIntegTestCase {
     public void testBulkProcessorConcurrentRequests() throws Exception {
         int bulkActions = randomIntBetween(10, 100);
         int numDocs = randomIntBetween(bulkActions, bulkActions + 100);
-        int concurrentRequests = randomIntBetween(0, 10);
+        int concurrentRequests = randomIntBetween(0, 7);
 
         int expectedBulkActions = numDocs / bulkActions;
 
@@ -141,7 +141,7 @@ public class BulkProcessorIT extends ESIntegTestCase {
 
         Set<String> ids = new HashSet<>();
         for (BulkItemResponse bulkItemResponse : listener.bulkItems) {
-            assertThat(bulkItemResponse.isFailed(), equalTo(false));
+            assertThat(bulkItemResponse.getFailureMessage(), bulkItemResponse.isFailed(), equalTo(false));
             assertThat(bulkItemResponse.getIndex(), equalTo("test"));
             assertThat(bulkItemResponse.getType(), equalTo("test"));
             //with concurrent requests > 1 we can't rely on the order of the bulk requests
diff --git a/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java b/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java
index 6946e35..825e3e4 100644
--- a/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java
+++ b/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java
@@ -45,8 +45,8 @@ public class IndexingMasterFailoverIT extends ESIntegTestCase {
                 .put(FaultDetection.SETTING_PING_TIMEOUT, "1s") // for hitting simulated network failures quickly
                 .put(FaultDetection.SETTING_PING_RETRIES, "1") // for hitting simulated network failures quickly
                 .put("discovery.zen.join_timeout", "10s")  // still long to induce failures but to long so test won't time out
-                .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s") // <-- for hitting simulated network failures quickly
-                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2)
+                .put(DiscoverySettings.PUBLISH_TIMEOUT, "1s") // <-- for hitting simulated network failures quickly
+                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 2)
                 .build();
 
         internalCluster().startMasterOnlyNodesAsync(3, sharedSettings).get();
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java b/core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java
index d31a024..4d17155 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java
@@ -20,7 +20,7 @@ package org.elasticsearch.action.support.replication;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.NoShardAvailableActionException;
 import org.elasticsearch.action.UnavailableShardsException;
 import org.elasticsearch.action.admin.indices.flush.FlushRequest;
@@ -101,7 +101,7 @@ public class BroadcastReplicationTests extends ESTestCase {
                 randomBoolean() ? ShardRoutingState.INITIALIZING : ShardRoutingState.UNASSIGNED, ShardRoutingState.UNASSIGNED));
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Future<BroadcastResponse> response = (broadcastReplicationAction.execute(new BroadcastRequest().indices(index)));
-        for (Tuple<ShardId, ActionListener<ActionWriteResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
+        for (Tuple<ShardId, ActionListener<ReplicationResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
             if (randomBoolean()) {
                 shardRequests.v2().onFailure(new NoShardAvailableActionException(shardRequests.v1()));
             } else {
@@ -120,10 +120,10 @@ public class BroadcastReplicationTests extends ESTestCase {
                 ShardRoutingState.STARTED));
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Future<BroadcastResponse> response = (broadcastReplicationAction.execute(new BroadcastRequest().indices(index)));
-        for (Tuple<ShardId, ActionListener<ActionWriteResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
-            ActionWriteResponse actionWriteResponse = new ActionWriteResponse();
-            actionWriteResponse.setShardInfo(new ActionWriteResponse.ShardInfo(1, 1, new ActionWriteResponse.ShardInfo.Failure[0]));
-            shardRequests.v2().onResponse(actionWriteResponse);
+        for (Tuple<ShardId, ActionListener<ReplicationResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
+            ReplicationResponse replicationResponse = new ReplicationResponse();
+            replicationResponse.setShardInfo(new ReplicationResponse.ShardInfo(1, 1, new ReplicationResponse.ShardInfo.Failure[0]));
+            shardRequests.v2().onResponse(replicationResponse);
         }
         logger.info("total shards: {}, ", response.get().getTotalShards());
         assertBroadcastResponse(1, 1, 0, response.get(), null);
@@ -137,20 +137,20 @@ public class BroadcastReplicationTests extends ESTestCase {
         Future<BroadcastResponse> response = (broadcastReplicationAction.execute(new BroadcastRequest().indices(index)));
         int succeeded = 0;
         int failed = 0;
-        for (Tuple<ShardId, ActionListener<ActionWriteResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
+        for (Tuple<ShardId, ActionListener<ReplicationResponse>> shardRequests : broadcastReplicationAction.capturedShardRequests) {
             if (randomBoolean()) {
-                ActionWriteResponse.ShardInfo.Failure[] failures = new ActionWriteResponse.ShardInfo.Failure[0];
+                ReplicationResponse.ShardInfo.Failure[] failures = new ReplicationResponse.ShardInfo.Failure[0];
                 int shardsSucceeded = randomInt(1) + 1;
                 succeeded += shardsSucceeded;
-                ActionWriteResponse actionWriteResponse = new ActionWriteResponse();
+                ReplicationResponse replicationResponse = new ReplicationResponse();
                 if (shardsSucceeded == 1 && randomBoolean()) {
                     //sometimes add failure (no failure means shard unavailable)
-                    failures = new ActionWriteResponse.ShardInfo.Failure[1];
-                    failures[0] = new ActionWriteResponse.ShardInfo.Failure(index, shardRequests.v1().id(), null, new Exception("pretend shard failed"), RestStatus.GATEWAY_TIMEOUT, false);
+                    failures = new ReplicationResponse.ShardInfo.Failure[1];
+                    failures[0] = new ReplicationResponse.ShardInfo.Failure(index, shardRequests.v1().id(), null, new Exception("pretend shard failed"), RestStatus.GATEWAY_TIMEOUT, false);
                     failed++;
                 }
-                actionWriteResponse.setShardInfo(new ActionWriteResponse.ShardInfo(2, shardsSucceeded, failures));
-                shardRequests.v2().onResponse(actionWriteResponse);
+                replicationResponse.setShardInfo(new ReplicationResponse.ShardInfo(2, shardsSucceeded, failures));
+                shardRequests.v2().onResponse(replicationResponse);
             } else {
                 // sometimes fail
                 failed += 2;
@@ -179,16 +179,16 @@ public class BroadcastReplicationTests extends ESTestCase {
         assertThat(shards.get(0), equalTo(shardId));
     }
 
-    private class TestBroadcastReplicationAction extends TransportBroadcastReplicationAction<BroadcastRequest, BroadcastResponse, ReplicationRequest, ActionWriteResponse> {
-        protected final Set<Tuple<ShardId, ActionListener<ActionWriteResponse>>> capturedShardRequests = ConcurrentCollections.newConcurrentSet();
+    private class TestBroadcastReplicationAction extends TransportBroadcastReplicationAction<BroadcastRequest, BroadcastResponse, ReplicationRequest, ReplicationResponse> {
+        protected final Set<Tuple<ShardId, ActionListener<ReplicationResponse>>> capturedShardRequests = ConcurrentCollections.newConcurrentSet();
 
         public TestBroadcastReplicationAction(Settings settings, ThreadPool threadPool, ClusterService clusterService, TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver, TransportReplicationAction replicatedBroadcastShardAction) {
             super("test-broadcast-replication-action", BroadcastRequest::new, settings, threadPool, clusterService, transportService, actionFilters, indexNameExpressionResolver, replicatedBroadcastShardAction);
         }
 
         @Override
-        protected ActionWriteResponse newShardResponse() {
-            return new ActionWriteResponse();
+        protected ReplicationResponse newShardResponse() {
+            return new ReplicationResponse();
         }
 
         @Override
@@ -202,7 +202,7 @@ public class BroadcastReplicationTests extends ESTestCase {
         }
 
         @Override
-        protected void shardExecute(BroadcastRequest request, ShardId shardId, ActionListener<ActionWriteResponse> shardActionListener) {
+        protected void shardExecute(BroadcastRequest request, ShardId shardId, ActionListener<ReplicationResponse> shardActionListener) {
             capturedShardRequests.add(new Tuple<>(shardId, shardActionListener));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java b/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
index 79f3853..5834b26 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java
@@ -20,7 +20,7 @@ package org.elasticsearch.action.support.replication;
 
 import org.apache.lucene.index.CorruptIndexException;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.UnavailableShardsException;
 import org.elasticsearch.action.WriteConsistencyLevel;
 import org.elasticsearch.action.support.ActionFilter;
@@ -28,7 +28,6 @@ import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.PlainActionFuture;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateObserver;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockException;
@@ -46,18 +45,17 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.shard.IndexShardNotStartedException;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.shard.ShardNotFoundException;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.cluster.TestClusterService;
 import org.elasticsearch.test.transport.CapturingTransport;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseOptions;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.BeforeClass;
@@ -132,22 +130,22 @@ public class TransportReplicationActionTests extends ESTestCase {
         ClusterBlocks.Builder block = ClusterBlocks.builder()
                 .addGlobalBlock(new ClusterBlock(1, "non retryable", false, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL));
         clusterService.setState(ClusterState.builder(clusterService.state()).blocks(block));
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
-        assertFalse("primary phase should stop execution", primaryPhase.checkBlocks());
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
         assertListenerThrows("primary phase should fail operation", listener, ClusterBlockException.class);
 
         block = ClusterBlocks.builder()
                 .addGlobalBlock(new ClusterBlock(1, "retryable", true, true, RestStatus.SERVICE_UNAVAILABLE, ClusterBlockLevel.ALL));
         clusterService.setState(ClusterState.builder(clusterService.state()).blocks(block));
         listener = new PlainActionFuture<>();
-        primaryPhase = action.new PrimaryPhase(new Request().timeout("5ms"), listener);
-        assertFalse("primary phase should stop execution on retryable block", primaryPhase.checkBlocks());
+        reroutePhase = action.new ReroutePhase(new Request().timeout("5ms"), listener);
+        reroutePhase.run();
         assertListenerThrows("failed to timeout on retryable block", listener, ClusterBlockException.class);
 
 
         listener = new PlainActionFuture<>();
-        primaryPhase = action.new PrimaryPhase(new Request(), listener);
-        assertFalse("primary phase should stop execution on retryable block", primaryPhase.checkBlocks());
+        reroutePhase = action.new ReroutePhase(new Request(), listener);
+        reroutePhase.run();
         assertFalse("primary phase should wait on retryable block", listener.isDone());
 
         block = ClusterBlocks.builder()
@@ -172,25 +170,47 @@ public class TransportReplicationActionTests extends ESTestCase {
 
         Request request = new Request(shardId).timeout("1ms");
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
-        primaryPhase.run();
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
         assertListenerThrows("unassigned primary didn't cause a timeout", listener, UnavailableShardsException.class);
 
         request = new Request(shardId);
         listener = new PlainActionFuture<>();
-        primaryPhase = action.new PrimaryPhase(request, listener);
-        primaryPhase.run();
+        reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
         assertFalse("unassigned primary didn't cause a retry", listener.isDone());
 
         clusterService.setState(state(index, true, ShardRoutingState.STARTED));
         logger.debug("--> primary assigned state:\n{}", clusterService.state().prettyPrint());
 
-        listener.get();
-        assertTrue("request wasn't processed on primary, despite of it being assigned", request.processedOnPrimary.get());
+        final IndexShardRoutingTable shardRoutingTable = clusterService.state().routingTable().index(index).shard(shardId.id());
+        final String primaryNodeId = shardRoutingTable.primaryShard().currentNodeId();
+        final List<CapturingTransport.CapturedRequest> capturedRequests = transport.capturedRequestsByTargetNode().get(primaryNodeId);
+        assertThat(capturedRequests, notNullValue());
+        assertThat(capturedRequests.size(), equalTo(1));
+        assertThat(capturedRequests.get(0).action, equalTo("testAction[p]"));
         assertIndexShardCounter(1);
     }
 
-    public void testRoutingToPrimary() {
+    public void testUnknownIndexOrShardOnReroute() throws InterruptedException {
+        final String index = "test";
+        // no replicas in oder to skip the replication part
+        clusterService.setState(state(index, true,
+                randomBoolean() ? ShardRoutingState.INITIALIZING : ShardRoutingState.UNASSIGNED));
+        logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
+        Request request = new Request(new ShardId("unknown_index", 0)).timeout("1ms");
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
+        assertListenerThrows("must throw index not found exception", listener, IndexNotFoundException.class);
+        request = new Request(new ShardId(index, 10)).timeout("1ms");
+        listener = new PlainActionFuture<>();
+        reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
+        assertListenerThrows("must throw shard not found exception", listener, ShardNotFoundException.class);
+    }
+
+    public void testRoutePhaseExecutesRequest() {
         final String index = "test";
         final ShardId shardId = new ShardId(index, 0);
 
@@ -203,25 +223,126 @@ public class TransportReplicationActionTests extends ESTestCase {
         Request request = new Request(shardId);
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
 
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
-        assertTrue(primaryPhase.checkBlocks());
-        primaryPhase.routeRequestOrPerformLocally(shardRoutingTable.primaryShard(), shardRoutingTable.shardsIt());
-        if (primaryNodeId.equals(clusterService.localNode().id())) {
-            logger.info("--> primary is assigned locally, testing for execution");
-            assertTrue("request failed to be processed on a local primary", request.processedOnPrimary.get());
-            if (transport.capturedRequests().length > 0) {
-                assertIndexShardCounter(2);
-            } else {
-                assertIndexShardCounter(1);
-            }
+        TransportReplicationAction.ReroutePhase reroutePhase = action.new ReroutePhase(request, listener);
+        reroutePhase.run();
+        assertThat(request.shardId(), equalTo(shardId));
+        logger.info("--> primary is assigned to [{}], checking request forwarded", primaryNodeId);
+        final List<CapturingTransport.CapturedRequest> capturedRequests = transport.capturedRequestsByTargetNode().get(primaryNodeId);
+        assertThat(capturedRequests, notNullValue());
+        assertThat(capturedRequests.size(), equalTo(1));
+        if (clusterService.state().nodes().localNodeId().equals(primaryNodeId)) {
+            assertThat(capturedRequests.get(0).action, equalTo("testAction[p]"));
         } else {
-            logger.info("--> primary is assigned to [{}], checking request forwarded", primaryNodeId);
-            final List<CapturingTransport.CapturedRequest> capturedRequests = transport.capturedRequestsByTargetNode().get(primaryNodeId);
-            assertThat(capturedRequests, notNullValue());
-            assertThat(capturedRequests.size(), equalTo(1));
             assertThat(capturedRequests.get(0).action, equalTo("testAction"));
-            assertIndexShardUninitialized();
         }
+        assertIndexShardUninitialized();
+    }
+
+    public void testPrimaryPhaseExecutesRequest() throws InterruptedException, ExecutionException {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, 0);
+        clusterService.setState(state(index, true, ShardRoutingState.STARTED, ShardRoutingState.STARTED));
+        Request request = new Request(shardId).timeout("1ms");
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
+        primaryPhase.run();
+        assertThat("request was not processed on primary", request.processedOnPrimary.get(), equalTo(true));
+        final String replicaNodeId = clusterService.state().getRoutingTable().shardRoutingTable(index, shardId.id()).replicaShards().get(0).currentNodeId();
+        final List<CapturingTransport.CapturedRequest> requests = transport.capturedRequestsByTargetNode().get(replicaNodeId);
+        assertThat(requests, notNullValue());
+        assertThat(requests.size(), equalTo(1));
+        assertThat("replica request was not sent", requests.get(0).action, equalTo("testAction[r]"));
+    }
+
+    public void testAddedReplicaAfterPrimaryOperation() {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, 0);
+        // start with no replicas
+        clusterService.setState(stateWithStartedPrimary(index, true, 0));
+        logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
+        final ClusterState stateWithAddedReplicas = state(index, true, ShardRoutingState.STARTED, randomBoolean() ? ShardRoutingState.INITIALIZING : ShardRoutingState.STARTED);
+
+        final Action actionWithAddedReplicaAfterPrimaryOp = new Action(Settings.EMPTY, "testAction", transportService, clusterService, threadPool) {
+            @Override
+            protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+                final Tuple<Response, Request> operationOnPrimary = super.shardOperationOnPrimary(metaData, shardRequest);
+                // add replicas after primary operation
+                ((TestClusterService) clusterService).setState(stateWithAddedReplicas);
+                logger.debug("--> state after primary operation:\n{}", clusterService.state().prettyPrint());
+                return operationOnPrimary;
+            }
+        };
+
+        Request request = new Request(shardId);
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = actionWithAddedReplicaAfterPrimaryOp.new PrimaryPhase(request, createTransportChannel(listener));
+        primaryPhase.run();
+        assertThat("request was not processed on primary", request.processedOnPrimary.get(), equalTo(true));
+        for (ShardRouting replica : stateWithAddedReplicas.getRoutingTable().shardRoutingTable(index, shardId.id()).replicaShards()) {
+            List<CapturingTransport.CapturedRequest> requests = transport.capturedRequestsByTargetNode().get(replica.currentNodeId());
+            assertThat(requests, notNullValue());
+            assertThat(requests.size(), equalTo(1));
+            assertThat("replica request was not sent", requests.get(0).action, equalTo("testAction[r]"));
+        }
+    }
+
+    public void testRelocatingReplicaAfterPrimaryOperation() {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, 0);
+        // start with a replica
+        clusterService.setState(state(index, true, ShardRoutingState.STARTED,  randomBoolean() ? ShardRoutingState.INITIALIZING : ShardRoutingState.STARTED));
+        logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
+        final ClusterState stateWithRelocatingReplica = state(index, true, ShardRoutingState.STARTED, ShardRoutingState.RELOCATING);
+
+        final Action actionWithRelocatingReplicasAfterPrimaryOp = new Action(Settings.EMPTY, "testAction", transportService, clusterService, threadPool) {
+            @Override
+            protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+                final Tuple<Response, Request> operationOnPrimary = super.shardOperationOnPrimary(metaData, shardRequest);
+                // set replica to relocating
+                ((TestClusterService) clusterService).setState(stateWithRelocatingReplica);
+                logger.debug("--> state after primary operation:\n{}", clusterService.state().prettyPrint());
+                return operationOnPrimary;
+            }
+        };
+
+        Request request = new Request(shardId);
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = actionWithRelocatingReplicasAfterPrimaryOp.new PrimaryPhase(request, createTransportChannel(listener));
+        primaryPhase.run();
+        assertThat("request was not processed on primary", request.processedOnPrimary.get(), equalTo(true));
+        ShardRouting relocatingReplicaShard = stateWithRelocatingReplica.getRoutingTable().shardRoutingTable(index, shardId.id()).replicaShards().get(0);
+        for (String node : new String[] {relocatingReplicaShard.currentNodeId(), relocatingReplicaShard.relocatingNodeId()}) {
+            List<CapturingTransport.CapturedRequest> requests = transport.capturedRequestsByTargetNode().get(node);
+            assertThat(requests, notNullValue());
+            assertThat(requests.size(), equalTo(1));
+            assertThat("replica request was not sent to replica", requests.get(0).action, equalTo("testAction[r]"));
+        }
+    }
+
+    public void testIndexDeletedAfterPrimaryOperation() {
+        final String index = "test";
+        final ShardId shardId = new ShardId(index, 0);
+        clusterService.setState(state(index, true, ShardRoutingState.STARTED, ShardRoutingState.STARTED));
+        logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
+        final ClusterState stateWithDeletedIndex = state(index + "_new", true, ShardRoutingState.STARTED, ShardRoutingState.RELOCATING);
+
+        final Action actionWithDeletedIndexAfterPrimaryOp = new Action(Settings.EMPTY, "testAction", transportService, clusterService, threadPool) {
+            @Override
+            protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+                final Tuple<Response, Request> operationOnPrimary = super.shardOperationOnPrimary(metaData, shardRequest);
+                // delete index after primary op
+                ((TestClusterService) clusterService).setState(stateWithDeletedIndex);
+                logger.debug("--> state after primary operation:\n{}", clusterService.state().prettyPrint());
+                return operationOnPrimary;
+            }
+        };
+
+        Request request = new Request(shardId);
+        PlainActionFuture<Response> listener = new PlainActionFuture<>();
+        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = actionWithDeletedIndexAfterPrimaryOp.new PrimaryPhase(request, createTransportChannel(listener));
+        primaryPhase.run();
+        assertThat("request was not processed on primary", request.processedOnPrimary.get(), equalTo(true));
+        assertThat("replication phase should be skipped if index gets deleted after primary operation", transport.capturedRequestsByTargetNode().size(), equalTo(0));
     }
 
     public void testWriteConsistency() throws ExecutionException, InterruptedException {
@@ -266,10 +387,9 @@ public class TransportReplicationActionTests extends ESTestCase {
 
         final IndexShardRoutingTable shardRoutingTable = clusterService.state().routingTable().index(index).shard(shardId.id());
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
-
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         if (passesWriteConsistency) {
-            assertThat(primaryPhase.checkWriteConsistency(shardRoutingTable.primaryShard()), nullValue());
+            assertThat(primaryPhase.checkWriteConsistency(shardRoutingTable.primaryShard().shardId()), nullValue());
             primaryPhase.run();
             assertTrue("operations should have been perform, consistency level is met", request.processedOnPrimary.get());
             if (assignedReplicas > 0) {
@@ -278,14 +398,18 @@ public class TransportReplicationActionTests extends ESTestCase {
                 assertIndexShardCounter(1);
             }
         } else {
-            assertThat(primaryPhase.checkWriteConsistency(shardRoutingTable.primaryShard()), notNullValue());
+            assertThat(primaryPhase.checkWriteConsistency(shardRoutingTable.primaryShard().shardId()), notNullValue());
             primaryPhase.run();
             assertFalse("operations should not have been perform, consistency level is *NOT* met", request.processedOnPrimary.get());
+            assertListenerThrows("should throw exception to trigger retry", listener, UnavailableShardsException.class);
             assertIndexShardUninitialized();
             for (int i = 0; i < replicaStates.length; i++) {
                 replicaStates[i] = ShardRoutingState.STARTED;
             }
             clusterService.setState(state(index, true, ShardRoutingState.STARTED, replicaStates));
+            listener = new PlainActionFuture<>();
+            primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
+            primaryPhase.run();
             assertTrue("once the consistency level met, operation should continue", request.processedOnPrimary.get());
             assertIndexShardCounter(2);
         }
@@ -340,23 +464,19 @@ public class TransportReplicationActionTests extends ESTestCase {
 
 
     protected void runReplicateTest(IndexShardRoutingTable shardRoutingTable, int assignedReplicas, int totalShards) throws InterruptedException, ExecutionException {
-        final ShardRouting primaryShard = shardRoutingTable.primaryShard();
         final ShardIterator shardIt = shardRoutingTable.shardsIt();
         final ShardId shardId = shardIt.shardId();
-        final Request request = new Request();
-        PlainActionFuture<Response> listener = new PlainActionFuture<>();
-
+        final Request request = new Request(shardId);
+        final PlainActionFuture<Response> listener = new PlainActionFuture<>();
         logger.debug("expecting [{}] assigned replicas, [{}] total shards. using state: \n{}", assignedReplicas, totalShards, clusterService.state().prettyPrint());
 
-        final TransportReplicationAction<Request, Request, Response>.InternalRequest internalRequest = action.new InternalRequest(request);
-        internalRequest.concreteIndex(shardId.index().name());
         Releasable reference = getOrCreateIndexShardOperationsCounter();
         assertIndexShardCounter(2);
         // TODO: set a default timeout
         TransportReplicationAction<Request, Request, Response>.ReplicationPhase replicationPhase =
-                action.new ReplicationPhase(shardIt, request,
-                        new Response(), new ClusterStateObserver(clusterService, logger),
-                        primaryShard, internalRequest, listener, reference, null);
+                action.new ReplicationPhase(request,
+                        new Response(),
+                        request.shardId(), createTransportChannel(listener), reference, null);
 
         assertThat(replicationPhase.totalShards(), equalTo(totalShards));
         assertThat(replicationPhase.pending(), equalTo(assignedReplicas));
@@ -401,7 +521,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
         assertThat(listener.isDone(), equalTo(true));
         Response response = listener.get();
-        final ActionWriteResponse.ShardInfo shardInfo = response.getShardInfo();
+        final ReplicationResponse.ShardInfo shardInfo = response.getShardInfo();
         assertThat(shardInfo.getFailed(), equalTo(criticalFailures));
         assertThat(shardInfo.getFailures(), arrayWithSize(criticalFailures));
         assertThat(shardInfo.getSuccessful(), equalTo(successful));
@@ -433,7 +553,7 @@ public class TransportReplicationActionTests extends ESTestCase {
          * However, this failure would only become apparent once listener.get is called. Seems a little implicit.
          * */
         action = new ActionWithDelay(Settings.EMPTY, "testActionWithExceptions", transportService, clusterService, threadPool);
-        final TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
+        final TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         Thread t = new Thread() {
             @Override
             public void run() {
@@ -464,7 +584,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Request request = new Request(shardId).timeout("100ms");
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         primaryPhase.run();
         assertIndexShardCounter(2);
         assertThat(transport.capturedRequests().length, equalTo(1));
@@ -473,7 +593,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         assertIndexShardCounter(1);
         transport.clear();
         request = new Request(shardId).timeout("100ms");
-        primaryPhase = action.new PrimaryPhase(request, listener);
+        primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         primaryPhase.run();
         assertIndexShardCounter(2);
         CapturingTransport.CapturedRequest[] replicationRequests = transport.capturedRequests();
@@ -498,7 +618,7 @@ public class TransportReplicationActionTests extends ESTestCase {
             @Override
             public void run() {
                 try {
-                    replicaOperationTransportHandler.messageReceived(new Request(), createTransportChannel());
+                    replicaOperationTransportHandler.messageReceived(new Request(), createTransportChannel(new PlainActionFuture<>()));
                 } catch (Exception e) {
                 }
             }
@@ -515,7 +635,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         action = new ActionWithExceptions(Settings.EMPTY, "testActionWithExceptions", transportService, clusterService, threadPool);
         final Action.ReplicaOperationTransportHandler replicaOperationTransportHandlerForException = action.new ReplicaOperationTransportHandler();
         try {
-            replicaOperationTransportHandlerForException.messageReceived(new Request(shardId), createTransportChannel());
+            replicaOperationTransportHandlerForException.messageReceived(new Request(shardId), createTransportChannel(new PlainActionFuture<>()));
             fail();
         } catch (Throwable t2) {
         }
@@ -531,7 +651,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         logger.debug("--> using initial state:\n{}", clusterService.state().prettyPrint());
         Request request = new Request(shardId).timeout("100ms");
         PlainActionFuture<Response> listener = new PlainActionFuture<>();
-        TransportReplicationAction<Request, Request, Response>.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, listener);
+        TransportReplicationAction.PrimaryPhase primaryPhase = action.new PrimaryPhase(request, createTransportChannel(listener));
         primaryPhase.run();
         // no replica request should have been sent yet
         assertThat(transport.capturedRequests().length, equalTo(0));
@@ -559,7 +679,6 @@ public class TransportReplicationActionTests extends ESTestCase {
     }
 
     public static class Request extends ReplicationRequest<Request> {
-        int shardId;
         public AtomicBoolean processedOnPrimary = new AtomicBoolean();
         public AtomicInteger processedOnReplicas = new AtomicInteger();
 
@@ -568,25 +687,23 @@ public class TransportReplicationActionTests extends ESTestCase {
 
         Request(ShardId shardId) {
             this();
-            this.shardId = shardId.id();
-            this.index(shardId.index().name());
+            this.shardId = shardId;
+            this.index = shardId.getIndex();
             // keep things simple
         }
 
         @Override
         public void writeTo(StreamOutput out) throws IOException {
             super.writeTo(out);
-            out.writeVInt(shardId);
         }
 
         @Override
         public void readFrom(StreamInput in) throws IOException {
             super.readFrom(in);
-            shardId = in.readVInt();
         }
     }
 
-    static class Response extends ActionWriteResponse {
+    static class Response extends ReplicationResponse {
     }
 
     class Action extends TransportReplicationAction<Request, Request, Response> {
@@ -605,23 +722,18 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected Tuple<Response, Request> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-            boolean executedBefore = shardRequest.request.processedOnPrimary.getAndSet(true);
+        protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+            boolean executedBefore = shardRequest.processedOnPrimary.getAndSet(true);
             assert executedBefore == false : "request has already been executed on the primary";
-            return new Tuple<>(new Response(), shardRequest.request);
+            return new Tuple<>(new Response(), shardRequest);
         }
 
         @Override
-        protected void shardOperationOnReplica(ShardId shardId, Request request) {
+        protected void shardOperationOnReplica(Request request) {
             request.processedOnReplicas.incrementAndGet();
         }
 
         @Override
-        protected ShardIterator shards(ClusterState clusterState, InternalRequest request) {
-            return clusterState.getRoutingTable().index(request.concreteIndex()).shard(request.request().shardId).shardsIt();
-        }
-
-        @Override
         protected boolean checkWriteConsistency() {
             return false;
         }
@@ -659,8 +771,8 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected Tuple<Response, Request> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
-            return throwException(shardRequest.shardId);
+        protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
+            return throwException(shardRequest.shardId());
         }
 
         private Tuple<Response, Request> throwException(ShardId shardId) {
@@ -681,8 +793,8 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected void shardOperationOnReplica(ShardId shardId, Request shardRequest) {
-            throwException(shardRequest.internalShardId);
+        protected void shardOperationOnReplica(Request shardRequest) {
+            throwException(shardRequest.shardId());
         }
     }
 
@@ -697,9 +809,9 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected Tuple<Response, Request> shardOperationOnPrimary(ClusterState clusterState, PrimaryOperationRequest shardRequest) throws Throwable {
+        protected Tuple<Response, Request> shardOperationOnPrimary(MetaData metaData, Request shardRequest) throws Throwable {
             awaitLatch();
-            return new Tuple<>(new Response(), shardRequest.request);
+            return new Tuple<>(new Response(), shardRequest);
         }
 
         private void awaitLatch() throws InterruptedException {
@@ -708,7 +820,7 @@ public class TransportReplicationActionTests extends ESTestCase {
         }
 
         @Override
-        protected void shardOperationOnReplica(ShardId shardId, Request shardRequest) {
+        protected void shardOperationOnReplica(Request shardRequest) {
             try {
                 awaitLatch();
             } catch (InterruptedException e) {
@@ -720,7 +832,7 @@ public class TransportReplicationActionTests extends ESTestCase {
     /*
     * Transport channel that is needed for replica operation testing.
     * */
-    public TransportChannel createTransportChannel() {
+    public TransportChannel createTransportChannel(final PlainActionFuture<Response> listener) {
         return new TransportChannel() {
 
             @Override
@@ -735,14 +847,17 @@ public class TransportReplicationActionTests extends ESTestCase {
 
             @Override
             public void sendResponse(TransportResponse response) throws IOException {
+                listener.onResponse(((Response) response));
             }
 
             @Override
             public void sendResponse(TransportResponse response, TransportResponseOptions options) throws IOException {
+                listener.onResponse(((Response) response));
             }
 
             @Override
             public void sendResponse(Throwable error) throws IOException {
+                listener.onFailure(error);
             }
         };
     }
diff --git a/core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java b/core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java
index 28d4b0f..0eb7c07 100644
--- a/core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java
+++ b/core/src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqIT.java
@@ -141,8 +141,7 @@ public class GetTermVectorsCheckDocFreqIT extends ESIntegTestCase {
         xBuilder.startObject();
         response.toXContent(xBuilder, null);
         xBuilder.endObject();
-        BytesStream bytesStream = xBuilder.bytesStream();
-        String utf8 = bytesStream.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
+        String utf8 = xBuilder.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
         String expectedString = "{\"_index\":\"test\",\"_type\":\"type1\",\"_id\":\""
                 + i
                 + "\",\"_version\":1,\"found\":true,\"term_vectors\":{\"field\":{\"terms\":{\"brown\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":2,\"start_offset\":10,\"end_offset\":15,\"payload\":\"d29yZA==\"}]},\"dog\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":8,\"start_offset\":40,\"end_offset\":43,\"payload\":\"d29yZA==\"}]},\"fox\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":3,\"start_offset\":16,\"end_offset\":19,\"payload\":\"d29yZA==\"}]},\"jumps\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":4,\"start_offset\":20,\"end_offset\":25,\"payload\":\"d29yZA==\"}]},\"lazy\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":7,\"start_offset\":35,\"end_offset\":39,\"payload\":\"d29yZA==\"}]},\"over\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":5,\"start_offset\":26,\"end_offset\":30,\"payload\":\"d29yZA==\"}]},\"quick\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":1,\"start_offset\":4,\"end_offset\":9,\"payload\":\"d29yZA==\"}]},\"the\":{\"doc_freq\":15,\"ttf\":30,\"term_freq\":2,\"tokens\":[{\"position\":0,\"start_offset\":0,\"end_offset\":3,\"payload\":\"d29yZA==\"},{\"position\":6,\"start_offset\":31,\"end_offset\":34,\"payload\":\"d29yZA==\"}]}}}}}";
@@ -198,8 +197,7 @@ public class GetTermVectorsCheckDocFreqIT extends ESIntegTestCase {
         xBuilder.startObject();
         response.toXContent(xBuilder, null);
         xBuilder.endObject();
-        BytesStream bytesStream = xBuilder.bytesStream();
-        String utf8 = bytesStream.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
+        String utf8 = xBuilder.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
         String expectedString = "{\"_index\":\"test\",\"_type\":\"type1\",\"_id\":\""
                 + i
                 + "\",\"_version\":1,\"found\":true,\"term_vectors\":{\"field\":{\"field_statistics\":{\"sum_doc_freq\":120,\"doc_count\":15,\"sum_ttf\":135},\"terms\":{\"brown\":{\"term_freq\":1,\"tokens\":[{\"position\":2,\"start_offset\":10,\"end_offset\":15,\"payload\":\"d29yZA==\"}]},\"dog\":{\"term_freq\":1,\"tokens\":[{\"position\":8,\"start_offset\":40,\"end_offset\":43,\"payload\":\"d29yZA==\"}]},\"fox\":{\"term_freq\":1,\"tokens\":[{\"position\":3,\"start_offset\":16,\"end_offset\":19,\"payload\":\"d29yZA==\"}]},\"jumps\":{\"term_freq\":1,\"tokens\":[{\"position\":4,\"start_offset\":20,\"end_offset\":25,\"payload\":\"d29yZA==\"}]},\"lazy\":{\"term_freq\":1,\"tokens\":[{\"position\":7,\"start_offset\":35,\"end_offset\":39,\"payload\":\"d29yZA==\"}]},\"over\":{\"term_freq\":1,\"tokens\":[{\"position\":5,\"start_offset\":26,\"end_offset\":30,\"payload\":\"d29yZA==\"}]},\"quick\":{\"term_freq\":1,\"tokens\":[{\"position\":1,\"start_offset\":4,\"end_offset\":9,\"payload\":\"d29yZA==\"}]},\"the\":{\"term_freq\":2,\"tokens\":[{\"position\":0,\"start_offset\":0,\"end_offset\":3,\"payload\":\"d29yZA==\"},{\"position\":6,\"start_offset\":31,\"end_offset\":34,\"payload\":\"d29yZA==\"}]}}}}}";
@@ -258,8 +256,7 @@ public class GetTermVectorsCheckDocFreqIT extends ESIntegTestCase {
         xBuilder.startObject();
         response.toXContent(xBuilder, ToXContent.EMPTY_PARAMS);
         xBuilder.endObject();
-        BytesStream bytesStream = xBuilder.bytesStream();
-        String utf8 = bytesStream.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
+        String utf8 = xBuilder.bytes().toUtf8().replaceFirst("\"took\":\\d+,", "");;
         String expectedString = "{\"_index\":\"test\",\"_type\":\"type1\",\"_id\":\""
                 + i
                 + "\",\"_version\":1,\"found\":true,\"term_vectors\":{\"field\":{\"field_statistics\":{\"sum_doc_freq\":120,\"doc_count\":15,\"sum_ttf\":135},\"terms\":{\"brown\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":2,\"start_offset\":10,\"end_offset\":15,\"payload\":\"d29yZA==\"}]},\"dog\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":8,\"start_offset\":40,\"end_offset\":43,\"payload\":\"d29yZA==\"}]},\"fox\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":3,\"start_offset\":16,\"end_offset\":19,\"payload\":\"d29yZA==\"}]},\"jumps\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":4,\"start_offset\":20,\"end_offset\":25,\"payload\":\"d29yZA==\"}]},\"lazy\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":7,\"start_offset\":35,\"end_offset\":39,\"payload\":\"d29yZA==\"}]},\"over\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":5,\"start_offset\":26,\"end_offset\":30,\"payload\":\"d29yZA==\"}]},\"quick\":{\"doc_freq\":15,\"ttf\":15,\"term_freq\":1,\"tokens\":[{\"position\":1,\"start_offset\":4,\"end_offset\":9,\"payload\":\"d29yZA==\"}]},\"the\":{\"doc_freq\":15,\"ttf\":30,\"term_freq\":2,\"tokens\":[{\"position\":0,\"start_offset\":0,\"end_offset\":3,\"payload\":\"d29yZA==\"},{\"position\":6,\"start_offset\":31,\"end_offset\":34,\"payload\":\"d29yZA==\"}]}}}}}";
diff --git a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
index 81b09c8..1cbe05d 100644
--- a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
+++ b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
@@ -759,7 +759,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
             admin().indices().prepareAliases().addAliasAction(AliasAction.newAddAliasAction("index1", null)).get();
             fail("Expected ActionRequestValidationException");
         } catch (ActionRequestValidationException e) {
-            assertThat(e.getMessage(), containsString("requires an [alias] to be set"));
+            assertThat(e.getMessage(), containsString("[alias] may not be empty string"));
         }
     }
 
@@ -768,7 +768,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
             admin().indices().prepareAliases().addAliasAction(AliasAction.newAddAliasAction("index1", "")).get();
             fail("Expected ActionRequestValidationException");
         } catch (ActionRequestValidationException e) {
-            assertThat(e.getMessage(), containsString("requires an [alias] to be set"));
+            assertThat(e.getMessage(), containsString("[alias] may not be empty string"));
         }
     }
 
@@ -865,7 +865,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
         assertAcked(prepareCreate("test")
                 .addMapping("type", "field", "type=string")
                 .addAlias(new Alias("alias1"))
-                .addAlias(new Alias("alias2").filter(QueryBuilders.missingQuery("field")))
+                .addAlias(new Alias("alias2").filter(QueryBuilders.boolQuery().mustNot(QueryBuilders.existsQuery("field"))))
                 .addAlias(new Alias("alias3").indexRouting("index").searchRouting("search")));
 
         checkAliases();
diff --git a/core/src/test/java/org/elasticsearch/benchmark/aliases/AliasesBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/aliases/AliasesBenchmark.java
deleted file mode 100644
index 7b5d489..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/aliases/AliasesBenchmark.java
+++ /dev/null
@@ -1,140 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.aliases;
-
-import org.elasticsearch.action.admin.indices.alias.IndicesAliasesRequestBuilder;
-import org.elasticsearch.action.admin.indices.alias.get.GetAliasesResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.metadata.AliasMetaData;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.monitor.jvm.JvmStats;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import java.io.IOException;
-import java.util.List;
-
-/**
- */
-public class AliasesBenchmark {
-
-    private final static String INDEX_NAME = "my-index";
-
-    public static void main(String[] args) throws IOException {
-        int NUM_ADDITIONAL_NODES = 1;
-        int BASE_ALIAS_COUNT = 100000;
-        int NUM_ADD_ALIAS_REQUEST = 1000;
-
-        Settings settings = Settings.settingsBuilder()
-                .put("node.master", false).build();
-        Node node1 = NodeBuilder.nodeBuilder().settings(
-                Settings.settingsBuilder().put(settings).put("node.master", true)
-        ).node();
-
-        Node[] otherNodes = new Node[NUM_ADDITIONAL_NODES];
-        for (int i = 0; i < otherNodes.length; i++) {
-            otherNodes[i] = NodeBuilder.nodeBuilder().settings(settings).node();
-        }
-
-        Client client = node1.client();
-        try {
-            client.admin().indices().prepareCreate(INDEX_NAME).execute().actionGet();
-        } catch (IndexAlreadyExistsException e) {}
-        client.admin().cluster().prepareHealth().setWaitForYellowStatus().execute().actionGet();
-        int numberOfAliases = countAliases(client);
-        System.out.println("Number of aliases: " + numberOfAliases);
-
-        if (numberOfAliases < BASE_ALIAS_COUNT) {
-            int diff = BASE_ALIAS_COUNT - numberOfAliases;
-            System.out.println("Adding " + diff + " more aliases to get to the start amount of " + BASE_ALIAS_COUNT + " aliases");
-            IndicesAliasesRequestBuilder builder = client.admin().indices().prepareAliases();
-            for (int i = 1; i <= diff; i++) {
-                builder.addAlias(INDEX_NAME, Strings.randomBase64UUID());
-                if (i % 1000 == 0) {
-                    builder.execute().actionGet();
-                    builder = client.admin().indices().prepareAliases();
-                }
-            }
-            if (!builder.request().getAliasActions().isEmpty()) {
-                builder.execute().actionGet();
-            }
-        } else if (numberOfAliases > BASE_ALIAS_COUNT) {
-            IndicesAliasesRequestBuilder builder = client.admin().indices().prepareAliases();
-            int diff = numberOfAliases - BASE_ALIAS_COUNT;
-            System.out.println("Removing " + diff + " aliases to get to the start amount of " + BASE_ALIAS_COUNT + " aliases");
-            List<AliasMetaData> aliases= client.admin().indices().prepareGetAliases("*")
-                    .addIndices(INDEX_NAME)
-                    .execute().actionGet().getAliases().get(INDEX_NAME);
-            for (int i = 0; i <= diff; i++) {
-                builder.removeAlias(INDEX_NAME, aliases.get(i).alias());
-                if (i % 1000 == 0) {
-                    builder.execute().actionGet();
-                    builder = client.admin().indices().prepareAliases();
-                }
-            }
-            if (!builder.request().getAliasActions().isEmpty()) {
-                builder.execute().actionGet();
-            }
-        }
-
-        numberOfAliases = countAliases(client);
-        System.out.println("Number of aliases: " + numberOfAliases);
-
-        long totalTime = 0;
-        int max = numberOfAliases + NUM_ADD_ALIAS_REQUEST;
-        for (int i = numberOfAliases; i <= max; i++) {
-            if (i != numberOfAliases && i % 100 == 0) {
-                long avgTime = totalTime / 100;
-                System.out.println("Added [" + (i - numberOfAliases) + "] aliases. Avg create time: "  + avgTime + " ms");
-                System.out.println("Heap used [" + JvmStats.jvmStats().getMem().getHeapUsed() + "]");
-                totalTime = 0;
-            }
-
-            long time = System.currentTimeMillis();
-//            String filter = termFilter("field" + i, "value" + i).toXContent(XContentFactory.jsonBuilder(), null).string();
-            client.admin().indices().prepareAliases().addAlias(INDEX_NAME, Strings.randomBase64UUID()/*, filter*/)
-                    .execute().actionGet();
-            totalTime += System.currentTimeMillis() - time;
-        }
-        System.gc();
-        System.out.println("Final heap used [" + JvmStats.jvmStats().getMem().getHeapUsed() + "]");
-        System.out.println("Number of aliases: " + countAliases(client));
-
-        client.close();
-        node1.close();
-        for (Node otherNode : otherNodes) {
-            otherNode.close();
-        }
-    }
-
-    private static int countAliases(Client client) {
-        GetAliasesResponse response = client.admin().indices().prepareGetAliases("*")
-                .addIndices(INDEX_NAME)
-                .execute().actionGet();
-        if (response.getAliases().isEmpty()) {
-            return 0;
-        } else {
-            return response.getAliases().get(INDEX_NAME).size();
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/bloom/BloomBench.java b/core/src/test/java/org/elasticsearch/benchmark/bloom/BloomBench.java
deleted file mode 100644
index 15745fc..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/bloom/BloomBench.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.bloom;
-
-import org.apache.lucene.codecs.bloom.FuzzySet;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.util.BloomFilter;
-
-import java.security.SecureRandom;
-
-/**
- */
-public class BloomBench {
-
-    public static void main(String[] args) throws Exception {
-        SecureRandom random = new SecureRandom();
-        final int ELEMENTS = (int) SizeValue.parseSizeValue("1m").singles();
-        final double fpp = 0.01;
-        BloomFilter gFilter = BloomFilter.create(ELEMENTS, fpp);
-        System.out.println("G SIZE: " + new ByteSizeValue(gFilter.getSizeInBytes()));
-
-        FuzzySet lFilter = FuzzySet.createSetBasedOnMaxMemory((int) gFilter.getSizeInBytes());
-        //FuzzySet lFilter = FuzzySet.createSetBasedOnQuality(ELEMENTS, 0.97f);
-
-        for (int i = 0; i < ELEMENTS; i++) {
-            BytesRef bytesRef = new BytesRef(Strings.randomBase64UUID(random));
-            gFilter.put(bytesRef);
-            lFilter.addValue(bytesRef);
-        }
-
-        int lFalse = 0;
-        int gFalse = 0;
-        for (int i = 0; i < ELEMENTS; i++) {
-            BytesRef bytesRef = new BytesRef(Strings.randomBase64UUID(random));
-            if (gFilter.mightContain(bytesRef)) {
-                gFalse++;
-            }
-            if (lFilter.contains(bytesRef) == FuzzySet.ContainsResult.MAYBE) {
-                lFalse++;
-            }
-        }
-        System.out.println("Failed positives, g[" + gFalse + "], l[" + lFalse + "]");
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/breaker/CircuitBreakerBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/breaker/CircuitBreakerBenchmark.java
deleted file mode 100644
index f6b0497..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/breaker/CircuitBreakerBenchmark.java
+++ /dev/null
@@ -1,189 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.breaker;
-
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.breaker.CircuitBreaker;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-
-import java.util.UUID;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static junit.framework.Assert.assertNotNull;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-
-/**
- * Benchmarks for different implementations of the circuit breaker
- */
-public class CircuitBreakerBenchmark {
-
-    private static final String INDEX = UUID.randomUUID().toString();
-    private static final int QUERIES = 100;
-    private static final int BULK_SIZE = 100;
-    private static final int NUM_DOCS = 2_000_000;
-    private static final int AGG_SIZE = 25;
-
-    private static void switchToNoop(Client client) {
-        Settings settings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING, CircuitBreaker.Type.NOOP)
-                .build();
-        client.admin().cluster().prepareUpdateSettings().setTransientSettings(settings).execute().actionGet();
-    }
-
-    private static void switchToMemory(Client client) {
-        Settings settings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING, CircuitBreaker.Type.MEMORY)
-                .build();
-        client.admin().cluster().prepareUpdateSettings().setTransientSettings(settings).execute().actionGet();
-    }
-
-    private static void runSingleThreadedQueries(Client client) {
-        long totalTime = 0;
-        for (int i = 0; i < QUERIES; i++) {
-            if (i % 10 == 0) {
-                System.out.println("--> query #" + i);
-            }
-            SearchResponse resp = client.prepareSearch(INDEX).setQuery(matchAllQuery())
-                    .addAggregation(
-                            terms("myterms")
-                                    .size(AGG_SIZE)
-                                    .field("num")
-                    ).setSize(0).get();
-            Terms terms = resp.getAggregations().get("myterms");
-            assertNotNull("term aggs were calculated", terms);
-            totalTime += resp.getTookInMillis();
-        }
-
-        System.out.println("--> single threaded average time: " + (totalTime / QUERIES) + "ms");
-    }
-
-    private static void runMultiThreadedQueries(final Client client) throws Exception {
-        final AtomicLong totalThreadedTime = new AtomicLong(0);
-        int THREADS = 10;
-        Thread threads[] = new Thread[THREADS];
-        for (int i = 0; i < THREADS; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    long tid = Thread.currentThread().getId();
-                    for (int i = 0; i < QUERIES; i++) {
-                        if (i % 30 == 0) {
-                            System.out.println("--> [" + tid + "] query # "+ i);
-                        }
-                        SearchResponse resp = client.prepareSearch(INDEX).setQuery(matchAllQuery())
-                                .addAggregation(
-                                        terms("myterms")
-                                                .size(AGG_SIZE)
-                                                .field("num")
-                                ).setSize(0).get();
-                        Terms terms = resp.getAggregations().get("myterms");
-                        assertNotNull("term aggs were calculated", terms);
-                        totalThreadedTime.addAndGet(resp.getTookInMillis());
-                    }
-                }
-            });
-        }
-
-        System.out.println("--> starting " + THREADS + " threads for parallel aggregating");
-        for (Thread t : threads) {
-            t.start();
-        }
-
-        for (Thread t : threads) {
-            t.join();
-        }
-
-        System.out.println("--> threaded average time: " + (totalThreadedTime.get() / (THREADS * QUERIES)) + "ms");
-    }
-
-    public static void main(String args[]) throws Exception {
-        Node node = NodeBuilder.nodeBuilder().settings(Settings.settingsBuilder()).node();
-        final Client client = node.client();
-        try {
-            try {
-                client.admin().indices().prepareDelete(INDEX).get();
-            } catch (Exception e) {
-                // Ignore
-            }
-            try {
-                client.admin().indices().prepareCreate(INDEX).setSettings(
-                        settingsBuilder().put("number_of_shards", 2).put("number_of_replicas", 0)).get();
-            } catch (IndexAlreadyExistsException e) {}
-            client.admin().cluster().prepareHealth().setWaitForYellowStatus().execute().actionGet();
-
-
-            System.out.println("--> indexing: " + NUM_DOCS + " documents...");
-            BulkRequestBuilder bulkBuilder = client.prepareBulk();
-            for (int i = 0; i < NUM_DOCS; i++) {
-                bulkBuilder.add(client.prepareIndex(INDEX, "doc").setSource("num", i));
-                if (i % BULK_SIZE == 0) {
-                    // Send off bulk request
-                    bulkBuilder.get();
-                    // Create a new holder
-                    bulkBuilder = client.prepareBulk();
-                }
-            }
-            bulkBuilder.get();
-            client.admin().indices().prepareRefresh(INDEX).get();
-            SearchResponse countResp = client.prepareSearch(INDEX).setQuery(matchAllQuery()).setSize(0).get();
-            assert countResp.getHits().getTotalHits() == NUM_DOCS : "all docs should be indexed";
-
-            final int warmupCount = 100;
-            for (int i = 0; i < warmupCount; i++) {
-                if (i % 15 == 0) {
-                    System.out.println("--> warmup #" + i);
-                }
-                SearchResponse resp = client.prepareSearch(INDEX).setQuery(matchAllQuery())
-                        .addAggregation(
-                                terms("myterms")
-                                        .size(AGG_SIZE)
-                                        .field("num")
-                        ).setSize(0).get();
-                Terms terms = resp.getAggregations().get("myterms");
-                assertNotNull("term aggs were calculated", terms);
-            }
-
-            System.out.println("--> running single-threaded tests");
-            runSingleThreadedQueries(client);
-            System.out.println("--> switching to NOOP breaker");
-            switchToNoop(client);
-            runSingleThreadedQueries(client);
-            switchToMemory(client);
-
-            System.out.println("--> running multi-threaded tests");
-            runMultiThreadedQueries(client);
-            System.out.println("--> switching to NOOP breaker");
-            switchToNoop(client);
-            runMultiThreadedQueries(client);
-        } finally {
-            client.close();
-            node.close();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/checksum/ChecksumBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/checksum/ChecksumBenchmark.java
deleted file mode 100644
index 660d042..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/checksum/ChecksumBenchmark.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.checksum;
-
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-
-import java.security.MessageDigest;
-import java.util.zip.Adler32;
-import java.util.zip.CRC32;
-
-/**
- *
- */
-public class ChecksumBenchmark {
-
-    public static final int BATCH_SIZE = 16 * 1024;
-
-    public static void main(String[] args) throws Exception {
-        System.out.println("Warning up");
-        long warmSize = ByteSizeValue.parseBytesSizeValue("1g", null).bytes();
-        crc(warmSize);
-        adler(warmSize);
-        md5(warmSize);
-
-        long dataSize = ByteSizeValue.parseBytesSizeValue("10g", null).bytes();
-        System.out.println("Running size: " + dataSize);
-        crc(dataSize);
-        adler(dataSize);
-        md5(dataSize);
-    }
-
-    private static void crc(long dataSize) {
-        long start = System.currentTimeMillis();
-        CRC32 crc = new CRC32();
-        byte[] data = new byte[BATCH_SIZE];
-        long iter = dataSize / BATCH_SIZE;
-        for (long i = 0; i < iter; i++) {
-            crc.update(data);
-        }
-        crc.getValue();
-        System.out.println("CRC took " + new TimeValue(System.currentTimeMillis() - start));
-    }
-
-    private static void adler(long dataSize) {
-        long start = System.currentTimeMillis();
-        Adler32 crc = new Adler32();
-        byte[] data = new byte[BATCH_SIZE];
-        long iter = dataSize / BATCH_SIZE;
-        for (long i = 0; i < iter; i++) {
-            crc.update(data);
-        }
-        crc.getValue();
-        System.out.println("Adler took " + new TimeValue(System.currentTimeMillis() - start));
-    }
-
-    private static void md5(long dataSize) throws Exception {
-        long start = System.currentTimeMillis();
-        byte[] data = new byte[BATCH_SIZE];
-        long iter = dataSize / BATCH_SIZE;
-        MessageDigest digest = MessageDigest.getInstance("MD5");
-        for (long i = 0; i < iter; i++) {
-            digest.update(data);
-        }
-        digest.digest();
-        System.out.println("md5 took " + new TimeValue(System.currentTimeMillis() - start));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
deleted file mode 100644
index c50574d..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.cluster;
-
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.RoutingTable;
-import org.elasticsearch.cluster.routing.allocation.AllocationService;
-import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.test.ESAllocationTestCase;
-
-import java.util.Random;
-
-import static java.util.Collections.singletonMap;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-
-public class ClusterAllocationRerouteBenchmark {
-
-    private static final ESLogger logger = Loggers.getLogger(ClusterAllocationRerouteBenchmark.class);
-
-    public static void main(String[] args) {
-        final int numberOfRuns = 1;
-        final int numIndices = 5 * 365; // five years
-        final int numShards = 6;
-        final int numReplicas = 2;
-        final int numberOfNodes = 30;
-        final int numberOfTags = 2;
-        AllocationService strategy = ESAllocationTestCase.createAllocationService(Settings.builder()
-            .put("cluster.routing.allocation.awareness.attributes", "tag")
-            .build(), new Random(1));
-
-        MetaData.Builder mb = MetaData.builder();
-        for (int i = 1; i <= numIndices; i++) {
-            mb.put(IndexMetaData.builder("test_" + i).numberOfShards(numShards).numberOfReplicas(numReplicas));
-        }
-        MetaData metaData = mb.build();
-        RoutingTable.Builder rb = RoutingTable.builder();
-        for (int i = 1; i <= numIndices; i++) {
-            rb.addAsNew(metaData.index("test_" + i));
-        }
-        RoutingTable routingTable = rb.build();
-        DiscoveryNodes.Builder nb = DiscoveryNodes.builder();
-        for (int i = 1; i <= numberOfNodes; i++) {
-            nb.put(ESAllocationTestCase.newNode("node" + i, singletonMap("tag", "tag_" + (i % numberOfTags))));
-        }
-        ClusterState initialClusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).nodes(nb).build();
-
-        long start = System.currentTimeMillis();
-        for (int i = 0; i < numberOfRuns; i++) {
-            logger.info("[{}] starting... ", i);
-            long runStart = System.currentTimeMillis();
-            ClusterState clusterState = initialClusterState;
-            while (clusterState.getRoutingNodes().hasUnassignedShards()) {
-                logger.info("[{}] remaining unassigned {}", i, clusterState.getRoutingNodes().unassigned().size());
-                RoutingAllocation.Result result = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));
-                clusterState = ClusterState.builder(clusterState).routingResult(result).build();
-                result = strategy.reroute(clusterState, "reroute");
-                clusterState = ClusterState.builder(clusterState).routingResult(result).build();
-            }
-            logger.info("[{}] took {}", i, TimeValue.timeValueMillis(System.currentTimeMillis() - runStart));
-        }
-        long took = System.currentTimeMillis() - start;
-        logger.info("total took {}, AVG {}", TimeValue.timeValueMillis(took), TimeValue.timeValueMillis(took / numberOfRuns));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/common/lucene/uidscan/LuceneUidScanBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/common/lucene/uidscan/LuceneUidScanBenchmark.java
deleted file mode 100644
index fe548b9..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/common/lucene/uidscan/LuceneUidScanBenchmark.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.common.lucene.uidscan;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.*;
-import org.apache.lucene.store.FSDirectory;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.unit.SizeValue;
-
-import java.nio.file.Paths;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.ThreadLocalRandom;
-
-/**
- *
- */
-public class LuceneUidScanBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        FSDirectory dir = FSDirectory.open(PathUtils.get("work/test"));
-        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Lucene.STANDARD_ANALYZER));
-
-        final int NUMBER_OF_THREADS = 2;
-        final long INDEX_COUNT = SizeValue.parseSizeValue("1m").singles();
-        final long SCAN_COUNT = SizeValue.parseSizeValue("100k").singles();
-        final long startUid = 1000000;
-
-        long LIMIT = startUid + INDEX_COUNT;
-        StopWatch watch = new StopWatch().start();
-        System.out.println("Indexing " + INDEX_COUNT + " docs...");
-        for (long i = startUid; i < LIMIT; i++) {
-            Document doc = new Document();
-            doc.add(new StringField("_uid", Long.toString(i), Store.NO));
-            doc.add(new NumericDocValuesField("_version", i));
-            writer.addDocument(doc);
-        }
-        System.out.println("Done indexing, took " + watch.stop().lastTaskTime());
-
-        final IndexReader reader = DirectoryReader.open(writer, true);
-
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    try {
-                        for (long i = 0; i < SCAN_COUNT; i++) {
-                            long id = startUid + (Math.abs(ThreadLocalRandom.current().nextInt()) % INDEX_COUNT);
-                            final long version = Versions.loadVersion(reader, new Term("_uid", Long.toString(id)));
-                            if (version != id) {
-                                System.err.println("wrong id...");
-                                break;
-                            }
-                        }
-                    } catch (Exception e) {
-                        e.printStackTrace();
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-            });
-        }
-
-        watch = new StopWatch().start();
-        for (int i = 0; i < threads.length; i++) {
-            threads[i].start();
-        }
-        latch.await();
-        watch.stop();
-        System.out.println("Scanned in " + watch.totalTime() + " TP Seconds " + ((SCAN_COUNT * NUMBER_OF_THREADS) / watch.totalTime().secondsFrac()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java
deleted file mode 100644
index 9710605..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.common.recycler;
-
-import org.elasticsearch.common.recycler.AbstractRecyclerC;
-import org.elasticsearch.common.recycler.Recycler;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicLong;
-
-import static org.elasticsearch.common.recycler.Recyclers.concurrent;
-import static org.elasticsearch.common.recycler.Recyclers.concurrentDeque;
-import static org.elasticsearch.common.recycler.Recyclers.deque;
-import static org.elasticsearch.common.recycler.Recyclers.dequeFactory;
-import static org.elasticsearch.common.recycler.Recyclers.locked;
-import static org.elasticsearch.common.recycler.Recyclers.none;
-
-/** Benchmark that tries to measure the overhead of object recycling depending on concurrent access. */
-public class RecyclerBenchmark {
-
-    private static final long NUM_RECYCLES = 5000000L;
-    private static final Random RANDOM = new Random(0);
-
-    private static long bench(final Recycler<?> recycler, long numRecycles, int numThreads) throws InterruptedException {
-        final AtomicLong recycles = new AtomicLong(numRecycles);
-        final CountDownLatch latch = new CountDownLatch(1);
-        final Thread[] threads = new Thread[numThreads];
-        for (int i = 0; i < numThreads; ++i){
-            // Thread ids happen to be generated sequentially, so we also generate random threads so that distribution of IDs
-            // is not perfect for the concurrent recycler
-            for (int j = RANDOM.nextInt(5); j >= 0; --j) {
-                new Thread();
-            }
-
-            threads[i] = new Thread() {
-                @Override
-                public void run() {
-                    try {
-                        latch.await();
-                    } catch (InterruptedException e) {
-                        return;
-                    }
-                    while (recycles.getAndDecrement() > 0) {
-                        final Recycler.V<?> v = recycler.obtain();
-                        v.close();
-                    }
-                }
-            };
-        }
-        for (Thread thread : threads) {
-            thread.start();
-        }
-        final long start = System.nanoTime();
-        latch.countDown();
-        for (Thread thread : threads) {
-            thread.join();
-        }
-        return System.nanoTime() - start;
-    }
-
-    public static void main(String[] args) throws InterruptedException {
-        final int limit = 100;
-        final Recycler.C<Object> c = new AbstractRecyclerC<Object>() {
-
-            @Override
-            public Object newInstance(int sizing) {
-                return new Object();
-            }
-
-            @Override
-            public void recycle(Object value) {
-                // do nothing
-            }
-        };
-
-        Map<String, Recycler<Object>> recyclers = new HashMap<>();
-        recyclers.put("none", none(c));
-        recyclers.put("concurrent-queue", concurrentDeque(c, limit));
-        recyclers.put("locked", locked(deque(c, limit)));
-        recyclers.put("concurrent", concurrent(dequeFactory(c, limit), Runtime.getRuntime().availableProcessors()));
-
-        // warmup
-        final long start = System.nanoTime();
-        while (System.nanoTime() - start < TimeUnit.SECONDS.toNanos(10)) {
-            for (Recycler<?> recycler : recyclers.values()) {
-                bench(recycler, NUM_RECYCLES, 2);
-            }
-        }
-
-        // run
-        for (int numThreads = 1; numThreads <= 4 * Runtime.getRuntime().availableProcessors(); numThreads *= 2) {
-            System.out.println("## " + numThreads + " threads\n");
-            System.gc();
-            Thread.sleep(1000);
-            for (Recycler<?> recycler : recyclers.values()) {
-                bench(recycler, NUM_RECYCLES, numThreads);
-            }
-            for (int i = 0; i < 5; ++i) {
-                for (Map.Entry<String, Recycler<Object>> entry : recyclers.entrySet()) {
-                    System.out.println(entry.getKey() + "\t" + TimeUnit.NANOSECONDS.toMillis(bench(entry.getValue(), NUM_RECYCLES, numThreads)));
-                }
-                System.out.println();
-            }
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/counter/SimpleCounterBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/counter/SimpleCounterBenchmark.java
deleted file mode 100644
index ea1e589..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/counter/SimpleCounterBenchmark.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.counter;
-
-import org.elasticsearch.common.StopWatch;
-
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicLong;
-
-/**
- *
- */
-public class SimpleCounterBenchmark {
-
-    private static long NUMBER_OF_ITERATIONS = 10000000;
-    private static int NUMBER_OF_THREADS = 100;
-
-    public static void main(String[] args) throws Exception {
-        final AtomicLong counter = new AtomicLong();
-        StopWatch stopWatch = new StopWatch().start();
-        System.out.println("Running " + NUMBER_OF_ITERATIONS);
-        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-            counter.incrementAndGet();
-        }
-        System.out.println("Took " + stopWatch.stop().totalTime() + " TP Millis " + (NUMBER_OF_ITERATIONS / stopWatch.totalTime().millisFrac()));
-
-        System.out.println("Running using " + NUMBER_OF_THREADS + " threads with " + NUMBER_OF_ITERATIONS + " iterations");
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                        counter.incrementAndGet();
-                    }
-                    latch.countDown();
-                }
-            });
-        }
-        stopWatch = new StopWatch().start();
-        for (Thread thread : threads) {
-            thread.start();
-        }
-        latch.await();
-        stopWatch.stop();
-        System.out.println("Took " + stopWatch.totalTime() + " TP Millis " + ((NUMBER_OF_ITERATIONS * NUMBER_OF_THREADS) / stopWatch.totalTime().millisFrac()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java
deleted file mode 100644
index 06fc39d..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.fs;
-
-import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.unit.ByteSizeValue;
-
-import java.nio.ByteBuffer;
-import java.nio.channels.FileChannel;
-import java.nio.file.Path;
-import java.nio.file.Paths;
-import java.nio.file.StandardOpenOption;
-import java.util.Random;
-
-/**
- *
- */
-public class FsAppendBenchmark {
-
-    public static void main(String[] args) throws Exception {
-        Path path = PathUtils.get("work/test.log");
-        IOUtils.deleteFilesIgnoringExceptions(path);
-
-        int CHUNK = (int) ByteSizeValue.parseBytesSizeValue("1k", "CHUNK").bytes();
-        long DATA = ByteSizeValue.parseBytesSizeValue("10gb", "DATA").bytes();
-
-        byte[] data = new byte[CHUNK];
-        new Random().nextBytes(data);
-
-        StopWatch watch = new StopWatch().start("write");
-        try (FileChannel channel = FileChannel.open(path, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW)) {
-            long position = 0;
-            while (position < DATA) {
-                channel.write(ByteBuffer.wrap(data), position);
-                position += data.length;
-            }
-            watch.stop().start("flush");
-            channel.force(true);
-        }
-        watch.stop();
-        System.out.println("Wrote [" + (new ByteSizeValue(DATA)) + "], chunk [" + (new ByteSizeValue(CHUNK)) + "], in " + watch);
-    }
-
-    private static final ByteBuffer fill = ByteBuffer.allocateDirect(1);
-
-//    public static long padLogFile(long position, long currentSize, long preAllocSize) throws IOException {
-//        if (position + 4096 >= currentSize) {
-//            currentSize = currentSize + preAllocSize;
-//            fill.position(0);
-//            f.getChannel().write(fill, currentSize - fill.remaining());
-//        }
-//        return currentSize;
-//    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/get/SimpleGetActionBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/get/SimpleGetActionBenchmark.java
deleted file mode 100644
index d78df7f..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/get/SimpleGetActionBenchmark.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.get;
-
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-// simple test for embedded / single remote lookup
-public class SimpleGetActionBenchmark {
-
-    public static void main(String[] args) {
-        long OPERATIONS = SizeValue.parseSizeValue("300k").singles();
-
-        Node node = NodeBuilder.nodeBuilder().node();
-
-        Client client;
-        if (false) {
-            client = NodeBuilder.nodeBuilder().client(true).node().client();
-        } else {
-            client = node.client();
-        }
-
-        client.prepareIndex("test", "type1", "1").setSource("field1", "value1").execute().actionGet();
-
-        StopWatch stopWatch = new StopWatch().start();
-        for (long i = 0; i < OPERATIONS; i++) {
-            client.prepareGet("test", "type1", "1").execute().actionGet();
-        }
-        stopWatch.stop();
-
-        System.out.println("Ran in " + stopWatch.totalTime() + ", per second: " + (((double) OPERATIONS) / stopWatch.totalTime().secondsFrac()));
-
-        node.close();
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/hppc/StringMapAdjustOrPutBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/hppc/StringMapAdjustOrPutBenchmark.java
deleted file mode 100644
index e51ba31..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/hppc/StringMapAdjustOrPutBenchmark.java
+++ /dev/null
@@ -1,262 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.hppc;
-
-import com.carrotsearch.hppc.IntIntHashMap;
-import com.carrotsearch.hppc.IntObjectHashMap;
-import com.carrotsearch.hppc.ObjectIntHashMap;
-import com.carrotsearch.hppc.ObjectObjectHashMap;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.unit.SizeValue;
-
-import java.util.HashMap;
-import java.util.IdentityHashMap;
-import java.util.concurrent.ThreadLocalRandom;
-
-// TODO: these benchmarks aren't too good and may be easily skewed by jit doing 
-// escape analysis/ side-effects/ local
-// optimisations. Proper benchmarks with JMH (bulk ops, single-shot mode) 
-// should be better here.
-// https://github.com/carrotsearch/hppc/blob/master/hppc-benchmarks/src/main/java/com/carrotsearch/hppc/benchmarks/B003_HashSet_Contains.java
-
-public class StringMapAdjustOrPutBenchmark {
-
-    public static void main(String[] args) {
-
-        int NUMBER_OF_KEYS = (int) SizeValue.parseSizeValue("20").singles();
-        int STRING_SIZE = 5;
-        long PUT_OPERATIONS = SizeValue.parseSizeValue("5m").singles();
-        long ITERATIONS = 10;
-        boolean REUSE = true;
-
-
-        String[] values = new String[NUMBER_OF_KEYS];
-        for (int i = 0; i < values.length; i++) {
-            values[i] = RandomStrings.randomAsciiOfLength(ThreadLocalRandom.current(), STRING_SIZE);
-        }
-
-        StopWatch stopWatch;
-
-        stopWatch = new StopWatch().start();
-        ObjectIntHashMap<String> map = new ObjectIntHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                map.clear();
-            } else {
-                map = new ObjectIntHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                map.addTo(values[(int) (i % NUMBER_OF_KEYS)], 1);
-            }
-        }
-        map.clear();
-        map = null;
-
-        stopWatch.stop();
-        System.out.println("TObjectIntHashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-        stopWatch = new StopWatch().start();
-//        TObjectIntCustomHashMap<String> iMap = new TObjectIntCustomHashMap<String>(new StringIdentityHashingStrategy());
-        ObjectIntHashMap<String> iMap = new ObjectIntHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                iMap.clear();
-            } else {
-                iMap = new ObjectIntHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                iMap.addTo(values[(int) (i % NUMBER_OF_KEYS)], 1);
-            }
-        }
-        stopWatch.stop();
-        System.out.println("TObjectIntCustomHashMap(StringIdentity): " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-        iMap.clear();
-        iMap = null;
-
-        stopWatch = new StopWatch().start();
-        iMap = new ObjectIntHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                iMap.clear();
-            } else {
-                iMap = new ObjectIntHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                iMap.addTo(values[(int) (i % NUMBER_OF_KEYS)], 1);
-            }
-        }
-        stopWatch.stop();
-        System.out.println("TObjectIntCustomHashMap(PureIdentity): " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-        iMap.clear();
-        iMap = null;
-
-        // now test with THashMap
-        stopWatch = new StopWatch().start();
-        ObjectObjectHashMap<String, StringEntry> tMap = new ObjectObjectHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                tMap.clear();
-            } else {
-                tMap = new ObjectObjectHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                String key = values[(int) (i % NUMBER_OF_KEYS)];
-                StringEntry stringEntry = tMap.get(key);
-                if (stringEntry == null) {
-                    stringEntry = new StringEntry(key, 1);
-                    tMap.put(key, stringEntry);
-                } else {
-                    stringEntry.counter++;
-                }
-            }
-        }
-
-        tMap.clear();
-        tMap = null;
-
-        stopWatch.stop();
-        System.out.println("THashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-        stopWatch = new StopWatch().start();
-        HashMap<String, StringEntry> hMap = new HashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                hMap.clear();
-            } else {
-                hMap = new HashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                String key = values[(int) (i % NUMBER_OF_KEYS)];
-                StringEntry stringEntry = hMap.get(key);
-                if (stringEntry == null) {
-                    stringEntry = new StringEntry(key, 1);
-                    hMap.put(key, stringEntry);
-                } else {
-                    stringEntry.counter++;
-                }
-            }
-        }
-
-        hMap.clear();
-        hMap = null;
-
-        stopWatch.stop();
-        System.out.println("HashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-
-        stopWatch = new StopWatch().start();
-        IdentityHashMap<String, StringEntry> ihMap = new IdentityHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                ihMap.clear();
-            } else {
-                hMap = new HashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                String key = values[(int) (i % NUMBER_OF_KEYS)];
-                StringEntry stringEntry = ihMap.get(key);
-                if (stringEntry == null) {
-                    stringEntry = new StringEntry(key, 1);
-                    ihMap.put(key, stringEntry);
-                } else {
-                    stringEntry.counter++;
-                }
-            }
-        }
-        stopWatch.stop();
-        System.out.println("IdentityHashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-        ihMap.clear();
-        ihMap = null;
-
-        int[] iValues = new int[NUMBER_OF_KEYS];
-        for (int i = 0; i < values.length; i++) {
-            iValues[i] = ThreadLocalRandom.current().nextInt();
-        }
-
-        stopWatch = new StopWatch().start();
-        IntIntHashMap intMap = new IntIntHashMap();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                intMap.clear();
-            } else {
-                intMap = new IntIntHashMap();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                int key = iValues[(int) (i % NUMBER_OF_KEYS)];
-                intMap.addTo(key, 1);
-            }
-        }
-        stopWatch.stop();
-        System.out.println("TIntIntHashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-
-        intMap.clear();
-        intMap = null;
-
-        // now test with THashMap
-        stopWatch = new StopWatch().start();
-        IntObjectHashMap<IntEntry> tIntMap = new IntObjectHashMap<>();
-        for (long iter = 0; iter < ITERATIONS; iter++) {
-            if (REUSE) {
-                tIntMap.clear();
-            } else {
-                tIntMap = new IntObjectHashMap<>();
-            }
-            for (long i = 0; i < PUT_OPERATIONS; i++) {
-                int key = iValues[(int) (i % NUMBER_OF_KEYS)];
-                IntEntry intEntry = tIntMap.get(key);
-                if (intEntry == null) {
-                    intEntry = new IntEntry(key, 1);
-                    tIntMap.put(key, intEntry);
-                } else {
-                    intEntry.counter++;
-                }
-            }
-        }
-
-        tIntMap.clear();
-        tIntMap = null;
-
-        stopWatch.stop();
-        System.out.println("TIntObjectHashMap: " + stopWatch.totalTime() + ", " + stopWatch.totalTime().millisFrac() / ITERATIONS + "ms");
-    }
-
-
-    static class StringEntry {
-        String key;
-        int counter;
-
-        StringEntry(String key, int counter) {
-            this.key = key;
-            this.counter = counter;
-        }
-    }
-
-    static class IntEntry {
-        int key;
-        int counter;
-
-        IntEntry(int key, int counter) {
-            this.key = key;
-            this.counter = counter;
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java
deleted file mode 100644
index ff7d084..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/mapping/ManyMappingsBenchmark.java
+++ /dev/null
@@ -1,156 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.mapping;
-
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- */
-@SuppressForbidden(reason = "not really source code or a test")
-public class ManyMappingsBenchmark {
-
-    private static final String MAPPING = "{\n" +
-            "        \"dynamic_templates\": [\n" +
-            "          {\n" +
-            "            \"t1\": {\n" +
-            "              \"mapping\": {\n" +
-            "                \"store\": false,\n" +
-            "                \"norms\": {\n" +
-            "                  \"enabled\": false\n" +
-            "                },\n" +
-            "                \"type\": \"string\"\n" +
-            "              },\n" +
-            "              \"match\": \"*_ss\"\n" +
-            "            }\n" +
-            "          },\n" +
-            "          {\n" +
-            "            \"t2\": {\n" +
-            "              \"mapping\": {\n" +
-            "                \"store\": false,\n" +
-            "                \"type\": \"date\"\n" +
-            "              },\n" +
-            "              \"match\": \"*_dt\"\n" +
-            "            }\n" +
-            "          },\n" +
-            "          {\n" +
-            "            \"t3\": {\n" +
-            "              \"mapping\": {\n" +
-            "                \"store\": false,\n" +
-            "                \"type\": \"integer\"\n" +
-            "              },\n" +
-            "              \"match\": \"*_i\"\n" +
-            "            }\n" +
-            "          }\n" +
-            "        ],\n" +
-            "        \"_source\": {\n" +
-            "          \"enabled\": false\n" +
-            "        },\n" +
-            "        \"properties\": {}\n" +
-            "      }";
-
-    private static final String INDEX_NAME = "index";
-    private static final String TYPE_NAME = "type";
-    private static final int FIELD_COUNT = 100000;
-    private static final int DOC_COUNT = 10000000;
-    private static final boolean TWO_NODES = true;
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-        BootstrapForTesting.ensureInitialized();
-        Settings settings = settingsBuilder()
-                .put("")
-                .put(SETTING_NUMBER_OF_SHARDS, 5)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = ManyMappingsBenchmark.class.getSimpleName();
-        Node node = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings))
-                .node();
-        if (TWO_NODES) {
-            Node node2 = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings))
-                    .node();
-        }
-
-        Client client = node.client();
-
-        client.admin().indices().prepareDelete(INDEX_NAME)
-                .setIndicesOptions(IndicesOptions.lenientExpandOpen())
-                .get();
-        client.admin().indices().prepareCreate(INDEX_NAME)
-                .addMapping(TYPE_NAME, MAPPING)
-                .get();
-
-        BulkRequestBuilder builder = client.prepareBulk();
-        int fieldCount = 0;
-        long time = System.currentTimeMillis();
-        final int PRINT = 1000;
-        for (int i = 0; i < DOC_COUNT; i++) {
-            XContentBuilder sourceBuilder = jsonBuilder().startObject();
-            sourceBuilder.field(++fieldCount + "_ss", "xyz");
-            sourceBuilder.field(++fieldCount + "_dt", System.currentTimeMillis());
-            sourceBuilder.field(++fieldCount + "_i", i % 100);
-            sourceBuilder.endObject();
-
-            if (fieldCount >= FIELD_COUNT) {
-                fieldCount = 0;
-                System.out.println("dynamic fields rolled up");
-            }
-
-            builder.add(
-                    client.prepareIndex(INDEX_NAME, TYPE_NAME, String.valueOf(i))
-                            .setSource(sourceBuilder)
-            );
-
-            if (builder.numberOfActions() >= 1000) {
-                builder.get();
-                builder = client.prepareBulk();
-            }
-
-            if (i % PRINT == 0) {
-                long took = System.currentTimeMillis() - time;
-                time = System.currentTimeMillis();
-                System.out.println("Indexed " + i +  " docs, in " + TimeValue.timeValueMillis(took));
-            }
-        }
-        if (builder.numberOfActions() > 0) {
-            builder.get();
-        }
-
-
-
-    }
-
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/monitor/os/OsProbeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/monitor/os/OsProbeBenchmark.java
deleted file mode 100644
index 6788490..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/monitor/os/OsProbeBenchmark.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.monitor.os;
-
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-import org.elasticsearch.monitor.os.OsProbe;
-
-@SuppressForbidden(reason = "not really source code or a test")
-public class OsProbeBenchmark {
-
-    private static final int ITERATIONS = 100_000;
-
-    public static void main(String[] args) {
-        System.setProperty("es.logger.prefix", "");
-        final ESLogger logger = ESLoggerFactory.getLogger("benchmark");
-
-        logger.info("--> loading OS probe");
-        OsProbe probe = OsProbe.getInstance();
-
-        logger.info("--> warming up...");
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getTotalPhysicalMemorySize();
-            probe.getFreePhysicalMemorySize();
-            probe.getTotalSwapSpaceSize();
-            probe.getFreeSwapSpaceSize();
-            probe.getSystemLoadAverage();
-            probe.getSystemCpuPercent();
-        }
-        logger.info("--> warmed up");
-
-        logger.info("--> testing 'getTotalPhysicalMemorySize' method...");
-        long start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getTotalPhysicalMemorySize();
-        }
-        long elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getFreePhysicalMemorySize' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getFreePhysicalMemorySize();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getTotalSwapSpaceSize' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getTotalSwapSpaceSize();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getFreeSwapSpaceSize' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getFreeSwapSpaceSize();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getSystemLoadAverage' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getSystemLoadAverage();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getSystemCpuPercent' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getSystemCpuPercent();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/monitor/process/ProcessProbeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/monitor/process/ProcessProbeBenchmark.java
deleted file mode 100644
index b91b516..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/monitor/process/ProcessProbeBenchmark.java
+++ /dev/null
@@ -1,131 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.monitor.process;
-
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-import org.elasticsearch.monitor.process.ProcessProbe;
-
-import java.lang.management.ManagementFactory;
-import java.lang.management.ThreadMXBean;
-
-@SuppressForbidden(reason = "use of om.sun.management.ThreadMXBean to compare performance")
-public class ProcessProbeBenchmark {
-
-    private static final int ITERATIONS = 100_000;
-
-    public static void main(String[] args) {
-        System.setProperty("es.logger.prefix", "");
-        final ESLogger logger = ESLoggerFactory.getLogger("benchmark");
-
-        logger.info("--> loading process probe");
-        ProcessProbe probe = ProcessProbe.getInstance();
-
-        logger.info("--> warming up...");
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getOpenFileDescriptorCount();
-            probe.getMaxFileDescriptorCount();
-            probe.getTotalVirtualMemorySize();
-            probe.getProcessCpuPercent();
-            probe.getProcessCpuTotalTime();
-        }
-        logger.info("--> warmed up");
-
-
-
-
-        logger.info("--> testing 'getOpenFileDescriptorCount' method...");
-        long start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getOpenFileDescriptorCount();
-        }
-        long elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getMaxFileDescriptorCount' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getMaxFileDescriptorCount();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getTotalVirtualMemorySize' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getTotalVirtualMemorySize();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getProcessCpuPercent' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getProcessCpuPercent();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-        logger.info("--> testing 'getProcessCpuTotalTime' method...");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            probe.getProcessCpuTotalTime();
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> total [{}] ms, avg [{}] ms", elapsed, (elapsed / (double)ITERATIONS));
-
-
-
-
-        logger.info("--> calculating process CPU user time with 'getAllThreadIds + getThreadUserTime' methods...");
-        final ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();
-        final long[] threadIds = threadMxBean.getAllThreadIds();
-        long sum = 0;
-
-        start = System.currentTimeMillis();
-        for (int i = 0; i < ITERATIONS; i++) {
-            for (long threadId : threadIds) {
-                sum += threadMxBean.getThreadUserTime(threadId);
-            }
-        }
-        elapsed = System.currentTimeMillis() - start;
-        logger.info("--> execution time [total: {} ms, avg: {} ms] for {} iterations with average result of {}",
-                elapsed, (elapsed / (double)ITERATIONS), ITERATIONS, (sum / (double)ITERATIONS));
-
-        if (threadMxBean instanceof com.sun.management.ThreadMXBean) {
-            logger.info("--> calculating process CPU user time with 'getAllThreadIds + getThreadUserTime(long[])' methods...");
-            final com.sun.management.ThreadMXBean threadMxBean2 = (com.sun.management.ThreadMXBean)threadMxBean;
-            sum = 0;
-
-            start = System.currentTimeMillis();
-            for (int i = 0; i < ITERATIONS; i++) {
-                long[] user = threadMxBean2.getThreadUserTime(threadIds);
-                for (int n = 0 ; n != threadIds.length; ++n) {
-                    sum += user[n];
-                }
-            }
-            elapsed = System.currentTimeMillis() - start;
-            logger.info("--> execution time [total: {} ms, avg: {} ms] for {} iterations with average result of {}",
-                    elapsed, (elapsed / (double)ITERATIONS), ITERATIONS, (sum / (double)ITERATIONS));
-
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java
deleted file mode 100644
index 81492eb..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/percolator/PercolatorStressBenchmark.java
+++ /dev/null
@@ -1,156 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.percolator;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.health.ClusterHealthStatus;
-import org.elasticsearch.action.percolate.PercolateResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.percolator.PercolatorService;
-
-import java.io.IOException;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class PercolatorStressBenchmark {
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 4)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node" + i)).node();
-        }
-
-        Node clientNode = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-        Client client = clientNode.client();
-
-        client.admin().indices().create(createIndexRequest("test")).actionGet();
-        ClusterHealthResponse healthResponse = client.admin().cluster().prepareHealth("test")
-                .setWaitForGreenStatus()
-                .execute().actionGet();
-        if (healthResponse.isTimedOut()) {
-            System.err.println("Quiting, because cluster health requested timed out...");
-            return;
-        } else if (healthResponse.getStatus() != ClusterHealthStatus.GREEN) {
-            System.err.println("Quiting, because cluster state isn't green...");
-            return;
-        }
-
-        int COUNT = 200000;
-        int QUERIES = 100;
-        int TERM_QUERIES = QUERIES / 2;
-        int RANGE_QUERIES = QUERIES - TERM_QUERIES;
-
-        client.prepareIndex("test", "type1", "1").setSource(jsonBuilder().startObject().field("numeric1", 1).endObject()).execute().actionGet();
-
-        // register queries
-        int i = 0;
-        for (; i < TERM_QUERIES; i++) {
-            client.prepareIndex("test", PercolatorService.TYPE_NAME, Integer.toString(i))
-                    .setSource(jsonBuilder().startObject()
-                            .field("query", termQuery("name", "value"))
-                            .endObject())
-                    .execute().actionGet();
-        }
-
-        int[] numbers = new int[RANGE_QUERIES];
-        for (; i < QUERIES; i++) {
-            client.prepareIndex("test", PercolatorService.TYPE_NAME, Integer.toString(i))
-                    .setSource(jsonBuilder().startObject()
-                            .field("query", rangeQuery("numeric1").from(i).to(i))
-                            .endObject())
-                    .execute().actionGet();
-            numbers[i - TERM_QUERIES] = i;
-        }
-
-        StopWatch stopWatch = new StopWatch().start();
-        System.out.println("Percolating [" + COUNT + "] ...");
-        for (i = 1; i <= COUNT; i++) {
-            XContentBuilder source;
-            int expectedMatches;
-            if (i % 2 == 0) {
-                source = source(Integer.toString(i), "value");
-                expectedMatches = TERM_QUERIES;
-            } else {
-                int number = numbers[i % RANGE_QUERIES];
-                source = source(Integer.toString(i), number);
-                expectedMatches = 1;
-            }
-            PercolateResponse percolate = client.preparePercolate()
-                    .setIndices("test").setDocumentType("type1")
-                    .setSource(source)
-                    .execute().actionGet();
-            if (percolate.getMatches().length != expectedMatches) {
-                System.err.println("No matching number of queries");
-            }
-
-            if ((i % 10000) == 0) {
-                System.out.println("Percolated " + i + " took " + stopWatch.stop().lastTaskTime());
-                stopWatch.start();
-            }
-        }
-        System.out.println("Percolation took " + stopWatch.totalTime() + ", TPS " + (((double) COUNT) / stopWatch.totalTime().secondsFrac()));
-
-        clientNode.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    private static XContentBuilder source(String id, String nameValue) throws IOException {
-        return jsonBuilder().startObject().startObject("doc")
-                .field("id", id)
-                .field("name", nameValue)
-                .endObject().endObject();
-    }
-
-    private static XContentBuilder source(String id, int number) throws IOException {
-        return jsonBuilder().startObject().startObject("doc")
-                .field("id", id)
-                .field("numeric1", number)
-                .field("numeric2", number)
-                .field("numeric3", number)
-                .field("numeric4", number)
-                .field("numeric5", number)
-                .field("numeric6", number)
-                .field("numeric7", number)
-                .field("numeric8", number)
-                .field("numeric9", number)
-                .field("numeric10", number)
-                .endObject().endObject();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java
deleted file mode 100644
index e1265d5..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java
+++ /dev/null
@@ -1,200 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.recovery;
-
-import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.ESLoggerFactory;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.indices.recovery.RecoveryState;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.test.BackgroundIndexer;
-import org.elasticsearch.transport.TransportModule;
-
-import java.util.List;
-import java.util.Random;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-@SuppressForbidden(reason = "not really source code or a test")
-public class ReplicaRecoveryBenchmark {
-
-    private static final String INDEX_NAME = "index";
-    private static final String TYPE_NAME = "type";
-
-
-    static int DOC_COUNT = (int) SizeValue.parseSizeValue("40k").singles();
-    static int CONCURRENT_INDEXERS = 2;
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-        BootstrapForTesting.ensureInitialized();
-
-        Settings settings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), "false")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .put(TransportModule.TRANSPORT_TYPE_KEY, "local")
-                .build();
-
-        String clusterName = ReplicaRecoveryBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings))
-                .node();
-
-        final ESLogger logger = ESLoggerFactory.getLogger("benchmark");
-
-        final Client client1 = node1.client();
-        client1.admin().cluster().prepareUpdateSettings().setPersistentSettings("logger.indices.recovery: TRACE").get();
-        final BackgroundIndexer indexer = new BackgroundIndexer(INDEX_NAME, TYPE_NAME, client1, 0, CONCURRENT_INDEXERS, false, new Random());
-        indexer.setMinFieldSize(10);
-        indexer.setMaxFieldSize(150);
-        try {
-            client1.admin().indices().prepareDelete(INDEX_NAME).get();
-        } catch (IndexNotFoundException e) {
-        }
-        client1.admin().indices().prepareCreate(INDEX_NAME).get();
-        indexer.start(DOC_COUNT / 2);
-        while (indexer.totalIndexedDocs() < DOC_COUNT / 2) {
-            Thread.sleep(5000);
-            logger.info("--> indexed {} of {}", indexer.totalIndexedDocs(), DOC_COUNT);
-        }
-        client1.admin().indices().prepareFlush().get();
-        indexer.continueIndexing(DOC_COUNT / 2);
-        while (indexer.totalIndexedDocs() < DOC_COUNT) {
-            Thread.sleep(5000);
-            logger.info("--> indexed {} of {}", indexer.totalIndexedDocs(), DOC_COUNT);
-        }
-
-
-        logger.info("--> starting another node and allocating a shard on it");
-
-        Node node2 = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings))
-                .node();
-
-        client1.admin().indices().prepareUpdateSettings(INDEX_NAME).setSettings(IndexMetaData.SETTING_NUMBER_OF_REPLICAS + ": 1").get();
-
-        final AtomicBoolean end = new AtomicBoolean(false);
-
-        final Thread backgroundLogger = new Thread(new Runnable() {
-
-            long lastTime = System.currentTimeMillis();
-            long lastDocs = indexer.totalIndexedDocs();
-            long lastBytes = 0;
-            long lastTranslogOps = 0;
-
-            @Override
-            public void run() {
-                while (true) {
-                    try {
-                        Thread.sleep(5000);
-                    } catch (InterruptedException e) {
-
-                    }
-                    if (end.get()) {
-                        return;
-                    }
-                    long currentTime = System.currentTimeMillis();
-                    long currentDocs = indexer.totalIndexedDocs();
-                    RecoveryResponse recoveryResponse = client1.admin().indices().prepareRecoveries(INDEX_NAME).setActiveOnly(true).get();
-                    List<RecoveryState> indexRecoveries = recoveryResponse.shardRecoveryStates().get(INDEX_NAME);
-                    long translogOps;
-                    long bytes;
-                    if (indexRecoveries.size() > 0) {
-                        translogOps = indexRecoveries.get(0).getTranslog().recoveredOperations();
-                        bytes = recoveryResponse.shardRecoveryStates().get(INDEX_NAME).get(0).getIndex().recoveredBytes();
-                    } else {
-                        bytes = lastBytes = 0;
-                        translogOps = lastTranslogOps = 0;
-                    }
-                    float seconds = (currentTime - lastTime) / 1000.0F;
-                    logger.info("--> indexed [{}];[{}] doc/s, recovered [{}] MB/s , translog ops [{}]/s ",
-                            currentDocs, (currentDocs - lastDocs) / seconds,
-                            (bytes - lastBytes) / 1024.0F / 1024F / seconds, (translogOps - lastTranslogOps) / seconds);
-                    lastBytes = bytes;
-                    lastTranslogOps = translogOps;
-                    lastTime = currentTime;
-                    lastDocs = currentDocs;
-                }
-            }
-        });
-
-        backgroundLogger.start();
-
-        client1.admin().cluster().prepareHealth().setWaitForGreenStatus().get();
-
-        logger.info("--> green. starting relocation cycles");
-
-        long startDocIndexed = indexer.totalIndexedDocs();
-        indexer.continueIndexing(DOC_COUNT * 50);
-
-        long totalRecoveryTime = 0;
-        long startTime = System.currentTimeMillis();
-        long[] recoveryTimes = new long[3];
-        for (int iteration = 0; iteration < 3; iteration++) {
-            logger.info("--> removing replicas");
-            client1.admin().indices().prepareUpdateSettings(INDEX_NAME).setSettings(IndexMetaData.SETTING_NUMBER_OF_REPLICAS + ": 0").get();
-            logger.info("--> adding replica again");
-            long recoveryStart = System.currentTimeMillis();
-            client1.admin().indices().prepareUpdateSettings(INDEX_NAME).setSettings(IndexMetaData.SETTING_NUMBER_OF_REPLICAS + ": 1").get();
-            client1.admin().cluster().prepareHealth(INDEX_NAME).setWaitForGreenStatus().setTimeout("15m").get();
-            long recoveryTime = System.currentTimeMillis() - recoveryStart;
-            totalRecoveryTime += recoveryTime;
-            recoveryTimes[iteration] = recoveryTime;
-            logger.info("--> recovery done in [{}]", new TimeValue(recoveryTime));
-
-            // sleep some to let things clean up
-            Thread.sleep(10000);
-        }
-
-        long endDocIndexed = indexer.totalIndexedDocs();
-        long totalTime = System.currentTimeMillis() - startTime;
-        indexer.stop();
-
-        end.set(true);
-
-        backgroundLogger.interrupt();
-
-        backgroundLogger.join();
-
-        logger.info("average doc/s [{}], average relocation time [{}], taking [{}], [{}], [{}]", (endDocIndexed - startDocIndexed) * 1000.0 / totalTime, new TimeValue(totalRecoveryTime / 3),
-                TimeValue.timeValueMillis(recoveryTimes[0]), TimeValue.timeValueMillis(recoveryTimes[1]), TimeValue.timeValueMillis(recoveryTimes[2])
-        );
-
-        client1.close();
-        node1.close();
-        node2.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java
deleted file mode 100644
index 6f666b3..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript1.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeScript1 extends AbstractSearchScript {
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeScript1();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    public static final String NATIVE_SCRIPT_1 = "native_1";
-
-    @Override
-    public Object run() {
-        return docFieldLongs("x").getValue();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java
deleted file mode 100644
index 585d7a5..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript2.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeScript2 extends AbstractSearchScript {
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeScript2();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    public static final String NATIVE_SCRIPT_2 = "native_2";
-
-    @Override
-    public Object run() {
-        return docFieldLongs("x").getValue() + docFieldDoubles("y").getValue();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java
deleted file mode 100644
index c2d50fe..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript3.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeScript3 extends AbstractSearchScript {
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeScript3();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    public static final String NATIVE_SCRIPT_3 = "native_3";
-
-    @Override
-    public Object run() {
-        return 1.2 * docFieldLongs("x").getValue() / docFieldDoubles("y").getValue();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java
deleted file mode 100644
index 2bda86e..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScript4.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeScript4 extends AbstractSearchScript {
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeScript4();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    public static final String NATIVE_SCRIPT_4 = "native_4";
-
-    @Override
-    public Object run() {
-        return Math.sqrt(Math.abs(docFieldDoubles("z").getValue())) + Math.log(Math.abs(docFieldLongs("x").getValue() * docFieldDoubles("y").getValue()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScriptPlugin.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScriptPlugin.java
deleted file mode 100644
index 92f19a7..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/NativeScriptPlugin.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.ScriptModule;
-
-public class NativeScriptPlugin extends Plugin {
-
-    @Override
-    public String name() {
-        return "native-benchmark-scripts";
-    }
-
-    @Override
-    public String description() {
-        return "Native benchmark script";
-    }
-
-    public void onModule(ScriptModule module) {
-        module.registerScript(NativeScript1.NATIVE_SCRIPT_1, NativeScript1.Factory.class);
-        module.registerScript(NativeScript2.NATIVE_SCRIPT_2, NativeScript2.Factory.class);
-        module.registerScript(NativeScript3.NATIVE_SCRIPT_3, NativeScript3.Factory.class);
-        module.registerScript(NativeScript4.NATIVE_SCRIPT_4, NativeScript4.Factory.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/ScriptComparisonBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/ScriptComparisonBenchmark.java
deleted file mode 100644
index ce4cbf1..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/expression/ScriptComparisonBenchmark.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.expression;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.IndicesAdminClient;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.sort.ScriptSortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.joda.time.PeriodType;
-
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Random;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-public class ScriptComparisonBenchmark {
-
-    static final String clusterName = ScriptComparisonBenchmark.class.getSimpleName();
-    static final String indexName = "test";
-
-    static String[] langs = {
-        "expression",
-        "native",
-        "groovy"
-    };
-    static String[][] scripts = {
-        // the first value is the "reference" version (pure math)
-        {
-            "x",
-            "doc['x'].value",
-            NativeScript1.NATIVE_SCRIPT_1,
-            "doc['x'].value"
-        }, {
-            "x + y",
-            "doc['x'].value + doc['y'].value",
-            NativeScript2.NATIVE_SCRIPT_2,
-            "doc['x'].value + doc['y'].value",
-        }, {
-            "1.2 * x / y",
-            "1.2 * doc['x'].value / doc['y'].value",
-            NativeScript3.NATIVE_SCRIPT_3,
-            "1.2 * doc['x'].value / doc['y'].value",
-        }, {
-            "sqrt(abs(z)) + ln(abs(x * y))",
-            "sqrt(abs(doc['z'].value)) + ln(abs(doc['x'].value * doc['y'].value))",
-            NativeScript4.NATIVE_SCRIPT_4,
-            "sqrt(abs(doc['z'].value)) + log(abs(doc['x'].value * doc['y'].value))"
-        }
-    };
-
-    public static void main(String[] args) throws Exception {
-        int numDocs = 1000000;
-        int numQueries = 1000;
-        Client client = setupIndex();
-        indexDocs(client, numDocs);
-
-        for (int scriptNum = 0; scriptNum < scripts.length; ++scriptNum) {
-            runBenchmark(client, scriptNum, numQueries);
-        }
-    }
-
-    static void runBenchmark(Client client, int scriptNum, int numQueries) {
-        System.out.println("");
-        System.out.println("Script: " + scripts[scriptNum][0]);
-        System.out.println("--------------------------------");
-        for (int langNum = 0; langNum < langs.length; ++langNum) {
-            String lang = langs[langNum];
-            String script = scripts[scriptNum][langNum + 1];
-
-            timeQueries(client, lang, script, numQueries / 10); // warmup
-            TimeValue time = timeQueries(client, lang, script, numQueries);
-            printResults(lang, time, numQueries);
-        }
-    }
-
-    static Client setupIndex() throws Exception {
-        // create cluster
-        Settings settings = settingsBuilder().put("name", "node1")
-                                             .put("cluster.name", clusterName).build();
-        Collection<Class<? extends Plugin>> plugins = Collections.<Class<? extends Plugin>>singletonList(NativeScriptPlugin.class);
-        Node node1 = new MockNode(settings, Version.CURRENT, plugins);
-        node1.start();
-        Client client = node1.client();
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        // delete the index, if it exists
-        try {
-            client.admin().indices().prepareDelete(indexName).execute().actionGet();
-        } catch (ElasticsearchException e) {
-            // ok if the index didn't exist
-        }
-
-        // create mappings
-        IndicesAdminClient admin = client.admin().indices();
-        admin.prepareCreate(indexName).addMapping("doc", "x", "type=long", "y", "type=double");
-
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        return client;
-    }
-
-    static void indexDocs(Client client, int numDocs) {
-        System.out.print("Indexing " + numDocs + " random docs...");
-        BulkRequestBuilder bulkRequest = client.prepareBulk();
-        Random r = new Random(1);
-        for (int i = 0; i < numDocs; i++) {
-            bulkRequest.add(client.prepareIndex("test", "doc", Integer.toString(i))
-                            .setSource("x", r.nextInt(), "y", r.nextDouble(), "z", r.nextDouble()));
-
-            if (i % 1000 == 0) {
-                bulkRequest.execute().actionGet();
-                bulkRequest = client.prepareBulk();
-            }
-        }
-        bulkRequest.execute().actionGet();
-        client.admin().indices().prepareRefresh("test").execute().actionGet();
-        client.admin().indices().prepareFlush("test").execute().actionGet();
-        System.out.println("done");
-    }
-
-    static TimeValue timeQueries(Client client, String lang, String script, int numQueries) {
-        ScriptSortBuilder sort = SortBuilders.scriptSort(new Script(script, ScriptType.INLINE, lang, null), "number");
-        SearchRequestBuilder req = client.prepareSearch(indexName)
-                .setQuery(QueryBuilders.matchAllQuery())
-                .addSort(sort);
-
-        StopWatch timer = new StopWatch();
-        timer.start();
-        for (int i = 0; i < numQueries; ++i) {
-            req.get();
-        }
-        timer.stop();
-        return timer.totalTime();
-    }
-
-    static void printResults(String lang, TimeValue time, int numQueries) {
-        long avgReq = time.millis() / numQueries;
-        System.out.println(lang + ": " + time.format(PeriodType.seconds()) + " (" + avgReq + " msec per req)");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java
deleted file mode 100644
index 81d4a78..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java
+++ /dev/null
@@ -1,335 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score;
-
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchType;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.lucene.search.function.CombineFunction;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionBuilder;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-
-import java.io.BufferedWriter;
-import java.io.IOException;
-import java.math.BigInteger;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.Files;
-import java.security.SecureRandom;
-import java.util.AbstractMap;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Random;
-
-import static org.elasticsearch.client.Requests.searchRequest;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-
-public class BasicScriptBenchmark {
-
-    public static class RequestInfo {
-        public RequestInfo(SearchRequest source, int i) {
-            request = source;
-            numTerms = i;
-        }
-
-        SearchRequest request;
-        int numTerms;
-    }
-
-    public static class Results {
-        public static final String TIME_PER_DOCIN_MILLIS = "timePerDocinMillis";
-        public static final String NUM_TERMS = "numTerms";
-        public static final String NUM_DOCS = "numDocs";
-        public static final String TIME_PER_QUERY_IN_SEC = "timePerQueryInSec";
-        public static final String TOTAL_TIME_IN_SEC = "totalTimeInSec";
-        Double[] resultSeconds;
-        Double[] resultMSPerQuery;
-        Long[] numDocs;
-        Integer[] numTerms;
-        Double[] timePerDoc;
-        String label;
-        String description;
-        public String lineStyle;
-        public String color;
-
-        void init(int numVariations, String label, String description, String color, String lineStyle) {
-            resultSeconds = new Double[numVariations];
-            resultMSPerQuery = new Double[numVariations];
-            numDocs = new Long[numVariations];
-            numTerms = new Integer[numVariations];
-            timePerDoc = new Double[numVariations];
-            this.label = label;
-            this.description = description;
-            this.color = color;
-            this.lineStyle = lineStyle;
-        }
-
-        void set(SearchResponse searchResponse, StopWatch stopWatch, String message, int maxIter, int which, int numTerms) {
-            resultSeconds[which] = (double) ((double) stopWatch.lastTaskTime().getMillis() / (double) 1000);
-            resultMSPerQuery[which] = (double) ((double) stopWatch.lastTaskTime().secondsFrac() / (double) maxIter);
-            numDocs[which] = searchResponse.getHits().totalHits();
-            this.numTerms[which] = numTerms;
-            timePerDoc[which] = resultMSPerQuery[which] / numDocs[which];
-        }
-
-        public void printResults(BufferedWriter writer) throws IOException {
-            String comma = (writer == null) ? "" : ";";
-            String results = description + "\n" + Results.TOTAL_TIME_IN_SEC + " = " + getResultArray(resultSeconds) + comma + "\n"
-                    + Results.TIME_PER_QUERY_IN_SEC + " = " + getResultArray(resultMSPerQuery) + comma + "\n" + Results.NUM_DOCS + " = "
-                    + getResultArray(numDocs) + comma + "\n" + Results.NUM_TERMS + " = " + getResultArray(numTerms) + comma + "\n"
-                    + Results.TIME_PER_DOCIN_MILLIS + " = " + getResultArray(timePerDoc) + comma + "\n";
-            if (writer != null) {
-                writer.write(results);
-            } else {
-                System.out.println(results);
-            }
-
-        }
-
-        private String getResultArray(Object[] resultArray) {
-            String result = "[";
-            for (int i = 0; i < resultArray.length; i++) {
-                result += resultArray[i].toString();
-                if (i != resultArray.length - 1) {
-                    result += ",";
-                }
-            }
-            result += "]";
-            return result;
-        }
-    }
-
-    public BasicScriptBenchmark() {
-    }
-
-    static List<String> termsList = new ArrayList<>();
-
-    static void init(int numTerms) {
-        SecureRandom random = new SecureRandom();
-        random.setSeed(1);
-        termsList.clear();
-        for (int i = 0; i < numTerms; i++) {
-            String term = new BigInteger(512, random).toString(32);
-            termsList.add(term);
-        }
-
-    }
-
-    static String[] getTerms(int numTerms) {
-        String[] terms = new String[numTerms];
-        for (int i = 0; i < numTerms; i++) {
-            terms[i] = termsList.get(i);
-        }
-        return terms;
-    }
-
-    public static void writeHelperFunction() throws IOException {
-        try (BufferedWriter out = Files.newBufferedWriter(PathUtils.get("addToPlot.m"), StandardCharsets.UTF_8)) {
-            out.write("function handle = addToPlot(numTerms, perDoc, color, linestyle, linewidth)\n" + "handle = line(numTerms, perDoc);\n"
-                + "set(handle, 'color', color);\n" + "set(handle, 'linestyle',linestyle);\n" + "set(handle, 'LineWidth',linewidth);\n"
-                + "end\n");
-        }
-    }
-
-    public static void printOctaveScript(List<Results> allResults, String[] args) throws IOException {
-        if (args.length == 0) {
-            return;
-        }
-        try (BufferedWriter out = Files.newBufferedWriter(PathUtils.get(args[0]), StandardCharsets.UTF_8)) {
-            out.write("#! /usr/local/bin/octave -qf");
-            out.write("\n\n\n\n");
-            out.write("######################################\n");
-            out.write("# Octave script for plotting results\n");
-            String filename = "scriptScoreBenchmark" + new DateTime(DateTimeZone.UTC).toString();
-            out.write("#Call '" + args[0] + "' from the command line. The plot is then in " + filename + "\n\n");
-
-            out.write("handleArray = [];\n tagArray = [];\n plot([]);\n hold on;\n");
-            for (Results result : allResults) {
-                out.write("\n");
-                out.write("# " + result.description);
-                result.printResults(out);
-                out.write("handleArray = [handleArray, addToPlot(" + Results.NUM_TERMS + ", " + Results.TIME_PER_DOCIN_MILLIS + ", '"
-                        + result.color + "','" + result.lineStyle + "',5)];\n");
-                out.write("tagArray = [tagArray; '" + result.label + "'];\n");
-                out.write("\n");
-            }
-
-            out.write("xlabel(\'number of query terms');");
-            out.write("ylabel(\'query time per document');");
-
-            out.write("legend(handleArray,tagArray);\n");
-
-            out.write("saveas(gcf,'" + filename + ".png','png')\n");
-            out.write("hold off;\n\n");
-        } catch (IOException e) {
-            System.err.println("Error: " + e.getMessage());
-        }
-        writeHelperFunction();
-    }
-
-    static void printResult(SearchResponse searchResponse, StopWatch stopWatch, String queryInfo) {
-        System.out.println("--> Searching with " + queryInfo + " took " + stopWatch.lastTaskTime() + ", per query "
-                + (stopWatch.lastTaskTime().secondsFrac() / 100) + " for " + searchResponse.getHits().totalHits() + " docs");
-    }
-
-    static void indexData(long numDocs, Client client, boolean randomizeTerms) throws IOException {
-        try {
-            client.admin().indices().prepareDelete("test").execute().actionGet();
-        } catch (Throwable t) {
-            // index might exist already, in this case we do nothing TODO: make
-            // saver in general
-        }
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1").startObject("properties")
-                .startObject("text").field("type", "string").field("index_options", "offsets").field("analyzer", "payload_float")
-                .endObject().endObject().endObject().endObject();
-        client.admin()
-                .indices()
-                .prepareCreate("test")
-                .addMapping("type1", mapping)
-                .setSettings(
-                        Settings.settingsBuilder().put("index.analysis.analyzer.payload_float.tokenizer", "whitespace")
-                                .putArray("index.analysis.analyzer.payload_float.filter", "delimited_float")
-                                .put("index.analysis.filter.delimited_float.delimiter", "|")
-                                .put("index.analysis.filter.delimited_float.encoding", "float")
-                                .put("index.analysis.filter.delimited_float.type", "delimited_payload_filter")
-                                .put("index.number_of_replicas", 0).put("index.number_of_shards", 1)).execute().actionGet();
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        BulkRequestBuilder bulkRequest = client.prepareBulk();
-        Random random = new Random(1);
-        for (int i = 0; i < numDocs; i++) {
-
-            bulkRequest.add(client.prepareIndex().setType("type1").setIndex("test")
-                    .setSource(jsonBuilder().startObject().field("text", randomText(random, randomizeTerms)).endObject()));
-            if (i % 1000 == 0) {
-                bulkRequest.execute().actionGet();
-                bulkRequest = client.prepareBulk();
-            }
-        }
-        bulkRequest.execute().actionGet();
-        client.admin().indices().prepareRefresh("test").execute().actionGet();
-        client.admin().indices().prepareFlush("test").execute().actionGet();
-        System.out.println("Done indexing " + numDocs + " documents");
-
-    }
-
-    private static String randomText(Random random, boolean randomizeTerms) {
-        String text = "";
-        for (int i = 0; i < termsList.size(); i++) {
-            if (random.nextInt(5) == 3 || !randomizeTerms) {
-                text = text + " " + termsList.get(i) + "|1";
-            }
-        }
-        return text;
-    }
-
-    static void printTimings(SearchResponse searchResponse, StopWatch stopWatch, String message, int maxIter) {
-        System.out.println(message);
-        System.out.println(stopWatch.lastTaskTime() + ", " + (stopWatch.lastTaskTime().secondsFrac() / maxIter) + ", "
-                + searchResponse.getHits().totalHits() + ", "
-                + (stopWatch.lastTaskTime().secondsFrac() / (maxIter + searchResponse.getHits().totalHits())));
-    }
-
-    static List<Entry<String, RequestInfo>> initTermQueries(int minTerms, int maxTerms) {
-        List<Entry<String, RequestInfo>> termSearchRequests = new ArrayList<>();
-        for (int nTerms = minTerms; nTerms < maxTerms; nTerms++) {
-            Map<String, Object> params = new HashMap<>();
-            String[] terms = getTerms(nTerms + 1);
-            params.put("text", terms);
-            SearchRequest request = searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                    searchSource().explain(false).size(0).query(QueryBuilders.termsQuery("text", terms)));
-            String infoString = "Results for term query with " + (nTerms + 1) + " terms:";
-            termSearchRequests.add(new AbstractMap.SimpleEntry<>(infoString, new RequestInfo(request, nTerms + 1)));
-        }
-        return termSearchRequests;
-    }
-
-    static List<Entry<String, RequestInfo>> initNativeSearchRequests(int minTerms, int maxTerms, String script, boolean langNative) {
-        List<Entry<String, RequestInfo>> nativeSearchRequests = new ArrayList<>();
-        for (int nTerms = minTerms; nTerms < maxTerms; nTerms++) {
-            Map<String, Object> params = new HashMap<>();
-            String[] terms = getTerms(nTerms + 1);
-            params.put("text", terms);
-            String infoString = "Results for native script with " + (nTerms + 1) + " terms:";
-            ScriptScoreFunctionBuilder scriptFunction = (langNative == true) ? scriptFunction(new Script(script, ScriptType.INLINE,
-                    "native", params)) : scriptFunction(new Script(script, ScriptType.INLINE, null, params));
-            SearchRequest request = searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                    searchSource()
-                            .explain(false)
-                            .size(0)
-                            .query(functionScoreQuery(QueryBuilders.termsQuery("text", terms), scriptFunction).boostMode(
-                                    CombineFunction.REPLACE)));
-            nativeSearchRequests.add(new AbstractMap.SimpleEntry<>(infoString, new RequestInfo(request, nTerms + 1)));
-        }
-        return nativeSearchRequests;
-    }
-
-    static List<Entry<String, RequestInfo>> initScriptMatchAllSearchRequests(String script, boolean langNative) {
-        List<Entry<String, RequestInfo>> nativeSearchRequests = new ArrayList<>();
-        String infoString = "Results for constant score script:";
-        ScriptScoreFunctionBuilder scriptFunction = (langNative == true) ? scriptFunction(new Script(script, ScriptType.INLINE, "native",
-                null)) : scriptFunction(new Script(script));
-        SearchRequest request = searchRequest().searchType(SearchType.QUERY_THEN_FETCH).source(
-                searchSource().explain(false).size(0)
-                        .query(functionScoreQuery(QueryBuilders.matchAllQuery(), scriptFunction).boostMode(CombineFunction.REPLACE)));
-        nativeSearchRequests.add(new AbstractMap.SimpleEntry<>(infoString, new RequestInfo(request, 0)));
-
-        return nativeSearchRequests;
-    }
-
-    static void runBenchmark(Client client, int maxIter, Results results, List<Entry<String, RequestInfo>> nativeSearchRequests,
-            int minTerms, int warmerIter) throws IOException {
-        int counter = 0;
-        for (Entry<String, RequestInfo> entry : nativeSearchRequests) {
-            SearchResponse searchResponse = null;
-            // warm up
-            for (int i = 0; i < warmerIter; i++) {
-                searchResponse = client.search(entry.getValue().request).actionGet();
-            }
-            System.gc();
-            // run benchmark
-            StopWatch stopWatch = new StopWatch();
-            stopWatch.start();
-            for (int i = 0; i < maxIter; i++) {
-                searchResponse = client.search(entry.getValue().request).actionGet();
-            }
-            stopWatch.stop();
-            results.set(searchResponse, stopWatch, entry.getKey(), maxIter, counter, entry.getValue().numTerms);
-            counter++;
-        }
-        results.printResults(null);
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsConstantScoreBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsConstantScoreBenchmark.java
deleted file mode 100644
index 53baf78..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsConstantScoreBenchmark.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.scripts.score;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.benchmark.scripts.score.plugin.NativeScriptExamplesPlugin;
-import org.elasticsearch.benchmark.scripts.score.script.NativeConstantForLoopScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativeConstantScoreScript;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map.Entry;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ScriptsConstantScoreBenchmark extends BasicScriptBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        int minTerms = 49;
-        int maxTerms = 50;
-        int maxIter = 1000;
-        int warmerIter = 1000;
-
-        init(maxTerms);
-        List<Results> allResults = new ArrayList<>();
-
-        String clusterName = ScriptsConstantScoreBenchmark.class.getSimpleName();
-        Settings settings = settingsBuilder().put("name", "node1")
-                                             .put("cluster.name", clusterName).build();
-        Collection<Class<? extends Plugin>> plugins = Collections.<Class<? extends Plugin>>singletonList(NativeScriptExamplesPlugin.class);
-        Node node1 = new MockNode(settings, Version.CURRENT, plugins);
-        node1.start();
-        Client client = node1.client();
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        indexData(10000, client, true);
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        Results results = new Results();
-
-        results.init(maxTerms - minTerms, "native const script score (log(2) 10X)",
-                "Results for native const script score with score = log(2) 10X:", "black", "-.");
-        // init script searches
-        List<Entry<String, RequestInfo>> searchRequests = initScriptMatchAllSearchRequests(
-                NativeConstantForLoopScoreScript.NATIVE_CONSTANT_FOR_LOOP_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        // init native script searches
-        results = new Results();
-        results.init(maxTerms - minTerms, "mvel const (log(2) 10X)", "Results for mvel const score = log(2) 10X:", "red", "-.");
-        searchRequests = initScriptMatchAllSearchRequests("score = 0; for (int i=0; i<10;i++) {score = score + log(2);} return score",
-                false);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-        results.init(maxTerms - minTerms, "native const script score (2)", "Results for native const script score with score = 2:",
-                "black", ":");
-        // init native script searches
-        searchRequests = initScriptMatchAllSearchRequests(NativeConstantScoreScript.NATIVE_CONSTANT_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-        results.init(maxTerms - minTerms, "mvel const (2)", "Results for mvel const score = 2:", "red", "--");
-        // init native script searches
-        searchRequests = initScriptMatchAllSearchRequests("2", false);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        printOctaveScript(allResults, args);
-
-        client.close();
-        node1.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScoreBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScoreBenchmark.java
deleted file mode 100644
index 53c34a2..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScoreBenchmark.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.scripts.score;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.benchmark.scripts.score.plugin.NativeScriptExamplesPlugin;
-import org.elasticsearch.benchmark.scripts.score.script.NativeNaiveTFIDFScoreScript;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map.Entry;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ScriptsScoreBenchmark extends BasicScriptBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        int minTerms = 1;
-        int maxTerms = 50;
-        int maxIter = 100;
-        int warmerIter = 10;
-
-        boolean runMVEL = false;
-        init(maxTerms);
-        List<Results> allResults = new ArrayList<>();
-        String clusterName = ScriptsScoreBenchmark.class.getSimpleName();
-        Settings settings = settingsBuilder().put("name", "node1")
-            .put("cluster.name", clusterName).build();
-        Collection<Class<? extends Plugin>> plugins = Collections.<Class<? extends Plugin>>singletonList(NativeScriptExamplesPlugin.class);
-        Node node1 = new MockNode(settings, Version.CURRENT, plugins);
-        node1.start();
-        Client client = node1.client();
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        indexData(10000, client, false);
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        Results results = new Results();
-        results.init(maxTerms - minTerms, "native tfidf script score dense posting list",
-                "Results for native script score with dense posting list:", "black", "--");
-        // init native script searches
-        List<Entry<String, RequestInfo>> searchRequests = initNativeSearchRequests(minTerms, maxTerms,
-                NativeNaiveTFIDFScoreScript.NATIVE_NAIVE_TFIDF_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-
-        results.init(maxTerms - minTerms, "term query dense posting list", "Results for term query with dense posting lists:", "green",
-                "--");
-        // init term queries
-        searchRequests = initTermQueries(minTerms, maxTerms);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        if (runMVEL) {
-
-            results = new Results();
-            results.init(maxTerms - minTerms, "mvel tfidf dense posting list", "Results for mvel score with dense posting list:", "red",
-                    "--");
-            // init native script searches
-            searchRequests = initNativeSearchRequests(
-                    minTerms,
-                    maxTerms,
-                    "score = 0.0; fi= _terminfo[\"text\"]; for(i=0; i<text.size(); i++){terminfo = fi[text.get(i)]; score = score + terminfo.tf()*fi.getDocCount()/terminfo.df();} return score;",
-                    false);
-            // run actual benchmark
-            runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-            allResults.add(results);
-        }
-
-        indexData(10000, client, true);
-        results = new Results();
-        results.init(maxTerms - minTerms, "native tfidf script score sparse posting list",
-                "Results for native script scorewith sparse posting list:", "black", "-.");
-        // init native script searches
-        searchRequests = initNativeSearchRequests(minTerms, maxTerms, NativeNaiveTFIDFScoreScript.NATIVE_NAIVE_TFIDF_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-
-        results.init(maxTerms - minTerms, "term query sparse posting list", "Results for term query with sparse posting lists:", "green",
-                "-.");
-        // init term queries
-        searchRequests = initTermQueries(minTerms, maxTerms);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        if (runMVEL) {
-
-            results = new Results();
-            results.init(maxTerms - minTerms, "mvel tfidf sparse posting list", "Results for mvel score with sparse posting list:", "red",
-                    "-.");
-            // init native script searches
-            searchRequests = initNativeSearchRequests(
-                    minTerms,
-                    maxTerms,
-                    "score = 0.0; fi= _terminfo[\"text\"]; for(i=0; i<text.size(); i++){terminfo = fi[text.get(i)]; score = score + terminfo.tf()*fi.getDocCount()/terminfo.df();} return score;",
-                    false);
-            // run actual benchmark
-            runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-            allResults.add(results);
-        }
-        printOctaveScript(allResults, args);
-
-        client.close();
-        node1.close();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScorePayloadSumBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScorePayloadSumBenchmark.java
deleted file mode 100644
index b809192..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/ScriptsScorePayloadSumBenchmark.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.scripts.score;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.benchmark.scripts.score.plugin.NativeScriptExamplesPlugin;
-import org.elasticsearch.benchmark.scripts.score.script.NativePayloadSumNoRecordScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativePayloadSumScoreScript;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.MockNode;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.plugins.Plugin;
-
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map.Entry;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ScriptsScorePayloadSumBenchmark extends BasicScriptBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        int minTerms = 1;
-        int maxTerms = 50;
-        int maxIter = 100;
-        int warmerIter = 10;
-
-        init(maxTerms);
-        List<Results> allResults = new ArrayList<>();
-        String clusterName = ScriptsScoreBenchmark.class.getSimpleName();
-        Settings settings = settingsBuilder().put("name", "node1")
-            .put("cluster.name", clusterName).build();
-        Collection<Class<? extends Plugin>> plugins = Collections.<Class<? extends Plugin>>singletonList(NativeScriptExamplesPlugin.class);
-        Node node1 = new MockNode(settings, Version.CURRENT, plugins);
-        node1.start();
-        Client client = node1.client();
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        indexData(10000, client, false);
-        client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-
-        Results results = new Results();
-        // init script searches
-        results.init(maxTerms - minTerms, "native payload sum script score", "Results for native script score:", "green", ":");
-        List<Entry<String, RequestInfo>> searchRequests = initNativeSearchRequests(minTerms, maxTerms,
-                NativePayloadSumScoreScript.NATIVE_PAYLOAD_SUM_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        results = new Results();
-        // init script searches
-        results.init(maxTerms - minTerms, "native payload sum script score no record", "Results for native script score:", "black", ":");
-        searchRequests = initNativeSearchRequests(minTerms, maxTerms,
-                NativePayloadSumNoRecordScoreScript.NATIVE_PAYLOAD_SUM_NO_RECORD_SCRIPT_SCORE, true);
-        // run actual benchmark
-        runBenchmark(client, maxIter, results, searchRequests, minTerms, warmerIter);
-        allResults.add(results);
-
-        printOctaveScript(allResults, args);
-
-        client.close();
-        node1.close();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/plugin/NativeScriptExamplesPlugin.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/plugin/NativeScriptExamplesPlugin.java
deleted file mode 100644
index 2a25f8f..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/plugin/NativeScriptExamplesPlugin.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.scripts.score.plugin;
-
-import org.elasticsearch.benchmark.scripts.score.script.NativeConstantForLoopScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativeConstantScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativeNaiveTFIDFScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativePayloadSumNoRecordScoreScript;
-import org.elasticsearch.benchmark.scripts.score.script.NativePayloadSumScoreScript;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.ScriptModule;
-
-public class NativeScriptExamplesPlugin extends Plugin {
-
-
-    @Override
-    public String name() {
-        return "native-script-example";
-    }
-
-    @Override
-    public String description() {
-        return "Native script examples";
-    }
-
-    public void onModule(ScriptModule module) {
-        module.registerScript(NativeNaiveTFIDFScoreScript.NATIVE_NAIVE_TFIDF_SCRIPT_SCORE, NativeNaiveTFIDFScoreScript.Factory.class);
-        module.registerScript(NativeConstantForLoopScoreScript.NATIVE_CONSTANT_FOR_LOOP_SCRIPT_SCORE, NativeConstantForLoopScoreScript.Factory.class);
-        module.registerScript(NativeConstantScoreScript.NATIVE_CONSTANT_SCRIPT_SCORE, NativeConstantScoreScript.Factory.class);
-        module.registerScript(NativePayloadSumScoreScript.NATIVE_PAYLOAD_SUM_SCRIPT_SCORE, NativePayloadSumScoreScript.Factory.class);
-        module.registerScript(NativePayloadSumNoRecordScoreScript.NATIVE_PAYLOAD_SUM_NO_RECORD_SCRIPT_SCORE, NativePayloadSumNoRecordScoreScript.Factory.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java
deleted file mode 100644
index fee0a7e..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantForLoopScoreScript.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeConstantForLoopScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_CONSTANT_FOR_LOOP_SCRIPT_SCORE = "native_constant_for_loop_script_score";
-    
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeConstantForLoopScoreScript(params);
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativeConstantForLoopScoreScript(Map<String, Object> params) {
-
-    }
-
-    @Override
-    public Object run() {
-        float score = 0;
-        for (int i = 0; i < 10; i++) {
-            score += Math.log(2);
-        }
-        return score;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java
deleted file mode 100644
index 17220cd..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeConstantScoreScript.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.Map;
-
-public class NativeConstantScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_CONSTANT_SCRIPT_SCORE = "native_constant_script_score";
-    
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeConstantScoreScript();
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativeConstantScoreScript() {
-    }
-
-    @Override
-    public Object run() {
-        return 2;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java
deleted file mode 100644
index e96b35d..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-import org.elasticsearch.search.lookup.IndexFieldTerm;
-import org.elasticsearch.search.lookup.IndexField;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Map;
-
-public class NativeNaiveTFIDFScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_NAIVE_TFIDF_SCRIPT_SCORE = "native_naive_tfidf_script_score";
-    String field = null;
-    String[] terms = null;
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativeNaiveTFIDFScoreScript(params);
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativeNaiveTFIDFScoreScript(Map<String, Object> params) {
-        params.entrySet();
-        terms = new String[params.size()];
-        field = params.keySet().iterator().next();
-        Object o = params.get(field);
-        ArrayList<String> arrayList = (ArrayList<String>) o;
-        terms = arrayList.toArray(new String[arrayList.size()]);
-
-    }
-
-    @Override
-    public Object run() {
-        float score = 0;
-        IndexField indexField = indexLookup().get(field);
-        for (int i = 0; i < terms.length; i++) {
-            IndexFieldTerm indexFieldTerm = indexField.get(terms[i]);
-            try {
-                if (indexFieldTerm.tf() != 0) {
-                    score += indexFieldTerm.tf() * indexField.docCount() / indexFieldTerm.df();
-                }
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-        }
-        return score;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java
deleted file mode 100644
index 7570426..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.search.lookup.IndexFieldTerm;
-import org.elasticsearch.search.lookup.IndexField;
-import org.elasticsearch.search.lookup.IndexLookup;
-import org.elasticsearch.search.lookup.TermPosition;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.ArrayList;
-import java.util.Map;
-
-public class NativePayloadSumNoRecordScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_PAYLOAD_SUM_NO_RECORD_SCRIPT_SCORE = "native_payload_sum_no_record_script_score";
-    String field = null;
-    String[] terms = null;
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativePayloadSumNoRecordScoreScript(params);
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativePayloadSumNoRecordScoreScript(Map<String, Object> params) {
-        params.entrySet();
-        terms = new String[params.size()];
-        field = params.keySet().iterator().next();
-        Object o = params.get(field);
-        ArrayList<String> arrayList = (ArrayList<String>) o;
-        terms = arrayList.toArray(new String[arrayList.size()]);
-
-    }
-
-    @Override
-    public Object run() {
-        float score = 0;
-        IndexField indexField = indexLookup().get(field);
-        for (int i = 0; i < terms.length; i++) {
-            IndexFieldTerm indexFieldTerm = indexField.get(terms[i], IndexLookup.FLAG_PAYLOADS);
-            for (TermPosition pos : indexFieldTerm) {
-                score += pos.payloadAsFloat(0);
-            }
-        }
-        return score;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java b/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java
deleted file mode 100644
index 1522b3a..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.scripts.score.script;
-
-import org.elasticsearch.search.lookup.IndexFieldTerm;
-import org.elasticsearch.search.lookup.IndexField;
-import org.elasticsearch.search.lookup.IndexLookup;
-import org.elasticsearch.search.lookup.TermPosition;
-
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.script.AbstractSearchScript;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.NativeScriptFactory;
-
-import java.util.ArrayList;
-import java.util.Map;
-
-public class NativePayloadSumScoreScript extends AbstractSearchScript {
-
-    public static final String NATIVE_PAYLOAD_SUM_SCRIPT_SCORE = "native_payload_sum_script_score";
-    String field = null;
-    String[] terms = null;
-
-    public static class Factory implements NativeScriptFactory {
-
-        @Override
-        public ExecutableScript newScript(@Nullable Map<String, Object> params) {
-            return new NativePayloadSumScoreScript(params);
-        }
-
-        @Override
-        public boolean needsScores() {
-            return false;
-        }
-    }
-
-    private NativePayloadSumScoreScript(Map<String, Object> params) {
-        params.entrySet();
-        terms = new String[params.size()];
-        field = params.keySet().iterator().next();
-        Object o = params.get(field);
-        ArrayList<String> arrayList = (ArrayList<String>) o;
-        terms = arrayList.toArray(new String[arrayList.size()]);
-
-    }
-
-    @Override
-    public Object run() {
-        float score = 0;
-        IndexField indexField = indexLookup().get(field);
-        for (int i = 0; i < terms.length; i++) {
-            IndexFieldTerm indexFieldTerm = indexField.get(terms[i], IndexLookup.FLAG_PAYLOADS | IndexLookup.FLAG_CACHE);
-            for (TermPosition pos : indexFieldTerm) {
-                score += pos.payloadAsFloat(0);
-            }
-        }
-        return score;
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java b/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
deleted file mode 100644
index d78a478..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.suggest.Suggest.Suggestion.Entry.Option;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-import org.elasticsearch.search.suggest.SuggestBuilders;
-
-import java.io.IOException;
-import java.util.List;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- */
-public class SuggestSearchBenchMark {
-
-    public static void main(String[] args) throws Exception {
-        int SEARCH_ITERS = 200;
-
-        Settings settings = settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node" + i)).node();
-        }
-
-        Client client = nodes[0].client();
-        try {
-            client.admin().indices().prepareCreate("test").setSettings(settings).addMapping("type1", XContentFactory.jsonBuilder().startObject().startObject("type1")
-                    .startObject("_source").field("enabled", false).endObject()
-                    .startObject("_all").field("enabled", false).endObject()
-                    .startObject("_type").field("index", "no").endObject()
-                    .startObject("_id").field("index", "no").endObject()
-                    .startObject("properties")
-                    .startObject("field").field("type", "string").field("index", "not_analyzed").field("omit_norms", true).endObject()
-                    .endObject()
-                    .endObject().endObject()).execute().actionGet();
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth("test").setWaitForGreenStatus().execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-
-            StopWatch stopWatch = new StopWatch().start();
-            long COUNT = SizeValue.parseSizeValue("10m").singles();
-            int BATCH = 100;
-            System.out.println("Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            char character = 'a';
-            int idCounter = 0;
-            for (; i <= ITERS; i++) {
-                int termCounter = 0;
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(idCounter++)).source(source("prefix" + character + termCounter++)));
-                }
-                character++;
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("failures...");
-                }
-            }
-            System.out.println("Indexing took " + stopWatch.totalTime());
-
-            client.admin().indices().prepareRefresh().execute().actionGet();
-            System.out.println("Count: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-            client.admin().indices().prepareRefresh().execute().actionGet();
-            System.out.println("Count: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-        }
-
-
-        System.out.println("Warming up...");
-        char startChar = 'a';
-        for (int i = 0; i <= 20; i++) {
-            String term = "prefix" + startChar;
-            SearchResponse response = client.prepareSearch()
-                    .setQuery(prefixQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").field("field").text(term)
-                                    .suggestMode("always")))
-                    .execute().actionGet();
-            if (response.getHits().totalHits() == 0) {
-                System.err.println("No hits");
-                continue;
-            }
-            startChar++;
-        }
-
-
-        System.out.println("Starting benchmarking suggestions.");
-        startChar = 'a';
-        long timeTaken = 0;
-        for (int i = 0; i <= SEARCH_ITERS; i++) {
-            String term = "prefix" + startChar;
-            SearchResponse response = client.prepareSearch()
-                    .setQuery(matchQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").text(term).field("field")
-                                    .suggestMode("always")))
-                    .execute().actionGet();
-            timeTaken += response.getTookInMillis();
-            if (response.getSuggest() == null) {
-                System.err.println("No suggestions");
-                continue;
-            }
-            List<? extends Option> options = response.getSuggest().getSuggestion("field").getEntries().get(0).getOptions();
-            if (options == null || options.isEmpty()) {
-                System.err.println("No suggestions");
-            }
-            startChar++;
-        }
-
-        System.out.println("Avg time taken without filter " + (timeTaken / SEARCH_ITERS));
-
-        client.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    private static XContentBuilder source(String nameValue) throws IOException {
-        return jsonBuilder().startObject()
-                .field("field", nameValue)
-                .endObject();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/CardinalityAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/CardinalityAggregationSearchBenchmark.java
deleted file mode 100644
index 40e2781..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/CardinalityAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.metrics.cardinality.Cardinality;
-
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.cardinality;
-
-public class CardinalityAggregationSearchBenchmark {
-
-    private static final Random R = new Random();
-    private static final String CLUSTER_NAME = CardinalityAggregationSearchBenchmark.class.getSimpleName();
-    private static final int NUM_DOCS = 10000000;
-    private static final int LOW_CARD = 1000;
-    private static final int HIGH_CARD = 1000000;
-    private static final int BATCH = 100;
-    private static final int WARM = 5;
-    private static final int RUNS = 10;
-    private static final int ITERS = 5;
-
-    public static void main(String[] args) {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 5)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(CLUSTER_NAME)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(CLUSTER_NAME)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = clientNode.client();
-
-        try {
-            client.admin().indices().create(createIndexRequest("index").settings(settings).mapping("type",
-                    jsonBuilder().startObject().startObject("type").startObject("properties")
-                        .startObject("low_card_str_value")
-                            .field("type", "multi_field")
-                            .startObject("fields")
-                                .startObject("low_card_str_value")
-                                    .field("type", "string")
-                                .endObject()
-                                .startObject("hash")
-                                    .field("type", "murmur3")
-                                .endObject()
-                            .endObject()
-                        .endObject()
-                        .startObject("high_card_str_value")
-                            .field("type", "multi_field")
-                            .startObject("fields")
-                                .startObject("high_card_str_value")
-                                    .field("type", "string")
-                                .endObject()
-                                .startObject("hash")
-                                    .field("type", "murmur3")
-                                .endObject()
-                            .endObject()
-                        .endObject()
-                        .startObject("low_card_num_value")
-                            .field("type", "long")
-                        .endObject()
-                        .startObject("high_card_num_value")
-                            .field("type", "long")
-                        .endObject()
-                    .endObject().endObject().endObject())).actionGet();
-
-            System.out.println("Indexing " + NUM_DOCS + " documents");
-
-            StopWatch stopWatch = new StopWatch().start();
-            for (int i = 0; i < NUM_DOCS; ) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH && i < NUM_DOCS; ++j) {
-                    final int lowCard = RandomInts.randomInt(R, LOW_CARD);
-                    final int highCard = RandomInts.randomInt(R, HIGH_CARD);
-                    request.add(client.prepareIndex("index", "type", Integer.toString(i)).setSource("low_card_str_value", "str" + lowCard, "high_card_str_value", "str" + highCard, "low_card_num_value", lowCard , "high_card_num_value", highCard));
-                    ++i;
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                    System.err.println(response.buildFailureMessage());
-                }
-                if ((i % 100000) == 0) {
-                    System.out.println("--> Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-
-            client.admin().indices().prepareRefresh("index").execute().actionGet();
-        } catch (Exception e) {
-            System.out.println("Index already exists, skipping index creation");
-        }
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        for (int i = 0; i < WARM + RUNS; ++i) {
-            if (i >= WARM) {
-                System.out.println("RUN " + (i - WARM));
-            }
-            for (String field : new String[] {"low_card_str_value", "low_card_str_value.hash", "high_card_str_value", "high_card_str_value.hash", "low_card_num_value", "high_card_num_value"}) {
-                long start = System.nanoTime();
-                SearchResponse resp = null;
-                for (int j = 0; j < ITERS; ++j) {
-                    resp = client.prepareSearch("index").setSize(0).addAggregation(cardinality("cardinality").field(field)).execute().actionGet();
-                }
-                long end = System.nanoTime();
-                final long cardinality = ((Cardinality) resp.getAggregations().get("cardinality")).getValue();
-                if (i >= WARM) {
-                    System.out.println(field + "\t" + new TimeValue((end - start) / ITERS, TimeUnit.NANOSECONDS) + "\tcardinality=" + cardinality);
-                }
-            }
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java
deleted file mode 100644
index ed94397..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/GlobalOrdinalsBenchmark.java
+++ /dev/null
@@ -1,248 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.hppc.IntIntHashMap;
-import com.carrotsearch.hppc.ObjectHashSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.benchmark.search.aggregations.TermsAggregationSearchBenchmark.StatsResult;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.transport.TransportModule;
-
-import java.util.*;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-@SuppressForbidden(reason = "not really source code or a test")
-public class GlobalOrdinalsBenchmark {
-
-    private static final String INDEX_NAME = "index";
-    private static final String TYPE_NAME = "type";
-    private static final int QUERY_WARMUP = 25;
-    private static final int QUERY_COUNT = 100;
-    private static final int FIELD_START = 1;
-    private static final int FIELD_LIMIT = 1 << 22;
-    private static final boolean USE_DOC_VALUES = false;
-
-    static long COUNT = SizeValue.parseSizeValue("5m").singles();
-    static Node node;
-    static Client client;
-
-    public static void main(String[] args) throws Exception {
-        System.setProperty("es.logger.prefix", "");
-        BootstrapForTesting.ensureInitialized();
-        Random random = new Random();
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .put(TransportModule.TRANSPORT_TYPE_KEY, "local")
-                .build();
-
-        String clusterName = GlobalOrdinalsBenchmark.class.getSimpleName();
-        node = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings))
-                    .node();
-
-        client = node.client();
-
-        try {
-            client.admin().indices().prepareCreate(INDEX_NAME)
-                    .addMapping(TYPE_NAME, jsonBuilder().startObject().startObject(TYPE_NAME)
-                            .startArray("dynamic_templates")
-                                .startObject()
-                                    .startObject("default")
-                                        .field("match", "*")
-                                        .field("match_mapping_type", "string")
-                                        .startObject("mapping")
-                                            .field("type", "string")
-                                            .field("index", "not_analyzed")
-                                            .startObject("fields")
-                                                .startObject("doc_values")
-                                                    .field("type", "string")
-                                                    .field("index", "no")
-                                                    .startObject("fielddata")
-                                                        .field("format", "doc_values")
-                                                    .endObject()
-                                                .endObject()
-                                            .endObject()
-                                        .endObject()
-                                    .endObject()
-                                .endObject()
-                            .endArray()
-                    .endObject().endObject())
-                    .get();
-            ObjectHashSet<String> uniqueTerms = new ObjectHashSet<>();
-            for (int i = 0; i < FIELD_LIMIT; i++) {
-                boolean added;
-                do {
-                    added = uniqueTerms.add(RandomStrings.randomAsciiOfLength(random, 16));
-                } while (!added);
-            }
-            String[] sValues = uniqueTerms.toArray(String.class);
-            uniqueTerms = null;
-
-            BulkRequestBuilder builder = client.prepareBulk();
-            IntIntHashMap tracker = new IntIntHashMap();
-            for (int i = 0; i < COUNT; i++) {
-                Map<String, Object> fieldValues = new HashMap<>();
-                for (int fieldSuffix = 1; fieldSuffix <= FIELD_LIMIT; fieldSuffix <<= 1) {
-                    int index = tracker.putOrAdd(fieldSuffix, 0, 0);
-                    if (index >= fieldSuffix) {
-                        index = random.nextInt(fieldSuffix);
-                        fieldValues.put("field_" + fieldSuffix, sValues[index]);
-                    } else {
-                        fieldValues.put("field_" + fieldSuffix, sValues[index]);
-                        tracker.put(fieldSuffix, ++index);
-                    }
-                }
-                builder.add(
-                        client.prepareIndex(INDEX_NAME, TYPE_NAME, String.valueOf(i))
-                        .setSource(fieldValues)
-                );
-
-                if (builder.numberOfActions() >= 1000) {
-                    builder.get();
-                    builder = client.prepareBulk();
-                }
-            }
-            if (builder.numberOfActions() > 0) {
-                builder.get();
-            }
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-
-        client.admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put("logger.index.fielddata.ordinals", "DEBUG"))
-                .get();
-
-        client.admin().indices().prepareRefresh(INDEX_NAME).execute().actionGet();
-        COUNT = client.prepareSearch(INDEX_NAME).setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        List<StatsResult> stats = new ArrayList<>();
-        for (int fieldSuffix = FIELD_START; fieldSuffix <= FIELD_LIMIT; fieldSuffix <<= 1) {
-            String fieldName = "field_" + fieldSuffix;
-            String name = "global_ordinals-" + fieldName;
-            if (USE_DOC_VALUES) {
-                fieldName = fieldName + ".doc_values";
-                name = name + "_doc_values"; // can't have . in agg name
-            }
-            stats.add(terms(name, fieldName, "global_ordinals_low_cardinality"));
-        }
-
-        for (int fieldSuffix = FIELD_START; fieldSuffix <= FIELD_LIMIT; fieldSuffix <<= 1) {
-            String fieldName = "field_" + fieldSuffix;
-            String name = "ordinals-" + fieldName;
-            if (USE_DOC_VALUES) {
-                fieldName = fieldName + ".doc_values";
-                name = name + "_doc_values"; // can't have . in agg name
-            }
-            stats.add(terms(name, fieldName, "ordinals"));
-        }
-
-        System.out.println("------------------ SUMMARY -----------------------------------------");
-        System.out.format(Locale.ENGLISH, "%30s%10s%10s%15s\n", "name", "took", "millis", "fieldata size");
-        for (StatsResult stat : stats) {
-            System.out.format(Locale.ENGLISH, "%30s%10s%10d%15s\n", stat.name, TimeValue.timeValueMillis(stat.took), (stat.took / QUERY_COUNT), stat.fieldDataMemoryUsed);
-        }
-        System.out.println("------------------ SUMMARY -----------------------------------------");
-
-        client.close();
-        node.close();
-    }
-
-    private static StatsResult terms(String name, String field, String executionHint) {
-        long totalQueryTime;// LM VALUE
-
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-        System.gc();
-
-        System.out.println("--> Warmup (" + name + ")...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch(INDEX_NAME)
-                    .setSize(0)
-                    .setQuery(matchAllQuery())
-                    .addAggregation(AggregationBuilders.terms(name).field(field).executionHint(executionHint))
-                    .get();
-            if (j == 0) {
-                System.out.println("--> Loading (" + field + "): took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(INDEX_NAME)
-                    .setSize(0)
-                    .setQuery(matchAllQuery())
-                    .addAggregation(AggregationBuilders.terms(name).field(field).executionHint(executionHint))
-                    .get();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Terms Agg (" + name + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        String nodeId = node.injector().getInstance(Discovery.class).localNode().getId();
-        ClusterStatsResponse clusterStateResponse = client.admin().cluster().prepareClusterStats().setNodesIds(nodeId).get();
-        System.out.println("--> Heap used: " + clusterStateResponse.getNodesStats().getJvm().getHeapUsed());
-        ByteSizeValue fieldDataMemoryUsed = clusterStateResponse.getIndicesStats().getFieldData().getMemorySize();
-        System.out.println("--> Fielddata memory size: " + fieldDataMemoryUsed);
-
-        return new StatsResult(name, totalQueryTime, fieldDataMemoryUsed);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HDRPercentilesAggregationBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HDRPercentilesAggregationBenchmark.java
deleted file mode 100644
index af0eee6..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HDRPercentilesAggregationBenchmark.java
+++ /dev/null
@@ -1,158 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeUnit;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.metrics.percentiles.PercentilesMethod;
-import org.elasticsearch.search.aggregations.metrics.percentiles.hdr.InternalHDRPercentiles;
-import org.elasticsearch.search.aggregations.metrics.percentiles.tdigest.InternalTDigestPercentiles;
-
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.percentiles;
-
-public class HDRPercentilesAggregationBenchmark {
-
-    private static final String TYPE_NAME = "type";
-    private static final String INDEX_NAME = "index";
-    private static final String HIGH_CARD_FIELD_NAME = "high_card";
-    private static final String LOW_CARD_FIELD_NAME = "low_card";
-    private static final String GAUSSIAN_FIELD_NAME = "gauss";
-    private static final Random R = new Random();
-    private static final String CLUSTER_NAME = HDRPercentilesAggregationBenchmark.class.getSimpleName();
-    private static final int NUM_DOCS = 10000000;
-    private static final int LOW_CARD = 1000;
-    private static final int HIGH_CARD = 1000000;
-    private static final int BATCH = 100;
-    private static final int WARM = 5;
-    private static final int RUNS = 10;
-    private static final int ITERS = 5;
-
-    public static void main(String[] args) {
-        long overallStartTime = System.currentTimeMillis();
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 5)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(CLUSTER_NAME)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(CLUSTER_NAME)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = clientNode.client();
-
-        try {
-            client.admin().indices().prepareCreate(INDEX_NAME);
-
-            System.out.println("Indexing " + NUM_DOCS + " documents");
-
-            StopWatch stopWatch = new StopWatch().start();
-            for (int i = 0; i < NUM_DOCS; ) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH && i < NUM_DOCS; ++j) {
-                    final int lowCard = RandomInts.randomInt(R, LOW_CARD);
-                    final int highCard = RandomInts.randomInt(R, HIGH_CARD);
-                    int gauss = -1;
-                    while (gauss < 0) {
-                        gauss = (int) (R.nextGaussian() * 1000) + 5000; // mean: 5 sec, std deviation: 1 sec
-                    }
-                    request.add(client.prepareIndex(INDEX_NAME, TYPE_NAME, Integer.toString(i)).setSource(LOW_CARD_FIELD_NAME, lowCard,
-                            HIGH_CARD_FIELD_NAME, highCard, GAUSSIAN_FIELD_NAME, gauss));
-                    ++i;
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                    System.err.println(response.buildFailureMessage());
-                }
-                if ((i % 100000) == 0) {
-                    System.out.println("--> Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-
-            client.admin().indices().prepareRefresh(INDEX_NAME).execute().actionGet();
-        } catch (Exception e) {
-            System.out.println("Index already exists, skipping index creation");
-        }
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        System.out.println("Run\tField\tMethod\tAggregationTime\tEstimatedMemory");
-        for (int i = 0; i < WARM + RUNS; ++i) {
-            for (String field : new String[] { LOW_CARD_FIELD_NAME, HIGH_CARD_FIELD_NAME, GAUSSIAN_FIELD_NAME }) {
-                for (PercentilesMethod method : new PercentilesMethod[] {PercentilesMethod.TDIGEST, PercentilesMethod.HDR}) {
-                    long start = System.nanoTime();
-                    SearchResponse resp = null;
-                    for (int j = 0; j < ITERS; ++j) {
-                        resp = client.prepareSearch(INDEX_NAME).setSize(0).addAggregation(percentiles("percentiles").field(field).method(method)).execute().actionGet();
-                    }
-                    long end = System.nanoTime();
-                    long memoryEstimate = 0;
-                    switch (method) {
-                    case TDIGEST:
-                        memoryEstimate = ((InternalTDigestPercentiles) resp.getAggregations().get("percentiles"))
-                                .getEstimatedMemoryFootprint();
-                        break;
-                    case HDR:
-                        memoryEstimate = ((InternalHDRPercentiles) resp.getAggregations().get("percentiles")).getEstimatedMemoryFootprint();
-                        break;
-                    }
-                    if (i >= WARM) {
-                        System.out.println((i - WARM) + "\t" + field + "\t" + method + "\t"
-                                + new TimeValue((end - start) / ITERS, TimeUnit.NANOSECONDS).millis() + "\t"
-                                + new SizeValue(memoryEstimate, SizeUnit.SINGLE).singles());
-                    }
-                }
-            }
-        }
-        long overallEndTime = System.currentTimeMillis();
-        System.out.println("Benchmark completed in " + ((overallEndTime - overallStartTime) / 1000) + " seconds");
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HistogramAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HistogramAggregationSearchBenchmark.java
deleted file mode 100644
index d54c295..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HistogramAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.node.Node;
-
-import java.util.Date;
-import java.util.Random;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.*;
-
-/**
- *
- */
-public class HistogramAggregationSearchBenchmark {
-
-    static final long COUNT = SizeValue.parseSizeValue("20m").singles();
-    static final int BATCH = 1000;
-    static final int QUERY_WARMUP = 5;
-    static final int QUERY_COUNT = 20;
-    static final int NUMBER_OF_TERMS = 1000;
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = HistogramAggregationSearchBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder()
-                .clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "node1")).node();
-
-        //Node clientNode = nodeBuilder().clusterName(clusterName).settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = node1.client();
-
-        long[] lValues = new long[NUMBER_OF_TERMS];
-        for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-            lValues[i] = i;
-        }
-
-        Random r = new Random();
-        try {
-            client.admin().indices().prepareCreate("test")
-                    .setSettings(settingsBuilder().put(settings))
-                    .addMapping("type1", jsonBuilder()
-                        .startObject()
-                            .startObject("type1")
-                                .startObject("properties")
-                                    .startObject("l_value")
-                                        .field("type", "long")
-                                    .endObject()
-                                    .startObject("i_value")
-                                        .field("type", "integer")
-                                    .endObject()
-                                    .startObject("s_value")
-                                        .field("type", "short")
-                                    .endObject()
-                                    .startObject("b_value")
-                                        .field("type", "byte")
-                                    .endObject()
-                                .endObject()
-                            .endObject()
-                        .endObject())
-                    .execute().actionGet();
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long iters = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= iters; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-                    final long value = lValues[r.nextInt(lValues.length)];
-                    XContentBuilder source = jsonBuilder().startObject()
-                            .field("id", Integer.valueOf(counter))
-                            .field("l_value", value)
-                            .field("i_value", (int) value)
-                            .field("s_value", (short) value)
-                            .field("b_value", (byte) value)
-                            .field("date", new Date())
-                            .endObject();
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(source));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            client.admin().indices().prepareFlush("test").execute().actionGet();
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        if (client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits() != COUNT) {
-            throw new Error();
-        }
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        System.out.println("--> Warmup...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addAggregation(histogram("l_value").field("l_value").interval(4))
-                    .addAggregation(histogram("i_value").field("i_value").interval(4))
-                    .addAggregation(histogram("s_value").field("s_value").interval(4))
-                    .addAggregation(histogram("b_value").field("b_value").interval(4))
-                    .addAggregation(histogram("date").field("date").interval(1000))
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Warmup took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup DONE");
-
-        long totalQueryTime = 0;
-        for (String field : new String[] {"b_value", "s_value", "i_value", "l_value"}) {
-            totalQueryTime = 0;
-            for (int j = 0; j < QUERY_COUNT; j++) {
-                SearchResponse searchResponse = client.prepareSearch()
-                        .setQuery(matchAllQuery())
-                        .addAggregation(histogram(field).field(field).interval(4))
-                        .execute().actionGet();
-                if (searchResponse.getHits().totalHits() != COUNT) {
-                    System.err.println("--> mismatch on hits");
-                }
-                totalQueryTime += searchResponse.getTookInMillis();
-            }
-            System.out.println("--> Histogram Aggregation (" + field + ") " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-            totalQueryTime = 0;
-            for (int j = 0; j < QUERY_COUNT; j++) {
-                SearchResponse searchResponse = client.prepareSearch()
-                        .setQuery(matchAllQuery())
-                        .addAggregation(histogram(field).field(field).subAggregation(stats(field).field(field)).interval(4))
-                        .execute().actionGet();
-                if (searchResponse.getHits().totalHits() != COUNT) {
-                    System.err.println("--> mismatch on hits");
-                }
-                totalQueryTime += searchResponse.getTookInMillis();
-            }
-            System.out.println("--> Histogram Aggregation (" + field + "/" + field + ") " + (totalQueryTime / QUERY_COUNT) + "ms");
-        }
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addAggregation(dateHistogram("date").field("date").interval(1000))
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Histogram Aggregation (date) " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addAggregation(dateHistogram("date").field("date").interval(1000).subAggregation(stats("stats").field("l_value")))
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Histogram Aggregation (date/l_value) " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        node1.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/IncludeExcludeAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/IncludeExcludeAggregationSearchBenchmark.java
deleted file mode 100644
index 1bf8a33..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/IncludeExcludeAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,130 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.apache.lucene.util.TestUtil;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.terms;
-
-public class IncludeExcludeAggregationSearchBenchmark {
-
-    private static final Random R = new Random();
-    private static final String CLUSTER_NAME = IncludeExcludeAggregationSearchBenchmark.class.getSimpleName();
-    private static final int NUM_DOCS = 10000000;
-    private static final int BATCH = 100;
-    private static final int WARM = 3;
-    private static final int RUNS = 10;
-    private static final int ITERS = 3;
-
-    public static void main(String[] args) {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(CLUSTER_NAME)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(CLUSTER_NAME)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = clientNode.client();
-
-        try {
-            client.admin().indices().create(createIndexRequest("index").settings(settings).mapping("type",
-                    jsonBuilder().startObject().startObject("type").startObject("properties")
-                        .startObject("str")
-                            .field("type", "string")
-                            .field("index", "not_analyzed")
-                        .endObject()
-                    .endObject().endObject().endObject())).actionGet();
-
-            System.out.println("Indexing " + NUM_DOCS + " documents");
-
-            StopWatch stopWatch = new StopWatch().start();
-            for (int i = 0; i < NUM_DOCS; ) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH && i < NUM_DOCS; ++j) {
-                    request.add(client.prepareIndex("index", "type", Integer.toString(i)).setSource("str", TestUtil.randomSimpleString(R)));
-                    ++i;
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                    System.err.println(response.buildFailureMessage());
-                }
-                if ((i % 100000) == 0) {
-                    System.out.println("--> Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-
-            client.admin().indices().prepareRefresh("index").execute().actionGet();
-        } catch (Exception e) {
-            System.out.println("Index already exists, skipping index creation");
-        }
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        for (int i = 0; i < WARM + RUNS; ++i) {
-            if (i >= WARM) {
-                System.out.println("RUN " + (i - WARM));
-            }
-            long start = System.nanoTime();
-            SearchResponse resp = null;
-            for (int j = 0; j < ITERS; ++j) {
-                resp = client.prepareSearch("index").setQuery(QueryBuilders.prefixQuery("str", "sf")).setSize(0).addAggregation(terms("t").field("str").include("s.*")).execute().actionGet();
-            }
-            long end = System.nanoTime();
-            if (i >= WARM) {
-                System.out.println(new TimeValue((end - start) / ITERS, TimeUnit.NANOSECONDS));
-            }
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/PercentilesAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/PercentilesAggregationSearchBenchmark.java
deleted file mode 100644
index 1d5bebe..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/PercentilesAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,213 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.metrics.percentiles.Percentile;
-import org.elasticsearch.search.aggregations.metrics.percentiles.Percentiles;
-
-import java.util.Arrays;
-import java.util.LinkedHashMap;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Random;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.client.Requests.getRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-import static org.elasticsearch.search.aggregations.AggregationBuilders.percentiles;
-
-public class PercentilesAggregationSearchBenchmark {
-
-    private static final int AMPLITUDE = 10000;
-    private static final int NUM_DOCS = (int) SizeValue.parseSizeValue("1m").singles();
-    private static final int BATCH = 100;
-    private static final String CLUSTER_NAME = PercentilesAggregationSearchBenchmark.class.getSimpleName();
-    private static final double[] PERCENTILES = new double[] { 0, 0.01, 0.1, 1, 10, 25, 50, 75, 90, 99, 99.9, 99.99, 100};
-    private static final int QUERY_WARMUP = 10;
-    private static final int QUERY_COUNT = 20;
-
-    private static Random R = new Random(0);
-
-    // we generate ints to not disadvantage qdigest which only works with integers
-    private enum Distribution {
-        UNIFORM {
-            @Override
-            int next() {
-                return (int) (R.nextDouble() * AMPLITUDE);
-            }
-        },
-        GAUSS {
-            @Override
-            int next() {
-                return (int) (R.nextDouble() * AMPLITUDE);
-            }
-        },
-        LOG_NORMAL {
-            @Override
-            int next() {
-                return (int) Math.exp(R.nextDouble() * Math.log(AMPLITUDE));
-            }
-        };
-        String indexName() {
-            return name().toLowerCase(Locale.ROOT);
-        }
-        abstract int next();
-    }
-
-    private static double accuratePercentile(double percentile, int[] sortedValues) {
-        final double index = percentile / 100 * (sortedValues.length - 1);
-        final int intIndex = (int) index;
-        final double delta = index - intIndex;
-        if (delta == 0) {
-            return sortedValues[intIndex];
-        } else {
-            return sortedValues[intIndex] * (1 - delta) + sortedValues[intIndex + 1] * delta;
-        }
-    }
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 100) // to also test performance and accuracy of the reduce phase
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(CLUSTER_NAME)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(CLUSTER_NAME)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        Client client = clientNode.client();
-
-        for (Distribution d : Distribution.values()) {
-            try {
-//                client.admin().indices().prepareDelete(d.indexName()).execute().actionGet();
-                client.admin().indices().create(createIndexRequest(d.indexName()).settings(settings)).actionGet();
-            } catch (Exception e) {
-                System.out.println("Index " + d.indexName() + " already exists, skipping index creation");
-                continue;
-            }
-
-            final int[] values = new int[NUM_DOCS];
-            for (int i = 0; i < NUM_DOCS; ++i) {
-                values[i] = d.next();
-            }
-            System.out.println("Indexing " + NUM_DOCS + " documents into " + d.indexName());
-            StopWatch stopWatch = new StopWatch().start();
-            for (int i = 0; i < NUM_DOCS; ) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH && i < NUM_DOCS; ++j) {
-                    request.add(client.prepareIndex(d.indexName(), "values", Integer.toString(i)).setSource("v", values[i]));
-                    ++i;
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                    System.err.println(response.buildFailureMessage());
-                }
-                if ((i % 100000) == 0) {
-                    System.out.println("--> Indexed " + i + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            Arrays.sort(values);
-            XContentBuilder builder = JsonXContent.contentBuilder().startObject();
-            for (double percentile : PERCENTILES) {
-                builder.field(Double.toString(percentile), accuratePercentile(percentile, values));
-            }
-            client.prepareIndex(d.indexName(), "values", "percentiles").setSource(builder.endObject()).execute().actionGet();
-            client.admin().indices().prepareRefresh(d.indexName()).execute().actionGet();
-        }
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        System.out.println("## Precision");
-        for (Distribution d : Distribution.values()) {
-            System.out.println("#### " + d);
-            final long count = client.prepareSearch(d.indexName()).setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-            if (count != NUM_DOCS + 1) {
-                throw new Error("Expected " + NUM_DOCS + " documents, got " + (count - 1));
-            }
-            Map<String, Object> percentilesUnsorted = client.get(getRequest(d.indexName()).type("values").id("percentiles")).actionGet().getSourceAsMap();
-            SortedMap<Double, Double> percentiles = new TreeMap<>();
-            for (Map.Entry<String, Object> entry : percentilesUnsorted.entrySet()) {
-                percentiles.put(Double.parseDouble(entry.getKey()), (Double) entry.getValue());
-            }
-            System.out.println("Expected percentiles: " + percentiles);
-            System.out.println();
-            SearchResponse resp = client.prepareSearch(d.indexName()).setSize(0).addAggregation(percentiles("pcts").field("v").percentiles(PERCENTILES)).execute().actionGet();
-            Percentiles pcts = resp.getAggregations().get("pcts");
-            Map<Double, Double> asMap = new LinkedHashMap<>();
-            double sumOfErrorSquares = 0;
-            for (Percentile percentile : pcts) {
-                asMap.put(percentile.getPercent(), percentile.getValue());
-                double error = percentile.getValue() - percentiles.get(percentile.getPercent());
-                sumOfErrorSquares += error * error;
-            }
-            System.out.println("Percentiles: " + asMap);
-            System.out.println("Sum of error squares: " + sumOfErrorSquares);
-            System.out.println();
-        }
-        
-        System.out.println("## Performance");
-        for (int i = 0; i < 3; ++i) {
-            for (Distribution d : Distribution.values()) {
-                System.out.println("#### " + d);
-                for (int j = 0; j < QUERY_WARMUP; ++j) {
-                    client.prepareSearch(d.indexName()).setSize(0).addAggregation(percentiles("pcts").field("v").percentiles(PERCENTILES)).execute().actionGet();
-                }
-                long start = System.nanoTime();
-                for (int j = 0; j < QUERY_COUNT; ++j) {
-                    client.prepareSearch(d.indexName()).setSize(0).addAggregation(percentiles("pcts").field("v").percentiles(PERCENTILES)).execute().actionGet();
-                }
-                System.out.println(new TimeValue((System.nanoTime() - start) / QUERY_COUNT, TimeUnit.NANOSECONDS));
-            }
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/QueryFilterAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/QueryFilterAggregationSearchBenchmark.java
deleted file mode 100644
index 7dd0167..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/QueryFilterAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-public class QueryFilterAggregationSearchBenchmark {
-
-    static final long COUNT = SizeValue.parseSizeValue("5m").singles();
-    static final int BATCH = 1000;
-    static final int QUERY_COUNT = 200;
-    static final int NUMBER_OF_TERMS = 200;
-
-    static Client client;
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 2)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = QueryFilterAggregationSearchBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder()
-                .clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "node1")).node();
-        client = node1.client();
-
-        long[] lValues = new long[NUMBER_OF_TERMS];
-        for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-            lValues[i] = ThreadLocalRandom.current().nextLong();
-        }
-
-        Thread.sleep(10000);
-        try {
-            client.admin().indices().create(createIndexRequest("test")).actionGet();
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    builder.field("l_value", lValues[ThreadLocalRandom.current().nextInt(NUMBER_OF_TERMS)]);
-
-                    builder.endObject();
-
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 100000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        if (client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits() != COUNT) {
-            throw new Error();
-        }
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        final long anyValue = ((Number) client.prepareSearch().execute().actionGet().getHits().hits()[0].sourceAsMap().get("l_value")).longValue();
-        
-        long totalQueryTime = 0;
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(termQuery("l_value", anyValue))
-                    .execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("-->  Simple Query on first l_value " + totalQueryTime + "ms");
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(termQuery("l_value", anyValue))
-                    .addAggregation(AggregationBuilders.filter("filter").filter(QueryBuilders.termQuery("l_value", anyValue)))
-                    .execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("-->  Filter agg first l_value " + totalQueryTime + "ms");
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java
deleted file mode 100644
index e58787e..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/SubAggregationSearchCollectModeBenchmark.java
+++ /dev/null
@@ -1,315 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.hppc.ObjectScatterSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class SubAggregationSearchCollectModeBenchmark {
-
-    static long COUNT = SizeValue.parseSizeValue("2m").singles();
-    static int BATCH = 1000;
-    static int QUERY_WARMUP = 10;
-    static int QUERY_COUNT = 100;
-    static int NUMBER_OF_TERMS = 200;
-    static int NUMBER_OF_MULTI_VALUE_TERMS = 10;
-    static int STRING_TERM_SIZE = 5;
-
-    static Client client;
-    static Node[] nodes;
-
-    public static void main(String[] args) throws Exception {
-        BootstrapForTesting.ensureInitialized();
-        Random random = new Random();
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = SubAggregationSearchCollectModeBenchmark.class.getSimpleName();
-        nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "client")).client(true).node();
-
-        client = clientNode.client();
-
-        Thread.sleep(10000);
-        try {
-            client.admin().indices().create(createIndexRequest("test").mapping("type1", jsonBuilder()
-              .startObject()
-                .startObject("type1")
-                  .startObject("properties")
-                    .startObject("s_value_dv")
-                      .field("type", "string")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("sm_value_dv")
-                      .field("type", "string")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("l_value_dv")
-                      .field("type", "long")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("lm_value_dv")
-                      .field("type", "long")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                  .endObject()
-                .endObject()
-              .endObject())).actionGet();
-
-            long[] lValues = new long[NUMBER_OF_TERMS];
-            for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-                lValues[i] = ThreadLocalRandom.current().nextLong();
-            }
-            ObjectScatterSet<String> uniqueTerms = new ObjectScatterSet<>();
-            for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-                boolean added;
-                do {
-                    added = uniqueTerms.add(RandomStrings.randomAsciiOfLength(random, STRING_TERM_SIZE));
-                } while (!added);
-            }
-            String[] sValues = uniqueTerms.toArray(String.class);
-            uniqueTerms = null;
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    final String sValue = sValues[ThreadLocalRandom.current().nextInt(sValues.length)];
-                    final long lValue = lValues[ThreadLocalRandom.current().nextInt(lValues.length)];
-                    builder.field("s_value", sValue);
-                    builder.field("l_value", lValue);
-                    builder.field("s_value_dv", sValue);
-                    builder.field("l_value_dv", lValue);
-
-                    for (String field : new String[] {"sm_value", "sm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(sValues[ThreadLocalRandom.current().nextInt(sValues.length)]);
-                        }
-                        builder.endArray();
-                    }
-
-                    for (String field : new String[] {"lm_value", "lm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(lValues[ThreadLocalRandom.current().nextInt(sValues.length)]);
-                        }
-                        builder.endArray();
-                    }
-
-                    builder.endObject();
-
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        COUNT = client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        List<StatsResult> stats = new ArrayList<>();
-        stats.add(runTest("0000", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("0001", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("0010", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("0011", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("0100", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("0101", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("0110", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("0111", new SubAggCollectionMode[] {SubAggCollectionMode.DEPTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("1000", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("1001", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("1010", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("1011", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("1100", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("1101", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-        stats.add(runTest("1110", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.DEPTH_FIRST}));
-        stats.add(runTest("1111", new SubAggCollectionMode[] {SubAggCollectionMode.BREADTH_FIRST,SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST, SubAggCollectionMode.BREADTH_FIRST}));
-
-        System.out.println("------------------ SUMMARY ----------------------------------------------");
-        System.out.format(Locale.ENGLISH, "%35s%10s%10s%15s%15s\n", "name", "took", "millis", "fieldata size", "heap used");
-        for (StatsResult stat : stats) {
-            System.out.format(Locale.ENGLISH, "%35s%10s%10d%15s%15s\n", stat.name, TimeValue.timeValueMillis(stat.took), (stat.took / QUERY_COUNT), stat.fieldDataMemoryUsed, stat.heapUsed);
-        }
-        System.out.println("------------------ SUMMARY ----------------------------------------------");
-
-        clientNode.close();
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    public static class StatsResult {
-        final String name;
-        final long took;
-        final ByteSizeValue fieldDataMemoryUsed;
-        final ByteSizeValue heapUsed;
-
-        public StatsResult(String name, long took, ByteSizeValue fieldDataMemoryUsed, ByteSizeValue heapUsed) {
-            this.name = name;
-            this.took = took;
-            this.fieldDataMemoryUsed = fieldDataMemoryUsed;
-            this.heapUsed = heapUsed;
-        }
-    }
-
-    private static StatsResult runTest(String name, SubAggCollectionMode[] collectionModes) {
-        long totalQueryTime;// LM VALUE
-
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-        System.gc();
-
-        System.out.println("--> Warmup (" + name + ")...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch("test")
-                    .setSize(0)
-                    .setQuery(matchAllQuery())
-                    .addAggregation(AggregationBuilders.terms(name + "s_value").field("s_value").collectMode(collectionModes[0])
-                            .subAggregation(AggregationBuilders.terms(name + "l_value").field("l_value").collectMode(collectionModes[1])
-                                    .subAggregation(AggregationBuilders.terms(name + "s_value_dv").field("s_value_dv").collectMode(collectionModes[2])
-                                            .subAggregation(AggregationBuilders.terms(name + "l_value_dv").field("l_value_dv").collectMode(collectionModes[3])))))
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Loading : took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch("test")
-                    .setSize(0)
-                    .setQuery(matchAllQuery())
-                    .addAggregation(AggregationBuilders.terms(name + "s_value").field("s_value").collectMode(collectionModes[0])
-                            .subAggregation(AggregationBuilders.terms(name + "l_value").field("l_value").collectMode(collectionModes[1])
-                                    .subAggregation(AggregationBuilders.terms(name + "s_value_dv").field("s_value_dv").collectMode(collectionModes[2])
-                                            .subAggregation(AggregationBuilders.terms(name + "l_value_dv").field("l_value_dv").collectMode(collectionModes[3])))))
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Terms Agg (" + name + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        String[] nodeIds = new String[nodes.length];
-        for (int i = 0; i < nodeIds.length; i++) {
-            nodeIds[i] = nodes[i].injector().getInstance(Discovery.class).localNode().getId();
-        }
-
-        ClusterStatsResponse clusterStateResponse = client.admin().cluster().prepareClusterStats().setNodesIds(nodeIds).get();
-        ByteSizeValue heapUsed = clusterStateResponse.getNodesStats().getJvm().getHeapUsed();
-        System.out.println("--> Heap used: " + heapUsed);
-        ByteSizeValue fieldDataMemoryUsed = clusterStateResponse.getIndicesStats().getFieldData().getMemorySize();
-        System.out.println("--> Fielddata memory size: " + fieldDataMemoryUsed);
-
-        return new StatsResult(name, totalQueryTime, fieldDataMemoryUsed, heapUsed);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java
deleted file mode 100644
index 9b544a7..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchAndIndexingBenchmark.java
+++ /dev/null
@@ -1,354 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.hppc.ObjectScatterSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.benchmark.search.aggregations.TermsAggregationSearchBenchmark.Method;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class TermsAggregationSearchAndIndexingBenchmark {
-
-    static String indexName = "test";
-    static String typeName = "type1";
-    static Random random = new Random();
-
-    static long COUNT = SizeValue.parseSizeValue("2m").singles();
-    static int BATCH = 1000;
-    static int NUMBER_OF_TERMS = (int) SizeValue.parseSizeValue("100k").singles();
-    static int NUMBER_OF_MULTI_VALUE_TERMS = 10;
-    static int STRING_TERM_SIZE = 5;
-
-    static Node[] nodes;
-
-    public static void main(String[] args) throws Exception {
-        BootstrapForTesting.ensureInitialized();
-        Settings settings = settingsBuilder()
-                .put("refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = TermsAggregationSearchAndIndexingBenchmark.class.getSimpleName();
-        nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node1"))
-                    .clusterName(clusterName)
-                    .node();
-        }
-        Client client = nodes[0].client();
-
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        try {
-            client.admin().indices().prepareCreate(indexName)
-                    .addMapping(typeName, generateMapping("eager", "lazy"))
-                    .get();
-            Thread.sleep(5000);
-
-            long startTime = System.currentTimeMillis();
-            ObjectScatterSet<String> uniqueTerms = new ObjectScatterSet<>();
-            for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-                boolean added;
-                do {
-                    added = uniqueTerms.add(RandomStrings.randomAsciiOfLength(random, STRING_TERM_SIZE));
-                } while (!added);
-            }
-            String[] sValues = uniqueTerms.toArray(String.class);
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    final String sValue = sValues[counter % sValues.length];
-                    builder.field("s_value", sValue);
-                    builder.field("s_value_dv", sValue);
-
-                    for (String field : new String[] {"sm_value", "sm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(sValues[ThreadLocalRandom.current().nextInt(sValues.length)]);
-                        }
-                        builder.endArray();
-                    }
-
-                    request.add(Requests.indexRequest(indexName).type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH));
-                }
-            }
-
-            System.out.println("--> Indexing took " + ((System.currentTimeMillis() - startTime) / 1000) + " seconds.");
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().preparePutMapping(indexName)
-                .setType(typeName)
-                .setSource(generateMapping("lazy", "lazy"))
-                .get();
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-
-        String[] nodeIds = new String[nodes.length];
-        for (int i = 0; i < nodeIds.length; i++) {
-            nodeIds[i] = nodes[i].injector().getInstance(Discovery.class).localNode().getId();
-        }
-
-        List<TestRun> testRuns = new ArrayList<>();
-        testRuns.add(new TestRun("Regular field ordinals", "eager", "lazy", "s_value", "ordinals"));
-        testRuns.add(new TestRun("Docvalues field ordinals", "lazy", "eager", "s_value_dv", "ordinals"));
-        testRuns.add(new TestRun("Regular field global ordinals", "eager_global_ordinals", "lazy", "s_value", null));
-        testRuns.add(new TestRun("Docvalues field global", "lazy", "eager_global_ordinals", "s_value_dv", null));
-
-        List<TestResult> testResults = new ArrayList<>();
-        for (TestRun testRun : testRuns) {
-            client.admin().indices().preparePutMapping(indexName).setType(typeName)
-                    .setSource(generateMapping(testRun.indexedFieldEagerLoading, testRun.docValuesEagerLoading)).get();
-            client.admin().indices().prepareClearCache(indexName).setFieldDataCache(true).get();
-            SearchThread searchThread = new SearchThread(client, testRun.termsAggsField, testRun.termsAggsExecutionHint);
-            RefreshThread refreshThread = new RefreshThread(client);
-            System.out.println("--> Running '" + testRun.name + "' round...");
-            new Thread(refreshThread).start();
-            new Thread(searchThread).start();
-            Thread.sleep(2 * 60 * 1000);
-            refreshThread.stop();
-            searchThread.stop();
-
-            System.out.println("--> Avg refresh time: " + refreshThread.avgRefreshTime + " ms");
-            System.out.println("--> Avg query time: " + searchThread.avgQueryTime + " ms");
-
-            ClusterStatsResponse clusterStateResponse = client.admin().cluster().prepareClusterStats().setNodesIds(nodeIds).get();
-            System.out.println("--> Heap used: " + clusterStateResponse.getNodesStats().getJvm().getHeapUsed());
-            ByteSizeValue fieldDataMemoryUsed = clusterStateResponse.getIndicesStats().getFieldData().getMemorySize();
-            System.out.println("--> Fielddata memory size: " + fieldDataMemoryUsed);
-            testResults.add(new TestResult(testRun.name, refreshThread.avgRefreshTime, searchThread.avgQueryTime, fieldDataMemoryUsed));
-        }
-
-        System.out.println("----------------------------------------- SUMMARY ----------------------------------------------");
-        System.out.format(Locale.ENGLISH, "%30s%18s%15s%15s\n", "name", "avg refresh time", "avg query time", "fieldata size");
-        for (TestResult testResult : testResults) {
-            System.out.format(Locale.ENGLISH, "%30s%18s%15s%15s\n", testResult.name, testResult.avgRefreshTime, testResult.avgQueryTime, testResult.fieldDataSizeInMemory);
-        }
-        System.out.println("----------------------------------------- SUMMARY ----------------------------------------------");
-
-        client.close();
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    static class RefreshThread implements Runnable {
-
-        private final Client client;
-        private volatile boolean run = true;
-        private volatile boolean stopped = false;
-        private volatile long avgRefreshTime = 0;
-
-        RefreshThread(Client client) throws IOException {
-            this.client = client;
-        }
-
-        @Override
-        public void run() {
-            long totalRefreshTime = 0;
-            int numExecutedRefreshed = 0;
-            while (run) {
-                long docIdLimit = COUNT;
-                for (long docId = 1; run && docId < docIdLimit;) {
-                    try {
-                        for (int j = 0; j < 8; j++) {
-                            GetResponse getResponse = client
-                                    .prepareGet(indexName, "type1", String.valueOf(++docId))
-                                    .get();
-                            client.prepareIndex(indexName, "type1", getResponse.getId())
-                                    .setSource(getResponse.getSource())
-                                    .get();
-                        }
-                        long startTime = System.currentTimeMillis();
-                        client.admin().indices().prepareRefresh(indexName).execute().actionGet();
-                        totalRefreshTime += System.currentTimeMillis() - startTime;
-                        numExecutedRefreshed++;
-                        Thread.sleep(500);
-                    } catch (Throwable e) {
-                        e.printStackTrace();
-                    }
-                }
-            }
-            avgRefreshTime = totalRefreshTime / numExecutedRefreshed;
-            stopped = true;
-        }
-
-        public void stop() throws InterruptedException {
-            run = false;
-            while (!stopped) {
-                Thread.sleep(100);
-            }
-        }
-
-    }
-
-    private static class TestRun {
-
-        final String name;
-        final String indexedFieldEagerLoading;
-        final String docValuesEagerLoading;
-        final String termsAggsField;
-        final String termsAggsExecutionHint;
-
-        private TestRun(String name, String indexedFieldEagerLoading, String docValuesEagerLoading, String termsAggsField, String termsAggsExecutionHint) {
-            this.name = name;
-            this.indexedFieldEagerLoading = indexedFieldEagerLoading;
-            this.docValuesEagerLoading = docValuesEagerLoading;
-            this.termsAggsField = termsAggsField;
-            this.termsAggsExecutionHint = termsAggsExecutionHint;
-        }
-    }
-
-    private static class TestResult {
-
-        final String name;
-        final TimeValue avgRefreshTime;
-        final TimeValue avgQueryTime;
-        final ByteSizeValue fieldDataSizeInMemory;
-
-        private TestResult(String name, long avgRefreshTime, long avgQueryTime, ByteSizeValue fieldDataSizeInMemory) {
-            this.name = name;
-            this.avgRefreshTime = TimeValue.timeValueMillis(avgRefreshTime);
-            this.avgQueryTime = TimeValue.timeValueMillis(avgQueryTime);
-            this.fieldDataSizeInMemory = fieldDataSizeInMemory;
-        }
-    }
-
-    static class SearchThread implements Runnable {
-
-        private final Client client;
-        private final String field;
-        private final String executionHint;
-        private volatile boolean run = true;
-        private volatile boolean stopped = false;
-        private volatile long avgQueryTime = 0;
-
-        SearchThread(Client client, String field, String executionHint) {
-            this.client = client;
-            this.field = field;
-            this.executionHint = executionHint;
-        }
-
-        @Override
-        public void run() {
-            long totalQueryTime = 0;
-            int numExecutedQueries = 0;
-            while (run) {
-                try {
-                    SearchResponse searchResponse = Method.AGGREGATION.addTermsAgg(client.prepareSearch()
-                            .setSize(0)
-                            .setQuery(matchAllQuery()), "test", field, executionHint)
-                            .execute().actionGet();
-                    if (searchResponse.getHits().totalHits() != COUNT) {
-                        System.err.println("--> mismatch on hits");
-                    }
-                    totalQueryTime += searchResponse.getTookInMillis();
-                    numExecutedQueries++;
-                } catch (Throwable e) {
-                    e.printStackTrace();
-                }
-            }
-            avgQueryTime = totalQueryTime / numExecutedQueries;
-            stopped = true;
-        }
-
-        public void stop() throws InterruptedException {
-            run = false;
-            while (!stopped) {
-                Thread.sleep(100);
-            }
-        }
-
-    }
-
-    private static XContentBuilder generateMapping(String loading1, String loading2) throws IOException {
-        return jsonBuilder().startObject().startObject("type1").startObject("properties")
-                .startObject("s_value")
-                .field("type", "string")
-                .field("index", "not_analyzed")
-                .startObject("fielddata")
-                .field("loading", loading1)
-                .endObject()
-                .endObject()
-                .startObject("s_value_dv")
-                .field("type", "string")
-                .field("index", "no")
-                .startObject("fielddata")
-                .field("loading", loading2)
-                .field("format", "doc_values")
-                .endObject()
-                .endObject()
-                .endObject().endObject().endObject();
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java
deleted file mode 100644
index e63fbfe..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java
+++ /dev/null
@@ -1,403 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.aggregations;
-
-import com.carrotsearch.hppc.ObjectScatterSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.bootstrap.BootstrapForTesting;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.discovery.Discovery;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-import java.util.Random;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class TermsAggregationSearchBenchmark {
-
-    static long COUNT = SizeValue.parseSizeValue("2m").singles();
-    static int BATCH = 1000;
-    static int QUERY_WARMUP = 10;
-    static int QUERY_COUNT = 100;
-    static int NUMBER_OF_TERMS = 200;
-    static int NUMBER_OF_MULTI_VALUE_TERMS = 10;
-    static int STRING_TERM_SIZE = 5;
-
-    static Client client;
-    static Node[] nodes;
-
-    public enum Method {
-        AGGREGATION {
-            @Override
-            SearchRequestBuilder addTermsAgg(SearchRequestBuilder builder, String name, String field, String executionHint) {
-                return builder.addAggregation(AggregationBuilders.terms(name).executionHint(executionHint).field(field));
-            }
-
-            @Override
-            SearchRequestBuilder addTermsStatsAgg(SearchRequestBuilder builder, String name, String keyField, String valueField) {
-                return builder.addAggregation(AggregationBuilders.terms(name).field(keyField).subAggregation(AggregationBuilders.stats("stats").field(valueField)));
-            }
-        },
-        AGGREGATION_DEFERRED {
-            @Override
-            SearchRequestBuilder addTermsAgg(SearchRequestBuilder builder, String name, String field, String executionHint) {
-                return builder.addAggregation(AggregationBuilders.terms(name).executionHint(executionHint).field(field).collectMode(SubAggCollectionMode.BREADTH_FIRST));
-            }
-
-            @Override
-            SearchRequestBuilder addTermsStatsAgg(SearchRequestBuilder builder, String name, String keyField, String valueField) {
-                return builder.addAggregation(AggregationBuilders.terms(name).field(keyField).collectMode(SubAggCollectionMode.BREADTH_FIRST).subAggregation(AggregationBuilders.stats("stats").field(valueField)));
-            }
-        };
-        abstract SearchRequestBuilder addTermsAgg(SearchRequestBuilder builder, String name, String field, String executionHint);
-        abstract SearchRequestBuilder addTermsStatsAgg(SearchRequestBuilder builder, String name, String keyField, String valueField);
-    }
-
-    public static void main(String[] args) throws Exception {
-        BootstrapForTesting.ensureInitialized();
-        Random random = new Random();
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = TermsAggregationSearchBenchmark.class.getSimpleName();
-        nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings).put("path.home", "."))
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Node clientNode = nodeBuilder()
-                .clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "client"))
-                .settings(settingsBuilder().put(settings).put("path.home", ".")).client(true).node();
-
-        client = clientNode.client();
-
-        Thread.sleep(10000);
-        try {
-            client.admin().indices().create(createIndexRequest("test").mapping("type1", jsonBuilder()
-              .startObject()
-                .startObject("type1")
-                  .startObject("properties")
-                    .startObject("s_value_dv")
-                      .field("type", "string")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("sm_value_dv")
-                      .field("type", "string")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("l_value_dv")
-                      .field("type", "long")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                    .startObject("lm_value_dv")
-                      .field("type", "long")
-                      .field("index", "no")
-                      .startObject("fielddata")
-                        .field("format", "doc_values")
-                      .endObject()
-                    .endObject()
-                  .endObject()
-                .endObject()
-              .endObject())).actionGet();
-
-            ObjectScatterSet<String> uniqueTerms = new ObjectScatterSet<>();
-            for (int i = 0; i < NUMBER_OF_TERMS; i++) {
-                boolean added;
-                do {
-                    added = uniqueTerms.add(RandomStrings.randomAsciiOfLength(random, STRING_TERM_SIZE));
-                } while (!added);
-            }
-            String[] sValues = uniqueTerms.toArray(String.class);
-            uniqueTerms = null;
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    final String sValue = sValues[ThreadLocalRandom.current().nextInt(sValues.length)];
-                    final long lValue = ThreadLocalRandom.current().nextInt(NUMBER_OF_TERMS);
-                    builder.field("s_value", sValue);
-                    builder.field("l_value", lValue);
-                    builder.field("s_value_dv", sValue);
-                    builder.field("l_value_dv", lValue);
-
-                    for (String field : new String[] {"sm_value", "sm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(sValues[ThreadLocalRandom.current().nextInt(sValues.length)]);
-                        }
-                        builder.endArray();
-                    }
-
-                    for (String field : new String[] {"lm_value", "lm_value_dv"}) {
-                        builder.startArray(field);
-                        for (int k = 0; k < NUMBER_OF_MULTI_VALUE_TERMS; k++) {
-                            builder.value(ThreadLocalRandom.current().nextInt(NUMBER_OF_TERMS));
-                        }
-                        builder.endArray();
-                    }
-
-                    builder.endObject();
-
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForYellowStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        COUNT = client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-
-        List<StatsResult> stats = new ArrayList<>();
-        stats.add(terms("terms_agg_s", Method.AGGREGATION, "s_value", null));
-        stats.add(terms("terms_agg_s_dv", Method.AGGREGATION, "s_value_dv", null));
-        stats.add(terms("terms_agg_map_s", Method.AGGREGATION, "s_value", "map"));
-        stats.add(terms("terms_agg_map_s_dv", Method.AGGREGATION, "s_value_dv", "map"));
-        stats.add(terms("terms_agg_def_s", Method.AGGREGATION_DEFERRED, "s_value", null));
-        stats.add(terms("terms_agg_def_s_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", null));
-        stats.add(terms("terms_agg_def_map_s", Method.AGGREGATION_DEFERRED, "s_value", "map"));
-        stats.add(terms("terms_agg_def_map_s_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "map"));
-        stats.add(terms("terms_agg_l", Method.AGGREGATION, "l_value", null));
-        stats.add(terms("terms_agg_l_dv", Method.AGGREGATION, "l_value_dv", null));
-        stats.add(terms("terms_agg_def_l", Method.AGGREGATION_DEFERRED, "l_value", null));
-        stats.add(terms("terms_agg_def_l_dv", Method.AGGREGATION_DEFERRED, "l_value_dv", null));
-        stats.add(terms("terms_agg_sm", Method.AGGREGATION, "sm_value", null));
-        stats.add(terms("terms_agg_sm_dv", Method.AGGREGATION, "sm_value_dv", null));
-        stats.add(terms("terms_agg_map_sm", Method.AGGREGATION, "sm_value", "map"));
-        stats.add(terms("terms_agg_map_sm_dv", Method.AGGREGATION, "sm_value_dv", "map"));
-        stats.add(terms("terms_agg_def_sm", Method.AGGREGATION_DEFERRED, "sm_value", null));
-        stats.add(terms("terms_agg_def_sm_dv", Method.AGGREGATION_DEFERRED, "sm_value_dv", null));
-        stats.add(terms("terms_agg_def_map_sm", Method.AGGREGATION_DEFERRED, "sm_value", "map"));
-        stats.add(terms("terms_agg_def_map_sm_dv", Method.AGGREGATION_DEFERRED, "sm_value_dv", "map"));
-        stats.add(terms("terms_agg_lm", Method.AGGREGATION, "lm_value", null));
-        stats.add(terms("terms_agg_lm_dv", Method.AGGREGATION, "lm_value_dv", null));
-        stats.add(terms("terms_agg_def_lm", Method.AGGREGATION_DEFERRED, "lm_value", null));
-        stats.add(terms("terms_agg_def_lm_dv", Method.AGGREGATION_DEFERRED, "lm_value_dv", null));
-
-        stats.add(termsStats("terms_stats_agg_s_l", Method.AGGREGATION, "s_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_s_l_dv", Method.AGGREGATION, "s_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_s_l", Method.AGGREGATION_DEFERRED, "s_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_def_s_l_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_s_lm", Method.AGGREGATION, "s_value", "lm_value", null));
-        stats.add(termsStats("terms_stats_agg_s_lm_dv", Method.AGGREGATION, "s_value_dv", "lm_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_s_lm", Method.AGGREGATION_DEFERRED, "s_value", "lm_value", null));
-        stats.add(termsStats("terms_stats_agg_def_s_lm_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "lm_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_sm_l", Method.AGGREGATION, "sm_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_sm_l_dv", Method.AGGREGATION, "sm_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_sm_l", Method.AGGREGATION_DEFERRED, "sm_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_def_sm_l_dv", Method.AGGREGATION_DEFERRED, "sm_value_dv", "l_value_dv", null));
-
-        stats.add(termsStats("terms_stats_agg_s_l", Method.AGGREGATION, "s_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_s_l_dv", Method.AGGREGATION, "s_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_s_l", Method.AGGREGATION_DEFERRED, "s_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_def_s_l_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_s_lm", Method.AGGREGATION, "s_value", "lm_value", null));
-        stats.add(termsStats("terms_stats_agg_s_lm_dv", Method.AGGREGATION, "s_value_dv", "lm_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_s_lm", Method.AGGREGATION_DEFERRED, "s_value", "lm_value", null));
-        stats.add(termsStats("terms_stats_agg_def_s_lm_dv", Method.AGGREGATION_DEFERRED, "s_value_dv", "lm_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_sm_l", Method.AGGREGATION, "sm_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_sm_l_dv", Method.AGGREGATION, "sm_value_dv", "l_value_dv", null));
-        stats.add(termsStats("terms_stats_agg_def_sm_l", Method.AGGREGATION_DEFERRED, "sm_value", "l_value", null));
-        stats.add(termsStats("terms_stats_agg_def_sm_l_dv", Method.AGGREGATION_DEFERRED, "sm_value_dv", "l_value_dv", null));
-
-        System.out.println("------------------ SUMMARY ----------------------------------------------");
-        System.out.format(Locale.ENGLISH, "%35s%10s%10s%15s\n", "name", "took", "millis", "fieldata size");
-        for (StatsResult stat : stats) {
-            System.out.format(Locale.ENGLISH, "%35s%10s%10d%15s\n", stat.name, TimeValue.timeValueMillis(stat.took), (stat.took / QUERY_COUNT), stat.fieldDataMemoryUsed);
-        }
-        System.out.println("------------------ SUMMARY ----------------------------------------------");
-
-        clientNode.close();
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    public static class StatsResult {
-        final String name;
-        final long took;
-        final ByteSizeValue fieldDataMemoryUsed;
-
-        public StatsResult(String name, long took, ByteSizeValue fieldDataMemoryUsed) {
-            this.name = name;
-            this.took = took;
-            this.fieldDataMemoryUsed = fieldDataMemoryUsed;
-        }
-    }
-
-    private static StatsResult terms(String name, Method method, String field, String executionHint) {
-        long totalQueryTime;// LM VALUE
-
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-        System.gc();
-
-        System.out.println("--> Warmup (" + name + ")...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = method.addTermsAgg(client.prepareSearch("test")
-                    .setSize(0)
-                    .setQuery(matchAllQuery()), name, field, executionHint)
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Loading (" + field + "): took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = method.addTermsAgg(client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(matchAllQuery()), name, field, executionHint)
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Terms Agg (" + name + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        String[] nodeIds = new String[nodes.length];
-        for (int i = 0; i < nodeIds.length; i++) {
-            nodeIds[i] = nodes[i].injector().getInstance(Discovery.class).localNode().getId();
-        }
-
-        ClusterStatsResponse clusterStateResponse = client.admin().cluster().prepareClusterStats().setNodesIds(nodeIds).get();
-        System.out.println("--> Heap used: " + clusterStateResponse.getNodesStats().getJvm().getHeapUsed());
-        ByteSizeValue fieldDataMemoryUsed = clusterStateResponse.getIndicesStats().getFieldData().getMemorySize();
-        System.out.println("--> Fielddata memory size: " + fieldDataMemoryUsed);
-
-        return new StatsResult(name, totalQueryTime, fieldDataMemoryUsed);
-    }
-
-    private static StatsResult termsStats(String name, Method method, String keyField, String valueField, String executionHint) {
-        long totalQueryTime;
-
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-        System.gc();
-
-        System.out.println("--> Warmup (" + name + ")...");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = method.addTermsStatsAgg(client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(matchAllQuery()), name, keyField, valueField)
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Loading (" + name + "): took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = method.addTermsStatsAgg(client.prepareSearch()
-                    .setSize(0)
-                    .setQuery(matchAllQuery()), name, keyField, valueField)
-                    .execute().actionGet();
-            if (searchResponse.getHits().totalHits() != COUNT) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Terms stats agg (" + name + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-        return new StatsResult(name, totalQueryTime, ByteSizeValue.parseBytesSizeValue("0b", "StatsResult"));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java
deleted file mode 100644
index 0e16b07..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/aggregations/TimeDataHistogramAggregationBenchmark.java
+++ /dev/null
@@ -1,262 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.aggregations;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.admin.indices.stats.CommonStatsFlags;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.concurrent.ThreadLocalRandom;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class TimeDataHistogramAggregationBenchmark {
-
-    static long COUNT = SizeValue.parseSizeValue("5m").singles();
-    static long TIME_PERIOD = 24 * 3600 * 1000;
-    static int BATCH = 100;
-    static int QUERY_WARMUP = 50;
-    static int QUERY_COUNT = 500;
-    static IndexFieldData.CommonSettings.MemoryStorageFormat MEMORY_FORMAT = IndexFieldData.CommonSettings.MemoryStorageFormat.PAGED;
-    static double ACCEPTABLE_OVERHEAD_RATIO = 0.5;
-    static float MATCH_PERCENTAGE = 0.1f;
-
-    static Client client;
-
-    public static void main(String[] args) throws Exception {
-
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put("node.local", true)
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = TimeDataHistogramAggregationBenchmark.class.getSimpleName();
-        Node[] nodes = new Node[1];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder().clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        client = nodes[0].client();
-
-        Thread.sleep(10000);
-        try {
-            client.admin().indices().create(createIndexRequest("test")).actionGet();
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + COUNT + "] ...");
-            long ITERS = COUNT / BATCH;
-            long i = 1;
-            int counter = 0;
-            long[] currentTimeInMillis1 = new long[]{System.currentTimeMillis()};
-            long[] currentTimeInMillis2 = new long[]{System.currentTimeMillis()};
-            long startTimeInMillis = currentTimeInMillis1[0];
-            long averageMillisChange = TIME_PERIOD / COUNT * 2;
-            long backwardSkew = Math.max(1, (long) (averageMillisChange * 0.1));
-            long bigOutOfOrder = 1;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-
-                    XContentBuilder builder = jsonBuilder().startObject();
-                    builder.field("id", Integer.toString(counter));
-                    // move forward in time and sometimes a little bit back (delayed delivery)
-                    long diff = ThreadLocalRandom.current().nextLong(2 * averageMillisChange + 2 * backwardSkew) - backwardSkew;
-                    long[] currentTime = counter % 2 == 0 ? currentTimeInMillis1 : currentTimeInMillis2;
-                    currentTime[0] += diff;
-                    if (ThreadLocalRandom.current().nextLong(100) <= bigOutOfOrder) {
-                        builder.field("l_value", currentTime[0] - 60000); // 1m delays
-                    } else {
-                        builder.field("l_value", currentTime[0]);
-                    }
-
-                    builder.endObject();
-
-                    request.add(Requests.indexRequest("test").type("type1").id(Integer.toString(counter))
-                            .source(builder));
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (COUNT)) / stopWatch.totalTime().secondsFrac()));
-            System.out.println("Time range 1: " + (currentTimeInMillis1[0] - startTimeInMillis) / 1000.0 / 3600 + " hours");
-            System.out.println("Time range 2: " + (currentTimeInMillis2[0] - startTimeInMillis) / 1000.0 / 3600 + " hours");
-            System.out.println("--> optimizing index");
-            client.admin().indices().prepareForceMerge().setMaxNumSegments(1).get();
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        COUNT = client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits();
-        System.out.println("--> Number of docs in index: " + COUNT);
-
-        // load with the reverse options to make sure jit doesn't optimize one away
-        setMapping(ACCEPTABLE_OVERHEAD_RATIO, MEMORY_FORMAT.equals(IndexFieldData.CommonSettings.MemoryStorageFormat.PACKED) ? IndexFieldData.CommonSettings.MemoryStorageFormat.PAGED : IndexFieldData.CommonSettings.MemoryStorageFormat.PACKED);
-        warmUp("hist_l", "l_value", MATCH_PERCENTAGE);
-
-        setMapping(ACCEPTABLE_OVERHEAD_RATIO, MEMORY_FORMAT);
-        warmUp("hist_l", "l_value", MATCH_PERCENTAGE);
-
-        List<StatsResult> stats = new ArrayList<>();
-        stats.add(measureAgg("hist_l", "l_value", MATCH_PERCENTAGE));
-
-        NodesStatsResponse nodeStats = client.admin().cluster().prepareNodesStats(nodes[0].settings().get("name")).clear()
-                .setIndices(new CommonStatsFlags(CommonStatsFlags.Flag.FieldData)).get();
-
-
-        System.out.println("------------------ SUMMARY -------------------------------");
-
-        System.out.println("docs: " + COUNT);
-        System.out.println("match percentage: " + MATCH_PERCENTAGE);
-        System.out.println("memory format hint: " + MEMORY_FORMAT);
-        System.out.println("acceptable_overhead_ratio: " + ACCEPTABLE_OVERHEAD_RATIO);
-        System.out.println("field data: " + nodeStats.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.format(Locale.ROOT, "%25s%10s%10s\n", "name", "took", "millis");
-        for (StatsResult stat : stats) {
-            System.out.format(Locale.ROOT, "%25s%10s%10d\n", stat.name, TimeValue.timeValueMillis(stat.took), (stat.took / QUERY_COUNT));
-        }
-        System.out.println("------------------ SUMMARY -------------------------------");
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-    protected static void setMapping(double acceptableOverheadRatio, IndexFieldData.CommonSettings.MemoryStorageFormat fielddataStorageFormat) throws IOException {
-        XContentBuilder mapping = JsonXContent.contentBuilder();
-        mapping.startObject().startObject("type1").startObject("properties").startObject("l_value")
-                .field("type", "long")
-                .startObject("fielddata")
-                .field("acceptable_transient_overhead_ratio", acceptableOverheadRatio)
-                .field("acceptable_overhead_ratio", acceptableOverheadRatio)
-                .field(IndexFieldData.CommonSettings.SETTING_MEMORY_STORAGE_HINT, fielddataStorageFormat.name().toLowerCase(Locale.ROOT))
-                .endObject()
-                .endObject().endObject().endObject().endObject();
-        client.admin().indices().preparePutMapping("test").setType("type1").setSource(mapping).get();
-    }
-
-    static class StatsResult {
-        final String name;
-        final long took;
-
-        StatsResult(String name, long took) {
-            this.name = name;
-            this.took = took;
-        }
-    }
-
-    private static SearchResponse doTermsAggsSearch(String name, String field, float matchPercentage) {
-        Map<String, Object> params = new HashMap<>();
-        params.put("matchP", matchPercentage);
-        SearchResponse response = client.prepareSearch()
-                .setSize(0)
-                .setQuery(
-                        QueryBuilders.constantScoreQuery(QueryBuilders.scriptQuery(new Script("random()<matchP", ScriptType.INLINE, null,
-                                params))))
-                .addAggregation(AggregationBuilders.histogram(name).field(field).interval(3600 * 1000)).get();
-
-        if (response.getHits().totalHits() < COUNT * matchPercentage * 0.7) {
-            System.err.println("--> warning - big deviation from expected count: " + response.getHits().totalHits() + " expected: " + COUNT * matchPercentage);
-        }
-
-        return response;
-    }
-
-    private static StatsResult measureAgg(String name, String field, float matchPercentage) {
-        long totalQueryTime;// LM VALUE
-
-        System.out.println("--> Running (" + name + ")...");
-        totalQueryTime = 0;
-        long previousCount = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = doTermsAggsSearch(name, field, matchPercentage);
-            if (previousCount == 0) {
-                previousCount = searchResponse.getHits().getTotalHits();
-            } else if (searchResponse.getHits().totalHits() != previousCount) {
-                System.err.println("*** HIT COUNT CHANGE -> CACHE EXPIRED? ***");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Histogram aggregations (" + field + "): " + (totalQueryTime / QUERY_COUNT) + "ms");
-        return new StatsResult(name, totalQueryTime);
-    }
-
-    private static void warmUp(String name, String field, float matchPercentage) {
-        System.out.println("--> Warmup (" + name + ")...");
-        client.admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();
-
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = doTermsAggsSearch(name, field, matchPercentage);
-            if (j == 0) {
-                System.out.println("--> Loading (" + field + "): took: " + searchResponse.getTook());
-            }
-        }
-        System.out.println("--> Warmup (" + name + ") DONE");
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java
deleted file mode 100644
index ffc7eb9..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java
+++ /dev/null
@@ -1,216 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.child;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-
-import java.util.Arrays;
-import java.util.Random;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ChildSearchAndIndexingBenchmark {
-
-    static int PARENT_COUNT = (int) SizeValue.parseSizeValue("1m").singles();
-    static int NUM_CHILDREN_PER_PARENT = 12;
-    static int QUERY_VALUE_RATIO_PER_PARENT = 3;
-    static int QUERY_COUNT = 50;
-    static String indexName = "test";
-    static Random random = new Random();
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = ChildSearchAndIndexingBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder().settings(settingsBuilder().put(settings).put("name", "node1"))
-                .clusterName(clusterName)
-                .node();
-        Client client = node1.client();
-
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        try {
-            client.admin().indices().create(createIndexRequest(indexName)).actionGet();
-            client.admin().indices().preparePutMapping(indexName).setType("child").setSource(XContentFactory.jsonBuilder().startObject().startObject("child")
-                    .startObject("_parent").field("type", "parent").endObject()
-                    .endObject().endObject()).execute().actionGet();
-            Thread.sleep(5000);
-
-            long startTime = System.currentTimeMillis();
-            ParentChildIndexGenerator generator = new ParentChildIndexGenerator(client, PARENT_COUNT, NUM_CHILDREN_PER_PARENT, QUERY_VALUE_RATIO_PER_PARENT);
-            generator.index();
-            System.out.println("--> Indexing took " + ((System.currentTimeMillis() - startTime) / 1000) + " seconds.");
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-        SearchThread searchThread = new SearchThread(client);
-        new Thread(searchThread).start();
-        IndexThread indexThread = new IndexThread(client);
-        new Thread(indexThread).start();
-
-        System.in.read();
-
-        indexThread.stop();
-        searchThread.stop();
-        client.close();
-        node1.close();
-    }
-
-    static class IndexThread implements Runnable {
-
-        private final Client client;
-        private volatile boolean run = true;
-
-        IndexThread(Client client) {
-            this.client = client;
-        }
-
-        @Override
-        public void run() {
-            while (run) {
-                int childIdLimit = PARENT_COUNT * NUM_CHILDREN_PER_PARENT;
-                for (int childId = 1; run && childId < childIdLimit;) {
-                    try {
-                        for (int j = 0; j < 8; j++) {
-                            GetResponse getResponse = client
-                                    .prepareGet(indexName, "child", String.valueOf(++childId))
-                                    .setFields("_source", "_parent")
-                                    .setRouting("1") // Doesn't matter what value, since there is only one shard
-                                    .get();
-                            client.prepareIndex(indexName, "child", Integer.toString(childId) + "_" + j)
-                                    .setParent(getResponse.getField("_parent").getValue().toString())
-                                    .setSource(getResponse.getSource())
-                                    .get();
-                        }
-                        client.admin().indices().prepareRefresh(indexName).execute().actionGet();
-                        Thread.sleep(1000);
-                        if (childId % 500 == 0) {
-                            NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                                    .clear().setIndices(true).execute().actionGet();
-                            System.out.println("Deleted docs: " + statsResponse.getAt(0).getIndices().getDocs().getDeleted());
-                        }
-                    } catch (Throwable e) {
-                        e.printStackTrace();
-                    }
-                }
-            }
-        }
-
-        public void stop() {
-            run = false;
-        }
-
-    }
-
-    static class SearchThread implements Runnable {
-
-        private final Client client;
-        private final int numValues;
-        private volatile boolean run = true;
-
-        SearchThread(Client client) {
-            this.client = client;
-            this.numValues = NUM_CHILDREN_PER_PARENT / NUM_CHILDREN_PER_PARENT;
-        }
-
-        @Override
-        public void run() {
-            while (run) {
-                try {
-                    long totalQueryTime = 0;
-                    for (int j = 0; j < QUERY_COUNT; j++) {
-                        SearchResponse searchResponse = client.prepareSearch(indexName)
-                                .setQuery(
-                                        boolQuery()
-                                        .must(matchAllQuery())
-                                        .filter(hasChildQuery("child", termQuery("field2", "value" + random.nextInt(numValues)))
-                                        )
-                                )
-                                .execute().actionGet();
-                        if (searchResponse.getFailedShards() > 0) {
-                            System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-                        }
-                        totalQueryTime += searchResponse.getTookInMillis();
-                    }
-                    System.out.println("--> has_child filter with term filter Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-                    totalQueryTime = 0;
-                    for (int j = 1; j <= QUERY_COUNT; j++) {
-                        SearchResponse searchResponse = client.prepareSearch(indexName)
-                                .setQuery(
-                                        boolQuery()
-                                        .must(matchAllQuery())
-                                        .filter(hasChildQuery("child", matchAllQuery()))
-                                )
-                                .execute().actionGet();
-                        if (searchResponse.getFailedShards() > 0) {
-                            System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-                        }
-                        totalQueryTime += searchResponse.getTookInMillis();
-                    }
-                    System.out.println("--> has_child filter with match_all child query, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-                    NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                            .setJvm(true).execute().actionGet();
-                    System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-                    System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-                    Thread.sleep(1000);
-                } catch (Throwable e) {
-                    e.printStackTrace();
-                }
-            }
-        }
-
-        public void stop() {
-            run = false;
-        }
-
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java
deleted file mode 100644
index 8889801..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java
+++ /dev/null
@@ -1,344 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.child;
-
-import org.apache.lucene.search.join.ScoreMode;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.bucket.children.Children;
-
-import java.util.Arrays;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.*;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ChildSearchBenchmark {
-
-    /*
-        Run: MAVEN_OPTS=-Xmx4g mvn test-compile exec:java -Dexec.mainClass="org.elasticsearch.benchmark.search.child.ChildSearchBenchmark" -Dexec.classpathScope="test" -Dexec.args="bwc false"
-     */
-
-    public static void main(String[] args) throws Exception {
-        boolean bwcMode = false;
-        int numParents = (int) SizeValue.parseSizeValue("2m").singles();;
-
-        if (args.length % 2 != 0) {
-            throw new IllegalArgumentException("Uneven number of arguments");
-        }
-        for (int i = 0; i < args.length; i += 2) {
-            String value = args[i + 1];
-            if ("--bwc_mode".equals(args[i])) {
-                bwcMode = Boolean.valueOf(value);
-            } else if ("--num_parents".equals(args[i])) {
-                numParents = Integer.valueOf(value);
-            }
-        }
-
-
-        Settings.Builder settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0);
-
-        // enable bwc parent child mode:
-        if (bwcMode) {
-            settings.put("tests.mock.version", Version.V_1_6_0);
-        }
-
-        String clusterName = ChildSearchBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings.build()).put("name", "node1")).node();
-        Client client = node1.client();
-
-        int CHILD_COUNT = 15;
-        int QUERY_VALUE_RATIO = 3;
-        int QUERY_WARMUP = 10;
-        int QUERY_COUNT = 20;
-        String indexName = "test";
-
-        ParentChildIndexGenerator parentChildIndexGenerator = new ParentChildIndexGenerator(client, numParents, CHILD_COUNT, QUERY_VALUE_RATIO);
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        try {
-            client.admin().indices().create(createIndexRequest(indexName)).actionGet();
-            client.admin().indices().preparePutMapping(indexName).setType("child").setSource(XContentFactory.jsonBuilder().startObject().startObject("child")
-                    .startObject("_parent").field("type", "parent").endObject()
-                    .endObject().endObject()).execute().actionGet();
-            Thread.sleep(5000);
-            long startTime = System.currentTimeMillis();
-            parentChildIndexGenerator.index();
-            System.out.println("--> Indexing took " + ((System.currentTimeMillis() - startTime) / 1000) + " seconds.");
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch(indexName).setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-        System.out.println("--> Running just child query");
-        // run just the child query, warm up first
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            client.prepareSearch(indexName).setQuery(termQuery("child.tag", "tag1")).execute().actionGet();
-        }
-
-        long totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(termQuery("child.tag", "tag1")).execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Just Child Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).execute().actionGet();
-        System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-        
-        // run parent child constant query
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-        }
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_child filter Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        System.out.println("--> Running has_child filter with match_all child query");
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasChildQuery("child", matchAllQuery()))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_child filter with match_all child query, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-
-        System.out.println("--> Running children agg");
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(matchQuery("field1", parentChildIndexGenerator.getQueryValue()))
-                    .addAggregation(
-                            AggregationBuilders.children("to-child").childType("child")
-                    )
-                    .execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            Children children = searchResponse.getAggregations().get("to-child");
-            if (j % 10 == 0) {
-                System.out.println("--> children doc count [" + j + "], got [" + children.getDocCount() + "]");
-            }
-        }
-        System.out.println("--> children agg, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        System.out.println("--> Running children agg with match_all");
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .addAggregation(
-                            AggregationBuilders.children("to-child").childType("child")
-                    )
-                    .execute().actionGet();
-            totalQueryTime += searchResponse.getTookInMillis();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            Children children = searchResponse.getAggregations().get("to-child");
-            if (j % 10 == 0) {
-                System.out.println("--> children doc count [" + j + "], got [" + children.getDocCount() + "]");
-            }
-        }
-        System.out.println("--> children agg, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        // run parent child constant query
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-        }
-
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_parent filter Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        System.out.println("--> Running has_parent filter with match_all parent query ");
-        totalQueryTime = 0;
-        for (int j = 1; j <= QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            boolQuery()
-                            .must(matchAllQuery())
-                            .filter(hasParentQuery("parent", matchAllQuery()))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_parent filter with match_all parent query, Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).setIndices(true).execute().actionGet();
-
-        System.out.println("--> Field data size: " + statsResponse.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        System.out.println("--> Running has_child query with score type");
-        // run parent child score query
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            client.prepareSearch(indexName).setQuery(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())).scoreMode(ScoreMode.Max)).execute().actionGet();
-        }
-
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasChildQuery("child", termQuery("field2", parentChildIndexGenerator.getQueryValue())).scoreMode(ScoreMode.Max)).execute().actionGet();
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_child Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-        
-        totalQueryTime = 0;
-        for (int j = 0; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasChildQuery("child", matchAllQuery()).scoreMode(ScoreMode.Max)).execute().actionGet();
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_child query with match_all Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-        
-        System.out.println("--> Running has_parent query with score type");
-        // run parent child score query
-        for (int j = 0; j < QUERY_WARMUP; j++) {
-            client.prepareSearch(indexName).setQuery(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())).score(true)).execute().actionGet();
-        }
-
-        totalQueryTime = 0;
-        for (int j = 1; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasParentQuery("parent", termQuery("field1", parentChildIndexGenerator.getQueryValue())).score(true)).execute().actionGet();
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_parent Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        totalQueryTime = 0;
-        for (int j = 1; j < QUERY_COUNT; j++) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(hasParentQuery("parent", matchAllQuery()).score(true)).execute().actionGet();
-            if (j % 10 == 0) {
-                System.out.println("--> hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "]");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> has_parent query with match_all Query Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-
-        System.gc();
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).setIndices(true).execute().actionGet();
-
-        System.out.println("--> Field data size: " + statsResponse.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        client.close();
-        node1.close();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java
deleted file mode 100644
index 0db0303..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java
+++ /dev/null
@@ -1,210 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.search.child;
-
-import org.apache.lucene.search.join.ScoreMode;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import static org.elasticsearch.client.Requests.createIndexRequest;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- *
- */
-public class ChildSearchShortCircuitBenchmark {
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        String clusterName = ChildSearchShortCircuitBenchmark.class.getSimpleName();
-        Node node1 = nodeBuilder().clusterName(clusterName)
-                .settings(settingsBuilder().put(settings).put("name", "node1"))
-                .node();
-        Client client = node1.client();
-
-        long PARENT_COUNT = SizeValue.parseSizeValue("10M").singles();
-        int BATCH = 100;
-        int QUERY_WARMUP = 5;
-        int QUERY_COUNT = 25;
-        String indexName = "test";
-
-        client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10s").execute().actionGet();
-        try {
-            client.admin().indices().create(createIndexRequest(indexName)).actionGet();
-            client.admin().indices().preparePutMapping(indexName).setType("child").setSource(XContentFactory.jsonBuilder().startObject().startObject("child")
-                    .startObject("_parent").field("type", "parent").endObject()
-                    .endObject().endObject()).execute().actionGet();
-            Thread.sleep(5000);
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + PARENT_COUNT + "] parent document and some child documents");
-            long ITERS = PARENT_COUNT / BATCH;
-            int i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < BATCH; j++) {
-                    counter++;
-                    request.add(Requests.indexRequest(indexName).type("parent").id(Integer.toString(counter))
-                            .source(parentSource(counter)));
-
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * BATCH) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * BATCH) + "parent docs; took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-
-            int id = 0;
-            for (i = 1; i <= PARENT_COUNT; i *= 2) {
-                int parentId = 1;
-                for (int j = 0; j < i; j++) {
-                    client.prepareIndex(indexName, "child", Integer.toString(id++))
-                            .setParent(Integer.toString(parentId++))
-                            .setSource(childSource(i))
-                            .execute().actionGet();
-                }
-            }
-
-            System.out.println("--> Indexing took " + stopWatch.totalTime());
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch(indexName).setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-        System.out.println("--> Running just child query");
-        // run just the child query, warm up first
-        for (int i = 1; i <= 10000; i *= 2) {
-            SearchResponse searchResponse = client.prepareSearch(indexName).setQuery(matchQuery("child.field2", i)).execute().actionGet();
-            System.out.println("--> Warmup took["+ i +"]: " + searchResponse.getTook());
-            if (searchResponse.getHits().totalHits() != i) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-
-        NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).execute().actionGet();
-        System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        // run parent child constant query
-        for (int j = 1; j < QUERY_WARMUP; j *= 2) {
-            SearchResponse searchResponse = client.prepareSearch(indexName)
-                    .setQuery(
-                            hasChildQuery("child", matchQuery("field2", j))
-                    )
-                    .execute().actionGet();
-            if (searchResponse.getFailedShards() > 0) {
-                System.err.println("Search Failures " + Arrays.toString(searchResponse.getShardFailures()));
-            }
-            if (searchResponse.getHits().totalHits() != j) {
-                System.err.println("--> mismatch on hits [" + j + "], got [" + searchResponse.getHits().totalHits() + "], expected [" + PARENT_COUNT + "]");
-            }
-        }
-
-        long totalQueryTime = 0;
-        for (int i = 1; i < PARENT_COUNT; i *= 2) {
-            for (int j = 0; j < QUERY_COUNT; j++) {
-                SearchResponse searchResponse = client.prepareSearch(indexName)
-                        .setQuery(boolQuery().must(matchAllQuery()).filter(hasChildQuery("child", matchQuery("field2", i))))
-                        .execute().actionGet();
-                if (searchResponse.getHits().totalHits() != i) {
-                    System.err.println("--> mismatch on hits");
-                }
-                totalQueryTime += searchResponse.getTookInMillis();
-            }
-            System.out.println("--> has_child filter " + i +" Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-        }
-
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).setIndices(true).execute().actionGet();
-
-        System.out.println("--> Field data size: " + statsResponse.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        totalQueryTime = 0;
-        for (int i = 1; i < PARENT_COUNT; i *= 2) {
-            for (int j = 0; j < QUERY_COUNT; j++) {
-                SearchResponse searchResponse = client.prepareSearch(indexName)
-                        .setQuery(hasChildQuery("child", matchQuery("field2", i)).scoreMode(ScoreMode.Max))
-                        .execute().actionGet();
-                if (searchResponse.getHits().totalHits() != i) {
-                    System.err.println("--> mismatch on hits");
-                }
-                totalQueryTime += searchResponse.getTookInMillis();
-            }
-            System.out.println("--> has_child query " + i +" Avg: " + (totalQueryTime / QUERY_COUNT) + "ms");
-        }
-
-        System.gc();
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).setIndices(true).execute().actionGet();
-
-        System.out.println("--> Field data size: " + statsResponse.getNodes()[0].getIndices().getFieldData().getMemorySize());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        client.close();
-        node1.close();
-    }
-
-    private static XContentBuilder parentSource(int val) throws IOException {
-        return jsonBuilder().startObject().field("field1", Integer.toString(val)).endObject();
-    }
-
-    private static XContentBuilder childSource(int val) throws IOException {
-        return jsonBuilder().startObject().field("field2", Integer.toString(val)).endObject();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/child/ParentChildIndexGenerator.java b/core/src/test/java/org/elasticsearch/benchmark/search/child/ParentChildIndexGenerator.java
deleted file mode 100644
index 1d02a1f..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/child/ParentChildIndexGenerator.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.child;
-
-import com.carrotsearch.hppc.ObjectArrayList;
-import com.carrotsearch.hppc.ObjectHashSet;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-
-import java.util.Random;
-
-/**
- */
-public class ParentChildIndexGenerator {
-
-    private final static Random RANDOM = new Random();
-
-    private final Client client;
-    private final int numParents;
-    private final int numChildrenPerParent;
-    private final int queryValueRatio;
-
-    public ParentChildIndexGenerator(Client client, int numParents, int numChildrenPerParent, int queryValueRatio) {
-        this.client = client;
-        this.numParents = numParents;
-        this.numChildrenPerParent = numChildrenPerParent;
-        this.queryValueRatio = queryValueRatio;
-    }
-
-    public void index() {
-        // Memory intensive...
-        ObjectHashSet<String> usedParentIds = new ObjectHashSet<>(numParents, 0.5d);
-        ObjectArrayList<ParentDocument> parents = new ObjectArrayList<>(numParents);
-
-        for (int i = 0; i < numParents; i++) {
-            String parentId;
-            do {
-                parentId = RandomStrings.randomAsciiOfLength(RANDOM, 10);
-            } while (!usedParentIds.add(parentId));
-            String[] queryValues = new String[numChildrenPerParent];
-            for (int j = 0; j < numChildrenPerParent; j++) {
-                queryValues[j] = getQueryValue();
-            }
-            parents.add(new ParentDocument(parentId, queryValues));
-        }
-
-        int indexCounter = 0;
-        int childIdCounter = 0;
-        while (!parents.isEmpty()) {
-            BulkRequestBuilder request = client.prepareBulk();
-            for (int i = 0; !parents.isEmpty() && i < 100; i++) {
-                int index = RANDOM.nextInt(parents.size());
-                ParentDocument parentDocument = parents.get(index);
-
-                if (parentDocument.indexCounter == -1) {
-                    request.add(Requests.indexRequest("test").type("parent")
-                            .id(parentDocument.parentId)
-                            .source("field1", getQueryValue()));
-                } else {
-                    request.add(Requests.indexRequest("test").type("child")
-                            .parent(parentDocument.parentId)
-                            .id(String.valueOf(++childIdCounter))
-                            .source("field2", parentDocument.queryValues[parentDocument.indexCounter]));
-                }
-
-                if (++parentDocument.indexCounter == parentDocument.queryValues.length) {
-                    parents.remove(index);
-                }
-            }
-
-            BulkResponse response = request.execute().actionGet();
-            if (response.hasFailures()) {
-                System.err.println("--> failures...");
-            }
-
-            indexCounter += response.getItems().length;
-            if (indexCounter % 100000 == 0) {
-                System.out.println("--> Indexed " + indexCounter + " documents");
-            }
-        }
-    }
-
-    public String getQueryValue() {
-        return "value" + RANDOM.nextInt(numChildrenPerParent / queryValueRatio);
-    }
-
-    class ParentDocument {
-
-        final String parentId;
-        final String[] queryValues;
-        int indexCounter;
-
-        ParentDocument(String parentId, String[] queryValues) {
-            this.parentId = parentId;
-            this.queryValues = queryValues;
-            this.indexCounter = -1;
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java
deleted file mode 100644
index 55c2918..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java
+++ /dev/null
@@ -1,207 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.geo;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.SuppressForbidden;
-import org.elasticsearch.common.geo.GeoDistance;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.geoDistanceQuery;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-
-/**
- */
-public class GeoDistanceSearchBenchmark {
-
-    public static void main(String[] args) throws Exception {
-
-        Node node = NodeBuilder.nodeBuilder().clusterName(GeoDistanceSearchBenchmark.class.getSimpleName()).node();
-        Client client = node.client();
-
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("Failed to wait for green status, bailing");
-            exit(1);
-        }
-
-        final long NUM_DOCS = SizeValue.parseSizeValue("1m").singles();
-        final long NUM_WARM = 50;
-        final long NUM_RUNS = 100;
-
-        if (client.admin().indices().prepareExists("test").execute().actionGet().isExists()) {
-            System.out.println("Found an index, count: " + client.prepareSearch("test").setSize(0).setQuery(QueryBuilders.matchAllQuery()).execute().actionGet().getHits().totalHits());
-        } else {
-            String mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                    .startObject("properties").startObject("location").field("type", "geo_point").field("lat_lon", true).endObject().endObject()
-                    .endObject().endObject().string();
-            client.admin().indices().prepareCreate("test")
-                    .setSettings(Settings.settingsBuilder().put("index.number_of_shards", 1).put("index.number_of_replicas", 0))
-                    .addMapping("type1", mapping)
-                    .execute().actionGet();
-
-            System.err.println("--> Indexing [" + NUM_DOCS + "]");
-            for (long i = 0; i < NUM_DOCS; ) {
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "New York")
-                        .startObject("location").field("lat", 40.7143528).field("lon", -74.0059731).endObject()
-                        .endObject()).execute().actionGet();
-
-                // to NY: 5.286 km
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "Times Square")
-                        .startObject("location").field("lat", 40.759011).field("lon", -73.9844722).endObject()
-                        .endObject()).execute().actionGet();
-
-                // to NY: 0.4621 km
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "Tribeca")
-                        .startObject("location").field("lat", 40.718266).field("lon", -74.007819).endObject()
-                        .endObject()).execute().actionGet();
-
-                // to NY: 1.258 km
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "Soho")
-                        .startObject("location").field("lat", 40.7247222).field("lon", -74).endObject()
-                        .endObject()).execute().actionGet();
-
-                // to NY: 8.572 km
-                client.prepareIndex("test", "type1", Long.toString(i++)).setSource(jsonBuilder().startObject()
-                        .field("name", "Brooklyn")
-                        .startObject("location").field("lat", 40.65).field("lon", -73.95).endObject()
-                        .endObject()).execute().actionGet();
-
-                if ((i % 10000) == 0) {
-                    System.err.println("--> indexed " + i);
-                }
-            }
-            System.err.println("Done indexed");
-            client.admin().indices().prepareFlush("test").execute().actionGet();
-            client.admin().indices().prepareRefresh().execute().actionGet();
-        }
-
-        System.err.println("--> Warming up (ARC) - optimize_bbox");
-        long start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.ARC, "memory");
-        }
-        long totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (ARC)  - optimize_bbox (memory) " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (ARC) - optimize_bbox (memory)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.ARC, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (ARC) - optimize_bbox " + (totalTime / NUM_RUNS) + "ms");
-
-        System.err.println("--> Warming up (ARC)  - optimize_bbox (indexed)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.ARC, "indexed");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (ARC) - optimize_bbox (indexed) " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (ARC) - optimize_bbox (indexed)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.ARC, "indexed");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (ARC) - optimize_bbox (indexed) " + (totalTime / NUM_RUNS) + "ms");
-
-
-        System.err.println("--> Warming up (ARC)  - no optimize_bbox");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.ARC, "none");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (ARC) - no optimize_bbox " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (ARC) - no optimize_bbox");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.ARC, "none");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (ARC) - no optimize_bbox " + (totalTime / NUM_RUNS) + "ms");
-
-        System.err.println("--> Warming up (SLOPPY_ARC)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.SLOPPY_ARC, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (SLOPPY_ARC) " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (SLOPPY_ARC)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.SLOPPY_ARC, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (SLOPPY_ARC) " + (totalTime / NUM_RUNS) + "ms");
-
-        System.err.println("--> Warming up (PLANE)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_WARM; i++) {
-            run(client, GeoDistance.PLANE, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Warmup (PLANE) " + (totalTime / NUM_WARM) + "ms");
-
-        System.err.println("--> Perf (PLANE)");
-        start = System.currentTimeMillis();
-        for (int i = 0; i < NUM_RUNS; i++) {
-            run(client, GeoDistance.PLANE, "memory");
-        }
-        totalTime = System.currentTimeMillis() - start;
-        System.err.println("--> Perf (PLANE) " + (totalTime / NUM_RUNS) + "ms");
-
-        node.close();
-    }
-
-    public static void run(Client client, GeoDistance geoDistance, String optimizeBbox) {
-        client.prepareSearch() // from NY
-                .setSize(0)
-                .setQuery(boolQuery().must(matchAllQuery()).filter(geoDistanceQuery("location")
-                        .distance("2km")
-                        .optimizeBbox(optimizeBbox)
-                        .geoDistance(geoDistance)
-                        .point(40.7143528, -74.0059731)))
-                .execute().actionGet();
-    }
-
-    @SuppressForbidden(reason = "Allowed to exit explicitly from #main()")
-    private static void exit(int status) {
-        System.exit(status);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/nested/NestedSearchBenchMark.java b/core/src/test/java/org/elasticsearch/benchmark/search/nested/NestedSearchBenchMark.java
deleted file mode 100644
index 1aa3310..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/nested/NestedSearchBenchMark.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.nested;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.bulk.BulkResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- */
-public class NestedSearchBenchMark {
-
-    public static void main(String[] args) throws Exception {
-        Settings settings = settingsBuilder()
-                .put("index.refresh_interval", "-1")
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node node1 = nodeBuilder()
-                .settings(settingsBuilder().put(settings).put("name", "node1"))
-                .node();
-        Client client = node1.client();
-
-        int count = (int) SizeValue.parseSizeValue("1m").singles();
-        int nestedCount = 10;
-        int rootDocs = count / nestedCount;
-        int batch = 100;
-        int queryWarmup = 5;
-        int queryCount = 500;
-        String indexName = "test";
-        ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth()
-                .setWaitForGreenStatus().execute().actionGet();
-        if (clusterHealthResponse.isTimedOut()) {
-            System.err.println("--> Timed out waiting for cluster health");
-        }
-
-        try {
-            client.admin().indices().prepareCreate(indexName)
-                    .addMapping("type", XContentFactory.jsonBuilder()
-                            .startObject()
-                            .startObject("type")
-                            .startObject("properties")
-                            .startObject("field1")
-                            .field("type", "integer")
-                            .endObject()
-                            .startObject("field2")
-                            .field("type", "nested")
-                            .startObject("properties")
-                            .startObject("field3")
-                            .field("type", "integer")
-                            .endObject()
-                            .endObject()
-                            .endObject()
-                            .endObject()
-                            .endObject()
-                            .endObject()
-                    ).execute().actionGet();
-            clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-
-            StopWatch stopWatch = new StopWatch().start();
-
-            System.out.println("--> Indexing [" + rootDocs + "] root documents and [" + (rootDocs * nestedCount) + "] nested objects");
-            long ITERS = rootDocs / batch;
-            long i = 1;
-            int counter = 0;
-            for (; i <= ITERS; i++) {
-                BulkRequestBuilder request = client.prepareBulk();
-                for (int j = 0; j < batch; j++) {
-                    counter++;
-                    XContentBuilder doc = XContentFactory.jsonBuilder().startObject()
-                            .field("field1", counter)
-                            .startArray("field2");
-                    for (int k = 0; k < nestedCount; k++) {
-                        doc = doc.startObject()
-                                .field("field3", k)
-                                .endObject();
-                    }
-                    doc = doc.endArray();
-                    request.add(
-                            Requests.indexRequest(indexName).type("type").id(Integer.toString(counter)).source(doc)
-                    );
-                }
-                BulkResponse response = request.execute().actionGet();
-                if (response.hasFailures()) {
-                    System.err.println("--> failures...");
-                }
-                if (((i * batch) % 10000) == 0) {
-                    System.out.println("--> Indexed " + (i * batch) + " took " + stopWatch.stop().lastTaskTime());
-                    stopWatch.start();
-                }
-            }
-            System.out.println("--> Indexing took " + stopWatch.totalTime() + ", TPS " + (((double) (count * (1 + nestedCount))) / stopWatch.totalTime().secondsFrac()));
-        } catch (Exception e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-        client.admin().indices().prepareRefresh().execute().actionGet();
-        System.out.println("--> Number of docs in index: " + client.prepareSearch().setSize(0).setQuery(matchAllQuery()).execute().actionGet().getHits().totalHits());
-
-        NodesStatsResponse statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).execute().actionGet();
-        System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-
-        System.out.println("--> Running match_all with sorting on nested field");
-        // run just the child query, warm up first
-        for (int j = 0; j < queryWarmup; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addSort(
-                            SortBuilders.fieldSort("field2.field3")
-                                    .setNestedPath("field2")
-                                    .sortMode("avg")
-                                    .order(SortOrder.ASC)
-                    )
-                    .execute().actionGet();
-            if (j == 0) {
-                System.out.println("--> Warmup took: " + searchResponse.getTook());
-            }
-            if (searchResponse.getHits().totalHits() != rootDocs) {
-                System.err.println("--> mismatch on hits");
-            }
-        }
-
-        long totalQueryTime = 0;
-        for (int j = 0; j < queryCount; j++) {
-            SearchResponse searchResponse = client.prepareSearch()
-                    .setQuery(matchAllQuery())
-                    .addSort(
-                            SortBuilders.fieldSort("field2.field3")
-                                    .setNestedPath("field2")
-                                    .sortMode("avg")
-                                    .order(j % 2 == 0 ? SortOrder.ASC : SortOrder.DESC)
-                    )
-                    .execute().actionGet();
-
-            if (searchResponse.getHits().totalHits() != rootDocs) {
-                System.err.println("--> mismatch on hits");
-            }
-            totalQueryTime += searchResponse.getTookInMillis();
-        }
-        System.out.println("--> Sorting by nested fields took: " + (totalQueryTime / queryCount) + "ms");
-
-        statsResponse = client.admin().cluster().prepareNodesStats()
-                .setJvm(true).execute().actionGet();
-        System.out.println("--> Committed heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapCommitted());
-        System.out.println("--> Used heap size: " + statsResponse.getNodes()[0].getJvm().getMem().getHeapUsed());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/scroll/ScrollSearchBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/search/scroll/ScrollSearchBenchmark.java
deleted file mode 100644
index 363facc..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/search/scroll/ScrollSearchBenchmark.java
+++ /dev/null
@@ -1,157 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.search.scroll;
-
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.bulk.BulkRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.SizeValue;
-import org.elasticsearch.indices.IndexAlreadyExistsException;
-import org.elasticsearch.monitor.jvm.JvmStats;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.sort.SortOrder;
-
-import java.util.Locale;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
-
-/**
- */
-public class ScrollSearchBenchmark {
-
-    // Run with: -Xms1G -Xms1G
-    public static void main(String[] args) {
-        String indexName = "test";
-        String typeName = "type";
-        String clusterName = ScrollSearchBenchmark.class.getSimpleName();
-        long numDocs = SizeValue.parseSizeValue("300k").singles();
-        int requestSize = 50;
-
-        Settings settings = settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 3)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .build();
-
-        Node[] nodes = new Node[3];
-        for (int i = 0; i < nodes.length; i++) {
-            nodes[i] = nodeBuilder()
-                    .clusterName(clusterName)
-                    .settings(settingsBuilder().put(settings).put("name", "node" + i))
-                    .node();
-        }
-
-        Client client = nodes[0].client();
-
-        try {
-            client.admin().indices().prepareCreate(indexName).get();
-            for (int counter = 1; counter <= numDocs;) {
-                BulkRequestBuilder bulkRequestBuilder = client.prepareBulk();
-                for (int bulkCounter = 0; bulkCounter < 100; bulkCounter++) {
-                    if (counter > numDocs) {
-                        break;
-                    }
-                    bulkRequestBuilder.add(
-                            client.prepareIndex(indexName, typeName, String.valueOf(counter))
-                                    .setSource("field1", counter++)
-                    );
-                }
-                int indexedDocs = counter - 1;
-                if (indexedDocs % 100000 == 0) {
-                     System.out.printf(Locale.ENGLISH, "--> Indexed %d so far\n", indexedDocs);
-                }
-                bulkRequestBuilder.get();
-            }
-        } catch (IndexAlreadyExistsException e) {
-            System.out.println("--> Index already exists, ignoring indexing phase, waiting for green");
-            ClusterHealthResponse clusterHealthResponse = client.admin().cluster().prepareHealth(indexName).setWaitForGreenStatus().setTimeout("10m").execute().actionGet();
-            if (clusterHealthResponse.isTimedOut()) {
-                System.err.println("--> Timed out waiting for cluster health");
-            }
-        }
-
-        client.admin().indices().prepareRefresh(indexName).get();
-        System.out.printf(Locale.ENGLISH, "--> Number of docs in index: %d\n", client.prepareSearch().setSize(0).get().getHits().totalHits());
-
-        Long counter = numDocs;
-        SearchResponse searchResponse = client.prepareSearch(indexName)
-                .addSort("field1", SortOrder.DESC)
-                .setSize(requestSize)
-                .setScroll("10m").get();
-
-        if (searchResponse.getHits().getTotalHits() != numDocs) {
-            System.err.printf(Locale.ENGLISH, "Expected total hits [%d] but got [%d]\n", numDocs, searchResponse.getHits().getTotalHits());
-        }
-
-        if (searchResponse.getHits().hits().length != requestSize) {
-            System.err.printf(Locale.ENGLISH, "Expected hits length [%d] but got [%d]\n", requestSize, searchResponse.getHits().hits().length);
-        }
-
-        for (SearchHit hit : searchResponse.getHits()) {
-            if (!hit.sortValues()[0].equals(counter--)) {
-                System.err.printf(Locale.ENGLISH, "Expected sort value [%d] but got [%s]\n", counter + 1, hit.sortValues()[0]);
-            }
-        }
-        String scrollId = searchResponse.getScrollId();
-        int scrollRequestCounter = 0;
-        long sumTimeSpent = 0;
-        while (true) {
-            long timeSpent = System.currentTimeMillis();
-            searchResponse = client.prepareSearchScroll(scrollId).setScroll("10m").get();
-            sumTimeSpent += (System.currentTimeMillis() - timeSpent);
-            scrollRequestCounter++;
-            if (searchResponse.getHits().getTotalHits() != numDocs) {
-                System.err.printf(Locale.ENGLISH, "Expected total hits [%d] but got [%d]\n", numDocs, searchResponse.getHits().getTotalHits());
-            }
-            if (scrollRequestCounter % 20 == 0) {
-                long avgTimeSpent = sumTimeSpent / 20;
-                JvmStats.Mem mem = JvmStats.jvmStats().getMem();
-                System.out.printf(Locale.ENGLISH, "Cursor location=%d, avg time spent=%d ms\n", (requestSize * scrollRequestCounter), (avgTimeSpent));
-                System.out.printf(Locale.ENGLISH, "heap max=%s, used=%s, percentage=%d\n", mem.getHeapMax(), mem.getHeapUsed(), mem.getHeapUsedPercent());
-                sumTimeSpent = 0;
-            }
-            if (searchResponse.getHits().hits().length == 0) {
-                break;
-            }
-            if (searchResponse.getHits().hits().length != requestSize) {
-                System.err.printf(Locale.ENGLISH, "Expected hits length [%d] but got [%d]\n", requestSize, searchResponse.getHits().hits().length);
-            }
-            for (SearchHit hit : searchResponse.getHits()) {
-                if (!hit.sortValues()[0].equals(counter--)) {
-                    System.err.printf(Locale.ENGLISH, "Expected sort value [%d] but got [%s]\n", counter + 1, hit.sortValues()[0]);
-                }
-            }
-            scrollId = searchResponse.getScrollId();
-        }
-        if (counter != 0) {
-            System.err.printf(Locale.ENGLISH, "Counter should be 0 because scroll has been consumed\n");
-        }
-
-        for (Node node : nodes) {
-            node.close();
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/time/SimpleTimeBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/time/SimpleTimeBenchmark.java
deleted file mode 100644
index 37b20bc..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/time/SimpleTimeBenchmark.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.benchmark.time;
-
-import org.elasticsearch.common.StopWatch;
-
-import java.util.concurrent.CountDownLatch;
-
-/**
- *
- */
-public class SimpleTimeBenchmark {
-
-    private static boolean USE_NANO_TIME = false;
-    private static long NUMBER_OF_ITERATIONS = 1000000;
-    private static int NUMBER_OF_THREADS = 100;
-
-    public static void main(String[] args) throws Exception {
-        StopWatch stopWatch = new StopWatch().start();
-        System.out.println("Running " + NUMBER_OF_ITERATIONS);
-        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-            System.currentTimeMillis();
-        }
-        System.out.println("Took " + stopWatch.stop().totalTime() + " TP Millis " + (NUMBER_OF_ITERATIONS / stopWatch.totalTime().millisFrac()));
-
-        System.out.println("Running using " + NUMBER_OF_THREADS + " threads with " + NUMBER_OF_ITERATIONS + " iterations");
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    if (USE_NANO_TIME) {
-                        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                            System.nanoTime();
-                        }
-                    } else {
-                        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                            System.currentTimeMillis();
-                        }
-                    }
-                    latch.countDown();
-                }
-            });
-        }
-        stopWatch = new StopWatch().start();
-        for (Thread thread : threads) {
-            thread.start();
-        }
-        latch.await();
-        stopWatch.stop();
-        System.out.println("Took " + stopWatch.totalTime() + " TP Millis " + ((NUMBER_OF_ITERATIONS * NUMBER_OF_THREADS) / stopWatch.totalTime().millisFrac()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageRequest.java b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageRequest.java
deleted file mode 100644
index 2978c5c..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageRequest.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.transport.TransportRequest;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class BenchmarkMessageRequest extends TransportRequest {
-
-    long id;
-    byte[] payload;
-
-    public BenchmarkMessageRequest(long id, byte[] payload) {
-        this.id = id;
-        this.payload = payload;
-    }
-
-    public BenchmarkMessageRequest() {
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readLong();
-        payload = new byte[in.readVInt()];
-        in.readFully(payload);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeLong(id);
-        out.writeVInt(payload.length);
-        out.writeBytes(payload);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageResponse.java b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageResponse.java
deleted file mode 100644
index 7a7e3d9..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkMessageResponse.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.transport.TransportResponse;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class BenchmarkMessageResponse extends TransportResponse {
-
-    long id;
-    byte[] payload;
-
-    public BenchmarkMessageResponse(BenchmarkMessageRequest request) {
-        this.id = request.id;
-        this.payload = request.payload;
-    }
-
-    public BenchmarkMessageResponse(long id, byte[] payload) {
-        this.id = id;
-        this.payload = payload;
-    }
-
-    public BenchmarkMessageResponse() {
-    }
-
-    public long id() {
-        return id;
-    }
-
-    public byte[] payload() {
-        return payload;
-    }
-
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        id = in.readLong();
-        payload = new byte[in.readVInt()];
-        in.readFully(payload);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        out.writeLong(id);
-        out.writeVInt(payload.length);
-        out.writeBytes(payload);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
deleted file mode 100644
index 14447fb..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.transport.InetSocketTransportAddress;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.util.BigArrays;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
-import org.elasticsearch.transport.netty.NettyTransport;
-
-import java.net.InetAddress;
-import java.util.concurrent.CountDownLatch;
-
-/**
- *
- */
-public class BenchmarkNettyLargeMessages {
-
-    public static void main(String[] args) throws Exception {
-        final ByteSizeValue payloadSize = new ByteSizeValue(10, ByteSizeUnit.MB);
-        final int NUMBER_OF_ITERATIONS = 100000;
-        final int NUMBER_OF_CLIENTS = 5;
-        final byte[] payload = new byte[(int) payloadSize.bytes()];
-
-        Settings settings = Settings.settingsBuilder()
-                .build();
-
-        NetworkService networkService = new NetworkService(settings);
-
-        final ThreadPool threadPool = new ThreadPool("BenchmarkNettyLargeMessages");
-        final TransportService transportServiceServer = new TransportService(
-                new NettyTransport(settings, threadPool, networkService, BigArrays.NON_RECYCLING_INSTANCE, Version.CURRENT, new NamedWriteableRegistry()), threadPool
-        ).start();
-        final TransportService transportServiceClient = new TransportService(
-                new NettyTransport(settings, threadPool, networkService, BigArrays.NON_RECYCLING_INSTANCE, Version.CURRENT, new NamedWriteableRegistry()), threadPool
-        ).start();
-
-        final DiscoveryNode bigNode = new DiscoveryNode("big", new InetSocketTransportAddress(InetAddress.getByName("localhost"), 9300), Version.CURRENT);
-//        final DiscoveryNode smallNode = new DiscoveryNode("small", new InetSocketTransportAddress("localhost", 9300));
-        final DiscoveryNode smallNode = bigNode;
-
-        transportServiceClient.connectToNode(bigNode);
-        transportServiceClient.connectToNode(smallNode);
-
-        transportServiceServer.registerRequestHandler("benchmark", BenchmarkMessageRequest::new, ThreadPool.Names.GENERIC, new TransportRequestHandler<BenchmarkMessageRequest>() {
-            @Override
-            public void messageReceived(BenchmarkMessageRequest request, TransportChannel channel) throws Exception {
-                channel.sendResponse(new BenchmarkMessageResponse(request));
-            }
-        });
-
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_CLIENTS);
-        for (int i = 0; i < NUMBER_OF_CLIENTS; i++) {
-            new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (int i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                        BenchmarkMessageRequest message = new BenchmarkMessageRequest(1, payload);
-                        transportServiceClient.submitRequest(bigNode, "benchmark", message, TransportRequestOptions.builder().withType(TransportRequestOptions.Type.BULK).build(), new BaseTransportResponseHandler<BenchmarkMessageResponse>() {
-                            @Override
-                            public BenchmarkMessageResponse newInstance() {
-                                return new BenchmarkMessageResponse();
-                            }
-
-                            @Override
-                            public String executor() {
-                                return ThreadPool.Names.SAME;
-                            }
-
-                            @Override
-                            public void handleResponse(BenchmarkMessageResponse response) {
-                            }
-
-                            @Override
-                            public void handleException(TransportException exp) {
-                                exp.printStackTrace();
-                            }
-                        }).txGet();
-                    }
-                    latch.countDown();
-                }
-            }).start();
-        }
-
-        new Thread(new Runnable() {
-            @Override
-            public void run() {
-                for (int i = 0; i < 1; i++) {
-                    BenchmarkMessageRequest message = new BenchmarkMessageRequest(2, BytesRef.EMPTY_BYTES);
-                    long start = System.currentTimeMillis();
-                    transportServiceClient.submitRequest(smallNode, "benchmark", message, TransportRequestOptions.builder().withType(TransportRequestOptions.Type.STATE).build(), new BaseTransportResponseHandler<BenchmarkMessageResponse>() {
-                        @Override
-                        public BenchmarkMessageResponse newInstance() {
-                            return new BenchmarkMessageResponse();
-                        }
-
-                        @Override
-                        public String executor() {
-                            return ThreadPool.Names.SAME;
-                        }
-
-                        @Override
-                        public void handleResponse(BenchmarkMessageResponse response) {
-                        }
-
-                        @Override
-                        public void handleException(TransportException exp) {
-                            exp.printStackTrace();
-                        }
-                    }).txGet();
-                    long took = System.currentTimeMillis() - start;
-                    System.out.println("Took " + took + "ms");
-                }
-            }
-        }).start();
-
-        latch.await();
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java
deleted file mode 100644
index 5ccc264..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/TransportBenchmark.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.StopWatch;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.network.NetworkService;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.util.BigArrays;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
-import org.elasticsearch.transport.local.LocalTransport;
-import org.elasticsearch.transport.netty.NettyTransport;
-
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicLong;
-
-/**
- *
- */
-public class TransportBenchmark {
-
-    static enum Type {
-        LOCAL {
-            @Override
-            public Transport newTransport(Settings settings, ThreadPool threadPool) {
-                return new LocalTransport(settings, threadPool, Version.CURRENT, new NamedWriteableRegistry());
-            }
-        },
-        NETTY {
-            @Override
-            public Transport newTransport(Settings settings, ThreadPool threadPool) {
-                return new NettyTransport(settings, threadPool, new NetworkService(Settings.EMPTY), BigArrays.NON_RECYCLING_INSTANCE, Version.CURRENT, new NamedWriteableRegistry());
-            }
-        };
-
-        public abstract Transport newTransport(Settings settings, ThreadPool threadPool);
-    }
-
-    public static void main(String[] args) {
-        final String executor = ThreadPool.Names.GENERIC;
-        final boolean waitForRequest = true;
-        final ByteSizeValue payloadSize = new ByteSizeValue(100, ByteSizeUnit.BYTES);
-        final int NUMBER_OF_CLIENTS = 10;
-        final int NUMBER_OF_ITERATIONS = 100000;
-        final byte[] payload = new byte[(int) payloadSize.bytes()];
-        final AtomicLong idGenerator = new AtomicLong();
-        final Type type = Type.NETTY;
-
-
-        Settings settings = Settings.settingsBuilder()
-                .build();
-
-        final ThreadPool serverThreadPool = new ThreadPool("server");
-        final TransportService serverTransportService = new TransportService(type.newTransport(settings, serverThreadPool), serverThreadPool).start();
-
-        final ThreadPool clientThreadPool = new ThreadPool("client");
-        final TransportService clientTransportService = new TransportService(type.newTransport(settings, clientThreadPool), clientThreadPool).start();
-
-        final DiscoveryNode node = new DiscoveryNode("server", serverTransportService.boundAddress().publishAddress(), Version.CURRENT);
-
-        serverTransportService.registerRequestHandler("benchmark", BenchmarkMessageRequest::new, executor, new TransportRequestHandler<BenchmarkMessageRequest>() {
-            @Override
-            public void messageReceived(BenchmarkMessageRequest request, TransportChannel channel) throws Exception {
-                channel.sendResponse(new BenchmarkMessageResponse(request));
-            }
-        });
-
-        clientTransportService.connectToNode(node);
-
-        for (int i = 0; i < 10000; i++) {
-            BenchmarkMessageRequest message = new BenchmarkMessageRequest(1, payload);
-            clientTransportService.submitRequest(node, "benchmark", message, new BaseTransportResponseHandler<BenchmarkMessageResponse>() {
-                @Override
-                public BenchmarkMessageResponse newInstance() {
-                    return new BenchmarkMessageResponse();
-                }
-
-                @Override
-                public String executor() {
-                    return ThreadPool.Names.SAME;
-                }
-
-                @Override
-                public void handleResponse(BenchmarkMessageResponse response) {
-                }
-
-                @Override
-                public void handleException(TransportException exp) {
-                    exp.printStackTrace();
-                }
-            }).txGet();
-        }
-
-
-        Thread[] clients = new Thread[NUMBER_OF_CLIENTS];
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_CLIENTS * NUMBER_OF_ITERATIONS);
-        for (int i = 0; i < NUMBER_OF_CLIENTS; i++) {
-            clients[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (int j = 0; j < NUMBER_OF_ITERATIONS; j++) {
-                        final long id = idGenerator.incrementAndGet();
-                        BenchmarkMessageRequest request = new BenchmarkMessageRequest(id, payload);
-                        BaseTransportResponseHandler<BenchmarkMessageResponse> handler = new BaseTransportResponseHandler<BenchmarkMessageResponse>() {
-                            @Override
-                            public BenchmarkMessageResponse newInstance() {
-                                return new BenchmarkMessageResponse();
-                            }
-
-                            @Override
-                            public String executor() {
-                                return executor;
-                            }
-
-                            @Override
-                            public void handleResponse(BenchmarkMessageResponse response) {
-                                if (response.id() != id) {
-                                    System.out.println("NO ID MATCH [" + response.id() + "] and [" + id + "]");
-                                }
-                                latch.countDown();
-                            }
-
-                            @Override
-                            public void handleException(TransportException exp) {
-                                exp.printStackTrace();
-                                latch.countDown();
-                            }
-                        };
-
-                        if (waitForRequest) {
-                            clientTransportService.submitRequest(node, "benchmark", request, handler).txGet();
-                        } else {
-                            clientTransportService.sendRequest(node, "benchmark", request, handler);
-                        }
-                    }
-                }
-            });
-        }
-
-        StopWatch stopWatch = new StopWatch().start();
-        for (int i = 0; i < NUMBER_OF_CLIENTS; i++) {
-            clients[i].start();
-        }
-
-        try {
-            latch.await();
-        } catch (InterruptedException e) {
-            e.printStackTrace();
-        }
-        stopWatch.stop();
-
-        System.out.println("Ran [" + NUMBER_OF_CLIENTS + "], each with [" + NUMBER_OF_ITERATIONS + "] iterations, payload [" + payloadSize + "]: took [" + stopWatch.totalTime() + "], TPS: " + (NUMBER_OF_CLIENTS * NUMBER_OF_ITERATIONS) / stopWatch.totalTime().secondsFrac());
-
-        clientTransportService.close();
-        clientThreadPool.shutdownNow();
-
-        serverTransportService.close();
-        serverThreadPool.shutdownNow();
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java
deleted file mode 100644
index fd76504..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/netty/NettyEchoBenchmark.java
+++ /dev/null
@@ -1,158 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.transport.netty;
-
-import org.jboss.netty.bootstrap.ClientBootstrap;
-import org.jboss.netty.bootstrap.ServerBootstrap;
-import org.jboss.netty.buffer.ChannelBuffer;
-import org.jboss.netty.buffer.ChannelBuffers;
-import org.jboss.netty.channel.*;
-import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
-import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
-
-import java.net.InetAddress;
-import java.net.InetSocketAddress;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.Executors;
-
-public class NettyEchoBenchmark {
-
-    public static void main(String[] args) throws Exception {
-        final int payloadSize = 100;
-        int CYCLE_SIZE = 50000;
-        final long NUMBER_OF_ITERATIONS = 500000;
-
-        ChannelBuffer message = ChannelBuffers.buffer(100);
-        for (int i = 0; i < message.capacity(); i++) {
-            message.writeByte((byte) i);
-        }
-
-        // Configure the server.
-        ServerBootstrap serverBootstrap = new ServerBootstrap(
-                new NioServerSocketChannelFactory(
-                        Executors.newCachedThreadPool(),
-                        Executors.newCachedThreadPool()));
-
-        // Set up the pipeline factory.
-        serverBootstrap.setPipelineFactory(new ChannelPipelineFactory() {
-            @Override
-            public ChannelPipeline getPipeline() throws Exception {
-                return Channels.pipeline(new EchoServerHandler());
-            }
-        });
-
-        // Bind and start to accept incoming connections.
-        serverBootstrap.bind(new InetSocketAddress(InetAddress.getLoopbackAddress(), 9000));
-
-        ClientBootstrap clientBootstrap = new ClientBootstrap(
-                new NioClientSocketChannelFactory(
-                        Executors.newCachedThreadPool(),
-                        Executors.newCachedThreadPool()));
-
-//        ClientBootstrap clientBootstrap = new ClientBootstrap(
-//                new OioClientSocketChannelFactory(Executors.newCachedThreadPool()));
-
-        // Set up the pipeline factory.
-        final EchoClientHandler clientHandler = new EchoClientHandler();
-        clientBootstrap.setPipelineFactory(new ChannelPipelineFactory() {
-            @Override
-            public ChannelPipeline getPipeline() throws Exception {
-                return Channels.pipeline(clientHandler);
-            }
-        });
-
-        // Start the connection attempt.
-        ChannelFuture future = clientBootstrap.connect(new InetSocketAddress(InetAddress.getLoopbackAddress(), 9000));
-        future.awaitUninterruptibly();
-        Channel clientChannel = future.getChannel();
-
-        System.out.println("Warming up...");
-        for (long i = 0; i < 10000; i++) {
-            clientHandler.latch = new CountDownLatch(1);
-            clientChannel.write(message);
-            try {
-                clientHandler.latch.await();
-            } catch (InterruptedException e) {
-                e.printStackTrace();
-            }
-        }
-        System.out.println("Warmed up");
-
-
-        long start = System.currentTimeMillis();
-        long cycleStart = System.currentTimeMillis();
-        for (long i = 1; i < NUMBER_OF_ITERATIONS; i++) {
-            clientHandler.latch = new CountDownLatch(1);
-            clientChannel.write(message);
-            try {
-                clientHandler.latch.await();
-            } catch (InterruptedException e) {
-                e.printStackTrace();
-            }
-            if ((i % CYCLE_SIZE) == 0) {
-                long cycleEnd = System.currentTimeMillis();
-                System.out.println("Ran 50000, TPS " + (CYCLE_SIZE / ((double) (cycleEnd - cycleStart) / 1000)));
-                cycleStart = cycleEnd;
-            }
-        }
-        long end = System.currentTimeMillis();
-        long seconds = (end - start) / 1000;
-        System.out.println("Ran [" + NUMBER_OF_ITERATIONS + "] iterations, payload [" + payloadSize + "]: took [" + seconds + "], TPS: " + ((double) NUMBER_OF_ITERATIONS) / seconds);
-
-        clientChannel.close().awaitUninterruptibly();
-        clientBootstrap.releaseExternalResources();
-        serverBootstrap.releaseExternalResources();
-    }
-
-    public static class EchoClientHandler extends SimpleChannelUpstreamHandler {
-
-        public volatile CountDownLatch latch;
-
-        public EchoClientHandler() {
-        }
-
-        @Override
-        public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) {
-            latch.countDown();
-        }
-
-        @Override
-        public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
-            e.getCause().printStackTrace();
-            e.getChannel().close();
-        }
-    }
-
-
-    public static class EchoServerHandler extends SimpleChannelUpstreamHandler {
-
-        @Override
-        public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) {
-            e.getChannel().write(e.getMessage());
-        }
-
-        @Override
-        public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
-            // Close the connection when an exception is raised.
-            e.getCause().printStackTrace();
-            e.getChannel().close();
-        }
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/benchmark/uuid/SimpleUuidBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/uuid/SimpleUuidBenchmark.java
deleted file mode 100644
index d9995e1..0000000
--- a/core/src/test/java/org/elasticsearch/benchmark/uuid/SimpleUuidBenchmark.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.benchmark.uuid;
-
-import org.elasticsearch.common.StopWatch;
-
-import java.util.UUID;
-import java.util.concurrent.CountDownLatch;
-
-/**
- *
- */
-public class SimpleUuidBenchmark {
-
-    private static long NUMBER_OF_ITERATIONS = 10000;
-    private static int NUMBER_OF_THREADS = 100;
-
-    public static void main(String[] args) throws Exception {
-        StopWatch stopWatch = new StopWatch().start();
-        System.out.println("Running " + NUMBER_OF_ITERATIONS);
-        for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-            UUID.randomUUID().toString();
-        }
-        System.out.println("Generated in " + stopWatch.stop().totalTime() + " TP Millis " + (NUMBER_OF_ITERATIONS / stopWatch.totalTime().millisFrac()));
-
-        System.out.println("Generating using " + NUMBER_OF_THREADS + " threads with " + NUMBER_OF_ITERATIONS + " iterations");
-        final CountDownLatch latch = new CountDownLatch(NUMBER_OF_THREADS);
-        Thread[] threads = new Thread[NUMBER_OF_THREADS];
-        for (int i = 0; i < threads.length; i++) {
-            threads[i] = new Thread(new Runnable() {
-                @Override
-                public void run() {
-                    for (long i = 0; i < NUMBER_OF_ITERATIONS; i++) {
-                        UUID.randomUUID().toString();
-                    }
-                    latch.countDown();
-                }
-            });
-        }
-        stopWatch = new StopWatch().start();
-        for (Thread thread : threads) {
-            thread.start();
-        }
-        latch.await();
-        stopWatch.stop();
-        System.out.println("Generate in " + stopWatch.totalTime() + " TP Millis " + ((NUMBER_OF_ITERATIONS * NUMBER_OF_THREADS) / stopWatch.totalTime().millisFrac()));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java
index fbc6cc5..95735b8 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityIT.java
@@ -71,7 +71,6 @@ import java.util.concurrent.ExecutionException;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
 import static org.elasticsearch.index.query.QueryBuilders.existsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
 import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
@@ -440,25 +439,9 @@ public class BasicBackwardsCompatibilityIT extends ESBackcompatTestCase {
             countResponse = client().prepareSearch().setSize(0).setQuery(existsQuery("obj1")).get();
             assertHitCount(countResponse, 2l);
 
-            countResponse = client().prepareSearch().setSize(0).setQuery(missingQuery("field1")).get();
-            assertHitCount(countResponse, 2l);
-
-            countResponse = client().prepareSearch().setSize(0).setQuery(missingQuery("field1")).get();
-            assertHitCount(countResponse, 2l);
-
-            countResponse = client().prepareSearch().setSize(0).setQuery(constantScoreQuery(missingQuery("field1"))).get();
-            assertHitCount(countResponse, 2l);
-
             countResponse = client().prepareSearch().setSize(0).setQuery(queryStringQuery("_missing_:field1")).get();
             assertHitCount(countResponse, 2l);
 
-            // wildcard check
-            countResponse = client().prepareSearch().setSize(0).setQuery(missingQuery("x*")).get();
-            assertHitCount(countResponse, 2l);
-
-            // object check
-            countResponse = client().prepareSearch().setSize(0).setQuery(missingQuery("obj1")).get();
-            assertHitCount(countResponse, 2l);
             if (!backwardsCluster().upgradeOneNode()) {
                 break;
             }
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java b/core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java
index 5a82384..39ed23b 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/NodesStatsBasicBackwardsCompatIT.java
@@ -69,7 +69,7 @@ public class NodesStatsBasicBackwardsCompatIT extends ESBackcompatTestCase {
             NodesStatsRequestBuilder nsBuilder = tc.admin().cluster().prepareNodesStats();
 
             Class c = nsBuilder.getClass();
-            for (Method method : c.getDeclaredMethods()) {
+            for (Method method : c.getMethods()) {
                 if (method.getName().startsWith("set")) {
                     if (method.getParameterTypes().length == 1 && method.getParameterTypes()[0] == boolean.class) {
                         method.invoke(nsBuilder, randomBoolean());
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
index b5e36c0..97e77a0 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
@@ -32,8 +32,6 @@ import org.elasticsearch.action.admin.indices.upgrade.UpgradeIT;
 import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.settings.Settings;
@@ -62,23 +60,12 @@ import org.junit.Before;
 
 import java.io.IOException;
 import java.io.InputStream;
-import java.nio.file.DirectoryStream;
-import java.nio.file.FileVisitResult;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.SimpleFileVisitor;
+import java.nio.file.*;
 import java.nio.file.attribute.BasicFileAttributes;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.SortedSet;
-import java.util.TreeSet;
+import java.util.*;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 
 // needs at least 2 nodes since it bumps replicas to 1
@@ -120,7 +107,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
     public Settings nodeSettings(int ord) {
         return Settings.builder()
                 .put(MergePolicyConfig.INDEX_MERGE_ENABLED, false) // disable merging so no segments will be upgraded
-                .put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING.getKey(), 30) // increase recovery speed for small files
+                .put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, 30) // increase recovery speed for small files
                 .build();
     }
 
@@ -271,7 +258,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
     public void testOldIndexes() throws Exception {
         setupCluster();
 
-        Collections.shuffle(indexes, getRandom());
+        Collections.shuffle(indexes, random());
         for (String index : indexes) {
             long startTime = System.currentTimeMillis();
             logger.info("--> Testing old index " + index);
@@ -341,13 +328,6 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
         searchRsp = searchReq.get();
         ElasticsearchAssertions.assertNoFailures(searchRsp);
         assertEquals(numDocs, searchRsp.getHits().getTotalHits());
-
-        logger.info("--> testing missing filter");
-        // the field for the missing filter here needs to be different than the exists filter above, to avoid being found in the cache
-        searchReq = client().prepareSearch(indexName).setQuery(QueryBuilders.missingQuery("long_sort"));
-        searchRsp = searchReq.get();
-        ElasticsearchAssertions.assertNoFailures(searchRsp);
-        assertEquals(0, searchRsp.getHits().getTotalHits());
     }
 
     void assertBasicAggregationWorks(String indexName) {
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
index b536c88..228f1a6 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
@@ -92,7 +92,7 @@ public class RestoreBackwardsCompatIT extends AbstractSnapshotIntegTestCase {
         }
 
         SortedSet<String> expectedVersions = new TreeSet<>();
-        for (java.lang.reflect.Field field : Version.class.getDeclaredFields()) {
+        for (java.lang.reflect.Field field : Version.class.getFields()) {
             if (Modifier.isStatic(field.getModifiers()) && field.getType() == Version.class) {
                 Version v = (Version) field.get(Version.class);
                 if (v.snapshot()) continue;
@@ -181,7 +181,7 @@ public class RestoreBackwardsCompatIT extends AbstractSnapshotIntegTestCase {
 
         logger.info("--> check settings");
         ClusterState clusterState = client().admin().cluster().prepareState().get().getState();
-        assertThat(clusterState.metaData().persistentSettings().get(FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + "version_attr"), equalTo(version));
+        assertThat(clusterState.metaData().persistentSettings().get(FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP + "version_attr"), equalTo(version));
 
         logger.info("--> check templates");
         IndexTemplateMetaData template = clusterState.getMetaData().templates().get("template_" + version.toLowerCase(Locale.ROOT));
diff --git a/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java b/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
index 4d82445..f01fdff 100644
--- a/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
+++ b/core/src/test/java/org/elasticsearch/client/transport/TransportClientIT.java
@@ -32,7 +32,6 @@ import org.elasticsearch.test.ESIntegTestCase.Scope;
 import org.elasticsearch.transport.TransportService;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 import static org.hamcrest.Matchers.is;
@@ -51,13 +50,15 @@ public class TransportClientIT extends ESIntegTestCase {
     public void testNodeVersionIsUpdated() {
         TransportClient client = (TransportClient)  internalCluster().client();
         TransportClientNodesService nodeService = client.nodeService();
-        Node node = nodeBuilder().data(false).settings(Settings.builder()
+        Node node = new Node(Settings.builder()
                 .put(internalCluster().getDefaultSettings())
                 .put("path.home", createTempDir())
                 .put("node.name", "testNodeVersionIsUpdated")
                 .put("http.enabled", false)
+                .put("node.data", false)
+                .put("cluster.name", "foobar")
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // make sure we get what we set :)
-                .build()).clusterName("foobar").build();
+                .build());
         node.start();
         try {
             TransportAddress transportAddress = node.injector().getInstance(TransportService.class).boundAddress().publishAddress();
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
index 8aa0655..5ed4562 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
@@ -39,7 +39,6 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.store.Store;
@@ -127,7 +126,7 @@ public class ClusterInfoServiceIT extends ESIntegTestCase {
     protected Settings nodeSettings(int nodeOrdinal) {
         return Settings.builder()
                 // manual collection or upon cluster forming.
-                .put(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT_SETTING.getKey(), "1s")
+                .put(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT, "1s")
                 .build();
     }
 
@@ -138,7 +137,9 @@ public class ClusterInfoServiceIT extends ESIntegTestCase {
     }
 
     public void testClusterInfoServiceCollectsInformation() throws Exception {
-        internalCluster().startNodesAsync(2).get();
+        internalCluster().startNodesAsync(2,
+                Settings.builder().put(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, "200ms").build())
+                .get();
         assertAcked(prepareCreate("test").setSettings(settingsBuilder()
                 .put(Store.INDEX_STORE_STATS_REFRESH_INTERVAL, 0)
                 .put(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE, EnableAllocationDecider.Rebalance.NONE).build()));
@@ -146,8 +147,6 @@ public class ClusterInfoServiceIT extends ESIntegTestCase {
         InternalTestCluster internalTestCluster = internalCluster();
         // Get the cluster info service on the master node
         final InternalClusterInfoService infoService = (InternalClusterInfoService) internalTestCluster.getInstance(ClusterInfoService.class, internalTestCluster.getMasterName());
-        infoService.setUpdateFrequency(TimeValue.timeValueMillis(200));
-        infoService.onMaster();
         ClusterInfo info = infoService.refresh();
         assertNotNull("info should not be null", info);
         ImmutableOpenMap<String, DiskUsage> leastUsages = info.getNodeLeastAvailableDiskUsages();
@@ -189,7 +188,7 @@ public class ClusterInfoServiceIT extends ESIntegTestCase {
     public void testClusterInfoServiceInformationClearOnError() throws InterruptedException, ExecutionException {
         internalCluster().startNodesAsync(2,
                 // manually control publishing
-                Settings.builder().put(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL_SETTING.getKey(), "60m").build())
+                Settings.builder().put(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, "60m").build())
                 .get();
         prepareCreate("test").setSettings(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1).get();
         ensureGreen("test");
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java b/core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java
index 48b2591..4ca0fff 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java
@@ -31,10 +31,11 @@ import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllo
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
+import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
 import org.elasticsearch.cluster.settings.DynamicSettings;
 import org.elasticsearch.cluster.settings.Validator;
 import org.elasticsearch.common.inject.ModuleTestCase;
-import org.elasticsearch.common.settings.*;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.settings.IndexDynamicSettings;
 
 public class ClusterModuleTests extends ModuleTestCase {
@@ -72,20 +73,18 @@ public class ClusterModuleTests extends ModuleTestCase {
     }
 
     public void testRegisterClusterDynamicSettingDuplicate() {
-        final SettingsFilter settingsFilter = new SettingsFilter(Settings.EMPTY);
-        SettingsModule module = new SettingsModule(Settings.EMPTY, settingsFilter);
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
         try {
-            module.registerSetting(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING);
+            module.registerClusterDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, Validator.EMPTY);
         } catch (IllegalArgumentException e) {
-            assertEquals(e.getMessage(), "Cannot register setting [" + EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey() + "] twice");
+            assertEquals(e.getMessage(), "Cannot register setting [" + EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE + "] twice");
         }
     }
 
     public void testRegisterClusterDynamicSetting() {
-        final SettingsFilter settingsFilter = new SettingsFilter(Settings.EMPTY);
-        SettingsModule module = new SettingsModule(Settings.EMPTY, settingsFilter);
-        module.registerSetting(Setting.boolSetting("foo.bar", false, true, Setting.Scope.CLUSTER));
-        assertInstanceBinding(module, ClusterSettings.class, service -> service.hasDynamicSetting("foo.bar"));
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        module.registerClusterDynamicSetting("foo.bar", Validator.EMPTY);
+        assertInstanceBindingWithAnnotation(module, DynamicSettings.class, dynamicSettings -> dynamicSettings.hasDynamicSetting("foo.bar"), ClusterDynamicSettings.class);
     }
 
     public void testRegisterIndexDynamicSettingDuplicate() {
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
index 5199d3f..9e842a3 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterServiceIT.java
@@ -940,7 +940,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
     public void testLongClusterStateUpdateLogging() throws Exception {
         Settings settings = settingsBuilder()
                 .put("discovery.type", "local")
-                .put(InternalClusterService.CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING.getKey(), "10s")
+                .put(InternalClusterService.SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD, "10s")
                 .build();
         internalCluster().startNode(settings);
         ClusterService clusterService1 = internalCluster().getInstance(ClusterService.class);
@@ -976,7 +976,7 @@ public class ClusterServiceIT extends ESIntegTestCase {
 
             processedFirstTask.await(1, TimeUnit.SECONDS);
             assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder()
-                    .put(InternalClusterService.CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD_SETTING.getKey(), "10ms")));
+                    .put(InternalClusterService.SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD, "10ms")));
 
             clusterService1.submitStateUpdateTask("test2", new ClusterStateUpdateTask() {
                 @Override
diff --git a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
index 2f1e5d3..648356b 100644
--- a/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/MinimumMasterNodesIT.java
@@ -280,7 +280,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
         assertNoMasterBlockOnAllNodes();
 
         logger.info("--> bringing another node up");
-        internalCluster().startNode(settingsBuilder().put(settings).put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2).build());
+        internalCluster().startNode(settingsBuilder().put(settings).put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 2).build());
         clusterHealthResponse = client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes("2").get();
         assertThat(clusterHealthResponse.isTimedOut(), equalTo(false));
     }
@@ -317,7 +317,7 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
 
         // set an initial value which is at least quorum to avoid split brains during initial startup
         int initialMinMasterNodes = randomIntBetween(nodeCount / 2 + 1, nodeCount);
-        settings.put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), initialMinMasterNodes);
+        settings.put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, initialMinMasterNodes);
 
 
         logger.info("--> starting [{}] nodes. min_master_nodes set to [{}]", nodeCount, initialMinMasterNodes);
@@ -328,21 +328,19 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
 
         int updateCount = randomIntBetween(1, nodeCount);
 
-        logger.info("--> updating [{}] to [{}]", ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), updateCount);
+        logger.info("--> updating [{}] to [{}]", ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, updateCount);
         assertAcked(client().admin().cluster().prepareUpdateSettings()
-                .setPersistentSettings(settingsBuilder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), updateCount)));
+                .setPersistentSettings(settingsBuilder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, updateCount)));
 
         logger.info("--> verifying no node left and master is up");
         assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes(Integer.toString(nodeCount)).get().isTimedOut());
 
         updateCount = nodeCount + randomIntBetween(1, 2000);
-        logger.info("--> trying to updating [{}] to [{}]", ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), updateCount);
-        try {
-            client().admin().cluster().prepareUpdateSettings()
-                    .setPersistentSettings(settingsBuilder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), updateCount));
-        } catch (IllegalArgumentException ex) {
-            assertEquals(ex.getMessage(), "cannot set discovery.zen.minimum_master_nodes to more than the current master nodes count [" +updateCount+ "]");
-        }
+        logger.info("--> trying to updating [{}] to [{}]", ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, updateCount);
+        assertThat(client().admin().cluster().prepareUpdateSettings()
+                        .setPersistentSettings(settingsBuilder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, updateCount))
+                        .get().getPersistentSettings().getAsMap().keySet(),
+                empty());
 
         logger.info("--> verifying no node left and master is up");
         assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes(Integer.toString(nodeCount)).get().isTimedOut());
@@ -353,8 +351,8 @@ public class MinimumMasterNodesIT extends ESIntegTestCase {
                 .put("discovery.type", "zen")
                 .put(FaultDetection.SETTING_PING_TIMEOUT, "1h") // disable it
                 .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
-                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2)
-                .put(DiscoverySettings.COMMIT_TIMEOUT_SETTING.getKey(), "100ms") // speed things up
+                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 2)
+                .put(DiscoverySettings.COMMIT_TIMEOUT, "100ms") // speed things up
                 .build();
         internalCluster().startNodesAsync(3, settings).get();
         ensureGreen(); // ensure cluster state is recovered before we disrupt things
diff --git a/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java b/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
index 8e5479d..e0f8b2c 100644
--- a/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/NoMasterNodeIT.java
@@ -67,7 +67,7 @@ public class NoMasterNodeIT extends ESIntegTestCase {
                 .put("discovery.zen.minimum_master_nodes", 2)
                 .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
-                .put(DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), "all")
+                .put(DiscoverySettings.NO_MASTER_BLOCK, "all")
                 .build();
 
         TimeValue timeout = TimeValue.timeValueMillis(200);
@@ -219,7 +219,7 @@ public class NoMasterNodeIT extends ESIntegTestCase {
                 .put("discovery.zen.minimum_master_nodes", 2)
                 .put(ZenDiscovery.SETTING_PING_TIMEOUT, "200ms")
                 .put("discovery.initial_state_timeout", "500ms")
-                .put(DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), "write")
+                .put(DiscoverySettings.NO_MASTER_BLOCK, "write")
                 .build();
 
         internalCluster().startNode(settings);
diff --git a/core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java b/core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java
index c5e48a9..81de8b1 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsIT.java
@@ -50,8 +50,8 @@ public class AckClusterUpdateSettingsIT extends ESIntegTestCase {
                 .put(super.nodeSettings(nodeOrdinal))
                 //make sure that enough concurrent reroutes can happen at the same time
                 //we have a minimum of 2 nodes, and a maximum of 10 shards, thus 5 should be enough
-                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), 5)
-                .put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING.getKey(), 10)
+                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, 5)
+                .put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, 10)
                 .build();
     }
 
@@ -69,7 +69,7 @@ public class AckClusterUpdateSettingsIT extends ESIntegTestCase {
     private void removePublishTimeout() {
         //to test that the acknowledgement mechanism is working we better disable the wait for publish
         //otherwise the operation is most likely acknowledged even if it doesn't support ack
-        assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "0")));
+        assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT, "0")));
     }
 
     public void testClusterUpdateSettingsAcknowledgement() {
diff --git a/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java b/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
index 7d3825a..47517a7 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ack/AckIT.java
@@ -67,8 +67,8 @@ public class AckIT extends ESIntegTestCase {
         //to test that the acknowledgement mechanism is working we better disable the wait for publish
         //otherwise the operation is most likely acknowledged even if it doesn't support ack
         return Settings.builder().put(super.nodeSettings(nodeOrdinal))
-                .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), 0).build();
-}
+                .put(DiscoverySettings.PUBLISH_TIMEOUT, 0).build();
+    }
 
     public void testUpdateSettingsAcknowledgement() {
         createIndex("test");
diff --git a/core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java b/core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java
index 7265901..f915162 100644
--- a/core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/allocation/AwarenessAllocationIT.java
@@ -106,9 +106,9 @@ public class AwarenessAllocationIT extends ESIntegTestCase {
 
     public void testAwarenessZones() throws Exception {
         Settings commonSettings = Settings.settingsBuilder()
-                .put(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP_SETTING.getKey() + "zone.values", "a,b")
-                .put(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTE_SETTING.getKey(), "zone")
-                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 3)
+                .put(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP + "zone.values", "a,b")
+                .put(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTES, "zone")
+                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 3)
                 .put(ZenDiscovery.SETTING_JOIN_TIMEOUT, "10s")
                 .build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java b/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
index b85c170..1605e70 100644
--- a/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java
@@ -56,7 +56,7 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_BLOCKS_ME
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_BLOCKS_READ;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_BLOCKS_WRITE;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_READ_ONLY;
-import static org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING;
+import static org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertBlocked;
@@ -71,15 +71,15 @@ public class ClusterRerouteIT extends ESIntegTestCase {
 
     public void testRerouteWithCommands_disableAllocationSettings() throws Exception {
         Settings commonSettings = settingsBuilder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, "none")
                 .build();
         rerouteWithCommands(commonSettings);
     }
 
     public void testRerouteWithCommands_enableAllocationSettings() throws Exception {
         Settings commonSettings = settingsBuilder()
-                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), Allocation.NONE.name())
+                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE, Allocation.NONE.name())
                 .build();
         rerouteWithCommands(commonSettings);
     }
@@ -147,15 +147,15 @@ public class ClusterRerouteIT extends ESIntegTestCase {
 
     public void testRerouteWithAllocateLocalGateway_disableAllocationSettings() throws Exception {
         Settings commonSettings = settingsBuilder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, "none")
                 .build();
         rerouteWithAllocateLocalGateway(commonSettings);
     }
 
     public void testRerouteWithAllocateLocalGateway_enableAllocationSettings() throws Exception {
         Settings commonSettings = settingsBuilder()
-                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), Allocation.NONE.name())
+                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE, Allocation.NONE.name())
                 .build();
         rerouteWithAllocateLocalGateway(commonSettings);
     }
@@ -279,7 +279,7 @@ public class ClusterRerouteIT extends ESIntegTestCase {
 
         logger.info("--> disable allocation");
         Settings newSettings = settingsBuilder()
-                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), Allocation.NONE.name())
+                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE, Allocation.NONE.name())
                 .build();
         client().admin().cluster().prepareUpdateSettings().setTransientSettings(newSettings).execute().actionGet();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java b/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
index ef54814..9a91e1c 100644
--- a/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
@@ -29,11 +29,7 @@ import org.junit.BeforeClass;
 
 import java.net.InetAddress;
 import java.net.UnknownHostException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 
 import static java.util.Collections.emptyMap;
 import static java.util.Collections.singletonMap;
@@ -246,7 +242,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
     private Settings shuffleSettings(Settings source) {
         Settings.Builder settings = Settings.settingsBuilder();
         List<String> keys = new ArrayList<>(source.getAsMap().keySet());
-        Collections.shuffle(keys, getRandom());
+        Collections.shuffle(keys, random());
         for (String o : keys) {
             settings.put(o, source.getAsMap().get(o));
         }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java b/core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java
index c236ea5..2d70438 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/DelayedAllocationIT.java
@@ -170,7 +170,7 @@ public class DelayedAllocationIT extends ESIntegTestCase {
     private String findNodeWithShard() {
         ClusterState state = client().admin().cluster().prepareState().get().getState();
         List<ShardRouting> startedShards = state.routingTable().shardsWithState(ShardRoutingState.STARTED);
-        Collections.shuffle(startedShards, getRandom());
+        Collections.shuffle(startedShards,random());
         return state.nodes().get(startedShards.get(0).currentNodeId()).getName();
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
index 29281e2..e8be4e3 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
@@ -26,13 +26,11 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.Node;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.VersionUtils;
 
 import java.io.BufferedReader;
 import java.io.InputStreamReader;
-import java.nio.file.Path;
 import java.util.Arrays;
 
 public class RoutingBackwardCompatibilityTests extends ESTestCase {
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
index 336846f..ee8bd06 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.cluster.routing.allocation;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -51,7 +50,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
     public void testAddNodesAndIndices() {
         Settings.Builder settings = settingsBuilder();
-        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());
+        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());
         AllocationService service = createAllocationService(settings.build());
 
         ClusterState clusterState = initCluster(service, 1, 3, 3, 1);
@@ -94,7 +93,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
     public void testMinimalRelocations() {
         Settings.Builder settings = settingsBuilder();
-        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString())
+        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString())
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 2);
         AllocationService service = createAllocationService(settings.build());
 
@@ -162,7 +161,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
     public void testMinimalRelocationsNoLimit() {
         Settings.Builder settings = settingsBuilder();
-        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString())
+        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString())
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 100)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 100);
         AllocationService service = createAllocationService(settings.build());
@@ -388,7 +387,7 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
         logger.info("Removing [{}] nodes", numNodes);
         DiscoveryNodes.Builder nodes = DiscoveryNodes.builder(clusterState.nodes());
         ArrayList<DiscoveryNode> discoveryNodes = CollectionUtils.iterableAsArrayList(clusterState.nodes());
-        Collections.shuffle(discoveryNodes, getRandom());
+        Collections.shuffle(discoveryNodes, random());
         for (DiscoveryNode node : discoveryNodes) {
             nodes.remove(node.id());
             numNodes--;
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
index 1cf5ba0..6ac2b7d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
@@ -98,8 +98,8 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
 
     public void testAllocateCommand() {
         AllocationService allocation = createAllocationService(settingsBuilder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, "none")
                 .build());
 
         logger.info("--> building initial routing table");
@@ -186,8 +186,8 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
 
     public void testCancelCommand() {
         AllocationService allocation = createAllocationService(settingsBuilder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, "none")
                 .build());
 
         logger.info("--> building initial routing table");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationPriorityTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationPriorityTests.java
index 8d510e7..d7a049d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationPriorityTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationPriorityTests.java
@@ -39,8 +39,8 @@ public class AllocationPriorityTests extends ESAllocationTestCase {
     public void testPrioritizedIndicesAllocatedFirst() {
         AllocationService allocation = createAllocationService(settingsBuilder().
                 put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CONCURRENT_RECOVERIES, 1)
-                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES_SETTING.getKey(), 1)
-                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), 1).build());
+                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES, 1)
+                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, 1).build());
         final String highPriorityName;
         final String lowPriorityName;
         final int priorityFirst;
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
index e9d0f75..7be6037 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
@@ -55,7 +55,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
     public void testMoveShardOnceNewNodeWithAttributeAdded1() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
                 .build());
 
@@ -123,7 +123,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
     public void testMoveShardOnceNewNodeWithAttributeAdded2() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
                 .build());
 
@@ -193,7 +193,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
                 .put("cluster.routing.allocation.balance.index", 0.0f)
@@ -293,7 +293,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
                 .build());
@@ -387,7 +387,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
     public void testMoveShardOnceNewNodeWithAttributeAdded5() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
                 .build());
 
@@ -465,7 +465,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
     public void testMoveShardOnceNewNodeWithAttributeAdded6() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
                 .build());
 
@@ -545,7 +545,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
     public void testFullAwareness1() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.awareness.force.rack_id.values", "1,2")
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
                 .build());
@@ -612,7 +612,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
     public void testFullAwareness2() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.awareness.force.rack_id.values", "1,2")
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
                 .build());
@@ -681,7 +681,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .put("cluster.routing.allocation.awareness.force.rack_id.values", "1,2")
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id")
@@ -767,7 +767,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
                 .put("cluster.routing.allocation.awareness.attributes", "zone")
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build());
 
@@ -828,7 +828,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
     public void testUnassignedShardsWithUnbalancedZones() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.awareness.attributes", "zone")
                 .build());
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
index e622036..1092b2e 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
@@ -37,10 +37,10 @@ import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllo
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
 import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.elasticsearch.test.gateway.NoopGatewayAllocator;
 import org.hamcrest.Matchers;
@@ -65,10 +65,10 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         final float balanceTreshold = 1.0f;
 
         Settings.Builder settings = settingsBuilder();
-        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());
-        settings.put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), indexBalance);
-        settings.put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), replicaBalance);
-        settings.put(BalancedShardsAllocator.THRESHOLD_SETTING.getKey(), balanceTreshold);
+        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());
+        settings.put(BalancedShardsAllocator.SETTING_INDEX_BALANCE_FACTOR, indexBalance);
+        settings.put(BalancedShardsAllocator.SETTING_SHARD_BALANCE_FACTOR, replicaBalance);
+        settings.put(BalancedShardsAllocator.SETTING_THRESHOLD, balanceTreshold);
 
         AllocationService strategy = createAllocationService(settings.build());
 
@@ -90,10 +90,10 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         final float balanceTreshold = 1.0f;
 
         Settings.Builder settings = settingsBuilder();
-        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());
-        settings.put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), indexBalance);
-        settings.put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), replicaBalance);
-        settings.put(BalancedShardsAllocator.THRESHOLD_SETTING.getKey(), balanceTreshold);
+        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());
+        settings.put(BalancedShardsAllocator.SETTING_INDEX_BALANCE_FACTOR, indexBalance);
+        settings.put(BalancedShardsAllocator.SETTING_SHARD_BALANCE_FACTOR, replicaBalance);
+        settings.put(BalancedShardsAllocator.SETTING_THRESHOLD, balanceTreshold);
 
         AllocationService strategy = createAllocationService(settings.build());
 
@@ -279,30 +279,36 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
 
     public void testPersistedSettings() {
         Settings.Builder settings = settingsBuilder();
-        settings.put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 0.2);
-        settings.put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 0.3);
-        settings.put(BalancedShardsAllocator.THRESHOLD_SETTING.getKey(), 2.0);
-        ClusterSettings service = new ClusterSettings(settingsBuilder().build(), ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
+        settings.put(BalancedShardsAllocator.SETTING_INDEX_BALANCE_FACTOR, 0.2);
+        settings.put(BalancedShardsAllocator.SETTING_SHARD_BALANCE_FACTOR, 0.3);
+        settings.put(BalancedShardsAllocator.SETTING_THRESHOLD, 2.0);
+        final NodeSettingsService.Listener[] listeners = new NodeSettingsService.Listener[1];
+        NodeSettingsService service = new NodeSettingsService(settingsBuilder().build()) {
+
+            @Override
+            public void addListener(Listener listener) {
+                assertNull("addListener was called twice while only one time was expected", listeners[0]);
+                listeners[0] = listener;
+            }
+
+        };
         BalancedShardsAllocator allocator = new BalancedShardsAllocator(settings.build(), service);
         assertThat(allocator.getIndexBalance(), Matchers.equalTo(0.2f));
         assertThat(allocator.getShardBalance(), Matchers.equalTo(0.3f));
         assertThat(allocator.getThreshold(), Matchers.equalTo(2.0f));
 
         settings = settingsBuilder();
-        settings.put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 0.2);
-        settings.put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 0.3);
-        settings.put(BalancedShardsAllocator.THRESHOLD_SETTING.getKey(), 2.0);
-        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());
-        service.applySettings(settings.build());
+        settings.put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString());
+        listeners[0].onRefreshSettings(settings.build());
         assertThat(allocator.getIndexBalance(), Matchers.equalTo(0.2f));
         assertThat(allocator.getShardBalance(), Matchers.equalTo(0.3f));
         assertThat(allocator.getThreshold(), Matchers.equalTo(2.0f));
 
         settings = settingsBuilder();
-        settings.put(BalancedShardsAllocator.INDEX_BALANCE_FACTOR_SETTING.getKey(), 0.5);
-        settings.put(BalancedShardsAllocator.SHARD_BALANCE_FACTOR_SETTING.getKey(), 0.1);
-        settings.put(BalancedShardsAllocator.THRESHOLD_SETTING.getKey(), 3.0);
-        service.applySettings(settings.build());
+        settings.put(BalancedShardsAllocator.SETTING_INDEX_BALANCE_FACTOR, 0.5);
+        settings.put(BalancedShardsAllocator.SETTING_SHARD_BALANCE_FACTOR, 0.1);
+        settings.put(BalancedShardsAllocator.SETTING_THRESHOLD, 3.0);
+        listeners[0].onRefreshSettings(settings.build());
         assertThat(allocator.getIndexBalance(), Matchers.equalTo(0.5f));
         assertThat(allocator.getShardBalance(), Matchers.equalTo(0.1f));
         assertThat(allocator.getThreshold(), Matchers.equalTo(3.0f));
@@ -311,7 +317,7 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
     public void testNoRebalanceOnPrimaryOverload() {
         Settings.Builder settings = settingsBuilder();
         AllocationService strategy = new AllocationService(settings.build(), randomAllocationDeciders(settings.build(),
-                new ClusterSettings(Settings.Builder.EMPTY_SETTINGS, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), getRandom()), new ShardsAllocators(settings.build(),
+                new NodeSettingsService(Settings.Builder.EMPTY_SETTINGS), getRandom()), new ShardsAllocators(settings.build(),
                 NoopGatewayAllocator.INSTANCE, new ShardsAllocator() {
 
             @Override
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
index 15a6ea0..8dad41d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java
@@ -46,7 +46,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
     private final ESLogger logger = Loggers.getLogger(ClusterRebalanceRoutingTests.class);
 
     public void testAlways() {
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());
 
         MetaData metaData = MetaData.builder()
@@ -132,7 +132,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
 
 
     public void testClusterPrimariesActive1() {
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_PRIMARIES_ACTIVE.toString()).build());
 
         MetaData metaData = MetaData.builder()
@@ -236,7 +236,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
     }
 
     public void testClusterPrimariesActive2() {
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_PRIMARIES_ACTIVE.toString()).build());
 
         MetaData metaData = MetaData.builder()
@@ -320,7 +320,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
     }
 
     public void testClusterAllActive1() {
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());
 
         MetaData metaData = MetaData.builder()
@@ -443,7 +443,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
     }
 
     public void testClusterAllActive2() {
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());
 
         MetaData metaData = MetaData.builder()
@@ -527,7 +527,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
     }
 
     public void testClusterAllActive3() {
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.INDICES_ALL_ACTIVE.toString()).build());
 
         MetaData metaData = MetaData.builder()
@@ -737,7 +737,7 @@ public class ClusterRebalanceRoutingTests extends ESAllocationTestCase {
 
     public void testRebalanceWhileShardFetching() {
         final AtomicBoolean hasFetches = new AtomicBoolean(true);
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build(), new NoopGatewayAllocator() {
             @Override
             public boolean allocateUnassigned(RoutingAllocation allocation) {
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java
index d807dc1..e16e7cc 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java
@@ -46,7 +46,7 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
     public void testSimpleDeadNodeOnStartedPrimaryShard() {
         AllocationService allocation = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("--> building initial routing table");
@@ -97,7 +97,7 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
     public void testDeadNodeWhileRelocatingOnToNode() {
         AllocationService allocation = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("--> building initial routing table");
@@ -171,7 +171,7 @@ public class DeadNodesAllocationTests extends ESAllocationTestCase {
     public void testDeadNodeWhileRelocatingOnFromNode() {
         AllocationService allocation = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("--> building initial routing table");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java
index affab78..3b242d8 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java
@@ -41,7 +41,7 @@ public class FailedNodeRoutingTests extends ESAllocationTestCase {
     private final ESLogger logger = Loggers.getLogger(FailedNodeRoutingTests.class);
 
     public void testSimpleFailedNodeTest() {
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());
 
         MetaData metaData = MetaData.builder()
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
index 8dffaca..9bfaf7e 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java
@@ -57,7 +57,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
     public void testFailedShardPrimaryRelocatingToAndFrom() {
         AllocationService allocation = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("--> building initial routing table");
@@ -145,7 +145,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
     public void testFailPrimaryStartedCheckReplicaElected() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("Building initial routing table");
@@ -226,7 +226,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
     public void testFirstAllocationFailureSingleNode() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("Building initial routing table");
@@ -282,7 +282,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
     public void testSingleShardMultipleAllocationFailures() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("Building initial routing table");
@@ -338,7 +338,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
     public void testFirstAllocationFailureTwoNodes() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("Building initial routing table");
@@ -398,7 +398,7 @@ public class FailedShardsRoutingTests extends ESAllocationTestCase {
     public void testRebalanceFailure() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         logger.info("Building initial routing table");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java
index d5f8134..aa6fdef 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java
@@ -48,7 +48,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1).build());
 
         logger.info("Building initial routing table");
@@ -178,7 +178,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1).build());
 
         logger.info("Building initial routing table");
@@ -340,7 +340,7 @@ public class IndexBalanceTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1).build());
 
         logger.info("Building initial routing table");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java
index c00cd84..2fe1d85 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java
@@ -39,15 +39,10 @@ import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 
-import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
-import static org.elasticsearch.cluster.routing.ShardRoutingState.UNASSIGNED;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -58,7 +53,7 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
     public void testDoNotAllocateFromPrimary() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build());
 
@@ -172,7 +167,7 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
     public void testRandom() {
         AllocationService service = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build());
 
@@ -199,7 +194,7 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
             DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder();
             int numNodes = between(1, 20);
             if (nodes.size() > numNodes) {
-                Collections.shuffle(nodes, getRandom());
+                Collections.shuffle(nodes, random());
                 nodes = nodes.subList(0, numNodes);
             } else {
                 for (int j = nodes.size(); j < numNodes; j++) {
@@ -221,7 +216,7 @@ public class NodeVersionAllocationDeciderTests extends ESAllocationTestCase {
     public void testRollingRestart() {
         AllocationService service = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build());
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
index 18725a0..fbc7425 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java
@@ -57,7 +57,7 @@ public class RebalanceAfterActiveTests extends ESAllocationTestCase {
 
         AllocationService strategy = createAllocationService(settingsBuilder()
                         .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                        .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                        .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                         .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                         .build(),
                 new ClusterInfoService() {
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java
index eec1b48..eca2a22 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java
@@ -47,7 +47,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1).build());
 
         logger.info("Building initial routing table");
@@ -119,7 +119,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1).build());
 
         logger.info("Building initial routing table");
@@ -211,7 +211,7 @@ public class RoutingNodesIntegrityTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 1)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 3)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1).build());
 
         logger.info("Building initial routing table");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java
index e1586c4..f096ab0 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java
@@ -41,7 +41,7 @@ public class ShardVersioningTests extends ESAllocationTestCase {
     private final ESLogger logger = Loggers.getLogger(ShardVersioningTests.class);
 
     public void testSimple() {
-        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(),
+        AllocationService strategy = createAllocationService(settingsBuilder().put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
                 ClusterRebalanceAllocationDecider.ClusterRebalanceType.ALWAYS.toString()).build());
 
         MetaData metaData = MetaData.builder()
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java
index c0f0c0c..11d41a6 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java
@@ -90,7 +90,7 @@ public class ShardsLimitAllocationTests extends ESAllocationTestCase {
     public void testClusterLevelShardsLimitAllocate() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING.getKey(), 1)
+                .put(ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE, 1)
                 .build());
 
         logger.info("Building initial routing table");
@@ -126,7 +126,7 @@ public class ShardsLimitAllocationTests extends ESAllocationTestCase {
         // Bump the cluster total shards to 2
         strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING.getKey(), 2)
+                .put(ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE, 2)
                 .build());
 
         logger.info("Do another reroute, make sure shards are now allocated");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java
index 29ef451..ed44b84 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java
@@ -211,7 +211,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
     public void testMultiIndexEvenDistribution() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build());
 
@@ -323,7 +323,7 @@ public class SingleShardNoReplicasRoutingTests extends ESAllocationTestCase {
     public void testMultiIndexUnevenNodes() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build());
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java
index aec81a6..671cce0 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java
@@ -50,7 +50,7 @@ public class TenShardsOneReplicaRoutingTests extends ESAllocationTestCase {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.node_concurrent_recoveries", 10)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .put("cluster.routing.allocation.balance.index", 0.0f)
                 .put("cluster.routing.allocation.balance.replica", 1.0f)
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
index 5377d09..a739f30 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
@@ -60,9 +60,9 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
     public void testDiskThreshold() {
         Settings diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), 0.7)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), 0.8).build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.8).build();
 
         ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
         usagesBuilder.put("node1", new DiskUsage("node1", "node1", "/dev/null", 100, 10)); // 90% used
@@ -96,7 +96,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         };
         AllocationService strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -170,9 +170,9 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         // Set the high threshold to 70 instead of 80
         // node2 now should not have new shards allocated to it, but shards can remain
         diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "60%")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), 0.7).build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "60%")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.7).build();
 
         deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -181,7 +181,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -201,9 +201,9 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         // Set the high threshold to 60 instead of 70
         // node2 now should not have new shards allocated to it, and shards cannot remain
         diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), 0.5)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), 0.6).build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.5)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.6).build();
 
         deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -212,7 +212,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -254,9 +254,9 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
     public void testDiskThresholdWithAbsoluteSizes() {
         Settings diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "30b")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "9b").build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "30b")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "9b").build();
 
         ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
         usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 10)); // 90% used
@@ -292,7 +292,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         AllocationService strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -349,7 +349,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         };
         strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -405,9 +405,9 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         // Set the high threshold to 70 instead of 80
         // node2 now should not have new shards allocated to it, but shards can remain
         diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "40b")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "30b").build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "40b")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "30b").build();
 
         deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -416,7 +416,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -436,9 +436,9 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         // Set the high threshold to 60 instead of 70
         // node2 now should not have new shards allocated to it, and shards cannot remain
         diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "50b")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "40b").build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "50b")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "40b").build();
 
         deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -447,7 +447,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -522,9 +522,9 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
     public void testDiskThresholdWithShardSizes() {
         Settings diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), 0.7)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "71%").build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "71%").build();
 
         ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
         usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 31)); // 69% used
@@ -556,7 +556,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         AllocationService strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -589,9 +589,9 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
     public void testUnknownDiskUsage() {
         Settings diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), 0.7)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), 0.85).build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.85).build();
 
         ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
         usagesBuilder.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 50)); // 50% used
@@ -624,7 +624,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         AllocationService strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -688,10 +688,10 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
     public void testShardRelocationsTakenIntoAccount() {
         Settings diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), 0.7)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), 0.8).build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.8).build();
 
         ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
         usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 40)); // 60% used
@@ -727,7 +727,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         AllocationService strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
 
@@ -794,10 +794,10 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
     public void testCanRemainWithShardRelocatingAway() {
         Settings diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "60%")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "70%").build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "60%")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "70%").build();
 
         // We have an index with 2 primary shards each taking 40 bytes. Each node has 100 bytes available
         ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
@@ -889,7 +889,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         )));
         AllocationService strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
         // Ensure that the reroute call doesn't alter the routing table, since the first primary is relocating away
@@ -906,10 +906,10 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
     public void testForSingleDataNode() {
         Settings diskSettings = settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING.getKey(), true)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "60%")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "70%").build();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, true)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "60%")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "70%").build();
 
         ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
         usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 100)); // 0% used
@@ -989,7 +989,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
 
         AllocationService strategy = new AllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", -1)
                 .build(), deciders, makeShardsAllocators(), cis);
         RoutingAllocation.Result result = strategy.reroute(clusterState, "reroute");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
index 52e88ea..a386883 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
@@ -28,12 +28,12 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.Arrays;
@@ -45,7 +45,7 @@ import static org.hamcrest.CoreMatchers.equalTo;
  */
 public class DiskThresholdDeciderUnitTests extends ESTestCase {
     public void testDynamicSettings() {
-        ClusterSettings nss = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
+        NodeSettingsService nss = new NodeSettingsService(Settings.EMPTY);
 
         ClusterInfoService cis = EmptyClusterInfoService.INSTANCE;
         DiskThresholdDecider decider = new DiskThresholdDecider(Settings.EMPTY, nss, cis, null);
@@ -59,15 +59,18 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         assertTrue(decider.isEnabled());
         assertTrue(decider.isIncludeRelocations());
 
+        DiskThresholdDecider.ApplySettings applySettings = decider.newApplySettings();
+
         Settings newSettings = Settings.builder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED_SETTING.getKey(), false)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS_SETTING.getKey(), false)
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "70%")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "500mb")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), "30s")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, false)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, false)
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "70%")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "500mb")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL, "30s")
                 .build();
 
-        nss.applySettings(newSettings);
+        applySettings.onRefreshSettings(newSettings);
+
         assertThat("high threshold bytes should be unset",
                 decider.getFreeBytesThresholdHigh(), equalTo(ByteSizeValue.parseBytesSizeValue("0b", "test")));
         assertThat("high threshold percentage should be changed",
@@ -83,7 +86,7 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
     }
 
     public void testCanAllocateUsesMaxAvailableSpace() {
-        ClusterSettings nss = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
+        NodeSettingsService nss = new NodeSettingsService(Settings.EMPTY);
         ClusterInfoService cis = EmptyClusterInfoService.INSTANCE;
         DiskThresholdDecider decider = new DiskThresholdDecider(Settings.EMPTY, nss, cis, null);
 
@@ -124,7 +127,7 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
     }
 
     public void testCanRemainUsesLeastAvailableSpace() {
-        ClusterSettings nss = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
+        NodeSettingsService nss = new NodeSettingsService(Settings.EMPTY);
         ClusterInfoService cis = EmptyClusterInfoService.INSTANCE;
         DiskThresholdDecider decider = new DiskThresholdDecider(Settings.EMPTY, nss, cis, null);
         ImmutableOpenMap.Builder<ShardRouting, String> shardRoutingMap = ImmutableOpenMap.builder();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDeciderIT.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDeciderIT.java
index be64aaf..940634a 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDeciderIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDeciderIT.java
@@ -37,7 +37,7 @@ public class EnableAllocationDeciderIT extends ESIntegTestCase {
 
     public void testEnableRebalance() throws InterruptedException {
         final String firstNode = internalCluster().startNode();
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), EnableAllocationDecider.Rebalance.NONE)).get();
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, EnableAllocationDecider.Rebalance.NONE)).get();
         // we test with 2 shards since otherwise it's pretty fragile if there are difference in the num or shards such that
         // all shards are relocated to the second node which is not what we want here. It's solely a test for the settings to take effect
         final int numShards = 2;
@@ -64,7 +64,7 @@ public class EnableAllocationDeciderIT extends ESIntegTestCase {
         assertThat("index: [test] expected to be rebalanced on both nodes", test.size(), equalTo(2));
 
         // flip the cluster wide setting such that we can also balance for index test_1 eventually we should have one shard of each index on each node
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), randomBoolean() ? EnableAllocationDecider.Rebalance.PRIMARIES : EnableAllocationDecider.Rebalance.ALL)).get();
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, randomBoolean() ? EnableAllocationDecider.Rebalance.PRIMARIES : EnableAllocationDecider.Rebalance.ALL)).get();
         logger.info("--> balance index [test_1]");
         client().admin().cluster().prepareReroute().get();
         ensureGreen("test_1");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java
index 1bdc390..0049a12 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.cluster.routing.allocation.decider;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
 
 import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterChangedEvent;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -31,10 +32,10 @@ import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.Allocation;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.Rebalance;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESAllocationTestCase;
 
 import java.util.EnumSet;
@@ -43,8 +44,8 @@ import java.util.List;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
-import static org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING;
-import static org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING;
+import static org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE;
+import static org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE;
 import static org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
@@ -57,7 +58,7 @@ public class EnableAllocationTests extends ESAllocationTestCase {
 
     public void testClusterEnableNone() {
         AllocationService strategy = createAllocationService(settingsBuilder()
-                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), Allocation.NONE.name())
+                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE, Allocation.NONE.name())
                 .build());
 
         logger.info("Building initial routing table");
@@ -85,7 +86,7 @@ public class EnableAllocationTests extends ESAllocationTestCase {
 
     public void testClusterEnableOnlyPrimaries() {
         AllocationService strategy = createAllocationService(settingsBuilder()
-                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), Allocation.PRIMARIES.name())
+                .put(CLUSTER_ROUTING_ALLOCATION_ENABLE, Allocation.PRIMARIES.name())
                 .build());
 
         logger.info("Building initial routing table");
@@ -158,11 +159,11 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         final boolean useClusterSetting = randomBoolean();
         final Rebalance allowedOnes = RandomPicks.randomFrom(getRandom(), EnumSet.of(Rebalance.PRIMARIES, Rebalance.REPLICAS, Rebalance.ALL));
         Settings build = settingsBuilder()
-                .put(CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), useClusterSetting ? Rebalance.NONE: RandomPicks.randomFrom(getRandom(), Rebalance.values())) // index settings override cluster settings
-                .put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING.getKey(), 3)
+                .put(CLUSTER_ROUTING_REBALANCE_ENABLE, useClusterSetting ? Rebalance.NONE: RandomPicks.randomFrom(getRandom(), Rebalance.values())) // index settings override cluster settings
+                .put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, 3)
                 .build();
-        ClusterSettings clusterSettings = new ClusterSettings(build, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-        AllocationService strategy = createAllocationService(build, clusterSettings, getRandom());
+        NodeSettingsService nodeSettingsService = new NodeSettingsService(build);
+        AllocationService strategy = createAllocationService(build, nodeSettingsService, getRandom());
         Settings indexSettings = useClusterSetting ? Settings.EMPTY : settingsBuilder().put(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE, Rebalance.NONE).build();
 
         logger.info("Building initial routing table");
@@ -212,7 +213,7 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         if (useClusterSetting) {
             prevState = clusterState;
             clusterState = ClusterState.builder(clusterState).metaData(MetaData.builder(metaData).transientSettings(settingsBuilder()
-                .put(CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), allowedOnes)
+                .put(CLUSTER_ROUTING_REBALANCE_ENABLE, allowedOnes)
                 .build())).build();
         } else {
             prevState = clusterState;
@@ -223,7 +224,7 @@ public class EnableAllocationTests extends ESAllocationTestCase {
                     .build();
 
         }
-        clusterSettings.applySettings(clusterState.metaData().settings());
+        nodeSettingsService.clusterChanged(new ClusterChangedEvent("foo", clusterState, prevState));
         routingTable = strategy.reroute(clusterState, "reroute").routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         assertThat("expected 6 shards to be started 2 to relocate useClusterSettings: " + useClusterSetting, clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(6));
@@ -260,11 +261,11 @@ public class EnableAllocationTests extends ESAllocationTestCase {
     public void testEnableClusterBalanceNoReplicas() {
         final boolean useClusterSetting = randomBoolean();
         Settings build = settingsBuilder()
-                .put(CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), useClusterSetting ? Rebalance.NONE: RandomPicks.randomFrom(getRandom(), Rebalance.values())) // index settings override cluster settings
-                .put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE_SETTING.getKey(), 3)
+                .put(CLUSTER_ROUTING_REBALANCE_ENABLE, useClusterSetting ? Rebalance.NONE: RandomPicks.randomFrom(getRandom(), Rebalance.values())) // index settings override cluster settings
+                .put(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, 3)
                 .build();
-        ClusterSettings clusterSettings = new ClusterSettings(build, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-        AllocationService strategy = createAllocationService(build, clusterSettings, getRandom());
+        NodeSettingsService nodeSettingsService = new NodeSettingsService(build);
+        AllocationService strategy = createAllocationService(build, nodeSettingsService, getRandom());
         Settings indexSettings = useClusterSetting ? Settings.EMPTY : settingsBuilder().put(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE, Rebalance.NONE).build();
 
         logger.info("Building initial routing table");
@@ -306,7 +307,7 @@ public class EnableAllocationTests extends ESAllocationTestCase {
         if (useClusterSetting) {
             prevState = clusterState;
             clusterState = ClusterState.builder(clusterState).metaData(MetaData.builder(metaData).transientSettings(settingsBuilder()
-                    .put(CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), randomBoolean() ? Rebalance.PRIMARIES : Rebalance.ALL)
+                    .put(CLUSTER_ROUTING_REBALANCE_ENABLE, randomBoolean() ? Rebalance.PRIMARIES : Rebalance.ALL)
                     .build())).build();
         } else {
             prevState = clusterState;
@@ -314,7 +315,7 @@ public class EnableAllocationTests extends ESAllocationTestCase {
             clusterState = ClusterState.builder(clusterState).metaData(MetaData.builder(metaData).removeAllIndices()
                     .put(IndexMetaData.builder(meta).settings(settingsBuilder().put(meta.getSettings()).put(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE, randomBoolean() ? Rebalance.PRIMARIES : Rebalance.ALL).build()))).build();
         }
-        clusterSettings.applySettings(clusterState.metaData().settings());
+        nodeSettingsService.clusterChanged(new ClusterChangedEvent("foo", clusterState, prevState));
         routingTable = strategy.reroute(clusterState, "reroute").routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
         assertThat("expected 4 primaries to be started and 2 to relocate useClusterSettings: " + useClusterSetting, clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(4));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesIT.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesIT.java
index a17017f..126799f 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesIT.java
@@ -28,7 +28,6 @@ import org.elasticsearch.cluster.InternalClusterInfoService;
 import org.elasticsearch.cluster.MockInternalClusterInfoService;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 
@@ -48,11 +47,21 @@ import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 public class MockDiskUsagesIT extends ESIntegTestCase {
 
     @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return Settings.builder()
+                .put(super.nodeSettings(nodeOrdinal))
+                        // Update more frequently
+                .put(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, "1s")
+                .build();
+    }
+
+    @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         // Use the mock internal cluster info service, which has fake-able disk usages
         return pluginList(MockInternalClusterInfoService.TestPlugin.class);
     }
 
+    //@TestLogging("org.elasticsearch.cluster:TRACE,org.elasticsearch.cluster.routing.allocation.decider:TRACE")
     public void testRerouteOccursOnDiskPassingHighWatermark() throws Exception {
         List<String> nodes = internalCluster().startNodesAsync(3).get();
 
@@ -68,16 +77,15 @@ public class MockDiskUsagesIT extends ESIntegTestCase {
         // Start with all nodes at 50% usage
         final MockInternalClusterInfoService cis = (MockInternalClusterInfoService)
                 internalCluster().getInstance(ClusterInfoService.class, internalCluster().getMasterName());
-        cis.setUpdateFrequency(TimeValue.timeValueMillis(200));
-        cis.onMaster();
         cis.setN1Usage(nodes.get(0), new DiskUsage(nodes.get(0), "n1", "/dev/null", 100, 50));
         cis.setN2Usage(nodes.get(1), new DiskUsage(nodes.get(1), "n2", "/dev/null", 100, 50));
         cis.setN3Usage(nodes.get(2), new DiskUsage(nodes.get(2), "n3", "/dev/null", 100, 50));
 
         client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder()
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), randomFrom("20b", "80%"))
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), randomFrom("10b", "90%"))
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), "1ms")).get();
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, randomFrom("20b", "80%"))
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, randomFrom("10b", "90%"))
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL, "1ms")).get();
+
         // Create an index with 10 shards so we can check allocation for it
         prepareCreate("test").setSettings(settingsBuilder()
                 .put("number_of_shards", 10)
diff --git a/core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java b/core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java
index 3a028fe..65d5b0b 100644
--- a/core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsIT.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.settings;
 
-import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsRequestBuilder;
 import org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsResponse;
 import org.elasticsearch.cluster.ClusterName;
@@ -33,8 +32,8 @@ import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.index.store.IndexStoreConfig;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.hamcrest.Matchers;
 
-import static org.elasticsearch.common.inject.matcher.Matchers.not;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.ESIntegTestCase.Scope.TEST;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
@@ -49,142 +48,22 @@ public class ClusterSettingsIT extends ESIntegTestCase {
     public void testClusterNonExistingSettingsUpdate() {
         String key1 = "no_idea_what_you_are_talking_about";
         int value1 = 10;
-        try {
-            client().admin().cluster()
-                    .prepareUpdateSettings()
-                    .setTransientSettings(Settings.builder().put(key1, value1).build())
-                    .get();
-            fail("bogus value");
-        } catch (IllegalArgumentException ex) {
-            assertEquals(ex.getMessage(), "transient setting [no_idea_what_you_are_talking_about], not dynamically updateable");
-        }
-    }
-
-    public void testDeleteIsAppliedFirst() {
-        DiscoverySettings discoverySettings = internalCluster().getInstance(DiscoverySettings.class);
-
-        assertEquals(discoverySettings.getPublishTimeout(), DiscoverySettings.PUBLISH_TIMEOUT_SETTING.get(Settings.EMPTY));
-        assertTrue(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY));
-
-        ClusterUpdateSettingsResponse response = client().admin().cluster()
-            .prepareUpdateSettings()
-            .setTransientSettings(Settings.builder()
-                .put(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey(), false)
-                .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s").build())
-            .get();
-
-        assertAcked(response);
-        assertEquals(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()), "1s");
-        assertTrue(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY));
-        assertFalse(response.getTransientSettings().getAsBoolean(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey(), null));
-
-        response = client().admin().cluster()
-            .prepareUpdateSettings()
-            .setTransientSettings(Settings.builder().putNull((randomBoolean() ? "discovery.zen.*" : "*")).put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "2s"))
-            .get();
-        assertEquals(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()), "2s");
-        assertNull(response.getTransientSettings().getAsBoolean(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey(), null));
-    }
-
-    public void testResetClusterSetting() {
-        DiscoverySettings discoverySettings = internalCluster().getInstance(DiscoverySettings.class);
-
-        assertThat(discoverySettings.getPublishTimeout(), equalTo(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.get(Settings.EMPTY)));
-        assertThat(discoverySettings.getPublishDiff(), equalTo(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY)));
 
         ClusterUpdateSettingsResponse response = client().admin().cluster()
                 .prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s").build())
+                .setTransientSettings(Settings.builder().put(key1, value1).build())
                 .get();
 
         assertAcked(response);
-        assertThat(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()), equalTo("1s"));
-        assertThat(discoverySettings.getPublishTimeout().seconds(), equalTo(1l));
-        assertThat(discoverySettings.getPublishDiff(), equalTo(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY)));
-
-
-        response = client().admin().cluster()
-                .prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().putNull(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()))
-                .get();
-
-        assertAcked(response);
-        assertNull(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()));
-        assertThat(discoverySettings.getPublishTimeout(), equalTo(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.get(Settings.EMPTY)));
-        assertThat(discoverySettings.getPublishDiff(), equalTo(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY)));
-
-        response = client().admin().cluster()
-                .prepareUpdateSettings()
-                .setTransientSettings(Settings.builder()
-                        .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s")
-                        .put(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey(), false).build())
-                .get();
-
-        assertAcked(response);
-        assertThat(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()), equalTo("1s"));
-        assertThat(discoverySettings.getPublishTimeout().seconds(), equalTo(1l));
-        assertFalse(discoverySettings.getPublishDiff());
-        response = client().admin().cluster()
-                .prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().putNull((randomBoolean() ? "discovery.zen.*" : "*")))
-                .get();
-
-        assertNull(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()));
-        assertNull(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey()));
-        assertThat(discoverySettings.getPublishTimeout(), equalTo(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.get(Settings.EMPTY)));
-        assertThat(discoverySettings.getPublishDiff(), equalTo(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY)));
-
-        // now persistent
-        response = client().admin().cluster()
-                .prepareUpdateSettings()
-                .setPersistentSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s").build())
-                .get();
-
-        assertAcked(response);
-        assertThat(response.getPersistentSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()), equalTo("1s"));
-        assertThat(discoverySettings.getPublishTimeout().seconds(), equalTo(1l));
-        assertThat(discoverySettings.getPublishDiff(), equalTo(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY)));
-
-
-        response = client().admin().cluster()
-                .prepareUpdateSettings()
-                .setPersistentSettings(Settings.builder().putNull((DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey())))
-                .get();
-
-        assertAcked(response);
-        assertNull(response.getPersistentSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()));
-        assertThat(discoverySettings.getPublishTimeout(), equalTo(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.get(Settings.EMPTY)));
-        assertThat(discoverySettings.getPublishDiff(), equalTo(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY)));
-
-
-        response = client().admin().cluster()
-                .prepareUpdateSettings()
-                .setPersistentSettings(Settings.builder()
-                        .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s")
-                        .put(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey(), false).build())
-                .get();
-
-        assertAcked(response);
-        assertThat(response.getPersistentSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()), equalTo("1s"));
-        assertThat(discoverySettings.getPublishTimeout().seconds(), equalTo(1l));
-        assertFalse(discoverySettings.getPublishDiff());
-        response = client().admin().cluster()
-                .prepareUpdateSettings()
-                .setPersistentSettings(Settings.builder().putNull((randomBoolean() ? "discovery.zen.*" : "*")))
-                .get();
-
-        assertNull(response.getPersistentSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()));
-        assertNull(response.getPersistentSettings().getAsMap().get(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey()));
-        assertThat(discoverySettings.getPublishTimeout(), equalTo(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.get(Settings.EMPTY)));
-        assertThat(discoverySettings.getPublishDiff(), equalTo(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.get(Settings.EMPTY)));
+        assertThat(response.getTransientSettings().getAsMap().entrySet(), Matchers.emptyIterable());
     }
 
     public void testClusterSettingsUpdateResponse() {
-        String key1 = IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING.getKey();
+        String key1 = IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC;
         int value1 = 10;
 
-        String key2 = EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey();
-        String value2 =  EnableAllocationDecider.Allocation.NONE.name();
+        String key2 = EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE;
+        boolean value2 = false;
 
         Settings transientSettings1 = Settings.builder().put(key1, value1, ByteSizeUnit.BYTES).build();
         Settings persistentSettings1 = Settings.builder().put(key2, value2).build();
@@ -239,45 +118,39 @@ public class ClusterSettingsIT extends ESIntegTestCase {
 
         DiscoverySettings discoverySettings = internalCluster().getInstance(DiscoverySettings.class);
 
-        assertThat(discoverySettings.getPublishTimeout(), equalTo(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.get(Settings.EMPTY)));
+        assertThat(discoverySettings.getPublishTimeout(), equalTo(DiscoverySettings.DEFAULT_PUBLISH_TIMEOUT));
 
         ClusterUpdateSettingsResponse response = client().admin().cluster()
                 .prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s").build())
+                .setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT, "1s").build())
                 .get();
 
         assertAcked(response);
-        assertThat(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey()), equalTo("1s"));
+        assertThat(response.getTransientSettings().getAsMap().get(DiscoverySettings.PUBLISH_TIMEOUT), equalTo("1s"));
         assertThat(discoverySettings.getPublishTimeout().seconds(), equalTo(1l));
 
-        try {
-            client().admin().cluster()
-                    .prepareUpdateSettings()
-                    .setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "whatever").build())
-                    .get();
-            fail("bogus value");
-        } catch (ElasticsearchParseException ex) {
-            assertEquals(ex.getMessage(), "Failed to parse setting [discovery.zen.commit_timeout] with value [whatever] as a time value: unit is missing or unrecognized");
-        }
+        response = client().admin().cluster()
+                .prepareUpdateSettings()
+                .setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT, "whatever").build())
+                .get();
 
+        assertAcked(response);
+        assertThat(response.getTransientSettings().getAsMap().entrySet(), Matchers.emptyIterable());
         assertThat(discoverySettings.getPublishTimeout().seconds(), equalTo(1l));
 
-        try {
-            client().admin().cluster()
-                    .prepareUpdateSettings()
-                    .setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), -1).build())
-                    .get();
-            fail("bogus value");
-        } catch (ElasticsearchParseException ex) {
-            assertEquals(ex.getMessage(), "Failed to parse value [-1] for setting [discovery.zen.publish_timeout] must be >= 0s");
-        }
+        response = client().admin().cluster()
+                .prepareUpdateSettings()
+                .setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT, -1).build())
+                .get();
 
+        assertAcked(response);
+        assertThat(response.getTransientSettings().getAsMap().entrySet(), Matchers.emptyIterable());
         assertThat(discoverySettings.getPublishTimeout().seconds(), equalTo(1l));
     }
 
     public void testClusterUpdateSettingsWithBlocks() {
         String key1 = "cluster.routing.allocation.enable";
-        Settings transientSettings = Settings.builder().put(key1, EnableAllocationDecider.Allocation.NONE.name()).build();
+        Settings transientSettings = Settings.builder().put(key1, false).build();
 
         String key2 = "cluster.routing.allocation.node_concurrent_recoveries";
         Settings persistentSettings = Settings.builder().put(key2, "5").build();
@@ -292,7 +165,7 @@ public class ClusterSettingsIT extends ESIntegTestCase {
             assertBlocked(request, MetaData.CLUSTER_READ_ONLY_BLOCK);
 
             // But it's possible to update the settings to update the "cluster.blocks.read_only" setting
-            Settings settings = settingsBuilder().put(MetaData.SETTING_READ_ONLY_SETTING.getKey(), false).build();
+            Settings settings = settingsBuilder().put(MetaData.SETTING_READ_ONLY, false).build();
             assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings).get());
 
         } finally {
diff --git a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
index 6094d49..c5a695d 100644
--- a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
@@ -225,7 +225,7 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
     public void testAttributePreferenceRouting() {
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.awareness.attributes", "rack_id,zone")
                 .build());
 
@@ -280,7 +280,7 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
     public void testNodeSelectorRouting(){
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 10)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .build());
 
         MetaData metaData = MetaData.builder()
diff --git a/core/src/test/java/org/elasticsearch/common/breaker/MemoryCircuitBreakerTests.java b/core/src/test/java/org/elasticsearch/common/breaker/MemoryCircuitBreakerTests.java
index bb9d23d..fa4ce35 100644
--- a/core/src/test/java/org/elasticsearch/common/breaker/MemoryCircuitBreakerTests.java
+++ b/core/src/test/java/org/elasticsearch/common/breaker/MemoryCircuitBreakerTests.java
@@ -19,12 +19,12 @@
 
 package org.elasticsearch.common.breaker;
 
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.indices.breaker.BreakerSettings;
 import org.elasticsearch.indices.breaker.CircuitBreakerService;
 import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -87,7 +87,7 @@ public class MemoryCircuitBreakerTests extends ESTestCase {
         final AtomicReference<Throwable> lastException = new AtomicReference<>(null);
 
         final AtomicReference<ChildMemoryCircuitBreaker> breakerRef = new AtomicReference<>(null);
-        final CircuitBreakerService service = new HierarchyCircuitBreakerService(Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS)) {
+        final CircuitBreakerService service = new HierarchyCircuitBreakerService(Settings.EMPTY, new NodeSettingsService(Settings.EMPTY)) {
 
             @Override
             public CircuitBreaker getBreaker(String name) {
@@ -147,7 +147,7 @@ public class MemoryCircuitBreakerTests extends ESTestCase {
 
         final AtomicInteger parentTripped = new AtomicInteger(0);
         final AtomicReference<ChildMemoryCircuitBreaker> breakerRef = new AtomicReference<>(null);
-        final CircuitBreakerService service = new HierarchyCircuitBreakerService(Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS)) {
+        final CircuitBreakerService service = new HierarchyCircuitBreakerService(Settings.EMPTY, new NodeSettingsService(Settings.EMPTY)) {
 
             @Override
             public CircuitBreaker getBreaker(String name) {
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
index fcd2f7d..17345fd 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
@@ -17,20 +17,14 @@
  * under the License.
  */
 package org.elasticsearch.common.lucene;
+
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoDeletionPolicy;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.index.Term;
+import org.apache.lucene.index.*;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
@@ -41,11 +35,7 @@ import org.apache.lucene.util.Version;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
+import java.util.*;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicBoolean;
 
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java b/core/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
index 96715a0..ad811a3 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
@@ -24,13 +24,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.Term;
+import org.apache.lucene.index.*;
 import org.apache.lucene.queries.TermsQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.Directory;
@@ -42,14 +36,7 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.After;
 import org.junit.Before;
 
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.is;
@@ -201,7 +188,7 @@ public class FreqTermsEnumTests extends ESTestCase {
         for (int i = 0; i < cycles; i++) {
             List<String> terms = new ArrayList<>(Arrays.asList(this.terms));
 
-           Collections.shuffle(terms, getRandom());
+           Collections.shuffle(terms, random());
             for (String term : terms) {
                 if (!termsEnum.seekExact(new BytesRef(term))) {
                     assertThat("term : " + term, reference.get(term).docFreq, is(0));
diff --git a/core/src/test/java/org/elasticsearch/common/network/NetworkServiceTests.java b/core/src/test/java/org/elasticsearch/common/network/NetworkServiceTests.java
index 13c2211..7ec4756 100644
--- a/core/src/test/java/org/elasticsearch/common/network/NetworkServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/common/network/NetworkServiceTests.java
@@ -24,14 +24,16 @@ import org.elasticsearch.test.ESTestCase;
 
 import java.net.InetAddress;
 
+import static org.hamcrest.Matchers.is;
+
 /**
  * Tests for network service... try to keep them safe depending upon configuration
  * please don't actually bind to anything, just test the addresses.
  */
 public class NetworkServiceTests extends ESTestCase {
 
-    /** 
-     * ensure exception if we bind to multicast ipv4 address 
+    /**
+     * ensure exception if we bind to multicast ipv4 address
      */
     public void testBindMulticastV4() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -42,9 +44,8 @@ public class NetworkServiceTests extends ESTestCase {
             assertTrue(e.getMessage().contains("invalid: multicast"));
         }
     }
-    
-    /** 
-     * ensure exception if we bind to multicast ipv6 address 
+    /**
+     * ensure exception if we bind to multicast ipv6 address
      */
     public void testBindMulticastV6() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -55,9 +56,9 @@ public class NetworkServiceTests extends ESTestCase {
             assertTrue(e.getMessage().contains("invalid: multicast"));
         }
     }
-    
-    /** 
-     * ensure exception if we publish to multicast ipv4 address 
+
+    /**
+     * ensure exception if we publish to multicast ipv4 address
      */
     public void testPublishMulticastV4() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -68,9 +69,9 @@ public class NetworkServiceTests extends ESTestCase {
             assertTrue(e.getMessage().contains("invalid: multicast"));
         }
     }
-    
-    /** 
-     * ensure exception if we publish to multicast ipv6 address 
+
+    /**
+     * ensure exception if we publish to multicast ipv6 address
      */
     public void testPublishMulticastV6() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -82,24 +83,24 @@ public class NetworkServiceTests extends ESTestCase {
         }
     }
 
-    /** 
-     * ensure specifying wildcard ipv4 address will bind to all interfaces 
+    /**
+     * ensure specifying wildcard ipv4 address will bind to all interfaces
      */
     public void testBindAnyLocalV4() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
         assertEquals(InetAddress.getByName("0.0.0.0"), service.resolveBindHostAddresses(new String[] { "0.0.0.0" })[0]);
     }
-    
-    /** 
-     * ensure specifying wildcard ipv6 address will bind to all interfaces 
+
+    /**
+     * ensure specifying wildcard ipv6 address will bind to all interfaces
      */
     public void testBindAnyLocalV6() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
         assertEquals(InetAddress.getByName("::"), service.resolveBindHostAddresses(new String[] { "::" })[0]);
     }
 
-    /** 
-     * ensure specifying wildcard ipv4 address selects reasonable publish address 
+    /**
+     * ensure specifying wildcard ipv4 address selects reasonable publish address
      */
     public void testPublishAnyLocalV4() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
@@ -107,12 +108,34 @@ public class NetworkServiceTests extends ESTestCase {
         assertFalse(address.isAnyLocalAddress());
     }
 
-    /** 
-     * ensure specifying wildcard ipv6 address selects reasonable publish address 
+    /**
+     * ensure specifying wildcard ipv6 address selects reasonable publish address
      */
     public void testPublishAnyLocalV6() throws Exception {
         NetworkService service = new NetworkService(Settings.EMPTY);
         InetAddress address = service.resolvePublishHostAddresses(new String[] { "::" });
         assertFalse(address.isAnyLocalAddress());
     }
+
+    /**
+     * ensure we can bind to multiple addresses
+     */
+    public void testBindMultipleAddresses() throws Exception {
+        NetworkService service = new NetworkService(Settings.EMPTY);
+        InetAddress[] addresses = service.resolveBindHostAddresses(new String[]{"127.0.0.1", "127.0.0.2"});
+        assertThat(addresses.length, is(2));
+    }
+
+    /**
+     * ensure we can't bind to multiple addresses when using wildcard
+     */
+    public void testBindMultipleAddressesWithWildcard() throws Exception {
+        NetworkService service = new NetworkService(Settings.EMPTY);
+        try {
+            service.resolveBindHostAddresses(new String[]{"0.0.0.0", "127.0.0.1"});
+            fail("should have hit exception");
+        } catch (IllegalArgumentException e) {
+            assertTrue(e.getMessage().contains("is wildcard, but multiple addresses specified"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/regex/RegexTests.java b/core/src/test/java/org/elasticsearch/common/regex/RegexTests.java
index ee0756f..85c2be7 100644
--- a/core/src/test/java/org/elasticsearch/common/regex/RegexTests.java
+++ b/core/src/test/java/org/elasticsearch/common/regex/RegexTests.java
@@ -28,7 +28,7 @@ import static org.hamcrest.Matchers.equalTo;
 public class RegexTests extends ESTestCase {
     public void testFlags() {
         String[] supportedFlags = new String[]{"CASE_INSENSITIVE", "MULTILINE", "DOTALL", "UNICODE_CASE", "CANON_EQ", "UNIX_LINES",
-                "LITERAL", "COMMENTS", "UNICODE_CHAR_CLASS"};
+                "LITERAL", "COMMENTS", "UNICODE_CHAR_CLASS", "UNICODE_CHARACTER_CLASS"};
         int[] flags = new int[]{Pattern.CASE_INSENSITIVE, Pattern.MULTILINE, Pattern.DOTALL, Pattern.UNICODE_CASE, Pattern.CANON_EQ,
                 Pattern.UNIX_LINES, Pattern.LITERAL, Pattern.COMMENTS, Regex.UNICODE_CHARACTER_CLASS};
         Random random = getRandom();
diff --git a/core/src/test/java/org/elasticsearch/common/settings/ScopedSettingsTests.java b/core/src/test/java/org/elasticsearch/common/settings/ScopedSettingsTests.java
deleted file mode 100644
index 9c5320b..0000000
--- a/core/src/test/java/org/elasticsearch/common/settings/ScopedSettingsTests.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.settings;
-
-import org.elasticsearch.cluster.routing.allocation.decider.FilterAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.ShardsLimitAllocationDecider;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.concurrent.atomic.AtomicInteger;
-
-public class ScopedSettingsTests extends ESTestCase {
-
-    public void testAddConsumer() {
-        Setting<Integer> testSetting = Setting.intSetting("foo.bar", 1, true, Setting.Scope.CLUSTER);
-        Setting<Integer> testSetting2 = Setting.intSetting("foo.bar.baz", 1, true, Setting.Scope.CLUSTER);
-        AbstractScopedSettings service = new ClusterSettings(Settings.EMPTY, Collections.singleton(testSetting));
-
-        AtomicInteger consumer = new AtomicInteger();
-        service.addSettingsUpdateConsumer(testSetting, consumer::set);
-        AtomicInteger consumer2 = new AtomicInteger();
-        try {
-            service.addSettingsUpdateConsumer(testSetting2, consumer2::set);
-            fail("setting not registered");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Setting is not registered for key [foo.bar.baz]", ex.getMessage());
-        }
-
-        try {
-            service.addSettingsUpdateConsumer(testSetting, testSetting2, (a, b) -> {consumer.set(a); consumer2.set(b);});
-            fail("setting not registered");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Setting is not registered for key [foo.bar.baz]", ex.getMessage());
-        }
-        assertEquals(0, consumer.get());
-        assertEquals(0, consumer2.get());
-        service.applySettings(Settings.builder().put("foo.bar", 2).put("foo.bar.baz", 15).build());
-        assertEquals(2, consumer.get());
-        assertEquals(0, consumer2.get());
-    }
-
-    public void testApply() {
-        Setting<Integer> testSetting = Setting.intSetting("foo.bar", 1, true, Setting.Scope.CLUSTER);
-        Setting<Integer> testSetting2 = Setting.intSetting("foo.bar.baz", 1, true, Setting.Scope.CLUSTER);
-        AbstractScopedSettings service = new ClusterSettings(Settings.EMPTY, new HashSet<>(Arrays.asList(testSetting, testSetting2)));
-
-        AtomicInteger consumer = new AtomicInteger();
-        service.addSettingsUpdateConsumer(testSetting, consumer::set);
-        AtomicInteger consumer2 = new AtomicInteger();
-        service.addSettingsUpdateConsumer(testSetting2, consumer2::set, (s) -> assertTrue(s > 0));
-
-        AtomicInteger aC = new AtomicInteger();
-        AtomicInteger bC = new AtomicInteger();
-        service.addSettingsUpdateConsumer(testSetting, testSetting2, (a, b) -> {aC.set(a); bC.set(b);});
-
-        assertEquals(0, consumer.get());
-        assertEquals(0, consumer2.get());
-        assertEquals(0, aC.get());
-        assertEquals(0, bC.get());
-        try {
-            service.applySettings(Settings.builder().put("foo.bar", 2).put("foo.bar.baz", -15).build());
-            fail("invalid value");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("illegal value can't update [foo.bar.baz] from [1] to [-15]", ex.getMessage());
-        }
-        assertEquals(0, consumer.get());
-        assertEquals(0, consumer2.get());
-        assertEquals(0, aC.get());
-        assertEquals(0, bC.get());
-        try {
-            service.dryRun(Settings.builder().put("foo.bar", 2).put("foo.bar.baz", -15).build());
-            fail("invalid value");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("illegal value can't update [foo.bar.baz] from [1] to [-15]", ex.getMessage());
-        }
-
-        assertEquals(0, consumer.get());
-        assertEquals(0, consumer2.get());
-        assertEquals(0, aC.get());
-        assertEquals(0, bC.get());
-        service.dryRun(Settings.builder().put("foo.bar", 2).put("foo.bar.baz", 15).build());
-        assertEquals(0, consumer.get());
-        assertEquals(0, consumer2.get());
-        assertEquals(0, aC.get());
-        assertEquals(0, bC.get());
-
-        service.applySettings(Settings.builder().put("foo.bar", 2).put("foo.bar.baz", 15).build());
-        assertEquals(2, consumer.get());
-        assertEquals(15, consumer2.get());
-        assertEquals(2, aC.get());
-        assertEquals(15, bC.get());
-    }
-
-    public void testGet() {
-        ClusterSettings settings = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-        Setting setting = settings.get("cluster.routing.allocation.require.value");
-        assertEquals(setting, FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP_SETTING);
-
-        setting = settings.get("cluster.routing.allocation.total_shards_per_node");
-        assertEquals(setting, ShardsLimitAllocationDecider.CLUSTER_TOTAL_SHARDS_PER_NODE_SETTING);
-    }
-
-    public void testIsDynamic(){
-        ClusterSettings settings = new ClusterSettings(Settings.EMPTY, new HashSet<>(Arrays.asList(Setting.intSetting("foo.bar", 1, true, Setting.Scope.CLUSTER), Setting.intSetting("foo.bar.baz", 1, false, Setting.Scope.CLUSTER))));
-        assertFalse(settings.hasDynamicSetting("foo.bar.baz"));
-        assertTrue(settings.hasDynamicSetting("foo.bar"));
-        assertNotNull(settings.get("foo.bar.baz"));
-    }
-
-    public void testDiff() throws IOException {
-        Setting<Integer> foobarbaz = Setting.intSetting("foo.bar.baz", 1, false, Setting.Scope.CLUSTER);
-        Setting<Integer> foobar = Setting.intSetting("foo.bar", 1, true, Setting.Scope.CLUSTER);
-        ClusterSettings settings = new ClusterSettings(Settings.EMPTY, new HashSet<>(Arrays.asList(foobar, foobarbaz)));
-        Settings diff = settings.diff(Settings.builder().put("foo.bar", 5).build(), Settings.EMPTY);
-        assertEquals(diff.getAsMap().size(), 1);
-        assertEquals(diff.getAsInt("foo.bar.baz", null), Integer.valueOf(1));
-
-        diff = settings.diff(Settings.builder().put("foo.bar", 5).build(), Settings.builder().put("foo.bar.baz", 17).build());
-        assertEquals(diff.getAsMap().size(), 1);
-        assertEquals(diff.getAsInt("foo.bar.baz", null), Integer.valueOf(17));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/settings/SettingTests.java b/core/src/test/java/org/elasticsearch/common/settings/SettingTests.java
deleted file mode 100644
index 1640cfd..0000000
--- a/core/src/test/java/org/elasticsearch/common/settings/SettingTests.java
+++ /dev/null
@@ -1,251 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.common.settings;
-
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicReference;
-
-public class SettingTests extends ESTestCase {
-
-
-    public void testGet() {
-        Setting<Boolean> booleanSetting = Setting.boolSetting("foo.bar", false, true, Setting.Scope.CLUSTER);
-        assertFalse(booleanSetting.get(Settings.EMPTY));
-        assertFalse(booleanSetting.get(Settings.builder().put("foo.bar", false).build()));
-        assertTrue(booleanSetting.get(Settings.builder().put("foo.bar", true).build()));
-    }
-
-    public void testByteSize() {
-        Setting<ByteSizeValue> byteSizeValueSetting = Setting.byteSizeSetting("a.byte.size", new ByteSizeValue(1024), true, Setting.Scope.CLUSTER);
-        assertFalse(byteSizeValueSetting.isGroupSetting());
-        ByteSizeValue byteSizeValue = byteSizeValueSetting.get(Settings.EMPTY);
-        assertEquals(byteSizeValue.bytes(), 1024);
-        AtomicReference<ByteSizeValue> value = new AtomicReference<>(null);
-        ClusterSettings.SettingUpdater settingUpdater = byteSizeValueSetting.newUpdater(value::set, logger);
-        try {
-            settingUpdater.apply(Settings.builder().put("a.byte.size", 12).build(), Settings.EMPTY);
-            fail("no unit");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("failed to parse setting [a.byte.size] with value [12] as a size in bytes: unit is missing or unrecognized", ex.getMessage());
-        }
-
-        assertTrue(settingUpdater.apply(Settings.builder().put("a.byte.size", "12b").build(), Settings.EMPTY));
-        assertEquals(new ByteSizeValue(12), value.get());
-    }
-
-    public void testSimpleUpdate() {
-        Setting<Boolean> booleanSetting = Setting.boolSetting("foo.bar", false, true, Setting.Scope.CLUSTER);
-        AtomicReference<Boolean> atomicBoolean = new AtomicReference<>(null);
-        ClusterSettings.SettingUpdater settingUpdater = booleanSetting.newUpdater(atomicBoolean::set, logger);
-        Settings build = Settings.builder().put("foo.bar", false).build();
-        settingUpdater.apply(build, Settings.EMPTY);
-        assertNull(atomicBoolean.get());
-        build = Settings.builder().put("foo.bar", true).build();
-        settingUpdater.apply(build, Settings.EMPTY);
-        assertTrue(atomicBoolean.get());
-
-        // try update bogus value
-        build = Settings.builder().put("foo.bar", "I am not a boolean").build();
-        try {
-            settingUpdater.apply(build, Settings.EMPTY);
-            fail("not a boolean");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("Failed to parse value [I am not a boolean] for setting [foo.bar]", ex.getMessage());
-        }
-    }
-
-    public void testUpdateNotDynamic() {
-        Setting<Boolean> booleanSetting = Setting.boolSetting("foo.bar", false, false, Setting.Scope.CLUSTER);
-        assertFalse(booleanSetting.isGroupSetting());
-        AtomicReference<Boolean> atomicBoolean = new AtomicReference<>(null);
-        try {
-            booleanSetting.newUpdater(atomicBoolean::set, logger);
-            fail("not dynamic");
-        } catch (IllegalStateException ex) {
-            assertEquals("setting [foo.bar] is not dynamic", ex.getMessage());
-        }
-    }
-
-    public void testUpdaterIsIsolated() {
-        Setting<Boolean> booleanSetting = Setting.boolSetting("foo.bar", false, true, Setting.Scope.CLUSTER);
-        AtomicReference<Boolean> ab1 = new AtomicReference<>(null);
-        AtomicReference<Boolean> ab2 = new AtomicReference<>(null);
-        ClusterSettings.SettingUpdater settingUpdater = booleanSetting.newUpdater(ab1::set, logger);
-        ClusterSettings.SettingUpdater settingUpdater2 = booleanSetting.newUpdater(ab2::set, logger);
-        settingUpdater.apply(Settings.builder().put("foo.bar", true).build(), Settings.EMPTY);
-        assertTrue(ab1.get());
-        assertNull(ab2.get());
-    }
-
-    public void testDefault() {
-        TimeValue defautlValue = TimeValue.timeValueMillis(randomIntBetween(0, 1000000));
-        Setting<TimeValue> setting = Setting.positiveTimeSetting("my.time.value", defautlValue, randomBoolean(), Setting.Scope.CLUSTER);
-        assertFalse(setting.isGroupSetting());
-        String aDefault = setting.getDefault(Settings.EMPTY);
-        assertEquals(defautlValue.millis() + "ms", aDefault);
-        assertEquals(defautlValue.millis(), setting.get(Settings.EMPTY).millis());
-
-        Setting<String> secondaryDefault = new Setting<>("foo.bar", (s) -> s.get("old.foo.bar", "some_default"), (s) -> s, randomBoolean(), Setting.Scope.CLUSTER);
-        assertEquals("some_default", secondaryDefault.get(Settings.EMPTY));
-        assertEquals("42", secondaryDefault.get(Settings.builder().put("old.foo.bar", 42).build()));
-    }
-
-    public void testComplexType() {
-        AtomicReference<ComplexType> ref = new AtomicReference<>(null);
-        Setting<ComplexType> setting = new Setting<>("foo.bar", (s) -> "", (s) -> new ComplexType(s), true, Setting.Scope.CLUSTER);
-        assertFalse(setting.isGroupSetting());
-        ref.set(setting.get(Settings.EMPTY));
-        ComplexType type = ref.get();
-        ClusterSettings.SettingUpdater settingUpdater = setting.newUpdater(ref::set, logger);
-        assertFalse(settingUpdater.apply(Settings.EMPTY, Settings.EMPTY));
-        assertSame("no update - type has not changed", type, ref.get());
-
-        // change from default
-        assertTrue(settingUpdater.apply(Settings.builder().put("foo.bar", "2").build(), Settings.EMPTY));
-        assertNotSame("update - type has changed", type, ref.get());
-        assertEquals("2", ref.get().foo);
-
-
-        // change back to default...
-        assertTrue(settingUpdater.apply(Settings.EMPTY, Settings.builder().put("foo.bar", "2").build()));
-        assertNotSame("update - type has changed", type, ref.get());
-        assertEquals("", ref.get().foo);
-    }
-
-    public void testType() {
-        Setting<Integer> integerSetting = Setting.intSetting("foo.int.bar", 1, true, Setting.Scope.CLUSTER);
-        assertEquals(integerSetting.getScope(), Setting.Scope.CLUSTER);
-        integerSetting = Setting.intSetting("foo.int.bar", 1, true, Setting.Scope.INDEX);
-        assertEquals(integerSetting.getScope(), Setting.Scope.INDEX);
-    }
-
-    public void testGroups() {
-        AtomicReference<Settings> ref = new AtomicReference<>(null);
-        Setting<Settings> setting = Setting.groupSetting("foo.bar.", true, Setting.Scope.CLUSTER);
-        assertTrue(setting.isGroupSetting());
-        ClusterSettings.SettingUpdater settingUpdater = setting.newUpdater(ref::set, logger);
-
-        Settings currentInput = Settings.builder().put("foo.bar.1.value", "1").put("foo.bar.2.value", "2").put("foo.bar.3.value", "3").build();
-        Settings previousInput = Settings.EMPTY;
-        assertTrue(settingUpdater.apply(currentInput, previousInput));
-        assertNotNull(ref.get());
-        Settings settings = ref.get();
-        Map<String, Settings> asMap = settings.getAsGroups();
-        assertEquals(3, asMap.size());
-        assertEquals(asMap.get("1").get("value"), "1");
-        assertEquals(asMap.get("2").get("value"), "2");
-        assertEquals(asMap.get("3").get("value"), "3");
-
-        previousInput = currentInput;
-        currentInput = Settings.builder().put("foo.bar.1.value", "1").put("foo.bar.2.value", "2").put("foo.bar.3.value", "3").build();
-        Settings current = ref.get();
-        assertFalse(settingUpdater.apply(currentInput, previousInput));
-        assertSame(current, ref.get());
-
-        previousInput = currentInput;
-        currentInput = Settings.builder().put("foo.bar.1.value", "1").put("foo.bar.2.value", "2").build();
-        // now update and check that we got it
-        assertTrue(settingUpdater.apply(currentInput, previousInput));
-        assertNotSame(current, ref.get());
-
-        asMap = ref.get().getAsGroups();
-        assertEquals(2, asMap.size());
-        assertEquals(asMap.get("1").get("value"), "1");
-        assertEquals(asMap.get("2").get("value"), "2");
-
-        previousInput = currentInput;
-        currentInput = Settings.builder().put("foo.bar.1.value", "1").put("foo.bar.2.value", "4").build();
-        // now update and check that we got it
-        assertTrue(settingUpdater.apply(currentInput, previousInput));
-        assertNotSame(current, ref.get());
-
-        asMap = ref.get().getAsGroups();
-        assertEquals(2, asMap.size());
-        assertEquals(asMap.get("1").get("value"), "1");
-        assertEquals(asMap.get("2").get("value"), "4");
-
-        assertTrue(setting.match("foo.bar.baz"));
-        assertFalse(setting.match("foo.baz.bar"));
-
-        ClusterSettings.SettingUpdater predicateSettingUpdater = setting.newUpdater(ref::set, logger,(s) -> assertFalse(true));
-        try {
-            predicateSettingUpdater.apply(Settings.builder().put("foo.bar.1.value", "1").put("foo.bar.2.value", "2").build(), Settings.EMPTY);
-            fail("not accepted");
-        } catch (IllegalArgumentException ex) {
-            assertEquals(ex.getMessage(), "illegal value can't update [foo.bar.] from [{}] to [{1.value=1, 2.value=2}]");
-        }
-    }
-
-    public static class ComplexType {
-
-        final String foo;
-
-        public ComplexType(String foo) {
-            this.foo = foo;
-        }
-    }
-
-    public static class Composite {
-
-        private Integer b;
-        private Integer a;
-
-        public void set(Integer a, Integer b) {
-            this.a = a;
-            this.b = b;
-        }
-    }
-
-
-    public void testComposite() {
-        Composite c = new Composite();
-        Setting<Integer> a = Setting.intSetting("foo.int.bar.a", 1, true, Setting.Scope.CLUSTER);
-        Setting<Integer> b = Setting.intSetting("foo.int.bar.b", 1, true, Setting.Scope.CLUSTER);
-        ClusterSettings.SettingUpdater<Tuple<Integer, Integer>> settingUpdater = Setting.compoundUpdater(c::set, a, b, logger);
-        assertFalse(settingUpdater.apply(Settings.EMPTY, Settings.EMPTY));
-        assertNull(c.a);
-        assertNull(c.b);
-
-        Settings build = Settings.builder().put("foo.int.bar.a", 2).build();
-        assertTrue(settingUpdater.apply(build, Settings.EMPTY));
-        assertEquals(2, c.a.intValue());
-        assertEquals(1, c.b.intValue());
-
-        Integer aValue = c.a;
-        assertFalse(settingUpdater.apply(build, build));
-        assertSame(aValue, c.a);
-        Settings previous = build;
-        build = Settings.builder().put("foo.int.bar.a", 2).put("foo.int.bar.b", 5).build();
-        assertTrue(settingUpdater.apply(build, previous));
-        assertEquals(2, c.a.intValue());
-        assertEquals(5, c.b.intValue());
-
-        // reset to default
-        assertTrue(settingUpdater.apply(Settings.EMPTY, build));
-        assertEquals(1, c.a.intValue());
-        assertEquals(1, c.b.intValue());
-
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java b/core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java
index 2945d86..ec0e260 100644
--- a/core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java
+++ b/core/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java
@@ -162,14 +162,4 @@ public class TimeValueTests extends ESTestCase {
             assertThat(e.getMessage(), containsString("Failed to parse"));
         }
     }
-
-    public void testToStringRep() {
-        assertEquals("-1", new TimeValue(-1).getStringRep());
-        assertEquals("10ms", new TimeValue(10, TimeUnit.MILLISECONDS).getStringRep());
-        assertEquals("1533ms", new TimeValue(1533, TimeUnit.MILLISECONDS).getStringRep());
-        assertEquals("90s", new TimeValue(90, TimeUnit.SECONDS).getStringRep());
-        assertEquals("90m", new TimeValue(90, TimeUnit.MINUTES).getStringRep());
-        assertEquals("36h", new TimeValue(36, TimeUnit.HOURS).getStringRep());
-        assertEquals("1000d", new TimeValue(1000, TimeUnit.DAYS).getStringRep());
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/util/BigArraysTests.java b/core/src/test/java/org/elasticsearch/common/util/BigArraysTests.java
index 184de7f..7d36c09 100644
--- a/core/src/test/java/org/elasticsearch/common/util/BigArraysTests.java
+++ b/core/src/test/java/org/elasticsearch/common/util/BigArraysTests.java
@@ -21,13 +21,13 @@ package org.elasticsearch.common.util;
 
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.breaker.CircuitBreaker;
 import org.elasticsearch.common.breaker.CircuitBreakingException;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
 import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.junit.Before;
 
@@ -336,9 +336,9 @@ public class BigArraysTests extends ESSingleNodeTestCase {
         for (String type : Arrays.asList("Byte", "Int", "Long", "Float", "Double", "Object")) {
             HierarchyCircuitBreakerService hcbs = new HierarchyCircuitBreakerService(
                     Settings.builder()
-                            .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), size - 1, ByteSizeUnit.BYTES)
+                            .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, size - 1, ByteSizeUnit.BYTES)
                             .build(),
-                    new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS));
+                    new NodeSettingsService(Settings.EMPTY));
             BigArrays bigArrays = new BigArrays(null, hcbs).withCircuitBreaking();
             Method create = BigArrays.class.getMethod("new" + type + "Array", long.class);
             try {
@@ -356,9 +356,9 @@ public class BigArraysTests extends ESSingleNodeTestCase {
             final long maxSize = randomIntBetween(1 << 10, 1 << 22);
             HierarchyCircuitBreakerService hcbs = new HierarchyCircuitBreakerService(
                     Settings.builder()
-                            .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), maxSize, ByteSizeUnit.BYTES)
+                            .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, maxSize, ByteSizeUnit.BYTES)
                             .build(),
-                    new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS));
+                    new NodeSettingsService(Settings.EMPTY));
             BigArrays bigArrays = new BigArrays(null, hcbs).withCircuitBreaking();
             Method create = BigArrays.class.getMethod("new" + type + "Array", long.class);
             final int size = scaledRandomIntBetween(1, 20);
diff --git a/core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java b/core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java
index fe9ba6b..4c3612d 100644
--- a/core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/util/CollectionUtilsTests.java
@@ -25,14 +25,7 @@ import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.test.ESTestCase;
 
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.SortedSet;
-import java.util.TreeSet;
+import java.util.*;
 
 import static org.elasticsearch.common.util.CollectionUtils.eagerPartition;
 import static org.hamcrest.Matchers.equalTo;
@@ -80,7 +73,7 @@ public class CollectionUtilsTests extends ESTestCase {
             array.append(new BytesRef(s));
         }
         if (randomBoolean()) {
-            Collections.shuffle(tmpList, getRandom());
+            Collections.shuffle(tmpList, random());
             for (BytesRef ref : tmpList) {
                 array.append(ref);
             }
@@ -111,7 +104,7 @@ public class CollectionUtilsTests extends ESTestCase {
             array.append(new BytesRef(s));
         }
         if (randomBoolean()) {
-            Collections.shuffle(values, getRandom());
+            Collections.shuffle(values, random());
         }
         int[] indices = new int[array.size()];
         for (int i = 0; i < indices.length; i++) {
diff --git a/core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java b/core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java
index 1d2d214..deac15b 100644
--- a/core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java
@@ -27,13 +27,7 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.PriorityBlockingQueue;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.ScheduledThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
+import java.util.concurrent.*;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import static org.hamcrest.Matchers.equalTo;
@@ -46,7 +40,7 @@ public class PrioritizedExecutorsTests extends ESTestCase {
     public void testPriorityQueue() throws Exception {
         PriorityBlockingQueue<Priority> queue = new PriorityBlockingQueue<>();
         List<Priority> priorities = Arrays.asList(Priority.values());
-        Collections.shuffle(priorities);
+        Collections.shuffle(priorities, random());
 
         for (Priority priority : priorities) {
             queue.add(priority);
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java b/core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java
new file mode 100644
index 0000000..1b91726
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/BaseXContentTestCase.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent;
+
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+
+public abstract class BaseXContentTestCase extends ESTestCase {
+
+    public abstract XContentType xcontentType();
+
+    public void testBasics() throws IOException {
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = xcontentType().xContent().createGenerator(os)) {
+            generator.writeStartObject();
+            generator.writeEndObject();
+        }
+        byte[] data = os.toByteArray();
+        assertEquals(xcontentType(), XContentFactory.xContentType(data));
+    }
+
+    public void testRawField() throws Exception {
+        for (boolean useStream : new boolean[] {false, true}) {
+            for (XContentType xcontentType : XContentType.values()) {
+                doTestRawField(xcontentType.xContent(), useStream);
+            }
+        }
+    }
+
+    void doTestRawField(XContent source, boolean useStream) throws Exception {
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = source.createGenerator(os)) {
+            generator.writeStartObject();
+            generator.writeFieldName("foo");
+            generator.writeNull();
+            generator.writeEndObject();
+        }
+        final byte[] rawData = os.toByteArray();
+
+        os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = xcontentType().xContent().createGenerator(os)) {
+            generator.writeStartObject();
+            if (useStream) {
+                generator.writeRawField("bar", new ByteArrayInputStream(rawData));
+            } else {
+                generator.writeRawField("bar", new BytesArray(rawData));
+            }
+            generator.writeEndObject();
+        }
+
+        XContentParser parser = xcontentType().xContent().createParser(os.toByteArray());
+        assertEquals(Token.START_OBJECT, parser.nextToken());
+        assertEquals(Token.FIELD_NAME, parser.nextToken());
+        assertEquals("bar", parser.currentName());
+        assertEquals(Token.START_OBJECT, parser.nextToken());
+        assertEquals(Token.FIELD_NAME, parser.nextToken());
+        assertEquals("foo", parser.currentName());
+        assertEquals(Token.VALUE_NULL, parser.nextToken());
+        assertEquals(Token.END_OBJECT, parser.nextToken());
+        assertEquals(Token.END_OBJECT, parser.nextToken());
+        assertNull(parser.nextToken());
+    }
+
+    public void testRawValue() throws Exception {
+        for (XContentType xcontentType : XContentType.values()) {
+            doTestRawValue(xcontentType.xContent());
+        }
+    }
+
+    void doTestRawValue(XContent source) throws Exception {
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = source.createGenerator(os)) {
+            generator.writeStartObject();
+            generator.writeFieldName("foo");
+            generator.writeNull();
+            generator.writeEndObject();
+        }
+        final byte[] rawData = os.toByteArray();
+
+        os = new ByteArrayOutputStream();
+        try (XContentGenerator generator = xcontentType().xContent().createGenerator(os)) {
+            generator.writeRawValue(new BytesArray(rawData));
+        }
+
+        XContentParser parser = xcontentType().xContent().createParser(os.toByteArray());
+        assertEquals(Token.START_OBJECT, parser.nextToken());
+        assertEquals(Token.FIELD_NAME, parser.nextToken());
+        assertEquals("foo", parser.currentName());
+        assertEquals(Token.VALUE_NULL, parser.nextToken());
+        assertEquals(Token.END_OBJECT, parser.nextToken());
+        assertNull(parser.nextToken());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/builder/BuilderRawFieldTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/builder/BuilderRawFieldTests.java
deleted file mode 100644
index 9bb26b6..0000000
--- a/core/src/test/java/org/elasticsearch/common/xcontent/builder/BuilderRawFieldTests.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.xcontent.builder;
-
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- *
- */
-public class BuilderRawFieldTests extends ESTestCase {
-    public void testJsonRawField() throws IOException {
-        testRawField(XContentType.JSON);
-    }
-
-    public void testSmileRawField() throws IOException {
-        testRawField(XContentType.SMILE);
-    }
-
-    public void testYamlRawField() throws IOException {
-        testRawField(XContentType.YAML);
-    }
-
-    public void testCborRawField() throws IOException {
-        testRawField(XContentType.CBOR);
-    }
-
-    private void testRawField(XContentType type) throws IOException {
-        XContentBuilder builder = XContentFactory.contentBuilder(type);
-        builder.startObject();
-        builder.field("field1", "value1");
-        builder.rawField("_source", XContentFactory.contentBuilder(type).startObject().field("s_field", "s_value").endObject().bytes());
-        builder.field("field2", "value2");
-        builder.rawField("payload_i", new BytesArray(Long.toString(1)));
-        builder.field("field3", "value3");
-        builder.rawField("payload_d", new BytesArray(Double.toString(1.1)));
-        builder.field("field4", "value4");
-        builder.rawField("payload_s", new BytesArray("test"));
-        builder.field("field5", "value5");
-        builder.endObject();
-
-        XContentParser parser = XContentFactory.xContent(type).createParser(builder.bytes());
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field1"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value1"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("_source"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.START_OBJECT));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("s_field"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("s_value"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field2"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value2"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("payload_i"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_NUMBER));
-        assertThat(parser.numberType(), equalTo(XContentParser.NumberType.INT));
-        assertThat(parser.longValue(), equalTo(1l));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field3"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value3"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("payload_d"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_NUMBER));
-        assertThat(parser.numberType(), equalTo(XContentParser.NumberType.DOUBLE));
-        assertThat(parser.doubleValue(), equalTo(1.1d));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field4"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value4"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("payload_s"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("test"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), equalTo("field5"));
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.VALUE_STRING));
-        assertThat(parser.text(), equalTo("value5"));
-
-        assertThat(parser.nextToken(), equalTo(XContentParser.Token.END_OBJECT));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
index d6cec17..7ffafc0 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/builder/XContentBuilderTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.common.xcontent.builder;
 
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.io.FastCharArrayWriter;
@@ -32,6 +33,7 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.test.ESTestCase;
 
+import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.nio.file.Path;
 import java.util.ArrayList;
@@ -54,8 +56,8 @@ import static org.hamcrest.Matchers.equalTo;
  */
 public class XContentBuilderTests extends ESTestCase {
     public void testPrettyWithLfAtEnd() throws Exception {
-        FastCharArrayWriter writer = new FastCharArrayWriter();
-        XContentGenerator generator = XContentFactory.xContent(XContentType.JSON).createGenerator(writer);
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        XContentGenerator generator = XContentFactory.xContent(XContentType.JSON).createGenerator(os);
         generator.usePrettyPrint();
         generator.usePrintLineFeedAtEnd();
 
@@ -68,27 +70,28 @@ public class XContentBuilderTests extends ESTestCase {
         // double close, and check there is no error...
         generator.close();
 
-        assertThat(writer.unsafeCharArray()[writer.size() - 1], equalTo('\n'));
+        byte[] bytes = os.toByteArray();
+        assertThat((char) bytes[bytes.length - 1], equalTo('\n'));
     }
 
     public void testReuseJsonGenerator() throws Exception {
-        FastCharArrayWriter writer = new FastCharArrayWriter();
-        XContentGenerator generator = XContentFactory.xContent(XContentType.JSON).createGenerator(writer);
+        ByteArrayOutputStream os = new ByteArrayOutputStream();
+        XContentGenerator generator = XContentFactory.xContent(XContentType.JSON).createGenerator(os);
         generator.writeStartObject();
         generator.writeStringField("test", "value");
         generator.writeEndObject();
         generator.flush();
 
-        assertThat(writer.toStringTrim(), equalTo("{\"test\":\"value\"}"));
+        assertThat(new BytesRef(os.toByteArray()), equalTo(new BytesRef("{\"test\":\"value\"}")));
 
         // try again...
-        writer.reset();
+        os.reset();
         generator.writeStartObject();
         generator.writeStringField("test", "value");
         generator.writeEndObject();
         generator.flush();
         // we get a space at the start here since it thinks we are not in the root object (fine, we will ignore it in the real code we use)
-        assertThat(writer.toStringTrim(), equalTo("{\"test\":\"value\"}"));
+        assertThat(new BytesRef(os.toByteArray()), equalTo(new BytesRef(" {\"test\":\"value\"}")));
     }
 
     public void testRaw() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentTests.java
new file mode 100644
index 0000000..928b8a6
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentTests.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.cbor;
+
+import org.elasticsearch.common.xcontent.BaseXContentTestCase;
+import org.elasticsearch.common.xcontent.XContentType;
+
+public class CborXContentTests extends BaseXContentTestCase {
+
+    @Override
+    public XContentType xcontentType() {
+        return XContentType.CBOR;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java
new file mode 100644
index 0000000..8a739ee
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/json/JsonXContentTests.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.json;
+
+import org.elasticsearch.common.xcontent.BaseXContentTestCase;
+import org.elasticsearch.common.xcontent.XContentType;
+
+public class JsonXContentTests extends BaseXContentTestCase {
+
+    @Override
+    public XContentType xcontentType() {
+        return XContentType.JSON;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/smile/SmileXContentTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/smile/SmileXContentTests.java
new file mode 100644
index 0000000..6961e84
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/smile/SmileXContentTests.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.smile;
+
+import org.elasticsearch.common.xcontent.BaseXContentTestCase;
+import org.elasticsearch.common.xcontent.XContentType;
+
+public class SmileXContentTests extends BaseXContentTestCase {
+
+    @Override
+    public XContentType xcontentType() {
+        return XContentType.SMILE;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java
index 9216f48..ed7aee3 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/AbstractFilteringJsonGeneratorTestCase.java
@@ -475,15 +475,10 @@ public abstract class AbstractFilteringJsonGeneratorTestCase extends ESTestCase
         assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", raw).endObject());
         assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", raw).endObject());
 
-        // Test method: rawField(String fieldName, byte[] content)
-        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", raw.toBytes()).endObject());
-
         // Test method: rawField(String fieldName, InputStream content)
-        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes()), getXContentType()).endObject());
-        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes()), getXContentType()).endObject());
-        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes()), getXContentType()).endObject());
+        assertXContentBuilder(expectedRawField, newXContentBuilder().startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
+        assertXContentBuilder(expectedRawFieldFiltered, newXContentBuilder("f*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
+        assertXContentBuilder(expectedRawFieldNotFiltered, newXContentBuilder("r*").startObject().field("foo", 0).rawField("raw", new ByteArrayInputStream(raw.toBytes())).endObject());
     }
 
     public void testArrays() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGeneratorBenchmark.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGeneratorBenchmark.java
deleted file mode 100644
index 97ce4fc..0000000
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/filtering/FilteringJsonGeneratorBenchmark.java
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.xcontent.support.filtering;
-
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Locale;
-
-/**
- * Benchmark class to compare filtered and unfiltered XContent generators.
- */
-public class FilteringJsonGeneratorBenchmark {
-
-    public static void main(String[] args) throws IOException {
-        final XContent XCONTENT = JsonXContent.jsonXContent;
-
-        System.out.println("Executing " + FilteringJsonGeneratorBenchmark.class + "...");
-
-        System.out.println("Warming up...");
-        run(XCONTENT, 500_000, 100, 0.5);
-        System.out.println("Warmed up.");
-
-        System.out.println("nb documents | nb fields | nb fields written | % fields written | time (millis) | rate (docs/sec) | avg size");
-
-        for (int nbFields : Arrays.asList(10, 25, 50, 100, 250)) {
-            for (int nbDocs : Arrays.asList(100, 1000, 10_000, 100_000, 500_000)) {
-                for (double ratio : Arrays.asList(0.0, 1.0, 0.99, 0.95, 0.9, 0.75, 0.5, 0.25, 0.1, 0.05, 0.01)) {
-                    run(XCONTENT, nbDocs, nbFields, ratio);
-                }
-            }
-        }
-        System.out.println("Done.");
-    }
-
-    private static void run(XContent xContent, long nbIterations, int nbFields, double ratio) throws IOException {
-        String[] fields = fields(nbFields);
-        String[] filters = fields((int) (nbFields * ratio));
-
-        long size = 0;
-        BytesStreamOutput os = new BytesStreamOutput();
-
-        long start = System.nanoTime();
-        for (int i = 0; i < nbIterations; i++) {
-            XContentBuilder builder = new XContentBuilder(xContent, os, filters);
-            builder.startObject();
-
-            for (String field : fields) {
-                builder.field(field, System.nanoTime());
-            }
-            builder.endObject();
-
-            size += builder.bytes().length();
-            os.reset();
-        }
-        double milliseconds = (System.nanoTime() - start) / 1_000_000d;
-
-        System.out.printf(Locale.ROOT, "%12d | %9d | %17d | %14.2f %% | %10.3f ms | %15.2f | %8.0f %n",
-                nbIterations, nbFields,
-                (int) (nbFields * ratio),
-                (ratio * 100d),
-                milliseconds,
-                ((double) nbIterations) / (milliseconds / 1000d),
-                size / ((double) nbIterations));
-    }
-
-    /**
-     * Returns a String array of field names starting from "field_0" with a length of n.
-     * If n=3, the array is ["field_0","field_1","field_2"]
-     */
-    private static String[] fields(int n) {
-        String[] fields = new String[n];
-        for (int i = 0; i < n; i++) {
-            fields[i] = "field_" + i;
-        }
-        return fields;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/yaml/YamlXContentTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/yaml/YamlXContentTests.java
new file mode 100644
index 0000000..17c2a59
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/yaml/YamlXContentTests.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.xcontent.yaml;
+
+import org.elasticsearch.common.xcontent.BaseXContentTestCase;
+import org.elasticsearch.common.xcontent.XContentType;
+
+public class YamlXContentTests extends BaseXContentTestCase {
+
+    @Override
+    public XContentType xcontentType() {
+        return XContentType.YAML;
+    }
+
+}
diff --git a/core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java b/core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java
index f124e19..b696c44 100644
--- a/core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java
+++ b/core/src/test/java/org/elasticsearch/consistencylevel/WriteConsistencyLevelIT.java
@@ -53,7 +53,7 @@ public class WriteConsistencyLevelIT extends ESIntegTestCase {
             fail("can't index, does not match consistency");
         } catch (UnavailableShardsException e) {
             assertThat(e.status(), equalTo(RestStatus.SERVICE_UNAVAILABLE));
-            assertThat(e.getMessage(), equalTo("[test][0] Not enough active copies to meet write consistency of [QUORUM] (have 1, needed 2). Timeout: [100ms], request: index {[test][type1][1], source[{ type1 : { \"id\" : \"1\", \"name\" : \"test\" } }]}"));
+            assertThat(e.getMessage(), equalTo("[test][0] Not enough active copies to meet write consistency of [QUORUM] (have 1, needed 2). Timeout: [100ms], request: [index {[test][type1][1], source[{ type1 : { \"id\" : \"1\", \"name\" : \"test\" } }]}]"));
             // but really, all is well
         }
 
@@ -76,7 +76,7 @@ public class WriteConsistencyLevelIT extends ESIntegTestCase {
             fail("can't index, does not match consistency");
         } catch (UnavailableShardsException e) {
             assertThat(e.status(), equalTo(RestStatus.SERVICE_UNAVAILABLE));
-            assertThat(e.getMessage(), equalTo("[test][0] Not enough active copies to meet write consistency of [ALL] (have 2, needed 3). Timeout: [100ms], request: index {[test][type1][1], source[{ type1 : { \"id\" : \"1\", \"name\" : \"test\" } }]}"));
+            assertThat(e.getMessage(), equalTo("[test][0] Not enough active copies to meet write consistency of [ALL] (have 2, needed 3). Timeout: [100ms], request: [index {[test][type1][1], source[{ type1 : { \"id\" : \"1\", \"name\" : \"test\" } }]}]"));
             // but really, all is well
         }
 
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index 570c0b4..f9778f6 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -132,7 +132,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
             .put(FaultDetection.SETTING_PING_TIMEOUT, "1s") // for hitting simulated network failures quickly
             .put(FaultDetection.SETTING_PING_RETRIES, "1") // for hitting simulated network failures quickly
             .put("discovery.zen.join_timeout", "10s")  // still long to induce failures but to long so test won't time out
-            .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s") // <-- for hitting simulated network failures quickly
+            .put(DiscoverySettings.PUBLISH_TIMEOUT, "1s") // <-- for hitting simulated network failures quickly
             .put("http.enabled", false) // just to make test quicker
             .put("gateway.local.list_timeout", "10s") // still long to induce failures but to long so test won't time out
             .build();
@@ -150,7 +150,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         // TODO: Rarely use default settings form some of these
         Settings nodeSettings = Settings.builder()
                 .put(DEFAULT_SETTINGS)
-                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), minimumMasterNode)
+                .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, minimumMasterNode)
                 .build();
 
         if (discoveryConfig == null) {
@@ -217,7 +217,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
 
         logger.info("--> reducing min master nodes to 2");
         assertAcked(client().admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2)).get());
+                .setTransientSettings(Settings.builder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 2)).get());
 
         String master = internalCluster().getMasterName();
         String nonMaster = null;
@@ -293,9 +293,9 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         // Wait until the master node sees al 3 nodes again.
         ensureStableCluster(3, new TimeValue(DISRUPTION_HEALING_OVERHEAD.millis() + networkPartition.expectedTimeToHeal().millis()));
 
-        logger.info("Verify no master block with {} set to {}", DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), "all");
+        logger.info("Verify no master block with {} set to {}", DiscoverySettings.NO_MASTER_BLOCK, "all");
         client().admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put(DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), "all"))
+                .setTransientSettings(Settings.builder().put(DiscoverySettings.NO_MASTER_BLOCK, "all"))
                 .get();
 
         networkPartition.startDisrupting();
@@ -473,7 +473,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
                 docsPerIndexer = 1 + randomInt(5);
                 logger.info("indexing " + docsPerIndexer + " docs per indexer during partition");
                 countDownLatchRef.set(new CountDownLatch(docsPerIndexer * indexers.size()));
-                Collections.shuffle(semaphores);
+                Collections.shuffle(semaphores, random());
                 for (Semaphore semaphore : semaphores) {
                     assertThat(semaphore.availablePermits(), equalTo(0));
                     semaphore.release(docsPerIndexer);
@@ -683,7 +683,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         ensureGreen("test");
 
         nodes = new ArrayList<>(nodes);
-        Collections.shuffle(nodes, getRandom());
+        Collections.shuffle(nodes, random());
         String isolatedNode = nodes.get(0);
         String notIsolatedNode = nodes.get(1);
 
@@ -863,7 +863,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
         internalCluster().startNodesAsync(3,
                 Settings.builder()
                         .put(DiscoveryService.SETTING_INITIAL_STATE_TIMEOUT, "1ms")
-                        .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "3s")
+                        .put(DiscoverySettings.PUBLISH_TIMEOUT, "3s")
                         .build()).get();
 
         logger.info("applying disruption while cluster is forming ...");
@@ -1038,7 +1038,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
                 new NetworkDisconnectPartition(getRandom()),
                 new SlowClusterStateProcessing(getRandom())
         );
-        Collections.shuffle(list);
+        Collections.shuffle(list, random());
         setDisruptionScheme(list.get(0));
         return list.get(0);
     }
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java
index e7bded3..c495556 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ElectMasterServiceTests.java
@@ -26,11 +26,7 @@ import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
 import org.elasticsearch.test.ESTestCase;
 
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 
 public class ElectMasterServiceTests extends ESTestCase {
 
@@ -53,7 +49,7 @@ public class ElectMasterServiceTests extends ESTestCase {
             nodes.add(node);
         }
 
-        Collections.shuffle(nodes, getRandom());
+        Collections.shuffle(nodes, random());
         return nodes;
     }
 
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java
index 1f6f0bd..42cb7cf 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java
@@ -30,7 +30,6 @@ import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.FailedRerouteAllocation;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.common.transport.LocalTransportAddress;
@@ -39,6 +38,7 @@ import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.BaseFuture;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.zen.membership.MembershipAction;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.cluster.TestClusterService;
 import org.elasticsearch.test.junit.annotations.TestLogging;
@@ -67,7 +67,7 @@ public class NodeJoinControllerTests extends ESTestCase {
         // make sure we have a master
         clusterService.setState(ClusterState.builder(clusterService.state()).nodes(DiscoveryNodes.builder(initialNodes).masterNodeId(localNode.id())));
         nodeJoinController = new NodeJoinController(clusterService, new NoopRoutingService(Settings.EMPTY),
-                new DiscoverySettings(Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS)), Settings.EMPTY);
+                new DiscoverySettings(Settings.EMPTY, new NodeSettingsService(Settings.EMPTY)), Settings.EMPTY);
     }
 
     public void testSimpleJoinAccumulation() throws InterruptedException, ExecutionException {
@@ -244,7 +244,7 @@ public class NodeJoinControllerTests extends ESTestCase {
 
         // add
 
-        Collections.shuffle(nodesToJoin);
+        Collections.shuffle(nodesToJoin, random());
         logger.debug("--> joining [{}] unique master nodes. Total of [{}] join requests", initialJoins, nodesToJoin.size());
         for (DiscoveryNode node : nodesToJoin) {
             pendingJoins.add(joinNodeAsync(node));
@@ -269,7 +269,7 @@ public class NodeJoinControllerTests extends ESTestCase {
             }
         }
 
-        Collections.shuffle(nodesToJoin);
+        Collections.shuffle(nodesToJoin, random());
         logger.debug("--> joining [{}] nodes, with repetition a total of [{}]", finalJoins, nodesToJoin.size());
         for (DiscoveryNode node : nodesToJoin) {
             pendingJoins.add(joinNodeAsync(node));
@@ -316,7 +316,7 @@ public class NodeJoinControllerTests extends ESTestCase {
                 nodesToJoin.add(node);
             }
         }
-        Collections.shuffle(nodesToJoin);
+        Collections.shuffle(nodesToJoin, random());
         logger.debug("--> joining [{}] nodes, with repetition a total of [{}]", initialJoins, nodesToJoin.size());
         for (DiscoveryNode node : nodesToJoin) {
             pendingJoins.add(joinNodeAsync(node));
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
index 3b67086..0b5f999 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java
@@ -84,7 +84,7 @@ public class ZenDiscoveryIT extends ESIntegTestCase {
         assertThat(zenDiscovery.isRejoinOnMasterGone(), is(true));
 
         client().admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put(ZenDiscovery.REJOIN_ON_MASTER_GONE_SETTING.getKey(), false))
+                .setTransientSettings(Settings.builder().put(ZenDiscovery.SETTING_REJOIN_ON_MASTER_GONE, false))
                 .get();
 
         assertThat(zenDiscovery.isRejoinOnMasterGone(), is(false));
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/ZenPingTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/ZenPingTests.java
index 82733d9..c54489b 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/ZenPingTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/ZenPingTests.java
@@ -62,7 +62,7 @@ public class ZenPingTests extends ESTestCase {
         }
 
         // shuffle
-        Collections.shuffle(pings);
+        Collections.shuffle(pings, random());
 
         ZenPing.PingCollection collection = new ZenPing.PingCollection();
         collection.addPings(pings.toArray(new ZenPing.PingResponse[pings.size()]));
diff --git a/core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java b/core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java
index e5b3337..8dea09b 100644
--- a/core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateActionTests.java
@@ -21,18 +21,14 @@ package org.elasticsearch.discovery.zen.publish;
 
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.ClusterChangedEvent;
-import org.elasticsearch.cluster.ClusterName;
-import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.cluster.ClusterStateListener;
-import org.elasticsearch.cluster.Diff;
+import org.elasticsearch.cluster.*;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -43,39 +39,25 @@ import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.zen.DiscoveryNodesProvider;
 import org.elasticsearch.node.service.NodeService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.BytesTransportRequest;
-import org.elasticsearch.transport.TransportChannel;
-import org.elasticsearch.transport.TransportConnectionListener;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseOptions;
-import org.elasticsearch.transport.TransportService;
+import org.elasticsearch.transport.*;
 import org.elasticsearch.transport.local.LocalTransport;
 import org.junit.After;
 import org.junit.Before;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
 
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.emptyIterable;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 @TestLogging("discovery.zen.publish:TRACE")
 public class PublishClusterStateActionTests extends ESTestCase {
@@ -156,7 +138,7 @@ public class PublishClusterStateActionTests extends ESTestCase {
     public MockNode createMockNode(String name, Settings settings, Version version, @Nullable ClusterStateListener listener) throws Exception {
         settings = Settings.builder()
                 .put("name", name)
-                .put(TransportService.TRACE_LOG_INCLUDE_SETTING.getKey(), "", TransportService.TRACE_LOG_EXCLUDE_SETTING.getKey(), "NOTHING")
+                .put(TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING")
                 .put(settings)
                 .build();
 
@@ -237,7 +219,7 @@ public class PublishClusterStateActionTests extends ESTestCase {
 
     protected MockPublishAction buildPublishClusterStateAction(Settings settings, MockTransportService transportService, DiscoveryNodesProvider nodesProvider,
                                                                PublishClusterStateAction.NewPendingClusterStateListener listener) {
-        DiscoverySettings discoverySettings = new DiscoverySettings(settings, new ClusterSettings(settings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS));
+        DiscoverySettings discoverySettings = new DiscoverySettings(settings, new NodeSettingsService(settings));
         return new MockPublishAction(settings, transportService, nodesProvider, listener, discoverySettings, ClusterName.DEFAULT);
     }
 
@@ -345,7 +327,7 @@ public class PublishClusterStateActionTests extends ESTestCase {
     }
 
     public void testDisablingDiffPublishing() throws Exception {
-        Settings noDiffPublishingSettings = Settings.builder().put(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey(), false).build();
+        Settings noDiffPublishingSettings = Settings.builder().put(DiscoverySettings.PUBLISH_DIFF_ENABLE, false).build();
 
         MockNode nodeA = createMockNode("nodeA", noDiffPublishingSettings, Version.CURRENT, new ClusterStateListener() {
             @Override
@@ -384,7 +366,7 @@ public class PublishClusterStateActionTests extends ESTestCase {
     public void testSimultaneousClusterStatePublishing() throws Exception {
         int numberOfNodes = randomIntBetween(2, 10);
         int numberOfIterations = scaledRandomIntBetween(5, 50);
-        Settings settings = Settings.builder().put(DiscoverySettings.PUBLISH_DIFF_ENABLE_SETTING.getKey(), randomBoolean()).build();
+        Settings settings = Settings.builder().put(DiscoverySettings.PUBLISH_DIFF_ENABLE, randomBoolean()).build();
         MockNode master = createMockNode("node0", settings, Version.CURRENT, new ClusterStateListener() {
             @Override
             public void clusterChanged(ClusterChangedEvent event) {
@@ -510,8 +492,8 @@ public class PublishClusterStateActionTests extends ESTestCase {
         final boolean expectingToCommit = randomBoolean();
         Settings.Builder settings = Settings.builder();
         // make sure we have a reasonable timeout if we expect to timeout, o.w. one that will make the test "hang"
-        settings.put(DiscoverySettings.COMMIT_TIMEOUT_SETTING.getKey(), expectingToCommit == false && timeOutNodes > 0 ? "100ms" : "1h")
-                .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "5ms"); // test is about committing
+        settings.put(DiscoverySettings.COMMIT_TIMEOUT, expectingToCommit == false && timeOutNodes > 0 ? "100ms" : "1h")
+                .put(DiscoverySettings.PUBLISH_TIMEOUT, "5ms"); // test is about committing
 
         MockNode master = createMockNode("master", settings.build());
 
@@ -675,7 +657,7 @@ public class PublishClusterStateActionTests extends ESTestCase {
 
         logger.info("--> committing states");
 
-        Collections.shuffle(states, random());
+        Randomness.shuffle(states);
         for (ClusterState state : states) {
             node.action.handleCommitRequest(new PublishClusterStateAction.CommitClusterStateRequest(state.stateUUID()), channel);
             assertThat(channel.response.get(), equalTo((TransportResponse) TransportResponse.Empty.INSTANCE));
@@ -695,7 +677,7 @@ public class PublishClusterStateActionTests extends ESTestCase {
      */
     public void testTimeoutOrCommit() throws Exception {
         Settings settings = Settings.builder()
-                .put(DiscoverySettings.COMMIT_TIMEOUT_SETTING.getKey(), "1ms").build(); // short but so we will sometime commit sometime timeout
+                .put(DiscoverySettings.COMMIT_TIMEOUT, "1ms").build(); // short but so we will sometime commit sometime timeout
 
         MockNode master = createMockNode("master", settings);
         MockNode node = createMockNode("node", settings);
diff --git a/core/src/test/java/org/elasticsearch/document/ShardInfoIT.java b/core/src/test/java/org/elasticsearch/document/ShardInfoIT.java
index d4907d8..4f28cf1 100644
--- a/core/src/test/java/org/elasticsearch/document/ShardInfoIT.java
+++ b/core/src/test/java/org/elasticsearch/document/ShardInfoIT.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.document;
 
-import org.elasticsearch.action.ActionWriteResponse;
+import org.elasticsearch.action.ReplicationResponse;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.action.admin.indices.recovery.RecoveryResponse;
 import org.elasticsearch.action.bulk.BulkItemResponse;
@@ -117,11 +117,11 @@ public class ShardInfoIT extends ESIntegTestCase {
         }
     }
 
-    private void assertShardInfo(ActionWriteResponse response) {
+    private void assertShardInfo(ReplicationResponse response) {
         assertShardInfo(response, numCopies, numNodes);
     }
 
-    private void assertShardInfo(ActionWriteResponse response, int expectedTotal, int expectedSuccessful) {
+    private void assertShardInfo(ReplicationResponse response, int expectedTotal, int expectedSuccessful) {
         assertThat(response.getShardInfo().getTotal(), greaterThanOrEqualTo(expectedTotal));
         assertThat(response.getShardInfo().getSuccessful(), greaterThanOrEqualTo(expectedSuccessful));
     }
diff --git a/core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java b/core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java
index aec73f2..e25b95b 100644
--- a/core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java
+++ b/core/src/test/java/org/elasticsearch/fieldstats/FieldStatsTests.java
@@ -67,7 +67,7 @@ public class FieldStatsTests extends ESSingleNodeTestCase {
     }
 
     public void testString() {
-        createIndex("test", Settings.EMPTY, "field", "value", "type=string");
+        createIndex("test", Settings.EMPTY, "test", "field", "type=string");
         for (int value = 0; value <= 10; value++) {
             client().prepareIndex("test", "test").setSource("field", String.format(Locale.ENGLISH, "%03d", value)).get();
         }
@@ -85,7 +85,7 @@ public class FieldStatsTests extends ESSingleNodeTestCase {
 
     public void testDouble() {
         String fieldName = "field";
-        createIndex("test", Settings.EMPTY, fieldName, "value", "type=double");
+        createIndex("test", Settings.EMPTY, "test", fieldName, "type=double");
         for (double value = -1; value <= 9; value++) {
             client().prepareIndex("test", "test").setSource(fieldName, value).get();
         }
@@ -102,7 +102,7 @@ public class FieldStatsTests extends ESSingleNodeTestCase {
 
     public void testFloat() {
         String fieldName = "field";
-        createIndex("test", Settings.EMPTY, fieldName, "value", "type=float");
+        createIndex("test", Settings.EMPTY, "test", fieldName, "type=float");
         for (float value = -1; value <= 9; value++) {
             client().prepareIndex("test", "test").setSource(fieldName, value).get();
         }
@@ -112,14 +112,14 @@ public class FieldStatsTests extends ESSingleNodeTestCase {
         assertThat(result.getAllFieldStats().get(fieldName).getMaxDoc(), equalTo(11l));
         assertThat(result.getAllFieldStats().get(fieldName).getDocCount(), equalTo(11l));
         assertThat(result.getAllFieldStats().get(fieldName).getDensity(), equalTo(100));
-        assertThat(result.getAllFieldStats().get(fieldName).getMinValue(), equalTo(-1.0));
-        assertThat(result.getAllFieldStats().get(fieldName).getMaxValue(), equalTo(9.0));
+        assertThat(result.getAllFieldStats().get(fieldName).getMinValue(), equalTo(-1f));
+        assertThat(result.getAllFieldStats().get(fieldName).getMaxValue(), equalTo(9f));
         assertThat(result.getAllFieldStats().get(fieldName).getMinValueAsString(), equalTo(Float.toString(-1)));
         assertThat(result.getAllFieldStats().get(fieldName).getMaxValueAsString(), equalTo(Float.toString(9)));
     }
 
     private void testNumberRange(String fieldName, String fieldType, long min, long max) {
-        createIndex("test", Settings.EMPTY, fieldName, "value", "type=" + fieldType);
+        createIndex("test", Settings.EMPTY, "test", fieldName, "type=" + fieldType);
         for (long value = min; value <= max; value++) {
             client().prepareIndex("test", "test").setSource(fieldName, value).get();
         }
@@ -180,11 +180,11 @@ public class FieldStatsTests extends ESSingleNodeTestCase {
     }
 
     public void testInvalidField() {
-        createIndex("test1", Settings.EMPTY, "field1", "value", "type=string");
+        createIndex("test1", Settings.EMPTY, "test", "field1", "type=string");
         client().prepareIndex("test1", "test").setSource("field1", "a").get();
         client().prepareIndex("test1", "test").setSource("field1", "b").get();
 
-        createIndex("test2", Settings.EMPTY, "field2", "value", "type=string");
+        createIndex("test2", Settings.EMPTY, "test", "field2",  "type=string");
         client().prepareIndex("test2", "test").setSource("field2", "a").get();
         client().prepareIndex("test2", "test").setSource("field2", "b").get();
         client().admin().indices().prepareRefresh().get();
diff --git a/core/src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java b/core/src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java
index 2c6a55d..15ddc9d 100644
--- a/core/src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java
@@ -57,7 +57,7 @@ public class GatewayMetaStateTests extends ESAllocationTestCase {
         //ridiculous settings to make sure we don't run into uninitialized because fo default
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 100)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", 100)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 100)
                 .build());
@@ -111,7 +111,7 @@ public class GatewayMetaStateTests extends ESAllocationTestCase {
         //ridiculous settings to make sure we don't run into uninitialized because fo default
         AllocationService strategy = createAllocationService(settingsBuilder()
                 .put("cluster.routing.allocation.concurrent_recoveries", 100)
-                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE_SETTING.getKey(), "always")
+                .put(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, "always")
                 .put("cluster.routing.allocation.cluster_concurrent_rebalance", 100)
                 .put("cluster.routing.allocation.node_initial_primaries_recoveries", 100)
                 .build());
diff --git a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
index d7f4c91..441314b 100644
--- a/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/MetaDataStateFormatTests.java
@@ -19,12 +19,7 @@
 package org.elasticsearch.gateway;
 
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.SimpleFSDirectory;
+import org.apache.lucene.store.*;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
@@ -33,11 +28,7 @@ import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
@@ -50,20 +41,10 @@ import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardOpenOption;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
+import java.util.*;
 import java.util.stream.StreamSupport;
 
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.startsWith;
+import static org.hamcrest.Matchers.*;
 
 @LuceneTestCase.SuppressFileSystems("ExtrasFS") // TODO: fix test to work with ExtrasFS
 public class MetaDataStateFormatTests extends ESTestCase {
@@ -349,7 +330,7 @@ public class MetaDataStateFormatTests extends ESTestCase {
 
         }
         List<Path> dirList = Arrays.asList(dirs);
-        Collections.shuffle(dirList, getRandom());
+        Collections.shuffle(dirList, random());
         MetaData loadedMetaData = format.loadLatestState(logger, dirList.toArray(new Path[0]));
         MetaData latestMetaData = meta.get(numStates-1);
         assertThat(loadedMetaData.clusterUUID(), not(equalTo("_na_")));
diff --git a/core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java
index dbdf747..2184fda 100644
--- a/core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java
@@ -82,9 +82,9 @@ public class RecoveryBackwardsCompatibilityIT extends ESBackcompatTestCase {
         SearchResponse countResponse = client().prepareSearch().setSize(0).get();
         assertHitCount(countResponse, numDocs);
 
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")).execute().actionGet();
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")).execute().actionGet();
         backwardsCluster().upgradeAllNodes();
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "all")).execute().actionGet();
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "all")).execute().actionGet();
         ensureGreen();
 
         countResponse = client().prepareSearch().setSize(0).get();
diff --git a/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java b/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
index 31e992a..01c76b4 100644
--- a/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
+++ b/core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java
@@ -360,7 +360,7 @@ public class RecoveryFromGatewayIT extends ESIntegTestCase {
             // Disable allocations while we are closing nodes
             client().admin().cluster().prepareUpdateSettings()
                     .setTransientSettings(settingsBuilder()
-                            .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), EnableAllocationDecider.Allocation.NONE))
+                            .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, EnableAllocationDecider.Allocation.NONE))
                     .get();
             logger.info("--> full cluster restart");
             internalCluster().fullRestart();
@@ -377,7 +377,7 @@ public class RecoveryFromGatewayIT extends ESIntegTestCase {
         // Disable allocations while we are closing nodes
         client().admin().cluster().prepareUpdateSettings()
                 .setTransientSettings(settingsBuilder()
-                        .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), EnableAllocationDecider.Allocation.NONE))
+                        .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, EnableAllocationDecider.Allocation.NONE))
                 .get();
         logger.info("--> full cluster restart");
         internalCluster().fullRestart();
diff --git a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
index 2b76d03..c230613 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
@@ -59,7 +59,6 @@ import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.script.ScriptContextRegistry;
 import org.elasticsearch.script.ScriptEngineService;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.IndexSettingsModule;
 import org.elasticsearch.test.engine.MockEngineFactory;
@@ -102,7 +101,6 @@ public class IndexModuleTests extends ESTestCase {
         BigArrays bigArrays = new BigArrays(recycler, circuitBreakerService);
         IndicesFieldDataCache indicesFieldDataCache = new IndicesFieldDataCache(settings, new IndicesFieldDataCacheListener(circuitBreakerService), threadPool);
         Set<ScriptEngineService> scriptEngines = new HashSet<>();
-        scriptEngines.add(new MustacheScriptEngineService(settings));
         scriptEngines.addAll(Arrays.asList(scriptEngineServices));
         ScriptService scriptService = new ScriptService(settings, environment, scriptEngines, new ResourceWatcherService(settings, threadPool), new ScriptContextRegistry(Collections.emptyList()));
         IndicesQueriesRegistry indicesQueriesRegistry = new IndicesQueriesRegistry(settings, Collections.emptySet(), new NamedWriteableRegistry());
diff --git a/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java b/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
index 457cf31..21ecdf7 100644
--- a/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
+++ b/core/src/test/java/org/elasticsearch/index/TransportIndexFailuresIT.java
@@ -56,7 +56,7 @@ public class TransportIndexFailuresIT extends ESIntegTestCase {
             .put("discovery.type", "zen") // <-- To override the local setting if set externally
             .put(FaultDetection.SETTING_PING_TIMEOUT, "1s") // <-- for hitting simulated network failures quickly
             .put(FaultDetection.SETTING_PING_RETRIES, "1") // <-- for hitting simulated network failures quickly
-            .put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "1s") // <-- for hitting simulated network failures quickly
+            .put(DiscoverySettings.PUBLISH_TIMEOUT, "1s") // <-- for hitting simulated network failures quickly
             .put("discovery.zen.minimum_master_nodes", 1)
             .build();
 
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
index b7bdbb2..d931b47 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java
@@ -237,7 +237,7 @@ public class NGramTokenizerFactoryTests extends ESTokenStreamTestCase {
 
 
     private Version randomVersion(Random random) throws IllegalArgumentException, IllegalAccessException {
-        Field[] declaredFields = Version.class.getDeclaredFields();
+        Field[] declaredFields = Version.class.getFields();
         List<Field> versionFields = new ArrayList<>();
         for (Field field : declaredFields) {
             if ((field.getModifiers() & Modifier.STATIC) != 0 && field.getName().startsWith("V_") && field.getType() == Version.class) {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
index f01df63..966ea01 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.index.mapper;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.mapping.get.GetMappingsResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -28,15 +29,21 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.IndexService;
+import org.elasticsearch.index.mapper.core.DoubleFieldMapper;
+import org.elasticsearch.index.mapper.core.FloatFieldMapper;
 import org.elasticsearch.index.mapper.core.IntegerFieldMapper;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
 import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
 
 import static java.util.Collections.emptyMap;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.nullValue;
 
 public class DynamicMappingTests extends ESSingleNodeTestCase {
@@ -407,4 +414,26 @@ public class DynamicMappingTests extends ESSingleNodeTestCase {
             // expected
         }
     }
+
+    public void testDefaultFloatingPointMappings() throws IOException {
+        DocumentMapper mapper = createIndex("test").mapperService().documentMapperWithAutoCreate("type").getDocumentMapper();
+        doTestDefaultFloatingPointMappings(mapper, XContentFactory.jsonBuilder());
+        doTestDefaultFloatingPointMappings(mapper, XContentFactory.yamlBuilder());
+        doTestDefaultFloatingPointMappings(mapper, XContentFactory.smileBuilder());
+        doTestDefaultFloatingPointMappings(mapper, XContentFactory.cborBuilder());
+    }
+
+    private void doTestDefaultFloatingPointMappings(DocumentMapper mapper, XContentBuilder builder) throws IOException {
+        BytesReference source = builder.startObject()
+                .field("foo", 3.2f) // float
+                .field("bar", 3.2d) // double
+                .field("baz", (double) 3.2f) // double that can be accurately represented as a float
+                .endObject().bytes();
+        ParsedDocument parsedDocument = mapper.parse("index", "type", "id", source);
+        Mapping update = parsedDocument.dynamicMappingsUpdate();
+        assertNotNull(update);
+        assertThat(update.root().getMapper("foo"), instanceOf(FloatFieldMapper.class));
+        assertThat(update.root().getMapper("bar"), instanceOf(FloatFieldMapper.class));
+        assertThat(update.root().getMapper("baz"), instanceOf(FloatFieldMapper.class));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java
index 2b20052..f4a7507 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/MapperServiceTests.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.index.mapper;
 
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
 import org.elasticsearch.common.compress.CompressedXContent;
@@ -117,8 +118,9 @@ public class MapperServiceTests extends ESSingleNodeTestCase {
             if (t instanceof ExecutionException) {
                 t = ((ExecutionException) t).getCause();
             }
-            if (t instanceof IllegalArgumentException) {
-                assertEquals("It is forbidden to index into the default mapping [_default_]", t.getMessage());
+            final Throwable throwable = ExceptionsHelper.unwrapCause(t);
+            if (throwable instanceof IllegalArgumentException) {
+                assertEquals("It is forbidden to index into the default mapping [_default_]", throwable.getMessage());
             } else {
                 throw t;
             }
@@ -133,8 +135,9 @@ public class MapperServiceTests extends ESSingleNodeTestCase {
             if (t instanceof ExecutionException) {
                 t = ((ExecutionException) t).getCause();
             }
-            if (t instanceof IllegalArgumentException) {
-                assertEquals("It is forbidden to index into the default mapping [_default_]", t.getMessage());
+            final Throwable throwable = ExceptionsHelper.unwrapCause(t);
+            if (throwable instanceof IllegalArgumentException) {
+                assertEquals("It is forbidden to index into the default mapping [_default_]", throwable.getMessage());
             } else {
                 throw t;
             }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
index bfb7ead..bbba343 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java
@@ -49,21 +49,12 @@ import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.hamcrest.Matchers;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.StreamsUtils.copyToBytesFromClasspath;
 import static org.elasticsearch.test.StreamsUtils.copyToStringFromClasspath;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.empty;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasItem;
-import static org.hamcrest.Matchers.hasSize;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class SimpleAllMapperTests extends ESSingleNodeTestCase {
 
@@ -251,7 +242,7 @@ public class SimpleAllMapperTests extends ESSingleNodeTestCase {
             if (randomBoolean()) {
                 booleanOptionList.add(new Tuple<>("store_term_vector_payloads", tv_payloads = randomBoolean()));
             }
-            Collections.shuffle(booleanOptionList, getRandom());
+            Collections.shuffle(booleanOptionList, random());
             for (Tuple<String, Boolean> option : booleanOptionList) {
                 mappingBuilder.field(option.v1(), option.v2().booleanValue());
             }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/core/MultiFieldCopyToMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/core/MultiFieldCopyToMapperTests.java
new file mode 100644
index 0000000..821eaeb
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/mapper/core/MultiFieldCopyToMapperTests.java
@@ -0,0 +1,105 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+
+package org.elasticsearch.index.mapper.core;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.collect.Tuple;
+import org.elasticsearch.common.compress.CompressedXContent;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.MapperTestUtils;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.MapperParsingException;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.VersionUtils;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.hamcrest.core.IsEqual.equalTo;
+
+public class MultiFieldCopyToMapperTests extends ESTestCase {
+
+    public void testExceptionForCopyToInMultiFields() throws IOException {
+        XContentBuilder mapping = createMappinmgWithCopyToInMultiField();
+        Tuple<List<Version>, List<Version>> versionsWithAndWithoutExpectedExceptions = versionsWithAndWithoutExpectedExceptions();
+
+        // first check that for newer versions we throw exception if copy_to is found withing multi field
+        Version indexVersion = randomFrom(versionsWithAndWithoutExpectedExceptions.v1());
+        MapperService mapperService = MapperTestUtils.newMapperService(createTempDir(), Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexVersion).build());
+        try {
+            mapperService.parse("type", new CompressedXContent(mapping.string()), true);
+            fail("Parsing should throw an exception because the mapping contains a copy_to in a multi field");
+        } catch (MapperParsingException e) {
+            assertThat(e.getMessage(), equalTo("copy_to in multi fields is not allowed. Found the copy_to in field [c] which is within a multi field."));
+        }
+
+        // now test that with an older version the pasring just works
+        indexVersion = randomFrom(versionsWithAndWithoutExpectedExceptions.v2());
+        mapperService = MapperTestUtils.newMapperService(createTempDir(), Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexVersion).build());
+        DocumentMapper documentMapper = mapperService.parse("type", new CompressedXContent(mapping.string()), true);
+        assertFalse(documentMapper.mapping().toString().contains("copy_to"));
+    }
+
+    private static XContentBuilder createMappinmgWithCopyToInMultiField() throws IOException {
+        XContentBuilder mapping = jsonBuilder();
+        mapping.startObject()
+            .startObject("type")
+            .startObject("properties")
+            .startObject("a")
+            .field("type", "string")
+            .endObject()
+            .startObject("b")
+            .field("type", "string")
+            .startObject("fields")
+            .startObject("c")
+            .field("type", "string")
+            .field("copy_to", "a")
+            .endObject()
+            .endObject()
+            .endObject()
+            .endObject()
+            .endObject()
+            .endObject();
+        return mapping;
+    }
+
+    // returs a tuple where
+    // v1 is a list of versions for which we expect an excpetion when a copy_to in multi fields is found and
+    // v2 is older versions where we throw no exception and we just log a warning
+    private static Tuple<List<Version>, List<Version>> versionsWithAndWithoutExpectedExceptions() {
+        List<Version> versionsWithException = new ArrayList<>();
+        List<Version> versionsWithoutException = new ArrayList<>();
+        for (Version version : VersionUtils.allVersions()) {
+            if (version.after(Version.V_2_1_0) ||
+                (version.after(Version.V_2_0_1) && version.before(Version.V_2_1_0))) {
+                versionsWithException.add(version);
+            } else {
+                versionsWithoutException.add(version);
+            }
+        }
+        return new Tuple<>(versionsWithException, versionsWithoutException);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java b/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java
index b1224d5..ba9303e 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java
@@ -19,11 +19,7 @@
 
 package org.elasticsearch.index.mapper.core;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.CannedTokenStream;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.*;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
@@ -85,7 +81,7 @@ public class TokenCountFieldMapperTests extends ESSingleNodeTestCase {
         t2.setPositionIncrement(2);  // Count funny tokens with more than one increment
         int finalTokenIncrement = 4; // Count the final token increment on the rare token streams that have them
         Token[] tokens = new Token[] {t1, t2, t3};
-        Collections.shuffle(Arrays.asList(tokens), getRandom());
+        Collections.shuffle(Arrays.asList(tokens), random());
         final TokenStream tokenStream = new CannedTokenStream(finalTokenIncrement, 0, tokens);
         // TODO: we have no CannedAnalyzer?
         Analyzer analyzer = new Analyzer() {
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java b/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java
index a5a073d..58fa8fd 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java
@@ -479,7 +479,7 @@ public class MultiFieldTests extends ESSingleNodeTestCase {
             .startObject("my_field").field("type", "string").startObject("fields").startObject(MY_MULTI_FIELD)
             .field("type", "string").startObject("fielddata");
         String[] keys = possibleSettings.keySet().toArray(new String[]{});
-        Collections.shuffle(Arrays.asList(keys));
+        Collections.shuffle(Arrays.asList(keys), random());
         for(int i = randomIntBetween(0, possibleSettings.size()-1); i >= 0; --i)
             builder.field(keys[i], possibleSettings.get(keys[i]));
         builder.endObject().endObject().endObject().endObject().endObject().endObject().endObject();
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java b/core/src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java
index de2957c..d93ae9b 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java
@@ -24,6 +24,8 @@ import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.DocValuesType;
 import org.apache.lucene.index.IndexableField;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.IndexService;
@@ -41,9 +43,11 @@ import org.elasticsearch.index.mapper.string.SimpleStringMappingTests;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 
 import java.io.IOException;
+import java.util.Arrays;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.hamcrest.Matchers.containsString;
 import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.nullValue;
@@ -510,4 +514,62 @@ public class SimpleNumericTests extends ESSingleNodeTestCase {
         assertThat(ts, instanceOf(NumericTokenStream.class));
         assertEquals(expected, ((NumericTokenStream)ts).getPrecisionStep());
     }
+
+    public void testTermVectorsBackCompat() throws Exception {
+        for (String type : Arrays.asList("byte", "short", "integer", "long", "float", "double")) {
+            doTestTermVectorsBackCompat(type);
+        }
+    }
+
+    private void doTestTermVectorsBackCompat(String type) throws Exception {
+        DocumentMapperParser parser = createIndex("index-" + type).mapperService().documentMapperParser();
+        String mappingWithTV = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("properties")
+                    .startObject("foo")
+                        .field("type", type)
+                        .field("term_vector", "yes")
+                    .endObject()
+                .endObject().endObject().endObject().string();
+        try {
+            parser.parse(mappingWithTV);
+            fail();
+        } catch (MapperParsingException e) {
+            assertThat(e.getMessage(), containsString("Mapping definition for [foo] has unsupported parameters:  [term_vector : yes]"));
+        }
+
+        Settings oldIndexSettings = Settings.builder()
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_2_1_0)
+                .build();
+        parser = createIndex("index2-" + type, oldIndexSettings).mapperService().documentMapperParser();
+        parser.parse(mappingWithTV); // no exception
+    }
+
+    public void testAnalyzerBackCompat() throws Exception {
+        for (String type : Arrays.asList("byte", "short", "integer", "long", "float", "double")) {
+            doTestAnalyzerBackCompat(type);
+        }
+    }
+
+    private void doTestAnalyzerBackCompat(String type) throws Exception {
+        DocumentMapperParser parser = createIndex("index-" + type).mapperService().documentMapperParser();
+        String mappingWithTV = XContentFactory.jsonBuilder().startObject().startObject("type")
+                .startObject("properties")
+                    .startObject("foo")
+                        .field("type", type)
+                        .field("analyzer", "keyword")
+                    .endObject()
+                .endObject().endObject().endObject().string();
+        try {
+            parser.parse(mappingWithTV);
+            fail();
+        } catch (MapperParsingException e) {
+            assertThat(e.getMessage(), containsString("Mapping definition for [foo] has unsupported parameters:  [analyzer : keyword]"));
+        }
+
+        Settings oldIndexSettings = Settings.builder()
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_2_1_0)
+                .build();
+        parser = createIndex("index2-" + type, oldIndexSettings).mapperService().documentMapperParser();
+        parser.parse(mappingWithTV); // no exception
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/source/CompressSourceMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/source/CompressSourceMappingTests.java
deleted file mode 100644
index 7c1875b..0000000
--- a/core/src/test/java/org/elasticsearch/index/mapper/source/CompressSourceMappingTests.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.mapper.source;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.compress.CompressorFactory;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.ParsedDocument;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- *
- */
-public class CompressSourceMappingTests extends ESSingleNodeTestCase {
-    Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build();
-
-    public void testCompressDisabled() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_source").field("compress", false).endObject()
-                .endObject().endObject().string();
-
-        DocumentMapper documentMapper = createIndex("test", settings).mapperService().documentMapperParser().parse(mapping);
-
-        ParsedDocument doc = documentMapper.parse("test", "type", "1", XContentFactory.jsonBuilder().startObject()
-                .field("field1", "value1")
-                .field("field2", "value2")
-                .endObject().bytes());
-        BytesRef bytes = doc.rootDoc().getBinaryValue("_source");
-        assertThat(CompressorFactory.isCompressed(new BytesArray(bytes)), equalTo(false));
-    }
-
-    public void testCompressEnabled() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_source").field("compress", true).endObject()
-                .endObject().endObject().string();
-
-        DocumentMapper documentMapper = createIndex("test", settings).mapperService().documentMapperParser().parse(mapping);
-
-        ParsedDocument doc = documentMapper.parse("test", "type", "1", XContentFactory.jsonBuilder().startObject()
-                .field("field1", "value1")
-                .field("field2", "value2")
-                .endObject().bytes());
-
-        BytesRef bytes = doc.rootDoc().getBinaryValue("_source");
-        assertThat(CompressorFactory.isCompressed(new BytesArray(bytes)), equalTo(true));
-    }
-
-    public void testCompressThreshold() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_source").field("compress_threshold", "200b").endObject()
-                .endObject().endObject().string();
-
-        DocumentMapper documentMapper = createIndex("test", settings).mapperService().documentMapperParser().parse(mapping);
-
-        ParsedDocument doc = documentMapper.parse("test", "type", "1", XContentFactory.jsonBuilder().startObject()
-                .field("field1", "value1")
-                .endObject().bytes());
-
-        BytesRef bytes = doc.rootDoc().getBinaryValue("_source");
-        assertThat(CompressorFactory.isCompressed(new BytesArray(bytes)), equalTo(false));
-
-        doc = documentMapper.parse("test", "type", "1", XContentFactory.jsonBuilder().startObject()
-                .field("field1", "value1")
-                .field("field2", "value2 xxxxxxxxxxxxxx yyyyyyyyyyyyyyyyyyy zzzzzzzzzzzzzzzzz")
-                .field("field2", "value2 xxxxxxxxxxxxxx yyyyyyyyyyyyyyyyyyy zzzzzzzzzzzzzzzzz")
-                .field("field2", "value2 xxxxxxxxxxxxxx yyyyyyyyyyyyyyyyyyy zzzzzzzzzzzzzzzzz")
-                .field("field2", "value2 xxxxxxxxxxxxxx yyyyyyyyyyyyyyyyyyy zzzzzzzzzzzzzzzzz")
-                .endObject().bytes());
-
-        bytes = doc.rootDoc().getBinaryValue("_source");
-        assertThat(CompressorFactory.isCompressed(new BytesArray(bytes)), equalTo(true));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java
index 4ec0ff5..364e9f2 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java
@@ -31,6 +31,7 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.mapper.*;
 import org.elasticsearch.test.ESSingleNodeTestCase;
+import org.elasticsearch.test.VersionUtils;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -63,51 +64,16 @@ public class DefaultSourceMappingTests extends ESSingleNodeTestCase {
         assertThat(XContentFactory.xContentType(doc.source()), equalTo(XContentType.SMILE));
     }
 
-    public void testJsonFormat() throws Exception {
+    public void testFormatBackCompat() throws Exception {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .startObject("_source").field("format", "json").endObject()
                 .endObject().endObject().string();
+        Settings settings = Settings.builder()
+                .put(IndexMetaData.SETTING_VERSION_CREATED, VersionUtils.randomVersionBetween(random(), Version.V_2_0_0, Version.V_2_2_0))
+                .build();
 
-        DocumentMapperParser parser = createIndex("test").mapperService().documentMapperParser();
-        DocumentMapper documentMapper = parser.parse(mapping);
-        ParsedDocument doc = documentMapper.parse("test", "type", "1", XContentFactory.jsonBuilder().startObject()
-                .field("field", "value")
-                .endObject().bytes());
-
-        assertThat(XContentFactory.xContentType(doc.source()), equalTo(XContentType.JSON));
-
-        documentMapper = parser.parse(mapping);
-        doc = documentMapper.parse("test", "type", "1", XContentFactory.smileBuilder().startObject()
-            .field("field", "value")
-            .endObject().bytes());
-
-        assertThat(XContentFactory.xContentType(doc.source()), equalTo(XContentType.JSON));
-    }
-
-    public void testJsonFormatCompressedBackcompat() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("_source").field("format", "json").field("compress", true).endObject()
-                .endObject().endObject().string();
-
-        Settings backcompatSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build();
-        DocumentMapperParser parser = createIndex("test", backcompatSettings).mapperService().documentMapperParser();
-        DocumentMapper documentMapper = parser.parse(mapping);
-        ParsedDocument doc = documentMapper.parse("test", "type", "1", XContentFactory.jsonBuilder().startObject()
-                .field("field", "value")
-                .endObject().bytes());
-
-        assertThat(CompressorFactory.isCompressed(doc.source()), equalTo(true));
-        byte[] uncompressed = CompressorFactory.uncompressIfNeeded(doc.source()).toBytes();
-        assertThat(XContentFactory.xContentType(uncompressed), equalTo(XContentType.JSON));
-
-        documentMapper = parser.parse(mapping);
-        doc = documentMapper.parse("test", "type", "1", XContentFactory.smileBuilder().startObject()
-                .field("field", "value")
-                .endObject().bytes());
-
-        assertThat(CompressorFactory.isCompressed(doc.source()), equalTo(true));
-        uncompressed = CompressorFactory.uncompressIfNeeded(doc.source()).toBytes();
-        assertThat(XContentFactory.xContentType(uncompressed), equalTo(XContentType.JSON));
+        DocumentMapperParser parser = createIndex("test", settings).mapperService().documentMapperParser();
+        parser.parse(mapping); // no exception
     }
 
     public void testIncludes() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index aebf00e..aa97d72 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -84,7 +84,6 @@ import org.elasticsearch.indices.mapper.MapperRegistry;
 import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.script.*;
 import org.elasticsearch.script.Script.ScriptParseException;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.IndexSettingsModule;
@@ -205,15 +204,8 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
                         MockScriptEngine mockScriptEngine = new MockScriptEngine();
                         Multibinder<ScriptEngineService> multibinder = Multibinder.newSetBinder(binder(), ScriptEngineService.class);
                         multibinder.addBinding().toInstance(mockScriptEngine);
-                        try {
-                            Class.forName("com.github.mustachejava.Mustache");
-                        } catch(ClassNotFoundException e) {
-                            throw new IllegalStateException("error while loading mustache", e);
-                        }
-                        MustacheScriptEngineService mustacheScriptEngineService = new MustacheScriptEngineService(settings);
                         Set<ScriptEngineService> engines = new HashSet<>();
                         engines.add(mockScriptEngine);
-                        engines.add(mustacheScriptEngineService);
                         List<ScriptContext.Plugin> customContexts = new ArrayList<>();
                         bind(ScriptContextRegistry.class).toInstance(new ScriptContextRegistry(customContexts));
                         try {
@@ -836,21 +828,21 @@ public abstract class AbstractQueryTestCase<QB extends AbstractQueryBuilder<QB>>
         AbstractQueryTestCase delegate;
         @Override
         public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
-            if (method.equals(Client.class.getDeclaredMethod("get", GetRequest.class))) {
+            if (method.equals(Client.class.getMethod("get", GetRequest.class))) {
                 return new PlainActionFuture<GetResponse>() {
                     @Override
                     public GetResponse get() throws InterruptedException, ExecutionException {
                         return delegate.executeGet((GetRequest) args[0]);
                     }
                 };
-            } else if (method.equals(Client.class.getDeclaredMethod("multiTermVectors", MultiTermVectorsRequest.class))) {
+            } else if (method.equals(Client.class.getMethod("multiTermVectors", MultiTermVectorsRequest.class))) {
                     return new PlainActionFuture<MultiTermVectorsResponse>() {
                         @Override
                         public MultiTermVectorsResponse get() throws InterruptedException, ExecutionException {
                             return delegate.executeMultiTermVectors((MultiTermVectorsRequest) args[0]);
                         }
                     };
-            } else if (method.equals(Object.class.getDeclaredMethod("toString"))) {
+            } else if (method.equals(Object.class.getMethod("toString"))) {
                 return "MockClient";
             }
             throw new UnsupportedOperationException("this test can't handle calls to: " + method);
diff --git a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTests.java
deleted file mode 100644
index 253509b..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTests.java
+++ /dev/null
@@ -1,107 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.containsString;
-
-public class MissingQueryBuilderTests extends AbstractQueryTestCase<MissingQueryBuilder> {
-
-    @Override
-    protected MissingQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName = randomBoolean() ? randomFrom(MAPPED_FIELD_NAMES) : randomAsciiOfLengthBetween(1, 10);
-        Boolean existence = randomBoolean();
-        Boolean nullValue = randomBoolean();
-        if (existence == false && nullValue == false) {
-            if (randomBoolean()) {
-                existence = true;
-            } else {
-                nullValue = true;
-            }
-        }
-        return new MissingQueryBuilder(fieldName, nullValue, existence);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MissingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        // too many mapping dependent cases to test, we don't want to end up
-        // duplication the toQuery method
-    }
-
-    public void testIllegalArguments() {
-        try {
-            if (randomBoolean()) {
-                new MissingQueryBuilder("", true, true);
-            } else {
-                new MissingQueryBuilder(null, true, true);
-            }
-            fail("must not be null or empty");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new MissingQueryBuilder("fieldname", false, false);
-            fail("existence and nullValue cannot both be false");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-
-        try {
-            new MissingQueryBuilder("fieldname", MissingQueryBuilder.DEFAULT_NULL_VALUE, false);
-            fail("existence and nullValue cannot both be false");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    public void testBothNullValueAndExistenceFalse() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        try {
-            MissingQueryBuilder.newFilter(context, "field", false, false);
-            fail("Expected QueryShardException");
-        } catch (QueryShardException e) {
-            assertThat(e.getMessage(), containsString("missing must have either existence, or null_value"));
-        }
-    }
-
-    public void testFromJson() throws IOException {
-        String json =
-            "{\n" + 
-                "  \"missing\" : {\n" + 
-                "    \"field\" : \"user\",\n" + 
-                "    \"null_value\" : false,\n" + 
-                "    \"existence\" : true,\n" + 
-                "    \"boost\" : 1.0\n" + 
-                "  }\n" + 
-                "}";
-
-        MissingQueryBuilder parsed = (MissingQueryBuilder) parseQuery(json);
-        checkGeneratedJson(json, parsed);
-
-        assertEquals(json, false, parsed.nullValue());
-        assertEquals(json, true, parsed.existence());
-        assertEquals(json, "user", parsed.fieldPattern());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/QueryDSLDocumentationTests.java b/core/src/test/java/org/elasticsearch/index/query/QueryDSLDocumentationTests.java
index 7ba1d11..0289874 100644
--- a/core/src/test/java/org/elasticsearch/index/query/QueryDSLDocumentationTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/QueryDSLDocumentationTests.java
@@ -58,7 +58,6 @@ import static org.elasticsearch.index.query.QueryBuilders.idsQuery;
 import static org.elasticsearch.index.query.QueryBuilders.indicesQuery;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
 import static org.elasticsearch.index.query.QueryBuilders.moreLikeThisQuery;
 import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
 import static org.elasticsearch.index.query.QueryBuilders.nestedQuery;
@@ -240,10 +239,6 @@ public class QueryDSLDocumentationTests extends ESTestCase {
         matchQuery("name", "kimchy elasticsearch");
     }
 
-    public void testMissing() {
-        missingQuery("user", true, true);
-    }
-
     public void testMLT() {
         String[] fields = {"name.first", "name.last"};
         String[] texts = {"text like this one"};
diff --git a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java
index bf580e5..4df799e 100644
--- a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTests.java
@@ -23,7 +23,9 @@ import org.apache.lucene.search.NumericRangeQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.ElasticsearchParseException;
+import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.lucene.BytesRefs;
+import org.hamcrest.core.IsEqual;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -353,4 +355,42 @@ public class RangeQueryBuilderTests extends AbstractQueryTestCase<RangeQueryBuil
         assertEquals(json, "2015-01-01 00:00:00", parsed.from());
         assertEquals(json, "now", parsed.to());
     }
+
+    public void testNamedQueryParsing() throws IOException {
+        String json =
+                "{\n" +
+                "  \"range\" : {\n" +
+                "    \"timestamp\" : {\n" +
+                "      \"from\" : \"2015-01-01 00:00:00\",\n" +
+                "      \"to\" : \"now\",\n" +
+                "      \"boost\" : 1.0,\n" +
+                "      \"_name\" : \"my_range\"\n" +
+                "    }\n" +
+                "  }\n" +
+                "}";
+        assertNotNull(parseQuery(json));
+
+        json =
+                "{\n" +
+                "  \"range\" : {\n" +
+                "    \"timestamp\" : {\n" +
+                "      \"from\" : \"2015-01-01 00:00:00\",\n" +
+                "      \"to\" : \"now\",\n" +
+                "      \"boost\" : 1.0\n" +
+                "    },\n" +
+                "    \"_name\" : \"my_range\"\n" +
+                "  }\n" +
+                "}";
+
+        // non strict parsing should accept "_name" on top level
+        assertNotNull(parseQuery(json, ParseFieldMatcher.EMPTY));
+
+        // with strict parsing, ParseField will throw exception
+        try {
+            parseQuery(json, ParseFieldMatcher.STRICT);
+            fail("Strict parsing should trigger exception for '_name' on top level");
+        } catch (IllegalArgumentException e) {
+            assertThat(e.getMessage(), equalTo("Deprecated field [_name] used, replaced by [query name is not supported in short version of range query]"));
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
index cdf0c5d..df7eb3c 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
@@ -51,7 +51,7 @@ public class TemplateQueryBuilderTests extends AbstractQueryTestCase<TemplateQue
 
     @Override
     protected TemplateQueryBuilder doCreateTestQueryBuilder() {
-        return new TemplateQueryBuilder(new Template(templateBase.toString()));
+        return new TemplateQueryBuilder(new Template(templateBase.toString(), ScriptType.INLINE, "mockscript", null, null));
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
deleted file mode 100644
index d4816f8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
+++ /dev/null
@@ -1,503 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.action.index.IndexRequest.OpType;
-import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.indexedscripts.delete.DeleteIndexedScriptResponse;
-import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptResponse;
-import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder;
-import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
-import org.elasticsearch.action.search.SearchPhaseExecutionException;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-
-/**
- * Full integration test of the template query plugin.
- */
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE)
-public class TemplateQueryIT extends ESIntegTestCase {
-
-    @Before
-    public void setup() throws IOException {
-        createIndex("test");
-        ensureGreen("test");
-
-        index("test", "testtype", "1", jsonBuilder().startObject().field("text", "value1").endObject());
-        index("test", "testtype", "2", jsonBuilder().startObject().field("text", "value2").endObject());
-        refresh();
-    }
-
-    @Override
-    public Settings nodeSettings(int nodeOrdinal) {
-        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
-                .put("path.conf", this.getDataPath("config")).build();
-    }
-
-    public void testTemplateInBody() throws IOException {
-        Map<String, Object> vars = new HashMap<>();
-        vars.put("template", "all");
-
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("{\"match_{{template}}\": {}}\"", ScriptType.INLINE, null,
-                null, vars));
-        SearchResponse sr = client().prepareSearch().setQuery(builder)
-                .execute().actionGet();
-        assertHitCount(sr, 2);
-    }
-
-    public void testTemplateInBodyWithSize() throws IOException {
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        SearchResponse sr = client().prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().size(0).query(
-                                QueryBuilders.templateQuery(new Template("{ \"match_{{template}}\": {} }",
-                                        ScriptType.INLINE, null, null, params)))).execute()
-                .actionGet();
-        assertNoFailures(sr);
-        assertThat(sr.getHits().hits().length, equalTo(0));
-    }
-
-    public void testTemplateWOReplacementInBody() throws IOException {
-        Map<String, Object> vars = new HashMap<>();
-
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
-                "{\"match_all\": {}}\"", ScriptType.INLINE, null, null, vars));
-        SearchResponse sr = client().prepareSearch().setQuery(builder)
-                .execute().actionGet();
-        assertHitCount(sr, 2);
-    }
-
-    public void testTemplateInFile() {
-        Map<String, Object> vars = new HashMap<>();
-        vars.put("template", "all");
-
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
-                "storedTemplate", ScriptService.ScriptType.FILE, null, null, vars));
-        SearchResponse sr = client().prepareSearch().setQuery(builder)
-                .execute().actionGet();
-        assertHitCount(sr, 2);
-    }
-
-    public void testRawFSTemplate() throws IOException {
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("storedTemplate", ScriptType.FILE, null, null, params));
-        SearchResponse sr = client().prepareSearch().setQuery(builder).get();
-        assertHitCount(sr, 2);
-    }
-
-    public void testSearchRequestTemplateSource() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-
-        String query = "{ \"template\" : { \"query\": {\"match_{{template}}\": {} } }, \"params\" : { \"template\":\"all\" } }";
-        searchRequest.template(parseTemplate(query));
-
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertHitCount(searchResponse, 2);
-    }
-
-    private Template parseTemplate(String template) throws IOException {
-        try (XContentParser parser = XContentFactory.xContent(template).createParser(template)) {
-            return TemplateQueryParser.parse(parser, ParseFieldMatcher.EMPTY, "params", "template");
-        }
-    }
-
-    // Releates to #6318
-    public void testSearchRequestFail() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        try {
-            String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  } }";
-            searchRequest.template(parseTemplate(query));
-            client().search(searchRequest).get();
-            fail("expected exception");
-        } catch (Exception ex) {
-            // expected - no params
-        }
-        String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  }, \"params\" : { \"my_size\": 1 } }";
-        searchRequest.template(parseTemplate(query));
-
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    public void testThatParametersCanBeSet() throws Exception {
-        index("test", "type", "1", jsonBuilder().startObject().field("theField", "foo").endObject());
-        index("test", "type", "2", jsonBuilder().startObject().field("theField", "foo 2").endObject());
-        index("test", "type", "3", jsonBuilder().startObject().field("theField", "foo 3").endObject());
-        index("test", "type", "4", jsonBuilder().startObject().field("theField", "foo 4").endObject());
-        index("test", "type", "5", jsonBuilder().startObject().field("otherField", "foo").endObject());
-        refresh();
-
-        Map<String, Object> templateParams = new HashMap<>();
-        templateParams.put("mySize", "2");
-        templateParams.put("myField", "theField");
-        templateParams.put("myValue", "foo");
-
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type")
-                .setTemplate(new Template("full-query-template", ScriptType.FILE, MustacheScriptEngineService.NAME, null, templateParams))
-                .get();
-        assertHitCount(searchResponse, 4);
-        // size kicks in here...
-        assertThat(searchResponse.getHits().getHits().length, is(2));
-
-        templateParams.put("myField", "otherField");
-        searchResponse = client().prepareSearch("test").setTypes("type")
-                .setTemplate(new Template("full-query-template", ScriptType.FILE, MustacheScriptEngineService.NAME, null, templateParams))
-                .get();
-        assertHitCount(searchResponse, 1);
-    }
-
-    public void testSearchTemplateQueryFromFile() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        String query = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
-                + "    \"myField\": \"text\"," + "    \"myValue\": \"value1\"" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    /**
-     * Test that template can be expressed as a single escaped string.
-     */
-    public void testTemplateQueryAsEscapedString() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        String query = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
-                + "  \"params\":{" + "    \"size\": 1" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    /**
-     * Test that template can contain conditional clause. In this case it is at
-     * the beginning of the string.
-     */
-    public void testTemplateQueryAsEscapedStringStartingWithConditionalClause() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        String templateString = "{"
-                + "  \"template\" : \"{ {{#use_size}} \\\"size\\\": \\\"{{size}}\\\", {{/use_size}} \\\"query\\\":{\\\"match_all\\\":{}}}\","
-                + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    /**
-     * Test that template can contain conditional clause. In this case it is at
-     * the end of the string.
-     */
-    public void testTemplateQueryAsEscapedStringWithConditionalClauseAtEnd() throws Exception {
-        SearchRequest searchRequest = new SearchRequest();
-        searchRequest.indices("_all");
-        String templateString = "{"
-                + "  \"inline\" : \"{ \\\"query\\\":{\\\"match_all\\\":{}} {{#use_size}}, \\\"size\\\": \\\"{{size}}\\\" {{/use_size}} }\","
-                + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
-        SearchResponse searchResponse = client().search(searchRequest).get();
-        assertThat(searchResponse.getHits().hits().length, equalTo(1));
-    }
-
-    public void testIndexedTemplateClient() throws Exception {
-        createIndex(ScriptService.SCRIPT_INDEX);
-        ensureGreen(ScriptService.SCRIPT_INDEX);
-
-        PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "testTemplate", "{" +
-                "\"template\":{" +
-                "                \"query\":{" +
-                "                   \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                "}" +
-                "}").get();
-
-        assertTrue(scriptResponse.isCreated());
-
-        scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "testTemplate", "{" +
-                "\"template\":{" +
-                "                \"query\":{" +
-                "                   \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                "}" +
-                "}").get();
-
-        assertEquals(scriptResponse.getVersion(), 2);
-
-        GetIndexedScriptResponse getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "testTemplate").get();
-        assertTrue(getResponse.isExists());
-
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-
-        builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
-        builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
-        builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
-        builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
-        builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
-
-        indexRandom(true, builders);
-
-        Map<String, Object> templateParams = new HashMap<>();
-        templateParams.put("fieldParam", "foo");
-
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type")
-                .setTemplate(new Template("testTemplate", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
-                .get();
-        assertHitCount(searchResponse, 4);
-
-        DeleteIndexedScriptResponse deleteResponse = client().prepareDeleteIndexedScript(MustacheScriptEngineService.NAME, "testTemplate")
-                .get();
-        assertTrue(deleteResponse.isFound());
-
-        getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "testTemplate").get();
-        assertFalse(getResponse.isExists());
-
-        try {
-            client().prepareSearch("test")
-                    .setTypes("type")
-                    .setTemplate(
-                            new Template("/template_index/mustache/1000", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                    templateParams)).get();
-            fail("Expected SearchPhaseExecutionException");
-        } catch (SearchPhaseExecutionException e) {
-            assertThat(e.toString(), containsString("Illegal index script format"));
-        }
-    }
-
-    public void testIndexedTemplate() throws Exception {
-        createIndex(ScriptService.SCRIPT_INDEX);
-        ensureGreen(ScriptService.SCRIPT_INDEX);
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "1a").setSource("{" +
-                "\"template\":{"+
-                "                \"query\":{" +
-                "                   \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                    "}" +
-                "}"));
-        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "2").setSource("{" +
-                "\"template\":{"+
-                "                \"query\":{" +
-                "                   \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                    "}" +
-                "}"));
-
-        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "3").setSource("{" +
-                "\"template\":{"+
-                "             \"match\":{" +
-                "                    \"theField\" : \"{{fieldParam}}\"}" +
-                "       }" +
-                "}"));
-
-        indexRandom(true, builders);
-
-        builders.clear();
-
-        builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
-        builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
-        builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
-        builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
-        builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
-
-        indexRandom(true, builders);
-
-        Map<String, Object> templateParams = new HashMap<>();
-        templateParams.put("fieldParam", "foo");
-
-        SearchResponse searchResponse = client()
-                .prepareSearch("test")
-                .setTypes("type")
-                .setTemplate(
-                        new Template("/mustache/1a", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                templateParams)).get();
-        assertHitCount(searchResponse, 4);
-
-        try {
-            client().prepareSearch("test")
-                    .setTypes("type")
-                    .setTemplate(
-                            new Template("/template_index/mustache/1000", ScriptService.ScriptType.INDEXED,
-                                    MustacheScriptEngineService.NAME, null, templateParams)).get();
-            fail("shouldn't get here");
-        } catch (SearchPhaseExecutionException spee) {
-            //all good
-        }
-
-        try {
-            searchResponse = client()
-                    .prepareSearch("test")
-                    .setTypes("type")
-                    .setTemplate(
-                            new Template("/myindex/mustache/1", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                    templateParams)).get();
-            assertFailures(searchResponse);
-        } catch (SearchPhaseExecutionException spee) {
-            //all good
-        }
-
-        searchResponse = client().prepareSearch("test").setTypes("type")
-                .setTemplate(new Template("1a", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
-                .get();
-        assertHitCount(searchResponse, 4);
-
-        templateParams.put("fieldParam", "bar");
-        searchResponse = client()
-                .prepareSearch("test")
-                .setTypes("type")
-                .setTemplate(
-                        new Template("/mustache/2", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                templateParams)).get();
-        assertHitCount(searchResponse, 1);
-
-        Map<String, Object> vars = new HashMap<>();
-        vars.put("fieldParam", "bar");
-
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
-                "3", ScriptService.ScriptType.INDEXED, null, null, vars));
-        SearchResponse sr = client().prepareSearch().setQuery(builder)
-                .execute().actionGet();
-        assertHitCount(sr, 1);
-
-        // "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("fieldParam", "foo");
-        TemplateQueryBuilder templateQuery = new TemplateQueryBuilder(new Template("3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
-        assertHitCount(sr, 4);
-
-        templateQuery = new TemplateQueryBuilder(new Template("/mustache/3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
-        assertHitCount(sr, 4);
-    }
-
-    // Relates to #10397
-    public void testIndexedTemplateOverwrite() throws Exception {
-        createIndex("testindex");
-        ensureGreen("testindex");
-
-        index("testindex", "test", "1", jsonBuilder().startObject().field("searchtext", "dev1").endObject());
-        refresh();
-
-        int iterations = randomIntBetween(2, 11);
-        for (int i = 1; i < iterations; i++) {
-            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
-                    "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"ooophrase_prefix\"}}}}").get();
-            assertEquals(i * 2 - 1, scriptResponse.getVersion());
-
-            GetIndexedScriptResponse getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "git01").get();
-            assertTrue(getResponse.isExists());
-
-            Map<String, Object> templateParams = new HashMap<>();
-            templateParams.put("P_Keyword1", "dev");
-
-            try {
-                client().prepareSearch("testindex")
-                        .setTypes("test")
-                        .setTemplate(
-                                new Template("git01", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                        templateParams)).get();
-                fail("Broken test template is parsing w/o error.");
-            } catch (SearchPhaseExecutionException e) {
-                // the above is expected to fail
-            }
-
-            PutIndexedScriptRequestBuilder builder = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
-                    "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"phrase_prefix\"}}}}").setOpType(
-                    OpType.INDEX);
-            scriptResponse = builder.get();
-            assertEquals(i * 2, scriptResponse.getVersion());
-            SearchResponse searchResponse = client()
-                    .prepareSearch("testindex")
-                    .setTypes("test")
-                    .setTemplate(
-                            new Template("git01", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
-                    .get();
-            assertHitCount(searchResponse, 1);
-        }
-    }
-
-    public void testIndexedTemplateWithArray() throws Exception {
-      createIndex(ScriptService.SCRIPT_INDEX);
-      ensureGreen(ScriptService.SCRIPT_INDEX);
-      List<IndexRequestBuilder> builders = new ArrayList<>();
-
-      String multiQuery = "{\"query\":{\"terms\":{\"theField\":[\"{{#fieldParam}}\",\"{{.}}\",\"{{/fieldParam}}\"]}}}";
-
-      builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "4").setSource(jsonBuilder().startObject().field("template", multiQuery).endObject()));
-
-      indexRandom(true,builders);
-
-      builders.clear();
-
-      builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
-      builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
-      builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
-      builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
-      builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
-
-      indexRandom(true,builders);
-
-      Map<String, Object> arrayTemplateParams = new HashMap<>();
-      String[] fieldParams = {"foo","bar"};
-      arrayTemplateParams.put("fieldParam", fieldParams);
-
-        SearchResponse searchResponse = client()
-                .prepareSearch("test")
-                .setTypes("type")
-                .setTemplate(
-                        new Template("/mustache/4", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
-                                arrayTemplateParams)).get();
-        assertHitCount(searchResponse, 5);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
deleted file mode 100644
index d62a110..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java
+++ /dev/null
@@ -1,204 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.Accountable;
-import org.elasticsearch.Version;
-import org.elasticsearch.client.Client;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsFilter;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexSettings;
-import org.elasticsearch.index.analysis.AnalysisRegistry;
-import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
-import org.elasticsearch.index.fielddata.IndexFieldDataService;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.similarity.SimilarityService;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.IndicesWarmer;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
-import org.elasticsearch.indices.mapper.MapperRegistry;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.IndexSettingsModule;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.After;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.lang.reflect.Proxy;
-import java.util.Collections;
-
-import static org.hamcrest.Matchers.containsString;
-
-/**
- * Test parsing and executing a template request.
- */
-// NOTE: this can't be migrated to ESSingleNodeTestCase because of the custom path.conf
-public class TemplateQueryParserTests extends ESTestCase {
-
-    private Injector injector;
-    private QueryShardContext context;
-
-    @Before
-    public void setup() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir().toString())
-                .put("path.conf", this.getDataPath("config"))
-                .put("name", getClass().getName())
-                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .build();
-        final Client proxy = (Client) Proxy.newProxyInstance(
-                Client.class.getClassLoader(),
-                new Class[]{Client.class}, (proxy1, method, args) -> {
-                    throw new UnsupportedOperationException("client is just a dummy");
-                });
-        Index index = new Index("test");
-        IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, settings);
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings, new SettingsFilter(settings)),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesModule() {
-                    @Override
-                    public void configure() {
-                        // skip services
-                        bindQueryParsersExtension();
-                    }
-                },
-                new ScriptModule(settings),
-                new IndexSettingsModule(index, settings),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        bind(Client.class).toInstance(proxy); // not needed here
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                    }
-                }
-        ).createInjector();
-
-        AnalysisService analysisService = new AnalysisRegistry(null, new Environment(settings)).build(idxSettings);
-        ScriptService scriptService = injector.getInstance(ScriptService.class);
-        SimilarityService similarityService = new SimilarityService(idxSettings, Collections.emptyMap());
-        MapperRegistry mapperRegistry = new IndicesModule().getMapperRegistry();
-        MapperService mapperService = new MapperService(idxSettings, analysisService, similarityService, mapperRegistry);
-        IndexFieldDataService indexFieldDataService =new IndexFieldDataService(idxSettings, injector.getInstance(IndicesFieldDataCache.class), injector.getInstance(CircuitBreakerService.class), mapperService);
-        BitsetFilterCache bitsetFilterCache = new BitsetFilterCache(idxSettings, new IndicesWarmer(idxSettings.getNodeSettings(), null), new BitsetFilterCache.Listener() {
-            @Override
-            public void onCache(ShardId shardId, Accountable accountable) {
-
-            }
-
-            @Override
-            public void onRemoval(ShardId shardId, Accountable accountable) {
-
-            }
-        });
-        IndicesQueriesRegistry indicesQueriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        context = new QueryShardContext(idxSettings, proxy, bitsetFilterCache, indexFieldDataService, mapperService, similarityService, scriptService, indicesQueriesRegistry);
-    }
-
-    @Override
-    @After
-    public void tearDown() throws Exception {
-        super.tearDown();
-        terminate(injector.getInstance(ThreadPool.class));
-    }
-
-    public void testParser() throws IOException {
-        String templateString = "{" + "\"query\":{\"match_{{template}}\": {}}," + "\"params\":{\"template\":\"all\"}" + "}";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-        templateSourceParser.nextToken();
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-
-    public void testParseTemplateAsSingleStringWithConditionalClause() throws IOException {
-        String templateString = "{" + "  \"inline\" : \"{ \\\"match_{{#use_it}}{{template}}{{/use_it}}\\\":{} }\"," + "  \"params\":{"
-                + "    \"template\":\"all\"," + "    \"use_it\": true" + "  }" + "}";
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-
-    /**
-     * Test that the template query parser can parse and evaluate template
-     * expressed as a single string but still it expects only the query
-     * specification (thus this test should fail with specific exception).
-     */
-    public void testParseTemplateFailsToParseCompleteQueryAsSingleString() throws IOException {
-        String templateString = "{" + "  \"inline\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
-                + "  \"params\":{" + "    \"size\":2" + "  }\n" + "}";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        try {
-            parser.fromXContent(context.parseContext()).toQuery(context);
-            fail("Expected ParsingException");
-        } catch (ParsingException e) {
-            assertThat(e.getMessage(), containsString("query malformed, no field after start_object"));
-        }
-    }
-
-    public void testParserCanExtractTemplateNames() throws Exception {
-        String templateString = "{ \"file\": \"storedTemplate\" ,\"params\":{\"template\":\"all\" } } ";
-
-        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
-        context.reset(templateSourceParser);
-        templateSourceParser.nextToken();
-
-        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
-        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
-        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java b/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
index 15249a4..53d18ea 100644
--- a/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
+++ b/core/src/test/java/org/elasticsearch/index/store/CorruptedFileIT.java
@@ -19,26 +19,21 @@
 package org.elasticsearch.index.store;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
 import org.apache.lucene.index.CheckIndex;
 import org.apache.lucene.index.IndexFileNames;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
-import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.GroupShardsIterator;
-import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
-import org.elasticsearch.cluster.routing.ShardIterator;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;
 import org.elasticsearch.common.Nullable;
@@ -48,7 +43,6 @@ import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
-import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.gateway.PrimaryShardAllocator;
 import org.elasticsearch.index.shard.*;
 import org.elasticsearch.indices.recovery.RecoveryFileChunkRequest;
@@ -75,14 +69,7 @@ import java.nio.charset.StandardCharsets;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.TreeSet;
+import java.util.*;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
@@ -91,16 +78,8 @@ import java.util.concurrent.atomic.AtomicBoolean;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.util.CollectionUtils.iterableAsArrayList;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllSuccessful;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.empty;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 @ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE)
 public class CorruptedFileIT extends ESIntegTestCase {
@@ -111,9 +90,9 @@ public class CorruptedFileIT extends ESIntegTestCase {
                 // and we need to make sure primaries are not just trashed if we don't have replicas
                 .put(super.nodeSettings(nodeOrdinal))
                         // speed up recoveries
-                .put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS_SETTING.getKey(), 10)
-                .put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING.getKey(), 10)
-                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), 5)
+                .put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, 10)
+                .put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, 10)
+                .put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, 5)
                 .build();
     }
 
@@ -320,7 +299,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
         }
 
         assertThat(dataNodeStats.size(), greaterThanOrEqualTo(2));
-        Collections.shuffle(dataNodeStats, getRandom());
+        Collections.shuffle(dataNodeStats, random());
         NodeStats primariesNode = dataNodeStats.get(0);
         NodeStats unluckyNode = dataNodeStats.get(1);
         assertAcked(prepareCreate("test").setSettings(Settings.builder()
@@ -380,7 +359,7 @@ public class CorruptedFileIT extends ESIntegTestCase {
         }
 
         assertThat(dataNodeStats.size(), greaterThanOrEqualTo(2));
-        Collections.shuffle(dataNodeStats, getRandom());
+        Collections.shuffle(dataNodeStats, random());
         NodeStats primariesNode = dataNodeStats.get(0);
         NodeStats unluckyNode = dataNodeStats.get(1);
 
diff --git a/core/src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java b/core/src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java
index aab980e..a29cc6c 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/BufferedTranslogTests.java
@@ -34,13 +34,12 @@ import java.nio.file.Path;
 public class BufferedTranslogTests extends TranslogTests {
 
     @Override
-    protected Translog create(Path path) throws IOException {
+    protected TranslogConfig getTranslogConfig(Path path) {
         Settings build = Settings.settingsBuilder()
                 .put("index.translog.fs.type", TranslogWriter.Type.BUFFERED.name())
                 .put("index.translog.fs.buffer_size", 10 + randomInt(128 * 1024), ByteSizeUnit.BYTES)
                 .put(IndexMetaData.SETTING_VERSION_CREATED, org.elasticsearch.Version.CURRENT)
                 .build();
-        TranslogConfig translogConfig = new TranslogConfig(shardId, path, IndexSettingsModule.newIndexSettings(shardId.index(), build), Translog.Durabilty.REQUEST, BigArrays.NON_RECYCLING_INSTANCE, null);
-        return new Translog(translogConfig);
+        return new TranslogConfig(shardId, path, IndexSettingsModule.newIndexSettings(shardId.index(), build), Translog.Durabilty.REQUEST, BigArrays.NON_RECYCLING_INSTANCE, null);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
index 26faa02..e35c04d 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
@@ -22,9 +22,11 @@ package org.elasticsearch.index.translog;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.index.Term;
+import org.apache.lucene.mockfile.FilterFileChannel;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -110,16 +112,19 @@ public class TranslogTests extends ESTestCase {
         }
     }
 
-    protected Translog create(Path path) throws IOException {
+    private Translog create(Path path) throws IOException {
+        return new Translog(getTranslogConfig(path));
+    }
+
+    protected TranslogConfig getTranslogConfig(Path path) {
         Settings build = Settings.settingsBuilder()
                 .put(TranslogConfig.INDEX_TRANSLOG_FS_TYPE, TranslogWriter.Type.SIMPLE.name())
                 .put(IndexMetaData.SETTING_VERSION_CREATED, org.elasticsearch.Version.CURRENT)
                 .build();
-        TranslogConfig translogConfig = new TranslogConfig(shardId, path, IndexSettingsModule.newIndexSettings(shardId.index(), build), Translog.Durabilty.REQUEST, BigArrays.NON_RECYCLING_INSTANCE, null);
-        return new Translog(translogConfig);
+        return new TranslogConfig(shardId, path, IndexSettingsModule.newIndexSettings(shardId.index(), build), Translog.Durabilty.REQUEST, BigArrays.NON_RECYCLING_INSTANCE, null);
     }
 
-    protected void addToTranslogAndList(Translog translog, ArrayList<Translog.Operation> list, Translog.Operation op) {
+    protected void addToTranslogAndList(Translog translog, ArrayList<Translog.Operation> list, Translog.Operation op) throws IOException {
         list.add(op);
         translog.add(op);
     }
@@ -330,7 +335,7 @@ public class TranslogTests extends ESTestCase {
         }
     }
 
-    public void testSnapshot() {
+    public void testSnapshot() throws IOException {
         ArrayList<Translog.Operation> ops = new ArrayList<>();
         Translog.Snapshot snapshot = translog.newSnapshot();
         assertThat(snapshot, SnapshotMatchers.size(0));
@@ -389,7 +394,7 @@ public class TranslogTests extends ESTestCase {
             Translog.Snapshot snapshot = translog.newSnapshot();
             fail("translog is closed");
         } catch (AlreadyClosedException ex) {
-            assertThat(ex.getMessage(), containsString("translog-1.tlog is already closed can't increment"));
+            assertEquals(ex.getMessage(), "translog is already closed");
         }
     }
 
@@ -634,7 +639,7 @@ public class TranslogTests extends ESTestCase {
             final String threadId = "writer_" + i;
             writers[i] = new Thread(new AbstractRunnable() {
                 @Override
-                public void doRun() throws BrokenBarrierException, InterruptedException {
+                public void doRun() throws BrokenBarrierException, InterruptedException, IOException {
                     barrier.await();
                     int counter = 0;
                     while (run.get()) {
@@ -1279,4 +1284,122 @@ public class TranslogTests extends ESTestCase {
             }
         }
     }
+
+    public void testFailFlush() throws IOException {
+        Path tempDir = createTempDir();
+        final AtomicBoolean simulateDiskFull = new AtomicBoolean();
+        TranslogConfig config = getTranslogConfig(tempDir);
+        Translog translog = new Translog(config) {
+            @Override
+            TranslogWriter.ChannelFactory getChannelFactory() {
+                final TranslogWriter.ChannelFactory factory = super.getChannelFactory();
+
+                return new TranslogWriter.ChannelFactory() {
+                    @Override
+                    public FileChannel open(Path file) throws IOException {
+                        FileChannel channel = factory.open(file);
+                        return new FilterFileChannel(channel) {
+
+                            @Override
+                            public int write(ByteBuffer src) throws IOException {
+                                if (simulateDiskFull.get()) {
+                                    if (src.limit() > 1) {
+                                        final int pos = src.position();
+                                        final int limit = src.limit();
+                                        src.limit(limit / 2);
+                                        super.write(src);
+                                        src.position(pos);
+                                        src.limit(limit);
+                                        throw new IOException("__FAKE__ no space left on device");
+                                    }
+                                }
+                                return super.write(src);
+                            }
+                        };
+                    }
+                };
+            }
+        };
+
+        List<Translog.Location> locations = new ArrayList<>();
+        int opsSynced = 0;
+        int opsAdded = 0;
+        boolean failed = false;
+        while(failed == false) {
+            try {
+                locations.add(translog.add(new Translog.Index("test", "" + opsSynced, Integer.toString(opsSynced).getBytes(Charset.forName("UTF-8")))));
+                opsAdded++;
+                translog.sync();
+                opsSynced++;
+            } catch (IOException ex) {
+                failed = true;
+                assertFalse(translog.isOpen());
+                assertEquals("__FAKE__ no space left on device", ex.getMessage());
+             }
+            simulateDiskFull.set(randomBoolean());
+        }
+        simulateDiskFull.set(false);
+        if (randomBoolean()) {
+            try {
+                locations.add(translog.add(new Translog.Index("test", "" + opsSynced, Integer.toString(opsSynced).getBytes(Charset.forName("UTF-8")))));
+                fail("we are already closed");
+            } catch (AlreadyClosedException ex) {
+                assertNotNull(ex.getCause());
+                assertEquals(ex.getCause().getMessage(), "__FAKE__ no space left on device");
+            }
+
+        }
+        Translog.TranslogGeneration translogGeneration = translog.getGeneration();
+        try {
+            translog.newSnapshot();
+            fail("already closed");
+        } catch (AlreadyClosedException ex) {
+            // all is well
+            assertNotNull(ex.getCause());
+            assertSame(translog.getTragicException(), ex.getCause());
+        }
+
+        try {
+            translog.commit();
+            fail("already closed");
+        } catch (AlreadyClosedException ex) {
+            assertNotNull(ex.getCause());
+            assertSame(translog.getTragicException(), ex.getCause());
+        }
+
+        assertFalse(translog.isOpen());
+        translog.close(); // we are closed
+        config.setTranslogGeneration(translogGeneration);
+        try (Translog tlog = new Translog(config)){
+            assertEquals("lastCommitted must be 1 less than current", translogGeneration.translogFileGeneration + 1, tlog.currentFileGeneration());
+            assertFalse(tlog.syncNeeded());
+
+            try (Translog.Snapshot snapshot = tlog.newSnapshot()) {
+                assertEquals(opsSynced, snapshot.estimatedTotalOperations());
+                for (int i = 0; i < opsSynced; i++) {
+                    assertEquals("expected operation" + i + " to be in the previous translog but wasn't", tlog.currentFileGeneration() - 1, locations.get(i).generation);
+                    Translog.Operation next = snapshot.next();
+                    assertNotNull("operation " + i + " must be non-null", next);
+                    assertEquals(i, Integer.parseInt(next.getSource().source.toUtf8()));
+                }
+            }
+        }
+    }
+
+    public void testTranslogOpsCountIsCorrect() throws IOException {
+        List<Translog.Location> locations = new ArrayList<>();
+        int numOps = randomIntBetween(100, 200);
+        LineFileDocs lineFileDocs = new LineFileDocs(random()); // writes pretty big docs so we cross buffer boarders regularly
+        for (int opsAdded = 0; opsAdded < numOps; opsAdded++) {
+            locations.add(translog.add(new Translog.Index("test", "" + opsAdded, lineFileDocs.nextDoc().toString().getBytes(Charset.forName("UTF-8")))));
+            try (Translog.Snapshot snapshot = translog.newSnapshot()) {
+                assertEquals(opsAdded+1, snapshot.estimatedTotalOperations());
+                for (int i = 0; i < opsAdded; i++) {
+                    assertEquals("expected operation" + i + " to be in the current translog but wasn't", translog.currentFileGeneration(), locations.get(i).generation);
+                    Translog.Operation next = snapshot.next();
+                    assertNotNull("operation " + i + " must be non-null", next);
+                }
+            }
+        }
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java b/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
index 4f6aaf2..8de3af2 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
@@ -170,14 +170,14 @@ public class IndicesLifecycleListenerIT extends ESIntegTestCase {
         //add a node: 3 out of the 6 shards will be relocated to it
         //disable allocation before starting a new node, as we need to register the listener first
         assertAcked(client().admin().cluster().prepareUpdateSettings()
-                .setPersistentSettings(builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")));
+                .setPersistentSettings(builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")));
         String node2 = internalCluster().startNode();
         IndexShardStateChangeListener stateChangeListenerNode2 = new IndexShardStateChangeListener();
         //add a listener that keeps track of the shard state changes
         internalCluster().getInstance(MockIndexEventListener.TestEventListener.class, node2).setNewDelegate(stateChangeListenerNode2);
         //re-enable allocation
         assertAcked(client().admin().cluster().prepareUpdateSettings()
-                .setPersistentSettings(builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "all")));
+                .setPersistentSettings(builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "all")));
         ensureGreen();
 
         //the 3 relocated shards get closed on the first node
diff --git a/core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java b/core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java
index 9f4f2b5..8099322 100644
--- a/core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java
@@ -22,11 +22,14 @@ import org.elasticsearch.action.admin.indices.alias.Alias;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder;
 import org.elasticsearch.action.admin.indices.analyze.AnalyzeResponse;
+import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.rest.action.admin.indices.analyze.RestAnalyzeAction;
 import org.elasticsearch.test.ESIntegTestCase;
+import org.hamcrest.core.IsNull;
 
 import java.io.IOException;
 
@@ -36,8 +39,10 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.hasSize;
 import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.startsWith;
 
+
 /**
  *
  */
@@ -201,7 +206,7 @@ public class AnalyzeActionIT extends ESIntegTestCase {
 
         AnalyzeRequest analyzeRequest = new AnalyzeRequest("for test");
 
-        RestAnalyzeAction.buildFromContent(content, analyzeRequest);
+        RestAnalyzeAction.buildFromContent(content, analyzeRequest, new ParseFieldMatcher(Settings.EMPTY));
 
         assertThat(analyzeRequest.text().length, equalTo(1));
         assertThat(analyzeRequest.text(), equalTo(new String[]{"THIS IS A TEST"}));
@@ -213,7 +218,7 @@ public class AnalyzeActionIT extends ESIntegTestCase {
         AnalyzeRequest analyzeRequest = new AnalyzeRequest("for test");
 
         try {
-            RestAnalyzeAction.buildFromContent(new BytesArray("{invalid_json}"), analyzeRequest);
+            RestAnalyzeAction.buildFromContent(new BytesArray("{invalid_json}"), analyzeRequest, new ParseFieldMatcher(Settings.EMPTY));
             fail("shouldn't get here");
         } catch (Exception e) {
             assertThat(e, instanceOf(IllegalArgumentException.class));
@@ -230,7 +235,7 @@ public class AnalyzeActionIT extends ESIntegTestCase {
             .endObject().bytes();
 
         try {
-            RestAnalyzeAction.buildFromContent(invalidContent, analyzeRequest);
+            RestAnalyzeAction.buildFromContent(invalidContent, analyzeRequest, new ParseFieldMatcher(Settings.EMPTY));
             fail("shouldn't get here");
         } catch (Exception e) {
             assertThat(e, instanceOf(IllegalArgumentException.class));
@@ -267,4 +272,235 @@ public class AnalyzeActionIT extends ESIntegTestCase {
 
     }
 
+    public void testDetailAnalyze() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
+            .setSettings(
+                settingsBuilder()
+                    .put("index.analysis.char_filter.my_mapping.type", "mapping")
+                    .putArray("index.analysis.char_filter.my_mapping.mappings", "PH=>F")
+                    .put("index.analysis.analyzer.test_analyzer.type", "custom")
+                    .put("index.analysis.analyzer.test_analyzer.position_increment_gap", "100")
+                    .put("index.analysis.analyzer.test_analyzer.tokenizer", "standard")
+                    .putArray("index.analysis.analyzer.test_analyzer.char_filter", "my_mapping")
+                    .putArray("index.analysis.analyzer.test_analyzer.filter", "snowball")));
+        ensureGreen();
+
+        for (int i = 0; i < 10; i++) {
+            AnalyzeResponse analyzeResponse = admin().indices().prepareAnalyze().setIndex(indexOrAlias()).setText("THIS IS A PHISH")
+                .setExplain(true).setCharFilters("my_mapping").setTokenizer("keyword").setTokenFilters("lowercase").get();
+
+            assertThat(analyzeResponse.detail().analyzer(), IsNull.nullValue());
+            //charfilters
+            // global charfilter is not change text.
+            assertThat(analyzeResponse.detail().charfilters().length, equalTo(1));
+            assertThat(analyzeResponse.detail().charfilters()[0].getName(), equalTo("my_mapping"));
+            assertThat(analyzeResponse.detail().charfilters()[0].getTexts().length, equalTo(1));
+            assertThat(analyzeResponse.detail().charfilters()[0].getTexts()[0], equalTo("THIS IS A FISH"));
+            //tokenizer
+            assertThat(analyzeResponse.detail().tokenizer().getName(), equalTo("keyword"));
+            assertThat(analyzeResponse.detail().tokenizer().getTokens().length, equalTo(1));
+            assertThat(analyzeResponse.detail().tokenizer().getTokens()[0].getTerm(), equalTo("THIS IS A FISH"));
+            assertThat(analyzeResponse.detail().tokenizer().getTokens()[0].getStartOffset(), equalTo(0));
+            assertThat(analyzeResponse.detail().tokenizer().getTokens()[0].getEndOffset(), equalTo(15));
+            //tokenfilters
+            assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(1));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("lowercase"));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(1));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getTerm(), equalTo("this is a fish"));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getPosition(), equalTo(0));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getStartOffset(), equalTo(0));
+            assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getEndOffset(), equalTo(15));
+        }
+    }
+
+    public void testDetailAnalyzeWithNoIndex() throws Exception {
+        //analyzer only
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze("THIS IS A TEST")
+            .setExplain(true).setAnalyzer("simple").get();
+
+        assertThat(analyzeResponse.detail().tokenizer(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().tokenfilters(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().charfilters(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().analyzer().getName(), equalTo("simple"));
+        assertThat(analyzeResponse.detail().analyzer().getTokens().length, equalTo(4));
+    }
+
+    public void testDetailAnalyzeCustomAnalyzerWithNoIndex() throws Exception {
+        //analyzer only
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze("THIS IS A TEST")
+            .setExplain(true).setAnalyzer("simple").get();
+
+        assertThat(analyzeResponse.detail().tokenizer(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().tokenfilters(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().charfilters(), IsNull.nullValue());
+        assertThat(analyzeResponse.detail().analyzer().getName(), equalTo("simple"));
+        assertThat(analyzeResponse.detail().analyzer().getTokens().length, equalTo(4));
+
+        //custom analyzer
+        analyzeResponse = client().admin().indices().prepareAnalyze("<text>THIS IS A TEST</text>")
+            .setExplain(true).setCharFilters("html_strip").setTokenizer("keyword").setTokenFilters("lowercase").get();
+        assertThat(analyzeResponse.detail().analyzer(), IsNull.nullValue());
+        //charfilters
+        // global charfilter is not change text.
+        assertThat(analyzeResponse.detail().charfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().charfilters()[0].getName(), equalTo("html_strip"));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts().length, equalTo(1));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts()[0], equalTo("\nTHIS IS A TEST\n"));
+        //tokenizer
+        assertThat(analyzeResponse.detail().tokenizer().getName(), equalTo("keyword"));
+        assertThat(analyzeResponse.detail().tokenizer().getTokens().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenizer().getTokens()[0].getTerm(), equalTo("\nTHIS IS A TEST\n"));
+        //tokenfilters
+        assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("lowercase"));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[0].getTerm(), equalTo("\nthis is a test\n"));
+
+
+        //check other attributes
+        analyzeResponse = client().admin().indices().prepareAnalyze("This is troubled")
+            .setExplain(true).setTokenizer("standard").setTokenFilters("snowball").get();
+
+        assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("snowball"));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(3));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getTerm(), equalTo("troubl"));
+        String[] expectedAttributesKey = {
+            "bytes",
+            "positionLength",
+            "keyword"};
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getAttributes().size(), equalTo(expectedAttributesKey.length));
+        Object extendedAttribute;
+
+        for (String key : expectedAttributesKey) {
+            extendedAttribute = analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getAttributes().get(key);
+            assertThat(extendedAttribute, notNullValue());
+        }
+    }
+
+    public void testDetailAnalyzeSpecifyAttributes() throws Exception {
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze("This is troubled")
+            .setExplain(true).setTokenizer("standard").setTokenFilters("snowball").setAttributes("keyword").get();
+
+        assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("snowball"));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(3));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getTerm(), equalTo("troubl"));
+        String[] expectedAttributesKey = {
+            "keyword"};
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getAttributes().size(), equalTo(expectedAttributesKey.length));
+        Object extendedAttribute;
+
+        for (String key : expectedAttributesKey) {
+            extendedAttribute = analyzeResponse.detail().tokenfilters()[0].getTokens()[2].getAttributes().get(key);
+            assertThat(extendedAttribute, notNullValue());
+        }
+    }
+
+    public void testDetailAnalyzeWithMultiValues() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias")));
+        ensureGreen();
+        client().admin().indices().preparePutMapping("test")
+            .setType("document").setSource("simple", "type=string,analyzer=simple,position_increment_gap=100").get();
+
+        String[] texts = new String[]{"THIS IS A TEST", "THE SECOND TEXT"};
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze().setIndex(indexOrAlias()).setText(texts)
+            .setExplain(true).setField("simple").setText(texts).execute().get();
+
+        assertThat(analyzeResponse.detail().analyzer().getName(), equalTo("simple"));
+        assertThat(analyzeResponse.detail().analyzer().getTokens().length, equalTo(7));
+        AnalyzeResponse.AnalyzeToken token = analyzeResponse.detail().analyzer().getTokens()[3];
+
+        assertThat(token.getTerm(), equalTo("test"));
+        assertThat(token.getPosition(), equalTo(3));
+        assertThat(token.getStartOffset(), equalTo(10));
+        assertThat(token.getEndOffset(), equalTo(14));
+
+        token = analyzeResponse.detail().analyzer().getTokens()[5];
+        assertThat(token.getTerm(), equalTo("second"));
+        assertThat(token.getPosition(), equalTo(105));
+        assertThat(token.getStartOffset(), equalTo(19));
+        assertThat(token.getEndOffset(), equalTo(25));
+    }
+
+    public void testDetailAnalyzeWithMultiValuesWithCustomAnalyzer() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
+            .setSettings(
+                settingsBuilder()
+                    .put("index.analysis.char_filter.my_mapping.type", "mapping")
+                    .putArray("index.analysis.char_filter.my_mapping.mappings", "PH=>F")
+                    .put("index.analysis.analyzer.test_analyzer.type", "custom")
+                    .put("index.analysis.analyzer.test_analyzer.position_increment_gap", "100")
+                    .put("index.analysis.analyzer.test_analyzer.tokenizer", "standard")
+                    .putArray("index.analysis.analyzer.test_analyzer.char_filter", "my_mapping")
+                    .putArray("index.analysis.analyzer.test_analyzer.filter", "snowball", "lowercase")));
+        ensureGreen();
+
+        client().admin().indices().preparePutMapping("test")
+            .setType("document").setSource("simple", "type=string,analyzer=simple,position_increment_gap=100").get();
+
+        //only analyzer =
+        String[] texts = new String[]{"this is a PHISH", "the troubled text"};
+        AnalyzeResponse analyzeResponse = client().admin().indices().prepareAnalyze().setIndex(indexOrAlias()).setText(texts)
+            .setExplain(true).setAnalyzer("test_analyzer").setText(texts).execute().get();
+
+        // charfilter
+        assertThat(analyzeResponse.detail().charfilters().length, equalTo(1));
+        assertThat(analyzeResponse.detail().charfilters()[0].getName(), equalTo("my_mapping"));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts().length, equalTo(2));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts()[0], equalTo("this is a FISH"));
+        assertThat(analyzeResponse.detail().charfilters()[0].getTexts()[1], equalTo("the troubled text"));
+
+        // tokenizer
+        assertThat(analyzeResponse.detail().tokenizer().getName(), equalTo("standard"));
+        assertThat(analyzeResponse.detail().tokenizer().getTokens().length, equalTo(7));
+        AnalyzeResponse.AnalyzeToken token = analyzeResponse.detail().tokenizer().getTokens()[3];
+
+        assertThat(token.getTerm(), equalTo("FISH"));
+        assertThat(token.getPosition(), equalTo(3));
+        assertThat(token.getStartOffset(), equalTo(10));
+        assertThat(token.getEndOffset(), equalTo(15));
+
+        token = analyzeResponse.detail().tokenizer().getTokens()[5];
+        assertThat(token.getTerm(), equalTo("troubled"));
+        assertThat(token.getPosition(), equalTo(105));
+        assertThat(token.getStartOffset(), equalTo(20));
+        assertThat(token.getEndOffset(), equalTo(28));
+
+        // tokenfilter(snowball)
+        assertThat(analyzeResponse.detail().tokenfilters().length, equalTo(2));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getName(), equalTo("snowball"));
+        assertThat(analyzeResponse.detail().tokenfilters()[0].getTokens().length, equalTo(7));
+        token = analyzeResponse.detail().tokenfilters()[0].getTokens()[3];
+
+        assertThat(token.getTerm(), equalTo("FISH"));
+        assertThat(token.getPosition(), equalTo(3));
+        assertThat(token.getStartOffset(), equalTo(10));
+        assertThat(token.getEndOffset(), equalTo(15));
+
+        token = analyzeResponse.detail().tokenfilters()[0].getTokens()[5];
+        assertThat(token.getTerm(), equalTo("troubl"));
+        assertThat(token.getPosition(), equalTo(105));
+        assertThat(token.getStartOffset(), equalTo(20));
+        assertThat(token.getEndOffset(), equalTo(28));
+
+        // tokenfilter(lowercase)
+        assertThat(analyzeResponse.detail().tokenfilters()[1].getName(), equalTo("lowercase"));
+        assertThat(analyzeResponse.detail().tokenfilters()[1].getTokens().length, equalTo(7));
+        token = analyzeResponse.detail().tokenfilters()[1].getTokens()[3];
+
+        assertThat(token.getTerm(), equalTo("fish"));
+        assertThat(token.getPosition(), equalTo(3));
+        assertThat(token.getStartOffset(), equalTo(10));
+        assertThat(token.getEndOffset(), equalTo(15));
+
+        token = analyzeResponse.detail().tokenfilters()[0].getTokens()[5];
+        assertThat(token.getTerm(), equalTo("troubl"));
+        assertThat(token.getPosition(), equalTo(105));
+        assertThat(token.getStartOffset(), equalTo(20));
+        assertThat(token.getEndOffset(), equalTo(28));
+
+
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerNoopIT.java b/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerNoopIT.java
index 08df601..3398839 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerNoopIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerNoopIT.java
@@ -43,10 +43,10 @@ public class CircuitBreakerNoopIT extends ESIntegTestCase {
         return Settings.builder()
                 .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_TYPE_SETTING, "noop")
                 // This is set low, because if the "noop" is not a noop, it will break
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), "10b")
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, "10b")
                 .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_TYPE_SETTING, "noop")
                 // This is set low, because if the "noop" is not a noop, it will break
-                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), "10b")
+                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, "10b")
                 .build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java b/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java
index 1af04e2..fcd94d9 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceIT.java
@@ -63,13 +63,13 @@ public class CircuitBreakerServiceIT extends ESIntegTestCase {
     private void reset() {
         logger.info("--> resetting breaker settings");
         Settings resetSettings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(),
-                        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getDefault(null))
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getKey(),
-                        HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getDefault(null))
-                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(),
-                        HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getDefault(null))
-                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING.getKey(), 1.0)
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING,
+                        HierarchyCircuitBreakerService.DEFAULT_FIELDDATA_BREAKER_LIMIT)
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING,
+                        HierarchyCircuitBreakerService.DEFAULT_FIELDDATA_OVERHEAD_CONSTANT)
+                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING,
+                        HierarchyCircuitBreakerService.DEFAULT_REQUEST_BREAKER_LIMIT)
+                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING, 1.0)
                 .build();
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(resetSettings));
     }
@@ -119,8 +119,8 @@ public class CircuitBreakerServiceIT extends ESIntegTestCase {
 
         // Update circuit breaker settings
         Settings settings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), "100b")
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getKey(), 1.05)
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, "100b")
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, 1.05)
                 .build();
         assertAcked(client.admin().cluster().prepareUpdateSettings().setTransientSettings(settings));
 
@@ -168,8 +168,8 @@ public class CircuitBreakerServiceIT extends ESIntegTestCase {
 
         // Update circuit breaker settings
         Settings settings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), "100b")
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getKey(), 1.05)
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, "100b")
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, 1.05)
                 .build();
         assertAcked(client.admin().cluster().prepareUpdateSettings().setTransientSettings(settings));
 
@@ -213,8 +213,8 @@ public class CircuitBreakerServiceIT extends ESIntegTestCase {
                 .getNodes()[0].getBreaker().getStats(CircuitBreaker.REQUEST).getLimit();
 
         Settings resetSettings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), "10b")
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getKey(), 1.0)
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, "10b")
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, 1.0)
                 .build();
         assertAcked(client.admin().cluster().prepareUpdateSettings().setTransientSettings(resetSettings));
 
@@ -234,9 +234,9 @@ public class CircuitBreakerServiceIT extends ESIntegTestCase {
 
         // Adjust settings so the parent breaker will fail, but the fielddata breaker doesn't
         resetSettings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), "15b")
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), "90%")
-                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING.getKey(), 1.0)
+                .put(HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING, "15b")
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, "90%")
+                .put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, 1.0)
                 .build();
         client.admin().cluster().prepareUpdateSettings().setTransientSettings(resetSettings).execute().actionGet();
 
@@ -261,7 +261,7 @@ public class CircuitBreakerServiceIT extends ESIntegTestCase {
 
         // Make request breaker limited to a small amount
         Settings resetSettings = settingsBuilder()
-                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), "10b")
+                .put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, "10b")
                 .build();
         assertAcked(client.admin().cluster().prepareUpdateSettings().setTransientSettings(resetSettings));
 
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerUnitTests.java b/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerUnitTests.java
index 212d7ec..741ea30 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerUnitTests.java
@@ -19,12 +19,12 @@
 
 package org.elasticsearch.indices.memory.breaker;
 
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.breaker.CircuitBreaker;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.indices.breaker.BreakerSettings;
 import org.elasticsearch.indices.breaker.CircuitBreakerService;
 import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESTestCase;
 
 import static org.hamcrest.Matchers.equalTo;
@@ -66,7 +66,7 @@ public class CircuitBreakerUnitTests extends ESTestCase {
     }
 
     public void testRegisterCustomBreaker() throws Exception {
-        CircuitBreakerService service = new HierarchyCircuitBreakerService(Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS));
+        CircuitBreakerService service = new HierarchyCircuitBreakerService(Settings.EMPTY, new NodeSettingsService(Settings.EMPTY));
         String customName = "custom";
         BreakerSettings settings = new BreakerSettings(customName, 20, 1.0);
         service.registerBreaker(settings);
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java b/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
index 50346a2..4cf6028 100644
--- a/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java
@@ -138,21 +138,25 @@ public class IndexRecoveryIT extends ESIntegTestCase {
     }
 
     private void slowDownRecovery(ByteSizeValue shardSize) {
-        long chunkSize = shardSize.bytes() / 10;
+        long chunkSize = Math.max(1, shardSize.bytes() / 10);
+        for(RecoverySettings settings : internalCluster().getInstances(RecoverySettings.class)) {
+            setChunkSize(settings, new ByteSizeValue(chunkSize, ByteSizeUnit.BYTES));
+        }
         assertTrue(client().admin().cluster().prepareUpdateSettings()
                 .setTransientSettings(Settings.builder()
                                 // one chunk per sec..
-                                .put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), chunkSize, ByteSizeUnit.BYTES)
-                                .put(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING.getKey(), chunkSize, ByteSizeUnit.BYTES)
+                                .put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, chunkSize, ByteSizeUnit.BYTES)
                 )
                 .get().isAcknowledged());
     }
 
     private void restoreRecoverySpeed() {
+        for(RecoverySettings settings : internalCluster().getInstances(RecoverySettings.class)) {
+            setChunkSize(settings, RecoverySettings.DEFAULT_CHUNK_SIZE);
+        }
         assertTrue(client().admin().cluster().prepareUpdateSettings()
                 .setTransientSettings(Settings.builder()
-                                .put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), "20mb")
-                                .put(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING.getKey(), "512kb")
+                                .put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, "20mb")
                 )
                 .get().isAcknowledged());
     }
@@ -525,8 +529,8 @@ public class IndexRecoveryIT extends ESIntegTestCase {
     public void testDisconnectsWhileRecovering() throws Exception {
         final String indexName = "test";
         final Settings nodeSettings = Settings.builder()
-                .put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.getKey(), "100ms")
-                .put(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.getKey(), "1s")
+                .put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK, "100ms")
+                .put(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT, "1s")
                 .put(MockFSDirectoryService.RANDOM_PREVENT_DOUBLE_WRITE, false) // restarted recoveries will delete temp files and write them again
                 .build();
         // start a master node
@@ -631,4 +635,8 @@ public class IndexRecoveryIT extends ESIntegTestCase {
             transport.sendRequest(node, requestId, action, request, options);
         }
     }
+
+    public static void setChunkSize(RecoverySettings recoverySettings, ByteSizeValue chunksSize) {
+        recoverySettings.setChunkSize(chunksSize);
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java b/core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java
index 75a2b14..8346003 100644
--- a/core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/recovery/RecoverySourceHandlerTests.java
@@ -34,7 +34,6 @@ import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.lucene.store.IndexOutputOutputStream;
 import org.elasticsearch.common.settings.Settings;
@@ -45,6 +44,7 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.store.DirectoryService;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.store.StoreFileMetaData;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.CorruptionUtils;
 import org.elasticsearch.test.DummyShardLock;
 import org.elasticsearch.test.ESTestCase;
@@ -60,7 +60,7 @@ import java.util.concurrent.atomic.AtomicBoolean;
 public class RecoverySourceHandlerTests extends ESTestCase {
     private static final IndexSettings INDEX_SETTINGS = IndexSettingsModule.newIndexSettings(new Index("index"), Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, org.elasticsearch.Version.CURRENT).build());
     private final ShardId shardId = new ShardId(INDEX_SETTINGS.getIndex(), 1);
-    private final ClusterSettings service = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
+    private final NodeSettingsService service = new NodeSettingsService(Settings.EMPTY);
 
     public void testSendFiles() throws Throwable {
         Settings settings = Settings.builder().put("indices.recovery.concurrent_streams", 1).
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTests.java b/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTests.java
index fea8f0f..8b23354 100644
--- a/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStateTests.java
@@ -26,13 +26,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.indices.recovery.RecoveryState.File;
-import org.elasticsearch.indices.recovery.RecoveryState.Index;
-import org.elasticsearch.indices.recovery.RecoveryState.Stage;
-import org.elasticsearch.indices.recovery.RecoveryState.Timer;
-import org.elasticsearch.indices.recovery.RecoveryState.Translog;
-import org.elasticsearch.indices.recovery.RecoveryState.Type;
-import org.elasticsearch.indices.recovery.RecoveryState.VerifyIndex;
+import org.elasticsearch.indices.recovery.RecoveryState.*;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
@@ -43,14 +37,7 @@ import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
 
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.hamcrest.Matchers.arrayContainingInAnyOrder;
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.either;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.lessThan;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
+import static org.hamcrest.Matchers.*;
 
 public class RecoveryStateTests extends ESTestCase {
     abstract class Streamer<T extends Streamable> extends Thread {
@@ -201,7 +188,7 @@ public class RecoveryStateTests extends ESTestCase {
             }
         }
 
-        Collections.shuffle(Arrays.asList(files));
+        Collections.shuffle(Arrays.asList(files), random());
         final RecoveryState.Index index = new RecoveryState.Index();
 
         if (randomBoolean()) {
diff --git a/core/src/test/java/org/elasticsearch/indices/state/CloseIndexDisableCloseAllIT.java b/core/src/test/java/org/elasticsearch/indices/state/CloseIndexDisableCloseAllIT.java
index 8ec629d..bde40aa 100644
--- a/core/src/test/java/org/elasticsearch/indices/state/CloseIndexDisableCloseAllIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/state/CloseIndexDisableCloseAllIT.java
@@ -40,7 +40,7 @@ public class CloseIndexDisableCloseAllIT extends ESIntegTestCase {
     // The cluster scope is test b/c we can't clear cluster settings.
     public void testCloseAllRequiresName() {
         Settings clusterSettings = Settings.builder()
-                .put(DestructiveOperations.REQUIRES_NAME_SETTING.getKey(), true)
+                .put(DestructiveOperations.REQUIRES_NAME, true)
                 .build();
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(clusterSettings));
         createIndex("test1", "test2", "test3");
@@ -91,7 +91,7 @@ public class CloseIndexDisableCloseAllIT extends ESIntegTestCase {
         createIndex("test_no_close");
         healthResponse = client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();
         assertThat(healthResponse.isTimedOut(), equalTo(false));
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(TransportCloseIndexAction.CLUSTER_INDICES_CLOSE_ENABLE_SETTING.getKey(), false)).get();
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(TransportCloseIndexAction.SETTING_CLUSTER_INDICES_CLOSE_ENABLE, false)).get();
 
         try {
             client.admin().indices().prepareClose("test_no_close").execute().actionGet();
diff --git a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
index 2e73a46..96611ae 100644
--- a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
@@ -171,7 +171,7 @@ public class RareClusterStateIT extends ESIntegTestCase {
         ensureGreen("test");
 
         // now that the cluster is stable, remove publishing timeout
-        assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "0")));
+        assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT, "0")));
 
         Set<String> nodes = new HashSet<>(Arrays.asList(internalCluster().getNodeNames()));
         nodes.remove(internalCluster().getMasterName());
@@ -200,7 +200,7 @@ public class RareClusterStateIT extends ESIntegTestCase {
         // but the change might not be on the node that performed the indexing
         // operation yet
 
-        Settings settings = Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), "0ms").build();
+        Settings settings = Settings.builder().put(DiscoverySettings.PUBLISH_TIMEOUT, "0ms").build();
         final List<String> nodeNames = internalCluster().startNodesAsync(2, settings).get();
         assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes("2").get().isTimedOut());
 
diff --git a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
index 948b76b..fc4dd4f 100644
--- a/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationIT.java
@@ -303,7 +303,7 @@ public class IndicesStoreIntegrationIT extends ESIntegTestCase {
 
         // disable allocation to control the situation more easily
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")));
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")));
 
         logger.debug("--> shutting down two random nodes");
         internalCluster().stopRandomNode(InternalTestCluster.nameFilter(node1, node2, node3));
@@ -322,7 +322,7 @@ public class IndicesStoreIntegrationIT extends ESIntegTestCase {
                         .put(FilterAllocationDecider.INDEX_ROUTING_EXCLUDE_GROUP + "_name", "NONE")));
 
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "all")));
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "all")));
 
         logger.debug("--> waiting for shards to recover on [{}]", node4);
         // we have to do this in two steps as we now do async shard fetching before assigning, so the change to the
@@ -340,7 +340,7 @@ public class IndicesStoreIntegrationIT extends ESIntegTestCase {
 
         // disable allocation again to control concurrency a bit and allow shard active to kick in before allocation
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")));
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")));
 
         logger.debug("--> starting the two old nodes back");
 
@@ -351,7 +351,7 @@ public class IndicesStoreIntegrationIT extends ESIntegTestCase {
 
 
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "all")));
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "all")));
 
         logger.debug("--> waiting for the lost shard to be recovered");
 
@@ -396,7 +396,7 @@ public class IndicesStoreIntegrationIT extends ESIntegTestCase {
 
         // disable relocations when we do this, to make sure the shards are not relocated from node2
         // due to rebalancing, and delete its content
-        client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), EnableAllocationDecider.Rebalance.NONE)).get();
+        client().admin().cluster().prepareUpdateSettings().setTransientSettings(settingsBuilder().put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, EnableAllocationDecider.Rebalance.NONE)).get();
         internalCluster().getInstance(ClusterService.class, nonMasterNode).submitStateUpdateTask("test", new ClusterStateUpdateTask(Priority.IMMEDIATE) {
             @Override
             public ClusterState execute(ClusterState currentState) throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java b/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
index ca2025c..b32cfef 100644
--- a/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java
@@ -560,7 +560,7 @@ public class SimpleIndexTemplateIT extends ESIntegTestCase {
                 .setOrder(0)
                 .addAlias(new Alias("alias1"))
                 .addAlias(new Alias("{index}-alias"))
-                .addAlias(new Alias("alias3").filter(QueryBuilders.missingQuery("test")))
+                .addAlias(new Alias("alias3").filter(QueryBuilders.boolQuery().mustNot(QueryBuilders.existsQuery("test"))))
                 .addAlias(new Alias("alias4")).get();
 
         client().admin().indices().preparePutTemplate("template2")
diff --git a/core/src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationIT.java b/core/src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationIT.java
index 514b175..49d22b8 100644
--- a/core/src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationIT.java
@@ -34,7 +34,7 @@ public class DestructiveOperationsIntegrationIT extends ESIntegTestCase {
     // The cluster scope is test b/c we can't clear cluster settings.
     public void testDestructiveOperations() throws Exception {
         Settings settings = Settings.builder()
-                .put(DestructiveOperations.REQUIRES_NAME_SETTING.getKey(), true)
+                .put(DestructiveOperations.REQUIRES_NAME, true)
                 .build();
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings));
 
@@ -58,7 +58,7 @@ public class DestructiveOperationsIntegrationIT extends ESIntegTestCase {
         }
 
         settings = Settings.builder()
-                .put(DestructiveOperations.REQUIRES_NAME_SETTING.getKey(), false)
+                .put(DestructiveOperations.REQUIRES_NAME, false)
                 .build();
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings));
 
@@ -68,7 +68,7 @@ public class DestructiveOperationsIntegrationIT extends ESIntegTestCase {
         // end delete index:
         // close index:
         settings = Settings.builder()
-                .put(DestructiveOperations.REQUIRES_NAME_SETTING.getKey(), true)
+                .put(DestructiveOperations.REQUIRES_NAME, true)
                 .build();
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings));
 
@@ -100,7 +100,7 @@ public class DestructiveOperationsIntegrationIT extends ESIntegTestCase {
         }
 
         settings = Settings.builder()
-                .put(DestructiveOperations.REQUIRES_NAME_SETTING.getKey(), false)
+                .put(DestructiveOperations.REQUIRES_NAME, false)
                 .build();
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings));
         assertAcked(client().admin().indices().prepareClose("_all").get());
diff --git a/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTests.java b/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTests.java
index 8d659c1..4bcbb8c 100644
--- a/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/recovery/RecoverySettingsTests.java
@@ -32,79 +32,54 @@ public class RecoverySettingsTests extends ESSingleNodeTestCase {
     }
 
     public void testAllSettingsAreDynamicallyUpdatable() {
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING.getKey(), randomIntBetween(1, 200), ByteSizeUnit.BYTES, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.fileChunkSize().bytesAsInt());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_TRANSLOG_OPS_SETTING.getKey(), randomIntBetween(1, 200), new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.translogOps());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_TRANSLOG_SIZE_SETTING.getKey(), randomIntBetween(1, 200), ByteSizeUnit.BYTES, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, int expectedValue) {
-                assertEquals(expectedValue, recoverySettings.translogSize().bytesAsInt());
-            }
-        });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS_SETTING.getKey(), randomIntBetween(1, 200), new Validator() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, randomIntBetween(1, 200), new Validator() {
             @Override
             public void validate(RecoverySettings recoverySettings, int expectedValue) {
                 assertEquals(expectedValue, recoverySettings.concurrentStreamPool().getMaximumPoolSize());
             }
         });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING.getKey(), randomIntBetween(1, 200), new Validator() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, randomIntBetween(1, 200), new Validator() {
             @Override
             public void validate(RecoverySettings recoverySettings, int expectedValue) {
                 assertEquals(expectedValue, recoverySettings.concurrentSmallFileStreamPool().getMaximumPoolSize());
             }
         });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), 0, new Validator() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, 0, new Validator() {
             @Override
             public void validate(RecoverySettings recoverySettings, int expectedValue) {
                 assertEquals(null, recoverySettings.rateLimiter());
             }
         });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING.getKey(), randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
             @Override
             public void validate(RecoverySettings recoverySettings, int expectedValue) {
                 assertEquals(expectedValue, recoverySettings.retryDelayStateSync().millis());
             }
         });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.getKey(), randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
             @Override
             public void validate(RecoverySettings recoverySettings, int expectedValue) {
                 assertEquals(expectedValue, recoverySettings.retryDelayNetwork().millis());
             }
         });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT_SETTING.getKey(), randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
             @Override
             public void validate(RecoverySettings recoverySettings, int expectedValue) {
                 assertEquals(expectedValue, recoverySettings.activityTimeout().millis());
             }
         });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT_SETTING.getKey(), randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
             @Override
             public void validate(RecoverySettings recoverySettings, int expectedValue) {
                 assertEquals(expectedValue, recoverySettings.internalActionTimeout().millis());
             }
         });
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT_SETTING.getKey(), randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
+        innerTestSettings(RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT, randomIntBetween(1, 200), TimeUnit.MILLISECONDS, new Validator() {
             @Override
             public void validate(RecoverySettings recoverySettings, int expectedValue) {
                 assertEquals(expectedValue, recoverySettings.internalActionLongTimeout().millis());
             }
         });
-
-        innerTestSettings(RecoverySettings.INDICES_RECOVERY_COMPRESS_SETTING.getKey(), false, new Validator() {
-            @Override
-            public void validate(RecoverySettings recoverySettings, boolean expectedValue) {
-                assertEquals(expectedValue, recoverySettings.compress());
-            }
-        });
     }
 
     private static class Validator {
diff --git a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
index 541911c..57b5e88 100644
--- a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
+++ b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
@@ -387,7 +387,7 @@ public class RelocationIT extends ESIntegTestCase {
 
         logger.info("--> stopping replica assignment");
         assertAcked(client().admin().cluster().prepareUpdateSettings()
-                .setTransientSettings(Settings.builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE_SETTING.getKey(), "none")));
+                .setTransientSettings(Settings.builder().put(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, "none")));
 
         logger.info("--> wait for all replica shards to be removed, on all nodes");
         assertBusy(new Runnable() {
diff --git a/core/src/test/java/org/elasticsearch/recovery/TruncatedRecoveryIT.java b/core/src/test/java/org/elasticsearch/recovery/TruncatedRecoveryIT.java
index 4f85eb7..60a14ab 100644
--- a/core/src/test/java/org/elasticsearch/recovery/TruncatedRecoveryIT.java
+++ b/core/src/test/java/org/elasticsearch/recovery/TruncatedRecoveryIT.java
@@ -29,8 +29,8 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeUnit;
 import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.indices.recovery.IndexRecoveryIT;
 import org.elasticsearch.indices.recovery.RecoveryFileChunkRequest;
 import org.elasticsearch.indices.recovery.RecoverySettings;
 import org.elasticsearch.indices.recovery.RecoveryTarget;
@@ -58,13 +58,6 @@ import static org.hamcrest.Matchers.greaterThanOrEqualTo;
 @ESIntegTestCase.ClusterScope(numDataNodes = 2, numClientNodes = 0, scope = ESIntegTestCase.Scope.TEST)
 @SuppressCodecs("*") // test relies on exact file extensions
 public class TruncatedRecoveryIT extends ESIntegTestCase {
-    @Override
-    protected Settings nodeSettings(int nodeOrdinal) {
-        Settings.Builder builder = Settings.builder()
-                .put(super.nodeSettings(nodeOrdinal))
-                .put(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE_SETTING.getKey(), new ByteSizeValue(randomIntBetween(50, 300), ByteSizeUnit.BYTES));
-        return builder.build();
-    }
 
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
@@ -78,6 +71,10 @@ public class TruncatedRecoveryIT extends ESIntegTestCase {
      * Later we allow full recovery to ensure we can still recover and don't run into corruptions.
      */
     public void testCancelRecoveryAndResume() throws Exception {
+        for(RecoverySettings settings : internalCluster().getInstances(RecoverySettings.class)) {
+            IndexRecoveryIT.setChunkSize(settings, new ByteSizeValue(randomIntBetween(50, 300), ByteSizeUnit.BYTES));
+        }
+
         NodesStatsResponse nodeStats = client().admin().cluster().prepareNodesStats().get();
         List<NodeStats> dataNodeStats = new ArrayList<>();
         for (NodeStats stat : nodeStats.getNodes()) {
@@ -86,7 +83,7 @@ public class TruncatedRecoveryIT extends ESIntegTestCase {
             }
         }
         assertThat(dataNodeStats.size(), greaterThanOrEqualTo(2));
-        Collections.shuffle(dataNodeStats, getRandom());
+        Collections.shuffle(dataNodeStats, random());
         // we use 2 nodes a lucky and unlucky one
         // the lucky one holds the primary
         // the unlucky one gets the replica and the truncated leftovers
diff --git a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
index daefc20..fc888c7 100644
--- a/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
+++ b/core/src/test/java/org/elasticsearch/script/FileScriptTests.java
@@ -19,12 +19,9 @@
 package org.elasticsearch.script;
 
 import org.elasticsearch.common.ContextAndHeaderHolder;
-import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
 
 import java.nio.file.Files;
 import java.nio.file.Path;
@@ -32,9 +29,6 @@ import java.util.Collections;
 import java.util.HashSet;
 import java.util.Set;
 
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.containsString;
-
 // TODO: these really should just be part of ScriptService tests, there is nothing special about them
 public class FileScriptTests extends ESTestCase {
 
diff --git a/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java b/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
deleted file mode 100644
index 1cdac14..0000000
--- a/core/src/test/java/org/elasticsearch/script/MockScriptEngine.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.script;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.io.IOException;
-import java.util.Map;
-
-/**
- * A dummy script engine used for testing. Scripts must be a number. Running the script
- */
-public class MockScriptEngine implements ScriptEngineService {
-    public static final String NAME = "mockscript";
-
-    public static class TestPlugin extends Plugin {
-
-        public TestPlugin() {
-        }
-
-        @Override
-        public String name() {
-            return NAME;
-        }
-
-        @Override
-        public String description() {
-            return "Mock script engine for integration tests";
-        }
-
-        public void onModule(ScriptModule module) {
-            module.addScriptEngine(MockScriptEngine.class);
-        }
-
-    }
-
-    @Override
-    public String[] types() {
-        return new String[]{ NAME };
-    }
-
-    @Override
-    public String[] extensions() {
-        return types();
-    }
-
-    @Override
-    public boolean sandboxed() {
-        return true;
-    }
-
-    @Override
-    public Object compile(String script) {
-        return Integer.parseInt(script);
-    }
-
-    @Override
-    public ExecutableScript executable(CompiledScript compiledScript, @Nullable Map<String, Object> vars) {
-        return null;
-    }
-
-    @Override
-    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, @Nullable Map<String, Object> vars) {
-        return new SearchScript() {
-            @Override
-            public LeafSearchScript getLeafSearchScript(LeafReaderContext context) throws IOException {
-                AbstractSearchScript leafSearchScript = new AbstractSearchScript() {
-
-                    @Override
-                    public Object run() {
-                        return compiledScript.compiled();
-                    }
-
-                };
-                leafSearchScript.setLookup(lookup.getLeafSearchLookup(context));
-                return leafSearchScript;
-            }
-
-            @Override
-            public boolean needsScores() {
-                return false;
-            }
-        };
-    }
-
-    @Override
-    public void scriptRemoved(@Nullable CompiledScript script) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
index 38ab78b..3e476d2 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
@@ -22,7 +22,6 @@ package org.elasticsearch.script;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.lookup.SearchLookup;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.After;
@@ -45,7 +44,7 @@ import static org.hamcrest.Matchers.containsString;
 // TODO: this needs to be a base test class, and all scripting engines extend it
 public class ScriptModesTests extends ESTestCase {
     private static final Set<String> ALL_LANGS = unmodifiableSet(
-            newHashSet(MustacheScriptEngineService.NAME, "custom", "test"));
+            newHashSet("custom", "test"));
 
     static final String[] ENABLE_VALUES = new String[]{"on", "true", "yes", "1"};
     static final String[] DISABLE_VALUES = new String[]{"off", "false", "no", "0"};
@@ -73,7 +72,6 @@ public class ScriptModesTests extends ESTestCase {
         scriptContextRegistry = new ScriptContextRegistry(contexts.values());
         scriptContexts = scriptContextRegistry.scriptContexts().toArray(new ScriptContext[scriptContextRegistry.scriptContexts().size()]);
         scriptEngines = buildScriptEnginesByLangMap(newHashSet(
-                new MustacheScriptEngineService(Settings.EMPTY),
                 //add the native engine just to make sure it gets filtered out
                 new NativeScriptEngineService(Settings.EMPTY, Collections.<String, NativeScriptFactory>emptyMap()),
                 new CustomScriptEngineService()));
@@ -93,8 +91,8 @@ public class ScriptModesTests extends ESTestCase {
     public void assertAllSettingsWereChecked() {
         if (assertScriptModesNonNull) {
             assertThat(scriptModes, notNullValue());
-            //3 is the number of engines (native excluded), custom is counted twice though as it's associated with two different names
-            int numberOfSettings = 3 * ScriptType.values().length * scriptContextRegistry.scriptContexts().size();
+            //2 is the number of engines (native excluded), custom is counted twice though as it's associated with two different names
+            int numberOfSettings = 2 * ScriptType.values().length * scriptContextRegistry.scriptContexts().size();
             assertThat(scriptModes.scriptModes.size(), equalTo(numberOfSettings));
             if (assertAllSettingsWereChecked) {
                 assertThat(checkedSettings.size(), equalTo(numberOfSettings));
@@ -190,21 +188,6 @@ public class ScriptModesTests extends ESTestCase {
         assertScriptModes(ScriptMode.SANDBOX, ALL_LANGS, new ScriptType[]{ScriptType.INLINE}, complementOf);
     }
 
-    public void testInteractionBetweenGenericAndEngineSpecificSettings() {
-        Settings.Builder builder = Settings.builder().put("script.inline", randomFrom(DISABLE_VALUES))
-                .put(specificEngineOpSettings(MustacheScriptEngineService.NAME, ScriptType.INLINE, ScriptContext.Standard.AGGS), randomFrom(ENABLE_VALUES))
-                .put(specificEngineOpSettings(MustacheScriptEngineService.NAME, ScriptType.INLINE, ScriptContext.Standard.SEARCH), randomFrom(ENABLE_VALUES));
-        Set<String> mustacheLangSet = singleton(MustacheScriptEngineService.NAME);
-        Set<String> allButMustacheLangSet = new HashSet<>(ALL_LANGS);
-        allButMustacheLangSet.remove(MustacheScriptEngineService.NAME);
-        this.scriptModes = new ScriptModes(scriptEngines, scriptContextRegistry, builder.build());
-        assertScriptModes(ScriptMode.ON, mustacheLangSet, new ScriptType[]{ScriptType.INLINE}, ScriptContext.Standard.AGGS, ScriptContext.Standard.SEARCH);
-        assertScriptModes(ScriptMode.OFF, mustacheLangSet, new ScriptType[]{ScriptType.INLINE}, complementOf(ScriptContext.Standard.AGGS, ScriptContext.Standard.SEARCH));
-        assertScriptModesAllOps(ScriptMode.OFF, allButMustacheLangSet, ScriptType.INLINE);
-        assertScriptModesAllOps(ScriptMode.SANDBOX, ALL_LANGS, ScriptType.INDEXED);
-        assertScriptModesAllOps(ScriptMode.ON, ALL_LANGS, ScriptType.FILE);
-    }
-
     private void assertScriptModesAllOps(ScriptMode expectedScriptMode, Set<String> langs, ScriptType... scriptTypes) {
         assertScriptModes(expectedScriptMode, langs, scriptTypes, scriptContexts);
     }
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
index aa7df3f..23cada0 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
@@ -25,7 +25,6 @@ import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.lookup.SearchLookup;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.watcher.ResourceWatcherService;
@@ -73,8 +72,7 @@ public class ScriptServiceTests extends ESTestCase {
                 .put("path.conf", genericConfigFolder)
                 .build();
         resourceWatcherService = new ResourceWatcherService(baseSettings, null);
-        scriptEngineServices = newHashSet(new TestEngineService(),
-                                               new MustacheScriptEngineService(baseSettings));
+        scriptEngineServices = newHashSet(new TestEngineService());
         scriptEnginesByLangMap = ScriptModesTests.buildScriptEnginesByLangMap(scriptEngineServices);
         //randomly register custom script contexts
         int randomInt = randomIntBetween(0, 3);
@@ -199,10 +197,6 @@ public class ScriptServiceTests extends ESTestCase {
         createFileScripts("groovy", "mustache", "test");
 
         for (ScriptContext scriptContext : scriptContexts) {
-            //mustache engine is sandboxed, all scripts are enabled by default
-            assertCompileAccepted(MustacheScriptEngineService.NAME, "script", ScriptType.INLINE, scriptContext, contextAndHeaders);
-            assertCompileAccepted(MustacheScriptEngineService.NAME, "script", ScriptType.INDEXED, scriptContext, contextAndHeaders);
-            assertCompileAccepted(MustacheScriptEngineService.NAME, "file_script", ScriptType.FILE, scriptContext, contextAndHeaders);
             //custom engine is sandboxed, all scripts are enabled by default
             assertCompileAccepted("test", "script", ScriptType.INLINE, scriptContext, contextAndHeaders);
             assertCompileAccepted("test", "script", ScriptType.INDEXED, scriptContext, contextAndHeaders);
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
deleted file mode 100644
index ce29bf2..0000000
--- a/core/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
+++ /dev/null
@@ -1,170 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.script.CompiledScript;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.test.ESTestCase;
-import org.junit.Before;
-
-import java.io.IOException;
-import java.io.StringWriter;
-import java.nio.charset.Charset;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.equalTo;
-
-/**
- * Mustache based templating test
- */
-public class MustacheScriptEngineTests extends ESTestCase {
-    private MustacheScriptEngineService qe;
-    private JsonEscapingMustacheFactory escaper;
-
-    @Before
-    public void setup() {
-        qe = new MustacheScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
-        escaper = new JsonEscapingMustacheFactory();
-    }
-
-    public void testSimpleParameterReplace() {
-        {
-            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
-            Map<String, Object> vars = new HashMap<>();
-            vars.put("boost_val", "0.3");
-            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
-            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.3 } }}",
-                    new String(o.toBytes(), Charset.forName("UTF-8")));
-        }
-        {
-            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"{{body_val}}\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
-            Map<String, Object> vars = new HashMap<>();
-            vars.put("boost_val", "0.3");
-            vars.put("body_val", "\"quick brown\"");
-            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
-            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"\\\"quick brown\\\"\"}}}, \"negative_boost\": 0.3 } }}",
-                    new String(o.toBytes(), Charset.forName("UTF-8")));
-        }
-    }
-
-    public void testEscapeJson() throws IOException {
-        {
-            StringWriter writer = new StringWriter();
-            escaper.encode("hello \n world", writer);
-            assertThat(writer.toString(), equalTo("hello \\n world"));
-        }
-        {
-            StringWriter writer = new StringWriter();
-            escaper.encode("\n", writer);
-            assertThat(writer.toString(), equalTo("\\n"));
-        }
-
-        Character[] specialChars = new Character[]{
-                '\"',
-                '\\',
-                '\u0000',
-                '\u0001',
-                '\u0002',
-                '\u0003',
-                '\u0004',
-                '\u0005',
-                '\u0006',
-                '\u0007',
-                '\u0008',
-                '\u0009',
-                '\u000B',
-                '\u000C',
-                '\u000E',
-                '\u000F',
-                '\u001F'};
-        String[] escapedChars = new String[]{
-                "\\\"",
-                "\\\\",
-                "\\u0000",
-                "\\u0001",
-                "\\u0002",
-                "\\u0003",
-                "\\u0004",
-                "\\u0005",
-                "\\u0006",
-                "\\u0007",
-                "\\u0008",
-                "\\u0009",
-                "\\u000B",
-                "\\u000C",
-                "\\u000E",
-                "\\u000F",
-                "\\u001F"};
-        int iters = scaledRandomIntBetween(100, 1000);
-        for (int i = 0; i < iters; i++) {
-            int rounds = scaledRandomIntBetween(1, 20);
-            StringWriter expect = new StringWriter();
-            StringWriter writer = new StringWriter();
-            for (int j = 0; j < rounds; j++) {
-                String s = getChars();
-                writer.write(s);
-                expect.write(s);
-
-                int charIndex = randomInt(7);
-                writer.append(specialChars[charIndex]);
-                expect.append(escapedChars[charIndex]);
-            }
-            StringWriter target = new StringWriter();
-            escaper.encode(writer.toString(), target);
-            assertThat(expect.toString(), equalTo(target.toString()));
-        }
-    }
-
-    private String getChars() {
-        String string = randomRealisticUnicodeOfCodepointLengthBetween(0, 10);
-        for (int i = 0; i < string.length(); i++) {
-            if (isEscapeChar(string.charAt(i))) {
-                return string.substring(0, i);
-            }
-        }
-        return string;
-    }
-
-    /**
-     * From https://www.ietf.org/rfc/rfc4627.txt:
-     *
-     * All Unicode characters may be placed within the
-     * quotation marks except for the characters that must be escaped:
-     * quotation mark, reverse solidus, and the control characters (U+0000
-     * through U+001F).
-     * */
-    private static boolean isEscapeChar(char c) {
-        switch (c) {
-        case '"':
-        case '\\':
-            return true;
-        }
-
-        if (c < '\u002F')
-            return true;
-        return false;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java b/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
deleted file mode 100644
index 76c8678..0000000
--- a/core/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.script.mustache;
-
-import com.github.mustachejava.DefaultMustacheFactory;
-import com.github.mustachejava.Mustache;
-import com.github.mustachejava.MustacheFactory;
-
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.StringReader;
-import java.io.StringWriter;
-import java.util.HashMap;
-
-/**
- * Figure out how Mustache works for the simplest use case. Leaving in here for now for reference.
- * */
-public class MustacheTests extends ESTestCase {
-    public void test() {
-        HashMap<String, Object> scopes = new HashMap<>();
-        scopes.put("boost_val", "0.2");
-
-        String template = "GET _search {\"query\": " + "{\"boosting\": {"
-                + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}"
-                + "}}, \"negative_boost\": {{boost_val}} } }}";
-        MustacheFactory f = new DefaultMustacheFactory();
-        Mustache mustache = f.compile(new StringReader(template), "example");
-        StringWriter writer = new StringWriter();
-        mustache.execute(writer, scopes);
-        writer.flush();
-        assertEquals(
-                "Mustache templating broken",
-                "GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
-                        + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.2 } }}",
-                writer.toString());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
index cefc232..f527367 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlightBuilderTests.java
@@ -19,24 +19,54 @@
 
 package org.elasticsearch.search.highlight;
 
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexSettings;
+import org.elasticsearch.index.mapper.ContentPath;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.Mapper;
+import org.elasticsearch.index.mapper.MapperBuilders;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.query.IdsQueryBuilder;
+import org.elasticsearch.index.query.IdsQueryParser;
 import org.elasticsearch.index.query.MatchAllQueryBuilder;
+import org.elasticsearch.index.query.MatchAllQueryParser;
 import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.TermQueryBuilder;
+import org.elasticsearch.index.query.TermQueryParser;
+import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.search.highlight.HighlightBuilder.Field;
+import org.elasticsearch.search.highlight.SearchContextHighlight.FieldOptions;
 import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.IndexSettingsModule;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.not;
@@ -45,23 +75,26 @@ public class HighlightBuilderTests extends ESTestCase {
 
     private static final int NUMBER_OF_TESTBUILDERS = 20;
     private static NamedWriteableRegistry namedWriteableRegistry;
+    private static IndicesQueriesRegistry indicesQueriesRegistry;
 
     /**
      * setup for the whole base test class
      */
     @BeforeClass
     public static void init() {
-        if (namedWriteableRegistry == null) {
-            namedWriteableRegistry = new NamedWriteableRegistry();
-            namedWriteableRegistry.registerPrototype(QueryBuilder.class, new MatchAllQueryBuilder());
-            namedWriteableRegistry.registerPrototype(QueryBuilder.class, new IdsQueryBuilder());
-            namedWriteableRegistry.registerPrototype(QueryBuilder.class, new TermQueryBuilder("field", "value"));
-        }
+        namedWriteableRegistry = new NamedWriteableRegistry();
+        @SuppressWarnings("rawtypes")
+        Set<QueryParser> injectedQueryParsers = new HashSet<>();
+        injectedQueryParsers.add(new MatchAllQueryParser());
+        injectedQueryParsers.add(new IdsQueryParser());
+        injectedQueryParsers.add(new TermQueryParser());
+        indicesQueriesRegistry = new IndicesQueriesRegistry(Settings.settingsBuilder().build(), injectedQueryParsers, namedWriteableRegistry);
     }
 
     @AfterClass
     public static void afterClass() throws Exception {
         namedWriteableRegistry = null;
+        indicesQueriesRegistry = null;
     }
 
     /**
@@ -108,6 +141,291 @@ public class HighlightBuilderTests extends ESTestCase {
     }
 
     /**
+     *  creates random highlighter, renders it to xContent and back to new instance that should be equal to original
+     */
+    public void testFromXContent() throws IOException {
+        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
+        context.parseFieldMatcher(new ParseFieldMatcher(Settings.EMPTY));
+        for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {
+            HighlightBuilder highlightBuilder = randomHighlighterBuilder();
+            System.out.println(highlightBuilder);
+            XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+            if (randomBoolean()) {
+                builder.prettyPrint();
+            }
+            builder.startObject();
+            highlightBuilder.innerXContent(builder);
+            builder.endObject();
+
+            XContentParser parser = XContentHelper.createParser(builder.bytes());
+            context.reset(parser);
+            parser.nextToken();
+            HighlightBuilder secondHighlightBuilder = HighlightBuilder.PROTOTYPE.fromXContent(context);
+            assertNotSame(highlightBuilder, secondHighlightBuilder);
+            assertEquals(highlightBuilder, secondHighlightBuilder);
+            assertEquals(highlightBuilder.hashCode(), secondHighlightBuilder.hashCode());
+        }
+    }
+
+    /**
+     * test that unknown array fields cause exception
+     */
+    public void testUnknownArrayNameExpection() throws IOException {
+        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
+        context.parseFieldMatcher(new ParseFieldMatcher(Settings.EMPTY));
+        String highlightElement = "{\n" +
+                "    \"bad_fieldname\" : [ \"field1\" 1 \"field2\" ]\n" +
+                "}\n";
+        XContentParser parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        try {
+            HighlightBuilder.PROTOTYPE.fromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("cannot parse array with name [bad_fieldname]", e.getMessage());
+        }
+
+        highlightElement = "{\n" +
+                "  \"fields\" : {\n" +
+                "     \"body\" : {\n" +
+                "        \"bad_fieldname\" : [ \"field1\" , \"field2\" ]\n" +
+                "     }\n" +
+                "   }\n" +
+                "}\n";
+        parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        try {
+            HighlightBuilder.PROTOTYPE.fromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("cannot parse array with name [bad_fieldname]", e.getMessage());
+        }
+    }
+
+    /**
+     * test that unknown field name cause exception
+     */
+    public void testUnknownFieldnameExpection() throws IOException {
+        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
+        context.parseFieldMatcher(new ParseFieldMatcher(Settings.EMPTY));
+        String highlightElement = "{\n" +
+                "    \"bad_fieldname\" : \"value\"\n" +
+                "}\n";
+        XContentParser parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        try {
+            HighlightBuilder.PROTOTYPE.fromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("unexpected fieldname [bad_fieldname]", e.getMessage());
+        }
+
+        highlightElement = "{\n" +
+                "  \"fields\" : {\n" +
+                "     \"body\" : {\n" +
+                "        \"bad_fieldname\" : \"value\"\n" +
+                "     }\n" +
+                "   }\n" +
+                "}\n";
+        parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        try {
+            HighlightBuilder.PROTOTYPE.fromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("unexpected fieldname [bad_fieldname]", e.getMessage());
+        }
+    }
+
+    /**
+     * test that unknown field name cause exception
+     */
+    public void testUnknownObjectFieldnameExpection() throws IOException {
+        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
+        context.parseFieldMatcher(new ParseFieldMatcher(Settings.EMPTY));
+        String highlightElement = "{\n" +
+                "    \"bad_fieldname\" :  { \"field\" : \"value\" }\n \n" +
+                "}\n";
+        XContentParser parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        try {
+            HighlightBuilder.PROTOTYPE.fromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("cannot parse object with name [bad_fieldname]", e.getMessage());
+        }
+
+        highlightElement = "{\n" +
+                "  \"fields\" : {\n" +
+                "     \"body\" : {\n" +
+                "        \"bad_fieldname\" : { \"field\" : \"value\" }\n" +
+                "     }\n" +
+                "   }\n" +
+                "}\n";
+        parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        try {
+            HighlightBuilder.PROTOTYPE.fromXContent(context);
+            fail("expected a parsing exception");
+        } catch (ParsingException e) {
+            assertEquals("cannot parse object with name [bad_fieldname]", e.getMessage());
+        }
+     }
+
+     /**
+     * test that build() outputs a {@link SearchContextHighlight} that is similar to the one
+     * we would get when parsing the xContent the test highlight builder is rendering out
+     */
+    public void testBuildSearchContextHighlight() throws IOException {
+        Settings indexSettings = Settings.settingsBuilder()
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
+        Index index = new Index(randomAsciiOfLengthBetween(1, 10));
+        IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, indexSettings);
+        // shard context will only need indicesQueriesRegistry for building Query objects nested in highlighter
+        QueryShardContext mockShardContext = new QueryShardContext(idxSettings, null, null, null, null, null, null, indicesQueriesRegistry) {
+            @Override
+            public MappedFieldType fieldMapper(String name) {
+                StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
+                return builder.build(new Mapper.BuilderContext(idxSettings.getSettings(), new ContentPath(1))).fieldType();
+            }
+        };
+        mockShardContext.setMapUnmappedFieldAsString(true);
+
+        for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {
+            HighlightBuilder highlightBuilder = randomHighlighterBuilder();
+            SearchContextHighlight highlight = highlightBuilder.build(mockShardContext);
+            XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+            if (randomBoolean()) {
+                builder.prettyPrint();
+            }
+            builder.startObject();
+            highlightBuilder.innerXContent(builder);
+            builder.endObject();
+            XContentParser parser = XContentHelper.createParser(builder.bytes());
+
+            SearchContextHighlight parsedHighlight = new HighlighterParseElement().parse(parser, mockShardContext);
+            assertNotSame(highlight, parsedHighlight);
+            assertEquals(highlight.globalForceSource(), parsedHighlight.globalForceSource());
+            assertEquals(highlight.fields().size(), parsedHighlight.fields().size());
+
+            Iterator<org.elasticsearch.search.highlight.SearchContextHighlight.Field> iterator = parsedHighlight.fields().iterator();
+            for (org.elasticsearch.search.highlight.SearchContextHighlight.Field field : highlight.fields()) {
+                org.elasticsearch.search.highlight.SearchContextHighlight.Field otherField = iterator.next();
+                assertEquals(field.field(), otherField.field());
+                FieldOptions options = field.fieldOptions();
+                FieldOptions otherOptions = otherField.fieldOptions();
+                assertArrayEquals(options.boundaryChars(), options.boundaryChars());
+                assertEquals(options.boundaryMaxScan(), otherOptions.boundaryMaxScan());
+                assertEquals(options.encoder(), otherOptions.encoder());
+                assertEquals(options.fragmentCharSize(), otherOptions.fragmentCharSize());
+                assertEquals(options.fragmenter(), otherOptions.fragmenter());
+                assertEquals(options.fragmentOffset(), otherOptions.fragmentOffset());
+                assertEquals(options.highlighterType(), otherOptions.highlighterType());
+                assertEquals(options.highlightFilter(), otherOptions.highlightFilter());
+                assertEquals(options.highlightQuery(), otherOptions.highlightQuery());
+                assertEquals(options.matchedFields(), otherOptions.matchedFields());
+                assertEquals(options.noMatchSize(), otherOptions.noMatchSize());
+                assertEquals(options.numberOfFragments(), otherOptions.numberOfFragments());
+                assertEquals(options.options(), otherOptions.options());
+                assertEquals(options.phraseLimit(), otherOptions.phraseLimit());
+                assertArrayEquals(options.preTags(), otherOptions.preTags());
+                assertArrayEquals(options.postTags(), otherOptions.postTags());
+                assertEquals(options.requireFieldMatch(), otherOptions.requireFieldMatch());
+                assertEquals(options.scoreOrdered(), otherOptions.scoreOrdered());
+            }
+        }
+    }
+
+    /**
+     * `tags_schema` is not produced by toXContent in the builder but should be parseable, so this
+     * adds a simple json test for this.
+     */
+    public void testParsingTagsSchema() throws IOException {
+        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
+        context.parseFieldMatcher(new ParseFieldMatcher(Settings.EMPTY));
+        String highlightElement = "{\n" +
+                "    \"tags_schema\" : \"styled\"\n" +
+                "}\n";
+        XContentParser parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        HighlightBuilder highlightBuilder = HighlightBuilder.PROTOTYPE.fromXContent(context);
+        assertArrayEquals("setting tags_schema 'styled' should alter pre_tags", HighlightBuilder.DEFAULT_STYLED_PRE_TAG,
+                highlightBuilder.preTags());
+        assertArrayEquals("setting tags_schema 'styled' should alter post_tags", HighlightBuilder.DEFAULT_STYLED_POST_TAGS,
+                highlightBuilder.postTags());
+
+        highlightElement = "{\n" +
+                "    \"tags_schema\" : \"default\"\n" +
+                "}\n";
+        parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        highlightBuilder = HighlightBuilder.PROTOTYPE.fromXContent(context);
+        assertArrayEquals("setting tags_schema 'default' should alter pre_tags", HighlightBuilder.DEFAULT_PRE_TAGS,
+                highlightBuilder.preTags());
+        assertArrayEquals("setting tags_schema 'default' should alter post_tags", HighlightBuilder.DEFAULT_POST_TAGS,
+                highlightBuilder.postTags());
+
+        highlightElement = "{\n" +
+                "    \"tags_schema\" : \"somthing_else\"\n" +
+                "}\n";
+        parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        try {
+            HighlightBuilder.PROTOTYPE.fromXContent(context);
+            fail("setting unknown tag schema should throw exception");
+        } catch (IllegalArgumentException e) {
+            assertEquals("Unknown tag schema [somthing_else]", e.getMessage());
+        }
+    }
+
+    /**
+     * test parsing empty highlight or empty fields blocks
+     */
+    public void testParsingEmptyStructure() throws IOException {
+        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
+        context.parseFieldMatcher(new ParseFieldMatcher(Settings.EMPTY));
+        String highlightElement = "{ }";
+        XContentParser parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        HighlightBuilder highlightBuilder = HighlightBuilder.PROTOTYPE.fromXContent(context);
+        assertEquals("expected plain HighlightBuilder", new HighlightBuilder(), highlightBuilder);
+
+        highlightElement = "{ \"fields\" : { } }";
+        parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        highlightBuilder = HighlightBuilder.PROTOTYPE.fromXContent(context);
+        assertEquals("defining no field should return plain HighlightBuilder", new HighlightBuilder(), highlightBuilder);
+
+        highlightElement = "{ \"fields\" : { \"foo\" : { } } }";
+        parser = XContentFactory.xContent(highlightElement).createParser(highlightElement);
+
+        context.reset(parser);
+        highlightBuilder = HighlightBuilder.PROTOTYPE.fromXContent(context);
+        assertEquals("expected HighlightBuilder with field", new HighlightBuilder().field(new Field("foo")), highlightBuilder);
+        System.out.println(Math.log(1/(double)(1+1)) + 1.0);
+    }
+
+    protected static XContentBuilder toXContent(HighlightBuilder highlight, XContentType contentType) throws IOException {
+        XContentBuilder builder = XContentFactory.contentBuilder(contentType);
+        if (randomBoolean()) {
+            builder.prettyPrint();
+        }
+        highlight.toXContent(builder, ToXContent.EMPTY_PARAMS);
+        return builder;
+    }
+
+    /**
      * create random shape that is put under test
      */
     private static HighlightBuilder randomHighlighterBuilder() {
@@ -132,11 +450,11 @@ public class HighlightBuilderTests extends ESTestCase {
         return testHighlighter;
     }
 
+    @SuppressWarnings({ "rawtypes", "unchecked" })
     private static void setRandomCommonOptions(AbstractHighlighterBuilder highlightBuilder) {
         if (randomBoolean()) {
+            // need to set this together, otherwise parsing will complain
             highlightBuilder.preTags(randomStringArray(0, 3));
-        }
-        if (randomBoolean()) {
             highlightBuilder.postTags(randomStringArray(0, 3));
         }
         if (randomBoolean()) {
@@ -213,7 +531,7 @@ public class HighlightBuilderTests extends ESTestCase {
         }
     }
 
-    @SuppressWarnings("unchecked")
+    @SuppressWarnings({ "unchecked", "rawtypes" })
     private static void mutateCommonOptions(AbstractHighlighterBuilder highlightBuilder) {
         switch (randomIntBetween(1, 16)) {
         case 1:
@@ -242,6 +560,7 @@ public class HighlightBuilderTests extends ESTestCase {
             break;
         case 9:
             highlightBuilder.highlightFilter(toggleOrSet(highlightBuilder.highlightFilter()));
+            break;
         case 10:
             highlightBuilder.forceSource(toggleOrSet(highlightBuilder.forceSource()));
             break;
@@ -316,6 +635,7 @@ public class HighlightBuilderTests extends ESTestCase {
                             fieldToChange.matchedFields(randomStringArray(5, 10));
                         }
                     }
+                    break;
             }
         }
         return mutation;
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 8a2506e..63378ba 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -58,7 +58,6 @@ import static org.elasticsearch.index.query.QueryBuilders.fuzzyQuery;
 import static org.elasticsearch.index.query.QueryBuilders.matchPhrasePrefixQuery;
 import static org.elasticsearch.index.query.QueryBuilders.matchPhraseQuery;
 import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
 import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
 import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
 import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
@@ -67,6 +66,7 @@ import static org.elasticsearch.index.query.QueryBuilders.regexpQuery;
 import static org.elasticsearch.index.query.QueryBuilders.termQuery;
 import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
 import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
+import static org.elasticsearch.index.query.QueryBuilders.existsQuery;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.highlight;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
@@ -2471,7 +2471,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery()
-                .should(constantScoreQuery(QueryBuilders.missingQuery("field1")))
+                .should(boolQuery().mustNot(QueryBuilders.existsQuery("field1")))
                 .should(matchQuery("field1", "test"))
                 .should(constantScoreQuery(queryStringQuery("field1:photo*"))))
                 .highlighter(highlight().field("field1"));
@@ -2501,7 +2501,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
 
         logger.info("--> highlighting and searching on field1");
-        SearchSourceBuilder source = searchSource().query(boolQuery().must(queryStringQuery("field1:photo*")).filter(missingQuery("field_null")))
+        SearchSourceBuilder source = searchSource().query(boolQuery()
+                .must(queryStringQuery("field1:photo*"))
+                .mustNot(existsQuery("field_null")))
                 .highlighter(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
diff --git a/core/src/test/java/org/elasticsearch/search/query/ExistsIT.java b/core/src/test/java/org/elasticsearch/search/query/ExistsIT.java
new file mode 100644
index 0000000..73906b2
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/search/query/ExistsIT.java
@@ -0,0 +1,150 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.search.query;
+
+import org.elasticsearch.action.explain.ExplainResponse;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.search.SearchHit;
+import org.elasticsearch.test.ESIntegTestCase;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonMap;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+
+public class ExistsIT extends ESIntegTestCase {
+
+    // TODO: move this to a unit test somewhere...
+    public void testEmptyIndex() throws Exception {
+        createIndex("test");
+        ensureYellow("test");
+        SearchResponse resp = client().prepareSearch("test").setQuery(QueryBuilders.existsQuery("foo")).execute().actionGet();
+        assertSearchResponse(resp);
+        resp = client().prepareSearch("test").setQuery(QueryBuilders.boolQuery().mustNot(QueryBuilders.existsQuery("foo"))).execute().actionGet();
+        assertSearchResponse(resp);
+    }
+
+    public void testExists() throws Exception {
+        XContentBuilder mapping = XContentBuilder.builder(JsonXContent.jsonXContent)
+            .startObject()
+                .startObject("type")
+                    .startObject(FieldNamesFieldMapper.NAME)
+                        .field("enabled", randomBoolean())
+                    .endObject()
+                    .startObject("properties")
+                        .startObject("foo")
+                            .field("type", "string")
+                        .endObject()
+                        .startObject("bar")
+                            .field("type", "object")
+                            .startObject("properties")
+                                .startObject("foo")
+                                    .field("type", "string")
+                                .endObject()
+                                .startObject("bar")
+                                    .field("type", "object")
+                                    .startObject("properties")
+                                        .startObject("bar")
+                                            .field("type", "string")
+                                        .endObject()
+                                    .endObject()
+                                .endObject()
+                                .startObject("baz")
+                                    .field("type", "long")
+                                .endObject()
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                .endObject()
+            .endObject();
+
+        assertAcked(client().admin().indices().prepareCreate("idx").addMapping("type", mapping));
+        @SuppressWarnings("unchecked")
+        Map<String, Object> barObject = new HashMap<>();
+        barObject.put("foo", "bar");
+        barObject.put("bar", singletonMap("bar", "foo"));
+        final Map<String, Object>[] sources = new Map[] {
+                // simple property
+                singletonMap("foo", "bar"),
+                // object fields
+                singletonMap("bar", barObject),
+                singletonMap("bar", singletonMap("baz", 42)),
+                // empty doc
+                emptyMap()
+        };
+        List<IndexRequestBuilder> reqs = new ArrayList<IndexRequestBuilder>();
+        for (Map<String, Object> source : sources) {
+            reqs.add(client().prepareIndex("idx", "type").setSource(source));
+        }
+        // We do NOT index dummy documents, otherwise the type for these dummy documents
+        // would have _field_names indexed while the current type might not which might
+        // confuse the exists/missing parser at query time
+        indexRandom(true, false, reqs);
+
+        final Map<String, Integer> expected = new LinkedHashMap<String, Integer>();
+        expected.put("foo", 1);
+        expected.put("f*", 1);
+        expected.put("bar", 2);
+        expected.put("bar.*", 2);
+        expected.put("bar.foo", 1);
+        expected.put("bar.bar", 1);
+        expected.put("bar.bar.bar", 1);
+        expected.put("foobar", 0);
+
+        ensureYellow("idx");
+        final long numDocs = sources.length;
+        SearchResponse allDocs = client().prepareSearch("idx").setSize(sources.length).get();
+        assertSearchResponse(allDocs);
+        assertHitCount(allDocs, numDocs);
+        for (Map.Entry<String, Integer> entry : expected.entrySet()) {
+            final String fieldName = entry.getKey();
+            final int count = entry.getValue();
+            // exists
+            SearchResponse resp = client().prepareSearch("idx").setQuery(QueryBuilders.existsQuery(fieldName)).execute().actionGet();
+            assertSearchResponse(resp);
+            try {
+                assertEquals(String.format(Locale.ROOT, "exists(%s, %d) mapping: %s response: %s", fieldName, count, mapping.string(), resp), count, resp.getHits().totalHits());
+            } catch (AssertionError e) {
+                for (SearchHit searchHit : allDocs.getHits()) {
+                    final String index = searchHit.getIndex();
+                    final String type = searchHit.getType();
+                    final String id = searchHit.getId();
+                    final ExplainResponse explanation = client().prepareExplain(index, type, id).setQuery(QueryBuilders.existsQuery(fieldName)).get();
+                    logger.info("Explanation for [{}] / [{}] / [{}]: [{}]", fieldName, id, searchHit.getSourceAsString(), explanation.getExplanation());
+                }
+                throw e;
+            }
+        }
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java b/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
deleted file mode 100644
index 349197d..0000000
--- a/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
+++ /dev/null
@@ -1,206 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.query;
-
-import org.elasticsearch.action.explain.ExplainResponse;
-import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.json.JsonXContent;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-
-import static java.util.Collections.emptyMap;
-import static java.util.Collections.singletonMap;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-
-public class ExistsMissingIT extends ESIntegTestCase {
-
-    // TODO: move this to a unit test somewhere...
-    public void testEmptyIndex() throws Exception {
-        createIndex("test");
-        ensureYellow("test");
-        SearchResponse resp = client().prepareSearch("test").setQuery(QueryBuilders.existsQuery("foo")).execute().actionGet();
-        assertSearchResponse(resp);
-        resp = client().prepareSearch("test").setQuery(QueryBuilders.missingQuery("foo")).execute().actionGet();
-        assertSearchResponse(resp);
-    }
-
-    public void testExistsMissing() throws Exception {
-        XContentBuilder mapping = XContentBuilder.builder(JsonXContent.jsonXContent)
-            .startObject()
-                .startObject("type")
-                    .startObject(FieldNamesFieldMapper.NAME)
-                        .field("enabled", randomBoolean())
-                    .endObject()
-                    .startObject("properties")
-                        .startObject("foo")
-                            .field("type", "string")
-                        .endObject()
-                        .startObject("bar")
-                            .field("type", "object")
-                            .startObject("properties")
-                                .startObject("foo")
-                                    .field("type", "string")
-                                .endObject()
-                                .startObject("bar")
-                                    .field("type", "object")
-                                    .startObject("properties")
-                                        .startObject("bar")
-                                            .field("type", "string")
-                                        .endObject()
-                                    .endObject()
-                                .endObject()
-                                .startObject("baz")
-                                    .field("type", "long")
-                                .endObject()
-                            .endObject()
-                        .endObject()
-                    .endObject()
-                .endObject()
-            .endObject();
-
-        assertAcked(client().admin().indices().prepareCreate("idx").addMapping("type", mapping));
-        @SuppressWarnings("unchecked")
-        Map<String, Object> barObject = new HashMap<>();
-        barObject.put("foo", "bar");
-        barObject.put("bar", singletonMap("bar", "foo"));
-        final Map<String, Object>[] sources = new Map[] {
-                // simple property
-                singletonMap("foo", "bar"),
-                // object fields
-                singletonMap("bar", barObject),
-                singletonMap("bar", singletonMap("baz", 42)),
-                // empty doc
-                emptyMap()
-        };
-        List<IndexRequestBuilder> reqs = new ArrayList<IndexRequestBuilder>();
-        for (Map<String, Object> source : sources) {
-            reqs.add(client().prepareIndex("idx", "type").setSource(source));
-        }
-        // We do NOT index dummy documents, otherwise the type for these dummy documents
-        // would have _field_names indexed while the current type might not which might
-        // confuse the exists/missing parser at query time
-        indexRandom(true, false, reqs);
-
-        final Map<String, Integer> expected = new LinkedHashMap<String, Integer>();
-        expected.put("foo", 1);
-        expected.put("f*", 1);
-        expected.put("bar", 2);
-        expected.put("bar.*", 2);
-        expected.put("bar.foo", 1);
-        expected.put("bar.bar", 1);
-        expected.put("bar.bar.bar", 1);
-        expected.put("foobar", 0);
-
-        ensureYellow("idx");
-        final long numDocs = sources.length;
-        SearchResponse allDocs = client().prepareSearch("idx").setSize(sources.length).get();
-        assertSearchResponse(allDocs);
-        assertHitCount(allDocs, numDocs);
-        for (Map.Entry<String, Integer> entry : expected.entrySet()) {
-            final String fieldName = entry.getKey();
-            final int count = entry.getValue();
-            // exists
-            SearchResponse resp = client().prepareSearch("idx").setQuery(QueryBuilders.existsQuery(fieldName)).execute().actionGet();
-            assertSearchResponse(resp);
-            try {
-                assertEquals(String.format(Locale.ROOT, "exists(%s, %d) mapping: %s response: %s", fieldName, count, mapping.string(), resp), count, resp.getHits().totalHits());
-            } catch (AssertionError e) {
-                for (SearchHit searchHit : allDocs.getHits()) {
-                    final String index = searchHit.getIndex();
-                    final String type = searchHit.getType();
-                    final String id = searchHit.getId();
-                    final ExplainResponse explanation = client().prepareExplain(index, type, id).setQuery(QueryBuilders.existsQuery(fieldName)).get();
-                    logger.info("Explanation for [{}] / [{}] / [{}]: [{}]", fieldName, id, searchHit.getSourceAsString(), explanation.getExplanation());
-                }
-                throw e;
-            }
-
-            // missing
-            resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery(fieldName)).execute().actionGet();
-            assertSearchResponse(resp);
-            assertEquals(String.format(Locale.ROOT, "missing(%s, %d) mapping: %s response: %s", fieldName, count, mapping.string(), resp), numDocs - count, resp.getHits().totalHits());
-        }
-    }
-
-    public void testNullValueUnset() throws Exception {
-        assertAcked(client().admin().indices().prepareCreate("idx").addMapping("type", "f", "type=string,index=not_analyzed"));
-        indexRandom(true,
-                client().prepareIndex("idx", "type", "1").setSource("f", "foo"),
-                client().prepareIndex("idx", "type", "2").setSource("f", null),
-                client().prepareIndex("idx", "type", "3").setSource("g", "bar"),
-                client().prepareIndex("idx", "type", "4").setSource("f", "bar"));
-
-        SearchResponse resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", true, true)).get();
-        assertSearchHits(resp, "2", "3");
-
-        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", false, true)).get();
-        assertSearchHits(resp, "2", "3");
-
-        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", true, false)).get();
-        assertSearchHits(resp);
-
-        try {
-            client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", false, false)).get();
-            fail("both existence and null_value can't be false");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-    public void testNullValueSet() throws Exception {
-        assertAcked(client().admin().indices().prepareCreate("idx").addMapping("type", "f", "type=string,index=not_analyzed,null_value=bar"));
-        indexRandom(true,
-                client().prepareIndex("idx", "type", "1").setSource("f", "foo"),
-                client().prepareIndex("idx", "type", "2").setSource("f", null),
-                client().prepareIndex("idx", "type", "3").setSource("g", "bar"),
-                client().prepareIndex("idx", "type", "4").setSource("f", "bar"));
-
-        SearchResponse resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", true, true)).get();
-        assertSearchHits(resp, "2", "3", "4");
-
-        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", false, true)).get();
-        assertSearchHits(resp, "3");
-
-        resp = client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", true, false)).get();
-        assertSearchHits(resp, "2", "4");
-
-        try {
-            client().prepareSearch("idx").setQuery(QueryBuilders.missingQuery("f", false, false)).get();
-            fail("both existence and null_value can't be false");
-        } catch (IllegalArgumentException e) {
-            // expected
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index 61a237c..9918d44 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -70,7 +70,6 @@ import static org.elasticsearch.index.query.QueryBuilders.idsQuery;
 import static org.elasticsearch.index.query.QueryBuilders.indicesQuery;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
 import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
 import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
 import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
@@ -805,32 +804,6 @@ public class SearchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch().setQuery(existsQuery("obj1")).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "1", "2");
-
-        searchResponse = client().prepareSearch().setQuery(missingQuery("field1")).get();
-        assertHitCount(searchResponse, 2l);
-        assertSearchHits(searchResponse, "3", "4");
-
-        searchResponse = client().prepareSearch().setQuery(missingQuery("field1")).get();
-        assertHitCount(searchResponse, 2l);
-        assertSearchHits(searchResponse, "3", "4");
-
-        searchResponse = client().prepareSearch().setQuery(constantScoreQuery(missingQuery("field1"))).get();
-        assertHitCount(searchResponse, 2l);
-        assertSearchHits(searchResponse, "3", "4");
-
-        searchResponse = client().prepareSearch().setQuery(queryStringQuery("_missing_:field1")).get();
-        assertHitCount(searchResponse, 2l);
-        assertSearchHits(searchResponse, "3", "4");
-
-        // wildcard check
-        searchResponse = client().prepareSearch().setQuery(missingQuery("x*")).get();
-        assertHitCount(searchResponse, 2l);
-        assertSearchHits(searchResponse, "3", "4");
-
-        // object check
-        searchResponse = client().prepareSearch().setQuery(missingQuery("obj1")).get();
-        assertHitCount(searchResponse, 2l);
-        assertSearchHits(searchResponse, "3", "4");
     }
 
     public void testPassQueryOrFilterAsJSONString() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
index 6f00e99..0f5ac1a 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java
@@ -942,13 +942,13 @@ public class CompletionSuggestSearchIT extends ESIntegTestCase {
 
     }
     public void assertSuggestions(String suggestion, String... suggestions) {
-        String suggestionName = RandomStrings.randomAsciiOfLength(new Random(), 10);
+        String suggestionName = RandomStrings.randomAsciiOfLength(random(), 10);
         CompletionSuggestionBuilder suggestionBuilder = SuggestBuilders.completionSuggestion(suggestionName).field(FIELD).text(suggestion).size(10);
         assertSuggestions(suggestionName, suggestionBuilder, suggestions);
     }
 
     public void assertSuggestionsNotInOrder(String suggestString, String... suggestions) {
-        String suggestionName = RandomStrings.randomAsciiOfLength(new Random(), 10);
+        String suggestionName = RandomStrings.randomAsciiOfLength(random(), 10);
         SuggestResponse suggestResponse = client().prepareSuggest(INDEX).addSuggestion(
                 SuggestBuilders.completionSuggestion(suggestionName).field(FIELD).text(suggestString).size(10)
         ).execute().actionGet();
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
deleted file mode 100644
index 1850abc..0000000
--- a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
+++ /dev/null
@@ -1,1291 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.suggest;
-
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
-import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.search.ReduceSearchPhaseException;
-import org.elasticsearch.action.search.SearchPhaseExecutionException;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.ShardSearchFailure;
-import org.elasticsearch.action.suggest.SuggestRequestBuilder;
-import org.elasticsearch.action.suggest.SuggestResponse;
-import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder;
-import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
-import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.DirectCandidateGenerator;
-import org.elasticsearch.search.suggest.term.TermSuggestionBuilder;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.test.hamcrest.ElasticsearchAssertions;
-
-import java.io.IOException;
-import java.net.URISyntaxException;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.Files;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.ExecutionException;
-
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
-import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
-import static org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.candidateGenerator;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionPhraseCollateMatchExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
-
-/**
- * Integration tests for term and phrase suggestions.  Many of these tests many requests that vary only slightly from one another.  Where
- * possible these tests should declare for the first request, make the request, modify the configuration for the next request, make that
- * request, modify again, request again, etc.  This makes it very obvious what changes between requests.
- */
-public class SuggestSearchIT extends ESIntegTestCase {
-    // see #3196
-    public void testSuggestAcrossMultipleIndices() throws IOException {
-        createIndex("test");
-        ensureGreen();
-
-        index("test", "type1", "1", "text", "abcd");
-        index("test", "type1", "2", "text", "aacd");
-        index("test", "type1", "3", "text", "abbd");
-        index("test", "type1", "4", "text", "abcc");
-        refresh();
-
-        TermSuggestionBuilder termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("abcd")
-                .field("text");
-        logger.info("--> run suggestions with one index");
-        searchSuggest( termSuggest);
-        createIndex("test_1");
-        ensureGreen();
-
-        index("test_1", "type1", "1", "text", "ab cd");
-        index("test_1", "type1", "2", "text", "aa cd");
-        index("test_1", "type1", "3", "text", "ab bd");
-        index("test_1", "type1", "4", "text", "ab cc");
-        refresh();
-        termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("ab cd")
-                .minWordLength(1)
-                .field("text");
-        logger.info("--> run suggestions with two indices");
-        searchSuggest( termSuggest);
-
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("properties")
-                .startObject("text").field("type", "string").field("analyzer", "keyword").endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(prepareCreate("test_2").addMapping("type1", mapping));
-        ensureGreen();
-
-        index("test_2", "type1", "1", "text", "ab cd");
-        index("test_2", "type1", "2", "text", "aa cd");
-        index("test_2", "type1", "3", "text", "ab bd");
-        index("test_2", "type1", "4", "text", "ab cc");
-        index("test_2", "type1", "1", "text", "abcd");
-        index("test_2", "type1", "2", "text", "aacd");
-        index("test_2", "type1", "3", "text", "abbd");
-        index("test_2", "type1", "4", "text", "abcc");
-        refresh();
-
-        termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("ab cd")
-                .minWordLength(1)
-                .field("text");
-        logger.info("--> run suggestions with three indices");
-        try {
-            searchSuggest( termSuggest);
-            fail(" can not suggest across multiple indices with different analysis chains");
-        } catch (ReduceSearchPhaseException ex) {
-            assertThat(ex.getCause(), instanceOf(IllegalStateException.class));
-            assertThat(ex.getCause().getMessage(),
-                    anyOf(endsWith("Suggest entries have different sizes actual [1] expected [2]"),
-                            endsWith("Suggest entries have different sizes actual [2] expected [1]")));
-        } catch (IllegalStateException ex) {
-            assertThat(ex.getMessage(), anyOf(endsWith("Suggest entries have different sizes actual [1] expected [2]"),
-                    endsWith("Suggest entries have different sizes actual [2] expected [1]")));
-        }
-
-
-        termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("ABCD")
-                .minWordLength(1)
-                .field("text");
-        logger.info("--> run suggestions with four indices");
-        try {
-            searchSuggest( termSuggest);
-            fail(" can not suggest across multiple indices with different analysis chains");
-        } catch (ReduceSearchPhaseException ex) {
-            assertThat(ex.getCause(), instanceOf(IllegalStateException.class));
-            assertThat(ex.getCause().getMessage(), anyOf(endsWith("Suggest entries have different text actual [ABCD] expected [abcd]"),
-                    endsWith("Suggest entries have different text actual [abcd] expected [ABCD]")));
-        } catch (IllegalStateException ex) {
-            assertThat(ex.getMessage(), anyOf(endsWith("Suggest entries have different text actual [ABCD] expected [abcd]"),
-                    endsWith("Suggest entries have different text actual [abcd] expected [ABCD]")));
-        }
-    }
-
-    // see #3037
-    public void testSuggestModes() throws IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put(SETTING_NUMBER_OF_REPLICAS, 0)
-                .put("index.analysis.analyzer.biword.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.biword.filter", "shingler", "lowercase")
-                .put("index.analysis.filter.shingler.type", "shingle")
-                .put("index.analysis.filter.shingler.min_shingle_size", 2)
-                .put("index.analysis.filter.shingler.max_shingle_size", 3));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("properties")
-                .startObject("name")
-                    .field("type", "multi_field")
-                    .startObject("fields")
-                        .startObject("name")
-                            .field("type", "string")
-                        .endObject()
-                        .startObject("shingled")
-                            .field("type", "string")
-                            .field("analyzer", "biword")
-                            .field("search_analyzer", "standard")
-                        .endObject()
-                    .endObject()
-                .endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-
-        index("test", "type1", "1", "name", "I like iced tea");
-        index("test", "type1", "2", "name", "I like tea.");
-        index("test", "type1", "3", "name", "I like ice cream.");
-        refresh();
-
-        DirectCandidateGenerator generator = candidateGenerator("name").prefixLength(0).minWordLength(0).suggestMode("always").maxEdits(2);
-        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("did_you_mean").field("name.shingled")
-                .addCandidateGenerator(generator)
-                .gramSize(3);
-        Suggest searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
-        assertSuggestion(searchSuggest, 0, "did_you_mean", "iced tea");
-
-        generator.suggestMode(null);
-        searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
-        assertSuggestionSize(searchSuggest, 0, 0, "did_you_mean");
-    }
-
-    // see #2729
-    public void testSizeOneShard() throws Exception {
-        prepareCreate("test").setSettings(
-                SETTING_NUMBER_OF_SHARDS, 1,
-                SETTING_NUMBER_OF_REPLICAS, 0).get();
-        ensureGreen();
-
-        for (int i = 0; i < 15; i++) {
-            index("test", "type1", Integer.toString(i), "text", "abc" + i);
-        }
-        refresh();
-
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellchecker")).get();
-        assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
-
-        TermSuggestionBuilder termSuggestion = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("abcd")
-                .field("text")
-                .size(10);
-        Suggest suggest = searchSuggest( termSuggestion);
-        assertSuggestion(suggest, 0, "test", 10, "abc0");
-
-        termSuggestion.text("abcd").shardSize(5);
-        suggest = searchSuggest( termSuggestion);
-        assertSuggestion(suggest, 0, "test", 5, "abc0");
-    }
-
-    public void testUnmappedField() throws IOException, InterruptedException, ExecutionException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.biword.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.biword.filter", "shingler", "lowercase")
-                .put("index.analysis.filter.shingler.type", "shingle")
-                .put("index.analysis.filter.shingler.min_shingle_size", 2)
-                .put("index.analysis.filter.shingler.max_shingle_size", 3));
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("properties")
-                .startObject("name")
-                    .field("type", "multi_field")
-                    .startObject("fields")
-                        .startObject("name")
-                            .field("type", "string")
-                        .endObject()
-                        .startObject("shingled")
-                            .field("type", "string")
-                            .field("analyzer", "biword")
-                            .field("search_analyzer", "standard")
-                        .endObject()
-                    .endObject()
-                .endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        indexRandom(true, client().prepareIndex("test", "type1").setSource("name", "I like iced tea"),
-        client().prepareIndex("test", "type1").setSource("name", "I like tea."),
-        client().prepareIndex("test", "type1").setSource("name", "I like ice cream."));
-        refresh();
-
-        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("did_you_mean").field("name.shingled")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("name").prefixLength(0).minWordLength(0).suggestMode("always").maxEdits(2))
-                .gramSize(3);
-        Suggest searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
-        assertSuggestion(searchSuggest, 0, 0, "did_you_mean", "iced tea");
-
-        phraseSuggestion.field("nosuchField");
-        {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
-        }
-        {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
-        }
-    }
-
-    public void testSimple() throws Exception {
-        createIndex("test");
-        ensureGreen();
-
-        index("test", "type1", "1", "text", "abcd");
-        index("test", "type1", "2", "text", "aacd");
-        index("test", "type1", "3", "text", "abbd");
-        index("test", "type1", "4", "text", "abcc");
-        refresh();
-
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellcecker")).get();
-        assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
-
-        TermSuggestionBuilder termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("abcd")
-                .field("text");
-        Suggest suggest = searchSuggest( termSuggest);
-        assertSuggestion(suggest, 0, "test", "aacd", "abbd", "abcc");
-        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
-
-        suggest = searchSuggest( termSuggest);
-        assertSuggestion(suggest, 0, "test", "aacd","abbd", "abcc");
-        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
-    }
-
-    public void testEmpty() throws Exception {
-        createIndex("test");
-        ensureGreen();
-
-        index("test", "type1", "1", "foo", "bar");
-        refresh();
-
-        TermSuggestionBuilder termSuggest = termSuggestion("test")
-                .suggestMode("always") // Always, otherwise the results can vary between requests.
-                .text("abcd")
-                .field("text");
-        Suggest suggest = searchSuggest( termSuggest);
-        assertSuggestionSize(suggest, 0, 0, "test");
-        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
-
-        suggest = searchSuggest( termSuggest);
-        assertSuggestionSize(suggest, 0, 0, "test");
-        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
-    }
-
-    public void testWithMultipleCommands() throws Exception {
-        createIndex("test");
-        ensureGreen();
-
-        index("test", "typ1", "1", "field1", "prefix_abcd", "field2", "prefix_efgh");
-        index("test", "typ1", "2", "field1", "prefix_aacd", "field2", "prefix_eeeh");
-        index("test", "typ1", "3", "field1", "prefix_abbd", "field2", "prefix_efff");
-        index("test", "typ1", "4", "field1", "prefix_abcc", "field2", "prefix_eggg");
-        refresh();
-
-        Suggest suggest = searchSuggest(
-                termSuggestion("size1")
-                        .size(1).text("prefix_abcd").maxTermFreq(10).prefixLength(1).minDocFreq(0)
-                        .field("field1").suggestMode("always"),
-                termSuggestion("field2")
-                        .field("field2").text("prefix_eeeh prefix_efgh")
-                        .maxTermFreq(10).minDocFreq(0).suggestMode("always"),
-                termSuggestion("accuracy")
-                        .field("field2").text("prefix_efgh").setAccuracy(1f)
-                        .maxTermFreq(10).minDocFreq(0).suggestMode("always"));
-        assertSuggestion(suggest, 0, "size1", "prefix_aacd");
-        assertThat(suggest.getSuggestion("field2").getEntries().get(0).getText().string(), equalTo("prefix_eeeh"));
-        assertSuggestion(suggest, 0, "field2", "prefix_efgh");
-        assertThat(suggest.getSuggestion("field2").getEntries().get(1).getText().string(), equalTo("prefix_efgh"));
-        assertSuggestion(suggest, 1, "field2", "prefix_eeeh", "prefix_efff", "prefix_eggg");
-        assertSuggestionSize(suggest, 0, 0, "accuracy");
-    }
-
-    public void testSizeAndSort() throws Exception {
-        createIndex("test");
-        ensureGreen();
-
-        Map<String, Integer> termsAndDocCount = new HashMap<>();
-        termsAndDocCount.put("prefix_aaad", 20);
-        termsAndDocCount.put("prefix_abbb", 18);
-        termsAndDocCount.put("prefix_aaca", 16);
-        termsAndDocCount.put("prefix_abba", 14);
-        termsAndDocCount.put("prefix_accc", 12);
-        termsAndDocCount.put("prefix_addd", 10);
-        termsAndDocCount.put("prefix_abaa", 8);
-        termsAndDocCount.put("prefix_dbca", 6);
-        termsAndDocCount.put("prefix_cbad", 4);
-        termsAndDocCount.put("prefix_aacd", 1);
-        termsAndDocCount.put("prefix_abcc", 1);
-        termsAndDocCount.put("prefix_accd", 1);
-
-        for (Map.Entry<String, Integer> entry : termsAndDocCount.entrySet()) {
-            for (int i = 0; i < entry.getValue(); i++) {
-                index("test", "type1", entry.getKey() + i, "field1", entry.getKey());
-            }
-        }
-        refresh();
-
-        Suggest suggest = searchSuggest( "prefix_abcd",
-                termSuggestion("size3SortScoreFirst")
-                        .size(3).minDocFreq(0).field("field1").suggestMode("always"),
-                termSuggestion("size10SortScoreFirst")
-                        .size(10).minDocFreq(0).field("field1").suggestMode("always").shardSize(50),
-                termSuggestion("size3SortScoreFirstMaxEdits1")
-                        .maxEdits(1)
-                        .size(10).minDocFreq(0).field("field1").suggestMode("always"),
-                termSuggestion("size10SortFrequencyFirst")
-                        .size(10).sort("frequency").shardSize(1000)
-                        .minDocFreq(0).field("field1").suggestMode("always"));
-
-        // The commented out assertions fail sometimes because suggestions are based off of shard frequencies instead of index frequencies.
-        assertSuggestion(suggest, 0, "size3SortScoreFirst", "prefix_aacd", "prefix_abcc", "prefix_accd");
-        assertSuggestion(suggest, 0, "size10SortScoreFirst", 10, "prefix_aacd", "prefix_abcc", "prefix_accd" /*, "prefix_aaad" */);
-        assertSuggestion(suggest, 0, "size3SortScoreFirstMaxEdits1", "prefix_aacd", "prefix_abcc", "prefix_accd");
-        assertSuggestion(suggest, 0, "size10SortFrequencyFirst", "prefix_aaad", "prefix_abbb", "prefix_aaca", "prefix_abba",
-                "prefix_accc", "prefix_addd", "prefix_abaa", "prefix_dbca", "prefix_cbad", "prefix_aacd");
-
-        // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_abcc"));
-        // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_accd"));
-    }
-
-    // see #2817
-    public void testStopwordsOnlyPhraseSuggest() throws IOException {
-        assertAcked(prepareCreate("test").addMapping("typ1", "body", "type=string,analyzer=stopwd").setSettings(
-                settingsBuilder()
-                        .put("index.analysis.analyzer.stopwd.tokenizer", "whitespace")
-                        .putArray("index.analysis.analyzer.stopwd.filter", "stop")
-        ));
-        ensureGreen();
-        index("test", "typ1", "1", "body", "this is a test");
-        refresh();
-
-        Suggest searchSuggest = searchSuggest( "a an the",
-                phraseSuggestion("simple_phrase").field("body").gramSize(1)
-                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always"))
-                        .size(1));
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-    }
-
-    public void testPrefixLength() throws IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase")
-                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", false)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
-                .startObject("properties")
-                .startObject("body").field("type", "string").field("analyzer", "body").endObject()
-                .startObject("body_reverse").field("type", "string").field("analyzer", "reverse").endObject()
-                .startObject("bigram").field("type", "string").field("analyzer", "bigram").endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        index("test", "type1", "1", "body", "hello world");
-        index("test", "type1", "2", "body", "hello world");
-        index("test", "type1", "3", "body", "hello words");
-        refresh();
-
-        Suggest searchSuggest = searchSuggest( "hello word",
-                phraseSuggestion("simple_phrase").field("body")
-                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(4).minWordLength(1).suggestMode("always"))
-                        .size(1).confidence(1.0f));
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "hello words");
-
-        searchSuggest = searchSuggest( "hello word",
-                phraseSuggestion("simple_phrase").field("body")
-                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(2).minWordLength(1).suggestMode("always"))
-                        .size(1).confidence(1.0f));
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "hello world");
-    }
-
-    @Nightly
-    public void testMarvelHerosPhraseSuggest() throws IOException, URISyntaxException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase")
-                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", false)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
-                    .startObject("_all")
-                        .field("store", "yes")
-                        .field("termVector", "with_positions_offsets")
-                    .endObject()
-                    .startObject("properties")
-                        .startObject("body").
-                            field("type", "string").
-                            field("analyzer", "body")
-                        .endObject()
-                        .startObject("body_reverse").
-                            field("type", "string").
-                            field("analyzer", "reverse")
-                         .endObject()
-                         .startObject("bigram").
-                             field("type", "string").
-                             field("analyzer", "bigram")
-                         .endObject()
-                     .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        for (String line : readMarvelHeroNames()) {
-            index("test", "type1", line, "body", line, "body_reverse", line, "bigram", line);
-        }
-        refresh();
-
-        PhraseSuggestionBuilder phraseSuggest = phraseSuggestion("simple_phrase")
-                .field("bigram").gramSize(2).analyzer("body")
-                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
-                .size(1);
-        Suggest searchSuggest = searchSuggest( "american ame", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
-        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("american ame"));
-
-        phraseSuggest.realWordErrorLikelihood(0.95f);
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-        // Check the "text" field this one time.
-        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel"));
-
-        // Ask for highlighting
-        phraseSuggest.highlight("<em>", "</em>");
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getOptions().get(0).getHighlighted().string(), equalTo("<em>xorr</em> the <em>god</em> jewel"));
-
-        // pass in a correct phrase
-        phraseSuggest.highlight(null, null).confidence(0f).size(1).maxErrors(0.5f);
-        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        // pass in a correct phrase - set confidence to 2
-        phraseSuggest.confidence(2f);
-        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        // pass in a correct phrase - set confidence to 0.99
-        phraseSuggest.confidence(0.99f);
-        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        //test reverse suggestions with pre & post filter
-        phraseSuggest
-            .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
-            .addCandidateGenerator(candidateGenerator("body_reverse").minWordLength(1).suggestMode("always").preFilter("reverse").postFilter("reverse"));
-        searchSuggest = searchSuggest( "xor the yod-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        // set all mass to trigrams (not indexed)
-        phraseSuggest.clearCandidateGenerators()
-            .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
-            .smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(1,0,0));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        // set all mass to bigrams
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0,1,0));
-        searchSuggest =  searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        // distribute mass
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        searchSuggest = searchSuggest( "american ame", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
-
-        // try all smoothing methods
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.Laplace(0.2));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-
-        // check tokenLimit
-        phraseSuggest.smoothingModel(null).tokenLimit(4);
-        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        phraseSuggest.tokenLimit(15).smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1));
-        searchSuggest = searchSuggest( "Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel", phraseSuggest);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel xorr the god jewel xorr the god jewel");
-        // Check the name this time because we're repeating it which is funky
-        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel"));
-    }
-    
-    private List<String> readMarvelHeroNames() throws IOException, URISyntaxException {
-        return Files.readAllLines(PathUtils.get(SuggestSearchIT.class.getResource("/config/names.txt").toURI()), StandardCharsets.UTF_8);
-    }
-
-    public void testSizePararm() throws IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(SETTING_NUMBER_OF_SHARDS, 1)
-                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase")
-                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", false)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("type1")
-                        .startObject("_all")
-                            .field("store", "yes")
-                            .field("termVector", "with_positions_offsets")
-                        .endObject()
-                        .startObject("properties")
-                            .startObject("body")
-                                .field("type", "string")
-                                .field("analyzer", "body")
-                            .endObject()
-                         .startObject("body_reverse")
-                             .field("type", "string")
-                             .field("analyzer", "reverse")
-                         .endObject()
-                         .startObject("bigram")
-                             .field("type", "string")
-                             .field("analyzer", "bigram")
-                         .endObject()
-                     .endObject()
-                 .endObject()
-             .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        String line = "xorr the god jewel";
-        index("test", "type1", "1", "body", line, "body_reverse", line, "bigram", line);
-        line = "I got it this time";
-        index("test", "type1", "2", "body", line, "body_reverse", line, "bigram", line);
-        refresh();
-
-        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("simple_phrase")
-                .realWordErrorLikelihood(0.95f)
-                .field("bigram")
-                .gramSize(2)
-                .analyzer("body")
-                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).prefixLength(1).suggestMode("always").size(1).accuracy(0.1f))
-                .smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1))
-                .maxErrors(1.0f)
-                .size(5);
-        Suggest searchSuggest = searchSuggest( "Xorr the Gut-Jewel", phraseSuggestion);
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        // we allow a size of 2 now on the shard generator level so "god" will be found since it's LD2
-        phraseSuggestion.clearCandidateGenerators()
-                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).prefixLength(1).suggestMode("always").size(2).accuracy(0.1f));
-        searchSuggest = searchSuggest( "Xorr the Gut-Jewel", phraseSuggestion);
-        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
-    }
-
-    @Nightly
-    public void testPhraseBoundaryCases() throws IOException, URISyntaxException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings()).put(SETTING_NUMBER_OF_SHARDS, 1) // to get reliable statistics we should put this all into one shard
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase")
-                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
-                .put("index.analysis.analyzer.ngram.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.ngram.filter", "my_shingle2", "lowercase")
-                .put("index.analysis.analyzer.myDefAnalyzer.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.myDefAnalyzer.filter", "shingle", "lowercase")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", false)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle2.type", "shingle")
-                .put("index.analysis.filter.my_shingle2.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle2.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle2.max_shingle_size", 2));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                    .startObject().startObject("type1")
-                    .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
-                .startObject("properties")
-                .startObject("body").field("type", "string").field("analyzer", "body").endObject()
-                .startObject("bigram").field("type", "string").field("analyzer", "bigram").endObject()
-                .startObject("ngram").field("type", "string").field("analyzer", "ngram").endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        for (String line : readMarvelHeroNames()) {
-            index("test", "type1", line, "body", line, "bigram", line, "ngram", line);
-        }
-        refresh();
-
-        NumShards numShards = getNumShards("test");
-
-        // Lets make sure some things throw exceptions
-        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("simple_phrase")
-                .field("bigram")
-                .analyzer("body")
-                .addCandidateGenerator(candidateGenerator("does_not_exist").minWordLength(1).suggestMode("always"))
-                .realWordErrorLikelihood(0.95f)
-                .maxErrors(0.5f)
-                .size(1);
-        try {
-            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
-            fail("field does not exists");
-        } catch (SearchPhaseExecutionException e) {}
-
-        phraseSuggestion.clearCandidateGenerators().analyzer(null);
-        try {
-            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
-            fail("analyzer does only produce ngrams");
-        } catch (SearchPhaseExecutionException e) {
-        }
-
-        phraseSuggestion.analyzer("bigram");
-        try {
-            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
-            fail("analyzer does only produce ngrams");
-        } catch (SearchPhaseExecutionException e) {
-        }
-
-        // Now we'll make sure some things don't
-        phraseSuggestion.forceUnigrams(false);
-        searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
-
-        // Field doesn't produce unigrams but the analyzer does
-        phraseSuggestion.forceUnigrams(true).field("bigram").analyzer("ngram");
-        searchSuggest( "Xor the Got-Jewel",
-                phraseSuggestion);
-
-        phraseSuggestion.field("ngram").analyzer("myDefAnalyzer")
-                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"));
-        Suggest suggest = searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
-
-        // "xorr the god jewel" and and "xorn the god jewel" have identical scores (we are only using unigrams to score), so we tie break by
-        // earlier term (xorn):
-        assertSuggestion(suggest, 0, "simple_phrase", "xorn the god jewel");
-
-        phraseSuggestion.analyzer(null);
-        suggest = searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
-
-        // In this case xorr has a better score than xorn because we set the field back to the default (my_shingle2) analyzer, so the
-        // probability that the term is not in the dictionary but is NOT a misspelling is relatively high in this case compared to the
-        // others that have no n-gram with the other terms in the phrase :) you can set this realWorldErrorLikelyhood
-        assertSuggestion(suggest, 0, "simple_phrase", "xorr the god jewel");
-    }
-
-    public void testDifferentShardSize() throws Exception {
-        createIndex("test");
-        ensureGreen();
-        indexRandom(true, client().prepareIndex("test", "type1", "1").setSource("field1", "foobar1").setRouting("1"),
-                client().prepareIndex("test", "type1", "2").setSource("field1", "foobar2").setRouting("2"),
-                client().prepareIndex("test", "type1", "3").setSource("field1", "foobar3").setRouting("3"));
-
-        Suggest suggest = searchSuggest( "foobar",
-                termSuggestion("simple")
-                        .size(10).minDocFreq(0).field("field1").suggestMode("always"));
-        ElasticsearchAssertions.assertSuggestionSize(suggest, 0, 3, "simple");
-    }
-
-    // see #3469
-    public void testShardFailures() throws IOException, InterruptedException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.suggest.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.suggest.filter", "standard", "lowercase", "shingler")
-                .put("index.analysis.filter.shingler.type", "shingle")
-                .put("index.analysis.filter.shingler.min_shingle_size", 2)
-                .put("index.analysis.filter.shingler.max_shingle_size", 5)
-                .put("index.analysis.filter.shingler.output_unigrams", true));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type2")
-                .startObject("properties")
-                    .startObject("name")
-                        .field("type", "multi_field")
-                        .startObject("fields")
-                            .startObject("name")
-                                .field("type", "string")
-                                .field("analyzer", "suggest")
-                            .endObject()
-                        .endObject()
-                    .endObject()
-                .endObject()
-                .endObject().endObject();
-        assertAcked(builder.addMapping("type2", mapping));
-        ensureGreen();
-
-        index("test", "type2", "1", "foo", "bar");
-        index("test", "type2", "2", "foo", "bar");
-        index("test", "type2", "3", "foo", "bar");
-        index("test", "type2", "4", "foo", "bar");
-        index("test", "type2", "5", "foo", "bar");
-        index("test", "type2", "1", "name", "Just testing the suggestions api");
-        index("test", "type2", "2", "name", "An other title about equal length");
-        // Note that the last document has to have about the same length as the other or cutoff rechecking will remove the useful suggestion.
-        refresh();
-
-        // When searching on a shard with a non existing mapping, we should fail
-        SearchRequestBuilder request = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f)));
-        assertThrows(request, SearchPhaseExecutionException.class);
-
-        // When searching on a shard which does not hold yet any document of an existing type, we should not fail
-        SearchResponse searchResponse = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
-            .get();
-        ElasticsearchAssertions.assertNoFailures(searchResponse);
-        ElasticsearchAssertions.assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
-    }
-
-    // see #3469
-    public void testEmptyShards() throws IOException, InterruptedException {
-        XContentBuilder mappingBuilder = XContentFactory.jsonBuilder().
-                startObject().
-                    startObject("type1").
-                        startObject("properties").
-                            startObject("name").
-                                field("type", "multi_field").
-                                startObject("fields").
-                                    startObject("name").
-                                        field("type", "string").
-                                        field("analyzer", "suggest").
-                                    endObject().
-                                endObject().
-                            endObject().
-                        endObject().
-                    endObject().
-                endObject();
-        assertAcked(prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.suggest.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.suggest.filter", "standard", "lowercase", "shingler")
-                .put("index.analysis.filter.shingler.type", "shingle")
-                .put("index.analysis.filter.shingler.min_shingle_size", 2)
-                .put("index.analysis.filter.shingler.max_shingle_size", 5)
-                .put("index.analysis.filter.shingler.output_unigrams", true)).addMapping("type1", mappingBuilder));
-        ensureGreen();
-
-        index("test", "type2", "1", "foo", "bar");
-        index("test", "type2", "2", "foo", "bar");
-        index("test", "type1", "1", "name", "Just testing the suggestions api");
-        index("test", "type1", "2", "name", "An other title about equal length");
-        refresh();
-
-        SearchResponse searchResponse = client().prepareSearch()
-                .setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
-                .get();
-
-        assertNoFailures(searchResponse);
-        assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
-    }
-
-    /**
-     * Searching for a rare phrase shouldn't provide any suggestions if confidence &gt; 1.  This was possible before we rechecked the cutoff
-     * score during the reduce phase.  Failures don't occur every time - maybe two out of five tries but we don't repeat it to save time.
-     */
-    public void testSearchForRarePhrase() throws IOException {
-        // If there isn't enough chaf per shard then shards can become unbalanced, making the cutoff recheck this is testing do more harm then good.
-        int chafPerShard = 100;
-
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put("index.analysis.analyzer.body.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.body.filter", "lowercase", "my_shingle")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("type1")
-                        .startObject("_all")
-                            .field("store", "yes")
-                            .field("termVector", "with_positions_offsets")
-                        .endObject()
-                        .startObject("properties")
-                            .startObject("body")
-                                .field("type", "string")
-                                .field("analyzer", "body")
-                            .endObject()
-                        .endObject()
-                    .endObject()
-                .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        NumShards test = getNumShards("test");
-
-        List<String> phrases = new ArrayList<>();
-        Collections.addAll(phrases, "nobel prize", "noble gases", "somethingelse prize", "pride and joy", "notes are fun");
-        for (int i = 0; i < 8; i++) {
-            phrases.add("noble somethingelse" + i);
-        }
-        for (int i = 0; i < test.numPrimaries * chafPerShard; i++) {
-            phrases.add("chaff" + i);
-        }
-        for (String phrase: phrases) {
-            index("test", "type1", phrase, "body", phrase);
-        }
-        refresh();
-
-        Suggest searchSuggest = searchSuggest("nobel prize", phraseSuggestion("simple_phrase")
-                .field("body")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always").maxTermFreq(.99f))
-                .confidence(2f)
-                .maxErrors(5f)
-                .size(1));
-        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
-
-        searchSuggest = searchSuggest("noble prize", phraseSuggestion("simple_phrase")
-                .field("body")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always").maxTermFreq(.99f))
-                .confidence(2f)
-                .maxErrors(5f)
-                .size(1));
-        assertSuggestion(searchSuggest, 0, 0, "simple_phrase", "nobel prize");
-    }
-
-    @Nightly
-    public void testSuggestWithManyCandidates() throws InterruptedException, ExecutionException, IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
-                .put("index.analysis.analyzer.text.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("type1")
-                        .startObject("properties")
-                            .startObject("title")
-                                .field("type", "string")
-                                .field("analyzer", "text")
-                            .endObject()
-                        .endObject()
-                    .endObject()
-                .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        List<String> titles = new ArrayList<>();
-
-        // We're going to be searching for:
-        //   united states house of representatives elections in washington 2006
-        // But we need to make sure we generate a ton of suggestions so we add a bunch of candidates.
-        // Many of these candidates are drawn from page names on English Wikipedia.
-
-        // Tons of different options very near the exact query term
-        titles.add("United States House of Representatives Elections in Washington 1789");
-        for (int year = 1790; year < 2014; year+= 2) {
-            titles.add("United States House of Representatives Elections in Washington " + year);
-        }
-        // Six of these are near enough to be viable suggestions, just not the top one
-
-        // But we can't stop there!  Titles that are just a year are pretty common so lets just add one per year
-        // since 0.  Why not?
-        for (int year = 0; year < 2015; year++) {
-            titles.add(Integer.toString(year));
-        }
-        // That ought to provide more less good candidates for the last term
-
-        // Now remove or add plural copies of every term we can
-        titles.add("State");
-        titles.add("Houses of Parliament");
-        titles.add("Representative Government");
-        titles.add("Election");
-
-        // Now some possessive
-        titles.add("Washington's Birthday");
-
-        // And some conjugation
-        titles.add("Unified Modeling Language");
-        titles.add("Unite Against Fascism");
-        titles.add("Stated Income Tax");
-        titles.add("Media organizations housed within colleges");
-
-        // And other stuff
-        titles.add("Untied shoelaces");
-        titles.add("Unit circle");
-        titles.add("Untitled");
-        titles.add("Unicef");
-        titles.add("Unrated");
-        titles.add("UniRed");
-        titles.add("Jalan Uniten–Dengkil"); // Highway in Malaysia
-        titles.add("UNITAS");
-        titles.add("UNITER");
-        titles.add("Un-Led-Ed");
-        titles.add("STATS LLC");
-        titles.add("Staples");
-        titles.add("Skates");
-        titles.add("Statues of the Liberators");
-        titles.add("Staten Island");
-        titles.add("Statens Museum for Kunst");
-        titles.add("Hause"); // The last name or the German word, whichever.
-        titles.add("Hose");
-        titles.add("Hoses");
-        titles.add("Howse Peak");
-        titles.add("The Hoose-Gow");
-        titles.add("Hooser");
-        titles.add("Electron");
-        titles.add("Electors");
-        titles.add("Evictions");
-        titles.add("Coronal mass ejection");
-        titles.add("Wasington"); // A film?
-        titles.add("Warrington"); // A town in England
-        titles.add("Waddington"); // Lots of places have this name
-        titles.add("Watlington"); // Ditto
-        titles.add("Waplington"); // Yup, also a town
-        titles.add("Washing of the Spears"); // Book
-
-        for (char c = 'A'; c <= 'Z'; c++) {
-            // Can't forget lists, glorious lists!
-            titles.add("List of former members of the United States House of Representatives (" + c + ")");
-
-            // Lots of people are named Washington <Middle Initial>. LastName
-            titles.add("Washington " + c + ". Lastname");
-
-            // Lets just add some more to be evil
-            titles.add("United " + c);
-            titles.add("States " + c);
-            titles.add("House " + c);
-            titles.add("Elections " + c);
-            titles.add("2006 " + c);
-            titles.add(c + " United");
-            titles.add(c + " States");
-            titles.add(c + " House");
-            titles.add(c + " Elections");
-            titles.add(c + " 2006");
-        }
-
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        for (String title: titles) {
-            builders.add(client().prepareIndex("test", "type1").setSource("title", title));
-        }
-        indexRandom(true, builders);
-
-        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
-                .field("title")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
-                        .suggestMode("always")
-                        .maxTermFreq(.99f)
-                        .size(1000) // Setting a silly high size helps of generate a larger list of candidates for testing.
-                        .maxInspections(1000) // This too
-                )
-                .confidence(0f)
-                .maxErrors(2f)
-                .shardSize(30000)
-                .size(30000);
-        Suggest searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
-        assertSuggestion(searchSuggest, 0, 0, "title", "united states house of representatives elections in washington 2006");
-        assertSuggestionSize(searchSuggest, 0, 25480, "title");  // Just to prove that we've run through a ton of options
-
-        suggest.size(1);
-        long start = System.currentTimeMillis();
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
-        long total = System.currentTimeMillis() - start;
-        assertSuggestion(searchSuggest, 0, 0, "title", "united states house of representatives elections in washington 2006");
-        // assertThat(total, lessThan(1000L)); // Takes many seconds without fix - just for debugging
-    }
-
-    public void testPhraseSuggesterCollate() throws InterruptedException, ExecutionException, IOException {
-        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
-                .put("index.analysis.analyzer.text.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("type1")
-                .startObject("properties")
-                .startObject("title")
-                .field("type", "string")
-                .field("analyzer", "text")
-                .endObject()
-                .endObject()
-                .endObject()
-                .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        List<String> titles = new ArrayList<>();
-
-        titles.add("United States House of Representatives Elections in Washington 2006");
-        titles.add("United States House of Representatives Elections in Washington 2005");
-        titles.add("State");
-        titles.add("Houses of Parliament");
-        titles.add("Representative Government");
-        titles.add("Election");
-
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        for (String title: titles) {
-            builders.add(client().prepareIndex("test", "type1").setSource("title", title));
-        }
-        indexRandom(true, builders);
-
-        // suggest without collate
-        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
-                .field("title")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
-                        .suggestMode("always")
-                        .maxTermFreq(.99f)
-                        .size(10)
-                        .maxInspections(200)
-                )
-                .confidence(0f)
-                .maxErrors(2f)
-                .shardSize(30000)
-                .size(10);
-        Suggest searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
-        assertSuggestionSize(searchSuggest, 0, 10, "title");
-
-        // suggest with collate
-        String filterString = XContentFactory.jsonBuilder()
-                    .startObject()
-                        .startObject("match_phrase")
-                            .field("title", "{{suggestion}}")
-                        .endObject()
-                    .endObject()
-                .string();
-        PhraseSuggestionBuilder filteredQuerySuggest = suggest.collateQuery(filterString);
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", filteredQuerySuggest);
-        assertSuggestionSize(searchSuggest, 0, 2, "title");
-
-        // collate suggest with no result (boundary case)
-        searchSuggest = searchSuggest("Elections of Representatives Parliament", filteredQuerySuggest);
-        assertSuggestionSize(searchSuggest, 0, 0, "title");
-
-        NumShards numShards = getNumShards("test");
-
-        // collate suggest with bad query
-        String incorrectFilterString = XContentFactory.jsonBuilder()
-                .startObject()
-                    .startObject("test")
-                        .field("title", "{{suggestion}}")
-                    .endObject()
-                .endObject()
-                .string();
-        PhraseSuggestionBuilder incorrectFilteredSuggest = suggest.collateQuery(incorrectFilterString);
-        try {
-            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, incorrectFilteredSuggest);
-            fail("Post query error has been swallowed");
-        } catch(ElasticsearchException e) {
-            // expected
-        }
-
-        // suggest with collation
-        String filterStringAsFilter = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("match_phrase")
-                .field("title", "{{suggestion}}")
-                .endObject()
-                .endObject()
-                .string();
-
-        PhraseSuggestionBuilder filteredFilterSuggest = suggest.collateQuery(filterStringAsFilter);
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", filteredFilterSuggest);
-        assertSuggestionSize(searchSuggest, 0, 2, "title");
-
-        // collate suggest with bad query
-        String filterStr = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("pprefix")
-                        .field("title", "{{suggestion}}")
-                .endObject()
-                .endObject()
-                .string();
-
-        PhraseSuggestionBuilder in = suggest.collateQuery(filterStr);
-        try {
-            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, in);
-            fail("Post filter error has been swallowed");
-        } catch(ElasticsearchException e) {
-            //expected
-        }
-
-        // collate script failure due to no additional params
-        String collateWithParams = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("{{query_type}}")
-                    .field("{{query_field}}", "{{suggestion}}")
-                .endObject()
-                .endObject()
-                .string();
-
-
-        PhraseSuggestionBuilder phraseSuggestWithNoParams = suggest.collateQuery(collateWithParams);
-        try {
-            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, phraseSuggestWithNoParams);
-            fail("Malformed query (lack of additional params) should fail");
-        } catch (ElasticsearchException e) {
-            // expected
-        }
-
-        // collate script with additional params
-        Map<String, Object> params = new HashMap<>();
-        params.put("query_type", "match_phrase");
-        params.put("query_field", "title");
-
-        PhraseSuggestionBuilder phraseSuggestWithParams = suggest.collateQuery(collateWithParams).collateParams(params);
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", phraseSuggestWithParams);
-        assertSuggestionSize(searchSuggest, 0, 2, "title");
-
-        // collate query request with prune set to true
-        PhraseSuggestionBuilder phraseSuggestWithParamsAndReturn = suggest.collateQuery(collateWithParams).collateParams(params).collatePrune(true);
-        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", phraseSuggestWithParamsAndReturn);
-        assertSuggestionSize(searchSuggest, 0, 10, "title");
-        assertSuggestionPhraseCollateMatchExists(searchSuggest, "title", 2);
-    }
-
-    protected Suggest searchSuggest(SuggestionBuilder<?>... suggestion) {
-        return searchSuggest(null, suggestion);
-    }
-
-    protected Suggest searchSuggest(String suggestText, SuggestionBuilder<?>... suggestions) {
-        return searchSuggest(suggestText, 0, suggestions);
-    }
-
-    protected Suggest searchSuggest(String suggestText, int expectShardsFailed, SuggestionBuilder<?>... suggestions) {
-        if (randomBoolean()) {
-            SearchRequestBuilder builder = client().prepareSearch().setSize(0);
-            SuggestBuilder suggestBuilder = new SuggestBuilder();
-            if (suggestText != null) {
-                suggestBuilder.setText(suggestText);
-            }
-            for (SuggestionBuilder<?> suggestion : suggestions) {
-                suggestBuilder.addSuggestion(suggestion);
-            }
-            builder.suggest(suggestBuilder);
-            SearchResponse actionGet = builder.execute().actionGet();
-            assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
-            return actionGet.getSuggest();
-        } else {
-            SuggestRequestBuilder builder = client().prepareSuggest();
-            if (suggestText != null) {
-                builder.setSuggestText(suggestText);
-            }
-            for (SuggestionBuilder<?> suggestion : suggestions) {
-                builder.addSuggestion(suggestion);
-            }
-
-            SuggestResponse actionGet = builder.execute().actionGet();
-            assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
-            if (expectShardsFailed > 0) {
-                throw new SearchPhaseExecutionException("suggest", "Suggest execution failed", new ShardSearchFailure[0]);
-            }
-            return actionGet.getSuggest();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java b/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
index 8fde9bb..51ae038 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotIntegTestCase.java
@@ -56,7 +56,7 @@ public abstract class AbstractSnapshotIntegTestCase extends ESIntegTestCase {
         return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
             // Rebalancing is causing some checks after restore to randomly fail
             // due to https://github.com/elastic/elasticsearch/issues/9421
-            .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), EnableAllocationDecider.Rebalance.NONE)
+            .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, EnableAllocationDecider.Rebalance.NONE)
             .build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
index 9133828..f939283 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java
@@ -123,14 +123,14 @@ public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTest
         logger.info("--> set test persistent setting");
         client.admin().cluster().prepareUpdateSettings().setPersistentSettings(
                 Settings.settingsBuilder()
-                        .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2)
-                        .put(IndicesTTLService.INDICES_TTL_INTERVAL_SETTING.getKey(), random, TimeUnit.MINUTES))
+                        .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 2)
+                        .put(IndicesTTLService.INDICES_TTL_INTERVAL, random, TimeUnit.MINUTES))
                 .execute().actionGet();
 
         assertThat(client.admin().cluster().prepareState().setRoutingTable(false).setNodes(false).execute().actionGet().getState()
-                .getMetaData().persistentSettings().getAsTime(IndicesTTLService.INDICES_TTL_INTERVAL_SETTING.getKey(), TimeValue.timeValueMinutes(1)).millis(), equalTo(TimeValue.timeValueMinutes(random).millis()));
+                .getMetaData().persistentSettings().getAsTime(IndicesTTLService.INDICES_TTL_INTERVAL, TimeValue.timeValueMinutes(1)).millis(), equalTo(TimeValue.timeValueMinutes(random).millis()));
         assertThat(client.admin().cluster().prepareState().setRoutingTable(false).setNodes(false).execute().actionGet().getState()
-                .getMetaData().persistentSettings().getAsInt(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), -1), equalTo(2));
+                .getMetaData().persistentSettings().getAsInt(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, -1), equalTo(2));
 
         logger.info("--> create repository");
         PutRepositoryResponse putRepositoryResponse = client.admin().cluster().preparePutRepository("test-repo")
@@ -146,26 +146,23 @@ public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTest
         logger.info("--> clean the test persistent setting");
         client.admin().cluster().prepareUpdateSettings().setPersistentSettings(
                 Settings.settingsBuilder()
-                        .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 1)
-                        .put(IndicesTTLService.INDICES_TTL_INTERVAL_SETTING.getKey(), TimeValue.timeValueMinutes(1)))
+                        .put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, 1)
+                        .put(IndicesTTLService.INDICES_TTL_INTERVAL, TimeValue.timeValueMinutes(1)))
                 .execute().actionGet();
         assertThat(client.admin().cluster().prepareState().setRoutingTable(false).setNodes(false).execute().actionGet().getState()
-                .getMetaData().persistentSettings().getAsTime(IndicesTTLService.INDICES_TTL_INTERVAL_SETTING.getKey(), TimeValue.timeValueMinutes(1)).millis(), equalTo(TimeValue.timeValueMinutes(1).millis()));
+                .getMetaData().persistentSettings().getAsTime(IndicesTTLService.INDICES_TTL_INTERVAL, TimeValue.timeValueMinutes(1)).millis(), equalTo(TimeValue.timeValueMinutes(1).millis()));
 
         stopNode(secondNode);
         assertThat(client.admin().cluster().prepareHealth().setWaitForNodes("1").get().isTimedOut(), equalTo(false));
 
         logger.info("--> restore snapshot");
-        try {
-            client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setRestoreGlobalState(true).setWaitForCompletion(true).execute().actionGet();
-            fail("can't restore minimum master nodes");
-        } catch (IllegalArgumentException ex) {
-            assertEquals("illegal value can't update [discovery.zen.minimum_master_nodes] from [1] to [2]", ex.getMessage());
-            assertEquals("cannot set discovery.zen.minimum_master_nodes to more than the current master nodes count [1]", ex.getCause().getMessage());
-        }
+        client.admin().cluster().prepareRestoreSnapshot("test-repo", "test-snap").setRestoreGlobalState(true).setWaitForCompletion(true).execute().actionGet();
+        assertThat(client.admin().cluster().prepareState().setRoutingTable(false).setNodes(false).execute().actionGet().getState()
+                .getMetaData().persistentSettings().getAsTime(IndicesTTLService.INDICES_TTL_INTERVAL, TimeValue.timeValueMinutes(1)).millis(), equalTo(TimeValue.timeValueMinutes(random).millis()));
+
         logger.info("--> ensure that zen discovery minimum master nodes wasn't restored");
         assertThat(client.admin().cluster().prepareState().setRoutingTable(false).setNodes(false).execute().actionGet().getState()
-                .getMetaData().persistentSettings().getAsInt(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), -1), not(equalTo(2)));
+                .getMetaData().persistentSettings().getAsInt(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, -1), not(equalTo(2)));
     }
 
     public void testRestoreCustomMetadata() throws Exception {
@@ -557,7 +554,7 @@ public class DedicatedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTest
     public void testRestoreIndexWithShardsMissingInLocalGateway() throws Exception {
         logger.info("--> start 2 nodes");
         Settings nodeSettings = settingsBuilder()
-                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), EnableAllocationDecider.Rebalance.NONE)
+                .put(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, EnableAllocationDecider.Rebalance.NONE)
                 .build();
 
         internalCluster().startNode(nodeSettings);
diff --git a/core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java b/core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java
index 0bfc76f..b0de061 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/RepositoriesIT.java
@@ -94,7 +94,8 @@ public class RepositoriesIT extends AbstractSnapshotIntegTestCase {
         assertThat(repositoriesMetaData.repository("test-repo-2").type(), equalTo("fs"));
 
         logger.info("--> check that both repositories can be retrieved by getRepositories query");
-        GetRepositoriesResponse repositoriesResponse = client.admin().cluster().prepareGetRepositories().get();
+        GetRepositoriesResponse repositoriesResponse = client.admin().cluster()
+            .prepareGetRepositories(randomFrom("_all", "*", "test-repo-*")).get();
         assertThat(repositoriesResponse.repositories().size(), equalTo(2));
         assertThat(findRepository(repositoriesResponse.repositories(), "test-repo-1"), notNullValue());
         assertThat(findRepository(repositoriesResponse.repositories(), "test-repo-2"), notNullValue());
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index afbdf9d..57a22c0 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -149,7 +149,10 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
         assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), greaterThan(0));
         assertThat(createSnapshotResponse.getSnapshotInfo().successfulShards(), equalTo(createSnapshotResponse.getSnapshotInfo().totalShards()));
 
-        SnapshotInfo snapshotInfo = client.admin().cluster().prepareGetSnapshots("test-repo").setSnapshots("test-snap").get().getSnapshots().get(0);
+        List<SnapshotInfo> snapshotInfos = client.admin().cluster().prepareGetSnapshots("test-repo")
+            .setSnapshots(randomFrom("test-snap", "_all", "*", "*-snap", "test*")).get().getSnapshots();
+        assertThat(snapshotInfos.size(), equalTo(1));
+        SnapshotInfo snapshotInfo = snapshotInfos.get(0);
         assertThat(snapshotInfo.state(), equalTo(SnapshotState.SUCCESS));
         assertThat(snapshotInfo.version(), equalTo(Version.CURRENT));
 
diff --git a/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java b/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
index 838c2a6..60f1bad 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java
@@ -29,7 +29,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.node.NodeBuilder;
+import org.elasticsearch.node.Node;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import org.elasticsearch.test.ESIntegTestCase.Scope;
@@ -46,13 +46,20 @@ import java.lang.management.ThreadMXBean;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.*;
+import java.util.concurrent.BrokenBarrierException;
+import java.util.concurrent.CyclicBarrier;
+import java.util.concurrent.Executor;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
 import java.util.regex.Pattern;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.sameInstance;
 
 /**
  */
@@ -185,7 +192,7 @@ public class SimpleThreadPoolIT extends ESIntegTestCase {
                 .put("tribe.t1.plugin.mandatory", "non_existing").build();
 
         try {
-            NodeBuilder.nodeBuilder().settings(settings).build();
+            new Node(settings);
             fail("The node startup is supposed to fail");
         } catch(Throwable t) {
             //all good
diff --git a/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolTypeSettingsValidatorTests.java b/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolTypeSettingsValidatorTests.java
new file mode 100644
index 0000000..3dfca5c
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/threadpool/ThreadPoolTypeSettingsValidatorTests.java
@@ -0,0 +1,73 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed wit[√h
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.threadpool;
+
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.settings.Validator;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.*;
+
+import static org.junit.Assert.*;
+
+public class ThreadPoolTypeSettingsValidatorTests extends ESTestCase {
+    private Validator validator;
+
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        validator = ThreadPool.THREAD_POOL_TYPE_SETTINGS_VALIDATOR;
+    }
+
+    public void testValidThreadPoolTypeSettings() {
+        for (Map.Entry<String, ThreadPool.ThreadPoolType> entry : ThreadPool.THREAD_POOL_TYPES.entrySet()) {
+            assertNull(validateSetting(validator, entry.getKey(), entry.getValue().getType()));
+        }
+    }
+
+    public void testInvalidThreadPoolTypeSettings() {
+        for (Map.Entry<String, ThreadPool.ThreadPoolType> entry : ThreadPool.THREAD_POOL_TYPES.entrySet()) {
+            Set<ThreadPool.ThreadPoolType> set = new HashSet<>();
+            set.addAll(Arrays.asList(ThreadPool.ThreadPoolType.values()));
+            set.remove(entry.getValue());
+            ThreadPool.ThreadPoolType invalidThreadPoolType = randomFrom(set.toArray(new ThreadPool.ThreadPoolType[set.size()]));
+            String expectedMessage = String.format(
+                    Locale.ROOT,
+                    "thread pool type for [%s] can only be updated to [%s] but was [%s]",
+                    entry.getKey(),
+                    entry.getValue().getType(),
+                    invalidThreadPoolType.getType());
+            String message = validateSetting(validator, entry.getKey(), invalidThreadPoolType.getType());
+            assertNotNull(message);
+            assertEquals(expectedMessage, message);
+        }
+    }
+
+    public void testNonThreadPoolTypeSetting() {
+        String setting = ThreadPool.THREADPOOL_GROUP + randomAsciiOfLength(10) + "foo";
+        String value = randomAsciiOfLength(10);
+        assertNull(validator.validate(setting, value, ClusterState.PROTO));
+    }
+
+    private String validateSetting(Validator validator, String threadPoolName, String value) {
+        return validator.validate(ThreadPool.THREADPOOL_GROUP + threadPoolName + ".type", value, ClusterState.PROTO);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
index 56b2a03..95ceea1 100644
--- a/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
+++ b/core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.threadpool;
 
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor;
 import org.elasticsearch.test.ESTestCase;
@@ -91,19 +90,17 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         ThreadPool threadPool = null;
         try {
             threadPool = new ThreadPool(settingsBuilder().put("name", "testUpdateSettingsCanNotChangeThreadPoolType").build());
-            ClusterSettings clusterSettings = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-            threadPool.setClusterSettings(clusterSettings);
 
-            clusterSettings.applySettings(
+
+            threadPool.updateSettings(
                     settingsBuilder()
                             .put("threadpool." + threadPoolName + ".type", invalidThreadPoolType.getType())
                             .build()
             );
             fail("expected IllegalArgumentException");
         } catch (IllegalArgumentException e) {
-            assertEquals("illegal value can't update [threadpool.] from [{}] to [{" + threadPoolName + ".type=" + invalidThreadPoolType.getType() + "}]", e.getMessage());
             assertThat(
-                    e.getCause().getMessage(),
+                    e.getMessage(),
                     is("setting threadpool." + threadPoolName + ".type to " + invalidThreadPoolType.getType() + " is not permitted; must be " + validThreadPoolType.getType()));
         } finally {
             terminateThreadPoolIfNeeded(threadPool);
@@ -114,16 +111,14 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         String threadPoolName = randomThreadPool(ThreadPool.ThreadPoolType.CACHED);
         ThreadPool threadPool = null;
         try {
-            Settings nodeSettings = Settings.settingsBuilder()
-                    .put("name", "testCachedExecutorType").build();
-            threadPool = new ThreadPool(nodeSettings);
-            ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-            threadPool.setClusterSettings(clusterSettings);
+            threadPool = new ThreadPool(
+                    Settings.settingsBuilder()
+                            .put("name", "testCachedExecutorType").build());
 
             assertEquals(info(threadPool, threadPoolName).getThreadPoolType(), ThreadPool.ThreadPoolType.CACHED);
             assertThat(threadPool.executor(threadPoolName), instanceOf(EsThreadPoolExecutor.class));
 
-            Settings settings = clusterSettings.applySettings(settingsBuilder()
+            threadPool.updateSettings(settingsBuilder()
                     .put("threadpool." + threadPoolName + ".keep_alive", "10m")
                     .build());
             assertEquals(info(threadPool, threadPoolName).getThreadPoolType(), ThreadPool.ThreadPoolType.CACHED);
@@ -139,7 +134,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
 
             // Change keep alive
             Executor oldExecutor = threadPool.executor(threadPoolName);
-            settings = clusterSettings.applySettings(settingsBuilder().put(settings).put("threadpool." + threadPoolName + ".keep_alive", "1m").build());
+            threadPool.updateSettings(settingsBuilder().put("threadpool." + threadPoolName + ".keep_alive", "1m").build());
             // Make sure keep alive value changed
             assertThat(info(threadPool, threadPoolName).getKeepAlive().minutes(), equalTo(1L));
             assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getKeepAliveTime(TimeUnit.MINUTES), equalTo(1L));
@@ -148,7 +143,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
             assertThat(threadPool.executor(threadPoolName), sameInstance(oldExecutor));
 
             // Set the same keep alive
-            settings = clusterSettings.applySettings(settingsBuilder().put(settings).put("threadpool." + threadPoolName + ".keep_alive", "1m").build());
+            threadPool.updateSettings(settingsBuilder().put("threadpool." + threadPoolName + ".keep_alive", "1m").build());
             // Make sure keep alive value didn't change
             assertThat(info(threadPool, threadPoolName).getKeepAlive().minutes(), equalTo(1L));
             assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getKeepAliveTime(TimeUnit.MINUTES), equalTo(1L));
@@ -165,13 +160,11 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         ThreadPool threadPool = null;
 
         try {
-            Settings nodeSettings = Settings.settingsBuilder()
-                    .put("name", "testFixedExecutorType").build();
-            threadPool = new ThreadPool(nodeSettings);
-            ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-            threadPool.setClusterSettings(clusterSettings);
+            threadPool = new ThreadPool(settingsBuilder()
+                    .put("name", "testCachedExecutorType").build());
             assertThat(threadPool.executor(threadPoolName), instanceOf(EsThreadPoolExecutor.class));
-            Settings settings = clusterSettings.applySettings(settingsBuilder()
+
+            threadPool.updateSettings(settingsBuilder()
                     .put("threadpool." + threadPoolName + ".size", "15")
                     .build());
             assertEquals(info(threadPool, threadPoolName).getThreadPoolType(), ThreadPool.ThreadPoolType.FIXED);
@@ -184,7 +177,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
             assertThat(((EsThreadPoolExecutor) threadPool.executor(threadPoolName)).getKeepAliveTime(TimeUnit.MINUTES), equalTo(0L));
 
             // Put old type back
-            settings = clusterSettings.applySettings(Settings.EMPTY);
+            threadPool.updateSettings(Settings.EMPTY);
             assertEquals(info(threadPool, threadPoolName).getThreadPoolType(), ThreadPool.ThreadPoolType.FIXED);
             // Make sure keep alive value is not used
             assertThat(info(threadPool, threadPoolName).getKeepAlive(), nullValue());
@@ -197,7 +190,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
 
             // Change size
             Executor oldExecutor = threadPool.executor(threadPoolName);
-            settings = clusterSettings.applySettings(settingsBuilder().put(settings).put("threadpool." + threadPoolName + ".size", "10").build());
+            threadPool.updateSettings(settingsBuilder().put("threadpool." + threadPoolName + ".size", "10").build());
             // Make sure size values changed
             assertThat(info(threadPool, threadPoolName).getMax(), equalTo(10));
             assertThat(info(threadPool, threadPoolName).getMin(), equalTo(10));
@@ -208,7 +201,8 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
             assertThat(threadPool.executor(threadPoolName), sameInstance(oldExecutor));
 
             // Change queue capacity
-            settings = clusterSettings.applySettings(settingsBuilder().put(settings).put("threadpool." + threadPoolName + ".queue", "500")
+            threadPool.updateSettings(settingsBuilder()
+                    .put("threadpool." + threadPoolName + ".queue", "500")
                     .build());
         } finally {
             terminateThreadPoolIfNeeded(threadPool);
@@ -219,12 +213,9 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         String threadPoolName = randomThreadPool(ThreadPool.ThreadPoolType.SCALING);
         ThreadPool threadPool = null;
         try {
-            Settings nodeSettings = settingsBuilder()
+            threadPool = new ThreadPool(settingsBuilder()
                     .put("threadpool." + threadPoolName + ".size", 10)
-                    .put("name", "testScalingExecutorType").build();
-            threadPool = new ThreadPool(nodeSettings);
-            ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-            threadPool.setClusterSettings(clusterSettings);
+                    .put("name", "testCachedExecutorType").build());
             assertThat(info(threadPool, threadPoolName).getMin(), equalTo(1));
             assertThat(info(threadPool, threadPoolName).getMax(), equalTo(10));
             assertThat(info(threadPool, threadPoolName).getKeepAlive().minutes(), equalTo(5L));
@@ -233,7 +224,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
 
             // Change settings that doesn't require pool replacement
             Executor oldExecutor = threadPool.executor(threadPoolName);
-            clusterSettings.applySettings(settingsBuilder()
+            threadPool.updateSettings(settingsBuilder()
                     .put("threadpool." + threadPoolName + ".keep_alive", "10m")
                     .put("threadpool." + threadPoolName + ".min", "2")
                     .put("threadpool." + threadPoolName + ".size", "15")
@@ -257,12 +248,9 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
         String threadPoolName = randomThreadPool(ThreadPool.ThreadPoolType.FIXED);
         ThreadPool threadPool = null;
         try {
-            Settings nodeSettings = Settings.settingsBuilder()
+            threadPool = new ThreadPool(Settings.settingsBuilder()
                     .put("threadpool." + threadPoolName + ".queue_size", 1000)
-                    .put("name", "testCachedExecutorType").build();
-            threadPool = new ThreadPool(nodeSettings);
-            ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-            threadPool.setClusterSettings(clusterSettings);
+                    .put("name", "testCachedExecutorType").build());
             assertEquals(info(threadPool, threadPoolName).getQueueSize().getSingles(), 1000L);
 
             final CountDownLatch latch = new CountDownLatch(1);
@@ -276,7 +264,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
                         }
                     }
             );
-            clusterSettings.applySettings(settingsBuilder().put("threadpool." + threadPoolName + ".queue_size", 2000).build());
+            threadPool.updateSettings(settingsBuilder().put("threadpool." + threadPoolName + ".queue_size", 2000).build());
             assertThat(threadPool.executor(threadPoolName), not(sameInstance(oldExecutor)));
             assertThat(oldExecutor.isShutdown(), equalTo(true));
             assertThat(oldExecutor.isTerminating(), equalTo(true));
@@ -291,15 +279,12 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
     public void testCustomThreadPool() throws Exception {
         ThreadPool threadPool = null;
         try {
-            Settings nodeSettings = Settings.settingsBuilder()
+            threadPool = new ThreadPool(Settings.settingsBuilder()
                     .put("threadpool.my_pool1.type", "scaling")
                     .put("threadpool.my_pool2.type", "fixed")
                     .put("threadpool.my_pool2.size", "1")
                     .put("threadpool.my_pool2.queue_size", "1")
-                    .put("name", "testCustomThreadPool").build();
-            threadPool = new ThreadPool(nodeSettings);
-            ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-            threadPool.setClusterSettings(clusterSettings);
+                    .put("name", "testCustomThreadPool").build());
             ThreadPoolInfo groups = threadPool.info();
             boolean foundPool1 = false;
             boolean foundPool2 = false;
@@ -331,7 +316,7 @@ public class UpdateThreadPoolSettingsTests extends ESTestCase {
             Settings settings = Settings.builder()
                     .put("threadpool.my_pool2.size", "10")
                     .build();
-            clusterSettings.applySettings(settings);
+            threadPool.updateSettings(settings);
 
             groups = threadPool.info();
             foundPool1 = false;
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
index 6599412..becb616 100644
--- a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
+++ b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
@@ -21,7 +21,6 @@ package org.elasticsearch.transport;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -70,12 +69,12 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
         super.setUp();
         threadPool = new ThreadPool(getClass().getName());
         serviceA = build(
-                Settings.builder().put("name", "TS_A", TransportService.TRACE_LOG_INCLUDE_SETTING.getKey(), "", TransportService.TRACE_LOG_EXCLUDE_SETTING.getKey(), "NOTHING").build(),
+                Settings.builder().put("name", "TS_A", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
                 version0, new NamedWriteableRegistry()
         );
         nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), emptyMap(), version0);
         serviceB = build(
-                Settings.builder().put("name", "TS_B", TransportService.TRACE_LOG_INCLUDE_SETTING.getKey(), "", TransportService.TRACE_LOG_EXCLUDE_SETTING.getKey(), "NOTHING").build(),
+                Settings.builder().put("name", "TS_B", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
                 version1, new NamedWriteableRegistry()
         );
         nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), emptyMap(), version1);
@@ -651,10 +650,9 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
             includeSettings = "test";
             excludeSettings = "DOESN'T_MATCH";
         }
-        ClusterSettings service = new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);
-        serviceA.setDynamicSettings(service);
-        service.applySettings(Settings.builder()
-                .put(TransportService.TRACE_LOG_INCLUDE_SETTING.getKey(), includeSettings, TransportService.TRACE_LOG_EXCLUDE_SETTING.getKey(), excludeSettings)
+
+        serviceA.applySettings(Settings.builder()
+                .put(TransportService.SETTING_TRACE_LOG_INCLUDE, includeSettings, TransportService.SETTING_TRACE_LOG_EXCLUDE, excludeSettings)
                 .build());
 
         tracer.reset(4);
diff --git a/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java b/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
index 7a3fd88..3f140b3 100644
--- a/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/NettySizeHeaderFrameDecoderTests.java
@@ -21,7 +21,6 @@ package org.elasticsearch.transport;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.cache.recycler.MockPageCacheRecycler;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.network.NetworkService;
 import org.elasticsearch.common.settings.Settings;
@@ -29,6 +28,7 @@ import org.elasticsearch.common.transport.InetSocketTransportAddress;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.MockBigArrays;
 import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.netty.NettyTransport;
@@ -64,7 +64,7 @@ public class NettySizeHeaderFrameDecoderTests extends ESTestCase {
     @Before
     public void startThreadPool() {
         threadPool = new ThreadPool(settings);
-        threadPool.setClusterSettings(new ClusterSettings(settings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS));
+        threadPool.setNodeSettingsService(new NodeSettingsService(settings));
         NetworkService networkService = new NetworkService(settings);
         BigArrays bigArrays = new MockBigArrays(new MockPageCacheRecycler(settings, threadPool), new NoneCircuitBreakerService());
         nettyTransport = new NettyTransport(settings, threadPool, networkService, bigArrays, Version.CURRENT, new NamedWriteableRegistry());
diff --git a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
index 19def40..28a3dea 100644
--- a/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
+++ b/core/src/test/java/org/elasticsearch/tribe/TribeIT.java
@@ -21,13 +21,13 @@ package org.elasticsearch.tribe;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.block.ClusterBlockException;
+import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.Priority;
@@ -37,7 +37,6 @@ import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.discovery.MasterNotDiscoveredException;
 import org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.InternalTestCluster;
 import org.elasticsearch.test.NodeConfigurationSource;
@@ -143,9 +142,7 @@ public class TribeIT extends ESIntegTestCase {
                 .put("node.name", "tribe_node") // make sure we can identify threads from this node
                 .build();
 
-        tribeNode = NodeBuilder.nodeBuilder()
-                .settings(merged)
-                .node();
+        tribeNode = new Node(merged).start();
         tribeClient = tribeNode.client();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java b/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
deleted file mode 100644
index e819286..0000000
--- a/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.validate;
-
-import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-
-@ESIntegTestCase.SuiteScopeTestCase
-public class RenderSearchTemplateIT extends ESIntegTestCase {
-    private static final String TEMPLATE_CONTENTS = "{\"size\":\"{{size}}\",\"query\":{\"match\":{\"foo\":\"{{value}}\"}},\"aggs\":{\"objects\":{\"terms\":{\"field\":\"{{value}}\",\"size\":\"{{size}}\"}}}}";
-
-    @Override
-    protected void setupSuiteScopeCluster() throws Exception {
-        client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "index_template_1", "{ \"template\": " + TEMPLATE_CONTENTS + " }").get();
-    }
-
-    @Override
-    public Settings nodeSettings(int nodeOrdinal) {
-        //Set path so ScriptService will pick up the test scripts
-        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
-                .put("path.conf", this.getDataPath("config")).build();
-    }
-
-    public void testInlineTemplate() {
-        Map<String, Object> params = new HashMap<>();
-        params.put("value", "bar");
-        params.put("size", 20);
-        Template template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        BytesReference source = response.source();
-        assertThat(source, notNullValue());
-        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        assertThat(sourceAsMap, notNullValue());
-        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
-        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-
-        params = new HashMap<>();
-        params.put("value", "baz");
-        params.put("size", 100);
-        template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        source = response.source();
-        assertThat(source, notNullValue());
-        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
-        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-    }
-
-    public void testIndexedTemplate() {
-        Map<String, Object> params = new HashMap<>();
-        params.put("value", "bar");
-        params.put("size", 20);
-        Template template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        BytesReference source = response.source();
-        assertThat(source, notNullValue());
-        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        assertThat(sourceAsMap, notNullValue());
-        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
-        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-
-        params = new HashMap<>();
-        params.put("value", "baz");
-        params.put("size", 100);
-        template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        source = response.source();
-        assertThat(source, notNullValue());
-        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
-        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-    }
-
-    public void testFileTemplate() {
-        Map<String, Object> params = new HashMap<>();
-        params.put("value", "bar");
-        params.put("size", 20);
-        Template template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        BytesReference source = response.source();
-        assertThat(source, notNullValue());
-        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        assertThat(sourceAsMap, notNullValue());
-        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
-        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-
-        params = new HashMap<>();
-        params.put("value", "baz");
-        params.put("size", 100);
-        template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
-        assertThat(response, notNullValue());
-        source = response.source();
-        assertThat(source, notNullValue());
-        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
-        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
-        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
-        assertThat(sourceAsMap, equalTo(expectedMap));
-    }
-}
diff --git a/core/src/test/resources/org/elasticsearch/index/query/config/scripts/full-query-template.mustache b/core/src/test/resources/org/elasticsearch/index/query/config/scripts/full-query-template.mustache
deleted file mode 100644
index 5191414..0000000
--- a/core/src/test/resources/org/elasticsearch/index/query/config/scripts/full-query-template.mustache
+++ /dev/null
@@ -1,6 +0,0 @@
-{
-  "query": {
-    "match": { "{{myField}}" : "{{myValue}}" }
-  },
-  "size" : {{mySize}}
-}
diff --git a/core/src/test/resources/org/elasticsearch/index/query/config/scripts/storedTemplate.mustache b/core/src/test/resources/org/elasticsearch/index/query/config/scripts/storedTemplate.mustache
deleted file mode 100644
index a779da7..0000000
--- a/core/src/test/resources/org/elasticsearch/index/query/config/scripts/storedTemplate.mustache
+++ /dev/null
@@ -1,3 +0,0 @@
-{
-    "match_{{template}}": {}
-}
diff --git a/core/src/test/resources/org/elasticsearch/validate/config/scripts/file_template_1.mustache b/core/src/test/resources/org/elasticsearch/validate/config/scripts/file_template_1.mustache
deleted file mode 100644
index 969dc8d..0000000
--- a/core/src/test/resources/org/elasticsearch/validate/config/scripts/file_template_1.mustache
+++ /dev/null
@@ -1 +0,0 @@
-{"size":"{{size}}","query":{"match":{"foo":"{{value}}"}},"aggs":{"objects":{"terms":{"field":"{{value}}","size":"{{size}}"}}}}
\ No newline at end of file
diff --git a/dev-tools/smoke_test_rc.py b/dev-tools/smoke_test_rc.py
index b7bc00d..3fa61c4 100644
--- a/dev-tools/smoke_test_rc.py
+++ b/dev-tools/smoke_test_rc.py
@@ -70,6 +70,7 @@ DEFAULT_PLUGINS = ["analysis-icu",
                    "lang-expression",
                    "lang-groovy",
                    "lang-javascript",
+                   "lang-plan-a",
                    "lang-python",
                    "mapper-murmur3",
                    "mapper-size",
diff --git a/distribution/build.gradle b/distribution/build.gradle
index 56a602c..4da1641 100644
--- a/distribution/build.gradle
+++ b/distribution/build.gradle
@@ -47,7 +47,7 @@ ext.dependencyFiles = project(':core').configurations.runtime.copyRecursive().ex
  *                                  Modules                                  *
  *****************************************************************************/
 
-task buildModules(type: Copy) {
+task buildModules(type: Sync) {
   into 'build/modules'
 } 
 
@@ -55,21 +55,31 @@ ext.restTestExpansions = [
   'expected.modules.count': 0,
 ]
 // we create the buildModules task above so the distribution subprojects can
-// depend on it, but we don't actually configure it until projects are evaluated
-// so it can depend on the bundling of plugins (ie modules must have been configured)
-project.gradle.projectsEvaluated {
-  project.rootProject.subprojects.findAll { it.path.startsWith(':modules:') }.each { Project module ->
-    buildModules {
-      dependsOn module.bundlePlugin
-      into(module.name) {
-        from { zipTree(module.bundlePlugin.outputs.files.singleFile) }
-      }
-    }
-    configure(subprojects.findAll { it.name != 'integ-test-zip' }) { Project distribution ->
-      distribution.integTest.mustRunAfter(module.integTest)      
+// depend on it, but we don't actually configure it until here so we can do a single
+// loop over modules to also setup cross task dependencies and increment our modules counter
+project.rootProject.subprojects.findAll { it.path.startsWith(':modules:') }.each { Project module ->
+  buildModules {
+    dependsOn({ project(module.path).bundlePlugin })
+    into(module.name) {
+      from { zipTree(project(module.path).bundlePlugin.outputs.files.singleFile) }
     }
-    restTestExpansions['expected.modules.count'] += 1
   }
+  // We would like to make sure integ tests for the distribution run after
+  // integ tests for the modules included in the distribution. However, gradle
+  // has a bug where depending on a task with a finalizer can sometimes not make
+  // the finalizer task follow the original task immediately. To work around this,
+  // we make the mustRunAfter the finalizer task itself.
+  // See https://discuss.gradle.org/t/cross-project-task-dependencies-ordering-screws-up-finalizers/13190
+  project.configure(project.subprojects.findAll { it.name != 'integ-test-zip' }) { Project distribution ->
+    distribution.afterEvaluate({
+      distribution.integTest.mustRunAfter("${module.path}:integTest#stop")
+    })
+  }
+  // also want to make sure the module's integration tests run after the integ-test-zip (ie rest tests)
+  module.afterEvaluate({
+    module.integTest.mustRunAfter(':distribution:integ-test-zip:integTest#stop')
+  })
+  restTestExpansions['expected.modules.count'] += 1
 }
 
 // make sure we have a clean task since we aren't a java project, but we have tasks that
@@ -84,11 +94,15 @@ subprojects {
    *****************************************************************************/
   apply plugin: 'elasticsearch.rest-test'
   project.integTest {
-    dependsOn(project.assemble)
+    dependsOn project.assemble
     includePackaged project.name == 'integ-test-zip'
     cluster {
       distribution = project.name
     }
+    if (project.name != 'integ-test-zip') {
+      // see note above with module mustRunAfter about why integTest#stop is used here
+      mustRunAfter ':distribution:integ-test-zip:integTest#stop'
+    }
   }
   
   processTestResources {
diff --git a/distribution/deb/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml b/distribution/deb/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
index a7f265d..da68232 100644
--- a/distribution/deb/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
+++ b/distribution/deb/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
@@ -10,4 +10,4 @@
     - do:
         nodes.info: {}
 
-    - length:  { nodes.$master.plugins: ${expected.modules.count}  }
+    - length:  { nodes.$master.modules: ${expected.modules.count}  }
diff --git a/distribution/licenses/compiler-0.9.1.jar.sha1 b/distribution/licenses/compiler-0.9.1.jar.sha1
deleted file mode 100644
index 96152e0..0000000
--- a/distribution/licenses/compiler-0.9.1.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-14aec5344639782ee76441401b773946c65eb2b3
diff --git a/distribution/licenses/compiler-LICENSE.txt b/distribution/licenses/compiler-LICENSE.txt
deleted file mode 100644
index ac68303..0000000
--- a/distribution/licenses/compiler-LICENSE.txt
+++ /dev/null
@@ -1,14 +0,0 @@
-Copyright 2010 RightTime, Inc.
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-
diff --git a/distribution/licenses/compiler-NOTICE.txt b/distribution/licenses/compiler-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/distribution/licenses/compiler-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index ff69dc3..0000000
--- a/distribution/licenses/lucene-analyzers-common-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-feaf885ed4155fb7202c1f90ac2eb40503961efc
\ No newline at end of file
diff --git a/distribution/licenses/lucene-analyzers-common-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-analyzers-common-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..5d95f64
--- /dev/null
+++ b/distribution/licenses/lucene-analyzers-common-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+9f2b9811a4f4a57a1b3a98bdc1e1b63476b9f628
\ No newline at end of file
diff --git a/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 1341c03..0000000
--- a/distribution/licenses/lucene-backward-codecs-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-5b5b5c950b4fcac38cf48fab911f75da61e780fa
\ No newline at end of file
diff --git a/distribution/licenses/lucene-backward-codecs-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-backward-codecs-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..0ae258b
--- /dev/null
+++ b/distribution/licenses/lucene-backward-codecs-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+038071889a5dbeb279e37fa46225e194139a427c
\ No newline at end of file
diff --git a/distribution/licenses/lucene-core-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-core-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 10ffbd1..0000000
--- a/distribution/licenses/lucene-core-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-84685d37a34b4d87e2928566ed266a7f005ca67d
\ No newline at end of file
diff --git a/distribution/licenses/lucene-core-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-core-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..aee7c10
--- /dev/null
+++ b/distribution/licenses/lucene-core-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+b986d0ad8ee4dda8172a5a61875c47631e4b21d4
\ No newline at end of file
diff --git a/distribution/licenses/lucene-grouping-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-grouping-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 6eed3e2..0000000
--- a/distribution/licenses/lucene-grouping-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-ff92011208ed5c28f041acc37bd77728a89fc6a5
\ No newline at end of file
diff --git a/distribution/licenses/lucene-grouping-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-grouping-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..aa1011e
--- /dev/null
+++ b/distribution/licenses/lucene-grouping-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+f46574fbdfbcc81d936c77e15ba5b3af2c2b7253
\ No newline at end of file
diff --git a/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index ac8fb4d..0000000
--- a/distribution/licenses/lucene-highlighter-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-5d46f26a6cb36aede89b8728b6fcbc427d4f9416
\ No newline at end of file
diff --git a/distribution/licenses/lucene-highlighter-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-highlighter-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..561f17e
--- /dev/null
+++ b/distribution/licenses/lucene-highlighter-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+f620262d667a294d390e8df7575cc2cca2626559
\ No newline at end of file
diff --git a/distribution/licenses/lucene-join-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-join-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index fade025..0000000
--- a/distribution/licenses/lucene-join-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-726ea07bbfdfbfbee80522353496fc6667dc33c9
\ No newline at end of file
diff --git a/distribution/licenses/lucene-join-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-join-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..4735bdf
--- /dev/null
+++ b/distribution/licenses/lucene-join-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+4c44b07242fd706f6f7f14c9063a725e0e5b98cd
\ No newline at end of file
diff --git a/distribution/licenses/lucene-memory-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-memory-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 4ce0f78..0000000
--- a/distribution/licenses/lucene-memory-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-d8d7a7b573a4cfc54745a126e905ccfd523b7a24
\ No newline at end of file
diff --git a/distribution/licenses/lucene-memory-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-memory-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..9c19a6a
--- /dev/null
+++ b/distribution/licenses/lucene-memory-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+1e33e0aa5fc227e90c8314f61b4cba1090035e33
\ No newline at end of file
diff --git a/distribution/licenses/lucene-misc-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-misc-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 245438c..0000000
--- a/distribution/licenses/lucene-misc-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-cd9d4fb4492bd2680cea2f038a051311329f6443
\ No newline at end of file
diff --git a/distribution/licenses/lucene-misc-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-misc-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..c4a61bf
--- /dev/null
+++ b/distribution/licenses/lucene-misc-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+e416893f7b781239a15d3e2c7200ff26574d14de
\ No newline at end of file
diff --git a/distribution/licenses/lucene-queries-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-queries-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 5244f41..0000000
--- a/distribution/licenses/lucene-queries-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-a1a04d191443e51f992ed3dd02d0e14fd48493c9
\ No newline at end of file
diff --git a/distribution/licenses/lucene-queries-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-queries-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..edc5675
--- /dev/null
+++ b/distribution/licenses/lucene-queries-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+b153b63b9333feedb18af2673eb6ccaf95bcc8bf
\ No newline at end of file
diff --git a/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 4600767..0000000
--- a/distribution/licenses/lucene-queryparser-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-c4d34b29b8b14ad3deb300a6d699e9d8965a3c2c
\ No newline at end of file
diff --git a/distribution/licenses/lucene-queryparser-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-queryparser-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..eddd3d6
--- /dev/null
+++ b/distribution/licenses/lucene-queryparser-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+0aa2758d70a79f2e0f33a87624fd9d31e155c864
\ No newline at end of file
diff --git a/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 7ad16ae..0000000
--- a/distribution/licenses/lucene-sandbox-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-bf45dbd653d66ce9d2c3f19b69997b8098d8b416
\ No newline at end of file
diff --git a/distribution/licenses/lucene-sandbox-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-sandbox-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..571903c
--- /dev/null
+++ b/distribution/licenses/lucene-sandbox-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+873c716ba629dae389b12ddb1aedf2f5c5f57fea
\ No newline at end of file
diff --git a/distribution/licenses/lucene-spatial-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-spatial-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index e366610..0000000
--- a/distribution/licenses/lucene-spatial-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-2bddfda70f5c657064d12860b03c2cd8a5029bfc
\ No newline at end of file
diff --git a/distribution/licenses/lucene-spatial-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-spatial-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..5e6a27b
--- /dev/null
+++ b/distribution/licenses/lucene-spatial-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+9d7e47c2fb73c614cc5ca41529b2c273c73b0ce7
\ No newline at end of file
diff --git a/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index ed120db..0000000
--- a/distribution/licenses/lucene-spatial3d-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-881b8cd571fb3ccdcc69f1316468d816812513fb
\ No newline at end of file
diff --git a/distribution/licenses/lucene-spatial3d-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-spatial3d-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..cf841e1
--- /dev/null
+++ b/distribution/licenses/lucene-spatial3d-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+4766305088797a66fe02d5aaa98e086867816e42
\ No newline at end of file
diff --git a/distribution/licenses/lucene-suggest-5.4.0-snapshot-1715952.jar.sha1 b/distribution/licenses/lucene-suggest-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index adab682..0000000
--- a/distribution/licenses/lucene-suggest-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-466e2bc02f45f04cbf516e5df78b9c2ebd99e944
\ No newline at end of file
diff --git a/distribution/licenses/lucene-suggest-5.5.0-snapshot-1719088.jar.sha1 b/distribution/licenses/lucene-suggest-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..1fbb60a
--- /dev/null
+++ b/distribution/licenses/lucene-suggest-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+f0ee6fb780ea8aa9ec6d31e6a9cc7d48700bd2ca
\ No newline at end of file
diff --git a/distribution/rpm/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml b/distribution/rpm/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
index a7f265d..da68232 100644
--- a/distribution/rpm/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
+++ b/distribution/rpm/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
@@ -10,4 +10,4 @@
     - do:
         nodes.info: {}
 
-    - length:  { nodes.$master.plugins: ${expected.modules.count}  }
+    - length:  { nodes.$master.modules: ${expected.modules.count}  }
diff --git a/distribution/src/main/resources/config/elasticsearch.yml b/distribution/src/main/resources/config/elasticsearch.yml
index 51630fe..4b335ce 100644
--- a/distribution/src/main/resources/config/elasticsearch.yml
+++ b/distribution/src/main/resources/config/elasticsearch.yml
@@ -60,19 +60,8 @@
 # For more information, see the documentation at:
 # <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html>
 #
-# ---------------------------------- Gateway -----------------------------------
-#
-# Block initial recovery after a full cluster restart until N nodes are started:
-#
-# gateway.recover_after_nodes: 3
-#
-# For more information, see the documentation at:
-# <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-gateway.html>
-#
 # --------------------------------- Discovery ----------------------------------
 #
-# Elasticsearch nodes will find each other via unicast, by default.
-#
 # Pass an initial list of hosts to perform discovery when new node is started:
 # The default list of hosts is ["127.0.0.1", "[::1]"]
 #
@@ -85,6 +74,15 @@
 # For more information, see the documentation at:
 # <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html>
 #
+# ---------------------------------- Gateway -----------------------------------
+#
+# Block initial recovery after a full cluster restart until N nodes are started:
+#
+# gateway.recover_after_nodes: 3
+#
+# For more information, see the documentation at:
+# <http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-gateway.html>
+#
 # ---------------------------------- Various -----------------------------------
 #
 # Disable starting multiple nodes on a single system:
diff --git a/distribution/tar/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml b/distribution/tar/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
index a7f265d..da68232 100644
--- a/distribution/tar/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
+++ b/distribution/tar/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
@@ -10,4 +10,4 @@
     - do:
         nodes.info: {}
 
-    - length:  { nodes.$master.plugins: ${expected.modules.count}  }
+    - length:  { nodes.$master.modules: ${expected.modules.count}  }
diff --git a/distribution/zip/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml b/distribution/zip/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
index a7f265d..da68232 100644
--- a/distribution/zip/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
+++ b/distribution/zip/src/test/resources/rest-api-spec/test/smoke_test_plugins/10_modules.yaml
@@ -10,4 +10,4 @@
     - do:
         nodes.info: {}
 
-    - length:  { nodes.$master.plugins: ${expected.modules.count}  }
+    - length:  { nodes.$master.modules: ${expected.modules.count}  }
diff --git a/docs/java-api/client.asciidoc b/docs/java-api/client.asciidoc
index cfc45b7..87aa729 100644
--- a/docs/java-api/client.asciidoc
+++ b/docs/java-api/client.asciidoc
@@ -37,11 +37,10 @@ that can execute operations against elasticsearch.
 
 [source,java]
 --------------------------------------------------
-import static org.elasticsearch.node.NodeBuilder.*;
 
 // on startup
 
-Node node = nodeBuilder().node();
+Node node = new Node(Settings.EMPTY).start();
 Client client = node.client();
 
 // on shutdown
@@ -86,17 +85,15 @@ it):
 
 [source,java]
 --------------------------------------------------
-import static org.elasticsearch.node.NodeBuilder.*;
 
 // on startup
 
 // Embedded node clients behave just like standalone nodes,
 // which means that they will leave the HTTP port open!
-Node node =
-    nodeBuilder()
-        .settings(Settings.settingsBuilder().put("http.enabled", false))
-        .client(true)
-    .node();
+Node node = new Node(Settings.settingsBuilder()
+        .put("http.enabled", false)
+        .put("node.client", true).build())
+    .start();
 
 Client client = node.client();
 
@@ -115,11 +112,10 @@ and form a cluster.
 
 [source,java]
 --------------------------------------------------
-import static org.elasticsearch.node.NodeBuilder.*;
 
 // on startup
 
-Node node = nodeBuilder().local(true).node();
+Node node = new Node(Settings.builder().put("node.local", true).build()).start();
 Client client = node.client();
 
 // on shutdown
diff --git a/docs/java-api/query-dsl/missing-query.asciidoc b/docs/java-api/query-dsl/missing-query.asciidoc
deleted file mode 100644
index 00086cf..0000000
--- a/docs/java-api/query-dsl/missing-query.asciidoc
+++ /dev/null
@@ -1,14 +0,0 @@
-[[java-query-dsl-missing-query]]
-==== Missing Query
-
-See {ref}/query-dsl-missing-query.html[Missing Query]
-
-[source,java]
---------------------------------------------------
-QueryBuilder qb = missingQuery("user",          <1>
-    true,                                       <2>
-    true);                                      <3>
---------------------------------------------------
-<1> field
-<2> find missing field with an explicit `null` value
-<3> find missing field that doesn’t exist
diff --git a/docs/java-api/query-dsl/term-level-queries.asciidoc b/docs/java-api/query-dsl/term-level-queries.asciidoc
index 44fc363..e7d5ad4 100644
--- a/docs/java-api/query-dsl/term-level-queries.asciidoc
+++ b/docs/java-api/query-dsl/term-level-queries.asciidoc
@@ -30,11 +30,6 @@ The queries in this group are:
 
     Find documents where the field specified contains any non-null value.
 
-<<java-query-dsl-missing-query,`missing` query>>::
-
-    Find documents where the field specified does is missing or contains only
-    `null` values.
-
 <<java-query-dsl-prefix-query,`prefix` query>>::
 
     Find documents where the field specified contains terms which being with
@@ -75,8 +70,6 @@ include::range-query.asciidoc[]
 
 include::exists-query.asciidoc[]
 
-include::missing-query.asciidoc[]
-
 include::prefix-query.asciidoc[]
 
 include::wildcard-query.asciidoc[]
@@ -88,6 +81,3 @@ include::fuzzy-query.asciidoc[]
 include::type-query.asciidoc[]
 
 include::ids-query.asciidoc[]
-
-
-
diff --git a/docs/plugins/discovery-ec2.asciidoc b/docs/plugins/discovery-ec2.asciidoc
index a2b8049..bdd46fb 100644
--- a/docs/plugins/discovery-ec2.asciidoc
+++ b/docs/plugins/discovery-ec2.asciidoc
@@ -64,16 +64,19 @@ cloud:
             protocol: https
 ----
 
-In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be
-`http` or `https`):
+In addition, a proxy can be configured with the `proxy.host`, `proxy.port`, `proxy.username` and `proxy.password` settings
+(note that protocol can be `http` or `https`):
 
 [source,yaml]
 ----
 cloud:
     aws:
         protocol: https
-        proxy_host: proxy1.company.com
-        proxy_port: 8083
+        proxy:
+            host: proxy1.company.com
+            port: 8083
+            username: myself
+            password: theBestPasswordEver!
 ----
 
 You can also set different proxies for `ec2` and `s3`:
@@ -83,11 +86,17 @@ You can also set different proxies for `ec2` and `s3`:
 cloud:
     aws:
         s3:
-            proxy_host: proxy1.company.com
-            proxy_port: 8083
+            proxy:
+                host: proxy1.company.com
+                port: 8083
+                username: myself1
+                password: theBestPasswordEver1!
         ec2:
-            proxy_host: proxy2.company.com
-            proxy_port: 8083
+            proxy:
+                host: proxy2.company.com
+                port: 8083
+                username: myself2
+                password: theBestPasswordEver2!
 ----
 
 [[discovery-ec2-usage-region]]
diff --git a/docs/plugins/repository-s3.asciidoc b/docs/plugins/repository-s3.asciidoc
index 1650588..faaa873 100644
--- a/docs/plugins/repository-s3.asciidoc
+++ b/docs/plugins/repository-s3.asciidoc
@@ -67,16 +67,19 @@ cloud:
             protocol: https
 ----
 
-In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be
-`http` or `https`):
+In addition, a proxy can be configured with the `proxy.host`, `proxy.port`, `proxy.username` and `proxy.password` settings
+(note that protocol can be `http` or `https`):
 
 [source,yaml]
 ----
 cloud:
     aws:
         protocol: https
-        proxy_host: proxy1.company.com
-        proxy_port: 8083
+        proxy:
+            host: proxy1.company.com
+            port: 8083
+            username: myself
+            password: theBestPasswordEver!
 ----
 
 You can also set different proxies for `ec2` and `s3`:
@@ -86,11 +89,17 @@ You can also set different proxies for `ec2` and `s3`:
 cloud:
     aws:
         s3:
-            proxy_host: proxy1.company.com
-            proxy_port: 8083
+            proxy:
+                host: proxy1.company.com
+                port: 8083
+                username: myself1
+                password: theBestPasswordEver1!
         ec2:
-            proxy_host: proxy2.company.com
-            proxy_port: 8083
+            proxy:
+                host: proxy2.company.com
+                port: 8083
+                username: myself2
+                password: theBestPasswordEver2!
 ----
 
 [[repository-s3-usage-region]]
diff --git a/docs/reference/api-conventions.asciidoc b/docs/reference/api-conventions.asciidoc
index 98fbcae..6bbd041 100644
--- a/docs/reference/api-conventions.asciidoc
+++ b/docs/reference/api-conventions.asciidoc
@@ -360,6 +360,22 @@ are:
 `s`::   Second
 `ms`::  Milli-second
 
+[[size-units]]
+[float]
+=== Data size units
+
+Whenever the size of data needs to be specified, eg when setting a buffer size
+parameter, the value must specify the unit, like `10kb` for 10 kilobytes.  The
+supported units are:
+
+[horizontal]
+`b`::   Bytes
+`kb`::  Kilobytes
+`mb`::  Megabytes
+`gb`::  Gigabytes
+`tb`::  Terabytes
+`pb`::  Petabytes
+
 [[distance-units]]
 [float]
 === Distance Units
diff --git a/docs/reference/cluster/health.asciidoc b/docs/reference/cluster/health.asciidoc
index 7d9bdc1..137b4ac 100644
--- a/docs/reference/cluster/health.asciidoc
+++ b/docs/reference/cluster/health.asciidoc
@@ -87,6 +87,10 @@ The cluster health API accepts the following request parameters:
     A time based parameter controlling how long to wait if one of
     the wait_for_XXX are provided. Defaults to `30s`.
 
+`local`::
+    If `true` returns the local node information and does not provide
+    the state from master node. Default: `false`.
+
 
 The following is an example of getting the cluster health at the
 `shards` level:
diff --git a/docs/reference/cluster/update-settings.asciidoc b/docs/reference/cluster/update-settings.asciidoc
index 8ec5842..08f4c90 100644
--- a/docs/reference/cluster/update-settings.asciidoc
+++ b/docs/reference/cluster/update-settings.asciidoc
@@ -38,44 +38,6 @@ last example will be:
 }'
 --------------------------------------------------
 
-Resetting persistent or transient settings can be done by assigning a
-`null` value. If a transient setting is reset, the persistent setting
-is applied if available. Otherwise Elasticsearch will fallback to the setting
-defined at the configuration file or, if not existent, to the default
-value. Here is an example:
-
-[source,js]
---------------------------------------------------
-curl -XPUT localhost:9200/_cluster/settings -d '{
-    "transient" : {
-        "discovery.zen.minimum_master_nodes" : null
-    }
-}'
---------------------------------------------------
-
-Reset settings will not be included in the cluster response. So
-the response for the last example will be:
-
-[source,js]
---------------------------------------------------
-{
-    "persistent" : {},
-    "transient" : {}
-}
---------------------------------------------------
-
-Settings can also be reset using simple wildcards. For instance to reset
-all dynamic `discovery.zen` setting a prefix can be used:
-
-[source,js]
---------------------------------------------------
-curl -XPUT localhost:9200/_cluster/settings -d '{
-    "transient" : {
-        "discovery.zen.*" : null
-    }
-}'
---------------------------------------------------
-
 Cluster wide settings can be returned using:
 
 [source,js]
diff --git a/docs/reference/index-modules/similarity.asciidoc b/docs/reference/index-modules/similarity.asciidoc
index 0ade819..ddec26b 100644
--- a/docs/reference/index-modules/similarity.asciidoc
+++ b/docs/reference/index-modules/similarity.asciidoc
@@ -112,7 +112,10 @@ Type name: `DFR`
 ==== IB similarity.
 
 http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/IBSimilarity.html[Information
-based model] . This similarity has the following options:
+based model] . The algorithm is based on the concept that the information content in any symbolic 'distribution'
+sequence is primarily determined by the repetitive usage of its basic elements.
+For written texts this challenge would correspond to comparing the writing styles of diferent authors.
+This similarity has the following options:
 
 [horizontal]
 `distribution`::  Possible values: `ll` and `spl`.
@@ -138,11 +141,11 @@ Type name: `LMDirichlet`
 ==== LM Jelinek Mercer similarity.
 
 http://lucene.apache.org/core/5_2_1/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.html[LM
-Jelinek Mercer similarity] . This similarity has the following options:
+Jelinek Mercer similarity] . The algorithm attempts to capture important patterns in the text, while leaving out noise. This similarity has the following options:
 
 [horizontal]
 `lambda`::  The optimal value depends on both the collection and the query. The optimal value is around `0.1`
-for title queries and `0.7` for long queries. Default to `0.1`.
+for title queries and `0.7` for long queries. Default to `0.1`. When value approaches `0`, documents that match more query terms will be ranked higher than those that match fewer terms.
 
 Type name: `LMJelinekMercer`
 
diff --git a/docs/reference/index.asciidoc b/docs/reference/index.asciidoc
index 34d1cba..4acd1f1 100644
--- a/docs/reference/index.asciidoc
+++ b/docs/reference/index.asciidoc
@@ -7,6 +7,7 @@
 :jdk:           1.8.0_25
 :defguide:      https://www.elastic.co/guide/en/elasticsearch/guide/current
 :plugins:       https://www.elastic.co/guide/en/elasticsearch/plugins/master
+:javaclient:    https://www.elastic.co/guide/en/elasticsearch/client/java-api/master/
 :issue:         https://github.com/elastic/elasticsearch/issues/
 :pull:          https://github.com/elastic/elasticsearch/pull/
 
diff --git a/docs/reference/indices/aliases.asciidoc b/docs/reference/indices/aliases.asciidoc
index 9a65c89..57faa97 100644
--- a/docs/reference/indices/aliases.asciidoc
+++ b/docs/reference/indices/aliases.asciidoc
@@ -63,7 +63,22 @@ curl -XPOST 'http://localhost:9200/_aliases' -d '
 }'
 --------------------------------------------------
 
-Alternatively, you can use a glob pattern to associate an alias to
+Multiple indices can be specified for an action with the `indices` array syntax:
+
+[source,js]
+--------------------------------------------------
+curl -XPOST 'http://localhost:9200/_aliases' -d '
+{
+    "actions" : [
+        { "add" : { "indices" : ["test1", "test2"], "alias" : "alias1" } }
+    ]
+}'
+--------------------------------------------------
+
+To specify multiple aliases in one action, the corresponding `aliases` array
+syntax exists as well.
+
+For the example above, a glob pattern can also be used to associate an alias to
 more than one index that share a common name:
 
 [source,js]
diff --git a/docs/reference/indices/analyze.asciidoc b/docs/reference/indices/analyze.asciidoc
index 1a256a6..1e8cd77 100644
--- a/docs/reference/indices/analyze.asciidoc
+++ b/docs/reference/indices/analyze.asciidoc
@@ -100,3 +100,74 @@ provided it doesn't start with `{` :
 --------------------------------------------------
 curl -XGET 'localhost:9200/_analyze?tokenizer=keyword&token_filters=lowercase&char_filters=html_strip' -d 'this is a <b>test</b>'
 --------------------------------------------------
+
+=== Explain Analyze
+
+If you want to get more advanced details, set `explain` to `true` (defaults to `false`). It will output all token attributes for each token.
+You can filter token attributes you want to output by setting `attributes` option.
+
+experimental[The format of the additional detail information is experimental and can change at any time]
+
+[source,js]
+--------------------------------------------------
+GET test/_analyze
+{
+  "tokenizer" : "standard",
+  "token_filters" : ["snowball"],
+  "text" : "detailed output",
+  "explain" : true,
+  "attributes" : ["keyword"] <1>
+}
+--------------------------------------------------
+// AUTOSENSE
+<1> Set "keyword" to output "keyword" attribute only
+
+coming[2.0.0, body based parameters were added in 2.0.0]
+
+The request returns the following result:
+
+[source,js]
+--------------------------------------------------
+{
+  "detail" : {
+    "custom_analyzer" : true,
+    "charfilters" : [ ],
+    "tokenizer" : {
+      "name" : "standard",
+      "tokens" : [ {
+        "token" : "detailed",
+        "start_offset" : 0,
+        "end_offset" : 8,
+        "type" : "<ALPHANUM>",
+        "position" : 0
+      }, {
+        "token" : "output",
+        "start_offset" : 9,
+        "end_offset" : 15,
+        "type" : "<ALPHANUM>",
+        "position" : 1
+      } ]
+    },
+    "tokenfilters" : [ {
+      "name" : "snowball",
+      "tokens" : [ {
+        "token" : "detail",
+        "start_offset" : 0,
+        "end_offset" : 8,
+        "type" : "<ALPHANUM>",
+        "position" : 0,
+        "keyword" : false <1>
+      }, {
+        "token" : "output",
+        "start_offset" : 9,
+        "end_offset" : 15,
+        "type" : "<ALPHANUM>",
+        "position" : 1,
+        "keyword" : false <1>
+      } ]
+    } ]
+  }
+}
+--------------------------------------------------
+<1> Output only "keyword" attribute, since specify "attributes" in the request.
+
diff --git a/docs/reference/mapping/dynamic/templates.asciidoc b/docs/reference/mapping/dynamic/templates.asciidoc
index e38fc31..b60c5f0 100644
--- a/docs/reference/mapping/dynamic/templates.asciidoc
+++ b/docs/reference/mapping/dynamic/templates.asciidoc
@@ -148,13 +148,14 @@ PUT my_index/my_type/1
 [[match-pattern]]
 ==== `match_pattern`
 
-The `match_pattern` parameter behaves just like the `match` parameter, but
-supports full Java regular expression matching on the field name instead of
-simple wildcards, for instance:
+The `match_pattern` parameter adjusts the behavior of the `match` parameter
+such that it supports full Java regular expression matching on the field name
+instead of simple wildcards, for instance:
 
 [source,js]
 --------------------------------------------------
-  "match_pattern": "^profit_\d+$"
+  "match_pattern": "regex",
+  "match": "^profit_\d+$"
 --------------------------------------------------
 
 [[path-match-unmatch]]
diff --git a/docs/reference/mapping/fields/field-names-field.asciidoc b/docs/reference/mapping/fields/field-names-field.asciidoc
index 2c40f72..bafc3e3 100644
--- a/docs/reference/mapping/fields/field-names-field.asciidoc
+++ b/docs/reference/mapping/fields/field-names-field.asciidoc
@@ -3,9 +3,8 @@
 
 The `_field_names` field indexes the names of every field in a document that
 contains any value other than `null`.  This field is used by the
-<<query-dsl-exists-query,`exists`>> and <<query-dsl-missing-query,`missing`>>
-queries to find documents that either have or don't have any non-+null+ value
-for a particular field.
+<<query-dsl-exists-query,`exists`>> query to find documents that
+either have or don't have any non-+null+ value for a particular field.
 
 The value of the `_field_name` field is accessible in queries, aggregations, and
 scripts:
@@ -49,7 +48,6 @@ GET my_index/_search
 --------------------------
 // AUTOSENSE
 
-<1> Querying on the `_field_names` field (also see the <<query-dsl-exists-query,`exists`>> and <<query-dsl-missing-query,`missing`>> queries)
+<1> Querying on the `_field_names` field (also see the <<query-dsl-exists-query,`exists`>> query)
 <2> Aggregating on the `_field_names` field
 <3> Accessing the `_field_names` field in scripts (inline scripts must be <<enable-dynamic-scripting,enabled>> for this example to work)
-
diff --git a/docs/reference/mapping/params/null-value.asciidoc b/docs/reference/mapping/params/null-value.asciidoc
index 552ce66..4d70d4a 100644
--- a/docs/reference/mapping/params/null-value.asciidoc
+++ b/docs/reference/mapping/params/null-value.asciidoc
@@ -53,7 +53,3 @@ IMPORTANT: The `null_value` needs to be the same datatype as the field.  For
 instance, a `long` field cannot have a string `null_value`.  String fields
 which are `analyzed` will also pass the `null_value` through the configured
 analyzer.
-
-Also see the <<query-dsl-missing-query,`missing` query>> for its `null_value` support.
-
-
diff --git a/docs/reference/mapping/types/geo-shape.asciidoc b/docs/reference/mapping/types/geo-shape.asciidoc
index d974847..1f0c76e 100644
--- a/docs/reference/mapping/types/geo-shape.asciidoc
+++ b/docs/reference/mapping/types/geo-shape.asciidoc
@@ -17,13 +17,13 @@ The geo_shape mapping maps geo_json geometry objects to the geo_shape
 type. To enable it, users must explicitly map fields to the geo_shape
 type.
 
-[cols="<,<",options="header",]
+[cols="<,<,<",options="header",]
 |=======================================================================
-|Option |Description
+|Option |Description| Default
 
 |`tree` |Name of the PrefixTree implementation to be used: `geohash` for
-GeohashPrefixTree and `quadtree` for QuadPrefixTree. Defaults to
-`geohash`.
+GeohashPrefixTree and `quadtree` for QuadPrefixTree.
+| `geohash`
 
 |`precision` |This parameter may be used instead of `tree_levels` to set
 an appropriate value for the `tree_levels` parameter. The value
@@ -31,7 +31,8 @@ specifies the desired precision and Elasticsearch will calculate the
 best tree_levels value to honor this precision. The value should be a
 number followed by an optional distance unit. Valid distance units
 include: `in`, `inch`, `yd`, `yard`, `mi`, `miles`, `km`, `kilometers`,
-`m`,`meters` (default), `cm`,`centimeters`, `mm`, `millimeters`.
+`m`,`meters`, `cm`,`centimeters`, `mm`, `millimeters`.
+| `meters`
 
 |`tree_levels` |Maximum number of layers to be used by the PrefixTree.
 This can be used to control the precision of shape representations and
@@ -41,27 +42,40 @@ certain level of understanding of the underlying implementation, users
 may use the `precision` parameter instead. However, Elasticsearch only
 uses the tree_levels parameter internally and this is what is returned
 via the mapping API even if you use the precision parameter.
+| `50m`
+
+|`strategy` |The strategy parameter defines the approach for how to
+represent shapes at indexing and search time. It also influences the
+capabilities available so it is recommended to let Elasticsearch set
+this parameter automatically. There are two strategies available:
+`recursive` and `term`. Term strategy supports point types only (the
+`points_only` parameter will be automatically set to true) while
+Recursive strategy supports all shape types. (IMPORTANT: see
+<<prefix-trees, Prefix trees>> for more detailed information)
+| `recursive`
 
 |`distance_error_pct` |Used as a hint to the PrefixTree about how
 precise it should be. Defaults to 0.025 (2.5%) with 0.5 as the maximum
-supported value. PERFORMANCE NOTE: This value will be default to 0 if a `precision` or
+supported value. PERFORMANCE NOTE: This value will default to 0 if a `precision` or
 `tree_level` definition is explicitly defined. This guarantees spatial precision
 at the level defined in the mapping. This can lead to significant memory usage
 for high resolution shapes with low error (e.g., large shapes at 1m with < 0.001 error).
 To improve indexing performance (at the cost of query accuracy) explicitly define
 `tree_level` or `precision` along with a reasonable `distance_error_pct`, noting
 that large shapes will have greater false positives.
+| `0.025`
 
 |`orientation` |Optionally define how to interpret vertex order for
 polygons / multipolygons.  This parameter defines one of two coordinate
 system rules (Right-hand or Left-hand) each of which can be specified in three
-different ways. 1. Right-hand rule (default): `right`, `ccw`, `counterclockwise`,
+different ways. 1. Right-hand rule: `right`, `ccw`, `counterclockwise`,
 2. Left-hand rule: `left`, `cw`, `clockwise`. The default orientation
 (`counterclockwise`) complies with the OGC standard which defines
 outer ring vertices in counterclockwise order with inner ring(s) vertices (holes)
 in clockwise order. Setting this parameter in the geo_shape mapping explicitly
 sets vertex order for the coordinate list of a geo_shape field but can be
 overridden in each individual GeoJSON document.
+| `ccw`
 
 |`points_only` |Setting this option to `true` (defaults to `false`) configures
 the `geo_shape` field type for point shapes only (NOTE: Multi-Points are not
@@ -70,18 +84,21 @@ yet supported). This optimizes index and search performance for the `geohash` an
 queries can not be executed on `geo_point` field types. This option bridges the gap
 by improving point performance on a `geo_shape` field so that `geo_shape` queries are
 optimal on a point only field.
+| `false`
 
 
 |=======================================================================
 
+[[prefix-trees]]
 [float]
 ==== Prefix trees
 
 To efficiently represent shapes in the index, Shapes are converted into
-a series of hashes representing grid squares using implementations of a
-PrefixTree. The tree notion comes from the fact that the PrefixTree uses
-multiple grid layers, each with an increasing level of precision to
-represent the Earth.
+a series of hashes representing grid squares (commonly referred to as "rasters")
+using implementations of a PrefixTree. The tree notion comes from the fact that
+the PrefixTree uses multiple grid layers, each with an increasing level of
+precision to represent the Earth. This can be thought of as increasing the level
+of detail of a map or image at higher zoom levels.
 
 Multiple PrefixTree implementations are provided:
 
@@ -100,6 +117,29 @@ longitude the resulting hash is a bit set. A tree level in a quad tree
 represents 2 bits in this bit set, one for each coordinate. The maximum
 amount of levels for the quad trees in Elasticsearch is 50.
 
+[[spatial-strategy]]
+[float]
+===== Spatial strategies
+The PrefixTree implementations rely on a SpatialStrategy for decomposing
+the provided Shape(s) into approximated grid squares. Each strategy answers
+the following:
+
+* What type of Shapes can be indexed?
+* What types of Query Operations and Shapes can be used?
+* Does it support more than one Shape per field?
+
+The following Strategy implementations (with corresponding capabilities)
+are provided:
+
+[cols="<,<,<,<",options="header",]
+|=======================================================================
+|Strategy |Supported Shapes |Supported Queries |Multiple Shapes
+
+|`recursive` |<<input-structure, All>> |`INTERSECTS`, `DISJOINT`, `WITHIN`, `CONTAINS` |Yes
+|`term` |<<point, Points>> |`INTERSECTS` |Yes
+
+|=======================================================================
+
 [float]
 ===== Accuracy
 
@@ -149,6 +189,7 @@ between index size and a reasonable level of precision of 50m at the
 equator. This allows for indexing tens of millions of shapes without
 overly bloating the resulting index too much relative to the input size.
 
+[[input-structure]]
 [float]
 ==== Input Structure
 
@@ -189,6 +230,7 @@ differs from many Geospatial APIs (e.g., Google Maps) that generally
 use the colloquial latitude, longitude (Y, X).
 =============================================
 
+[[point]]
 [float]
 ===== http://geojson.org/geojson-spec.html#id2[Point]
 
diff --git a/docs/reference/migration/migrate_2_0/mapping.asciidoc b/docs/reference/migration/migrate_2_0/mapping.asciidoc
index 33ef9eb..09170a2 100644
--- a/docs/reference/migration/migrate_2_0/mapping.asciidoc
+++ b/docs/reference/migration/migrate_2_0/mapping.asciidoc
@@ -429,3 +429,11 @@ to use the old default of 0. This was done to prevent phrase queries from
 matching across different values of the same term unexpectedly. Specifically,
 100 was chosen to cause phrase queries with slops up to 99 to match only within
 a single value of a field.
+
+==== copy_to and multi fields
+
+A <<copy-to,copy_to>> within a <<multi-fields,multi field>> is ignored from version 2.0 on. With any version after
+2.1 or 2.0.1 creating a mapping that has a copy_to within a multi field will result 
+in an exception.
+
+
diff --git a/docs/reference/migration/migrate_3_0.asciidoc b/docs/reference/migration/migrate_3_0.asciidoc
index b8683bc..6588f22 100644
--- a/docs/reference/migration/migrate_3_0.asciidoc
+++ b/docs/reference/migration/migrate_3_0.asciidoc
@@ -206,6 +206,20 @@ cluster settings please use the settings update API and set their superseded key
 
 The `transform` feature from mappings has been removed. It made issues very hard to debug.
 
+==== Default number mappings
+
+When a floating-point number is encountered, it is now dynamically mapped as a
+float by default instead of a double. The reasoning is that floats should be
+more than enough for most cases but would decrease storage requirements
+significantly.
+
+==== `_source`'s `format` option
+
+The `_source` mapping does not support the `format` option anymore. This option
+will still be accepted for indices created before the upgrade to 3.0 for backward
+compatibility, but it will have no effect. Indices created on or after 3.0 will
+reject this option.
+
 [[breaking_30_plugins]]
 === Plugin changes
 
@@ -237,6 +251,15 @@ Cloud AWS plugin has been split in two plugins:
 * {plugins}/discovery-ec2.html[Discovery EC2 plugin]
 * {plugins}/repository-s3.html[Repository S3 plugin]
 
+Proxy settings for both plugins have been renamed:
+
+* from `cloud.aws.proxy_host` to `cloud.aws.proxy.host`
+* from `cloud.aws.ec2.proxy_host` to `cloud.aws.ec2.proxy.host`
+* from `cloud.aws.s3.proxy_host` to `cloud.aws.s3.proxy.host`
+* from `cloud.aws.proxy_port` to `cloud.aws.proxy.port`
+* from `cloud.aws.ec2.proxy_port` to `cloud.aws.ec2.proxy.port`
+* from `cloud.aws.s3.proxy_port` to `cloud.aws.s3.proxy.port`
+
 ==== Cloud Azure plugin changes
 
 Cloud Azure plugin has been split in three plugins:
@@ -411,9 +434,9 @@ Use the `field(String, float)` method instead.
 
 ==== MissingQueryBuilder
 
-The two individual setters for existence() and nullValue() were removed in favour of
-optional constructor settings in order to better capture and validate their interdependent
-settings at construction time.
+The MissingQueryBuilder which was deprecated in 2.2.0 is removed. As a replacement use ExistsQueryBuilder
+inside a mustNot() clause. So instead of using `new ExistsQueryBuilder(name)` now use
+`new BoolQueryBuilder().mustNot(new ExistsQueryBuilder(name))`.
 
 ==== NotQueryBuilder
 
@@ -491,3 +514,4 @@ from `OsStats.Cpu#getPercent`.
 === Fields option
 Only stored fields are retrievable with this option.
 The fields option won't be able to load non stored fields from _source anymore.
+
diff --git a/docs/reference/modules/network.asciidoc b/docs/reference/modules/network.asciidoc
index 4572efe..5a71059 100644
--- a/docs/reference/modules/network.asciidoc
+++ b/docs/reference/modules/network.asciidoc
@@ -1,94 +1,175 @@
 [[modules-network]]
 == Network Settings
 
-There are several modules within a Node that use network based
-configuration, for example, the
-<<modules-transport,transport>> and
-<<modules-http,http>> modules. Node level
-network settings allows to set common settings that will be shared among
-all network based modules (unless explicitly overridden in each module).
+Elasticsearch binds to localhost only by default.  This is sufficient for you
+to run a local development server (or even a development cluster, if you start
+multiple nodes on the same machine), but you will need to configure some
+<<common-network-settings,basic network settings>> in order to run a real
+production cluster across multiple servers.
+
+[WARNING]
+.Be careful with the network configuration!
+=============================
+Never expose an unprotected node to the public internet.
+=============================
 
-Be careful with host configuration! Never expose an unprotected instance
-to the public internet.
+[float]
+[[common-network-settings]]
+=== Commonly Used Network Settings
+
+`network.host`::
+
+The node will bind to this hostname or IP address and _publish_ (advertise)
+this host to other nodes in the cluster. Accepts an IP address, hostname, or a
+<<network-interface-values,special value>>.
++
+Defaults to `_local_`.
+
+`discovery.zen.ping.unicast.hosts`::
+
+In order to join a cluster, a node needs to know the hostname or IP address of
+at least some of the other nodes in the cluster.  This settting provides the
+initial list of other nodes that this node will try to contact. Accepts IP
+addresses or hostnames.
++
+Defaults to `["127.0.0.1", "[::1]"]`.
+
+`http.port`::
+
+Port to bind to for incoming HTTP requests. Accepts a single value or a range.
+If a range is specified, the node will bind to the first available port in the
+range.
++
+Defaults to `9200-9300`.
+
+`transport.tcp.port`::
+
+Port to bind for communication between nodes. Accepts a single value or a
+range. If a range is specified, the node will bind to the first available port
+in the range.
++
+Defaults to `9300-9400`.
+
+[float]
+[[network-interface-values]]
+=== Special values for `network.host`
+
+The following special values may be passed to `network.host`:
+
+[horizontal]
+`_[networkInterface]_`::
+
+  Addresses of a network interface, for example `_en0_`.
 
-The `network.bind_host` setting allows to control the host different network
-components will bind on. By default, the bind host will be `_local_`
-(loopback addresses such as `127.0.0.1`, `::1`).
+`_local_`::
 
-The `network.publish_host` setting allows to control the host the node will
-publish itself within the cluster so other nodes will be able to connect to it.
-Currently an elasticsearch node may be bound to multiple addresses, but only
-publishes one.  If not specified, this defaults to the "best" address from 
-`network.bind_host`, sorted by IPv4/IPv6 stack preference, then by reachability.
+  Any loopback addresses on the system, for example `127.0.0.1`.
 
-The `network.host` setting is a simple setting to automatically set both
-`network.bind_host` and `network.publish_host` to the same host value.
+`_site_`::
 
-Both settings allows to be configured with either explicit host address(es)
-or host name(s). The settings also accept logical setting value(s) explained
-in the following table:
+  Any site-local addresses on the system, for example `192.168.0.1`.
 
-[cols="<,<",options="header",]
-|=======================================================================
-|Logical Host Setting Value |Description
-|`_local_` |Will be resolved to loopback addresses
+`_global_`::
 
-|`_local:ipv4_` |Will be resolved to loopback IPv4 addresses (e.g. 127.0.0.1)
+  Any globally-scoped addresses on the system, for example `8.8.8.8`.
 
-|`_local:ipv6_` |Will be resolved to loopback IPv6 addresses (e.g. ::1)
 
-|`_site_` |Will be resolved to site-local addresses ("private network")
+[float]
+==== IPv4 vs IPv6
 
-|`_site:ipv4_` |Will be resolved to site-local IPv4 addresses (e.g. 192.168.0.1)
+These special values will work over both IPv4 and IPv6 by default, but you can
+also limit this with the use of `:ipv4` of `:ipv6` specifiers. For example,
+`_en0:ipv4_` would only bind to the IPv4 addresses of interface `en0`.
 
-|`_site:ipv6_` |Will be resolved to site-local IPv6 addresses (e.g. fec0::1)
+[TIP]
+.Discovery in the cloud
+================================
 
-|`_global_` |Will be resolved to globally-scoped addresses ("publicly reachable")
+More special settings are available when running in the cloud with either the
+{plugins}/discovery-ec2-discovery.html#discovery-ec2-network-host[EC2 discovery plugin] or the
+{plugins}/discovery-gce-network-host.html#discovery-gce-network-host[Google Compute Engine discovery plugin]
+installed.
 
-|`_global:ipv4_` |Will be resolved to globally-scoped IPv4 addresses (e.g. 8.8.8.8)
+================================
 
-|`_global:ipv6_` |Will be resolved to globally-scoped IPv6 addresses (e.g. 2001:4860:4860::8888)
+[float]
+[[advanced-network-settings]]
+=== Advanced network settings
 
-|`_[networkInterface]_` |Resolves to the addresses of the provided
-network interface. For example `_en0_`.
+The `network.host` setting explained in <<common-network-settings,Commonly used network settings>>
+is a shortcut which sets the _bind host_ and the _publish host_ at the same
+time. In advanced used cases, such as when running behind a proxy server, you
+may need to set these settings to different values:
 
-|`_[networkInterface]:ipv4_` |Resolves to the ipv4 addresses of the
-provided network interface. For example `_en0:ipv4_`.
+`network.bind_host`::
 
-|`_[networkInterface]:ipv6_` |Resolves to the ipv6 addresses of the
-provided network interface. For example `_en0:ipv6_`.
-|=======================================================================
+This specifies which network interface(s) a node should bind to in order to
+listen for incoming requests.  A node can bind to multiple interfaces, e.g.
+two network cards, or a site-local address and a local address. Defaults to
+`network.host`.
 
-When the `discovery-ec2` plugin is installed, you can use
-{plugins}/discovery-ec2-discovery.html#discovery-ec2-network-host[ec2 specific host settings].
+`network.publish_host`::
 
-When the `discovery-gce` plugin is installed, you can use
-{plugins}/discovery-gce-network-host.html[gce specific host settings].
+The publish host is the single interface that the node advertises to other
+nodes in the cluster, so that those nodes can connect to it.   Currently an
+elasticsearch node may be bound to multiple addresses, but only publishes one.
+If not specified, this defaults to the ``best'' address from
+`network.bind_host`, sorted by IPv4/IPv6 stack preference, then by
+reachability.
 
+Both of the above settings can be configured just like `network.host` -- they
+accept IP addresses, host names, and
+<<network-interface-values,special values>>.
 
 [float]
 [[tcp-settings]]
-=== TCP Settings
+=== Advanced TCP Settings
+
+Any component that uses TCP (like the <<modules-http,HTTP>> and
+<<modules-transport,Transport>> modules) share the following settings:
+
+[horizontal]
+`network.tcp.no_delay`::
 
-Any component that uses TCP (like the HTTP, Transport and Memcached)
-share the following allowed settings:
+Enable or disable the https://en.wikipedia.org/wiki/Nagle%27s_algorithm[TCP no delay]
+setting. Defaults to `true`.
 
-[cols="<,<",options="header",]
-|=======================================================================
-|Setting |Description
-|`network.tcp.no_delay` |Enable or disable tcp no delay setting.
+`network.tcp.keep_alive`::
+
+Enable or disable https://en.wikipedia.org/wiki/Keepalive[TCP keep alive].
 Defaults to `true`.
 
-|`network.tcp.keep_alive` |Enable or disable tcp keep alive. Defaults
-to `true`.
+`network.tcp.reuse_address`::
+
+Should an address be reused or not. Defaults to `true` on non-windows
+machines.
+
+`network.tcp.send_buffer_size`::
+
+The size of the TCP send buffer (specified with <<size-units,size units>>).
+By default not explicitly set.
+
+`network.tcp.receive_buffer_size`::
+
+The size of the TCP receive buffer (specified with <<size-units,size units>>).
+By default not explicitly set.
+
+[float]
+=== Transport and HTTP protocols
+
+An Elasticsearch node exposes two network protocols which inherit the above
+settings, but may be further configured independently:
+
+TCP Transport::
 
-|`network.tcp.reuse_address` |Should an address be reused or not.
-Defaults to `true` on non-windows machines.
+Used for communication between nodes in the cluster and by the Java
+{javaclient}/node-client.html[Node client],
+{javaclient}/transport-client.html[Transport client], and by the
+<<modules-tribe,Tribe node>>.  See the <<modules-transport,Transport module>>
+for more information.
 
-|`network.tcp.send_buffer_size` |The size of the tcp send buffer size
-(in size setting format). By default not explicitly set.
+HTTP::
 
-|`network.tcp.receive_buffer_size` |The size of the tcp receive buffer
-size (in size setting format). By default not explicitly set.
-|=======================================================================
+Exposes the JSON-over-HTTP interface used by all clients other than the Java
+clients. See the <<modules-http,HTTP module>> for more information.
 
diff --git a/docs/reference/modules/snapshots.asciidoc b/docs/reference/modules/snapshots.asciidoc
index b6371dc..dbb9f39 100644
--- a/docs/reference/modules/snapshots.asciidoc
+++ b/docs/reference/modules/snapshots.asciidoc
@@ -45,6 +45,15 @@ which returns:
 }
 -----------------------------------
 
+Information about multiple repositories can be fetched in one go by using a comma-delimited list of repository names.
+Star wildcards are supported as well. For example, information about repositories that start with `repo` or that contain `backup`
+can be obtained using the following command:
+
+[source,js]
+-----------------------------------
+GET /_snapshot/repo*,*backup*
+-----------------------------------
+
 If a repository name is not specified, or `_all` is used as repository name Elasticsearch will return information about
 all repositories currently registered in the cluster:
 
@@ -251,6 +260,14 @@ GET /_snapshot/my_backup/snapshot_1
 -----------------------------------
 // AUTOSENSE
 
+Similar as for repositories, information about multiple snapshots can be queried in one go, supporting wildcards as well:
+
+[source,sh]
+-----------------------------------
+GET /_snapshot/my_backup/snapshot_*,some_other_snapshot
+-----------------------------------
+// AUTOSENSE
+
 All snapshots currently stored in the repository can be listed using the following command:
 
 [source,sh]
diff --git a/docs/reference/query-dsl/exists-query.asciidoc b/docs/reference/query-dsl/exists-query.asciidoc
index 0ae3bf2..404dce4 100644
--- a/docs/reference/query-dsl/exists-query.asciidoc
+++ b/docs/reference/query-dsl/exists-query.asciidoc
@@ -38,7 +38,7 @@ These documents would *not* match the above query:
 <3> The `user` field is missing completely.
 
 [float]
-===== `null_value` mapping
+==== `null_value` mapping
 
 If the field mapping includes the <<null-value,`null_value`>> setting
 then explicit `null` values are replaced with the specified `null_value`.  For
@@ -70,3 +70,21 @@ no values in the `user` field and thus would not match the `exists` filter:
 { "foo": "bar" }
 --------------------------------------------------
 
+==== `missing` query
+
+'missing' query has been removed because it can be advantageously replaced by an `exists` query inside a must_not
+clause as follows:
+
+[source,js]
+--------------------------------------------------
+"bool": {
+    "must_not": {
+        "exists": {
+            "field": "user"
+        }
+    }
+}
+--------------------------------------------------
+
+This query returns documents that have no value in the user field.
+
diff --git a/docs/reference/query-dsl/geo-shape-query.asciidoc b/docs/reference/query-dsl/geo-shape-query.asciidoc
index a6d1356..d389380 100644
--- a/docs/reference/query-dsl/geo-shape-query.asciidoc
+++ b/docs/reference/query-dsl/geo-shape-query.asciidoc
@@ -104,7 +104,10 @@ shape:
 
 ==== Spatial Relations
 
-The Query supports the following spatial relations:
+The <<spatial-strategy, geo_shape strategy>> mapping parameter determines
+which spatial relation operators may be used at search time.
+
+The following is a complete list of spatial relation operators available:
 
 * `INTERSECTS` - (default) Return all documents whose `geo_shape` field
 intersects the query geometry.
diff --git a/docs/reference/query-dsl/missing-query.asciidoc b/docs/reference/query-dsl/missing-query.asciidoc
deleted file mode 100644
index 648da06..0000000
--- a/docs/reference/query-dsl/missing-query.asciidoc
+++ /dev/null
@@ -1,132 +0,0 @@
-[[query-dsl-missing-query]]
-=== Missing Query
-
-Returns documents that have only `null` values or no value in the original field:
-
-[source,js]
---------------------------------------------------
-{
-    "constant_score" : {
-        "filter" : {
-            "missing" : { "field" : "user" }
-        }
-    }
-}
---------------------------------------------------
-
-For instance, the following docs would match the above filter:
-
-[source,js]
---------------------------------------------------
-{ "user": null }
-{ "user": [] } <1>
-{ "user": [null] } <2>
-{ "foo":  "bar" } <3>
---------------------------------------------------
-<1> This field has no values.
-<2> This field has no non-`null` values.
-<3> The `user` field is missing completely.
-
-These documents would *not* match the above filter:
-
-[source,js]
---------------------------------------------------
-{ "user": "jane" }
-{ "user": "" } <1>
-{ "user": "-" } <2>
-{ "user": ["jane"] }
-{ "user": ["jane", null ] } <3>
---------------------------------------------------
-<1> An empty string is a non-`null` value.
-<2> Even though the `standard` analyzer would emit zero tokens, the original field is non-`null`.
-<3> This field has one non-`null` value.
-
-[float]
-==== `null_value` mapping
-
-If the field mapping includes a <<null-value,`null_value`>> then explicit `null` values
-are replaced with the specified `null_value`.  For instance, if the `user` field were mapped
-as follows:
-
-[source,js]
---------------------------------------------------
-  "user": {
-    "type": "string",
-    "null_value": "_null_"
-  }
---------------------------------------------------
-
-then explicit `null` values would be indexed as the string `_null_`, and the
-the following docs would *not* match the `missing` filter:
-
-[source,js]
---------------------------------------------------
-{ "user": null }
-{ "user": [null] }
---------------------------------------------------
-
-However, these docs--without explicit `null` values--would still have
-no values in the `user` field and thus would match the `missing` filter:
-
-[source,js]
---------------------------------------------------
-{ "user": [] }
-{ "foo": "bar" }
---------------------------------------------------
-
-[float]
-===== `existence` and `null_value` parameters
-
-When the field being queried has a `null_value` mapping, then the behaviour of
-the `missing` filter can be altered with the `existence` and `null_value`
-parameters:
-
-[source,js]
---------------------------------------------------
-{
-    "constant_score" : {
-        "filter" : {
-            "missing" : {
-                "field" : "user",
-                "existence" : true,
-                "null_value" : false
-            }
-        }
-    }
-}
---------------------------------------------------
-
-
-`existence`::
-+
---
-When the `existence` parameter is set to `true` (the default), the missing
-filter will include documents where the field has *no* values, ie:
-
-[source,js]
---------------------------------------------------
-{ "user": [] }
-{ "foo": "bar" }
---------------------------------------------------
-
-When set to `false`, these documents will not be included.
---
-
-`null_value`::
-+
---
-When the `null_value` parameter is set to `true`, the missing
-filter will include documents where the field contains a `null` value, ie:
-
-[source,js]
---------------------------------------------------
-{ "user": null }
-{ "user": [null] }
-{ "user": ["jane",null] } <1>
---------------------------------------------------
-<1> Matches because the field contains a `null` value, even though it also contains a non-`null` value.
-
-When set to `false` (the default), these documents will not be included.
---
-
-NOTE: Either `existence` or `null_value` or both must be set to `true`.
diff --git a/docs/reference/query-dsl/term-level-queries.asciidoc b/docs/reference/query-dsl/term-level-queries.asciidoc
index 7e9f5e5..9c28a72 100644
--- a/docs/reference/query-dsl/term-level-queries.asciidoc
+++ b/docs/reference/query-dsl/term-level-queries.asciidoc
@@ -30,11 +30,6 @@ The queries in this group are:
 
     Find documents where the field specified contains any non-null value.
 
-<<query-dsl-missing-query,`missing` query>>::
-
-    Find documents where the field specified does is missing or contains only
-    `null` values.
-
 <<query-dsl-prefix-query,`prefix` query>>::
 
     Find documents where the field specified contains terms which being with
@@ -75,8 +70,6 @@ include::range-query.asciidoc[]
 
 include::exists-query.asciidoc[]
 
-include::missing-query.asciidoc[]
-
 include::prefix-query.asciidoc[]
 
 include::wildcard-query.asciidoc[]
@@ -88,6 +81,3 @@ include::fuzzy-query.asciidoc[]
 include::type-query.asciidoc[]
 
 include::ids-query.asciidoc[]
-
-
-
diff --git a/docs/reference/redirects.asciidoc b/docs/reference/redirects.asciidoc
index 6de6699..823bdb7 100644
--- a/docs/reference/redirects.asciidoc
+++ b/docs/reference/redirects.asciidoc
@@ -96,14 +96,6 @@ The `exists` filter has been replaced by the <<query-dsl-exists-query>>.  It beh
 as a query in ``query context'' and as a filter in ``filter context'' (see
 <<query-dsl>>).
 
-[role="exclude",id="query-dsl-missing-filter"]
-=== Missing Filter
-
-The `missing` filter has been replaced by the <<query-dsl-missing-query>>.  It behaves
-as a query in ``query context'' and as a filter in ``filter context'' (see
-<<query-dsl>>).
-
-
 [role="exclude",id="query-dsl-geo-bounding-box-filter"]
 === Geo Bounding Box Filter
 
@@ -451,4 +443,3 @@ The `not` query has been replaced by using a `mustNot` clause in a Boolean query
 === Nested type
 
 The docs for the `nested` field datatype have moved to <<nested>>.
-
diff --git a/docs/reference/search/suggesters/completion-suggest.asciidoc b/docs/reference/search/suggesters/completion-suggest.asciidoc
index 4aa0798..b0be107 100644
--- a/docs/reference/search/suggesters/completion-suggest.asciidoc
+++ b/docs/reference/search/suggesters/completion-suggest.asciidoc
@@ -92,7 +92,7 @@ PUT music/song/1?refresh=true
 The following parameters are supported:
 
 `input`::
-    The input to store, this can be a an array of strings or just
+    The input to store, this can be an array of strings or just
     a string. This field is mandatory.
 
 `weight`::
diff --git a/modules/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1715952.jar.sha1 b/modules/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index d81842d..0000000
--- a/modules/lang-expression/licenses/lucene-expressions-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-414dfcf600b6c02b90a21bd219a5e115bbda0d14
\ No newline at end of file
diff --git a/modules/lang-expression/licenses/lucene-expressions-5.5.0-snapshot-1719088.jar.sha1 b/modules/lang-expression/licenses/lucene-expressions-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..50bb58f
--- /dev/null
+++ b/modules/lang-expression/licenses/lucene-expressions-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+787356d4ae6142bb8ca7e9713d0a281a797b57fb
\ No newline at end of file
diff --git a/modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java b/modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java
index 48c7b4c..c72428c 100644
--- a/modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java
+++ b/modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionPlugin.java
@@ -19,38 +19,11 @@
 
 package org.elasticsearch.script.expression;
 
-import org.apache.lucene.expressions.js.JavascriptCompiler;
-import org.elasticsearch.SpecialPermission;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.ScriptModule;
 
-import java.security.AccessController;
-import java.security.PrivilegedAction;
-import java.text.ParseException;
-
 public class ExpressionPlugin extends Plugin {
     
-    // lucene expressions has crazy checks in its clinit for the functions map
-    // it violates rules of classloaders to detect accessibility
-    // TODO: clean that up
-    static {
-        SecurityManager sm = System.getSecurityManager();
-        if (sm != null) {
-            sm.checkPermission(new SpecialPermission());
-        }
-        AccessController.doPrivileged(new PrivilegedAction<Void>() {
-            @Override
-            public Void run() {
-                try {
-                    JavascriptCompiler.compile("0");
-                } catch (ParseException e) {
-                    throw new RuntimeException(e);
-                }
-                return null;
-            }
-        });
-    }
-
     @Override
     public String name() {
         return "lang-expression";
diff --git a/modules/lang-expression/src/main/plugin-metadata/plugin-security.policy b/modules/lang-expression/src/main/plugin-metadata/plugin-security.policy
index 9f50be3..c11af51 100644
--- a/modules/lang-expression/src/main/plugin-metadata/plugin-security.policy
+++ b/modules/lang-expression/src/main/plugin-metadata/plugin-security.policy
@@ -20,8 +20,6 @@
 grant {
   // needed to generate runtime classes
   permission java.lang.RuntimePermission "createClassLoader";
-  // needed because of security problems in JavascriptCompiler
-  permission java.lang.RuntimePermission "getClassLoader";
   
   // expression runtime
   permission org.elasticsearch.script.ClassPermission "java.lang.String";
diff --git a/modules/lang-groovy/src/main/plugin-metadata/plugin-security.policy b/modules/lang-groovy/src/main/plugin-metadata/plugin-security.policy
index 7de3e1a..e1fd920 100644
--- a/modules/lang-groovy/src/main/plugin-metadata/plugin-security.policy
+++ b/modules/lang-groovy/src/main/plugin-metadata/plugin-security.policy
@@ -23,6 +23,7 @@ grant {
   // needed by IndyInterface
   permission java.lang.RuntimePermission "getClassLoader";
   // needed by groovy engine
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
   // needed by GroovyScriptEngineService to close its classloader (why?)
   permission java.lang.RuntimePermission "closeClassLoader";
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
index 1362975..728a932 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
@@ -27,16 +27,13 @@ import org.elasticsearch.action.ActionModule;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.ActionRequestBuilder;
 import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
 import org.elasticsearch.action.get.GetRequest;
 import org.elasticsearch.action.index.IndexRequest;
-import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequest;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.support.ActionFilter;
 import org.elasticsearch.action.termvectors.MultiTermVectorsRequest;
@@ -47,8 +44,6 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.http.HttpServerTransport;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.GeoShapeQueryBuilder;
@@ -62,15 +57,8 @@ import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestController;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.script.Template;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders;
-import org.elasticsearch.search.suggest.Suggest;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
 import org.elasticsearch.test.rest.client.http.HttpRequestBuilder;
@@ -79,13 +67,10 @@ import org.junit.After;
 import org.junit.Before;
 
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
-import java.util.Map;
 import java.util.concurrent.CopyOnWriteArrayList;
 
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
@@ -284,172 +269,6 @@ public class ContextAndHeaderTransportTests extends ESIntegTestCase {
         assertRequestsContainHeader(PutIndexedScriptRequest.class);
     }
 
-    public void testThatIndexedScriptGetRequestInTemplateQueryContainsContextAndHeaders() throws Exception {
-        PutIndexedScriptResponse scriptResponse = transportClient()
-                .preparePutIndexedScript(
-                        MustacheScriptEngineService.NAME,
-                        "my_script",
-                        jsonBuilder().startObject().field("script", "{ \"match\": { \"name\": \"Star Wars\" }}").endObject()
-                                .string()).get();
-        assertThat(scriptResponse.isCreated(), is(true));
-
-        transportClient().prepareIndex(queryIndex, "type", "1")
-                .setSource(jsonBuilder().startObject().field("name", "Star Wars - The new republic").endObject()).get();
-        transportClient().admin().indices().prepareRefresh(queryIndex).get();
-
-        SearchResponse searchResponse = transportClient()
-                .prepareSearch(queryIndex)
-                .setQuery(
-                        QueryBuilders.templateQuery(new Template("my_script", ScriptType.INDEXED,
-                                MustacheScriptEngineService.NAME, null, null))).get();
-        assertNoFailures(searchResponse);
-        assertHitCount(searchResponse, 1);
-
-        assertGetRequestsContainHeaders(".scripts");
-        assertRequestsContainHeader(PutIndexedScriptRequest.class);
-    }
-
-    public void testThatIndexedScriptGetRequestInReducePhaseContainsContextAndHeaders() throws Exception {
-        PutIndexedScriptResponse scriptResponse = transportClient().preparePutIndexedScript(GroovyScriptEngineService.NAME, "my_script",
-                jsonBuilder().startObject().field("script", "_value0 * 10").endObject().string()).get();
-        assertThat(scriptResponse.isCreated(), is(true));
-
-        transportClient().prepareIndex(queryIndex, "type", "1")
-                .setSource(jsonBuilder().startObject().field("s_field", "foo").field("l_field", 10).endObject()).get();
-        transportClient().admin().indices().prepareRefresh(queryIndex).get();
-
-        SearchResponse searchResponse = transportClient()
-                .prepareSearch(queryIndex)
-                .addAggregation(
-                        AggregationBuilders
-                                .terms("terms")
-                                .field("s_field")
-                                .subAggregation(AggregationBuilders.max("max").field("l_field"))
-                                .subAggregation(
-                                        PipelineAggregatorBuilders.bucketScript("scripted").setBucketsPaths("max").script(
-                                                new Script("my_script", ScriptType.INDEXED, GroovyScriptEngineService.NAME, null)))).get();
-        assertNoFailures(searchResponse);
-        assertHitCount(searchResponse, 1);
-
-        assertGetRequestsContainHeaders(".scripts");
-        assertRequestsContainHeader(PutIndexedScriptRequest.class);
-    }
-
-    public void testThatSearchTemplatesWithIndexedTemplatesGetRequestContainsContextAndHeaders() throws Exception {
-        PutIndexedScriptResponse scriptResponse = transportClient().preparePutIndexedScript(MustacheScriptEngineService.NAME, "the_template",
-                jsonBuilder().startObject().startObject("template").startObject("query").startObject("match")
-                        .field("name", "{{query_string}}").endObject().endObject().endObject().endObject().string()
-        ).get();
-        assertThat(scriptResponse.isCreated(), is(true));
-
-        transportClient().prepareIndex(queryIndex, "type", "1")
-                .setSource(jsonBuilder().startObject().field("name", "Star Wars - The new republic").endObject())
-                .get();
-        transportClient().admin().indices().prepareRefresh(queryIndex).get();
-
-        Map<String, Object> params = new HashMap<>();
-        params.put("query_string", "star wars");
-
-        SearchResponse searchResponse = transportClient().prepareSearch(queryIndex).setTemplate(new Template("the_template", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, params))
-                .get();
-
-        assertNoFailures(searchResponse);
-        assertHitCount(searchResponse, 1);
-
-        assertGetRequestsContainHeaders(".scripts");
-        assertRequestsContainHeader(PutIndexedScriptRequest.class);
-    }
-
-    public void testThatIndexedScriptGetRequestInPhraseSuggestContainsContextAndHeaders() throws Exception {
-        CreateIndexRequestBuilder builder = transportClient().admin().indices().prepareCreate("test").setSettings(settingsBuilder()
-                .put(indexSettings())
-                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
-                .put("index.analysis.analyzer.text.tokenizer", "standard")
-                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
-                .put("index.analysis.filter.my_shingle.type", "shingle")
-                .put("index.analysis.filter.my_shingle.output_unigrams", true)
-                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
-                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
-
-        XContentBuilder mapping = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("type1")
-                .startObject("properties")
-                .startObject("title")
-                .field("type", "string")
-                .field("analyzer", "text")
-                .endObject()
-                .endObject()
-                .endObject()
-                .endObject();
-        assertAcked(builder.addMapping("type1", mapping));
-        ensureGreen();
-
-        List<String> titles = new ArrayList<>();
-
-        titles.add("United States House of Representatives Elections in Washington 2006");
-        titles.add("United States House of Representatives Elections in Washington 2005");
-        titles.add("State");
-        titles.add("Houses of Parliament");
-        titles.add("Representative Government");
-        titles.add("Election");
-
-        List<IndexRequestBuilder> builders = new ArrayList<>();
-        for (String title: titles) {
-            transportClient().prepareIndex("test", "type1").setSource("title", title).get();
-        }
-        transportClient().admin().indices().prepareRefresh("test").get();
-
-        String filterStringAsFilter = XContentFactory.jsonBuilder()
-                .startObject()
-                .startObject("match_phrase")
-                .field("title", "{{suggestion}}")
-                .endObject()
-                .endObject()
-                .string();
-
-        PutIndexedScriptResponse scriptResponse = transportClient()
-                .preparePutIndexedScript(
-                        MustacheScriptEngineService.NAME,
-                        "my_script",
-                jsonBuilder().startObject().field("script", filterStringAsFilter).endObject()
-                                .string()).get();
-        assertThat(scriptResponse.isCreated(), is(true));
-
-        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
-                .field("title")
-                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
-                        .suggestMode("always")
-                        .maxTermFreq(.99f)
-                        .size(10)
-                        .maxInspections(200)
-                )
-                .confidence(0f)
-                .maxErrors(2f)
-                .shardSize(30000)
-                .size(10);
-
-        PhraseSuggestionBuilder filteredFilterSuggest = suggest.collateQuery(new Template("my_script", ScriptType.INDEXED,
-                MustacheScriptEngineService.NAME, null, null));
-
-        SearchRequestBuilder searchRequestBuilder = transportClient().prepareSearch("test").setSize(0);
-        SuggestBuilder suggestBuilder = new SuggestBuilder();
-        String suggestText = "united states house of representatives elections in washington 2006";
-        if (suggestText != null) {
-            suggestBuilder.setText(suggestText);
-        }
-        suggestBuilder.addSuggestion(filteredFilterSuggest);
-        searchRequestBuilder.suggest(suggestBuilder);
-        SearchResponse actionGet = searchRequestBuilder.execute().actionGet();
-        assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(0));
-        Suggest searchSuggest = actionGet.getSuggest();
-
-        assertSuggestionSize(searchSuggest, 0, 2, "title");
-
-        assertGetRequestsContainHeaders(".scripts");
-        assertRequestsContainHeader(PutIndexedScriptRequest.class);
-    }
-
     public void testThatRelevantHttpHeadersBecomeRequestHeaders() throws Exception {
         String releventHeaderName = "relevant_" + randomHeaderKey;
         for (RestController restController : internalCluster().getDataNodeInstances(RestController.class)) {
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java
index 4291f00..66a764d 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndicesRequestTests.java
@@ -178,7 +178,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testIndex() {
-        String[] indexShardActions = new String[]{IndexAction.NAME, IndexAction.NAME + "[r]"};
+        String[] indexShardActions = new String[]{IndexAction.NAME, IndexAction.NAME + "[p]", IndexAction.NAME + "[r]"};
         interceptTransportActions(indexShardActions);
 
         IndexRequest indexRequest = new IndexRequest(randomIndexOrAlias(), "type", "id").source("field", "value");
@@ -189,7 +189,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testDelete() {
-        String[] deleteShardActions = new String[]{DeleteAction.NAME, DeleteAction.NAME + "[r]"};
+        String[] deleteShardActions = new String[]{DeleteAction.NAME, DeleteAction.NAME + "[p]", DeleteAction.NAME + "[r]"};
         interceptTransportActions(deleteShardActions);
 
         DeleteRequest deleteRequest = new DeleteRequest(randomIndexOrAlias(), "type", "id");
@@ -244,7 +244,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testBulk() {
-        String[] bulkShardActions = new String[]{BulkAction.NAME + "[s]", BulkAction.NAME + "[s][r]"};
+        String[] bulkShardActions = new String[]{BulkAction.NAME + "[s][p]", BulkAction.NAME + "[s][r]"};
         interceptTransportActions(bulkShardActions);
 
         List<String> indices = new ArrayList<>();
@@ -344,7 +344,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testFlush() {
-        String[] indexShardActions = new String[]{TransportShardFlushAction.NAME + "[r]", TransportShardFlushAction.NAME};
+        String[] indexShardActions = new String[]{TransportShardFlushAction.NAME, TransportShardFlushAction.NAME + "[r]", TransportShardFlushAction.NAME + "[p]"};
         interceptTransportActions(indexShardActions);
 
         FlushRequest flushRequest = new FlushRequest(randomIndicesOrAliases());
@@ -367,7 +367,7 @@ public class IndicesRequestTests extends ESIntegTestCase {
     }
 
     public void testRefresh() {
-        String[] indexShardActions = new String[]{TransportShardRefreshAction.NAME + "[r]", TransportShardRefreshAction.NAME};
+        String[] indexShardActions = new String[]{TransportShardRefreshAction.NAME, TransportShardRefreshAction.NAME + "[r]", TransportShardRefreshAction.NAME + "[p]"};
         interceptTransportActions(indexShardActions);
 
         RefreshRequest refreshRequest = new RefreshRequest(randomIndicesOrAliases());
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
index 5a00bca..db8a13c 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
@@ -48,58 +48,24 @@ import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.FieldSortBuilder;
-import org.elasticsearch.search.sort.GeoDistanceSortBuilder;
-import org.elasticsearch.search.sort.ScriptSortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
+import org.elasticsearch.search.sort.*;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.VersionUtils;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.hamcrest.Matchers;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Locale;
+import java.util.*;
 import java.util.Map.Entry;
-import java.util.Random;
-import java.util.Set;
-import java.util.TreeMap;
 import java.util.concurrent.ExecutionException;
 
+import static org.apache.lucene.util.GeoUtils.TOLERANCE;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.fieldValueFactorFunction;
 import static org.elasticsearch.search.sort.SortBuilders.fieldSort;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertOrderedSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSortValues;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.lessThan;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-
-import static org.apache.lucene.util.GeoUtils.TOLERANCE;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -503,7 +469,7 @@ public class SimpleSortTests extends ESIntegTestCase {
     }
 
     public void testSimpleSorts() throws Exception {
-        Random random = getRandom();
+        Random random = random();
         assertAcked(prepareCreate("test")
                 .addMapping("type1", XContentFactory.jsonBuilder().startObject().startObject("type1").startObject("properties")
                         .startObject("str_value").field("type", "string").field("index", "not_analyzed").startObject("fielddata").field("format", random().nextBoolean() ? "doc_values" : null).endObject().endObject()
diff --git a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
index af27047..adf3492 100644
--- a/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
+++ b/modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
@@ -42,6 +42,7 @@
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/metrics/CardinalityIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/CardinalityTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ChildQuerySearchTests.java
   renamed:    core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+  ^^^^^ note: the methods from this test using mustache were moved to the mustache module under its messy tests package.
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateHistogramTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DateRangeTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/DoubleTermsTests.java
diff --git a/modules/lang-mustache/build.gradle b/modules/lang-mustache/build.gradle
new file mode 100644
index 0000000..4e8e9cc
--- /dev/null
+++ b/modules/lang-mustache/build.gradle
@@ -0,0 +1,36 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+esplugin {
+  description 'Mustache scripting integration for Elasticsearch'
+  classname 'org.elasticsearch.script.mustache.MustachePlugin'
+}
+
+dependencies {
+  compile "com.github.spullara.mustache.java:compiler:0.9.1"
+}
+
+compileTestJava.options.compilerArgs << '-Xlint:-rawtypes,-unchecked'
+
+integTest {
+  cluster {
+    systemProperty 'es.script.inline', 'on'
+    systemProperty 'es.script.indexed', 'on'
+  }
+}
diff --git a/modules/lang-mustache/licenses/compiler-0.9.1.jar.sha1 b/modules/lang-mustache/licenses/compiler-0.9.1.jar.sha1
new file mode 100644
index 0000000..96152e0
--- /dev/null
+++ b/modules/lang-mustache/licenses/compiler-0.9.1.jar.sha1
@@ -0,0 +1 @@
+14aec5344639782ee76441401b773946c65eb2b3
diff --git a/modules/lang-mustache/licenses/compiler-LICENSE.txt b/modules/lang-mustache/licenses/compiler-LICENSE.txt
new file mode 100644
index 0000000..ac68303
--- /dev/null
+++ b/modules/lang-mustache/licenses/compiler-LICENSE.txt
@@ -0,0 +1,14 @@
+Copyright 2010 RightTime, Inc.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
diff --git a/modules/lang-mustache/licenses/compiler-NOTICE.txt b/modules/lang-mustache/licenses/compiler-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/modules/lang-mustache/licenses/compiler-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
new file mode 100644
index 0000000..7734d03
--- /dev/null
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import com.fasterxml.jackson.core.io.JsonStringEncoder;
+import com.github.mustachejava.DefaultMustacheFactory;
+import com.github.mustachejava.MustacheException;
+
+import java.io.IOException;
+import java.io.Writer;
+
+/**
+ * A MustacheFactory that does simple JSON escaping.
+ */
+public final class JsonEscapingMustacheFactory extends DefaultMustacheFactory {
+    
+    @Override
+    public void encode(String value, Writer writer) {
+        try {
+            JsonStringEncoder utils = new JsonStringEncoder();
+            writer.write(utils.quoteAsString(value));;
+        } catch (IOException e) {
+            throw new MustacheException("Failed to encode value: " + value);
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java
new file mode 100644
index 0000000..3f6f6e0
--- /dev/null
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustachePlugin.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.mustache;
+
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptModule;
+
+public class MustachePlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "lang-mustache";
+    }
+
+    @Override
+    public String description() {
+        return "Mustache scripting integration for Elasticsearch";
+    }
+
+    public void onModule(ScriptModule module) {
+        module.addScriptEngine(MustacheScriptEngineService.class);
+    }
+}
diff --git a/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
new file mode 100644
index 0000000..9317205
--- /dev/null
+++ b/modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
@@ -0,0 +1,184 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import com.github.mustachejava.Mustache;
+
+import org.elasticsearch.SpecialPermission;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.io.FastStringReader;
+import org.elasticsearch.common.io.UTF8StreamWriter;
+import org.elasticsearch.common.io.stream.BytesStreamOutput;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptEngineService;
+import org.elasticsearch.script.ScriptException;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
+
+import java.lang.ref.SoftReference;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
+import java.util.Collections;
+import java.util.Map;
+
+/**
+ * Main entry point handling template registration, compilation and
+ * execution.
+ *
+ * Template handling is based on Mustache. Template handling is a two step
+ * process: First compile the string representing the template, the resulting
+ * {@link Mustache} object can then be re-used for subsequent executions.
+ */
+public class MustacheScriptEngineService extends AbstractComponent implements ScriptEngineService {
+
+    public static final String NAME = "mustache";
+
+    /** Thread local UTF8StreamWriter to store template execution results in, thread local to save object creation.*/
+    private static ThreadLocal<SoftReference<UTF8StreamWriter>> utf8StreamWriter = new ThreadLocal<>();
+
+    /** If exists, reset and return, otherwise create, reset and return a writer.*/
+    private static UTF8StreamWriter utf8StreamWriter() {
+        SoftReference<UTF8StreamWriter> ref = utf8StreamWriter.get();
+        UTF8StreamWriter writer = (ref == null) ? null : ref.get();
+        if (writer == null) {
+            writer = new UTF8StreamWriter(1024 * 4);
+            utf8StreamWriter.set(new SoftReference<>(writer));
+        }
+        writer.reset();
+        return writer;
+    }
+
+    /**
+     * @param settings automatically wired by Guice.
+     * */
+    @Inject
+    public MustacheScriptEngineService(Settings settings) {
+        super(settings);
+    }
+
+    /**
+     * Compile a template string to (in this case) a Mustache object than can
+     * later be re-used for execution to fill in missing parameter values.
+     *
+     * @param template
+     *            a string representing the template to compile.
+     * @return a compiled template object for later execution.
+     * */
+    @Override
+    public Object compile(String template) {
+        /** Factory to generate Mustache objects from. */
+        return (new JsonEscapingMustacheFactory()).compile(new FastStringReader(template), "query-template");
+    }
+
+    @Override
+    public String[] types() {
+        return new String[] {NAME};
+    }
+
+    @Override
+    public String[] extensions() {
+        return new String[] {NAME};
+    }
+
+    @Override
+    public boolean sandboxed() {
+        return true;
+    }
+
+    @Override
+    public ExecutableScript executable(CompiledScript compiledScript,
+            @Nullable Map<String, Object> vars) {
+        return new MustacheExecutableScript(compiledScript, vars);
+    }
+
+    @Override
+    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup,
+            @Nullable Map<String, Object> vars) {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public void close() {
+        // Nothing to do here
+    }
+
+    @Override
+    public void scriptRemoved(CompiledScript script) {
+        // Nothing to do here
+    }
+
+    // permission checked before doing crazy reflection
+    static final SpecialPermission SPECIAL_PERMISSION = new SpecialPermission();
+
+    /**
+     * Used at query execution time by script service in order to execute a query template.
+     * */
+    private class MustacheExecutableScript implements ExecutableScript {
+        /** Compiled template object wrapper. */
+        private CompiledScript template;
+        /** Parameters to fill above object with. */
+        private Map<String, Object> vars;
+
+        /**
+         * @param template the compiled template object wrapper
+         * @param vars the parameters to fill above object with
+         **/
+        public MustacheExecutableScript(CompiledScript template, Map<String, Object> vars) {
+            this.template = template;
+            this.vars = vars == null ? Collections.<String, Object>emptyMap() : vars;
+        }
+
+        @Override
+        public void setNextVar(String name, Object value) {
+            this.vars.put(name, value);
+        }
+
+        @Override
+        public Object run() {
+            final BytesStreamOutput result = new BytesStreamOutput();
+            try (UTF8StreamWriter writer = utf8StreamWriter().setOutput(result)) {
+                // crazy reflection here
+                SecurityManager sm = System.getSecurityManager();
+                if (sm != null) {
+                    sm.checkPermission(SPECIAL_PERMISSION);
+                }
+                AccessController.doPrivileged(new PrivilegedAction<Void>() {
+                    @Override
+                    public Void run() {
+                        ((Mustache) template.compiled()).execute(writer, vars);
+                        return null;
+                    }
+                });
+            } catch (Exception e) {
+                logger.error("Error running " + template, e);
+                throw new ScriptException("Error running " + template, e);
+            }
+            return result.bytes();
+        }
+
+        @Override
+        public Object unwrap(Object value) {
+            return value;
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/main/plugin-metadata/plugin-security.policy b/modules/lang-mustache/src/main/plugin-metadata/plugin-security.policy
new file mode 100644
index 0000000..ea2db55
--- /dev/null
+++ b/modules/lang-mustache/src/main/plugin-metadata/plugin-security.policy
@@ -0,0 +1,23 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+grant {
+  // needed to do crazy reflection
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
+};
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
new file mode 100644
index 0000000..92d1533
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
@@ -0,0 +1,389 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.messy.tests;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionModule;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
+import org.elasticsearch.action.admin.indices.refresh.RefreshRequest;
+import org.elasticsearch.action.get.GetRequest;
+import org.elasticsearch.action.index.IndexRequest;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequest;
+import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
+import org.elasticsearch.action.search.SearchRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.action.support.ActionFilter;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.client.FilterClient;
+import org.elasticsearch.common.inject.AbstractModule;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.inject.Module;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustachePlugin;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.search.suggest.Suggest;
+import org.elasticsearch.search.suggest.SuggestBuilder;
+import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.junit.After;
+import org.junit.Before;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.concurrent.CopyOnWriteArrayList;
+
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.node.Node.HTTP_ENABLED;
+import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
+import static org.elasticsearch.test.ESIntegTestCase.Scope.SUITE;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.is;
+
+@ClusterScope(scope = SUITE)
+public class ContextAndHeaderTransportTests extends ESIntegTestCase {
+    private static final List<ActionRequest> requests =  new CopyOnWriteArrayList<>();
+    private String randomHeaderKey = randomAsciiOfLength(10);
+    private String randomHeaderValue = randomAsciiOfLength(20);
+    private String queryIndex = "query-" + randomAsciiOfLength(10).toLowerCase(Locale.ROOT);
+    private String lookupIndex = "lookup-" + randomAsciiOfLength(10).toLowerCase(Locale.ROOT);
+
+    @Override
+    protected Settings nodeSettings(int nodeOrdinal) {
+        return settingsBuilder()
+                .put(super.nodeSettings(nodeOrdinal))
+                .put("script.indexed", "on")
+                .put(HTTP_ENABLED, true)
+                .build();
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(ActionLoggingPlugin.class, MustachePlugin.class);
+    }
+
+    @Before
+    public void createIndices() throws Exception {
+        String mapping = jsonBuilder().startObject().startObject("type")
+                .startObject("properties")
+                .startObject("location").field("type", "geo_shape").endObject()
+                .startObject("name").field("type", "string").endObject()
+                .endObject()
+                .endObject().endObject().string();
+
+        Settings settings = settingsBuilder()
+                .put(indexSettings())
+                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
+                .build();
+        assertAcked(transportClient().admin().indices().prepareCreate(lookupIndex)
+                .setSettings(settings).addMapping("type", mapping));
+        assertAcked(transportClient().admin().indices().prepareCreate(queryIndex)
+                .setSettings(settings).addMapping("type", mapping));
+        ensureGreen(queryIndex, lookupIndex);
+
+        requests.clear();
+    }
+
+    @After
+    public void checkAllRequestsContainHeaders() {
+        assertRequestsContainHeader(IndexRequest.class);
+        assertRequestsContainHeader(RefreshRequest.class);
+    }
+
+    public void testThatIndexedScriptGetRequestInTemplateQueryContainsContextAndHeaders() throws Exception {
+        PutIndexedScriptResponse scriptResponse = transportClient()
+                .preparePutIndexedScript(
+                        MustacheScriptEngineService.NAME,
+                        "my_script",
+                        jsonBuilder().startObject().field("script", "{ \"match\": { \"name\": \"Star Wars\" }}").endObject()
+                                .string()).get();
+        assertThat(scriptResponse.isCreated(), is(true));
+
+        transportClient().prepareIndex(queryIndex, "type", "1")
+                .setSource(jsonBuilder().startObject().field("name", "Star Wars - The new republic").endObject()).get();
+        transportClient().admin().indices().prepareRefresh(queryIndex).get();
+
+        SearchResponse searchResponse = transportClient()
+                .prepareSearch(queryIndex)
+                .setQuery(
+                        QueryBuilders.templateQuery(new Template("my_script", ScriptType.INDEXED,
+                                MustacheScriptEngineService.NAME, null, null))).get();
+        assertNoFailures(searchResponse);
+        assertHitCount(searchResponse, 1);
+
+        assertGetRequestsContainHeaders(".scripts");
+        assertRequestsContainHeader(PutIndexedScriptRequest.class);
+    }
+
+    public void testThatSearchTemplatesWithIndexedTemplatesGetRequestContainsContextAndHeaders() throws Exception {
+        PutIndexedScriptResponse scriptResponse = transportClient().preparePutIndexedScript(MustacheScriptEngineService.NAME, "the_template",
+                jsonBuilder().startObject().startObject("template").startObject("query").startObject("match")
+                        .field("name", "{{query_string}}").endObject().endObject().endObject().endObject().string()
+        ).get();
+        assertThat(scriptResponse.isCreated(), is(true));
+
+        transportClient().prepareIndex(queryIndex, "type", "1")
+                .setSource(jsonBuilder().startObject().field("name", "Star Wars - The new republic").endObject())
+                .get();
+        transportClient().admin().indices().prepareRefresh(queryIndex).get();
+
+        Map<String, Object> params = new HashMap<>();
+        params.put("query_string", "star wars");
+
+        SearchResponse searchResponse = transportClient().prepareSearch(queryIndex).setTemplate(new Template("the_template", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, params))
+                .get();
+
+        assertNoFailures(searchResponse);
+        assertHitCount(searchResponse, 1);
+
+        assertGetRequestsContainHeaders(".scripts");
+        assertRequestsContainHeader(PutIndexedScriptRequest.class);
+    }
+
+    public void testThatIndexedScriptGetRequestInPhraseSuggestContainsContextAndHeaders() throws Exception {
+        CreateIndexRequestBuilder builder = transportClient().admin().indices().prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
+                .put("index.analysis.analyzer.text.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("type1")
+                .startObject("properties")
+                .startObject("title")
+                .field("type", "string")
+                .field("analyzer", "text")
+                .endObject()
+                .endObject()
+                .endObject()
+                .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        List<String> titles = new ArrayList<>();
+
+        titles.add("United States House of Representatives Elections in Washington 2006");
+        titles.add("United States House of Representatives Elections in Washington 2005");
+        titles.add("State");
+        titles.add("Houses of Parliament");
+        titles.add("Representative Government");
+        titles.add("Election");
+
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (String title: titles) {
+            transportClient().prepareIndex("test", "type1").setSource("title", title).get();
+        }
+        transportClient().admin().indices().prepareRefresh("test").get();
+
+        String filterStringAsFilter = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("match_phrase")
+                .field("title", "{{suggestion}}")
+                .endObject()
+                .endObject()
+                .string();
+
+        PutIndexedScriptResponse scriptResponse = transportClient()
+                .preparePutIndexedScript(
+                        MustacheScriptEngineService.NAME,
+                        "my_script",
+                jsonBuilder().startObject().field("script", filterStringAsFilter).endObject()
+                                .string()).get();
+        assertThat(scriptResponse.isCreated(), is(true));
+
+        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
+                .field("title")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
+                        .suggestMode("always")
+                        .maxTermFreq(.99f)
+                        .size(10)
+                        .maxInspections(200)
+                )
+                .confidence(0f)
+                .maxErrors(2f)
+                .shardSize(30000)
+                .size(10);
+
+        PhraseSuggestionBuilder filteredFilterSuggest = suggest.collateQuery(new Template("my_script", ScriptType.INDEXED,
+                MustacheScriptEngineService.NAME, null, null));
+
+        SearchRequestBuilder searchRequestBuilder = transportClient().prepareSearch("test").setSize(0);
+        SuggestBuilder suggestBuilder = new SuggestBuilder();
+        String suggestText = "united states house of representatives elections in washington 2006";
+        if (suggestText != null) {
+            suggestBuilder.setText(suggestText);
+        }
+        suggestBuilder.addSuggestion(filteredFilterSuggest);
+        searchRequestBuilder.suggest(suggestBuilder);
+        SearchResponse actionGet = searchRequestBuilder.execute().actionGet();
+        assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(0));
+        Suggest searchSuggest = actionGet.getSuggest();
+
+        assertSuggestionSize(searchSuggest, 0, 2, "title");
+
+        assertGetRequestsContainHeaders(".scripts");
+        assertRequestsContainHeader(PutIndexedScriptRequest.class);
+    }
+
+    private <T> List<T> getRequests(Class<T> clazz) {
+        List<T> results = new ArrayList<>();
+        for (ActionRequest request : requests) {
+            if (request.getClass().equals(clazz)) {
+                results.add((T) request);
+            }
+        }
+
+        return results;
+    }
+
+    private void assertRequestsContainHeader(Class<? extends ActionRequest> clazz) {
+        List<? extends ActionRequest> classRequests = getRequests(clazz);
+        for (ActionRequest request : classRequests) {
+            assertRequestContainsHeader(request);
+        }
+    }
+
+    private void assertGetRequestsContainHeaders() {
+        assertGetRequestsContainHeaders(this.lookupIndex);
+    }
+
+    private void assertGetRequestsContainHeaders(String index) {
+        List<GetRequest> getRequests = getRequests(GetRequest.class);
+        assertThat(getRequests, hasSize(greaterThan(0)));
+
+        for (GetRequest request : getRequests) {
+            if (!request.index().equals(index)) {
+                continue;
+            }
+            assertRequestContainsHeader(request);
+        }
+    }
+
+    private void assertRequestContainsHeader(ActionRequest request) {
+        String msg = String.format(Locale.ROOT, "Expected header %s to be in request %s", randomHeaderKey, request.getClass().getName());
+        if (request instanceof IndexRequest) {
+            IndexRequest indexRequest = (IndexRequest) request;
+            msg = String.format(Locale.ROOT, "Expected header %s to be in index request %s/%s/%s", randomHeaderKey,
+                    indexRequest.index(), indexRequest.type(), indexRequest.id());
+        }
+        assertThat(msg, request.hasHeader(randomHeaderKey), is(true));
+        assertThat(request.getHeader(randomHeaderKey).toString(), is(randomHeaderValue));
+    }
+
+    /**
+     * a transport client that adds our random header
+     */
+    private Client transportClient() {
+        Client transportClient = internalCluster().transportClient();
+        FilterClient filterClient = new FilterClient(transportClient) {
+            @Override
+            protected <Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>> void doExecute(Action<Request, Response, RequestBuilder> action, Request request, ActionListener<Response> listener) {
+                request.putHeader(randomHeaderKey, randomHeaderValue);
+                super.doExecute(action, request, listener);
+            }
+        };
+
+        return filterClient;
+    }
+
+    public static class ActionLoggingPlugin extends Plugin {
+
+        @Override
+        public String name() {
+            return "test-action-logging";
+        }
+
+        @Override
+        public String description() {
+            return "Test action logging";
+        }
+
+        @Override
+        public Collection<Module> nodeModules() {
+            return Collections.<Module>singletonList(new ActionLoggingModule());
+        }
+
+        public void onModule(ActionModule module) {
+            module.registerFilter(LoggingFilter.class);
+        }
+    }
+
+    public static class ActionLoggingModule extends AbstractModule {
+        @Override
+        protected void configure() {
+            bind(LoggingFilter.class).asEagerSingleton();
+        }
+
+    }
+
+    public static class LoggingFilter extends ActionFilter.Simple {
+
+        @Inject
+        public LoggingFilter(Settings settings) {
+            super(settings);
+        }
+
+        @Override
+        public int order() {
+            return 999;
+        }
+
+        @Override
+        protected boolean apply(String action, ActionRequest request, ActionListener listener) {
+            requests.add(request);
+            return true;
+        }
+
+        @Override
+        protected boolean apply(String action, ActionResponse response, ActionListener listener) {
+            return true;
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
new file mode 100644
index 0000000..87cc51c
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
@@ -0,0 +1,162 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.messy.tests;
+
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.FileSystemUtils;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustachePlugin;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.rest.support.FileUtils;
+
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.notNullValue;
+
+@ESIntegTestCase.SuiteScopeTestCase
+public class RenderSearchTemplateTests extends ESIntegTestCase {
+    private static final String TEMPLATE_CONTENTS = "{\"size\":\"{{size}}\",\"query\":{\"match\":{\"foo\":\"{{value}}\"}},\"aggs\":{\"objects\":{\"terms\":{\"field\":\"{{value}}\",\"size\":\"{{size}}\"}}}}";
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Collections.singleton(MustachePlugin.class);
+    }
+
+    @Override
+    protected void setupSuiteScopeCluster() throws Exception {
+        client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "index_template_1", "{ \"template\": " + TEMPLATE_CONTENTS + " }").get();
+    }
+
+    @Override
+    public Settings nodeSettings(int nodeOrdinal) {
+        Path configDir = createTempDir();
+        Path scriptsDir = configDir.resolve("scripts");
+        try {
+            Files.createDirectories(scriptsDir);
+            Files.write(scriptsDir.resolve("file_template_1.mustache"), TEMPLATE_CONTENTS.getBytes("UTF-8"));
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
+                .put("path.conf", configDir).build();
+    }
+
+    public void testInlineTemplate() {
+        Map<String, Object> params = new HashMap<>();
+        params.put("value", "bar");
+        params.put("size", 20);
+        Template template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        BytesReference source = response.source();
+        assertThat(source, notNullValue());
+        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        assertThat(sourceAsMap, notNullValue());
+        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
+        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+
+        params = new HashMap<>();
+        params.put("value", "baz");
+        params.put("size", 100);
+        template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        source = response.source();
+        assertThat(source, notNullValue());
+        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
+        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+    }
+
+    public void testIndexedTemplate() {
+        Map<String, Object> params = new HashMap<>();
+        params.put("value", "bar");
+        params.put("size", 20);
+        Template template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        BytesReference source = response.source();
+        assertThat(source, notNullValue());
+        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        assertThat(sourceAsMap, notNullValue());
+        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
+        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+
+        params = new HashMap<>();
+        params.put("value", "baz");
+        params.put("size", 100);
+        template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        source = response.source();
+        assertThat(source, notNullValue());
+        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
+        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+    }
+
+    public void testFileTemplate() {
+        Map<String, Object> params = new HashMap<>();
+        params.put("value", "bar");
+        params.put("size", 20);
+        Template template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        BytesReference source = response.source();
+        assertThat(source, notNullValue());
+        Map<String, Object> sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        assertThat(sourceAsMap, notNullValue());
+        String expected = TEMPLATE_CONTENTS.replace("{{value}}", "bar").replace("{{size}}", "20");
+        Map<String, Object> expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+
+        params = new HashMap<>();
+        params.put("value", "baz");
+        params.put("size", 100);
+        template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
+        assertThat(response, notNullValue());
+        source = response.source();
+        assertThat(source, notNullValue());
+        sourceAsMap = XContentHelper.convertToMap(source, false).v2();
+        expected = TEMPLATE_CONTENTS.replace("{{value}}", "baz").replace("{{size}}", "100");
+        expectedMap = XContentHelper.convertToMap(new BytesArray(expected), false).v2();
+        assertThat(sourceAsMap, equalTo(expectedMap));
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java
new file mode 100644
index 0000000..a0699a3
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java
@@ -0,0 +1,1302 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.messy.tests;
+
+
+import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.ReduceSearchPhaseException;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
+import org.elasticsearch.action.search.SearchRequestBuilder;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.action.suggest.SuggestRequestBuilder;
+import org.elasticsearch.action.suggest.SuggestResponse;
+import org.elasticsearch.common.io.PathUtils;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.mustache.MustachePlugin;
+import org.elasticsearch.search.suggest.Suggest;
+import org.elasticsearch.search.suggest.SuggestBuilder;
+import org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder;
+import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
+import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.DirectCandidateGenerator;
+import org.elasticsearch.search.suggest.term.TermSuggestionBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.hamcrest.ElasticsearchAssertions;
+
+import java.io.IOException;
+import java.net.URISyntaxException;
+import java.nio.charset.StandardCharsets;
+import java.nio.file.Files;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ExecutionException;
+
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
+import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
+import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
+import static org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.candidateGenerator;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestion;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionPhraseCollateMatchExists;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
+import static org.hamcrest.Matchers.anyOf;
+import static org.hamcrest.Matchers.endsWith;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.nullValue;
+
+/**
+ * Integration tests for term and phrase suggestions.  Many of these tests many requests that vary only slightly from one another.  Where
+ * possible these tests should declare for the first request, make the request, modify the configuration for the next request, make that
+ * request, modify again, request again, etc.  This makes it very obvious what changes between requests.
+ */
+public class SuggestSearchTests extends ESIntegTestCase {
+    
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Collections.singleton(MustachePlugin.class);
+    }
+    
+    // see #3196
+    public void testSuggestAcrossMultipleIndices() throws IOException {
+        createIndex("test");
+        ensureGreen();
+
+        index("test", "type1", "1", "text", "abcd");
+        index("test", "type1", "2", "text", "aacd");
+        index("test", "type1", "3", "text", "abbd");
+        index("test", "type1", "4", "text", "abcc");
+        refresh();
+
+        TermSuggestionBuilder termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("abcd")
+                .field("text");
+        logger.info("--> run suggestions with one index");
+        searchSuggest( termSuggest);
+        createIndex("test_1");
+        ensureGreen();
+
+        index("test_1", "type1", "1", "text", "ab cd");
+        index("test_1", "type1", "2", "text", "aa cd");
+        index("test_1", "type1", "3", "text", "ab bd");
+        index("test_1", "type1", "4", "text", "ab cc");
+        refresh();
+        termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("ab cd")
+                .minWordLength(1)
+                .field("text");
+        logger.info("--> run suggestions with two indices");
+        searchSuggest( termSuggest);
+
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("properties")
+                .startObject("text").field("type", "string").field("analyzer", "keyword").endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(prepareCreate("test_2").addMapping("type1", mapping));
+        ensureGreen();
+
+        index("test_2", "type1", "1", "text", "ab cd");
+        index("test_2", "type1", "2", "text", "aa cd");
+        index("test_2", "type1", "3", "text", "ab bd");
+        index("test_2", "type1", "4", "text", "ab cc");
+        index("test_2", "type1", "1", "text", "abcd");
+        index("test_2", "type1", "2", "text", "aacd");
+        index("test_2", "type1", "3", "text", "abbd");
+        index("test_2", "type1", "4", "text", "abcc");
+        refresh();
+
+        termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("ab cd")
+                .minWordLength(1)
+                .field("text");
+        logger.info("--> run suggestions with three indices");
+        try {
+            searchSuggest( termSuggest);
+            fail(" can not suggest across multiple indices with different analysis chains");
+        } catch (ReduceSearchPhaseException ex) {
+            assertThat(ex.getCause(), instanceOf(IllegalStateException.class));
+            assertThat(ex.getCause().getMessage(),
+                    anyOf(endsWith("Suggest entries have different sizes actual [1] expected [2]"),
+                            endsWith("Suggest entries have different sizes actual [2] expected [1]")));
+        } catch (IllegalStateException ex) {
+            assertThat(ex.getMessage(), anyOf(endsWith("Suggest entries have different sizes actual [1] expected [2]"),
+                    endsWith("Suggest entries have different sizes actual [2] expected [1]")));
+        }
+
+
+        termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("ABCD")
+                .minWordLength(1)
+                .field("text");
+        logger.info("--> run suggestions with four indices");
+        try {
+            searchSuggest( termSuggest);
+            fail(" can not suggest across multiple indices with different analysis chains");
+        } catch (ReduceSearchPhaseException ex) {
+            assertThat(ex.getCause(), instanceOf(IllegalStateException.class));
+            assertThat(ex.getCause().getMessage(), anyOf(endsWith("Suggest entries have different text actual [ABCD] expected [abcd]"),
+                    endsWith("Suggest entries have different text actual [abcd] expected [ABCD]")));
+        } catch (IllegalStateException ex) {
+            assertThat(ex.getMessage(), anyOf(endsWith("Suggest entries have different text actual [ABCD] expected [abcd]"),
+                    endsWith("Suggest entries have different text actual [abcd] expected [ABCD]")));
+        }
+    }
+
+    // see #3037
+    public void testSuggestModes() throws IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(SETTING_NUMBER_OF_SHARDS, 1)
+                .put(SETTING_NUMBER_OF_REPLICAS, 0)
+                .put("index.analysis.analyzer.biword.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.biword.filter", "shingler", "lowercase")
+                .put("index.analysis.filter.shingler.type", "shingle")
+                .put("index.analysis.filter.shingler.min_shingle_size", 2)
+                .put("index.analysis.filter.shingler.max_shingle_size", 3));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("properties")
+                .startObject("name")
+                    .field("type", "multi_field")
+                    .startObject("fields")
+                        .startObject("name")
+                            .field("type", "string")
+                        .endObject()
+                        .startObject("shingled")
+                            .field("type", "string")
+                            .field("analyzer", "biword")
+                            .field("search_analyzer", "standard")
+                        .endObject()
+                    .endObject()
+                .endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+
+        index("test", "type1", "1", "name", "I like iced tea");
+        index("test", "type1", "2", "name", "I like tea.");
+        index("test", "type1", "3", "name", "I like ice cream.");
+        refresh();
+
+        DirectCandidateGenerator generator = candidateGenerator("name").prefixLength(0).minWordLength(0).suggestMode("always").maxEdits(2);
+        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("did_you_mean").field("name.shingled")
+                .addCandidateGenerator(generator)
+                .gramSize(3);
+        Suggest searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
+        assertSuggestion(searchSuggest, 0, "did_you_mean", "iced tea");
+
+        generator.suggestMode(null);
+        searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
+        assertSuggestionSize(searchSuggest, 0, 0, "did_you_mean");
+    }
+
+    // see #2729
+    public void testSizeOneShard() throws Exception {
+        prepareCreate("test").setSettings(
+                SETTING_NUMBER_OF_SHARDS, 1,
+                SETTING_NUMBER_OF_REPLICAS, 0).get();
+        ensureGreen();
+
+        for (int i = 0; i < 15; i++) {
+            index("test", "type1", Integer.toString(i), "text", "abc" + i);
+        }
+        refresh();
+
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellchecker")).get();
+        assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
+
+        TermSuggestionBuilder termSuggestion = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("abcd")
+                .field("text")
+                .size(10);
+        Suggest suggest = searchSuggest( termSuggestion);
+        assertSuggestion(suggest, 0, "test", 10, "abc0");
+
+        termSuggestion.text("abcd").shardSize(5);
+        suggest = searchSuggest( termSuggestion);
+        assertSuggestion(suggest, 0, "test", 5, "abc0");
+    }
+
+    public void testUnmappedField() throws IOException, InterruptedException, ExecutionException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.biword.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.biword.filter", "shingler", "lowercase")
+                .put("index.analysis.filter.shingler.type", "shingle")
+                .put("index.analysis.filter.shingler.min_shingle_size", 2)
+                .put("index.analysis.filter.shingler.max_shingle_size", 3));
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("properties")
+                .startObject("name")
+                    .field("type", "multi_field")
+                    .startObject("fields")
+                        .startObject("name")
+                            .field("type", "string")
+                        .endObject()
+                        .startObject("shingled")
+                            .field("type", "string")
+                            .field("analyzer", "biword")
+                            .field("search_analyzer", "standard")
+                        .endObject()
+                    .endObject()
+                .endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        indexRandom(true, client().prepareIndex("test", "type1").setSource("name", "I like iced tea"),
+        client().prepareIndex("test", "type1").setSource("name", "I like tea."),
+        client().prepareIndex("test", "type1").setSource("name", "I like ice cream."));
+        refresh();
+
+        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("did_you_mean").field("name.shingled")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("name").prefixLength(0).minWordLength(0).suggestMode("always").maxEdits(2))
+                .gramSize(3);
+        Suggest searchSuggest = searchSuggest( "ice tea", phraseSuggestion);
+        assertSuggestion(searchSuggest, 0, 0, "did_you_mean", "iced tea");
+
+        phraseSuggestion.field("nosuchField");
+        {
+            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
+            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
+            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+        }
+        {
+            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
+            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
+            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+        }
+    }
+
+    public void testSimple() throws Exception {
+        createIndex("test");
+        ensureGreen();
+
+        index("test", "type1", "1", "text", "abcd");
+        index("test", "type1", "2", "text", "aacd");
+        index("test", "type1", "3", "text", "abbd");
+        index("test", "type1", "4", "text", "abcc");
+        refresh();
+
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("text", "spellcecker")).get();
+        assertThat("didn't ask for suggestions but got some", search.getSuggest(), nullValue());
+
+        TermSuggestionBuilder termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("abcd")
+                .field("text");
+        Suggest suggest = searchSuggest( termSuggest);
+        assertSuggestion(suggest, 0, "test", "aacd", "abbd", "abcc");
+        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
+
+        suggest = searchSuggest( termSuggest);
+        assertSuggestion(suggest, 0, "test", "aacd","abbd", "abcc");
+        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
+    }
+
+    public void testEmpty() throws Exception {
+        createIndex("test");
+        ensureGreen();
+
+        index("test", "type1", "1", "foo", "bar");
+        refresh();
+
+        TermSuggestionBuilder termSuggest = termSuggestion("test")
+                .suggestMode("always") // Always, otherwise the results can vary between requests.
+                .text("abcd")
+                .field("text");
+        Suggest suggest = searchSuggest( termSuggest);
+        assertSuggestionSize(suggest, 0, 0, "test");
+        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
+
+        suggest = searchSuggest( termSuggest);
+        assertSuggestionSize(suggest, 0, 0, "test");
+        assertThat(suggest.getSuggestion("test").getEntries().get(0).getText().string(), equalTo("abcd"));
+    }
+
+    public void testWithMultipleCommands() throws Exception {
+        createIndex("test");
+        ensureGreen();
+
+        index("test", "typ1", "1", "field1", "prefix_abcd", "field2", "prefix_efgh");
+        index("test", "typ1", "2", "field1", "prefix_aacd", "field2", "prefix_eeeh");
+        index("test", "typ1", "3", "field1", "prefix_abbd", "field2", "prefix_efff");
+        index("test", "typ1", "4", "field1", "prefix_abcc", "field2", "prefix_eggg");
+        refresh();
+
+        Suggest suggest = searchSuggest(
+                termSuggestion("size1")
+                        .size(1).text("prefix_abcd").maxTermFreq(10).prefixLength(1).minDocFreq(0)
+                        .field("field1").suggestMode("always"),
+                termSuggestion("field2")
+                        .field("field2").text("prefix_eeeh prefix_efgh")
+                        .maxTermFreq(10).minDocFreq(0).suggestMode("always"),
+                termSuggestion("accuracy")
+                        .field("field2").text("prefix_efgh").setAccuracy(1f)
+                        .maxTermFreq(10).minDocFreq(0).suggestMode("always"));
+        assertSuggestion(suggest, 0, "size1", "prefix_aacd");
+        assertThat(suggest.getSuggestion("field2").getEntries().get(0).getText().string(), equalTo("prefix_eeeh"));
+        assertSuggestion(suggest, 0, "field2", "prefix_efgh");
+        assertThat(suggest.getSuggestion("field2").getEntries().get(1).getText().string(), equalTo("prefix_efgh"));
+        assertSuggestion(suggest, 1, "field2", "prefix_eeeh", "prefix_efff", "prefix_eggg");
+        assertSuggestionSize(suggest, 0, 0, "accuracy");
+    }
+
+    public void testSizeAndSort() throws Exception {
+        createIndex("test");
+        ensureGreen();
+
+        Map<String, Integer> termsAndDocCount = new HashMap<>();
+        termsAndDocCount.put("prefix_aaad", 20);
+        termsAndDocCount.put("prefix_abbb", 18);
+        termsAndDocCount.put("prefix_aaca", 16);
+        termsAndDocCount.put("prefix_abba", 14);
+        termsAndDocCount.put("prefix_accc", 12);
+        termsAndDocCount.put("prefix_addd", 10);
+        termsAndDocCount.put("prefix_abaa", 8);
+        termsAndDocCount.put("prefix_dbca", 6);
+        termsAndDocCount.put("prefix_cbad", 4);
+        termsAndDocCount.put("prefix_aacd", 1);
+        termsAndDocCount.put("prefix_abcc", 1);
+        termsAndDocCount.put("prefix_accd", 1);
+
+        for (Map.Entry<String, Integer> entry : termsAndDocCount.entrySet()) {
+            for (int i = 0; i < entry.getValue(); i++) {
+                index("test", "type1", entry.getKey() + i, "field1", entry.getKey());
+            }
+        }
+        refresh();
+
+        Suggest suggest = searchSuggest( "prefix_abcd",
+                termSuggestion("size3SortScoreFirst")
+                        .size(3).minDocFreq(0).field("field1").suggestMode("always"),
+                termSuggestion("size10SortScoreFirst")
+                        .size(10).minDocFreq(0).field("field1").suggestMode("always").shardSize(50),
+                termSuggestion("size3SortScoreFirstMaxEdits1")
+                        .maxEdits(1)
+                        .size(10).minDocFreq(0).field("field1").suggestMode("always"),
+                termSuggestion("size10SortFrequencyFirst")
+                        .size(10).sort("frequency").shardSize(1000)
+                        .minDocFreq(0).field("field1").suggestMode("always"));
+
+        // The commented out assertions fail sometimes because suggestions are based off of shard frequencies instead of index frequencies.
+        assertSuggestion(suggest, 0, "size3SortScoreFirst", "prefix_aacd", "prefix_abcc", "prefix_accd");
+        assertSuggestion(suggest, 0, "size10SortScoreFirst", 10, "prefix_aacd", "prefix_abcc", "prefix_accd" /*, "prefix_aaad" */);
+        assertSuggestion(suggest, 0, "size3SortScoreFirstMaxEdits1", "prefix_aacd", "prefix_abcc", "prefix_accd");
+        assertSuggestion(suggest, 0, "size10SortFrequencyFirst", "prefix_aaad", "prefix_abbb", "prefix_aaca", "prefix_abba",
+                "prefix_accc", "prefix_addd", "prefix_abaa", "prefix_dbca", "prefix_cbad", "prefix_aacd");
+
+        // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_abcc"));
+        // assertThat(suggest.get(3).getSuggestedWords().get("prefix_abcd").get(4).getTerm(), equalTo("prefix_accd"));
+    }
+
+    // see #2817
+    public void testStopwordsOnlyPhraseSuggest() throws IOException {
+        assertAcked(prepareCreate("test").addMapping("typ1", "body", "type=string,analyzer=stopwd").setSettings(
+                settingsBuilder()
+                        .put("index.analysis.analyzer.stopwd.tokenizer", "whitespace")
+                        .putArray("index.analysis.analyzer.stopwd.filter", "stop")
+        ));
+        ensureGreen();
+        index("test", "typ1", "1", "body", "this is a test");
+        refresh();
+
+        Suggest searchSuggest = searchSuggest( "a an the",
+                phraseSuggestion("simple_phrase").field("body").gramSize(1)
+                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always"))
+                        .size(1));
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+    }
+
+    public void testPrefixLength() throws IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(SETTING_NUMBER_OF_SHARDS, 1)
+                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase")
+                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", false)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
+                .startObject("properties")
+                .startObject("body").field("type", "string").field("analyzer", "body").endObject()
+                .startObject("body_reverse").field("type", "string").field("analyzer", "reverse").endObject()
+                .startObject("bigram").field("type", "string").field("analyzer", "bigram").endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        index("test", "type1", "1", "body", "hello world");
+        index("test", "type1", "2", "body", "hello world");
+        index("test", "type1", "3", "body", "hello words");
+        refresh();
+
+        Suggest searchSuggest = searchSuggest( "hello word",
+                phraseSuggestion("simple_phrase").field("body")
+                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(4).minWordLength(1).suggestMode("always"))
+                        .size(1).confidence(1.0f));
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "hello words");
+
+        searchSuggest = searchSuggest( "hello word",
+                phraseSuggestion("simple_phrase").field("body")
+                        .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").prefixLength(2).minWordLength(1).suggestMode("always"))
+                        .size(1).confidence(1.0f));
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "hello world");
+    }
+
+    @Nightly
+    public void testMarvelHerosPhraseSuggest() throws IOException, URISyntaxException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase")
+                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", false)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type1")
+                    .startObject("_all")
+                        .field("store", "yes")
+                        .field("termVector", "with_positions_offsets")
+                    .endObject()
+                    .startObject("properties")
+                        .startObject("body").
+                            field("type", "string").
+                            field("analyzer", "body")
+                        .endObject()
+                        .startObject("body_reverse").
+                            field("type", "string").
+                            field("analyzer", "reverse")
+                         .endObject()
+                         .startObject("bigram").
+                             field("type", "string").
+                             field("analyzer", "bigram")
+                         .endObject()
+                     .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        for (String line : readMarvelHeroNames()) {
+            index("test", "type1", line, "body", line, "body_reverse", line, "bigram", line);
+        }
+        refresh();
+
+        PhraseSuggestionBuilder phraseSuggest = phraseSuggestion("simple_phrase")
+                .field("bigram").gramSize(2).analyzer("body")
+                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
+                .size(1);
+        Suggest searchSuggest = searchSuggest( "american ame", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
+        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("american ame"));
+
+        phraseSuggest.realWordErrorLikelihood(0.95f);
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+        // Check the "text" field this one time.
+        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel"));
+
+        // Ask for highlighting
+        phraseSuggest.highlight("<em>", "</em>");
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getOptions().get(0).getHighlighted().string(), equalTo("<em>xorr</em> the <em>god</em> jewel"));
+
+        // pass in a correct phrase
+        phraseSuggest.highlight(null, null).confidence(0f).size(1).maxErrors(0.5f);
+        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        // pass in a correct phrase - set confidence to 2
+        phraseSuggest.confidence(2f);
+        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        // pass in a correct phrase - set confidence to 0.99
+        phraseSuggest.confidence(0.99f);
+        searchSuggest = searchSuggest( "Xorr the God-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        //test reverse suggestions with pre & post filter
+        phraseSuggest
+            .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
+            .addCandidateGenerator(candidateGenerator("body_reverse").minWordLength(1).suggestMode("always").preFilter("reverse").postFilter("reverse"));
+        searchSuggest = searchSuggest( "xor the yod-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        // set all mass to trigrams (not indexed)
+        phraseSuggest.clearCandidateGenerators()
+            .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"))
+            .smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(1,0,0));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        // set all mass to bigrams
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0,1,0));
+        searchSuggest =  searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        // distribute mass
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        searchSuggest = searchSuggest( "american ame", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "american ace");
+
+        // try all smoothing methods
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.LinearInterpolation(0.4,0.4,0.2));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.Laplace(0.2));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        phraseSuggest.smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+
+        // check tokenLimit
+        phraseSuggest.smoothingModel(null).tokenLimit(4);
+        searchSuggest = searchSuggest( "Xor the Got-Jewel", phraseSuggest);
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        phraseSuggest.tokenLimit(15).smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1));
+        searchSuggest = searchSuggest( "Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel", phraseSuggest);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel xorr the god jewel xorr the god jewel");
+        // Check the name this time because we're repeating it which is funky
+        assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel"));
+    }
+    
+    private List<String> readMarvelHeroNames() throws IOException, URISyntaxException {
+        return Files.readAllLines(PathUtils.get(Suggest.class.getResource("/config/names.txt").toURI()), StandardCharsets.UTF_8);
+    }
+
+    public void testSizePararm() throws IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(SETTING_NUMBER_OF_SHARDS, 1)
+                .put("index.analysis.analyzer.reverse.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.reverse.filter", "lowercase", "reverse")
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase")
+                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", false)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("type1")
+                        .startObject("_all")
+                            .field("store", "yes")
+                            .field("termVector", "with_positions_offsets")
+                        .endObject()
+                        .startObject("properties")
+                            .startObject("body")
+                                .field("type", "string")
+                                .field("analyzer", "body")
+                            .endObject()
+                         .startObject("body_reverse")
+                             .field("type", "string")
+                             .field("analyzer", "reverse")
+                         .endObject()
+                         .startObject("bigram")
+                             .field("type", "string")
+                             .field("analyzer", "bigram")
+                         .endObject()
+                     .endObject()
+                 .endObject()
+             .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        String line = "xorr the god jewel";
+        index("test", "type1", "1", "body", line, "body_reverse", line, "bigram", line);
+        line = "I got it this time";
+        index("test", "type1", "2", "body", line, "body_reverse", line, "bigram", line);
+        refresh();
+
+        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("simple_phrase")
+                .realWordErrorLikelihood(0.95f)
+                .field("bigram")
+                .gramSize(2)
+                .analyzer("body")
+                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).prefixLength(1).suggestMode("always").size(1).accuracy(0.1f))
+                .smoothingModel(new PhraseSuggestionBuilder.StupidBackoff(0.1))
+                .maxErrors(1.0f)
+                .size(5);
+        Suggest searchSuggest = searchSuggest( "Xorr the Gut-Jewel", phraseSuggestion);
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        // we allow a size of 2 now on the shard generator level so "god" will be found since it's LD2
+        phraseSuggestion.clearCandidateGenerators()
+                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).prefixLength(1).suggestMode("always").size(2).accuracy(0.1f));
+        searchSuggest = searchSuggest( "Xorr the Gut-Jewel", phraseSuggestion);
+        assertSuggestion(searchSuggest, 0, "simple_phrase", "xorr the god jewel");
+    }
+
+    @Nightly
+    public void testPhraseBoundaryCases() throws IOException, URISyntaxException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings()).put(SETTING_NUMBER_OF_SHARDS, 1) // to get reliable statistics we should put this all into one shard
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase")
+                .put("index.analysis.analyzer.bigram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.bigram.filter", "my_shingle", "lowercase")
+                .put("index.analysis.analyzer.ngram.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.ngram.filter", "my_shingle2", "lowercase")
+                .put("index.analysis.analyzer.myDefAnalyzer.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.myDefAnalyzer.filter", "shingle", "lowercase")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", false)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle2.type", "shingle")
+                .put("index.analysis.filter.my_shingle2.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle2.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle2.max_shingle_size", 2));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                    .startObject().startObject("type1")
+                    .startObject("_all").field("store", "yes").field("termVector", "with_positions_offsets").endObject()
+                .startObject("properties")
+                .startObject("body").field("type", "string").field("analyzer", "body").endObject()
+                .startObject("bigram").field("type", "string").field("analyzer", "bigram").endObject()
+                .startObject("ngram").field("type", "string").field("analyzer", "ngram").endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        for (String line : readMarvelHeroNames()) {
+            index("test", "type1", line, "body", line, "bigram", line, "ngram", line);
+        }
+        refresh();
+
+        NumShards numShards = getNumShards("test");
+
+        // Lets make sure some things throw exceptions
+        PhraseSuggestionBuilder phraseSuggestion = phraseSuggestion("simple_phrase")
+                .field("bigram")
+                .analyzer("body")
+                .addCandidateGenerator(candidateGenerator("does_not_exist").minWordLength(1).suggestMode("always"))
+                .realWordErrorLikelihood(0.95f)
+                .maxErrors(0.5f)
+                .size(1);
+        try {
+            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
+            fail("field does not exists");
+        } catch (SearchPhaseExecutionException e) {}
+
+        phraseSuggestion.clearCandidateGenerators().analyzer(null);
+        try {
+            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
+            fail("analyzer does only produce ngrams");
+        } catch (SearchPhaseExecutionException e) {
+        }
+
+        phraseSuggestion.analyzer("bigram");
+        try {
+            searchSuggest( "Xor the Got-Jewel", numShards.numPrimaries, phraseSuggestion);
+            fail("analyzer does only produce ngrams");
+        } catch (SearchPhaseExecutionException e) {
+        }
+
+        // Now we'll make sure some things don't
+        phraseSuggestion.forceUnigrams(false);
+        searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
+
+        // Field doesn't produce unigrams but the analyzer does
+        phraseSuggestion.forceUnigrams(true).field("bigram").analyzer("ngram");
+        searchSuggest( "Xor the Got-Jewel",
+                phraseSuggestion);
+
+        phraseSuggestion.field("ngram").analyzer("myDefAnalyzer")
+                .addCandidateGenerator(candidateGenerator("body").minWordLength(1).suggestMode("always"));
+        Suggest suggest = searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
+
+        // "xorr the god jewel" and and "xorn the god jewel" have identical scores (we are only using unigrams to score), so we tie break by
+        // earlier term (xorn):
+        assertSuggestion(suggest, 0, "simple_phrase", "xorn the god jewel");
+
+        phraseSuggestion.analyzer(null);
+        suggest = searchSuggest( "Xor the Got-Jewel", phraseSuggestion);
+
+        // In this case xorr has a better score than xorn because we set the field back to the default (my_shingle2) analyzer, so the
+        // probability that the term is not in the dictionary but is NOT a misspelling is relatively high in this case compared to the
+        // others that have no n-gram with the other terms in the phrase :) you can set this realWorldErrorLikelyhood
+        assertSuggestion(suggest, 0, "simple_phrase", "xorr the god jewel");
+    }
+
+    public void testDifferentShardSize() throws Exception {
+        createIndex("test");
+        ensureGreen();
+        indexRandom(true, client().prepareIndex("test", "type1", "1").setSource("field1", "foobar1").setRouting("1"),
+                client().prepareIndex("test", "type1", "2").setSource("field1", "foobar2").setRouting("2"),
+                client().prepareIndex("test", "type1", "3").setSource("field1", "foobar3").setRouting("3"));
+
+        Suggest suggest = searchSuggest( "foobar",
+                termSuggestion("simple")
+                        .size(10).minDocFreq(0).field("field1").suggestMode("always"));
+        ElasticsearchAssertions.assertSuggestionSize(suggest, 0, 3, "simple");
+    }
+
+    // see #3469
+    public void testShardFailures() throws IOException, InterruptedException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.suggest.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.suggest.filter", "standard", "lowercase", "shingler")
+                .put("index.analysis.filter.shingler.type", "shingle")
+                .put("index.analysis.filter.shingler.min_shingle_size", 2)
+                .put("index.analysis.filter.shingler.max_shingle_size", 5)
+                .put("index.analysis.filter.shingler.output_unigrams", true));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject().startObject("type2")
+                .startObject("properties")
+                    .startObject("name")
+                        .field("type", "multi_field")
+                        .startObject("fields")
+                            .startObject("name")
+                                .field("type", "string")
+                                .field("analyzer", "suggest")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                .endObject()
+                .endObject().endObject();
+        assertAcked(builder.addMapping("type2", mapping));
+        ensureGreen();
+
+        index("test", "type2", "1", "foo", "bar");
+        index("test", "type2", "2", "foo", "bar");
+        index("test", "type2", "3", "foo", "bar");
+        index("test", "type2", "4", "foo", "bar");
+        index("test", "type2", "5", "foo", "bar");
+        index("test", "type2", "1", "name", "Just testing the suggestions api");
+        index("test", "type2", "2", "name", "An other title about equal length");
+        // Note that the last document has to have about the same length as the other or cutoff rechecking will remove the useful suggestion.
+        refresh();
+
+        // When searching on a shard with a non existing mapping, we should fail
+        SearchRequestBuilder request = client().prepareSearch().setSize(0)
+                .suggest(
+                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
+                                phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f)));
+        assertThrows(request, SearchPhaseExecutionException.class);
+
+        // When searching on a shard which does not hold yet any document of an existing type, we should not fail
+        SearchResponse searchResponse = client().prepareSearch().setSize(0)
+                .suggest(
+                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
+                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+            .get();
+        ElasticsearchAssertions.assertNoFailures(searchResponse);
+        ElasticsearchAssertions.assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
+    }
+
+    // see #3469
+    public void testEmptyShards() throws IOException, InterruptedException {
+        XContentBuilder mappingBuilder = XContentFactory.jsonBuilder().
+                startObject().
+                    startObject("type1").
+                        startObject("properties").
+                            startObject("name").
+                                field("type", "multi_field").
+                                startObject("fields").
+                                    startObject("name").
+                                        field("type", "string").
+                                        field("analyzer", "suggest").
+                                    endObject().
+                                endObject().
+                            endObject().
+                        endObject().
+                    endObject().
+                endObject();
+        assertAcked(prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.suggest.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.suggest.filter", "standard", "lowercase", "shingler")
+                .put("index.analysis.filter.shingler.type", "shingle")
+                .put("index.analysis.filter.shingler.min_shingle_size", 2)
+                .put("index.analysis.filter.shingler.max_shingle_size", 5)
+                .put("index.analysis.filter.shingler.output_unigrams", true)).addMapping("type1", mappingBuilder));
+        ensureGreen();
+
+        index("test", "type2", "1", "foo", "bar");
+        index("test", "type2", "2", "foo", "bar");
+        index("test", "type1", "1", "name", "Just testing the suggestions api");
+        index("test", "type1", "2", "name", "An other title about equal length");
+        refresh();
+
+        SearchResponse searchResponse = client().prepareSearch()
+                .setSize(0)
+                .suggest(
+                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
+                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+                .get();
+
+        assertNoFailures(searchResponse);
+        assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
+    }
+
+    /**
+     * Searching for a rare phrase shouldn't provide any suggestions if confidence &gt; 1.  This was possible before we rechecked the cutoff
+     * score during the reduce phase.  Failures don't occur every time - maybe two out of five tries but we don't repeat it to save time.
+     */
+    public void testSearchForRarePhrase() throws IOException {
+        // If there isn't enough chaf per shard then shards can become unbalanced, making the cutoff recheck this is testing do more harm then good.
+        int chafPerShard = 100;
+
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put("index.analysis.analyzer.body.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.body.filter", "lowercase", "my_shingle")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 2));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("type1")
+                        .startObject("_all")
+                            .field("store", "yes")
+                            .field("termVector", "with_positions_offsets")
+                        .endObject()
+                        .startObject("properties")
+                            .startObject("body")
+                                .field("type", "string")
+                                .field("analyzer", "body")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        NumShards test = getNumShards("test");
+
+        List<String> phrases = new ArrayList<>();
+        Collections.addAll(phrases, "nobel prize", "noble gases", "somethingelse prize", "pride and joy", "notes are fun");
+        for (int i = 0; i < 8; i++) {
+            phrases.add("noble somethingelse" + i);
+        }
+        for (int i = 0; i < test.numPrimaries * chafPerShard; i++) {
+            phrases.add("chaff" + i);
+        }
+        for (String phrase: phrases) {
+            index("test", "type1", phrase, "body", phrase);
+        }
+        refresh();
+
+        Suggest searchSuggest = searchSuggest("nobel prize", phraseSuggestion("simple_phrase")
+                .field("body")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always").maxTermFreq(.99f))
+                .confidence(2f)
+                .maxErrors(5f)
+                .size(1));
+        assertSuggestionSize(searchSuggest, 0, 0, "simple_phrase");
+
+        searchSuggest = searchSuggest("noble prize", phraseSuggestion("simple_phrase")
+                .field("body")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("body").minWordLength(1).suggestMode("always").maxTermFreq(.99f))
+                .confidence(2f)
+                .maxErrors(5f)
+                .size(1));
+        assertSuggestion(searchSuggest, 0, 0, "simple_phrase", "nobel prize");
+    }
+
+    @Nightly
+    public void testSuggestWithManyCandidates() throws InterruptedException, ExecutionException, IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
+                .put("index.analysis.analyzer.text.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("type1")
+                        .startObject("properties")
+                            .startObject("title")
+                                .field("type", "string")
+                                .field("analyzer", "text")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        List<String> titles = new ArrayList<>();
+
+        // We're going to be searching for:
+        //   united states house of representatives elections in washington 2006
+        // But we need to make sure we generate a ton of suggestions so we add a bunch of candidates.
+        // Many of these candidates are drawn from page names on English Wikipedia.
+
+        // Tons of different options very near the exact query term
+        titles.add("United States House of Representatives Elections in Washington 1789");
+        for (int year = 1790; year < 2014; year+= 2) {
+            titles.add("United States House of Representatives Elections in Washington " + year);
+        }
+        // Six of these are near enough to be viable suggestions, just not the top one
+
+        // But we can't stop there!  Titles that are just a year are pretty common so lets just add one per year
+        // since 0.  Why not?
+        for (int year = 0; year < 2015; year++) {
+            titles.add(Integer.toString(year));
+        }
+        // That ought to provide more less good candidates for the last term
+
+        // Now remove or add plural copies of every term we can
+        titles.add("State");
+        titles.add("Houses of Parliament");
+        titles.add("Representative Government");
+        titles.add("Election");
+
+        // Now some possessive
+        titles.add("Washington's Birthday");
+
+        // And some conjugation
+        titles.add("Unified Modeling Language");
+        titles.add("Unite Against Fascism");
+        titles.add("Stated Income Tax");
+        titles.add("Media organizations housed within colleges");
+
+        // And other stuff
+        titles.add("Untied shoelaces");
+        titles.add("Unit circle");
+        titles.add("Untitled");
+        titles.add("Unicef");
+        titles.add("Unrated");
+        titles.add("UniRed");
+        titles.add("Jalan Uniten–Dengkil"); // Highway in Malaysia
+        titles.add("UNITAS");
+        titles.add("UNITER");
+        titles.add("Un-Led-Ed");
+        titles.add("STATS LLC");
+        titles.add("Staples");
+        titles.add("Skates");
+        titles.add("Statues of the Liberators");
+        titles.add("Staten Island");
+        titles.add("Statens Museum for Kunst");
+        titles.add("Hause"); // The last name or the German word, whichever.
+        titles.add("Hose");
+        titles.add("Hoses");
+        titles.add("Howse Peak");
+        titles.add("The Hoose-Gow");
+        titles.add("Hooser");
+        titles.add("Electron");
+        titles.add("Electors");
+        titles.add("Evictions");
+        titles.add("Coronal mass ejection");
+        titles.add("Wasington"); // A film?
+        titles.add("Warrington"); // A town in England
+        titles.add("Waddington"); // Lots of places have this name
+        titles.add("Watlington"); // Ditto
+        titles.add("Waplington"); // Yup, also a town
+        titles.add("Washing of the Spears"); // Book
+
+        for (char c = 'A'; c <= 'Z'; c++) {
+            // Can't forget lists, glorious lists!
+            titles.add("List of former members of the United States House of Representatives (" + c + ")");
+
+            // Lots of people are named Washington <Middle Initial>. LastName
+            titles.add("Washington " + c + ". Lastname");
+
+            // Lets just add some more to be evil
+            titles.add("United " + c);
+            titles.add("States " + c);
+            titles.add("House " + c);
+            titles.add("Elections " + c);
+            titles.add("2006 " + c);
+            titles.add(c + " United");
+            titles.add(c + " States");
+            titles.add(c + " House");
+            titles.add(c + " Elections");
+            titles.add(c + " 2006");
+        }
+
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (String title: titles) {
+            builders.add(client().prepareIndex("test", "type1").setSource("title", title));
+        }
+        indexRandom(true, builders);
+
+        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
+                .field("title")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
+                        .suggestMode("always")
+                        .maxTermFreq(.99f)
+                        .size(1000) // Setting a silly high size helps of generate a larger list of candidates for testing.
+                        .maxInspections(1000) // This too
+                )
+                .confidence(0f)
+                .maxErrors(2f)
+                .shardSize(30000)
+                .size(30000);
+        Suggest searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
+        assertSuggestion(searchSuggest, 0, 0, "title", "united states house of representatives elections in washington 2006");
+        assertSuggestionSize(searchSuggest, 0, 25480, "title");  // Just to prove that we've run through a ton of options
+
+        suggest.size(1);
+        long start = System.currentTimeMillis();
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
+        long total = System.currentTimeMillis() - start;
+        assertSuggestion(searchSuggest, 0, 0, "title", "united states house of representatives elections in washington 2006");
+        // assertThat(total, lessThan(1000L)); // Takes many seconds without fix - just for debugging
+    }
+
+    public void testPhraseSuggesterCollate() throws InterruptedException, ExecutionException, IOException {
+        CreateIndexRequestBuilder builder = prepareCreate("test").setSettings(settingsBuilder()
+                .put(indexSettings())
+                .put(SETTING_NUMBER_OF_SHARDS, 1) // A single shard will help to keep the tests repeatable.
+                .put("index.analysis.analyzer.text.tokenizer", "standard")
+                .putArray("index.analysis.analyzer.text.filter", "lowercase", "my_shingle")
+                .put("index.analysis.filter.my_shingle.type", "shingle")
+                .put("index.analysis.filter.my_shingle.output_unigrams", true)
+                .put("index.analysis.filter.my_shingle.min_shingle_size", 2)
+                .put("index.analysis.filter.my_shingle.max_shingle_size", 3));
+
+        XContentBuilder mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("type1")
+                .startObject("properties")
+                .startObject("title")
+                .field("type", "string")
+                .field("analyzer", "text")
+                .endObject()
+                .endObject()
+                .endObject()
+                .endObject();
+        assertAcked(builder.addMapping("type1", mapping));
+        ensureGreen();
+
+        List<String> titles = new ArrayList<>();
+
+        titles.add("United States House of Representatives Elections in Washington 2006");
+        titles.add("United States House of Representatives Elections in Washington 2005");
+        titles.add("State");
+        titles.add("Houses of Parliament");
+        titles.add("Representative Government");
+        titles.add("Election");
+
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        for (String title: titles) {
+            builders.add(client().prepareIndex("test", "type1").setSource("title", title));
+        }
+        indexRandom(true, builders);
+
+        // suggest without collate
+        PhraseSuggestionBuilder suggest = phraseSuggestion("title")
+                .field("title")
+                .addCandidateGenerator(PhraseSuggestionBuilder.candidateGenerator("title")
+                        .suggestMode("always")
+                        .maxTermFreq(.99f)
+                        .size(10)
+                        .maxInspections(200)
+                )
+                .confidence(0f)
+                .maxErrors(2f)
+                .shardSize(30000)
+                .size(10);
+        Suggest searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", suggest);
+        assertSuggestionSize(searchSuggest, 0, 10, "title");
+
+        // suggest with collate
+        String filterString = XContentFactory.jsonBuilder()
+                    .startObject()
+                        .startObject("match_phrase")
+                            .field("title", "{{suggestion}}")
+                        .endObject()
+                    .endObject()
+                .string();
+        PhraseSuggestionBuilder filteredQuerySuggest = suggest.collateQuery(filterString);
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", filteredQuerySuggest);
+        assertSuggestionSize(searchSuggest, 0, 2, "title");
+
+        // collate suggest with no result (boundary case)
+        searchSuggest = searchSuggest("Elections of Representatives Parliament", filteredQuerySuggest);
+        assertSuggestionSize(searchSuggest, 0, 0, "title");
+
+        NumShards numShards = getNumShards("test");
+
+        // collate suggest with bad query
+        String incorrectFilterString = XContentFactory.jsonBuilder()
+                .startObject()
+                    .startObject("test")
+                        .field("title", "{{suggestion}}")
+                    .endObject()
+                .endObject()
+                .string();
+        PhraseSuggestionBuilder incorrectFilteredSuggest = suggest.collateQuery(incorrectFilterString);
+        try {
+            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, incorrectFilteredSuggest);
+            fail("Post query error has been swallowed");
+        } catch(ElasticsearchException e) {
+            // expected
+        }
+
+        // suggest with collation
+        String filterStringAsFilter = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("match_phrase")
+                .field("title", "{{suggestion}}")
+                .endObject()
+                .endObject()
+                .string();
+
+        PhraseSuggestionBuilder filteredFilterSuggest = suggest.collateQuery(filterStringAsFilter);
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", filteredFilterSuggest);
+        assertSuggestionSize(searchSuggest, 0, 2, "title");
+
+        // collate suggest with bad query
+        String filterStr = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("pprefix")
+                        .field("title", "{{suggestion}}")
+                .endObject()
+                .endObject()
+                .string();
+
+        PhraseSuggestionBuilder in = suggest.collateQuery(filterStr);
+        try {
+            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, in);
+            fail("Post filter error has been swallowed");
+        } catch(ElasticsearchException e) {
+            //expected
+        }
+
+        // collate script failure due to no additional params
+        String collateWithParams = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("{{query_type}}")
+                    .field("{{query_field}}", "{{suggestion}}")
+                .endObject()
+                .endObject()
+                .string();
+
+
+        PhraseSuggestionBuilder phraseSuggestWithNoParams = suggest.collateQuery(collateWithParams);
+        try {
+            searchSuggest("united states house of representatives elections in washington 2006", numShards.numPrimaries, phraseSuggestWithNoParams);
+            fail("Malformed query (lack of additional params) should fail");
+        } catch (ElasticsearchException e) {
+            // expected
+        }
+
+        // collate script with additional params
+        Map<String, Object> params = new HashMap<>();
+        params.put("query_type", "match_phrase");
+        params.put("query_field", "title");
+
+        PhraseSuggestionBuilder phraseSuggestWithParams = suggest.collateQuery(collateWithParams).collateParams(params);
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", phraseSuggestWithParams);
+        assertSuggestionSize(searchSuggest, 0, 2, "title");
+
+        // collate query request with prune set to true
+        PhraseSuggestionBuilder phraseSuggestWithParamsAndReturn = suggest.collateQuery(collateWithParams).collateParams(params).collatePrune(true);
+        searchSuggest = searchSuggest("united states house of representatives elections in washington 2006", phraseSuggestWithParamsAndReturn);
+        assertSuggestionSize(searchSuggest, 0, 10, "title");
+        assertSuggestionPhraseCollateMatchExists(searchSuggest, "title", 2);
+    }
+
+    protected Suggest searchSuggest(SuggestionBuilder<?>... suggestion) {
+        return searchSuggest(null, suggestion);
+    }
+
+    protected Suggest searchSuggest(String suggestText, SuggestionBuilder<?>... suggestions) {
+        return searchSuggest(suggestText, 0, suggestions);
+    }
+
+    protected Suggest searchSuggest(String suggestText, int expectShardsFailed, SuggestionBuilder<?>... suggestions) {
+        if (randomBoolean()) {
+            SearchRequestBuilder builder = client().prepareSearch().setSize(0);
+            SuggestBuilder suggestBuilder = new SuggestBuilder();
+            if (suggestText != null) {
+                suggestBuilder.setText(suggestText);
+            }
+            for (SuggestionBuilder<?> suggestion : suggestions) {
+                suggestBuilder.addSuggestion(suggestion);
+            }
+            builder.suggest(suggestBuilder);
+            SearchResponse actionGet = builder.execute().actionGet();
+            assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
+            return actionGet.getSuggest();
+        } else {
+            SuggestRequestBuilder builder = client().prepareSuggest();
+            if (suggestText != null) {
+                builder.setSuggestText(suggestText);
+            }
+            for (SuggestionBuilder<?> suggestion : suggestions) {
+                builder.addSuggestion(suggestion);
+            }
+
+            SuggestResponse actionGet = builder.execute().actionGet();
+            assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
+            if (expectShardsFailed > 0) {
+                throw new SearchPhaseExecutionException("suggest", "Suggest execution failed", new ShardSearchFailure[0]);
+            }
+            return actionGet.getSuggest();
+        }
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
new file mode 100644
index 0000000..29213f0
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
@@ -0,0 +1,210 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.messy.tests;
+
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.Accountable;
+import org.elasticsearch.Version;
+import org.elasticsearch.client.Client;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.ParsingException;
+import org.elasticsearch.common.inject.AbstractModule;
+import org.elasticsearch.common.inject.Injector;
+import org.elasticsearch.common.inject.ModulesBuilder;
+import org.elasticsearch.common.inject.multibindings.Multibinder;
+import org.elasticsearch.common.inject.util.Providers;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsFilter;
+import org.elasticsearch.common.settings.SettingsModule;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.env.EnvironmentModule;
+import org.elasticsearch.index.Index;
+import org.elasticsearch.index.IndexSettings;
+import org.elasticsearch.index.analysis.AnalysisRegistry;
+import org.elasticsearch.index.analysis.AnalysisService;
+import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
+import org.elasticsearch.index.fielddata.IndexFieldDataService;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.TemplateQueryParser;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.similarity.SimilarityService;
+import org.elasticsearch.indices.IndicesModule;
+import org.elasticsearch.indices.IndicesWarmer;
+import org.elasticsearch.indices.breaker.CircuitBreakerService;
+import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
+import org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache;
+import org.elasticsearch.indices.mapper.MapperRegistry;
+import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.script.ScriptModule;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.test.IndexSettingsModule;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.threadpool.ThreadPoolModule;
+import org.junit.After;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.lang.reflect.Proxy;
+import java.util.Collections;
+
+import static org.hamcrest.Matchers.containsString;
+
+/**
+ * Test parsing and executing a template request.
+ */
+// NOTE: this can't be migrated to ESSingleNodeTestCase because of the custom path.conf
+public class TemplateQueryParserTests extends ESTestCase {
+
+    private Injector injector;
+    private QueryShardContext context;
+
+    @Before
+    public void setup() throws IOException {
+        Settings settings = Settings.settingsBuilder()
+                .put("path.home", createTempDir().toString())
+                .put("path.conf", this.getDataPath("config"))
+                .put("name", getClass().getName())
+                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
+                .build();
+        final Client proxy = (Client) Proxy.newProxyInstance(
+                Client.class.getClassLoader(),
+                new Class<?>[]{Client.class}, (proxy1, method, args) -> {
+                    throw new UnsupportedOperationException("client is just a dummy");
+                });
+        Index index = new Index("test");
+        IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(index, settings);
+        ScriptModule scriptModule = new ScriptModule(settings);
+        // TODO: make this use a mock engine instead of mustache and it will no longer be messy!
+        scriptModule.addScriptEngine(MustacheScriptEngineService.class);
+        injector = new ModulesBuilder().add(
+                new EnvironmentModule(new Environment(settings)),
+                new SettingsModule(settings, new SettingsFilter(settings)),
+                new ThreadPoolModule(new ThreadPool(settings)),
+                new IndicesModule() {
+                    @Override
+                    public void configure() {
+                        // skip services
+                        bindQueryParsersExtension();
+                    }
+                },
+                scriptModule,
+                new IndexSettingsModule(index, settings),
+                new AbstractModule() {
+                    @Override
+                    protected void configure() {
+                        bind(Client.class).toInstance(proxy); // not needed here
+                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
+                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
+                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
+                    }
+                }
+        ).createInjector();
+
+        AnalysisService analysisService = new AnalysisRegistry(null, new Environment(settings)).build(idxSettings);
+        ScriptService scriptService = injector.getInstance(ScriptService.class);
+        SimilarityService similarityService = new SimilarityService(idxSettings, Collections.emptyMap());
+        MapperRegistry mapperRegistry = new IndicesModule().getMapperRegistry();
+        MapperService mapperService = new MapperService(idxSettings, analysisService, similarityService, mapperRegistry);
+        IndexFieldDataService indexFieldDataService =new IndexFieldDataService(idxSettings, injector.getInstance(IndicesFieldDataCache.class), injector.getInstance(CircuitBreakerService.class), mapperService);
+        BitsetFilterCache bitsetFilterCache = new BitsetFilterCache(idxSettings, new IndicesWarmer(idxSettings.getNodeSettings(), null), new BitsetFilterCache.Listener() {
+            @Override
+            public void onCache(ShardId shardId, Accountable accountable) {
+
+            }
+
+            @Override
+            public void onRemoval(ShardId shardId, Accountable accountable) {
+
+            }
+        });
+        IndicesQueriesRegistry indicesQueriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
+        context = new QueryShardContext(idxSettings, proxy, bitsetFilterCache, indexFieldDataService, mapperService, similarityService, scriptService, indicesQueriesRegistry);
+    }
+
+    @Override
+    @After
+    public void tearDown() throws Exception {
+        super.tearDown();
+        terminate(injector.getInstance(ThreadPool.class));
+    }
+
+    public void testParser() throws IOException {
+        String templateString = "{" + "\"query\":{\"match_{{template}}\": {}}," + "\"params\":{\"template\":\"all\"}" + "}";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+        templateSourceParser.nextToken();
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+
+    public void testParseTemplateAsSingleStringWithConditionalClause() throws IOException {
+        String templateString = "{" + "  \"inline\" : \"{ \\\"match_{{#use_it}}{{template}}{{/use_it}}\\\":{} }\"," + "  \"params\":{"
+                + "    \"template\":\"all\"," + "    \"use_it\": true" + "  }" + "}";
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+
+    /**
+     * Test that the template query parser can parse and evaluate template
+     * expressed as a single string but still it expects only the query
+     * specification (thus this test should fail with specific exception).
+     */
+    public void testParseTemplateFailsToParseCompleteQueryAsSingleString() throws IOException {
+        String templateString = "{" + "  \"inline\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
+                + "  \"params\":{" + "    \"size\":2" + "  }\n" + "}";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        try {
+            parser.fromXContent(context.parseContext()).toQuery(context);
+            fail("Expected ParsingException");
+        } catch (ParsingException e) {
+            assertThat(e.getMessage(), containsString("query malformed, no field after start_object"));
+        }
+    }
+
+    public void testParserCanExtractTemplateNames() throws Exception {
+        String templateString = "{ \"file\": \"storedTemplate\" ,\"params\":{\"template\":\"all\" } } ";
+
+        XContentParser templateSourceParser = XContentFactory.xContent(templateString).createParser(templateString);
+        context.reset(templateSourceParser);
+        templateSourceParser.nextToken();
+
+        TemplateQueryParser parser = injector.getInstance(TemplateQueryParser.class);
+        Query query = parser.fromXContent(context.parseContext()).toQuery(context);
+        assertTrue("Parsing template query failed.", query instanceof MatchAllDocsQuery);
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
new file mode 100644
index 0000000..7029826
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
@@ -0,0 +1,515 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.messy.tests;
+
+import org.elasticsearch.action.index.IndexRequest.OpType;
+import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.indexedscripts.delete.DeleteIndexedScriptResponse;
+import org.elasticsearch.action.indexedscripts.get.GetIndexedScriptResponse;
+import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder;
+import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
+import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.index.query.TemplateQueryBuilder;
+import org.elasticsearch.index.query.TemplateQueryParser;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.ScriptService.ScriptType;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustachePlugin;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
+import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+
+/**
+ * Full integration test of the template query plugin.
+ */
+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.SUITE)
+public class TemplateQueryTests extends ESIntegTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Collections.singleton(MustachePlugin.class);
+    }
+
+    @Before
+    public void setup() throws IOException {
+        createIndex("test");
+        ensureGreen("test");
+
+        index("test", "testtype", "1", jsonBuilder().startObject().field("text", "value1").endObject());
+        index("test", "testtype", "2", jsonBuilder().startObject().field("text", "value2").endObject());
+        refresh();
+    }
+
+    @Override
+    public Settings nodeSettings(int nodeOrdinal) {
+        return settingsBuilder().put(super.nodeSettings(nodeOrdinal))
+                .put("path.conf", this.getDataPath("config")).build();
+    }
+
+    public void testTemplateInBody() throws IOException {
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("template", "all");
+
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("{\"match_{{template}}\": {}}\"", ScriptType.INLINE, null,
+                null, vars));
+        SearchResponse sr = client().prepareSearch().setQuery(builder)
+                .execute().actionGet();
+        assertHitCount(sr, 2);
+    }
+
+    public void testTemplateInBodyWithSize() throws IOException {
+        Map<String, Object> params = new HashMap<>();
+        params.put("template", "all");
+        SearchResponse sr = client().prepareSearch()
+                .setSource(
+                        new SearchSourceBuilder().size(0).query(
+                                QueryBuilders.templateQuery(new Template("{ \"match_{{template}}\": {} }",
+                                        ScriptType.INLINE, null, null, params)))).execute()
+                .actionGet();
+        assertNoFailures(sr);
+        assertThat(sr.getHits().hits().length, equalTo(0));
+    }
+
+    public void testTemplateWOReplacementInBody() throws IOException {
+        Map<String, Object> vars = new HashMap<>();
+
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
+                "{\"match_all\": {}}\"", ScriptType.INLINE, null, null, vars));
+        SearchResponse sr = client().prepareSearch().setQuery(builder)
+                .execute().actionGet();
+        assertHitCount(sr, 2);
+    }
+
+    public void testTemplateInFile() {
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("template", "all");
+
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
+                "storedTemplate", ScriptService.ScriptType.FILE, null, null, vars));
+        SearchResponse sr = client().prepareSearch().setQuery(builder)
+                .execute().actionGet();
+        assertHitCount(sr, 2);
+    }
+
+    public void testRawFSTemplate() throws IOException {
+        Map<String, Object> params = new HashMap<>();
+        params.put("template", "all");
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("storedTemplate", ScriptType.FILE, null, null, params));
+        SearchResponse sr = client().prepareSearch().setQuery(builder).get();
+        assertHitCount(sr, 2);
+    }
+
+    public void testSearchRequestTemplateSource() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+
+        String query = "{ \"template\" : { \"query\": {\"match_{{template}}\": {} } }, \"params\" : { \"template\":\"all\" } }";
+        searchRequest.template(parseTemplate(query));
+
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertHitCount(searchResponse, 2);
+    }
+
+    private Template parseTemplate(String template) throws IOException {
+        try (XContentParser parser = XContentFactory.xContent(template).createParser(template)) {
+            return TemplateQueryParser.parse(parser, ParseFieldMatcher.EMPTY, "params", "template");
+        }
+    }
+
+    // Releates to #6318
+    public void testSearchRequestFail() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        try {
+            String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  } }";
+            searchRequest.template(parseTemplate(query));
+            client().search(searchRequest).get();
+            fail("expected exception");
+        } catch (Exception ex) {
+            // expected - no params
+        }
+        String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  }, \"params\" : { \"my_size\": 1 } }";
+        searchRequest.template(parseTemplate(query));
+
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    public void testThatParametersCanBeSet() throws Exception {
+        index("test", "type", "1", jsonBuilder().startObject().field("theField", "foo").endObject());
+        index("test", "type", "2", jsonBuilder().startObject().field("theField", "foo 2").endObject());
+        index("test", "type", "3", jsonBuilder().startObject().field("theField", "foo 3").endObject());
+        index("test", "type", "4", jsonBuilder().startObject().field("theField", "foo 4").endObject());
+        index("test", "type", "5", jsonBuilder().startObject().field("otherField", "foo").endObject());
+        refresh();
+
+        Map<String, Object> templateParams = new HashMap<>();
+        templateParams.put("mySize", "2");
+        templateParams.put("myField", "theField");
+        templateParams.put("myValue", "foo");
+
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type")
+                .setTemplate(new Template("full-query-template", ScriptType.FILE, MustacheScriptEngineService.NAME, null, templateParams))
+                .get();
+        assertHitCount(searchResponse, 4);
+        // size kicks in here...
+        assertThat(searchResponse.getHits().getHits().length, is(2));
+
+        templateParams.put("myField", "otherField");
+        searchResponse = client().prepareSearch("test").setTypes("type")
+                .setTemplate(new Template("full-query-template", ScriptType.FILE, MustacheScriptEngineService.NAME, null, templateParams))
+                .get();
+        assertHitCount(searchResponse, 1);
+    }
+
+    public void testSearchTemplateQueryFromFile() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        String query = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
+                + "    \"myField\": \"text\"," + "    \"myValue\": \"value1\"" + "  }" + "}";
+        searchRequest.template(parseTemplate(query));
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    /**
+     * Test that template can be expressed as a single escaped string.
+     */
+    public void testTemplateQueryAsEscapedString() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        String query = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
+                + "  \"params\":{" + "    \"size\": 1" + "  }" + "}";
+        searchRequest.template(parseTemplate(query));
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    /**
+     * Test that template can contain conditional clause. In this case it is at
+     * the beginning of the string.
+     */
+    public void testTemplateQueryAsEscapedStringStartingWithConditionalClause() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        String templateString = "{"
+                + "  \"template\" : \"{ {{#use_size}} \\\"size\\\": \\\"{{size}}\\\", {{/use_size}} \\\"query\\\":{\\\"match_all\\\":{}}}\","
+                + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
+        searchRequest.template(parseTemplate(templateString));
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    /**
+     * Test that template can contain conditional clause. In this case it is at
+     * the end of the string.
+     */
+    public void testTemplateQueryAsEscapedStringWithConditionalClauseAtEnd() throws Exception {
+        SearchRequest searchRequest = new SearchRequest();
+        searchRequest.indices("_all");
+        String templateString = "{"
+                + "  \"inline\" : \"{ \\\"query\\\":{\\\"match_all\\\":{}} {{#use_size}}, \\\"size\\\": \\\"{{size}}\\\" {{/use_size}} }\","
+                + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
+        searchRequest.template(parseTemplate(templateString));
+        SearchResponse searchResponse = client().search(searchRequest).get();
+        assertThat(searchResponse.getHits().hits().length, equalTo(1));
+    }
+
+    public void testIndexedTemplateClient() throws Exception {
+        createIndex(ScriptService.SCRIPT_INDEX);
+        ensureGreen(ScriptService.SCRIPT_INDEX);
+
+        PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "testTemplate", "{" +
+                "\"template\":{" +
+                "                \"query\":{" +
+                "                   \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                "}" +
+                "}").get();
+
+        assertTrue(scriptResponse.isCreated());
+
+        scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "testTemplate", "{" +
+                "\"template\":{" +
+                "                \"query\":{" +
+                "                   \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                "}" +
+                "}").get();
+
+        assertEquals(scriptResponse.getVersion(), 2);
+
+        GetIndexedScriptResponse getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "testTemplate").get();
+        assertTrue(getResponse.isExists());
+
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+
+        builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
+        builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
+        builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
+        builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
+        builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
+
+        indexRandom(true, builders);
+
+        Map<String, Object> templateParams = new HashMap<>();
+        templateParams.put("fieldParam", "foo");
+
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type")
+                .setTemplate(new Template("testTemplate", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
+                .get();
+        assertHitCount(searchResponse, 4);
+
+        DeleteIndexedScriptResponse deleteResponse = client().prepareDeleteIndexedScript(MustacheScriptEngineService.NAME, "testTemplate")
+                .get();
+        assertTrue(deleteResponse.isFound());
+
+        getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "testTemplate").get();
+        assertFalse(getResponse.isExists());
+
+        try {
+            client().prepareSearch("test")
+                    .setTypes("type")
+                    .setTemplate(
+                            new Template("/template_index/mustache/1000", ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                    templateParams)).get();
+            fail("Expected SearchPhaseExecutionException");
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString(), containsString("Illegal index script format"));
+        }
+    }
+
+    public void testIndexedTemplate() throws Exception {
+        createIndex(ScriptService.SCRIPT_INDEX);
+        ensureGreen(ScriptService.SCRIPT_INDEX);
+        List<IndexRequestBuilder> builders = new ArrayList<>();
+        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "1a").setSource("{" +
+                "\"template\":{"+
+                "                \"query\":{" +
+                "                   \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                    "}" +
+                "}"));
+        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "2").setSource("{" +
+                "\"template\":{"+
+                "                \"query\":{" +
+                "                   \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                    "}" +
+                "}"));
+
+        builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "3").setSource("{" +
+                "\"template\":{"+
+                "             \"match\":{" +
+                "                    \"theField\" : \"{{fieldParam}}\"}" +
+                "       }" +
+                "}"));
+
+        indexRandom(true, builders);
+
+        builders.clear();
+
+        builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
+        builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
+        builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
+        builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
+        builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
+
+        indexRandom(true, builders);
+
+        Map<String, Object> templateParams = new HashMap<>();
+        templateParams.put("fieldParam", "foo");
+
+        SearchResponse searchResponse = client()
+                .prepareSearch("test")
+                .setTypes("type")
+                .setTemplate(
+                        new Template("/mustache/1a", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                templateParams)).get();
+        assertHitCount(searchResponse, 4);
+
+        try {
+            client().prepareSearch("test")
+                    .setTypes("type")
+                    .setTemplate(
+                            new Template("/template_index/mustache/1000", ScriptService.ScriptType.INDEXED,
+                                    MustacheScriptEngineService.NAME, null, templateParams)).get();
+            fail("shouldn't get here");
+        } catch (SearchPhaseExecutionException spee) {
+            //all good
+        }
+
+        try {
+            searchResponse = client()
+                    .prepareSearch("test")
+                    .setTypes("type")
+                    .setTemplate(
+                            new Template("/myindex/mustache/1", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                    templateParams)).get();
+            assertFailures(searchResponse);
+        } catch (SearchPhaseExecutionException spee) {
+            //all good
+        }
+
+        searchResponse = client().prepareSearch("test").setTypes("type")
+                .setTemplate(new Template("1a", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
+                .get();
+        assertHitCount(searchResponse, 4);
+
+        templateParams.put("fieldParam", "bar");
+        searchResponse = client()
+                .prepareSearch("test")
+                .setTypes("type")
+                .setTemplate(
+                        new Template("/mustache/2", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                templateParams)).get();
+        assertHitCount(searchResponse, 1);
+
+        Map<String, Object> vars = new HashMap<>();
+        vars.put("fieldParam", "bar");
+
+        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template(
+                "3", ScriptService.ScriptType.INDEXED, null, null, vars));
+        SearchResponse sr = client().prepareSearch().setQuery(builder)
+                .execute().actionGet();
+        assertHitCount(sr, 1);
+
+        // "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
+        Map<String, Object> params = new HashMap<>();
+        params.put("fieldParam", "foo");
+        TemplateQueryBuilder templateQuery = new TemplateQueryBuilder(new Template("3", ScriptType.INDEXED, null, null, params));
+        sr = client().prepareSearch().setQuery(templateQuery).get();
+        assertHitCount(sr, 4);
+
+        templateQuery = new TemplateQueryBuilder(new Template("/mustache/3", ScriptType.INDEXED, null, null, params));
+        sr = client().prepareSearch().setQuery(templateQuery).get();
+        assertHitCount(sr, 4);
+    }
+
+    // Relates to #10397
+    public void testIndexedTemplateOverwrite() throws Exception {
+        createIndex("testindex");
+        ensureGreen("testindex");
+
+        index("testindex", "test", "1", jsonBuilder().startObject().field("searchtext", "dev1").endObject());
+        refresh();
+
+        int iterations = randomIntBetween(2, 11);
+        for (int i = 1; i < iterations; i++) {
+            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
+                    "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"ooophrase_prefix\"}}}}").get();
+            assertEquals(i * 2 - 1, scriptResponse.getVersion());
+
+            GetIndexedScriptResponse getResponse = client().prepareGetIndexedScript(MustacheScriptEngineService.NAME, "git01").get();
+            assertTrue(getResponse.isExists());
+
+            Map<String, Object> templateParams = new HashMap<>();
+            templateParams.put("P_Keyword1", "dev");
+
+            try {
+                client().prepareSearch("testindex")
+                        .setTypes("test")
+                        .setTemplate(
+                                new Template("git01", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                        templateParams)).get();
+                fail("Broken test template is parsing w/o error.");
+            } catch (SearchPhaseExecutionException e) {
+                // the above is expected to fail
+            }
+
+            PutIndexedScriptRequestBuilder builder = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
+                    "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"phrase_prefix\"}}}}").setOpType(
+                    OpType.INDEX);
+            scriptResponse = builder.get();
+            assertEquals(i * 2, scriptResponse.getVersion());
+            SearchResponse searchResponse = client()
+                    .prepareSearch("testindex")
+                    .setTypes("test")
+                    .setTemplate(
+                            new Template("git01", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null, templateParams))
+                    .get();
+            assertHitCount(searchResponse, 1);
+        }
+    }
+
+    public void testIndexedTemplateWithArray() throws Exception {
+      createIndex(ScriptService.SCRIPT_INDEX);
+      ensureGreen(ScriptService.SCRIPT_INDEX);
+      List<IndexRequestBuilder> builders = new ArrayList<>();
+
+      String multiQuery = "{\"query\":{\"terms\":{\"theField\":[\"{{#fieldParam}}\",\"{{.}}\",\"{{/fieldParam}}\"]}}}";
+
+      builders.add(client().prepareIndex(ScriptService.SCRIPT_INDEX, MustacheScriptEngineService.NAME, "4").setSource(jsonBuilder().startObject().field("template", multiQuery).endObject()));
+
+      indexRandom(true,builders);
+
+      builders.clear();
+
+      builders.add(client().prepareIndex("test", "type", "1").setSource("{\"theField\":\"foo\"}"));
+      builders.add(client().prepareIndex("test", "type", "2").setSource("{\"theField\":\"foo 2\"}"));
+      builders.add(client().prepareIndex("test", "type", "3").setSource("{\"theField\":\"foo 3\"}"));
+      builders.add(client().prepareIndex("test", "type", "4").setSource("{\"theField\":\"foo 4\"}"));
+      builders.add(client().prepareIndex("test", "type", "5").setSource("{\"theField\":\"bar\"}"));
+
+      indexRandom(true,builders);
+
+      Map<String, Object> arrayTemplateParams = new HashMap<>();
+      String[] fieldParams = {"foo","bar"};
+      arrayTemplateParams.put("fieldParam", fieldParams);
+
+        SearchResponse searchResponse = client()
+                .prepareSearch("test")
+                .setTypes("type")
+                .setTemplate(
+                        new Template("/mustache/4", ScriptService.ScriptType.INDEXED, MustacheScriptEngineService.NAME, null,
+                                arrayTemplateParams)).get();
+        assertHitCount(searchResponse, 5);
+    }
+
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/package-info.java b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/package-info.java
new file mode 100644
index 0000000..a2325b2
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/package-info.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+/**
+ * This package contains tests that use mustache to test what looks
+ * to be unrelated functionality, or functionality that should be 
+ * tested with a mock instead. Instead of doing an epic battle
+ * with these tests, they are temporarily moved here to the mustache
+ * module's tests, but that is likely not where they belong. Please 
+ * help by cleaning them up and we can remove this package!
+ *
+ * <ul>
+ *   <li>If the test is actually testing mustache specifically, move to 
+ *       the org.elasticsearch.script.mustache tests package of this module</li>
+ *   <li>If the test is testing templating integration with another core subsystem,
+ *       fix it to use a mock instead, so it can be in the core tests again</li>
+ *   <li>If the test is just being lazy, and does not really need templating to test
+ *       something, clean it up!</li>
+ * </ul>
+ */
+/* List of renames that took place:  
+renamed:    core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java -> modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/RenderSearchTemplateTests.java
+renamed:    core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java -> modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/SuggestSearchTests.java
+renamed:    core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTests.java -> modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryParserTests.java
+renamed:    core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java -> modules/lang-mustache/src/test/java/org/elasticsearch/messy/tests/TemplateQueryTests.java
+renamed:    core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java -> module/lang-mustache/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+  ^^^^^ note: just the methods from this test using mustache were moved here, the others use groovy and are in the groovy module under its messy tests package.
+renamed:    rest-api-spec/test/msearch/10_basic.yaml -> module/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml 
+ */
+
+package org.elasticsearch.messy.tests;
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheRestIT.java b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheRestIT.java
new file mode 100644
index 0000000..0c489b3
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheRestIT.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script.mustache;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+public class MustacheRestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(MustachePlugin.class);
+    }
+
+    public MustacheRestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
new file mode 100644
index 0000000..ce29bf2
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
@@ -0,0 +1,170 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.io.IOException;
+import java.io.StringWriter;
+import java.nio.charset.Charset;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.hamcrest.Matchers.equalTo;
+
+/**
+ * Mustache based templating test
+ */
+public class MustacheScriptEngineTests extends ESTestCase {
+    private MustacheScriptEngineService qe;
+    private JsonEscapingMustacheFactory escaper;
+
+    @Before
+    public void setup() {
+        qe = new MustacheScriptEngineService(Settings.Builder.EMPTY_SETTINGS);
+        escaper = new JsonEscapingMustacheFactory();
+    }
+
+    public void testSimpleParameterReplace() {
+        {
+            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
+            Map<String, Object> vars = new HashMap<>();
+            vars.put("boost_val", "0.3");
+            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
+            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.3 } }}",
+                    new String(o.toBytes(), Charset.forName("UTF-8")));
+        }
+        {
+            String template = "GET _search {\"query\": " + "{\"boosting\": {" + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"{{body_val}}\"}" + "}}, \"negative_boost\": {{boost_val}} } }}";
+            Map<String, Object> vars = new HashMap<>();
+            vars.put("boost_val", "0.3");
+            vars.put("body_val", "\"quick brown\"");
+            BytesReference o = (BytesReference) qe.executable(new CompiledScript(ScriptService.ScriptType.INLINE, "", "mustache", qe.compile(template)), vars).run();
+            assertEquals("GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                    + "\"negative\": {\"term\": {\"body\": {\"value\": \"\\\"quick brown\\\"\"}}}, \"negative_boost\": 0.3 } }}",
+                    new String(o.toBytes(), Charset.forName("UTF-8")));
+        }
+    }
+
+    public void testEscapeJson() throws IOException {
+        {
+            StringWriter writer = new StringWriter();
+            escaper.encode("hello \n world", writer);
+            assertThat(writer.toString(), equalTo("hello \\n world"));
+        }
+        {
+            StringWriter writer = new StringWriter();
+            escaper.encode("\n", writer);
+            assertThat(writer.toString(), equalTo("\\n"));
+        }
+
+        Character[] specialChars = new Character[]{
+                '\"',
+                '\\',
+                '\u0000',
+                '\u0001',
+                '\u0002',
+                '\u0003',
+                '\u0004',
+                '\u0005',
+                '\u0006',
+                '\u0007',
+                '\u0008',
+                '\u0009',
+                '\u000B',
+                '\u000C',
+                '\u000E',
+                '\u000F',
+                '\u001F'};
+        String[] escapedChars = new String[]{
+                "\\\"",
+                "\\\\",
+                "\\u0000",
+                "\\u0001",
+                "\\u0002",
+                "\\u0003",
+                "\\u0004",
+                "\\u0005",
+                "\\u0006",
+                "\\u0007",
+                "\\u0008",
+                "\\u0009",
+                "\\u000B",
+                "\\u000C",
+                "\\u000E",
+                "\\u000F",
+                "\\u001F"};
+        int iters = scaledRandomIntBetween(100, 1000);
+        for (int i = 0; i < iters; i++) {
+            int rounds = scaledRandomIntBetween(1, 20);
+            StringWriter expect = new StringWriter();
+            StringWriter writer = new StringWriter();
+            for (int j = 0; j < rounds; j++) {
+                String s = getChars();
+                writer.write(s);
+                expect.write(s);
+
+                int charIndex = randomInt(7);
+                writer.append(specialChars[charIndex]);
+                expect.append(escapedChars[charIndex]);
+            }
+            StringWriter target = new StringWriter();
+            escaper.encode(writer.toString(), target);
+            assertThat(expect.toString(), equalTo(target.toString()));
+        }
+    }
+
+    private String getChars() {
+        String string = randomRealisticUnicodeOfCodepointLengthBetween(0, 10);
+        for (int i = 0; i < string.length(); i++) {
+            if (isEscapeChar(string.charAt(i))) {
+                return string.substring(0, i);
+            }
+        }
+        return string;
+    }
+
+    /**
+     * From https://www.ietf.org/rfc/rfc4627.txt:
+     *
+     * All Unicode characters may be placed within the
+     * quotation marks except for the characters that must be escaped:
+     * quotation mark, reverse solidus, and the control characters (U+0000
+     * through U+001F).
+     * */
+    private static boolean isEscapeChar(char c) {
+        switch (c) {
+        case '"':
+        case '\\':
+            return true;
+        }
+
+        if (c < '\u002F')
+            return true;
+        return false;
+    }
+}
diff --git a/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
new file mode 100644
index 0000000..76c8678
--- /dev/null
+++ b/modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheTests.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.elasticsearch.script.mustache;
+
+import com.github.mustachejava.DefaultMustacheFactory;
+import com.github.mustachejava.Mustache;
+import com.github.mustachejava.MustacheFactory;
+
+import org.elasticsearch.test.ESTestCase;
+
+import java.io.StringReader;
+import java.io.StringWriter;
+import java.util.HashMap;
+
+/**
+ * Figure out how Mustache works for the simplest use case. Leaving in here for now for reference.
+ * */
+public class MustacheTests extends ESTestCase {
+    public void test() {
+        HashMap<String, Object> scopes = new HashMap<>();
+        scopes.put("boost_val", "0.2");
+
+        String template = "GET _search {\"query\": " + "{\"boosting\": {"
+                + "\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}"
+                + "}}, \"negative_boost\": {{boost_val}} } }}";
+        MustacheFactory f = new DefaultMustacheFactory();
+        Mustache mustache = f.compile(new StringReader(template), "example");
+        StringWriter writer = new StringWriter();
+        mustache.execute(writer, scopes);
+        writer.flush();
+        assertEquals(
+                "Mustache templating broken",
+                "GET _search {\"query\": {\"boosting\": {\"positive\": {\"match\": {\"body\": \"gift\"}},"
+                        + "\"negative\": {\"term\": {\"body\": {\"value\": \"solr\"}}}, \"negative_boost\": 0.2 } }}",
+                writer.toString());
+    }
+}
diff --git a/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/full-query-template.mustache b/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/full-query-template.mustache
new file mode 100644
index 0000000..5191414
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/full-query-template.mustache
@@ -0,0 +1,6 @@
+{
+  "query": {
+    "match": { "{{myField}}" : "{{myValue}}" }
+  },
+  "size" : {{mySize}}
+}
diff --git a/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/storedTemplate.mustache b/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/storedTemplate.mustache
new file mode 100644
index 0000000..a779da7
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/org/elasticsearch/messy/tests/config/scripts/storedTemplate.mustache
@@ -0,0 +1,3 @@
+{
+    "match_{{template}}": {}
+}
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml
new file mode 100644
index 0000000..9bfea28
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/10_basic.yaml
@@ -0,0 +1,71 @@
+# Integration tests for Mustache scripts
+#
+"Mustache loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.modules.0.name: lang-mustache  }
+    - match:  { nodes.$master.modules.0.jvm: true  }
+
+---
+"Indexed template":
+
+  - do:
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match_all": {}}, "size": "{{my_size}}" } }
+  - match: { _id: "1" }
+
+  - do:
+      get_template:
+        id: 1
+  - match: { found: true }
+  - match: { lang: mustache }
+  - match: { _id: "1" }
+  - match: { _version: 1 }
+  - match: { template: /.*query\S\S\S\Smatch_all.*/ }
+
+  - do:
+      catch: missing
+      get_template:
+        id: 2
+  - match: { found: false }
+  - match: { lang: mustache }
+  - match: { _id: "2" }
+  - is_false: _version
+  - is_false: template
+
+  - do:
+      delete_template:
+        id: "1"
+  - match: { found: true }
+  - match: { _index: ".scripts" }
+  - match: { _id: "1" }
+  - match: { _version: 2}
+
+  - do:
+      catch: missing
+      delete_template:
+        id: "non_existing"
+  - match: { found: false }
+  - match: { _index: ".scripts" }
+  - match: { _id: "non_existing" }
+  - match: { _version: 1 }
+
+  - do:
+      catch: request
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match{{}}_all": {}}, "size": "{{my_size}}" } }
+
+  - do:
+      catch: /Unable\sto\sparse.*/
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match{{}}_all": {}}, "size": "{{my_size}}" } }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/20_search.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/20_search.yaml
new file mode 100644
index 0000000..4da748a
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/20_search.yaml
@@ -0,0 +1,38 @@
+---
+"Indexed Template query tests":
+
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     1
+        body:   { "text": "value1_foo" }
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     2
+        body:   { "text": "value2_foo value3_foo" }
+  - do:
+      indices.refresh: {}
+
+  - do:
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match" : { "text": "{{my_value}}" } }, "size": "{{my_size}}" } }
+  - match: { _id: "1" }
+
+  - do:
+      indices.refresh: {}
+
+
+  - do:
+      search_template:
+        body: {  "id" : "1", "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
+  - match: { hits.total: 1 }
+
+  - do:
+      catch: /Unable.to.find.on.disk.file.script.\[simple1\].using.lang.\[mustache\]/
+      search_template:
+        body: { "file" : "simple1"}
+
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_render_search_template.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_render_search_template.yaml
new file mode 100644
index 0000000..5d5c3d5
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_render_search_template.yaml
@@ -0,0 +1,110 @@
+---
+"Indexed Template validate tests":
+
+  - do:
+      put_template:
+        id: "1"
+        body: { "template": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } } }
+  - match: { _id: "1" }
+
+  - do:
+      indices.refresh: {}
+
+  - do:
+      render_search_template:
+        body: { "id": "1", "params": { "my_value": "foo", "my_field": "field1" } }
+
+  - match: { template_output.query.match.text: "foo" }
+  - match: { template_output.aggs.my_terms.terms.field: "field1" }
+
+  - do:
+      render_search_template:
+        body: { "id": "1", "params": { "my_value": "bar", "my_field": "my_other_field" } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.aggs.my_terms.terms.field: "my_other_field" }
+
+  - do:
+      render_search_template:
+        id: "1"
+        body: { "params": { "my_value": "bar", "my_field": "field1" } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.aggs.my_terms.terms.field: "field1" }
+
+---
+"Inline Template validate tests":
+
+  - do:
+      render_search_template:
+        body: { "inline": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "foo", "my_field": "field1" } }
+
+  - match: { template_output.query.match.text: "foo" }
+  - match: { template_output.aggs.my_terms.terms.field: "field1" }
+
+  - do:
+      render_search_template:
+        body: { "inline": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "bar", "my_field": "my_other_field" } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.aggs.my_terms.terms.field: "my_other_field" }
+
+  - do:
+      catch: /Improperly.closed.variable.in.query-template/
+      render_search_template:
+        body: { "inline": { "query": { "match": { "text": "{{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "bar", "my_field": "field1" } }
+---
+"Escaped Indexed Template validate tests":
+
+  - do:
+      put_template:
+        id: "1"
+        body: { "template": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }" }
+  - match: { _id: "1" }
+
+  - do:
+      indices.refresh: {}
+
+  - do:
+      render_search_template:
+        body: { "id": "1", "params": { "my_value": "foo", "my_size": 20 } }
+
+  - match: { template_output.query.match.text: "foo" }
+  - match: { template_output.size: 20 }
+
+  - do:
+      render_search_template:
+        body: { "id": "1", "params": { "my_value": "bar", "my_size": 100 } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.size: 100 }
+
+  - do:
+      render_search_template:
+        id: "1"
+        body: { "params": { "my_value": "bar", "my_size": 100 } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.size: 100 }
+
+---
+"Escaped Inline Template validate tests":
+
+  - do:
+      render_search_template:
+        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "foo", "my_size": 20 } }
+
+  - match: { template_output.query.match.text: "foo" }
+  - match: { template_output.size: 20 }
+
+  - do:
+      render_search_template:
+        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "bar", "my_size": 100 } }
+
+  - match: { template_output.query.match.text: "bar" }
+  - match: { template_output.size: 100 }
+
+  - do:
+      catch: /Improperly.closed.variable.in.query-template/
+      render_search_template:
+        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "bar", "my_size": 100 } }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_template_query_execution.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_template_query_execution.yaml
new file mode 100644
index 0000000..d2474b7
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/30_template_query_execution.yaml
@@ -0,0 +1,48 @@
+---
+"Template query":
+
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     1
+        body:   { "text": "value1" }
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     2
+        body:   { "text": "value2 value3" }
+  - do:
+      indices.refresh: {}
+
+  - do:
+      search:
+        body: { "query": { "template": { "query": { "term": { "text": { "value": "{{template}}" } } }, "params": { "template": "value1" } } } }
+
+  - match: { hits.total: 1 }
+
+  - do:
+      search: 
+        body: { "query": { "template": { "query": {"match_{{template}}": {}}, "params" : { "template" : "all" } } } }
+
+  - match: { hits.total: 2 }
+
+  - do:
+      search:
+        body: { "query": { "template": { "query": "{ \"term\": { \"text\": { \"value\": \"{{template}}\" } } }", "params": { "template": "value1" } } } }
+
+  - match: { hits.total: 1 }
+
+  - do:
+      search:
+        body: { "query": { "template": { "query": "{\"match_{{template}}\": {}}", "params" : { "template" : "all" } } } }
+
+  - match: { hits.total: 2 }
+
+  - do:
+      search:
+        body: { "query": { "template": { "query": "{\"query_string\": { \"query\" : \"{{query}}\" }}", "params" : { "query" : "text:\"value2 value3\"" } } } }
+
+
+  - match: { hits.total: 1 }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/40_search_request_template.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/40_search_request_template.yaml
new file mode 100644
index 0000000..a0a6695
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/40_search_request_template.yaml
@@ -0,0 +1,29 @@
+---
+"Template search request":
+
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     1
+        body:   { "text": "value1" }
+  - do:
+      index:
+        index:  test
+        type:   testtype
+        id:     2
+        body:   { "text": "value2" }
+  - do:
+      indices.refresh: {}
+
+  - do:
+      search_template:
+        body: { "template" : { "query": { "term": { "text": { "value": "{{template}}" } } } }, "params": { "template": "value1" } }
+
+  - match: { hits.total: 1 }
+
+  - do:
+      search_template:
+        body: { "template" : { "query": { "match_{{template}}": {} } }, "params" : { "template" : "all" } }
+
+  - match: { hits.total: 2 }
diff --git a/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml
new file mode 100644
index 0000000..205070be
--- /dev/null
+++ b/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml
@@ -0,0 +1,54 @@
+---
+"Basic multi-search":
+  - do:
+      index:
+          index:  test_1
+          type:   test
+          id:     1
+          body:   { foo: bar }
+
+  - do:
+      index:
+          index:  test_1
+          type:   test
+          id:     2
+          body:   { foo: baz }
+
+  - do:
+      index:
+          index:  test_1
+          type:   test
+          id:     3
+          body:   { foo: foo }
+
+  - do:
+      indices.refresh: {}
+
+  - do:
+      msearch:
+        body:
+          - index: test_1
+          - query:
+              match_all: {}
+          - index: test_2
+          - query:
+              match_all: {}
+          - search_type: query_then_fetch
+            index: test_1
+          - query:
+              match: {foo: bar}
+
+  - match:  { responses.0.hits.total:     3  }
+  - match:  { responses.1.error.root_cause.0.type: index_not_found_exception }
+  - match:  { responses.1.error.root_cause.0.reason: "/no.such.index/" }
+  - match:  { responses.1.error.root_cause.0.index: test_2 }
+  - match:  { responses.2.hits.total:     1  }
+
+  - do:
+      msearch:
+        body: 
+          - index: test_1
+          - query:
+              { "template": { "query": { "term": { "foo": { "value": "{{template}}" } } }, "params": { "template": "bar" } } }
+  - match: { responses.0.hits.total: 1 }
+
diff --git a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 22ca2f2..0000000
--- a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-d957c3956797c9c057e65f6342eeb104fa20951e
\ No newline at end of file
diff --git a/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..4942bbc
--- /dev/null
+++ b/plugins/analysis-icu/licenses/lucene-analyzers-icu-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+4e56ba76d6b23756b2bd4d9e42b2b00122cd4fa5
\ No newline at end of file
diff --git a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 3a31061..0000000
--- a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-2ea1253cd3a704dea02382d6f9abe7c2a58874ac
\ No newline at end of file
diff --git a/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..1ba2a93
--- /dev/null
+++ b/plugins/analysis-kuromoji/licenses/lucene-analyzers-kuromoji-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+d6ccac802dc1e4c177be043a173377cf5e517cff
\ No newline at end of file
diff --git a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index 44a895a..0000000
--- a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-7f2132a00895c6eacfc735a4d6056275a692363b
\ No newline at end of file
diff --git a/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..2b61186
--- /dev/null
+++ b/plugins/analysis-phonetic/licenses/lucene-analyzers-phonetic-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+70ad9f6c3738727229867419d949527cc7789f62
\ No newline at end of file
diff --git a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index d00f2ec..0000000
--- a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-5eff0c934476356fcd608d574ce0af4104e5e2a4
\ No newline at end of file
diff --git a/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..e28887a
--- /dev/null
+++ b/plugins/analysis-smartcn/licenses/lucene-analyzers-smartcn-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+75504fd906929700e7d11f9600e4a79de48e1090
\ No newline at end of file
diff --git a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1715952.jar.sha1 b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1715952.jar.sha1
deleted file mode 100644
index ccc82e3..0000000
--- a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.4.0-snapshot-1715952.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-210cab15ecf74e5a1bf35173ef5b0d420475694c
\ No newline at end of file
diff --git a/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.5.0-snapshot-1719088.jar.sha1 b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.5.0-snapshot-1719088.jar.sha1
new file mode 100644
index 0000000..739ecc4
--- /dev/null
+++ b/plugins/analysis-stempel/licenses/lucene-analyzers-stempel-5.5.0-snapshot-1719088.jar.sha1
@@ -0,0 +1 @@
+9eeeeabeab89ec305e831d80bdcc7e85a1140fbb
\ No newline at end of file
diff --git a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
index 2b70834..c44608c 100644
--- a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
+++ b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
@@ -32,6 +32,7 @@ import org.elasticsearch.common.text.StringText;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.util.concurrent.CountDown;
+import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.internal.InternalSearchHit;
@@ -225,7 +226,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
                 } else {
                     deleted++;
                 }
-                items[i] = new BulkItemResponse(i, "delete", new DeleteResponse("test", "type", String.valueOf(i), 1, delete));
+                items[i] = new BulkItemResponse(i, "delete", new DeleteResponse(new ShardId("test", 0), "type", String.valueOf(i), 1, delete));
             } else {
                 items[i] = new BulkItemResponse(i, "delete", new BulkItemResponse.Failure("test", "type", String.valueOf(i), new Throwable("item failed")));
                 failed++;
@@ -281,7 +282,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
                     deleted[0] = deleted[0] + 1;
                     deleted[index] = deleted[index] + 1;
                 }
-                items[i] = new BulkItemResponse(i, "delete", new DeleteResponse("test-" + index, "type", String.valueOf(i), 1, delete));
+                items[i] = new BulkItemResponse(i, "delete", new DeleteResponse(new ShardId("test-" + index, 0), "type", String.valueOf(i), 1, delete));
             } else {
                 items[i] = new BulkItemResponse(i, "delete", new BulkItemResponse.Failure("test-" + index, "type", String.valueOf(i), new Throwable("item failed")));
                 failed[0] = failed[0] + 1;
diff --git a/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java b/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
index 89d6d17..36b20b0 100755
--- a/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
+++ b/plugins/discovery-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
@@ -22,12 +22,12 @@ package org.elasticsearch.discovery.azure;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
 import org.elasticsearch.discovery.zen.ping.ZenPingService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
@@ -40,9 +40,9 @@ public class AzureDiscovery extends ZenDiscovery {
 
     @Inject
     public AzureDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
-                          ClusterService clusterService, ClusterSettings clusterSettings, ZenPingService pingService,
+                          ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
                           DiscoverySettings discoverySettings, ElectMasterService electMasterService) {
-        super(settings, clusterName, threadPool, transportService, clusterService, clusterSettings,
+        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
                 pingService, electMasterService, discoverySettings);
     }
 }
diff --git a/plugins/discovery-ec2/build.gradle b/plugins/discovery-ec2/build.gradle
index 2570661..77cfd66 100644
--- a/plugins/discovery-ec2/build.gradle
+++ b/plugins/discovery-ec2/build.gradle
@@ -42,7 +42,7 @@ dependencyLicenses {
   mapping from: /jackson-.*/, to: 'jackson'
 }
 
-compileJava.options.compilerArgs << '-Xlint:-rawtypes'
+compileJava.options.compilerArgs << '-Xlint:-rawtypes,-deprecation'
 
 test {
   // this is needed for insecure plugins, remove if possible!
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
index a427b4a..d71d9df 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java
@@ -27,20 +27,32 @@ public interface AwsEc2Service extends LifecycleComponent<AwsEc2Service> {
         public static final String KEY = "cloud.aws.access_key";
         public static final String SECRET = "cloud.aws.secret_key";
         public static final String PROTOCOL = "cloud.aws.protocol";
-        public static final String PROXY_HOST = "cloud.aws.proxy_host";
-        public static final String PROXY_PORT = "cloud.aws.proxy_port";
+        public static final String PROXY_HOST = "cloud.aws.proxy.host";
+        public static final String PROXY_PORT = "cloud.aws.proxy.port";
+        public static final String PROXY_USERNAME = "cloud.aws.proxy.username";
+        public static final String PROXY_PASSWORD = "cloud.aws.proxy.password";
         public static final String SIGNER = "cloud.aws.signer";
         public static final String REGION = "cloud.aws.region";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.proxy_host";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.proxy_port";
     }
 
     final class CLOUD_EC2 {
         public static final String KEY = "cloud.aws.ec2.access_key";
         public static final String SECRET = "cloud.aws.ec2.secret_key";
         public static final String PROTOCOL = "cloud.aws.ec2.protocol";
-        public static final String PROXY_HOST = "cloud.aws.ec2.proxy_host";
-        public static final String PROXY_PORT = "cloud.aws.ec2.proxy_port";
+        public static final String PROXY_HOST = "cloud.aws.ec2.proxy.host";
+        public static final String PROXY_PORT = "cloud.aws.ec2.proxy.port";
+        public static final String PROXY_USERNAME = "cloud.aws.ec2.proxy.username";
+        public static final String PROXY_PASSWORD = "cloud.aws.ec2.proxy.password";
         public static final String SIGNER = "cloud.aws.ec2.signer";
         public static final String ENDPOINT = "cloud.aws.ec2.endpoint";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.ec2.proxy_host";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.ec2.proxy_port";
     }
 
     final class DISCOVERY_EC2 {
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
index 76c3262..b6306e6 100644
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/cloud/aws/AwsEc2ServiceImpl.java
@@ -32,6 +32,7 @@ import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.cloud.aws.network.Ec2NameResolver;
 import org.elasticsearch.cloud.aws.node.Ec2CustomNodeAttributes;
 import org.elasticsearch.cluster.node.DiscoveryNodeService;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.network.NetworkService;
@@ -56,8 +57,10 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
         // Filter global settings
         settingsFilter.addFilter(CLOUD_AWS.KEY);
         settingsFilter.addFilter(CLOUD_AWS.SECRET);
+        settingsFilter.addFilter(CLOUD_AWS.PROXY_PASSWORD);
         settingsFilter.addFilter(CLOUD_EC2.KEY);
         settingsFilter.addFilter(CLOUD_EC2.SECRET);
+        settingsFilter.addFilter(CLOUD_EC2.PROXY_PASSWORD);
         // add specific ec2 name resolver
         networkService.addCustomNameResolver(new Ec2NameResolver(settings));
         discoveryNodeService.addCustomAttributeProvider(new Ec2CustomNodeAttributes(settings));
@@ -83,16 +86,25 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
         String account = settings.get(CLOUD_EC2.KEY, settings.get(CLOUD_AWS.KEY));
         String key = settings.get(CLOUD_EC2.SECRET, settings.get(CLOUD_AWS.SECRET));
 
-        String proxyHost = settings.get(CLOUD_EC2.PROXY_HOST, settings.get(CLOUD_AWS.PROXY_HOST));
+        String proxyHost = settings.get(CLOUD_AWS.PROXY_HOST, settings.get(CLOUD_AWS.DEPRECATED_PROXY_HOST));
+        proxyHost = settings.get(CLOUD_EC2.PROXY_HOST, settings.get(CLOUD_EC2.DEPRECATED_PROXY_HOST, proxyHost));
         if (proxyHost != null) {
-            String portString = settings.get(CLOUD_EC2.PROXY_PORT, settings.get(CLOUD_AWS.PROXY_PORT, "80"));
+            String portString = settings.get(CLOUD_AWS.PROXY_PORT, settings.get(CLOUD_AWS.DEPRECATED_PROXY_PORT, "80"));
+            portString = settings.get(CLOUD_EC2.PROXY_PORT, settings.get(CLOUD_EC2.DEPRECATED_PROXY_PORT, portString));
             Integer proxyPort;
             try {
                 proxyPort = Integer.parseInt(portString, 10);
             } catch (NumberFormatException ex) {
                 throw new IllegalArgumentException("The configured proxy port value [" + portString + "] is invalid", ex);
             }
-            clientConfiguration.withProxyHost(proxyHost).setProxyPort(proxyPort);
+            String proxyUsername = settings.get(CLOUD_EC2.PROXY_USERNAME, settings.get(CLOUD_AWS.PROXY_USERNAME));
+            String proxyPassword = settings.get(CLOUD_EC2.PROXY_PASSWORD, settings.get(CLOUD_AWS.PROXY_PASSWORD));
+
+            clientConfiguration
+                .withProxyHost(proxyHost)
+                .withProxyPort(proxyPort)
+                .withProxyUsername(proxyUsername)
+                .withProxyPassword(proxyPassword);
         }
 
         // #155: we might have 3rd party users using older EC2 API version
@@ -108,7 +120,7 @@ public class AwsEc2ServiceImpl extends AbstractLifecycleComponent<AwsEc2Service>
         }
 
         // Increase the number of retries in case of 5xx API responses
-        final Random rand = new Random();
+        final Random rand = Randomness.get();
         RetryPolicy retryPolicy = new RetryPolicy(
                 RetryPolicy.RetryCondition.NO_RETRY_CONDITION,
                 new RetryPolicy.BackoffStrategy() {
diff --git a/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java b/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
index aa3cef0..e94b761 100755
--- a/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
+++ b/plugins/discovery-ec2/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
@@ -22,12 +22,12 @@ package org.elasticsearch.discovery.ec2;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
 import org.elasticsearch.discovery.zen.ping.ZenPingService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
@@ -40,10 +40,10 @@ public class Ec2Discovery extends ZenDiscovery {
 
     @Inject
     public Ec2Discovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
-                        ClusterService clusterService, ClusterSettings clusterSettings, ZenPingService pingService,
+                        ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
                         DiscoverySettings discoverySettings,
                         ElectMasterService electMasterService) {
-        super(settings, clusterName, threadPool, transportService, clusterService, clusterSettings,
+        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
                 pingService, electMasterService, discoverySettings);
     }
 }
diff --git a/plugins/discovery-ec2/src/main/plugin-metadata/plugin-security.policy b/plugins/discovery-ec2/src/main/plugin-metadata/plugin-security.policy
index 42bd707..d5c92a9 100644
--- a/plugins/discovery-ec2/src/main/plugin-metadata/plugin-security.policy
+++ b/plugins/discovery-ec2/src/main/plugin-metadata/plugin-security.policy
@@ -19,7 +19,8 @@
 
 grant {
   // needed because of problems in ClientConfiguration
-  // TODO: get this fixed in aws sdk
+  // TODO: get these fixed in aws sdk
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   // NOTE: no tests fail without this, but we know the problem
   // exists in AWS sdk, and tests here are not thorough
   permission java.lang.RuntimePermission "getClassLoader";
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
index 943b1cd..07e05f0 100644
--- a/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/cloud/gce/GceComputeServiceImpl.java
@@ -45,6 +45,7 @@ import java.io.IOException;
 import java.net.URL;
 import java.security.AccessController;
 import java.security.GeneralSecurityException;
+import java.security.PrivilegedAction;
 import java.security.PrivilegedActionException;
 import java.security.PrivilegedExceptionAction;
 import java.util.*;
@@ -103,23 +104,33 @@ public class GceComputeServiceImpl extends AbstractLifecycleComponent<GceCompute
     public String metadata(String metadataPath) throws IOException {
         String urlMetadataNetwork = GCE_METADATA_URL + "/" + metadataPath;
         logger.debug("get metadata from [{}]", urlMetadataNetwork);
-        URL url = new URL(urlMetadataNetwork);
+        final URL url = new URL(urlMetadataNetwork);
         HttpHeaders headers;
         try {
             // hack around code messiness in GCE code
             // TODO: get this fixed
+            SecurityManager sm = System.getSecurityManager();
+            if (sm != null) {
+                sm.checkPermission(new SpecialPermission());
+            }
             headers = AccessController.doPrivileged(new PrivilegedExceptionAction<HttpHeaders>() {
                 @Override
                 public HttpHeaders run() throws IOException {
                     return new HttpHeaders();
                 }
             });
+            GenericUrl genericUrl = AccessController.doPrivileged(new PrivilegedAction<GenericUrl>() {
+                @Override
+                public GenericUrl run() {
+                    return new GenericUrl(url);
+                }
+            });
 
             // This is needed to query meta data: https://cloud.google.com/compute/docs/metadata
             headers.put("Metadata-Flavor", "Google");
             HttpResponse response;
             response = getGceHttpTransport().createRequestFactory()
-                    .buildGetRequest(new GenericUrl(url))
+                    .buildGetRequest(genericUrl)
                     .setHeaders(headers)
                     .execute();
             String metadata = response.parseAsString();
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
index fe87b92..f20d1c7 100755
--- a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
@@ -22,12 +22,12 @@ package org.elasticsearch.discovery.gce;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.discovery.zen.ZenDiscovery;
 import org.elasticsearch.discovery.zen.elect.ElectMasterService;
 import org.elasticsearch.discovery.zen.ping.ZenPingService;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
@@ -40,10 +40,10 @@ public class GceDiscovery extends ZenDiscovery {
 
     @Inject
     public GceDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
-                        ClusterService clusterService, ClusterSettings clusterSettings, ZenPingService pingService,
+                        ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
                         DiscoverySettings discoverySettings,
                         ElectMasterService electMasterService) {
-        super(settings, clusterName, threadPool, transportService, clusterService, clusterSettings,
+        super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
                 pingService, electMasterService, discoverySettings);
     }
 }
diff --git a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapper.java b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapper.java
index 6d48943..1d73e1d 100644
--- a/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapper.java
+++ b/plugins/discovery-gce/src/main/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapper.java
@@ -20,13 +20,18 @@
 package org.elasticsearch.discovery.gce;
 
 import com.google.api.client.auth.oauth2.Credential;
+import com.google.api.client.googleapis.testing.auth.oauth2.MockGoogleCredential;
 import com.google.api.client.http.*;
 import com.google.api.client.util.ExponentialBackOff;
 import com.google.api.client.util.Sleeper;
+
+import org.elasticsearch.SpecialPermission;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.ESLoggerFactory;
 
 import java.io.IOException;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
 import java.util.Objects;
 
 public class RetryHttpInitializerWrapper implements HttpRequestInitializer {
@@ -60,6 +65,21 @@ public class RetryHttpInitializerWrapper implements HttpRequestInitializer {
         this.sleeper = sleeper;
         this.maxWait = maxWait;
     }
+    
+    // Use only for testing
+    static MockGoogleCredential.Builder newMockCredentialBuilder() {
+        // TODO: figure out why GCE is so bad like this
+        SecurityManager sm = System.getSecurityManager();
+        if (sm != null) {
+            sm.checkPermission(new SpecialPermission());
+        }
+        return AccessController.doPrivileged(new PrivilegedAction<MockGoogleCredential.Builder>() {
+            @Override
+            public MockGoogleCredential.Builder run() {
+                return new MockGoogleCredential.Builder();
+            }
+        });
+    }
 
     @Override
     public void initialize(HttpRequest httpRequest) {
diff --git a/plugins/discovery-gce/src/main/plugin-metadata/plugin-security.policy b/plugins/discovery-gce/src/main/plugin-metadata/plugin-security.policy
index 80a9978..429c472 100644
--- a/plugins/discovery-gce/src/main/plugin-metadata/plugin-security.policy
+++ b/plugins/discovery-gce/src/main/plugin-metadata/plugin-security.policy
@@ -19,5 +19,6 @@
 
 grant {
   // needed because of problems in gce
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   permission java.lang.reflect.ReflectPermission "suppressAccessChecks";
 };
diff --git a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapperTests.java b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapperTests.java
index dcbd878..ef92bd7 100644
--- a/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapperTests.java
+++ b/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/RetryHttpInitializerWrapperTests.java
@@ -97,7 +97,7 @@ public class RetryHttpInitializerWrapperTests extends ESTestCase {
         FailThenSuccessBackoffTransport fakeTransport =
                 new FailThenSuccessBackoffTransport(HttpStatusCodes.STATUS_CODE_SERVER_ERROR, 3);
 
-        MockGoogleCredential credential = new MockGoogleCredential.Builder()
+        MockGoogleCredential credential = RetryHttpInitializerWrapper.newMockCredentialBuilder()
                 .build();
         MockSleeper mockSleeper = new MockSleeper();
 
@@ -122,7 +122,7 @@ public class RetryHttpInitializerWrapperTests extends ESTestCase {
         FailThenSuccessBackoffTransport fakeTransport =
                 new FailThenSuccessBackoffTransport(HttpStatusCodes.STATUS_CODE_SERVER_ERROR, maxRetryTimes);
         JsonFactory jsonFactory = new JacksonFactory();
-        MockGoogleCredential credential = new MockGoogleCredential.Builder()
+        MockGoogleCredential credential = RetryHttpInitializerWrapper.newMockCredentialBuilder()
                 .build();
 
         MockSleeper oneTimeSleeper = new MockSleeper() {
@@ -155,7 +155,7 @@ public class RetryHttpInitializerWrapperTests extends ESTestCase {
         FailThenSuccessBackoffTransport fakeTransport =
                 new FailThenSuccessBackoffTransport(HttpStatusCodes.STATUS_CODE_SERVER_ERROR, 1, true);
 
-        MockGoogleCredential credential = new MockGoogleCredential.Builder()
+        MockGoogleCredential credential = RetryHttpInitializerWrapper.newMockCredentialBuilder()
                 .build();
         MockSleeper mockSleeper = new MockSleeper();
         RetryHttpInitializerWrapper retryHttpInitializerWrapper = new RetryHttpInitializerWrapper(credential, mockSleeper, 500);
diff --git a/plugins/lang-plan-a/ant.xml b/plugins/lang-plan-a/ant.xml
new file mode 100644
index 0000000..bf1c9b9
--- /dev/null
+++ b/plugins/lang-plan-a/ant.xml
@@ -0,0 +1,145 @@
+<?xml version="1.0"?>
+<project name="ant-stuff">
+
+<!-- 
+ grammar regeneration logic
+ we do this with ant for several reasons:
+ * remove generated tabs for forbidden-apis
+ * remove generated timestamps/filenames for reproducible build
+ * fix CRLF line endings for windows consistency
+ * ability to make classes package-private
+ * keeping in source code control is easier on IDEs
+ * regeneration should be rare, no reason to be religious about generated files 
+ * all logic already written and battle tested in lucene build
+-->
+  <target name="regenerate" description="Regenerate antlr lexer and parser" depends="run-antlr"/>
+
+  <target name="run-antlr">
+    <regen-delete grammar="PlanA"/>
+    <regen-lexer grammar="PlanA"/>
+    <regen-parser grammar="PlanA"/>
+    <regen-fix grammar="PlanA"/>
+  </target>
+
+  <macrodef name="replace-value">
+    <attribute name="value" />
+    <attribute name="property" />
+    <attribute name="from" />
+    <attribute name="to" />
+    <sequential>
+      <loadresource property="@{property}">
+        <string value="@{value}"/>
+        <filterchain>
+          <tokenfilter>
+            <filetokenizer/>
+            <replacestring from="@{from}" to="@{to}"/>
+          </tokenfilter>
+        </filterchain>
+      </loadresource>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="regen-delete">
+    <attribute name="grammar" />
+    <sequential>
+      <local name="output.path"/>
+      <patternset id="grammar.@{grammar}.patternset">
+        <include name="@{grammar}Lexer.java" />
+        <include name="@{grammar}Parser.java" />
+        <include name="@{grammar}ParserVisitor.java" />
+        <include name="@{grammar}ParserBaseVisitor.java" />
+      </patternset>
+      <property name="output.path" location="src/main/java/org/elasticsearch/plan/a"/>
+      <!-- delete parser and lexer so files will be generated -->
+      <delete dir="${output.path}">
+        <patternset refid="grammar.@{grammar}.patternset"/>
+      </delete>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="regen-lexer">
+    <attribute name="grammar" />
+    <sequential>
+      <local name="grammar.path"/>
+      <local name="output.path"/>
+      <property name="grammar.path" location="src/main/antlr"/>
+      <property name="output.path" location="src/main/java/org/elasticsearch/plan/a"/>
+      <!-- invoke ANTLR4 -->
+      <java classname="org.antlr.v4.Tool" fork="true" failonerror="true" classpathref="regenerate.classpath" taskname="antlr">
+        <sysproperty key="file.encoding" value="UTF-8"/>
+        <sysproperty key="user.language" value="en"/>
+        <sysproperty key="user.country" value="US"/>
+        <sysproperty key="user.variant" value=""/>
+        <arg value="-package"/>
+        <arg value="org.elasticsearch.plan.a"/>
+        <arg value="-o"/>
+        <arg path="${output.path}"/>
+        <arg path="${grammar.path}/@{grammar}Lexer.g4"/>
+      </java>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="regen-parser">
+    <attribute name="grammar" />
+    <sequential>
+      <local name="grammar.path"/>
+      <local name="output.path"/>
+      <property name="grammar.path" location="src/main/antlr"/>
+      <property name="output.path" location="src/main/java/org/elasticsearch/plan/a"/>
+      <!-- invoke ANTLR4 -->
+      <java classname="org.antlr.v4.Tool" fork="true" failonerror="true" classpathref="regenerate.classpath" taskname="antlr">
+        <sysproperty key="file.encoding" value="UTF-8"/>
+        <sysproperty key="user.language" value="en"/>
+        <sysproperty key="user.country" value="US"/>
+        <sysproperty key="user.variant" value=""/>
+        <arg value="-package"/>
+        <arg value="org.elasticsearch.plan.a"/>
+        <arg value="-no-listener"/>
+        <arg value="-visitor"/>
+        <!-- <arg value="-Xlog"/> -->
+        <arg value="-o"/>
+        <arg path="${output.path}"/>
+        <arg path="${grammar.path}/@{grammar}Parser.g4"/>
+      </java>
+    </sequential>
+  </macrodef>
+
+  <macrodef name="regen-fix">
+    <attribute name="grammar" />
+    <sequential>
+      <local name="grammar.path"/>
+      <local name="output.path"/>
+      <property name="grammar.path" location="src/main/antlr"/>
+      <property name="output.path" location="src/main/java/org/elasticsearch/plan/a"/>
+      <patternset id="grammar.@{grammar}.patternset">
+        <include name="@{grammar}Lexer.java" />
+        <include name="@{grammar}Parser.java" />
+        <include name="@{grammar}ParserVisitor.java" />
+        <include name="@{grammar}ParserBaseVisitor.java" />
+      </patternset>
+      <!-- fileset with files to edit -->
+      <fileset id="grammar.fileset" dir="${output.path}">
+        <patternset refid="grammar.@{grammar}.patternset"/>
+      </fileset>
+      <!-- remove files that are not needed to compile or at runtime -->
+      <delete dir="${grammar.path}" includes="@{grammar}*.tokens"/>
+      <delete dir="${output.path}" includes="@{grammar}*.tokens"/>
+      <!-- make the generated classes package private -->
+      <replaceregexp match="public ((interface|class) \Q@{grammar}\E\w+)" replace="\1" encoding="UTF-8">
+        <fileset refid="grammar.fileset"/>
+      </replaceregexp>
+      <!-- nuke timestamps/filenames in generated files -->
+      <replaceregexp match="\Q// Generated from \E.*" replace="\/\/ ANTLR GENERATED CODE: DO NOT EDIT" encoding="UTF-8">
+        <fileset refid="grammar.fileset"/>
+      </replaceregexp>
+      <!-- remove tabs in antlr generated files -->
+      <replaceregexp match="\t" flags="g" replace="  " encoding="UTF-8">
+        <fileset refid="grammar.fileset"/>
+      </replaceregexp>
+      <!-- fix line endings -->
+      <fixcrlf srcdir="${output.path}">
+        <patternset refid="grammar.@{grammar}.patternset"/>
+      </fixcrlf>
+    </sequential>
+  </macrodef>
+</project>
diff --git a/plugins/lang-plan-a/build.gradle b/plugins/lang-plan-a/build.gradle
new file mode 100644
index 0000000..618c094
--- /dev/null
+++ b/plugins/lang-plan-a/build.gradle
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import org.apache.tools.ant.types.Path
+
+esplugin {
+  description 'An easy, safe and fast scripting language for Elasticsearch'
+  classname 'org.elasticsearch.plan.a.PlanAPlugin'
+}
+
+dependencies {
+  compile 'org.antlr:antlr4-runtime:4.5.1-1'
+  compile 'org.ow2.asm:asm:5.0.4'
+  compile 'org.ow2.asm:asm-commons:5.0.4'
+}
+
+compileJava.options.compilerArgs << '-Xlint:-cast,-fallthrough,-rawtypes'
+compileTestJava.options.compilerArgs << '-Xlint:-unchecked'
+
+// regeneration logic, comes in via ant right now
+// don't port it to gradle, it works fine.
+
+configurations {
+  regenerate
+}
+
+dependencies {
+  regenerate 'org.antlr:antlr4:4.5.1-1'
+}
+
+ant.references['regenerate.classpath'] = new Path(ant.project, configurations.regenerate.asPath)
+ant.importBuild 'ant.xml'
diff --git a/plugins/lang-plan-a/licenses/antlr4-runtime-4.5.1-1.jar.sha1 b/plugins/lang-plan-a/licenses/antlr4-runtime-4.5.1-1.jar.sha1
new file mode 100644
index 0000000..37f80b9
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/antlr4-runtime-4.5.1-1.jar.sha1
@@ -0,0 +1 @@
+66144204f9d6d7d3f3f775622c2dd7e9bd511d97
\ No newline at end of file
diff --git a/plugins/lang-plan-a/licenses/antlr4-runtime-LICENSE.txt b/plugins/lang-plan-a/licenses/antlr4-runtime-LICENSE.txt
new file mode 100644
index 0000000..95d0a25
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/antlr4-runtime-LICENSE.txt
@@ -0,0 +1,26 @@
+[The "BSD license"]
+Copyright (c) 2015 Terence Parr, Sam Harwell
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+
+ 1. Redistributions of source code must retain the above copyright
+    notice, this list of conditions and the following disclaimer.
+ 2. Redistributions in binary form must reproduce the above copyright
+    notice, this list of conditions and the following disclaimer in the
+    documentation and/or other materials provided with the distribution.
+ 3. The name of the author may not be used to endorse or promote products
+    derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-plan-a/licenses/antlr4-runtime-NOTICE.txt b/plugins/lang-plan-a/licenses/antlr4-runtime-NOTICE.txt
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/lang-plan-a/licenses/asm-5.0.4.jar.sha1 b/plugins/lang-plan-a/licenses/asm-5.0.4.jar.sha1
new file mode 100644
index 0000000..9223dba
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-5.0.4.jar.sha1
@@ -0,0 +1 @@
+0da08b8cce7bbf903602a25a3a163ae252435795
diff --git a/plugins/lang-plan-a/licenses/asm-LICENSE.txt b/plugins/lang-plan-a/licenses/asm-LICENSE.txt
new file mode 100644
index 0000000..afb064f
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-LICENSE.txt
@@ -0,0 +1,26 @@
+Copyright (c) 2012 France Télécom
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+3. Neither the name of the copyright holders nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-plan-a/licenses/asm-NOTICE.txt b/plugins/lang-plan-a/licenses/asm-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/lang-plan-a/licenses/asm-commons-5.0.4.jar.sha1 b/plugins/lang-plan-a/licenses/asm-commons-5.0.4.jar.sha1
new file mode 100644
index 0000000..94fe0cd
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-commons-5.0.4.jar.sha1
@@ -0,0 +1 @@
+5a556786086c23cd689a0328f8519db93821c04c
diff --git a/plugins/lang-plan-a/licenses/asm-commons-LICENSE.txt b/plugins/lang-plan-a/licenses/asm-commons-LICENSE.txt
new file mode 100644
index 0000000..afb064f
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-commons-LICENSE.txt
@@ -0,0 +1,26 @@
+Copyright (c) 2012 France Télécom
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+3. Neither the name of the copyright holders nor the names of its
+   contributors may be used to endorse or promote products derived from
+   this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/plugins/lang-plan-a/licenses/asm-commons-NOTICE.txt b/plugins/lang-plan-a/licenses/asm-commons-NOTICE.txt
new file mode 100644
index 0000000..8d1c8b6
--- /dev/null
+++ b/plugins/lang-plan-a/licenses/asm-commons-NOTICE.txt
@@ -0,0 +1 @@
+ 
diff --git a/plugins/lang-plan-a/src/main/antlr/PlanALexer.g4 b/plugins/lang-plan-a/src/main/antlr/PlanALexer.g4
new file mode 100644
index 0000000..5110a73
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/antlr/PlanALexer.g4
@@ -0,0 +1,120 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+lexer grammar PlanALexer;
+
+@header {
+    import java.util.Set;
+}
+
+@members {
+    private Set<String> types = null;
+
+    void setTypes(Set<String> types) {
+        this.types = types;
+    }
+}
+
+WS: [ \t\n\r]+ -> skip;
+COMMENT: ( '//' .*? [\n\r] | '/*' .*? '*/' ) -> skip;
+
+LBRACK:    '{';
+RBRACK:    '}';
+LBRACE:    '[';
+RBRACE:    ']';
+LP:        '(';
+RP:        ')';
+DOT:       '.' -> mode(EXT);
+COMMA:     ',';
+SEMICOLON: ';';
+IF:        'if';
+ELSE:      'else';
+WHILE:     'while';
+DO:        'do';
+FOR:       'for';
+CONTINUE:  'continue';
+BREAK:     'break';
+RETURN:    'return';
+NEW:       'new';
+TRY:       'try';
+CATCH:     'catch';
+THROW:     'throw';
+
+BOOLNOT: '!';
+BWNOT:   '~';
+MUL:     '*';
+DIV:     '/';
+REM:     '%';
+ADD:     '+';
+SUB:     '-';
+LSH:     '<<';
+RSH:     '>>';
+USH:     '>>>';
+LT:      '<';
+LTE:     '<=';
+GT:      '>';
+GTE:     '>=';
+EQ:      '==';
+EQR:     '===';
+NE:      '!=';
+NER:     '!==';
+BWAND:   '&';
+BWXOR:   '^';
+BWOR:    '|';
+BOOLAND: '&&';
+BOOLOR:  '||';
+COND:    '?';
+COLON:   ':';
+INCR:    '++';
+DECR:    '--';
+
+ASSIGN: '=';
+AADD:   '+=';
+ASUB:   '-=';
+AMUL:   '*=';
+ADIV:   '/=';
+AREM:   '%=';
+AAND:   '&=';
+AXOR:   '^=';
+AOR:    '|=';
+ALSH:   '<<=';
+ARSH:   '>>=';
+AUSH:   '>>>=';
+ACAT:   '..=';
+
+OCTAL: '0' [0-7]+ [lL]?;
+HEX: '0' [xX] [0-9a-fA-F]+ [lL]?;
+INTEGER: ( '0' | [1-9] [0-9]* ) [lLfFdD]?;
+DECIMAL: ( '0' | [1-9] [0-9]* ) DOT [0-9]* ( [eE] [+\-]? [0-9]+ )? [fF]?;
+
+STRING: '"' ( '\\"' | '\\\\' | ~[\\"] )*? '"' {setText(getText().substring(1, getText().length() - 1));};
+CHAR: '\'' . '\''                             {setText(getText().substring(1, getText().length() - 1));};
+
+TRUE:  'true';
+FALSE: 'false';
+
+NULL: 'null';
+
+TYPE: ID GENERIC? {types.contains(getText().replace(" ", ""))}? {setText(getText().replace(" ", ""));};
+fragment GENERIC: ' '* '<' ' '* ( ID GENERIC? ) ' '* ( COMMA ' '* ( ID GENERIC? ) ' '* )* '>';
+ID: [_a-zA-Z] [_a-zA-Z0-9]*;
+
+mode EXT;
+EXTINTEGER: ( '0' | [1-9] [0-9]* ) -> mode(DEFAULT_MODE);
+EXTID: [_a-zA-Z] [_a-zA-Z0-9]* -> mode(DEFAULT_MODE);
diff --git a/plugins/lang-plan-a/src/main/antlr/PlanAParser.g4 b/plugins/lang-plan-a/src/main/antlr/PlanAParser.g4
new file mode 100644
index 0000000..1b177a4
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/antlr/PlanAParser.g4
@@ -0,0 +1,127 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+parser grammar PlanAParser;
+
+options { tokenVocab=PlanALexer; }
+
+source
+    : statement+ EOF
+    ;
+
+statement
+    : IF LP expression RP block ( ELSE block )?                                              # if
+    | WHILE LP expression RP ( block | empty )                                               # while
+    | DO block WHILE LP expression RP SEMICOLON?                                             # do
+    | FOR LP initializer? SEMICOLON expression? SEMICOLON afterthought? RP ( block | empty ) # for
+    | declaration SEMICOLON?                                                                 # decl
+    | CONTINUE SEMICOLON?                                                                    # continue
+    | BREAK SEMICOLON?                                                                       # break
+    | RETURN expression SEMICOLON?                                                           # return
+    | TRY block ( CATCH LP ( TYPE ID ) RP block )+                                           # try
+    | THROW expression SEMICOLON?                                                            # throw
+    | expression SEMICOLON?                                                                  # expr
+    ;
+
+block
+    : LBRACK statement* RBRACK                 # multiple
+    | statement                                # single
+    ;
+
+empty
+    : SEMICOLON
+    ;
+
+initializer
+    : declaration
+    | expression
+    ;
+
+afterthought
+    : expression
+    ;
+
+declaration
+    : decltype declvar ( COMMA declvar )*
+    ;
+
+decltype
+    : TYPE (LBRACE RBRACE)*
+    ;
+
+declvar
+    : ID ( ASSIGN expression )?
+    ;
+
+expression
+    :               LP expression RP                                    # precedence
+    |               ( OCTAL | HEX | INTEGER | DECIMAL )                 # numeric
+    |               CHAR                                                # char
+    |               TRUE                                                # true
+    |               FALSE                                               # false
+    |               NULL                                                # null
+    | <assoc=right> extstart increment                                  # postinc
+    | <assoc=right> increment extstart                                  # preinc
+    |               extstart                                            # external
+    | <assoc=right> ( BOOLNOT | BWNOT | ADD | SUB ) expression          # unary
+    | <assoc=right> LP decltype RP expression                           # cast
+    |               expression ( MUL | DIV | REM ) expression           # binary
+    |               expression ( ADD | SUB ) expression                 # binary
+    |               expression ( LSH | RSH | USH ) expression           # binary
+    |               expression ( LT | LTE | GT | GTE ) expression       # comp
+    |               expression ( EQ | EQR | NE | NER ) expression       # comp
+    |               expression BWAND expression                         # binary
+    |               expression BWXOR expression                         # binary
+    |               expression BWOR expression                          # binary
+    |               expression BOOLAND expression                       # bool
+    |               expression BOOLOR expression                        # bool
+    | <assoc=right> expression COND expression COLON expression         # conditional
+    | <assoc=right> extstart ( ASSIGN | AADD | ASUB | AMUL | ADIV
+                                      | AREM | AAND | AXOR | AOR
+                                      | ALSH | ARSH | AUSH ) expression # assignment
+    ;
+
+extstart
+    : extprec
+    | extcast
+    | exttype
+    | extvar
+    | extnew
+    | extstring
+    ;
+
+extprec:   LP ( extprec | extcast | exttype | extvar | extnew | extstring ) RP ( extdot | extbrace )?;
+extcast:   LP decltype RP ( extprec | extcast | exttype | extvar | extnew | extstring );
+extbrace:  LBRACE expression RBRACE ( extdot | extbrace )?;
+extdot:    DOT ( extcall | extfield );
+exttype:   TYPE extdot;
+extcall:   EXTID arguments ( extdot | extbrace )?;
+extvar:    ID ( extdot | extbrace )?;
+extfield:  ( EXTID | EXTINTEGER ) ( extdot | extbrace )?;
+extnew:    NEW TYPE ( ( arguments ( extdot | extbrace)? ) | ( ( LBRACE expression RBRACE )+ extdot? ) );
+extstring: STRING (extdot | extbrace )?;
+
+arguments
+    : ( LP ( expression ( COMMA expression )* )? RP )
+    ;
+
+increment
+    : INCR
+    | DECR
+    ;
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Adapter.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Adapter.java
new file mode 100644
index 0000000..baa06f4
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Adapter.java
@@ -0,0 +1,276 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.util.HashMap;
+import java.util.Map;
+
+import org.antlr.v4.runtime.ParserRuleContext;
+import org.antlr.v4.runtime.tree.ParseTree;
+
+import static org.elasticsearch.plan.a.Definition.*;
+import static org.elasticsearch.plan.a.PlanAParser.*;
+
+class Adapter {
+    static class StatementMetadata {
+        final ParserRuleContext source;
+
+        boolean last;
+
+        boolean allExit;
+        boolean allReturn;
+        boolean anyReturn;
+        boolean allBreak;
+        boolean anyBreak;
+        boolean allContinue;
+        boolean anyContinue;
+
+        private StatementMetadata(final ParserRuleContext source) {
+            this.source = source;
+
+            last = false;
+
+            allExit = false;
+            allReturn = false;
+            anyReturn = false;
+            allBreak = false;
+            anyBreak = false;
+            allContinue = false;
+            anyContinue = false;
+        }
+    }
+
+    static class ExpressionMetadata {
+        final ParserRuleContext source;
+
+        boolean read;
+        boolean statement;
+
+        Object preConst;
+        Object postConst;
+        boolean isNull;
+
+        Type to;
+        Type from;
+        boolean explicit;
+        boolean typesafe;
+
+        Cast cast;
+
+        private ExpressionMetadata(final ParserRuleContext source) {
+            this.source = source;
+
+            read = true;
+            statement = false;
+
+            preConst = null;
+            postConst = null;
+            isNull = false;
+
+            to = null;
+            from = null;
+            explicit = false;
+            typesafe = true;
+
+            cast = null;
+        }
+    }
+
+    static class ExternalMetadata {
+        final ParserRuleContext source;
+
+        boolean read;
+        ParserRuleContext storeExpr;
+        int token;
+        boolean pre;
+        boolean post;
+
+        int scope;
+        Type current;
+        boolean statik;
+        boolean statement;
+        Object constant;
+
+        private ExternalMetadata(final ParserRuleContext source) {
+            this.source = source;
+
+            read = false;
+            storeExpr = null;
+            token = 0;
+            pre = false;
+            post = false;
+
+            scope = 0;
+            current = null;
+            statik = false;
+            statement = false;
+            constant = null;
+        }
+    }
+
+    static class ExtNodeMetadata {
+        final ParserRuleContext parent;
+        final ParserRuleContext source;
+
+        Object target;
+        boolean last;
+
+        Type type;
+        Type promote;
+
+        Cast castFrom;
+        Cast castTo;
+
+        private ExtNodeMetadata(final ParserRuleContext parent, final ParserRuleContext source) {
+            this.parent = parent;
+            this.source = source;
+
+            target = null;
+            last = false;
+
+            type = null;
+            promote = null;
+
+            castFrom = null;
+            castTo = null;
+        }
+    }
+
+    static String error(final ParserRuleContext ctx) {
+        return "Error [" + ctx.getStart().getLine() + ":" + ctx.getStart().getCharPositionInLine() + "]: ";
+    }
+
+    final Definition definition;
+    final String source;
+    final ParserRuleContext root;
+    final CompilerSettings settings;
+
+    private final Map<ParserRuleContext, StatementMetadata> statementMetadata;
+    private final Map<ParserRuleContext, ExpressionMetadata> expressionMetadata;
+    private final Map<ParserRuleContext, ExternalMetadata> externalMetadata;
+    private final Map<ParserRuleContext, ExtNodeMetadata> extNodeMetadata;
+
+    Adapter(final Definition definition, final String source, final ParserRuleContext root, final CompilerSettings settings) {
+        this.definition = definition;
+        this.source = source;
+        this.root = root;
+        this.settings = settings;
+
+        statementMetadata = new HashMap<>();
+        expressionMetadata = new HashMap<>();
+        externalMetadata = new HashMap<>();
+        extNodeMetadata = new HashMap<>();
+    }
+
+    StatementMetadata createStatementMetadata(final ParserRuleContext source) {
+        final StatementMetadata sourcesmd = new StatementMetadata(source);
+        statementMetadata.put(source, sourcesmd);
+
+        return sourcesmd;
+    }
+
+    StatementMetadata getStatementMetadata(final ParserRuleContext source) {
+        final StatementMetadata sourcesmd = statementMetadata.get(source);
+
+        if (sourcesmd == null) {
+            throw new IllegalStateException(error(source) + "Statement metadata does not exist at" +
+                    " the parse node with text [" + source.getText() + "].");
+        }
+
+        return sourcesmd;
+    }
+
+    ExpressionContext updateExpressionTree(ExpressionContext source) {
+        if (source instanceof PrecedenceContext) {
+            final ParserRuleContext parent = source.getParent();
+            int index = 0;
+
+            for (final ParseTree child : parent.children) {
+                if (child == source) {
+                    break;
+                }
+
+                ++index;
+            }
+
+            while (source instanceof PrecedenceContext) {
+                source = ((PrecedenceContext)source).expression();
+            }
+
+            parent.children.set(index, source);
+        }
+
+        return source;
+    }
+
+    ExpressionMetadata createExpressionMetadata(ParserRuleContext source) {
+        final ExpressionMetadata sourceemd = new ExpressionMetadata(source);
+        expressionMetadata.put(source, sourceemd);
+
+        return sourceemd;
+    }
+    
+    ExpressionMetadata getExpressionMetadata(final ParserRuleContext source) {
+        final ExpressionMetadata sourceemd = expressionMetadata.get(source);
+
+        if (sourceemd == null) {
+            throw new IllegalStateException(error(source) + "Expression metadata does not exist at" +
+                    " the parse node with text [" + source.getText() + "].");
+        }
+
+        return sourceemd;
+    }
+
+    ExternalMetadata createExternalMetadata(final ParserRuleContext source) {
+        final ExternalMetadata sourceemd = new ExternalMetadata(source);
+        externalMetadata.put(source, sourceemd);
+
+        return sourceemd;
+    }
+
+    ExternalMetadata getExternalMetadata(final ParserRuleContext source) {
+        final ExternalMetadata sourceemd = externalMetadata.get(source);
+
+        if (sourceemd == null) {
+            throw new IllegalStateException(error(source) + "External metadata does not exist at" +
+                    " the parse node with text [" + source.getText() + "].");
+        }
+
+        return sourceemd;
+    }
+
+    ExtNodeMetadata createExtNodeMetadata(final ParserRuleContext parent, final ParserRuleContext source) {
+        final ExtNodeMetadata sourceemd = new ExtNodeMetadata(parent, source);
+        extNodeMetadata.put(source, sourceemd);
+
+        return sourceemd;
+    }
+
+    ExtNodeMetadata getExtNodeMetadata(final ParserRuleContext source) {
+        final ExtNodeMetadata sourceemd = extNodeMetadata.get(source);
+
+        if (sourceemd == null) {
+            throw new IllegalStateException(error(source) + "External metadata does not exist at" +
+                    " the parse node with text [" + source.getText() + "].");
+        }
+
+        return sourceemd;
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Analyzer.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Analyzer.java
new file mode 100644
index 0000000..a7e2986
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Analyzer.java
@@ -0,0 +1,2983 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.util.ArrayDeque;
+import java.util.Arrays;
+import java.util.Deque;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.antlr.v4.runtime.ParserRuleContext;
+
+import static org.elasticsearch.plan.a.Adapter.*;
+import static org.elasticsearch.plan.a.Definition.*;
+import static org.elasticsearch.plan.a.PlanAParser.*;
+
+class Analyzer extends PlanAParserBaseVisitor<Void> {
+    private static class Variable {
+        final String name;
+        final Type type;
+        final int slot;
+
+        private Variable(final String name, final Type type, final int slot) {
+            this.name = name;
+            this.type = type;
+            this.slot = slot;
+        }
+    }
+
+    static void analyze(final Adapter adapter) {
+        new Analyzer(adapter);
+    }
+
+    private final Adapter adapter;
+    private final Definition definition;
+    private final CompilerSettings settings;
+
+    private final Deque<Integer> scopes;
+    private final Deque<Variable> variables;
+
+    private Analyzer(final Adapter adapter) {
+        this.adapter = adapter;
+        definition = adapter.definition;
+        settings = adapter.settings;
+
+        scopes = new ArrayDeque<>();
+        variables = new ArrayDeque<>();
+
+        incrementScope();
+        addVariable(null, "this", definition.execType);
+        addVariable(null, "input", definition.smapType);
+
+        adapter.createStatementMetadata(adapter.root);
+        visit(adapter.root);
+
+        decrementScope();
+    }
+
+    void incrementScope() {
+        scopes.push(0);
+    }
+
+    void decrementScope() {
+        int remove = scopes.pop();
+
+        while (remove > 0) {
+            variables.pop();
+            --remove;
+        }
+    }
+
+    Variable getVariable(final String name) {
+        final Iterator<Variable> itr = variables.iterator();
+
+        while (itr.hasNext()) {
+            final Variable variable = itr.next();
+
+            if (variable.name.equals(name)) {
+                return variable;
+            }
+        }
+
+        return null;
+    }
+
+    Variable addVariable(final ParserRuleContext source, final String name, final Type type) {
+        if (getVariable(name) != null) {
+            if (source == null) {
+                throw new IllegalArgumentException("Argument name [" + name + "] already defined within the scope.");
+            } else {
+                throw new IllegalArgumentException(
+                        error(source) + "Variable name [" + name + "] already defined within the scope.");
+            }
+        }
+
+        final Variable previous = variables.peekFirst();
+        int slot = 0;
+
+        if (previous != null) {
+            slot += previous.slot + previous.type.type.getSize();
+        }
+
+        final Variable variable = new Variable(name, type, slot);
+        variables.push(variable);
+
+        final int update = scopes.pop() + 1;
+        scopes.push(update);
+
+        return variable;
+    }
+
+    @Override
+    public Void visitSource(final SourceContext ctx) {
+        final StatementMetadata sourcesmd = adapter.getStatementMetadata(ctx);
+        final List<StatementContext> statectxs = ctx.statement();
+        final StatementContext lastctx = statectxs.get(statectxs.size() - 1);
+
+        incrementScope();
+
+        for (final StatementContext statectx : statectxs) {
+            if (sourcesmd.allExit) {
+                throw new IllegalArgumentException(error(statectx) +
+                        "Statement will never be executed because all prior paths exit.");
+            }
+
+            final StatementMetadata statesmd = adapter.createStatementMetadata(statectx);
+            statesmd.last = statectx == lastctx;
+            visit(statectx);
+
+            if (statesmd.anyContinue) {
+                throw new IllegalArgumentException(error(statectx) +
+                        "Cannot have a continue statement outside of a loop.");
+            }
+
+            if (statesmd.anyBreak) {
+                throw new IllegalArgumentException(error(statectx) +
+                        "Cannot have a break statement outside of a loop.");
+            }
+
+            sourcesmd.allExit = statesmd.allExit;
+            sourcesmd.allReturn = statesmd.allReturn;
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitIf(final IfContext ctx) {
+        final StatementMetadata ifsmd = adapter.getStatementMetadata(ctx);
+
+        incrementScope();
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = definition.booleanType;
+        visit(exprctx);
+        markCast(expremd);
+
+        if (expremd.postConst != null) {
+            throw new IllegalArgumentException(error(ctx) + "If statement is not necessary.");
+        }
+
+        final BlockContext blockctx0 = ctx.block(0);
+        final StatementMetadata blocksmd0 = adapter.createStatementMetadata(blockctx0);
+        blocksmd0.last = ifsmd.last;
+        visit(blockctx0);
+
+        ifsmd.anyReturn = blocksmd0.anyReturn;
+        ifsmd.anyBreak = blocksmd0.anyBreak;
+        ifsmd.anyContinue = blocksmd0.anyContinue;
+
+        if (ctx.ELSE() != null) {
+            final BlockContext blockctx1 = ctx.block(1);
+            final StatementMetadata blocksmd1 = adapter.createStatementMetadata(blockctx1);
+            blocksmd1.last = ifsmd.last;
+            visit(blockctx1);
+
+            ifsmd.allExit = blocksmd0.allExit && blocksmd1.allExit;
+            ifsmd.allReturn = blocksmd0.allReturn && blocksmd1.allReturn;
+            ifsmd.anyReturn |= blocksmd1.anyReturn;
+            ifsmd.allBreak = blocksmd0.allBreak && blocksmd1.allBreak;
+            ifsmd.anyBreak |= blocksmd1.anyBreak;
+            ifsmd.allContinue = blocksmd0.allContinue && blocksmd1.allContinue;
+            ifsmd.anyContinue |= blocksmd1.anyContinue;
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitWhile(final WhileContext ctx) {
+        final StatementMetadata whilesmd = adapter.getStatementMetadata(ctx);
+
+        incrementScope();
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = definition.booleanType;
+        visit(exprctx);
+        markCast(expremd);
+
+        boolean exitrequired = false;
+
+        if (expremd.postConst != null) {
+            boolean constant = (boolean)expremd.postConst;
+
+            if (!constant) {
+                throw new IllegalArgumentException(error(ctx) + "The loop will never be executed.");
+            }
+
+            exitrequired = true;
+        }
+
+        final BlockContext blockctx = ctx.block();
+
+        if (blockctx != null) {
+            final StatementMetadata blocksmd = adapter.createStatementMetadata(blockctx);
+            visit(blockctx);
+
+            if (blocksmd.allReturn) {
+                throw new IllegalArgumentException(error(ctx) + "All paths return so the loop is not necessary.");
+            }
+
+            if (blocksmd.allBreak) {
+                throw new IllegalArgumentException(error(ctx) + "All paths break so the loop is not necessary.");
+            }
+
+            if (exitrequired && !blocksmd.anyReturn && !blocksmd.anyBreak) {
+                throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+            }
+
+            if (exitrequired && blocksmd.anyReturn && !blocksmd.anyBreak) {
+                whilesmd.allExit = true;
+                whilesmd.allReturn = true;
+            }
+        } else if (exitrequired) {
+            throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitDo(final DoContext ctx) {
+        final StatementMetadata dosmd = adapter.getStatementMetadata(ctx);
+
+        incrementScope();
+
+        final BlockContext blockctx = ctx.block();
+        final StatementMetadata blocksmd = adapter.createStatementMetadata(blockctx);
+        visit(blockctx);
+
+        if (blocksmd.allReturn) {
+            throw new IllegalArgumentException(error(ctx) + "All paths return so the loop is not necessary.");
+        }
+
+        if (blocksmd.allBreak) {
+            throw new IllegalArgumentException(error(ctx) + "All paths break so the loop is not necessary.");
+        }
+
+        if (blocksmd.allContinue) {
+            throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+        }
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = definition.booleanType;
+        visit(exprctx);
+        markCast(expremd);
+
+        if (expremd.postConst != null) {
+            final boolean exitrequired = (boolean)expremd.postConst;
+
+            if (exitrequired && !blocksmd.anyReturn && !blocksmd.anyBreak) {
+                throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+            }
+
+            if (exitrequired && blocksmd.anyReturn && !blocksmd.anyBreak) {
+                dosmd.allExit = true;
+                dosmd.allReturn = true;
+            }
+
+            if (!exitrequired && !blocksmd.anyContinue) {
+                throw new IllegalArgumentException(error(ctx) + "All paths exit so the loop is not necessary.");
+            }
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitFor(final ForContext ctx) {
+        final StatementMetadata forsmd = adapter.getStatementMetadata(ctx);
+        boolean exitrequired = false;
+
+        incrementScope();
+
+        final InitializerContext initctx = ctx.initializer();
+
+        if (initctx != null) {
+            adapter.createStatementMetadata(initctx);
+            visit(initctx);
+        }
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        if (exprctx != null) {
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.to = definition.booleanType;
+            visit(exprctx);
+            markCast(expremd);
+
+            if (expremd.postConst != null) {
+                boolean constant = (boolean)expremd.postConst;
+
+                if (!constant) {
+                    throw new IllegalArgumentException(error(ctx) + "The loop will never be executed.");
+                }
+
+                exitrequired = true;
+            }
+        } else {
+            exitrequired = true;
+        }
+
+        final AfterthoughtContext atctx = ctx.afterthought();
+
+        if (atctx != null) {
+            adapter.createStatementMetadata(atctx);
+            visit(atctx);
+        }
+
+        final BlockContext blockctx = ctx.block();
+
+        if (blockctx != null) {
+            final StatementMetadata blocksmd = adapter.createStatementMetadata(blockctx);
+            visit(blockctx);
+
+            if (blocksmd.allReturn) {
+                throw new IllegalArgumentException(error(ctx) + "All paths return so the loop is not necessary.");
+            }
+
+            if (blocksmd.allBreak) {
+                throw new IllegalArgumentException(error(ctx) + "All paths break so the loop is not necessary.");
+            }
+
+            if (exitrequired && !blocksmd.anyReturn && !blocksmd.anyBreak) {
+                throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+            }
+
+            if (exitrequired && blocksmd.anyReturn && !blocksmd.anyBreak) {
+                forsmd.allExit = true;
+                forsmd.allReturn = true;
+            }
+        } else if (exitrequired) {
+            throw new IllegalArgumentException(error(ctx) + "The loop will never exit.");
+        }
+
+        decrementScope();
+
+        return null;
+    }
+
+    @Override
+    public Void visitDecl(final DeclContext ctx) {
+        final DeclarationContext declctx = ctx.declaration();
+        adapter.createStatementMetadata(declctx);
+        visit(declctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitContinue(final ContinueContext ctx) {
+        final StatementMetadata continuesmd = adapter.getStatementMetadata(ctx);
+
+        continuesmd.allExit = true;
+        continuesmd.allContinue = true;
+        continuesmd.anyContinue = true;
+
+        return null;
+    }
+
+    @Override
+    public Void visitBreak(final BreakContext ctx) {
+        final StatementMetadata breaksmd = adapter.getStatementMetadata(ctx);
+
+        breaksmd.allExit = true;
+        breaksmd.allBreak = true;
+        breaksmd.anyBreak = true;
+
+        return null;
+    }
+
+    @Override
+    public Void visitReturn(final ReturnContext ctx) {
+        final StatementMetadata returnsmd = adapter.getStatementMetadata(ctx);
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = definition.objectType;
+        visit(exprctx);
+        markCast(expremd);
+
+        returnsmd.allExit = true;
+        returnsmd.allReturn = true;
+        returnsmd.anyReturn = true;
+
+        return null;
+    }
+
+    @Override
+    public Void visitExpr(final ExprContext ctx) {
+        final StatementMetadata exprsmd = adapter.getStatementMetadata(ctx);
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.read = exprsmd.last;
+        visit(exprctx);
+
+        if (!expremd.statement && !exprsmd.last) {
+            throw new IllegalArgumentException(error(ctx) + "Not a statement.");
+        }
+
+        final boolean rtn = exprsmd.last && expremd.from.sort != Sort.VOID;
+        exprsmd.allExit = rtn;
+        exprsmd.allReturn = rtn;
+        exprsmd.anyReturn = rtn;
+        expremd.to = rtn ? definition.objectType : expremd.from;
+        markCast(expremd);
+
+        return null;
+    }
+
+    @Override
+    public Void visitMultiple(final MultipleContext ctx) {
+        final StatementMetadata multiplesmd = adapter.getStatementMetadata(ctx);
+        final List<StatementContext> statectxs = ctx.statement();
+        final StatementContext lastctx = statectxs.get(statectxs.size() - 1);
+
+        for (StatementContext statectx : statectxs) {
+            if (multiplesmd.allExit) {
+                throw new IllegalArgumentException(error(statectx) +
+                        "Statement will never be executed because all prior paths exit.");
+            }
+
+            final StatementMetadata statesmd = adapter.createStatementMetadata(statectx);
+            statesmd.last = multiplesmd.last && statectx == lastctx;
+            visit(statectx);
+
+            multiplesmd.allExit = statesmd.allExit;
+            multiplesmd.allReturn = statesmd.allReturn && !statesmd.anyBreak && !statesmd.anyContinue;
+            multiplesmd.anyReturn |= statesmd.anyReturn;
+            multiplesmd.allBreak = !statesmd.anyReturn && statesmd.allBreak && !statesmd.anyContinue;
+            multiplesmd.anyBreak |= statesmd.anyBreak;
+            multiplesmd.allContinue = !statesmd.anyReturn && !statesmd.anyBreak && statesmd.allContinue;
+            multiplesmd.anyContinue |= statesmd.anyContinue;
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitSingle(final SingleContext ctx) {
+        final StatementMetadata singlesmd = adapter.getStatementMetadata(ctx);
+
+        final StatementContext statectx = ctx.statement();
+        final StatementMetadata statesmd = adapter.createStatementMetadata(statectx);
+        statesmd.last = singlesmd.last;
+        visit(statectx);
+
+        singlesmd.allExit = statesmd.allExit;
+        singlesmd.allReturn = statesmd.allReturn;
+        singlesmd.anyReturn = statesmd.anyReturn;
+        singlesmd.allBreak = statesmd.allBreak;
+        singlesmd.anyBreak = statesmd.anyBreak;
+        singlesmd.allContinue = statesmd.allContinue;
+        singlesmd.anyContinue = statesmd.anyContinue;
+
+        return null;
+    }
+
+    @Override
+    public Void visitEmpty(final EmptyContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected parser state.");
+    }
+
+    @Override
+    public Void visitInitializer(InitializerContext ctx) {
+        final DeclarationContext declctx = ctx.declaration();
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        if (declctx != null) {
+            adapter.createStatementMetadata(declctx);
+            visit(declctx);
+        } else if (exprctx != null) {
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.read = false;
+            visit(exprctx);
+
+            expremd.to = expremd.from;
+            markCast(expremd);
+
+            if (!expremd.statement) {
+                throw new IllegalArgumentException(error(exprctx) +
+                        "The intializer of a for loop must be a statement.");
+            }
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitAfterthought(AfterthoughtContext ctx) {
+        ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        if (exprctx != null) {
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.read = false;
+            visit(exprctx);
+
+            expremd.to = expremd.from;
+            markCast(expremd);
+
+            if (!expremd.statement) {
+                throw new IllegalArgumentException(error(exprctx) +
+                        "The afterthought of a for loop must be a statement.");
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitDeclaration(final DeclarationContext ctx) {
+        final DecltypeContext decltypectx = ctx.decltype();
+        final ExpressionMetadata decltypeemd = adapter.createExpressionMetadata(decltypectx);
+        visit(decltypectx);
+
+        for (final DeclvarContext declvarctx : ctx.declvar()) {
+            final ExpressionMetadata declvaremd = adapter.createExpressionMetadata(declvarctx);
+            declvaremd.to = decltypeemd.from;
+            visit(declvarctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitDecltype(final DecltypeContext ctx) {
+        final ExpressionMetadata decltypeemd = adapter.getExpressionMetadata(ctx);
+
+        final String name = ctx.getText();
+        decltypeemd.from = definition.getType(name);
+
+        return null;
+    }
+
+    @Override
+    public Void visitDeclvar(final DeclvarContext ctx) {
+        final ExpressionMetadata declvaremd = adapter.getExpressionMetadata(ctx);
+
+        final String name = ctx.ID().getText();
+        declvaremd.postConst = addVariable(ctx, name, declvaremd.to).slot;
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        if (exprctx != null) {
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.to = declvaremd.to;
+            visit(exprctx);
+            markCast(expremd);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitPrecedence(final PrecedenceContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected parser state.");
+    }
+
+    @Override
+    public Void visitNumeric(final NumericContext ctx) {
+        final ExpressionMetadata numericemd = adapter.getExpressionMetadata(ctx);
+        final boolean negate = ctx.parent instanceof UnaryContext && ((UnaryContext)ctx.parent).SUB() != null;
+
+        if (ctx.DECIMAL() != null) {
+            final String svalue = (negate ? "-" : "") + ctx.DECIMAL().getText();
+
+            if (svalue.endsWith("f") || svalue.endsWith("F")) {
+                try {
+                    numericemd.from = definition.floatType;
+                    numericemd.preConst = Float.parseFloat(svalue.substring(0, svalue.length() - 1));
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid float constant [" + svalue + "].");
+                }
+            } else {
+                try {
+                    numericemd.from = definition.doubleType;
+                    numericemd.preConst = Double.parseDouble(svalue);
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid double constant [" + svalue + "].");
+                }
+            }
+        } else {
+            String svalue = negate ? "-" : "";
+            int radix;
+
+            if (ctx.OCTAL() != null) {
+                svalue += ctx.OCTAL().getText();
+                radix = 8;
+            } else if (ctx.INTEGER() != null) {
+                svalue += ctx.INTEGER().getText();
+                radix = 10;
+            } else if (ctx.HEX() != null) {
+                svalue += ctx.HEX().getText();
+                radix = 16;
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+
+            if (svalue.endsWith("d") || svalue.endsWith("D")) {
+                try {
+                    numericemd.from = definition.doubleType;
+                    numericemd.preConst = Double.parseDouble(svalue.substring(0, svalue.length() - 1));
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid float constant [" + svalue + "].");
+                }
+            } else if (svalue.endsWith("f") || svalue.endsWith("F")) {
+                try {
+                    numericemd.from = definition.floatType;
+                    numericemd.preConst = Float.parseFloat(svalue.substring(0, svalue.length() - 1));
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid float constant [" + svalue + "].");
+                }
+            } else if (svalue.endsWith("l") || svalue.endsWith("L")) {
+                try {
+                    numericemd.from = definition.longType;
+                    numericemd.preConst = Long.parseLong(svalue.substring(0, svalue.length() - 1), radix);
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid long constant [" + svalue + "].");
+                }
+            } else {
+                try {
+                    final Type type = numericemd.to;
+                    final Sort sort = type == null ? Sort.INT : type.sort;
+                    final int value = Integer.parseInt(svalue, radix);
+
+                    if (sort == Sort.BYTE && value >= Byte.MIN_VALUE && value <= Byte.MAX_VALUE) {
+                        numericemd.from = definition.byteType;
+                        numericemd.preConst = (byte)value;
+                    } else if (sort == Sort.CHAR && value >= Character.MIN_VALUE && value <= Character.MAX_VALUE) {
+                        numericemd.from = definition.charType;
+                        numericemd.preConst = (char)value;
+                    } else if (sort == Sort.SHORT && value >= Short.MIN_VALUE && value <= Short.MAX_VALUE) {
+                        numericemd.from = definition.shortType;
+                        numericemd.preConst = (short)value;
+                    } else {
+                        numericemd.from = definition.intType;
+                        numericemd.preConst = value;
+                    }
+                } catch (NumberFormatException exception) {
+                    throw new IllegalArgumentException(error(ctx) + "Invalid int constant [" + svalue + "].");
+                }
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitChar(final CharContext ctx) {
+        final ExpressionMetadata charemd = adapter.getExpressionMetadata(ctx);
+
+        if (ctx.CHAR() == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        charemd.preConst = ctx.CHAR().getText().charAt(0);
+        charemd.from = definition.charType;
+
+        return null;
+    }
+
+    @Override
+    public Void visitTrue(final TrueContext ctx) {
+        final ExpressionMetadata trueemd = adapter.getExpressionMetadata(ctx);
+
+        if (ctx.TRUE() == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        trueemd.preConst = true;
+        trueemd.from = definition.booleanType;
+
+        return null;
+    }
+
+    @Override
+    public Void visitFalse(final FalseContext ctx) {
+        final ExpressionMetadata falseemd = adapter.getExpressionMetadata(ctx);
+
+        if (ctx.FALSE() == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        falseemd.preConst = false;
+        falseemd.from = definition.booleanType;
+
+        return null;
+    }
+
+    @Override
+    public Void visitNull(final NullContext ctx) {
+        final ExpressionMetadata nullemd = adapter.getExpressionMetadata(ctx);
+
+        if (ctx.NULL() == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        nullemd.isNull = true;
+
+        if (nullemd.to != null) {
+            if (nullemd.to.sort.primitive) {
+                throw new IllegalArgumentException("Cannot cast null to a primitive type [" + nullemd.to.name + "].");
+            }
+
+            nullemd.from = nullemd.to;
+        } else {
+            nullemd.from = definition.objectType;
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExternal(final ExternalContext ctx) {
+        final ExpressionMetadata extemd = adapter.getExpressionMetadata(ctx);
+
+        final ExtstartContext extstartctx = ctx.extstart();
+        final ExternalMetadata extstartemd = adapter.createExternalMetadata(extstartctx);
+        extstartemd.read = extemd.read;
+        visit(extstartctx);
+
+        extemd.statement = extstartemd.statement;
+        extemd.preConst = extstartemd.constant;
+        extemd.from = extstartemd.current;
+        extemd.typesafe = extstartemd.current.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitPostinc(final PostincContext ctx) {
+        final ExpressionMetadata postincemd = adapter.getExpressionMetadata(ctx);
+
+        final ExtstartContext extstartctx = ctx.extstart();
+        final ExternalMetadata extstartemd = adapter.createExternalMetadata(extstartctx);
+        extstartemd.read = postincemd.read;
+        extstartemd.storeExpr = ctx.increment();
+        extstartemd.token = ADD;
+        extstartemd.post = true;
+        visit(extstartctx);
+
+        postincemd.statement = true;
+        postincemd.from = extstartemd.read ? extstartemd.current : definition.voidType;
+        postincemd.typesafe = extstartemd.current.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitPreinc(final PreincContext ctx) {
+        final ExpressionMetadata preincemd = adapter.getExpressionMetadata(ctx);
+
+        final ExtstartContext extstartctx = ctx.extstart();
+        final ExternalMetadata extstartemd = adapter.createExternalMetadata(extstartctx);
+        extstartemd.read = preincemd.read;
+        extstartemd.storeExpr = ctx.increment();
+        extstartemd.token = ADD;
+        extstartemd.pre = true;
+        visit(extstartctx);
+
+        preincemd.statement = true;
+        preincemd.from = extstartemd.read ? extstartemd.current : definition.voidType;
+        preincemd.typesafe = extstartemd.current.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitUnary(final UnaryContext ctx) {
+        final ExpressionMetadata unaryemd = adapter.getExpressionMetadata(ctx);
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+
+        if (ctx.BOOLNOT() != null) {
+            expremd.to = definition.booleanType;
+            visit(exprctx);
+            markCast(expremd);
+
+            if (expremd.postConst != null) {
+                unaryemd.preConst = !(boolean)expremd.postConst;
+            }
+
+            unaryemd.from = definition.booleanType;
+        } else if (ctx.BWNOT() != null || ctx.ADD() != null || ctx.SUB() != null) {
+            visit(exprctx);
+
+            final Type promote = promoteNumeric(expremd.from, ctx.BWNOT() == null, true);
+
+            if (promote == null) {
+                throw new ClassCastException("Cannot apply [" + ctx.getChild(0).getText() + "] " +
+                        "operation to type [" + expremd.from.name + "].");
+            }
+
+            expremd.to = promote;
+            markCast(expremd);
+
+            if (expremd.postConst != null) {
+                final Sort sort = promote.sort;
+
+                if (ctx.BWNOT() != null) {
+                    if (sort == Sort.INT) {
+                        unaryemd.preConst = ~(int)expremd.postConst;
+                    } else if (sort == Sort.LONG) {
+                        unaryemd.preConst = ~(long)expremd.postConst;
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                    }
+                } else if (ctx.SUB() != null) {
+                    if (exprctx instanceof NumericContext) {
+                        unaryemd.preConst = expremd.postConst;
+                    } else {
+                        if (sort == Sort.INT) {
+                            if (settings.getNumericOverflow()) {
+                                unaryemd.preConst = -(int)expremd.postConst;
+                            } else {
+                                unaryemd.preConst = Math.negateExact((int)expremd.postConst);
+                            }
+                        } else if (sort == Sort.LONG) {
+                            if (settings.getNumericOverflow()) {
+                                unaryemd.preConst = -(long)expremd.postConst;
+                            } else {
+                                unaryemd.preConst = Math.negateExact((long)expremd.postConst);
+                            }
+                        } else if (sort == Sort.FLOAT) {
+                            unaryemd.preConst = -(float)expremd.postConst;
+                        } else if (sort == Sort.DOUBLE) {
+                            unaryemd.preConst = -(double)expremd.postConst;
+                        } else {
+                            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                        }
+                    }
+                } else if (ctx.ADD() != null) {
+                    if (sort == Sort.INT) {
+                        unaryemd.preConst = +(int)expremd.postConst;
+                    } else if (sort == Sort.LONG) {
+                        unaryemd.preConst = +(long)expremd.postConst;
+                    } else if (sort == Sort.FLOAT) {
+                        unaryemd.preConst = +(float)expremd.postConst;
+                    } else if (sort == Sort.DOUBLE) {
+                        unaryemd.preConst = +(double)expremd.postConst;
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            }
+
+            unaryemd.from = promote;
+            unaryemd.typesafe = expremd.typesafe;
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitCast(final CastContext ctx) {
+        final ExpressionMetadata castemd = adapter.getExpressionMetadata(ctx);
+
+        final DecltypeContext decltypectx = ctx.decltype();
+        final ExpressionMetadata decltypemd = adapter.createExpressionMetadata(decltypectx);
+        visit(decltypectx);
+
+        final Type type = decltypemd.from;
+        castemd.from = type;
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+        expremd.to = type;
+        expremd.explicit = true;
+        visit(exprctx);
+        markCast(expremd);
+
+        if (expremd.postConst != null) {
+            castemd.preConst = expremd.postConst;
+        }
+
+        castemd.typesafe = expremd.typesafe && castemd.from.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitBinary(final BinaryContext ctx) {
+        final ExpressionMetadata binaryemd = adapter.getExpressionMetadata(ctx);
+
+        final ExpressionContext exprctx0 = adapter.updateExpressionTree(ctx.expression(0));
+        final ExpressionMetadata expremd0 = adapter.createExpressionMetadata(exprctx0);
+        visit(exprctx0);
+
+        final ExpressionContext exprctx1 = adapter.updateExpressionTree(ctx.expression(1));
+        final ExpressionMetadata expremd1 = adapter.createExpressionMetadata(exprctx1);
+        visit(exprctx1);
+
+        final boolean decimal = ctx.MUL() != null || ctx.DIV() != null || ctx.REM() != null || ctx.SUB() != null;
+        final boolean add = ctx.ADD() != null;
+        final boolean xor = ctx.BWXOR() != null;
+        final Type promote = add ? promoteAdd(expremd0.from, expremd1.from) :
+                             xor ? promoteXor(expremd0.from, expremd1.from) :
+                                   promoteNumeric(expremd0.from, expremd1.from, decimal, true);
+
+        if (promote == null) {
+            throw new ClassCastException("Cannot apply [" + ctx.getChild(1).getText() + "] " +
+                    "operation to types [" + expremd0.from.name + "] and [" + expremd1.from.name + "].");
+        }
+
+        final Sort sort = promote.sort;
+        expremd0.to = add && sort == Sort.STRING ? expremd0.from : promote;
+        expremd1.to = add && sort == Sort.STRING ? expremd1.from : promote;
+        markCast(expremd0);
+        markCast(expremd1);
+
+        if (expremd0.postConst != null && expremd1.postConst != null) {
+            if (ctx.MUL() != null) {
+                if (sort == Sort.INT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (int)expremd0.postConst * (int)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.multiplyExact((int)expremd0.postConst, (int)expremd1.postConst);
+                    }
+                } else if (sort == Sort.LONG) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (long)expremd0.postConst * (long)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.multiplyExact((long)expremd0.postConst, (long)expremd1.postConst);
+                    }
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst * (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.multiplyWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst * (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.multiplyWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.DIV() != null) {
+                if (sort == Sort.INT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (int)expremd0.postConst / (int)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.divideWithoutOverflow((int)expremd0.postConst, (int)expremd1.postConst);
+                    }
+                } else if (sort == Sort.LONG) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (long)expremd0.postConst / (long)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.divideWithoutOverflow((long)expremd0.postConst, (long)expremd1.postConst);
+                    }
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst / (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.divideWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst / (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.divideWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.REM() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst % (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst % (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst % (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.remainderWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst % (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.remainderWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.ADD() != null) {
+                if (sort == Sort.INT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (int)expremd0.postConst + (int)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.addExact((int)expremd0.postConst, (int)expremd1.postConst);
+                    }
+                } else if (sort == Sort.LONG) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (long)expremd0.postConst + (long)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.addExact((long)expremd0.postConst, (long)expremd1.postConst);
+                    }
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst + (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.addWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst + (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.addWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else if (sort == Sort.STRING) {
+                    binaryemd.preConst = "" + expremd0.postConst + expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.SUB() != null) {
+                if (sort == Sort.INT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (int)expremd0.postConst - (int)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.subtractExact((int)expremd0.postConst, (int)expremd1.postConst);
+                    }
+                } else if (sort == Sort.LONG) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (long)expremd0.postConst - (long)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Math.subtractExact((long)expremd0.postConst, (long)expremd1.postConst);
+                    }
+                } else if (sort == Sort.FLOAT) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (float)expremd0.postConst - (float)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.subtractWithoutOverflow((float)expremd0.postConst, (float)expremd1.postConst);
+                    }
+                } else if (sort == Sort.DOUBLE) {
+                    if (settings.getNumericOverflow()) {
+                        binaryemd.preConst = (double)expremd0.postConst - (double)expremd1.postConst;
+                    } else {
+                        binaryemd.preConst = Utility.subtractWithoutOverflow((double)expremd0.postConst, (double)expremd1.postConst);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.LSH() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst << (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst << (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.RSH() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst >> (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst >> (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.USH() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst >>> (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst >>> (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.BWAND() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst & (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst & (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.BWXOR() != null) {
+                if (sort == Sort.BOOL) {
+                    binaryemd.preConst = (boolean)expremd0.postConst ^ (boolean)expremd1.postConst;
+                } else if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst ^ (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst ^ (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else if (ctx.BWOR() != null) {
+                if (sort == Sort.INT) {
+                    binaryemd.preConst = (int)expremd0.postConst | (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    binaryemd.preConst = (long)expremd0.postConst | (long)expremd1.postConst;
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                }
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+        }
+
+        binaryemd.from = promote;
+        binaryemd.typesafe = expremd0.typesafe && expremd1.typesafe;
+
+        return null;
+    }
+
+    @Override
+    public Void visitComp(final CompContext ctx) {
+        final ExpressionMetadata compemd = adapter.getExpressionMetadata(ctx);
+        final boolean equality = ctx.EQ() != null || ctx.NE() != null;
+        final boolean reference = ctx.EQR() != null || ctx.NER() != null;
+
+        final ExpressionContext exprctx0 = adapter.updateExpressionTree(ctx.expression(0));
+        final ExpressionMetadata expremd0 = adapter.createExpressionMetadata(exprctx0);
+        visit(exprctx0);
+
+        final ExpressionContext exprctx1 = adapter.updateExpressionTree(ctx.expression(1));
+        final ExpressionMetadata expremd1 = adapter.createExpressionMetadata(exprctx1);
+        visit(exprctx1);
+
+        if (expremd0.isNull && expremd1.isNull) {
+            throw new IllegalArgumentException(error(ctx) + "Unnecessary comparison of null constants.");
+        }
+
+        final Type promote = equality ? promoteEquality(expremd0.from, expremd1.from) :
+                reference ? promoteReference(expremd0.from, expremd1.from) :
+                            promoteNumeric(expremd0.from, expremd1.from, true, true);
+
+        if (promote == null) {
+            throw new ClassCastException("Cannot apply [" + ctx.getChild(1).getText() + "] " +
+                    "operation to types [" + expremd0.from.name + "] and [" + expremd1.from.name + "].");
+        }
+
+        expremd0.to = promote;
+        expremd1.to = promote;
+        markCast(expremd0);
+        markCast(expremd1);
+
+        if (expremd0.postConst != null && expremd1.postConst != null) {
+            final Sort sort = promote.sort;
+
+            if (ctx.EQ() != null || ctx.EQR() != null) {
+                if (sort == Sort.BOOL) {
+                    compemd.preConst = (boolean)expremd0.postConst == (boolean)expremd1.postConst;
+                } else if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst == (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst == (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst == (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst == (double)expremd1.postConst;
+                } else {
+                    if (ctx.EQ() != null && !expremd0.isNull && !expremd1.isNull) {
+                        compemd.preConst = expremd0.postConst.equals(expremd1.postConst);
+                    } else if (ctx.EQR() != null) {
+                        compemd.preConst = expremd0.postConst == expremd1.postConst;
+                    }
+                }
+            } else if (ctx.NE() != null || ctx.NER() != null) {
+                if (sort == Sort.BOOL) {
+                    compemd.preConst = (boolean)expremd0.postConst != (boolean)expremd1.postConst;
+                } else if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst != (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst != (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst != (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst != (double)expremd1.postConst;
+                } else {
+                    if (ctx.NE() != null && !expremd0.isNull && !expremd1.isNull) {
+                        compemd.preConst = expremd0.postConst.equals(expremd1.postConst);
+                    } else if (ctx.NER() != null) {
+                        compemd.preConst = expremd0.postConst == expremd1.postConst;
+                    }
+                }
+            } else if (ctx.GTE() != null) {
+                if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst >= (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst >= (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst >= (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst >= (double)expremd1.postConst;
+                }
+            } else if (ctx.GT() != null) {
+                if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst > (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst > (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst > (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst > (double)expremd1.postConst;
+                }
+            } else if (ctx.LTE() != null) {
+                if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst <= (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst <= (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst <= (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst <= (double)expremd1.postConst;
+                }
+            } else if (ctx.LT() != null) {
+                if (sort == Sort.INT) {
+                    compemd.preConst = (int)expremd0.postConst < (int)expremd1.postConst;
+                } else if (sort == Sort.LONG) {
+                    compemd.preConst = (long)expremd0.postConst < (long)expremd1.postConst;
+                } else if (sort == Sort.FLOAT) {
+                    compemd.preConst = (float)expremd0.postConst < (float)expremd1.postConst;
+                } else if (sort == Sort.DOUBLE) {
+                    compemd.preConst = (double)expremd0.postConst < (double)expremd1.postConst;
+                }
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+        }
+
+        compemd.from = definition.booleanType;
+        compemd.typesafe = expremd0.typesafe && expremd1.typesafe;
+
+        return null;
+    }
+
+    @Override
+    public Void visitBool(final BoolContext ctx) {
+        final ExpressionMetadata boolemd = adapter.getExpressionMetadata(ctx);
+
+        final ExpressionContext exprctx0 = adapter.updateExpressionTree(ctx.expression(0));
+        final ExpressionMetadata expremd0 = adapter.createExpressionMetadata(exprctx0);
+        expremd0.to = definition.booleanType;
+        visit(exprctx0);
+        markCast(expremd0);
+
+        final ExpressionContext exprctx1 = adapter.updateExpressionTree(ctx.expression(1));
+        final ExpressionMetadata expremd1 = adapter.createExpressionMetadata(exprctx1);
+        expremd1.to = definition.booleanType;
+        visit(exprctx1);
+        markCast(expremd1);
+
+        if (expremd0.postConst != null && expremd1.postConst != null) {
+            if (ctx.BOOLAND() != null) {
+                boolemd.preConst = (boolean)expremd0.postConst && (boolean)expremd1.postConst;
+            } else if (ctx.BOOLOR() != null) {
+                boolemd.preConst = (boolean)expremd0.postConst || (boolean)expremd1.postConst;
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+        }
+
+        boolemd.from = definition.booleanType;
+        boolemd.typesafe = expremd0.typesafe && expremd1.typesafe;
+
+        return null;
+    }
+
+    @Override
+    public Void visitConditional(final ConditionalContext ctx) {
+        final ExpressionMetadata condemd = adapter.getExpressionMetadata(ctx);
+
+        final ExpressionContext exprctx0 = adapter.updateExpressionTree(ctx.expression(0));
+        final ExpressionMetadata expremd0 = adapter.createExpressionMetadata(exprctx0);
+        expremd0.to = definition.booleanType;
+        visit(exprctx0);
+        markCast(expremd0);
+
+        if (expremd0.postConst != null) {
+            throw new IllegalArgumentException(error(ctx) + "Unnecessary conditional statement.");
+        }
+
+        final ExpressionContext exprctx1 = adapter.updateExpressionTree(ctx.expression(1));
+        final ExpressionMetadata expremd1 = adapter.createExpressionMetadata(exprctx1);
+        expremd1.to = condemd.to;
+        expremd1.explicit = condemd.explicit;
+        visit(exprctx1);
+
+        final ExpressionContext exprctx2 = adapter.updateExpressionTree(ctx.expression(2));
+        final ExpressionMetadata expremd2 = adapter.createExpressionMetadata(exprctx2);
+        expremd2.to = condemd.to;
+        expremd2.explicit = condemd.explicit;
+        visit(exprctx2);
+
+        if (condemd.to == null) {
+            final Type promote = promoteConditional(expremd1.from, expremd2.from, expremd1.preConst, expremd2.preConst);
+
+            expremd1.to = promote;
+            expremd2.to = promote;
+            condemd.from = promote;
+        } else {
+            condemd.from = condemd.to;
+        }
+
+        markCast(expremd1);
+        markCast(expremd2);
+
+        condemd.typesafe = expremd0.typesafe && expremd1.typesafe;
+
+        return null;
+    }
+
+    @Override
+    public Void visitAssignment(final AssignmentContext ctx) {
+        final ExpressionMetadata assignemd = adapter.getExpressionMetadata(ctx);
+
+        final ExtstartContext extstartctx = ctx.extstart();
+        final ExternalMetadata extstartemd = adapter.createExternalMetadata(extstartctx);
+
+        extstartemd.read = assignemd.read;
+        extstartemd.storeExpr = adapter.updateExpressionTree(ctx.expression());
+
+        if (ctx.AMUL() != null) {
+            extstartemd.token = MUL;
+        } else if (ctx.ADIV() != null) {
+            extstartemd.token = DIV;
+        } else if (ctx.AREM() != null) {
+            extstartemd.token = REM;
+        } else if (ctx.AADD() != null) {
+            extstartemd.token = ADD;
+        } else if (ctx.ASUB() != null) {
+            extstartemd.token = SUB;
+        } else if (ctx.ALSH() != null) {
+            extstartemd.token = LSH;
+        } else if (ctx.AUSH() != null) {
+            extstartemd.token = USH;
+        } else if (ctx.ARSH() != null) {
+            extstartemd.token = RSH;
+        } else if (ctx.AAND() != null) {
+            extstartemd.token = BWAND;
+        } else if (ctx.AXOR() != null) {
+            extstartemd.token = BWXOR;
+        } else if (ctx.AOR() != null) {
+            extstartemd.token = BWOR;
+        }
+
+        visit(extstartctx);
+
+        assignemd.statement = true;
+        assignemd.from = extstartemd.read ? extstartemd.current : definition.voidType;
+        assignemd.typesafe = extstartemd.current.sort != Sort.DEF;
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtstart(final ExtstartContext ctx) {
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            adapter.createExtNodeMetadata(ctx, precctx);
+            visit(precctx);
+        } else if (castctx != null) {
+            adapter.createExtNodeMetadata(ctx, castctx);
+            visit(castctx);
+        } else if (typectx != null) {
+            adapter.createExtNodeMetadata(ctx, typectx);
+            visit(typectx);
+        } else if (varctx != null) {
+            adapter.createExtNodeMetadata(ctx, varctx);
+            visit(varctx);
+        } else if (newctx != null) {
+            adapter.createExtNodeMetadata(ctx, newctx);
+            visit(newctx);
+        } else if (stringctx != null) {
+            adapter.createExtNodeMetadata(ctx, stringctx);
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException();
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtprec(final ExtprecContext ctx) {
+        final ExtNodeMetadata precenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = precenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null || bracectx != null) {
+            ++parentemd.scope;
+        }
+
+        if (precctx != null) {
+            adapter.createExtNodeMetadata(parent, precctx);
+            visit(precctx);
+        } else if (castctx != null) {
+            adapter.createExtNodeMetadata(parent, castctx);
+            visit(castctx);
+        } else if (typectx != null) {
+            adapter.createExtNodeMetadata(parent, typectx);
+            visit(typectx);
+        } else if (varctx != null) {
+            adapter.createExtNodeMetadata(parent, varctx);
+            visit(varctx);
+        } else if (newctx != null) {
+            adapter.createExtNodeMetadata(parent, newctx);
+            visit(newctx);
+        } else if (stringctx != null) {
+            adapter.createExtNodeMetadata(ctx, stringctx);
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        parentemd.statement = false;
+
+        if (dotctx != null) {
+            --parentemd.scope;
+
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            --parentemd.scope;
+
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtcast(final ExtcastContext ctx) {
+        final ExtNodeMetadata castenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = castenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            adapter.createExtNodeMetadata(parent, precctx);
+            visit(precctx);
+        } else if (castctx != null) {
+            adapter.createExtNodeMetadata(parent, castctx);
+            visit(castctx);
+        } else if (typectx != null) {
+            adapter.createExtNodeMetadata(parent, typectx);
+            visit(typectx);
+        } else if (varctx != null) {
+            adapter.createExtNodeMetadata(parent, varctx);
+            visit(varctx);
+        } else if (newctx != null) {
+            adapter.createExtNodeMetadata(parent, newctx);
+            visit(newctx);
+        } else if (stringctx != null) {
+            adapter.createExtNodeMetadata(ctx, stringctx);
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+        }
+
+        final DecltypeContext declctx = ctx.decltype();
+        final ExpressionMetadata declemd = adapter.createExpressionMetadata(declctx);
+        visit(declctx);
+
+        castenmd.castTo = getLegalCast(ctx, parentemd.current, declemd.from, true);
+        castenmd.type = declemd.from;
+        parentemd.current = declemd.from;
+        parentemd.statement = false;
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtbrace(final ExtbraceContext ctx) {
+        final ExtNodeMetadata braceenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = braceenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final boolean array = parentemd.current.sort == Sort.ARRAY;
+        final boolean def = parentemd.current.sort == Sort.DEF;
+        boolean map = false;
+        boolean list = false;
+
+        try {
+            parentemd.current.clazz.asSubclass(Map.class);
+            map = true;
+        } catch (ClassCastException exception) {
+            // Do nothing.
+        }
+
+        try {
+            parentemd.current.clazz.asSubclass(List.class);
+            list = true;
+        } catch (ClassCastException exception) {
+            // Do nothing.
+        }
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        braceenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+        final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+
+        if (array || def) {
+            expremd.to = array ? definition.intType : definition.objectType;
+            visit(exprctx);
+            markCast(expremd);
+
+            braceenmd.target = "#brace";
+            braceenmd.type = def ? definition.defType :
+                    definition.getType(parentemd.current.struct, parentemd.current.type.getDimensions() - 1);
+            analyzeLoadStoreExternal(ctx);
+            parentemd.current = braceenmd.type;
+
+            if (dotctx != null) {
+                adapter.createExtNodeMetadata(parent, dotctx);
+                visit(dotctx);
+            } else if (bracectx != null) {
+                adapter.createExtNodeMetadata(parent, bracectx);
+                visit(bracectx);
+            }
+        } else {
+            final boolean store = braceenmd.last && parentemd.storeExpr != null;
+            final boolean get = parentemd.read || parentemd.token > 0 || !braceenmd.last;
+            final boolean set = braceenmd.last && store;
+
+            Method getter;
+            Method setter;
+            Type valuetype;
+            Type settype;
+
+            if (map) {
+                getter = parentemd.current.struct.methods.get("get");
+                setter = parentemd.current.struct.methods.get("put");
+
+                if (getter != null && (getter.rtn.sort == Sort.VOID || getter.arguments.size() != 1)) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal map get shortcut for type [" + parentemd.current.name + "].");
+                }
+
+                if (setter != null && setter.arguments.size() != 2) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal map set shortcut for type [" + parentemd.current.name + "].");
+                }
+
+                if (getter != null && setter != null && (!getter.arguments.get(0).equals(setter.arguments.get(0))
+                        || !getter.rtn.equals(setter.arguments.get(1)))) {
+                    throw new IllegalArgumentException(error(ctx) + "Shortcut argument types must match.");
+                }
+
+                valuetype = setter != null ? setter.arguments.get(0) : getter != null ? getter.arguments.get(0) : null;
+                settype = setter == null ? null : setter.arguments.get(1);
+            } else if (list) {
+                getter = parentemd.current.struct.methods.get("get");
+                setter = parentemd.current.struct.methods.get("add");
+
+                if (getter != null && (getter.rtn.sort == Sort.VOID || getter.arguments.size() != 1 ||
+                        getter.arguments.get(0).sort != Sort.INT)) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal list get shortcut for type [" + parentemd.current.name + "].");
+                }
+
+                if (setter != null && (setter.arguments.size() != 2 || setter.arguments.get(0).sort != Sort.INT)) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal list set shortcut for type [" + parentemd.current.name + "].");
+                }
+
+                if (getter != null && setter != null && (!getter.arguments.get(0).equals(setter.arguments.get(0))
+                        || !getter.rtn.equals(setter.arguments.get(1)))) {
+                    throw new IllegalArgumentException(error(ctx) + "Shortcut argument types must match.");
+                }
+
+                valuetype = definition.intType;
+                settype = setter == null ? null : setter.arguments.get(1);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+            }
+
+            if ((get || set) && (!get || getter != null) && (!set || setter != null)) {
+                expremd.to = valuetype;
+                visit(exprctx);
+                markCast(expremd);
+
+                braceenmd.target = new Object[] {getter, setter, true, null};
+                braceenmd.type = get ? getter.rtn : settype;
+                analyzeLoadStoreExternal(ctx);
+                parentemd.current = get ? getter.rtn : setter.rtn;
+            }
+        }
+
+        if (braceenmd.target == null) {
+            throw new IllegalArgumentException(error(ctx) +
+                    "Attempting to address a non-array type [" + parentemd.current.name + "] as an array.");
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtdot(final ExtdotContext ctx) {
+        final ExtNodeMetadata dotemnd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = dotemnd.parent;
+
+        final ExtcallContext callctx = ctx.extcall();
+        final ExtfieldContext fieldctx = ctx.extfield();
+
+        if (callctx != null) {
+            adapter.createExtNodeMetadata(parent, callctx);
+            visit(callctx);
+        } else if (fieldctx != null) {
+            adapter.createExtNodeMetadata(parent, fieldctx);
+            visit(fieldctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExttype(final ExttypeContext ctx) {
+        final ExtNodeMetadata typeenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = typeenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        if (parentemd.current != null) {
+            throw new IllegalArgumentException(error(ctx) + "Unexpected static type.");
+        }
+
+        final String typestr = ctx.TYPE().getText();
+        typeenmd.type = definition.getType(typestr);
+        parentemd.current = typeenmd.type;
+        parentemd.statik = true;
+
+        final ExtdotContext dotctx = ctx.extdot();
+        adapter.createExtNodeMetadata(parent, dotctx);
+        visit(dotctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtcall(final ExtcallContext ctx) {
+        final ExtNodeMetadata callenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = callenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        callenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+
+        final String name = ctx.EXTID().getText();
+
+        if (parentemd.current.sort == Sort.ARRAY) {
+            throw new IllegalArgumentException(error(ctx) + "Unexpected call [" + name + "] on an array.");
+        } else if (callenmd.last && parentemd.storeExpr != null) {
+            throw new IllegalArgumentException(error(ctx) + "Cannot assign a value to a call [" + name + "].");
+        }
+
+        final Struct struct = parentemd.current.struct;
+        final List<ExpressionContext> arguments = ctx.arguments().expression();
+        final int size = arguments.size();
+        Type[] types;
+
+        final Method method = parentemd.statik ? struct.functions.get(name) : struct.methods.get(name);
+        final boolean def = parentemd.current.sort == Sort.DEF;
+
+        if (method == null && !def) {
+            throw new IllegalArgumentException(
+                    error(ctx) + "Unknown call [" + name + "] on type [" + struct.name + "].");
+        } else if (method != null) {
+            types = new Type[method.arguments.size()];
+            method.arguments.toArray(types);
+
+            callenmd.target = method;
+            callenmd.type = method.rtn;
+            parentemd.statement = !parentemd.read && callenmd.last;
+            parentemd.current = method.rtn;
+
+            if (size != types.length) {
+                throw new IllegalArgumentException(error(ctx) + "When calling [" + name + "] on type " +
+                        "[" + struct.name + "] expected [" + types.length + "] arguments," +
+                        " but found [" + arguments.size() + "].");
+            }
+        } else {
+            types = new Type[arguments.size()];
+            Arrays.fill(types, definition.defType);
+
+            callenmd.target = name;
+            callenmd.type = definition.defType;
+            parentemd.statement = !parentemd.read && callenmd.last;
+            parentemd.current = callenmd.type;
+        }
+
+        for (int argument = 0; argument < size; ++argument) {
+            final ExpressionContext exprctx = adapter.updateExpressionTree(arguments.get(argument));
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.to = types[argument];
+            visit(exprctx);
+            markCast(expremd);
+        }
+
+        parentemd.statik = false;
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtvar(final ExtvarContext ctx) {
+        final ExtNodeMetadata varenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = varenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final String name = ctx.ID().getText();
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (parentemd.current != null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected variable [" + name + "] load.");
+        }
+
+        varenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+
+        final Variable variable = getVariable(name);
+
+        if (variable == null) {
+            throw new IllegalArgumentException(error(ctx) + "Unknown variable [" + name + "].");
+        }
+
+        varenmd.target = variable.slot;
+        varenmd.type = variable.type;
+        analyzeLoadStoreExternal(ctx);
+        parentemd.current = varenmd.type;
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtfield(final ExtfieldContext ctx) {
+        final ExtNodeMetadata memberenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = memberenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        if (ctx.EXTID() == null && ctx.EXTINTEGER() == null) {
+            throw new IllegalArgumentException(error(ctx) + "Unexpected parser state.");
+        }
+
+        final String value = ctx.EXTID() == null ? ctx.EXTINTEGER().getText() : ctx.EXTID().getText();
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        memberenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+        final boolean store = memberenmd.last && parentemd.storeExpr != null;
+
+        if (parentemd.current == null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected field [" + value + "] load.");
+        }
+
+        if (parentemd.current.sort == Sort.ARRAY) {
+            if ("length".equals(value)) {
+                if (!parentemd.read) {
+                    throw new IllegalArgumentException(error(ctx) + "Must read array field [length].");
+                } else if (store) {
+                    throw new IllegalArgumentException(
+                            error(ctx) + "Cannot write to read-only array field [length].");
+                }
+
+                memberenmd.target = "#length";
+                memberenmd.type = definition.intType;
+                parentemd.current = definition.intType;
+            } else {
+                throw new IllegalArgumentException(error(ctx) + "Unexpected array field [" + value + "].");
+            }
+        } else if (parentemd.current.sort == Sort.DEF) {
+            memberenmd.target = value;
+            memberenmd.type = definition.defType;
+            analyzeLoadStoreExternal(ctx);
+            parentemd.current = memberenmd.type;
+        } else {
+            final Struct struct = parentemd.current.struct;
+            final Field field = parentemd.statik ? struct.statics.get(value) : struct.members.get(value);
+
+            if (field != null) {
+                if (store && java.lang.reflect.Modifier.isFinal(field.reflect.getModifiers())) {
+                    throw new IllegalArgumentException(error(ctx) + "Cannot write to read-only" +
+                            " field [" + value + "] for type [" + struct.name + "].");
+                }
+
+                memberenmd.target = field;
+                memberenmd.type = field.type;
+                analyzeLoadStoreExternal(ctx);
+                parentemd.current = memberenmd.type;
+            } else {
+                final boolean get = parentemd.read || parentemd.token > 0 || !memberenmd.last;
+                final boolean set = memberenmd.last && store;
+
+                Method getter = struct.methods.get("get" + Character.toUpperCase(value.charAt(0)) + value.substring(1));
+                Method setter = struct.methods.get("set" + Character.toUpperCase(value.charAt(0)) + value.substring(1));
+                Object constant = null;
+
+                if (getter != null && (getter.rtn.sort == Sort.VOID || !getter.arguments.isEmpty())) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal get shortcut on field [" + value + "] for type [" + struct.name + "].");
+                }
+
+                if (setter != null && (setter.rtn.sort != Sort.VOID || setter.arguments.size() != 1)) {
+                    throw new IllegalArgumentException(error(ctx) +
+                            "Illegal set shortcut on field [" + value + "] for type [" + struct.name + "].");
+                }
+
+                Type settype = setter == null ? null : setter.arguments.get(0);
+
+                if (getter == null && setter == null) {
+                    if (ctx.EXTID() != null) {
+                        try {
+                            parentemd.current.clazz.asSubclass(Map.class);
+
+                            getter = parentemd.current.struct.methods.get("get");
+                            setter = parentemd.current.struct.methods.get("put");
+
+                            if (getter != null && (getter.rtn.sort == Sort.VOID || getter.arguments.size() != 1 ||
+                                getter.arguments.get(0).sort != Sort.STRING)) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal map get shortcut [" + value + "] for type [" + struct.name + "].");
+                            }
+
+                            if (setter != null && (setter.arguments.size() != 2 ||
+                                    setter.arguments.get(0).sort != Sort.STRING)) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal map set shortcut [" + value + "] for type [" + struct.name + "].");
+                            }
+
+                            if (getter != null && setter != null && !getter.rtn.equals(setter.arguments.get(1))) {
+                                throw new IllegalArgumentException(error(ctx) + "Shortcut argument types must match.");
+                            }
+
+                            settype = setter == null ? null : setter.arguments.get(1);
+                            constant = value;
+                        } catch (ClassCastException exception) {
+                            //Do nothing.
+                        }
+                    } else if (ctx.EXTINTEGER() != null) {
+                        try {
+                            parentemd.current.clazz.asSubclass(List.class);
+
+                            getter = parentemd.current.struct.methods.get("get");
+                            setter = parentemd.current.struct.methods.get("add");
+
+                            if (getter != null && (getter.rtn.sort == Sort.VOID || getter.arguments.size() != 1 ||
+                                    getter.arguments.get(0).sort != Sort.INT)) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal list get shortcut [" + value + "] for type [" + struct.name + "].");
+                            }
+
+                            if (setter != null && (setter.rtn.sort != Sort.VOID || setter.arguments.size() != 2 ||
+                                    setter.arguments.get(0).sort != Sort.INT)) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal list add shortcut [" + value + "] for type [" + struct.name + "].");
+                            }
+
+                            if (getter != null && setter != null && !getter.rtn.equals(setter.arguments.get(1))) {
+                                throw new IllegalArgumentException(error(ctx) + "Shortcut argument types must match.");
+                            }
+
+                            settype = setter == null ? null : setter.arguments.get(1);
+
+                            try {
+                                constant = Integer.parseInt(value);
+                            } catch (NumberFormatException exception) {
+                                throw new IllegalArgumentException(error(ctx) +
+                                        "Illegal list shortcut value [" + value + "].");
+                            }
+                        } catch (ClassCastException exception) {
+                            //Do nothing.
+                        }
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected parser state.");
+                    }
+                }
+
+                if ((get || set) && (!get || getter != null) && (!set || setter != null)) {
+                    memberenmd.target = new Object[] {getter, setter, constant != null, constant};
+                    memberenmd.type = get ? getter.rtn : settype;
+                    analyzeLoadStoreExternal(ctx);
+                    parentemd.current = get ? getter.rtn : setter.rtn;
+                }
+            }
+
+            if (memberenmd.target == null) {
+                throw new IllegalArgumentException(
+                        error(ctx) + "Unknown field [" + value + "] for type [" + struct.name + "].");
+            }
+        }
+
+        parentemd.statik = false;
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtnew(ExtnewContext ctx) {
+        final ExtNodeMetadata newenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = newenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        newenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+
+        final String name = ctx.TYPE().getText();
+        final Struct struct = definition.structs.get(name);
+
+        if (parentemd.current != null) {
+            throw new IllegalArgumentException(error(ctx) + "Unexpected new call.");
+        } else if (struct == null) {
+            throw new IllegalArgumentException(error(ctx) + "Specified type [" + name + "] not found.");
+        } else if (newenmd.last && parentemd.storeExpr != null) {
+            throw new IllegalArgumentException(error(ctx) + "Cannot assign a value to a new call.");
+        }
+
+        final boolean newclass = ctx.arguments() != null;
+        final boolean newarray = !ctx.expression().isEmpty();
+
+        final List<ExpressionContext> arguments = newclass ? ctx.arguments().expression() : ctx.expression();
+        final int size = arguments.size();
+
+        Type[] types;
+
+        if (newarray) {
+            if (!parentemd.read) {
+                throw new IllegalArgumentException(error(ctx) + "A newly created array must be assigned.");
+            }
+
+            types = new Type[size];
+            Arrays.fill(types, definition.intType);
+
+            newenmd.target = "#makearray";
+
+            if (size > 1) {
+                newenmd.type = definition.getType(struct, size);
+                parentemd.current = newenmd.type;
+            } else if (size == 1) {
+                newenmd.type = definition.getType(struct, 0);
+                parentemd.current = definition.getType(struct, 1);
+            } else {
+                throw new IllegalArgumentException(error(ctx) + "A newly created array cannot have zero dimensions.");
+            }
+        } else if (newclass) {
+            final Constructor constructor = struct.constructors.get("new");
+
+            if (constructor != null) {
+                types = new Type[constructor.arguments.size()];
+                constructor.arguments.toArray(types);
+
+                newenmd.target = constructor;
+                newenmd.type = definition.getType(struct, 0);
+                parentemd.statement = !parentemd.read && newenmd.last;
+                parentemd.current = newenmd.type;
+            } else {
+                throw new IllegalArgumentException(
+                        error(ctx) + "Unknown new call on type [" + struct.name + "].");
+            }
+        } else {
+            throw new IllegalArgumentException(error(ctx) + "Unknown parser state.");
+        }
+
+        if (size != types.length) {
+            throw new IllegalArgumentException(error(ctx) + "When calling [" + name + "] on type " +
+                    "[" + struct.name + "] expected [" + types.length + "] arguments," +
+                    " but found [" + arguments.size() + "].");
+        }
+
+        for (int argument = 0; argument < size; ++argument) {
+            final ExpressionContext exprctx = adapter.updateExpressionTree(arguments.get(argument));
+            final ExpressionMetadata expremd = adapter.createExpressionMetadata(exprctx);
+            expremd.to = types[argument];
+            visit(exprctx);
+            markCast(expremd);
+        }
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtstring(final ExtstringContext ctx) {
+        final ExtNodeMetadata memberenmd = adapter.getExtNodeMetadata(ctx);
+        final ParserRuleContext parent = memberenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        final String string = ctx.STRING().getText();
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        memberenmd.last = parentemd.scope == 0 && dotctx == null && bracectx == null;
+        final boolean store = memberenmd.last && parentemd.storeExpr != null;
+
+        if (parentemd.current != null) {
+            throw new IllegalStateException(error(ctx) + "Unexpected String constant [" + string + "].");
+        }
+
+        if (!parentemd.read) {
+            throw new IllegalArgumentException(error(ctx) + "Must read String constant [" + string + "].");
+        } else if (store) {
+            throw new IllegalArgumentException(
+                    error(ctx) + "Cannot write to read-only String constant [" + string + "].");
+        }
+
+        memberenmd.target = string;
+        memberenmd.type = definition.stringType;
+        parentemd.current = definition.stringType;
+
+        if (memberenmd.last) {
+            parentemd.constant = string;
+        }
+
+        if (dotctx != null) {
+            adapter.createExtNodeMetadata(parent, dotctx);
+            visit(dotctx);
+        } else if (bracectx != null) {
+            adapter.createExtNodeMetadata(parent, bracectx);
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitArguments(final ArgumentsContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected parser state.");
+    }
+
+    @Override
+    public Void visitIncrement(IncrementContext ctx) {
+        final ExpressionMetadata incremd = adapter.getExpressionMetadata(ctx);
+        final Sort sort = incremd.to == null ? null : incremd.to.sort;
+        final boolean positive = ctx.INCR() != null;
+
+        if (incremd.to == null) {
+            incremd.preConst = positive ? 1 : -1;
+            incremd.from = definition.intType;
+        } else {
+            switch (sort) {
+                case LONG:
+                    incremd.preConst = positive ? 1L : -1L;
+                    incremd.from = definition.longType;
+                case FLOAT:
+                    incremd.preConst = positive ? 1.0F : -1.0F;
+                    incremd.from = definition.floatType;
+                case DOUBLE:
+                    incremd.preConst = positive ? 1.0 : -1.0;
+                    incremd.from = definition.doubleType;
+                default:
+                    incremd.preConst = positive ? 1 : -1;
+                    incremd.from = definition.intType;
+            }
+        }
+
+        return null;
+    }
+
+    private void analyzeLoadStoreExternal(final ParserRuleContext source) {
+        final ExtNodeMetadata extenmd = adapter.getExtNodeMetadata(source);
+        final ParserRuleContext parent = extenmd.parent;
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(parent);
+
+        if (extenmd.last && parentemd.storeExpr != null) {
+            final ParserRuleContext store = parentemd.storeExpr;
+            final ExpressionMetadata storeemd = adapter.createExpressionMetadata(parentemd.storeExpr);
+            final int token = parentemd.token;
+
+            if (token > 0) {
+                visit(store);
+
+                final boolean add = token == ADD;
+                final boolean xor = token == BWAND || token == BWXOR || token == BWOR;
+                final boolean decimal = token == MUL || token == DIV || token == REM || token == SUB;
+
+                extenmd.promote = add ? promoteAdd(extenmd.type, storeemd.from) :
+                                  xor ? promoteXor(extenmd.type, storeemd.from) :
+                                        promoteNumeric(extenmd.type, storeemd.from, decimal, true);
+
+                if (extenmd.promote == null) {
+                    throw new IllegalArgumentException("Cannot apply compound assignment to " +
+                            " types [" + extenmd.type.name + "] and [" + storeemd.from.name + "].");
+                }
+
+                extenmd.castFrom = getLegalCast(source, extenmd.type, extenmd.promote, false);
+                extenmd.castTo = getLegalCast(source, extenmd.promote, extenmd.type, true);
+
+                storeemd.to = add && extenmd.promote.sort == Sort.STRING ? storeemd.from : extenmd.promote;
+                markCast(storeemd);
+            } else {
+                storeemd.to = extenmd.type;
+                visit(store);
+                markCast(storeemd);
+            }
+        }
+    }
+
+    private void markCast(final ExpressionMetadata emd) {
+        if (emd.from == null) {
+            throw new IllegalStateException(error(emd.source) + "From cast type should never be null.");
+        }
+
+        if (emd.to != null) {
+            emd.cast = getLegalCast(emd.source, emd.from, emd.to, emd.explicit || !emd.typesafe);
+
+            if (emd.preConst != null && emd.to.sort.constant) {
+                emd.postConst = constCast(emd.source, emd.preConst, emd.cast);
+            }
+        } else {
+            throw new IllegalStateException(error(emd.source) + "To cast type should never be null.");
+        }
+    }
+
+    private Cast getLegalCast(final ParserRuleContext source, final Type from, final Type to, final boolean explicit) {
+        final Cast cast = new Cast(from, to);
+
+        if (from.equals(to)) {
+            return cast;
+        }
+
+        if (from.sort == Sort.DEF && to.sort != Sort.VOID || from.sort != Sort.VOID && to.sort == Sort.DEF) {
+            final Transform transform = definition.transforms.get(cast);
+
+            if (transform != null) {
+                return transform;
+            }
+
+            return cast;
+        }
+
+        switch (from.sort) {
+            case BOOL:
+                switch (to.sort) {
+                    case OBJECT:
+                    case BOOL_OBJ:
+                        return checkTransform(source, cast);
+                }
+
+                break;
+            case BYTE:
+                switch (to.sort) {
+                    case SHORT:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case CHAR:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case SHORT:
+                switch (to.sort) {
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case CHAR:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case SHORT_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case CHAR:
+                switch (to.sort) {
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case SHORT:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case INT:
+                switch (to.sort) {
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case LONG:
+                switch (to.sort) {
+                    case FLOAT:
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case FLOAT:
+                switch (to.sort) {
+                    case DOUBLE:
+                        return cast;
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case DOUBLE:
+                switch (to.sort) {
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                        if (explicit)
+                            return cast;
+
+                        break;
+                    case OBJECT:
+                    case NUMBER:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case OBJECT:
+            case NUMBER:
+                switch (to.sort) {
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case BOOL_OBJ:
+                switch (to.sort) {
+                    case BOOL:
+                        return checkTransform(source, cast);
+                }
+
+                break;
+            case BYTE_OBJ:
+                switch (to.sort) {
+                    case BYTE:
+                    case SHORT:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case SHORT_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case CHAR:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case SHORT_OBJ:
+                switch (to.sort) {
+                    case SHORT:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case CHAR:
+                    case BYTE_OBJ:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case CHAR_OBJ:
+                switch (to.sort) {
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case INT_OBJ:
+                switch (to.sort) {
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case LONG_OBJ:
+                switch (to.sort) {
+                    case LONG:
+                    case FLOAT:
+                    case DOUBLE:
+                    case FLOAT_OBJ:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case FLOAT_OBJ:
+                switch (to.sort) {
+                    case FLOAT:
+                    case DOUBLE:
+                    case DOUBLE_OBJ:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+            case DOUBLE_OBJ:
+                switch (to.sort) {
+                    case DOUBLE:
+                        return checkTransform(source, cast);
+                    case BYTE:
+                    case SHORT:
+                    case CHAR:
+                    case INT:
+                    case LONG:
+                    case FLOAT:
+                    case BYTE_OBJ:
+                    case SHORT_OBJ:
+                    case CHAR_OBJ:
+                    case INT_OBJ:
+                    case LONG_OBJ:
+                    case FLOAT_OBJ:
+                        if (explicit)
+                            return checkTransform(source, cast);
+
+                        break;
+                }
+
+                break;
+        }
+
+        try {
+            from.clazz.asSubclass(to.clazz);
+
+            return cast;
+        } catch (ClassCastException cce0) {
+            try {
+                if (explicit) {
+                    to.clazz.asSubclass(from.clazz);
+
+                    return cast;
+                } else {
+                    throw new ClassCastException(
+                            error(source) + "Cannot cast from [" + from.name + "] to [" + to.name + "].");
+                }
+            } catch (ClassCastException cce1) {
+                throw new ClassCastException(
+                        error(source) + "Cannot cast from [" + from.name + "] to [" + to.name + "].");
+            }
+        }
+    }
+
+    private Transform checkTransform(final ParserRuleContext source, final Cast cast) {
+        final Transform transform = definition.transforms.get(cast);
+
+        if (transform == null) {
+            throw new ClassCastException(
+                    error(source) + "Cannot cast from [" + cast.from.name + "] to [" + cast.to.name + "].");
+        }
+
+        return transform;
+    }
+
+    private Object constCast(final ParserRuleContext source, final Object constant, final Cast cast) {
+        if (cast instanceof Transform) {
+            final Transform transform = (Transform)cast;
+            return invokeTransform(source, transform, constant);
+        } else {
+            final Sort fsort = cast.from.sort;
+            final Sort tsort = cast.to.sort;
+
+            if (fsort == tsort) {
+                return constant;
+            } else if (fsort.numeric && tsort.numeric) {
+                Number number;
+
+                if (fsort == Sort.CHAR) {
+                    number = (int)(char)constant;
+                } else {
+                    number = (Number)constant;
+                }
+
+                switch (tsort) {
+                    case BYTE:   return number.byteValue();
+                    case SHORT:  return number.shortValue();
+                    case CHAR:   return (char)number.intValue();
+                    case INT:    return number.intValue();
+                    case LONG:   return number.longValue();
+                    case FLOAT:  return number.floatValue();
+                    case DOUBLE: return number.doubleValue();
+                    default:
+                        throw new IllegalStateException(error(source) + "Expected numeric type for cast.");
+                }
+            } else {
+                throw new IllegalStateException(error(source) + "No valid constant cast from " +
+                        "[" + cast.from.clazz.getCanonicalName() + "] to " +
+                        "[" + cast.to.clazz.getCanonicalName() + "].");
+            }
+        }
+    }
+
+    private Object invokeTransform(final ParserRuleContext source, final Transform transform, final Object object) {
+        final Method method = transform.method;
+        final java.lang.reflect.Method jmethod = method.reflect;
+        final int modifiers = jmethod.getModifiers();
+
+        try {
+            if (java.lang.reflect.Modifier.isStatic(modifiers)) {
+                return jmethod.invoke(null, object);
+            } else {
+                return jmethod.invoke(object);
+            }
+        } catch (IllegalAccessException | IllegalArgumentException |
+                java.lang.reflect.InvocationTargetException | NullPointerException |
+                ExceptionInInitializerError exception) {
+            throw new IllegalStateException(error(source) + "Unable to invoke transform to cast constant from " +
+                    "[" + transform.from.name + "] to [" + transform.to.name + "].");
+        }
+    }
+
+    private Type promoteNumeric(final Type from, boolean decimal, boolean primitive) {
+        final Sort sort = from.sort;
+
+        if (sort == Sort.DEF) {
+            return definition.defType;
+        } else if ((sort == Sort.DOUBLE || sort == Sort.DOUBLE_OBJ || sort == Sort.NUMBER) && decimal) {
+             return primitive ? definition.doubleType : definition.doubleobjType;
+        } else if ((sort == Sort.FLOAT || sort == Sort.FLOAT_OBJ) && decimal) {
+            return primitive ? definition.floatType : definition.floatobjType;
+        } else if (sort == Sort.LONG || sort == Sort.LONG_OBJ || sort == Sort.NUMBER) {
+            return primitive ? definition.longType : definition.longobjType;
+        } else if (sort.numeric) {
+            return primitive ? definition.intType : definition.intobjType;
+        }
+
+        return null;
+    }
+
+    private Type promoteNumeric(final Type from0, final Type from1, boolean decimal, boolean primitive) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.DEF || sort1 == Sort.DEF) {
+            return definition.defType;
+        }
+
+        if (decimal) {
+            if (sort0 == Sort.DOUBLE || sort0 == Sort.DOUBLE_OBJ || sort0 == Sort.NUMBER ||
+                    sort1 == Sort.DOUBLE || sort1 == Sort.DOUBLE_OBJ || sort1 == Sort.NUMBER) {
+                return primitive ? definition.doubleType : definition.doubleobjType;
+            } else if (sort0 == Sort.FLOAT || sort0 == Sort.FLOAT_OBJ || sort1 == Sort.FLOAT || sort1 == Sort.FLOAT_OBJ) {
+                return primitive ? definition.floatType : definition.floatobjType;
+            }
+        }
+
+        if (sort0 == Sort.LONG || sort0 == Sort.LONG_OBJ || sort0 == Sort.NUMBER ||
+                sort1 == Sort.LONG || sort1 == Sort.LONG_OBJ || sort1 == Sort.NUMBER) {
+             return primitive ? definition.longType : definition.longobjType;
+        } else if (sort0.numeric && sort1.numeric) {
+            return primitive ? definition.intType : definition.intobjType;
+        }
+
+        return null;
+    }
+
+    private Type promoteAdd(final Type from0, final Type from1) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.STRING || sort1 == Sort.STRING) {
+            return definition.stringType;
+        }
+
+        return promoteNumeric(from0, from1, true, true);
+    }
+
+    private Type promoteXor(final Type from0, final Type from1) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0.bool || sort1.bool) {
+            return definition.booleanType;
+        }
+
+        return promoteNumeric(from0, from1, false, true);
+    }
+
+    private Type promoteEquality(final Type from0, final Type from1) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.DEF || sort1 == Sort.DEF) {
+            return definition.defType;
+        }
+
+        final boolean primitive = sort0.primitive && sort1.primitive;
+
+        if (sort0.bool && sort1.bool) {
+            return primitive ? definition.booleanType : definition.byteobjType;
+        }
+
+        if (sort0.numeric && sort1.numeric) {
+            return promoteNumeric(from0, from1, true, primitive);
+        }
+
+        return definition.objectType;
+    }
+
+    private Type promoteReference(final Type from0, final Type from1) {
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.DEF || sort1 == Sort.DEF) {
+            return definition.defType;
+        }
+
+        if (sort0.primitive && sort1.primitive) {
+            if (sort0.bool && sort1.bool) {
+                return definition.booleanType;
+            }
+
+            if (sort0.numeric && sort1.numeric) {
+                return promoteNumeric(from0, from1, true, true);
+            }
+        }
+
+        return definition.objectType;
+    }
+
+    private Type promoteConditional(final Type from0, final Type from1, final Object const0, final Object const1) {
+        if (from0.equals(from1)) {
+            return from0;
+        }
+
+        final Sort sort0 = from0.sort;
+        final Sort sort1 = from1.sort;
+
+        if (sort0 == Sort.DEF || sort1 == Sort.DEF) {
+            return definition.defType;
+        }
+
+        final boolean primitive = sort0.primitive && sort1.primitive;
+
+        if (sort0.bool && sort1.bool) {
+            return primitive ? definition.booleanType : definition.booleanobjType;
+        }
+
+        if (sort0.numeric && sort1.numeric) {
+            if (sort0 == Sort.DOUBLE || sort0 == Sort.DOUBLE_OBJ || sort1 == Sort.DOUBLE || sort1 == Sort.DOUBLE_OBJ) {
+                return primitive ? definition.doubleType : definition.doubleobjType;
+            } else if (sort0 == Sort.FLOAT || sort0 == Sort.FLOAT_OBJ || sort1 == Sort.FLOAT || sort1 == Sort.FLOAT_OBJ) {
+                return primitive ? definition.floatType : definition.floatobjType;
+            } else if (sort0 == Sort.LONG || sort0 == Sort.LONG_OBJ || sort1 == Sort.LONG || sort1 == Sort.LONG_OBJ) {
+                return sort0.primitive && sort1.primitive ? definition.longType : definition.longobjType;
+            } else {
+                if (sort0 == Sort.BYTE || sort0 == Sort.BYTE_OBJ) {
+                    if (sort1 == Sort.BYTE || sort1 == Sort.BYTE_OBJ) {
+                        return primitive ? definition.byteType : definition.byteobjType;
+                    } else if (sort1 == Sort.SHORT || sort1 == Sort.SHORT_OBJ) {
+                        if (const1 != null) {
+                            final short constant = (short)const1;
+
+                            if (constant <= Byte.MAX_VALUE && constant >= Byte.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.shortType : definition.shortobjType;
+                    } else if (sort1 == Sort.CHAR || sort1 == Sort.CHAR_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.INT || sort1 == Sort.INT_OBJ) {
+                        if (const1 != null) {
+                            final int constant = (int)const1;
+
+                            if (constant <= Byte.MAX_VALUE && constant >= Byte.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    }
+                } else if (sort0 == Sort.SHORT || sort0 == Sort.SHORT_OBJ) {
+                    if (sort1 == Sort.BYTE || sort1 == Sort.BYTE_OBJ) {
+                        if (const0 != null) {
+                            final short constant = (short)const0;
+
+                            if (constant <= Byte.MAX_VALUE && constant >= Byte.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.shortType : definition.shortobjType;
+                    } else if (sort1 == Sort.SHORT || sort1 == Sort.SHORT_OBJ) {
+                        return primitive ? definition.shortType : definition.shortobjType;
+                    } else if (sort1 == Sort.CHAR || sort1 == Sort.CHAR_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.INT || sort1 == Sort.INT_OBJ) {
+                        if (const1 != null) {
+                            final int constant = (int)const1;
+
+                            if (constant <= Short.MAX_VALUE && constant >= Short.MIN_VALUE) {
+                                return primitive ? definition.shortType : definition.shortobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    }
+                } else if (sort0 == Sort.CHAR || sort0 == Sort.CHAR_OBJ) {
+                    if (sort1 == Sort.BYTE || sort1 == Sort.BYTE_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.SHORT || sort1 == Sort.SHORT_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.CHAR || sort1 == Sort.CHAR_OBJ) {
+                        return primitive ? definition.charType : definition.charobjType;
+                    } else if (sort1 == Sort.INT || sort1 == Sort.INT_OBJ) {
+                        if (const1 != null) {
+                            final int constant = (int)const1;
+
+                            if (constant <= Character.MAX_VALUE && constant >= Character.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    }
+                } else if (sort0 == Sort.INT || sort0 == Sort.INT_OBJ) {
+                    if (sort1 == Sort.BYTE || sort1 == Sort.BYTE_OBJ) {
+                        if (const0 != null) {
+                            final int constant = (int)const0;
+
+                            if (constant <= Byte.MAX_VALUE && constant >= Byte.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.SHORT || sort1 == Sort.SHORT_OBJ) {
+                        if (const0 != null) {
+                            final int constant = (int)const0;
+
+                            if (constant <= Short.MAX_VALUE && constant >= Short.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.CHAR || sort1 == Sort.CHAR_OBJ) {
+                        if (const0 != null) {
+                            final int constant = (int)const0;
+
+                            if (constant <= Character.MAX_VALUE && constant >= Character.MIN_VALUE) {
+                                return primitive ? definition.byteType : definition.byteobjType;
+                            }
+                        }
+
+                        return primitive ? definition.intType : definition.intobjType;
+                    } else if (sort1 == Sort.INT || sort1 == Sort.INT_OBJ) {
+                        return primitive ? definition.intType : definition.intobjType;
+                    }
+                }
+            }
+        }
+
+        final Pair pair = new Pair(from0, from1);
+        final Type bound = definition.bounds.get(pair);
+
+        return bound == null ? definition.objectType : bound;
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Compiler.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Compiler.java
new file mode 100644
index 0000000..6f4a237
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Compiler.java
@@ -0,0 +1,154 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.net.MalformedURLException;
+import java.net.URL;
+import java.security.CodeSource;
+import java.security.SecureClassLoader;
+import java.security.cert.Certificate;
+
+import org.antlr.v4.runtime.ANTLRInputStream;
+import org.antlr.v4.runtime.CommonTokenStream;
+import org.antlr.v4.runtime.ParserRuleContext;
+import org.elasticsearch.bootstrap.BootstrapInfo;
+
+final class Compiler {
+    private static Definition DEFAULT_DEFINITION = new Definition(new Definition());
+
+    /** we define the class with lowest privileges */
+    private static final CodeSource CODESOURCE;
+
+    static {
+        try {
+            CODESOURCE = new CodeSource(new URL("file:" + BootstrapInfo.UNTRUSTED_CODEBASE), (Certificate[]) null);
+        } catch (MalformedURLException impossible) {
+            throw new RuntimeException(impossible);
+        }
+    }
+
+    static class Loader extends SecureClassLoader {
+        Loader(ClassLoader parent) {
+            super(parent);
+        }
+
+        Class<? extends Executable> define(String name, byte[] bytes) {
+            return defineClass(name, bytes, 0, bytes.length, CODESOURCE).asSubclass(Executable.class);
+        }
+    }
+
+    static Executable compile(Loader loader, final String name, final String source, final Definition custom, CompilerSettings settings) {
+        long start = System.currentTimeMillis();
+
+        final Definition definition = custom == null ? DEFAULT_DEFINITION : new Definition(custom);
+
+        //long end = System.currentTimeMillis() - start;
+        //System.out.println("types: " + end);
+        //start = System.currentTimeMillis();
+
+        //final ParserRuleContext root = createParseTree(source, types);
+        final ANTLRInputStream stream = new ANTLRInputStream(source);
+        final ErrorHandlingLexer lexer = new ErrorHandlingLexer(stream);
+        final PlanAParser parser = new PlanAParser(new CommonTokenStream(lexer));
+        final ParserErrorStrategy strategy = new ParserErrorStrategy();
+
+        lexer.removeErrorListeners();
+        lexer.setTypes(definition.structs.keySet());
+
+        //List<? extends Token> tokens = lexer.getAllTokens();
+
+        //for (final Token token : tokens) {
+        //    System.out.println(token.getType() + " " + token.getText());
+        //}
+
+        parser.removeErrorListeners();
+        parser.setErrorHandler(strategy);
+
+        ParserRuleContext root = parser.source();
+
+        //end = System.currentTimeMillis() - start;
+        //System.out.println("tree: " + end);
+
+        final Adapter adapter = new Adapter(definition, source, root, settings);
+
+        start = System.currentTimeMillis();
+
+        Analyzer.analyze(adapter);
+        //System.out.println(root.toStringTree(parser));
+
+        //end = System.currentTimeMillis() - start;
+        //System.out.println("analyze: " + end);
+        //start = System.currentTimeMillis();
+
+        final byte[] bytes = Writer.write(adapter);
+
+        //end = System.currentTimeMillis() - start;
+        //System.out.println("write: " + end);
+        //start = System.currentTimeMillis();
+
+        final Executable executable = createExecutable(loader, definition, name, source, bytes);
+
+        //end = System.currentTimeMillis() - start;
+        //System.out.println("create: " + end);
+
+        return executable;
+    }
+
+    private static ParserRuleContext createParseTree(String source, Definition definition) {
+        final ANTLRInputStream stream = new ANTLRInputStream(source);
+        final ErrorHandlingLexer lexer = new ErrorHandlingLexer(stream);
+        final PlanAParser parser = new PlanAParser(new CommonTokenStream(lexer));
+        final ParserErrorStrategy strategy = new ParserErrorStrategy();
+
+        lexer.removeErrorListeners();
+        lexer.setTypes(definition.structs.keySet());
+
+        parser.removeErrorListeners();
+        parser.setErrorHandler(strategy);
+
+        ParserRuleContext root = parser.source();
+        // System.out.println(root.toStringTree(parser));
+        return root;
+    }
+
+    private static Executable createExecutable(Loader loader, Definition definition, String name, String source, byte[] bytes) {
+        try {
+            // for debugging:
+             //try {
+             //   FileOutputStream f = new FileOutputStream(new File("/Users/jdconrad/lang/generated/out.class"), false);
+             //   f.write(bytes);
+             //   f.close();
+             //} catch (Exception e) {
+             //   throw new RuntimeException(e);
+             //}
+
+            final Class<? extends Executable> clazz = loader.define(Writer.CLASS_NAME, bytes);
+            final java.lang.reflect.Constructor<? extends Executable> constructor =
+                    clazz.getConstructor(Definition.class, String.class, String.class);
+
+            return constructor.newInstance(definition, name, source);
+        } catch (Exception exception) {
+            throw new IllegalStateException(
+                    "An internal error occurred attempting to define the script [" + name + "].", exception);
+        }
+    }
+
+    private Compiler() {}
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/CompilerSettings.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/CompilerSettings.java
new file mode 100644
index 0000000..f66b65d
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/CompilerSettings.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** 
+ * Settings to use when compiling a script 
+ */
+final class CompilerSettings {
+
+    private boolean numericOverflow = true;
+
+    /**
+     * Returns {@code true} if numeric operations should overflow, {@code false}
+     * if they should signal an exception.
+     * <p>
+     * If this value is {@code true} (default), then things behave like java:
+     * overflow for integer types can result in unexpected values / unexpected
+     * signs, and overflow for floating point types can result in infinite or
+     * {@code NaN} values.
+     */
+    public boolean getNumericOverflow() {
+        return numericOverflow;
+    }
+
+    /**
+     * Set {@code true} for numerics to overflow, false to deliver exceptions.
+     * @see #getNumericOverflow
+     */
+    public void setNumericOverflow(boolean allow) {
+        this.numericOverflow = allow;
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java
new file mode 100644
index 0000000..2a1eb13
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Def.java
@@ -0,0 +1,1250 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.lang.invoke.MethodHandle;
+import java.lang.reflect.Array;
+import java.util.List;
+import java.util.Map;
+
+import static org.elasticsearch.plan.a.Definition.*;
+
+public class Def {
+    public static Object methodCall(final Object owner, final String name, final Definition definition,
+                                    final Object[] arguments, final boolean[] typesafe) {
+        final Method method = getMethod(owner, name, definition);
+
+        if (method == null) {
+            throw new IllegalArgumentException("Unable to find dynamic method [" + name + "] " +
+                    "for class [" + owner.getClass().getCanonicalName() + "].");
+        }
+
+        final MethodHandle handle = method.handle;
+        final List<Type> types = method.arguments;
+        final Object[] parameters = new Object[arguments.length + 1];
+
+        parameters[0] = owner;
+
+        if (types.size() != arguments.length) {
+            throw new IllegalArgumentException("When dynamically calling [" + name + "] from class " +
+                    "[" + owner.getClass() + "] expected [" + types.size() + "] arguments," +
+                    " but found [" + arguments.length + "].");
+        }
+
+        try {
+            for (int count = 0; count < arguments.length; ++count) {
+                if (typesafe[count]) {
+                    parameters[count + 1] = arguments[count];
+                } else {
+                    final Transform transform = getTransform(arguments[count].getClass(), types.get(count).clazz, definition);
+                    parameters[count + 1] = transform == null ? arguments[count] : transform.method.handle.invoke(arguments[count]);
+                }
+            }
+
+            return handle.invokeWithArguments(parameters);
+        } catch (Throwable throwable) {
+            throw new IllegalArgumentException("Error invoking method [" + name + "] " +
+                    "with owner class [" + owner.getClass().getCanonicalName() + "].", throwable);
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    public static void fieldStore(final Object owner, Object value, final String name,
+                                  final Definition definition, final boolean typesafe) {
+        final Field field = getField(owner, name, definition);
+        MethodHandle handle = null;
+
+        if (field == null) {
+            final String set = "set" + Character.toUpperCase(name.charAt(0)) + name.substring(1);
+            final Method method = getMethod(owner, set, definition);
+
+            if (method != null) {
+                handle = method.handle;
+            }
+        } else {
+            handle = field.setter;
+        }
+
+        if (handle != null) {
+            try {
+                if (!typesafe) {
+                    final Transform transform = getTransform(value.getClass(), handle.type().parameterType(1), definition);
+
+                    if (transform != null) {
+                        value = transform.method.handle.invoke(value);
+                    }
+                }
+
+                handle.invoke(owner, value);
+            } catch (Throwable throwable) {
+                throw new IllegalArgumentException("Error storing value [" + value + "] " +
+                        "in field [" + name + "] with owner class [" + owner.getClass() + "].", throwable);
+            }
+        } else if (owner instanceof Map) {
+            ((Map)owner).put(name, value);
+        } else if (owner instanceof List) {
+            try {
+                final int index = Integer.parseInt(name);
+                ((List)owner).add(index, value);
+            } catch (NumberFormatException exception) {
+                throw new IllegalArgumentException( "Illegal list shortcut value [" + name + "].");
+            }
+        } else {
+            throw new IllegalArgumentException("Unable to find dynamic field [" + name + "] " +
+                    "for class [" + owner.getClass().getCanonicalName() + "].");
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    public static Object fieldLoad(final Object owner, final String name, final Definition definition) {
+        if (owner.getClass().isArray() && "length".equals(name)) {
+            return Array.getLength(owner);
+        } else {
+            final Field field = getField(owner, name, definition);
+            MethodHandle handle;
+
+            if (field == null) {
+                final String get = "get" + Character.toUpperCase(name.charAt(0)) + name.substring(1);
+                final Method method = getMethod(owner, get, definition);
+
+                if (method != null) {
+                    handle = method.handle;
+                } else if (owner instanceof Map) {
+                    return ((Map)owner).get(name);
+                } else if (owner instanceof List) {
+                    try {
+                        final int index = Integer.parseInt(name);
+
+                        return ((List)owner).get(index);
+                    } catch (NumberFormatException exception) {
+                        throw new IllegalArgumentException( "Illegal list shortcut value [" + name + "].");
+                    }
+                } else {
+                    throw new IllegalArgumentException("Unable to find dynamic field [" + name + "] " +
+                            "for class [" + owner.getClass().getCanonicalName() + "].");
+                }
+            } else {
+                handle = field.getter;
+            }
+
+            if (handle == null) {
+                throw new IllegalArgumentException(
+                        "Unable to read from field [" + name + "] with owner class [" + owner.getClass() + "].");
+            } else {
+                try {
+                    return handle.invoke(owner);
+                } catch (final Throwable throwable) {
+                    throw new IllegalArgumentException("Error loading value from " +
+                            "field [" + name + "] with owner class [" + owner.getClass() + "].", throwable);
+                }
+            }
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    public static void arrayStore(final Object array, Object index, Object value, final Definition definition,
+                                  final boolean indexsafe, final boolean valuesafe) {
+        if (array instanceof Map) {
+            ((Map)array).put(index, value);
+        } else {
+            try {
+                if (!indexsafe) {
+                    final Transform transform = getTransform(index.getClass(), Integer.class, definition);
+
+                    if (transform != null) {
+                        index = transform.method.handle.invoke(index);
+                    }
+                }
+            } catch (final Throwable throwable) {
+                throw new IllegalArgumentException(
+                        "Error storing value [" + value + "] in list using index [" + index + "].", throwable);
+            }
+
+            if (array.getClass().isArray()) {
+                try {
+                    if (!valuesafe) {
+                        final Transform transform = getTransform(value.getClass(), array.getClass().getComponentType(), definition);
+
+                        if (transform != null) {
+                            value = transform.method.handle.invoke(value);
+                        }
+                    }
+
+                    Array.set(array, (int)index, value);
+                } catch (final Throwable throwable) {
+                    throw new IllegalArgumentException("Error storing value [" + value + "] " +
+                            "in array class [" + array.getClass().getCanonicalName() + "].", throwable);
+                }
+            } else if (array instanceof List) {
+                ((List)array).add((int)index, value);
+            } else {
+                throw new IllegalArgumentException("Attempting to address a non-array type " +
+                        "[" + array.getClass().getCanonicalName() + "] as an array.");
+            }
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    public static Object arrayLoad(final Object array, Object index,
+                                   final Definition definition, final boolean indexsafe) {
+        if (array instanceof Map) {
+            return ((Map)array).get(index);
+        } else {
+            try {
+                if (!indexsafe) {
+                    final Transform transform = getTransform(index.getClass(), Integer.class, definition);
+
+                    if (transform != null) {
+                        index = transform.method.handle.invoke(index);
+                    }
+                }
+            } catch (final Throwable throwable) {
+                throw new IllegalArgumentException(
+                        "Error loading value using index [" + index + "].", throwable);
+            }
+
+            if (array.getClass().isArray()) {
+                try {
+                    return Array.get(array, (int)index);
+                } catch (final Throwable throwable) {
+                    throw new IllegalArgumentException("Error loading value from " +
+                            "array class [" + array.getClass().getCanonicalName() + "].", throwable);
+                }
+            } else if (array instanceof List) {
+                return ((List)array).get((int)index);
+            } else {
+                throw new IllegalArgumentException("Attempting to address a non-array type " +
+                        "[" + array.getClass().getCanonicalName() + "] as an array.");
+            }
+        }
+    }
+
+    public static Method getMethod(final Object owner, final String name, final Definition definition) {
+        Struct struct = null;
+        Class<?> clazz = owner.getClass();
+        Method method = null;
+
+        while (clazz != null) {
+            struct = definition.classes.get(clazz);
+
+            if (struct != null) {
+                method = struct.methods.get(name);
+
+                if (method != null) {
+                    break;
+                }
+            }
+
+            for (final Class iface : clazz.getInterfaces()) {
+                struct = definition.classes.get(iface);
+
+                if (struct != null) {
+                    method = struct.methods.get(name);
+
+                    if (method != null) {
+                        break;
+                    }
+                }
+            }
+
+            if (struct != null) {
+                method = struct.methods.get(name);
+
+                if (method != null) {
+                    break;
+                }
+            }
+
+            clazz = clazz.getSuperclass();
+        }
+
+        if (struct == null) {
+            throw new IllegalArgumentException("Unable to find a dynamic struct for class [" + owner.getClass() + "].");
+        }
+
+        return method;
+    }
+
+    public static Field getField(final Object owner, final String name, final Definition definition) {
+        Struct struct = null;
+        Class<?> clazz = owner.getClass();
+        Field field = null;
+
+        while (clazz != null) {
+            struct = definition.classes.get(clazz);
+
+            if (struct != null) {
+                field = struct.members.get(name);
+
+                if (field != null) {
+                    break;
+                }
+            }
+
+            for (final Class iface : clazz.getInterfaces()) {
+                struct = definition.classes.get(iface);
+
+                if (struct != null) {
+                    field = struct.members.get(name);
+
+                    if (field != null) {
+                        break;
+                    }
+                }
+            }
+
+            if (struct != null) {
+                field = struct.members.get(name);
+
+                if (field != null) {
+                    break;
+                }
+            }
+
+            clazz = clazz.getSuperclass();
+        }
+
+        if (struct == null) {
+            throw new IllegalArgumentException("Unable to find a dynamic struct for class [" + owner.getClass() + "].");
+        }
+
+        return field;
+    }
+
+    public static Transform getTransform(Class<?> fromClass, Class<?> toClass, final Definition definition) {
+        Struct fromStruct = null;
+        Struct toStruct = null;
+
+        if (fromClass.equals(toClass)) {
+            return null;
+        }
+
+        while (fromClass != null) {
+            fromStruct = definition.classes.get(fromClass);
+
+            if (fromStruct != null) {
+                break;
+            }
+
+            for (final Class iface : fromClass.getInterfaces()) {
+                fromStruct = definition.classes.get(iface);
+
+                if (fromStruct != null) {
+                    break;
+                }
+            }
+
+            if (fromStruct != null) {
+                break;
+            }
+
+            fromClass = fromClass.getSuperclass();
+        }
+
+        if (fromStruct != null) {
+            while (toClass != null) {
+                toStruct = definition.classes.get(toClass);
+
+                if (toStruct != null) {
+                    break;
+                }
+
+                for (final Class iface : toClass.getInterfaces()) {
+                    toStruct = definition.classes.get(iface);
+
+                    if (toStruct != null) {
+                        break;
+                    }
+                }
+
+                if (toStruct != null) {
+                    break;
+                }
+
+                toClass = toClass.getSuperclass();
+            }
+        }
+
+        if (toStruct != null) {
+            final Type fromType = definition.getType(fromStruct.name);
+            final Type toType = definition.getType(toStruct.name);
+            final Cast cast = new Cast(fromType, toType);
+
+            return definition.transforms.get(cast);
+        }
+
+        return null;
+    }
+
+    public static Object not(final Object unary) {
+        if (unary instanceof Double || unary instanceof Float || unary instanceof Long) {
+            return ~((Number)unary).longValue();
+        } else if (unary instanceof Number) {
+            return ~((Number)unary).intValue();
+        } else if (unary instanceof Character) {
+            return ~(int)(char)unary;
+        }
+
+        throw new ClassCastException("Cannot apply [~] operation to type " +
+                "[" + unary.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object neg(final Object unary) {
+        if (unary instanceof Double) {
+            return -(double)unary;
+        } else if (unary instanceof Float) {
+            return -(float)unary;
+        } else if (unary instanceof Long) {
+            return -(long)unary;
+        } else if (unary instanceof Number) {
+            return -((Number)unary).intValue();
+        } else if (unary instanceof Character) {
+            return -(char)unary;
+        }
+
+        throw new ClassCastException("Cannot apply [-] operation to type " +
+                "[" + unary.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object mul(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() * ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() * ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() * ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() * ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() * (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() * (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() * (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() * (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left * ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left * ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left * ((Number)right).longValue();
+                } else {
+                    return (int)(char)left * ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left * (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [*] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object div(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() / ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() / ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() / ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() / ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() / (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() / (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() / (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() / (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left / ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left / ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left / ((Number)right).longValue();
+                } else {
+                    return (int)(char)left / ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left / (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [/] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object rem(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() % ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() % ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() % ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() % ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() % (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() % (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() % (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() % (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left % ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left % ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left % ((Number)right).longValue();
+                } else {
+                    return (int)(char)left % ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left % (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [%] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+    
+    public static Object add(final Object left, final Object right) {
+        if (left instanceof String || right instanceof String) {
+            return "" + left + right;
+        } else if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() + ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() + ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() + ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() + ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() + (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() + (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() + (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() + (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left + ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left + ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left + ((Number)right).longValue();
+                } else {
+                    return (int)(char)left + ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left + (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [+] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object sub(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() - ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() - ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() - ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() - ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() - (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() - (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() - (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() - (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left - ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left - ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left - ((Number)right).longValue();
+                } else {
+                    return (int)(char)left - ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left - (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [-] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object lsh(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() << ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() << ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() << (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() << (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left << ((Number)right).longValue();
+                } else {
+                    return (int)(char)left << ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left << (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [<<] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object rsh(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() >> ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() >> ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() >> (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() >> (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left >> ((Number)right).longValue();
+                } else {
+                    return (int)(char)left >> ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left >> (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [>>] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object ush(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() >>> ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() >>> ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() >>> (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() >>> (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left >>> ((Number)right).longValue();
+                } else {
+                    return (int)(char)left >>> ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left >>> (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [>>>] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+    
+    public static Object and(final Object left, final Object right) {
+        if (left instanceof Boolean && right instanceof Boolean) {
+            return (boolean)left && (boolean)right;
+        } else if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() & ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() & ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() & (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() & (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left & ((Number)right).longValue();
+                } else {
+                    return (int)(char)left & ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left & (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [&] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object xor(final Object left, final Object right) {
+        if (left instanceof Boolean && right instanceof Boolean) {
+            return (boolean)left ^ (boolean)right;
+        } else if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() ^ ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() ^ ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() ^ (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() ^ (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left ^ ((Number)right).longValue();
+                } else {
+                    return (int)(char)left ^ ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left ^ (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [^] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static Object or(final Object left, final Object right) {
+        if (left instanceof Boolean && right instanceof Boolean) {
+            return (boolean)left || (boolean)right;
+        } else if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double ||
+                        left instanceof Float || right instanceof Float ||
+                        left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() | ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() | ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double || left instanceof Float || left instanceof Long) {
+                    return ((Number)left).longValue() | (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() | (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double || right instanceof Float || right instanceof Long) {
+                    return (long)(char)left | ((Number)right).longValue();
+                } else {
+                    return (int)(char)left | ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left | (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [|] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean eq(final Object left, final Object right) {
+        if (left != null && right != null) {
+            if (left instanceof Double) {
+                if (right instanceof Number) {
+                    return (double)left == ((Number)right).doubleValue();
+                } else if (right instanceof Character) {
+                    return (double)left == (double)(char)right;
+                }
+            } else if (right instanceof Double) {
+                if (left instanceof Number) {
+                    return ((Number)left).doubleValue() == (double)right;
+                } else if (left instanceof Character) {
+                    return (double)(char)left == ((Number)right).doubleValue();
+                }
+            } else if (left instanceof Float) {
+                if (right instanceof Number) {
+                    return (float)left == ((Number)right).floatValue();
+                } else if (right instanceof Character) {
+                    return (float)left == (float)(char)right;
+                }
+            } else if (right instanceof Float) {
+                if (left instanceof Number) {
+                    return ((Number)left).floatValue() == (float)right;
+                } else if (left instanceof Character) {
+                    return (float)(char)left == ((Number)right).floatValue();
+                }
+            } else if (left instanceof Long) {
+                if (right instanceof Number) {
+                    return (long)left == ((Number)right).longValue();
+                } else if (right instanceof Character) {
+                    return (long)left == (long)(char)right;
+                }
+            } else if (right instanceof Long) {
+                if (left instanceof Number) {
+                    return ((Number)left).longValue() == (long)right;
+                } else if (left instanceof Character) {
+                    return (long)(char)left == ((Number)right).longValue();
+                }
+            } else if (left instanceof Number) {
+                if (right instanceof Number) {
+                    return ((Number)left).intValue() == ((Number)right).intValue();
+                } else if (right instanceof Character) {
+                    return ((Number)left).intValue() == (int)(char)right;
+                }
+            } else if (right instanceof Number && left instanceof Character) {
+                return (int)(char)left == ((Number)right).intValue();
+            } else if (left instanceof Character && right instanceof Character) {
+                return (int)(char)left == (int)(char)right;
+            }
+
+            return left.equals(right);
+        }
+
+        return left == null && right == null;
+    }
+
+    public static boolean lt(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() < ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() < ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() < ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() < ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() < (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() < (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() < (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() < (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left < ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left < ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left < ((Number)right).longValue();
+                } else {
+                    return (int)(char)left < ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left < (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [<] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean lte(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() <= ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() <= ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() <= ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() <= ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() <= (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() <= (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() <= (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() <= (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left <= ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left <= ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left <= ((Number)right).longValue();
+                } else {
+                    return (int)(char)left <= ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left <= (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [<=] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean gt(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() > ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() > ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() > ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() > ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() > (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() > (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() > (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() > (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left > ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left > ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left > ((Number)right).longValue();
+                } else {
+                    return (int)(char)left > ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left > (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [>] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean gte(final Object left, final Object right) {
+        if (left instanceof Number) {
+            if (right instanceof Number) {
+                if (left instanceof Double || right instanceof Double) {
+                    return ((Number)left).doubleValue() >= ((Number)right).doubleValue();
+                } else if (left instanceof Float || right instanceof Float) {
+                    return ((Number)left).floatValue() >= ((Number)right).floatValue();
+                } else if (left instanceof Long || right instanceof Long) {
+                    return ((Number)left).longValue() >= ((Number)right).longValue();
+                } else {
+                    return ((Number)left).intValue() >= ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                if (left instanceof Double) {
+                    return ((Number)left).doubleValue() >= (double)(char)right;
+                } else if (left instanceof Float) {
+                    return ((Number)left).floatValue() >= (float)(char)right;
+                } else if (left instanceof Long) {
+                    return ((Number)left).longValue() >= (long)(char)right;
+                } else {
+                    return ((Number)left).intValue() >= (int)(char)right;
+                }
+            }
+        } else if (left instanceof Character) {
+            if (right instanceof Number) {
+                if (right instanceof Double) {
+                    return (double)(char)left >= ((Number)right).doubleValue();
+                } else if (right instanceof Float) {
+                    return (float)(char)left >= ((Number)right).floatValue();
+                } else if (right instanceof Long) {
+                    return (long)(char)left >= ((Number)right).longValue();
+                } else {
+                    return (int)(char)left >= ((Number)right).intValue();
+                }
+            } else if (right instanceof Character) {
+                return (int)(char)left >= (int)(char)right;
+            }
+        }
+
+        throw new ClassCastException("Cannot apply [>] operation to types " +
+                "[" + left.getClass().getCanonicalName() + "] and [" + right.getClass().getCanonicalName() + "].");
+    }
+
+    public static boolean DefToboolean(final Object value) {
+        if (value instanceof Boolean) {
+            return (boolean)value;
+        } else if (value instanceof Character) {
+            return ((char)value) != 0;
+        } else {
+            return ((Number)value).intValue() != 0;
+        }
+    }
+
+    public static byte DefTobyte(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (byte)1 : 0;
+        } else if (value instanceof Character) {
+            return (byte)(char)value;
+        } else {
+            return ((Number)value).byteValue();
+        }
+    }
+
+    public static short DefToshort(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (short)1 : 0;
+        } else if (value instanceof Character) {
+            return (short)(char)value;
+        } else {
+            return ((Number)value).shortValue();
+        }
+    }
+
+    public static char DefTochar(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (char)1 : 0;
+        } else if (value instanceof Character) {
+            return ((Character)value);
+        } else {
+            return (char)((Number)value).intValue();
+        }
+    }
+
+    public static int DefToint(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? 1 : 0;
+        } else if (value instanceof Character) {
+            return (int)(char)value;
+        } else {
+            return ((Number)value).intValue();
+        }
+    }
+
+    public static long DefTolong(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? 1L : 0;
+        } else if (value instanceof Character) {
+            return (long)(char)value;
+        } else {
+            return ((Number)value).longValue();
+        }
+    }
+
+    public static float DefTofloat(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (float)1 : 0;
+        } else if (value instanceof Character) {
+            return (float)(char)value;
+        } else {
+            return ((Number)value).floatValue();
+        }
+    }
+
+    public static double DefTodouble(final Object value) {
+        if (value instanceof Boolean) {
+            return ((Boolean)value) ? (double)1 : 0;
+        } else if (value instanceof Character) {
+            return (double)(char)value;
+        } else {
+            return ((Number)value).doubleValue();
+        }
+    }
+
+    public static Boolean DefToBoolean(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return (boolean)value;
+        } else if (value instanceof Character) {
+            return ((char)value) != 0;
+        } else {
+            return ((Number)value).intValue() != 0;
+        }
+    }
+
+    public static Byte DefToByte(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (byte)1 : 0;
+        } else if (value instanceof Character) {
+            return (byte)(char)value;
+        } else {
+            return ((Number)value).byteValue();
+        }
+    }
+
+    public static Short DefToShort(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (short)1 : 0;
+        } else if (value instanceof Character) {
+            return (short)(char)value;
+        } else {
+            return ((Number)value).shortValue();
+        }
+    }
+
+    public static Character DefToCharacter(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (char)1 : 0;
+        } else if (value instanceof Character) {
+            return ((Character)value);
+        } else {
+            return (char)((Number)value).intValue();
+        }
+    }
+
+    public static Integer DefToInteger(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? 1 : 0;
+        } else if (value instanceof Character) {
+            return (int)(char)value;
+        } else {
+            return ((Number)value).intValue();
+        }
+    }
+
+    public static Long DefToLong(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? 1L : 0;
+        } else if (value instanceof Character) {
+            return (long)(char)value;
+        } else {
+            return ((Number)value).longValue();
+        }
+    }
+
+    public static Float DefToFloat(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (float)1 : 0;
+        } else if (value instanceof Character) {
+            return (float)(char)value;
+        } else {
+            return ((Number)value).floatValue();
+        }
+    }
+
+    public static Double DefToDouble(final Object value) {
+        if (value == null) {
+            return null;
+        } else if (value instanceof Boolean) {
+            return ((Boolean)value) ? (double)1 : 0;
+        } else if (value instanceof Character) {
+            return (double)(char)value;
+        } else {
+            return ((Number)value).doubleValue();
+        }
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Definition.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Definition.java
new file mode 100644
index 0000000..5c52a20
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Definition.java
@@ -0,0 +1,1809 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.lang.invoke.MethodHandle;
+import java.lang.invoke.MethodHandles;
+import java.lang.invoke.MethodType;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+class Definition {
+    enum Sort {
+        VOID(       void.class      , 0 , true  , false , false , false ),
+        BOOL(       boolean.class   , 1 , true  , true  , false , true  ),
+        BYTE(       byte.class      , 1 , true  , false , true  , true  ),
+        SHORT(      short.class     , 1 , true  , false , true  , true  ),
+        CHAR(       char.class      , 1 , true  , false , true  , true  ),
+        INT(        int.class       , 1 , true  , false , true  , true  ),
+        LONG(       long.class      , 2 , true  , false , true  , true  ),
+        FLOAT(      float.class     , 1 , true  , false , true  , true  ),
+        DOUBLE(     double.class    , 2 , true  , false , true  , true  ),
+
+        VOID_OBJ(   Void.class      , 1 , true  , false , false , false ),
+        BOOL_OBJ(   Boolean.class   , 1 , false , true  , false , false ),
+        BYTE_OBJ(   Byte.class      , 1 , false , false , true  , false ),
+        SHORT_OBJ(  Short.class     , 1 , false , false , true  , false ),
+        CHAR_OBJ(   Character.class , 1 , false , false , true  , false ),
+        INT_OBJ(    Integer.class   , 1 , false , false , true  , false ),
+        LONG_OBJ(   Long.class      , 1 , false , false , true  , false ),
+        FLOAT_OBJ(  Float.class     , 1 , false , false , true  , false ),
+        DOUBLE_OBJ( Double.class    , 1 , false , false , true  , false ),
+
+        NUMBER(     Number.class    , 1 , false , false , true  , false ),
+        STRING(     String.class    , 1 , false , false , false , true  ),
+
+        OBJECT(     null            , 1 , false , false , false , false ),
+        DEF(        null            , 1 , false , false , false , false ),
+        ARRAY(      null            , 1 , false , false , false , false );
+
+        final Class<?> clazz;
+        final int size;
+        final boolean primitive;
+        final boolean bool;
+        final boolean numeric;
+        final boolean constant;
+
+        Sort(final Class<?> clazz, final int size, final boolean primitive,
+             final boolean bool, final boolean numeric, final boolean constant) {
+            this.clazz = clazz;
+            this.size = size;
+            this.bool = bool;
+            this.primitive = primitive;
+            this.numeric = numeric;
+            this.constant = constant;
+        }
+    }
+
+    static class Type {
+        final String name;
+        final Struct struct;
+        final Class<?> clazz;
+        final org.objectweb.asm.Type type;
+        final Sort sort;
+
+        private Type(final String name, final Struct struct, final Class<?> clazz,
+                     final org.objectweb.asm.Type type, final Sort sort) {
+            this.name = name;
+            this.struct = struct;
+            this.clazz = clazz;
+            this.type = type;
+            this.sort = sort;
+        }
+
+        @Override
+        public boolean equals(final Object object) {
+            if (this == object) {
+                return true;
+            }
+
+            if (object == null || getClass() != object.getClass()) {
+                return false;
+            }
+
+            final Type type = (Type)object;
+
+            return this.type.equals(type.type) && struct.equals(type.struct);
+        }
+
+        @Override
+        public int hashCode() {
+            int result = struct.hashCode();
+            result = 31 * result + type.hashCode();
+
+            return result;
+        }
+    }
+
+    static class Constructor {
+        final String name;
+        final Struct owner;
+        final List<Type> arguments;
+        final org.objectweb.asm.commons.Method method;
+        final java.lang.reflect.Constructor<?> reflect;
+
+        private Constructor(final String name, final Struct owner, final List<Type> arguments,
+                            final org.objectweb.asm.commons.Method method, final java.lang.reflect.Constructor<?> reflect) {
+            this.name = name;
+            this.owner = owner;
+            this.arguments = Collections.unmodifiableList(arguments);
+            this.method = method;
+            this.reflect = reflect;
+        }
+    }
+
+    static class Method {
+        final String name;
+        final Struct owner;
+        final Type rtn;
+        final List<Type> arguments;
+        final org.objectweb.asm.commons.Method method;
+        final java.lang.reflect.Method reflect;
+        final MethodHandle handle;
+
+        private Method(final String name, final Struct owner, final Type rtn, final List<Type> arguments,
+                       final org.objectweb.asm.commons.Method method, final java.lang.reflect.Method reflect,
+                       final MethodHandle handle) {
+            this.name = name;
+            this.owner = owner;
+            this.rtn = rtn;
+            this.arguments = Collections.unmodifiableList(arguments);
+            this.method = method;
+            this.reflect = reflect;
+            this.handle = handle;
+        }
+    }
+
+    static class Field {
+        final String name;
+        final Struct owner;
+        final Type generic;
+        final Type type;
+        final java.lang.reflect.Field reflect;
+        final MethodHandle getter;
+        final MethodHandle setter;
+
+        private Field(final String name, final Struct owner, final Type generic, final Type type,
+                      final java.lang.reflect.Field reflect, final MethodHandle getter, final MethodHandle setter) {
+            this.name = name;
+            this.owner = owner;
+            this.generic = generic;
+            this.type = type;
+            this.reflect = reflect;
+            this.getter = getter;
+            this.setter = setter;
+        }
+    }
+
+    static class Struct {
+        final String name;
+        final Class<?> clazz;
+        final org.objectweb.asm.Type type;
+
+        final Map<String, Constructor> constructors;
+        final Map<String, Method> functions;
+        final Map<String, Method> methods;
+
+        final Map<String, Field> statics;
+        final Map<String, Field> members;
+
+        private Struct(final String name, final Class<?> clazz, final org.objectweb.asm.Type type) {
+            this.name = name;
+            this.clazz = clazz;
+            this.type = type;
+
+            constructors = new HashMap<>();
+            functions = new HashMap<>();
+            methods = new HashMap<>();
+
+            statics = new HashMap<>();
+            members = new HashMap<>();
+        }
+
+        private Struct(final Struct struct) {
+            name = struct.name;
+            clazz = struct.clazz;
+            type = struct.type;
+
+            constructors = Collections.unmodifiableMap(struct.constructors);
+            functions = Collections.unmodifiableMap(struct.functions);
+            methods = Collections.unmodifiableMap(struct.methods);
+
+            statics = Collections.unmodifiableMap(struct.statics);
+            members = Collections.unmodifiableMap(struct.members);
+        }
+
+        @Override
+        public boolean equals(Object object) {
+            if (this == object) {
+                return true;
+            }
+
+            if (object == null || getClass() != object.getClass()) {
+                return false;
+            }
+
+            Struct struct = (Struct)object;
+
+            return name.equals(struct.name);
+        }
+
+        @Override
+        public int hashCode() {
+            return name.hashCode();
+        }
+    }
+
+    static class Pair {
+        final Type type0;
+        final Type type1;
+
+        Pair(final Type type0, final Type type1) {
+            this.type0 = type0;
+            this.type1 = type1;
+        }
+
+        @Override
+        public boolean equals(final Object object) {
+            if (this == object) {
+                return true;
+            }
+
+            if (object == null || getClass() != object.getClass()) {
+                return false;
+            }
+
+            final Pair pair = (Pair)object;
+
+            return type0.equals(pair.type0) && type1.equals(pair.type1);
+        }
+
+        @Override
+        public int hashCode() {
+            int result = type0.hashCode();
+            result = 31 * result + type1.hashCode();
+
+            return result;
+        }
+    }
+
+    static class Cast {
+        final Type from;
+        final Type to;
+
+        Cast(final Type from, final Type to) {
+            this.from = from;
+            this.to = to;
+        }
+
+        @Override
+        public boolean equals(final Object object) {
+            if (this == object) {
+                return true;
+            }
+
+            if (object == null || getClass() != object.getClass()) {
+                return false;
+            }
+
+            final Cast cast = (Cast)object;
+
+            return from.equals(cast.from) && to.equals(cast.to);
+        }
+
+        @Override
+        public int hashCode() {
+            int result = from.hashCode();
+            result = 31 * result + to.hashCode();
+
+            return result;
+        }
+    }
+
+    static class Transform extends Cast {
+        final Cast cast;
+        final Method method;
+        final Type upcast;
+        final Type downcast;
+
+        private Transform(final Cast cast, Method method, final Type upcast, final Type downcast) {
+            super(cast.from, cast.to);
+
+            this.cast = cast;
+            this.method = method;
+            this.upcast = upcast;
+            this.downcast = downcast;
+        }
+    }
+
+    final Map<String, Struct> structs;
+    final Map<Class<?>, Struct> classes;
+    final Map<Cast, Transform> transforms;
+    final Map<Pair, Type> bounds;
+
+    final Type voidType;
+    final Type booleanType;
+    final Type byteType;
+    final Type shortType;
+    final Type charType;
+    final Type intType;
+    final Type longType;
+    final Type floatType;
+    final Type doubleType;
+
+    final Type voidobjType;
+    final Type booleanobjType;
+    final Type byteobjType;
+    final Type shortobjType;
+    final Type charobjType;
+    final Type intobjType;
+    final Type longobjType;
+    final Type floatobjType;
+    final Type doubleobjType;
+
+    final Type objectType;
+    final Type defType;
+    final Type numberType;
+    final Type charseqType;
+    final Type stringType;
+    final Type mathType;
+    final Type utilityType;
+    final Type defobjType;
+
+    final Type listType;
+    final Type arraylistType;
+    final Type mapType;
+    final Type hashmapType;
+
+    final Type olistType;
+    final Type oarraylistType;
+    final Type omapType;
+    final Type ohashmapType;
+
+    final Type smapType;
+    final Type shashmapType;
+    final Type somapType;
+    final Type sohashmapType;
+
+    final Type execType;
+
+    public Definition() {
+        structs = new HashMap<>();
+        classes = new HashMap<>();
+        transforms = new HashMap<>();
+        bounds = new HashMap<>();
+
+        addDefaultStructs();
+        addDefaultClasses();
+
+        voidType = getType("void");
+        booleanType = getType("boolean");
+        byteType = getType("byte");
+        shortType = getType("short");
+        charType = getType("char");
+        intType = getType("int");
+        longType = getType("long");
+        floatType = getType("float");
+        doubleType = getType("double");
+
+        voidobjType = getType("Void");
+        booleanobjType = getType("Boolean");
+        byteobjType = getType("Byte");
+        shortobjType = getType("Short");
+        charobjType = getType("Character");
+        intobjType = getType("Integer");
+        longobjType = getType("Long");
+        floatobjType = getType("Float");
+        doubleobjType = getType("Double");
+
+        objectType = getType("Object");
+        defType = getType("def");
+        numberType = getType("Number");
+        charseqType = getType("CharSequence");
+        stringType = getType("String");
+        mathType = getType("Math");
+        utilityType = getType("Utility");
+        defobjType = getType("Def");
+
+        listType = getType("List");
+        arraylistType = getType("ArrayList");
+        mapType = getType("Map");
+        hashmapType = getType("HashMap");
+
+        olistType = getType("List<Object>");
+        oarraylistType = getType("ArrayList<Object>");
+        omapType = getType("Map<Object,Object>");
+        ohashmapType = getType("HashMap<Object,Object>");
+
+        smapType = getType("Map<String,def>");
+        shashmapType = getType("HashMap<String,def>");
+        somapType = getType("Map<String,Object>");
+        sohashmapType = getType("HashMap<String,Object>");
+
+        execType = getType("Executable");
+
+        addDefaultElements();
+        copyDefaultStructs();
+        addDefaultTransforms();
+        addDefaultBounds();
+    }
+
+    Definition(final Definition definition) {
+        final Map<String, Struct> structs = new HashMap<>();
+
+        for (final Struct struct : definition.structs.values()) {
+            structs.put(struct.name, new Struct(struct));
+        }
+
+        this.structs = Collections.unmodifiableMap(structs);
+
+        final Map<Class<?>, Struct> classes = new HashMap<>();
+
+        for (final Struct struct : definition.classes.values()) {
+            classes.put(struct.clazz, this.structs.get(struct.name));
+        }
+
+        this.classes = Collections.unmodifiableMap(classes);
+
+        transforms = Collections.unmodifiableMap(definition.transforms);
+        bounds = Collections.unmodifiableMap(definition.bounds);
+
+        voidType = definition.voidType;
+        booleanType = definition.booleanType;
+        byteType = definition.byteType;
+        shortType = definition.shortType;
+        charType = definition.charType;
+        intType = definition.intType;
+        longType = definition.longType;
+        floatType = definition.floatType;
+        doubleType = definition.doubleType;
+
+        voidobjType = definition.voidobjType;
+        booleanobjType = definition.booleanobjType;
+        byteobjType = definition.byteobjType;
+        shortobjType = definition.shortobjType;
+        charobjType = definition.charobjType;
+        intobjType = definition.intobjType;
+        longobjType = definition.longobjType;
+        floatobjType = definition.floatobjType;
+        doubleobjType = definition.doubleobjType;
+
+        objectType = definition.objectType;
+        defType = definition.defType;
+        numberType = definition.numberType;
+        charseqType = definition.charseqType;
+        stringType = definition.stringType;
+        mathType = definition.mathType;
+        utilityType = definition.utilityType;
+        defobjType = definition.defobjType;
+
+        listType = definition.listType;
+        arraylistType = definition.arraylistType;
+        mapType = definition.mapType;
+        hashmapType = definition.hashmapType;
+
+        olistType = definition.olistType;
+        oarraylistType = definition.oarraylistType;
+        omapType = definition.omapType;
+        ohashmapType = definition.ohashmapType;
+
+        smapType = definition.smapType;
+        shashmapType = definition.shashmapType;
+        somapType = definition.somapType;
+        sohashmapType = definition.sohashmapType;
+
+        execType = definition.execType;
+    }
+
+    private void addDefaultStructs() {
+        addStruct( "void"    , void.class    );
+        addStruct( "boolean" , boolean.class );
+        addStruct( "byte"    , byte.class    );
+        addStruct( "short"   , short.class   );
+        addStruct( "char"    , char.class    );
+        addStruct( "int"     , int.class     );
+        addStruct( "long"    , long.class    );
+        addStruct( "float"   , float.class   );
+        addStruct( "double"  , double.class  );
+
+        addStruct( "Void"      , Void.class      );
+        addStruct( "Boolean"   , Boolean.class   );
+        addStruct( "Byte"      , Byte.class      );
+        addStruct( "Short"     , Short.class     );
+        addStruct( "Character" , Character.class );
+        addStruct( "Integer"   , Integer.class   );
+        addStruct( "Long"      , Long.class      );
+        addStruct( "Float"     , Float.class     );
+        addStruct( "Double"    , Double.class    );
+
+        addStruct( "Object"       , Object.class       );
+        addStruct( "def"          , Object.class       );
+        addStruct( "Number"       , Number.class       );
+        addStruct( "CharSequence" , CharSequence.class );
+        addStruct( "String"       , String.class       );
+        addStruct( "Math"         , Math.class         );
+        addStruct( "Utility"      , Utility.class      );
+        addStruct( "Def"          , Def.class          );
+
+        addStruct( "List"      , List.class      );
+        addStruct( "ArrayList" , ArrayList.class );
+        addStruct( "Map"       , Map.class       );
+        addStruct( "HashMap"   , HashMap.class   );
+
+        addStruct( "List<Object>"           , List.class      );
+        addStruct( "ArrayList<Object>"      , ArrayList.class );
+        addStruct( "Map<Object,Object>"     , Map.class       );
+        addStruct( "HashMap<Object,Object>" , HashMap.class   );
+
+        addStruct( "Map<String,def>"        , Map.class       );
+        addStruct( "HashMap<String,def>"    , HashMap.class   );
+        addStruct( "Map<String,Object>"     , Map.class       );
+        addStruct( "HashMap<String,Object>" , HashMap.class   );
+
+        addStruct( "Executable" , Executable.class );
+    }
+
+    private void addDefaultClasses() {
+        addClass("boolean");
+        addClass("byte");
+        addClass("short");
+        addClass("char");
+        addClass("int");
+        addClass("long");
+        addClass("float");
+        addClass("double");
+
+        addClass("Boolean");
+        addClass("Byte");
+        addClass("Short");
+        addClass("Character");
+        addClass("Integer");
+        addClass("Long");
+        addClass("Float");
+        addClass("Double");
+
+        addClass("Object");
+        addClass("Number");
+        addClass("CharSequence");
+        addClass("String");
+
+        addClass("List");
+        addClass("ArrayList");
+        addClass("Map");
+        addClass("HashMap");
+    }
+
+    private void addDefaultElements() {
+        addMethod("Object", "toString", null, false, stringType, new Type[] {}, null, null);
+        addMethod("Object", "equals", null, false, booleanType, new Type[] {objectType}, null, null);
+        addMethod("Object", "hashCode", null, false, intType, new Type[] {}, null, null);
+
+        addMethod("def", "toString", null, false, stringType, new Type[] {}, null, null);
+        addMethod("def", "equals", null, false, booleanType, new Type[] {objectType}, null, null);
+        addMethod("def", "hashCode", null, false, intType, new Type[] {}, null, null);
+
+        addConstructor("Boolean", "new", new Type[] {booleanType}, null);
+        addMethod("Boolean", "valueOf", null, true, booleanobjType, new Type[] {booleanType}, null, null);
+        addMethod("Boolean", "booleanValue", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("Byte", "new", new Type[]{byteType}, null);
+        addMethod("Byte", "valueOf", null, true, byteobjType, new Type[] {byteType}, null, null);
+        addMethod("Byte", "byteValue", null, false, byteType, new Type[] {}, null, null);
+        addField("Byte", "MIN_VALUE", null, true, byteType, null);
+        addField("Byte", "MAX_VALUE", null, true, byteType, null);
+
+        addConstructor("Short", "new", new Type[]{shortType}, null);
+        addMethod("Short", "valueOf", null, true, shortobjType, new Type[] {shortType}, null, null);
+        addMethod("Short", "shortValue", null, false, shortType, new Type[] {}, null, null);
+        addField("Short", "MIN_VALUE", null, true, shortType, null);
+        addField("Short", "MAX_VALUE", null, true, shortType, null);
+
+        addConstructor("Character", "new", new Type[]{charType}, null);
+        addMethod("Character", "valueOf", null, true, charobjType, new Type[] {charType}, null, null);
+        addMethod("Character", "charValue", null, false, charType, new Type[] {}, null, null);
+        addField("Character", "MIN_VALUE", null, true, charType, null);
+        addField("Character", "MAX_VALUE", null, true, charType, null);
+
+        addConstructor("Integer", "new", new Type[]{intType}, null);
+        addMethod("Integer", "valueOf", null, true, intobjType, new Type[] {intType}, null, null);
+        addMethod("Integer", "intValue", null, false, intType, new Type[] {}, null, null);
+        addField("Integer", "MIN_VALUE", null, true, intType, null);
+        addField("Integer", "MAX_VALUE", null, true, intType, null);
+
+        addConstructor("Long", "new", new Type[]{longType}, null);
+        addMethod("Long", "valueOf", null, true, longobjType, new Type[] {longType}, null, null);
+        addMethod("Long", "longValue", null, false, longType, new Type[] {}, null, null);
+        addField("Long", "MIN_VALUE", null, true, longType, null);
+        addField("Long", "MAX_VALUE", null, true, longType, null);
+
+        addConstructor("Float", "new", new Type[]{floatType}, null);
+        addMethod("Float", "valueOf", null, true, floatobjType, new Type[] {floatType}, null, null);
+        addMethod("Float", "floatValue", null, false, floatType, new Type[] {}, null, null);
+        addField("Float", "MIN_VALUE", null, true, floatType, null);
+        addField("Float", "MAX_VALUE", null, true, floatType, null);
+
+        addConstructor("Double", "new", new Type[]{doubleType}, null);
+        addMethod("Double", "valueOf", null, true, doubleobjType, new Type[] {doubleType}, null, null);
+        addMethod("Double", "doubleValue", null, false, doubleType, new Type[] {}, null, null);
+        addField("Double", "MIN_VALUE", null, true, doubleType, null);
+        addField("Double", "MAX_VALUE", null, true, doubleType, null);
+
+        addMethod("Number", "byteValue", null, false, byteType, new Type[] {}, null, null);
+        addMethod("Number", "shortValue", null, false, shortType, new Type[] {}, null, null);
+        addMethod("Number", "intValue", null, false, intType, new Type[] {}, null, null);
+        addMethod("Number", "longValue", null, false, longType, new Type[] {}, null, null);
+        addMethod("Number", "floatValue", null, false, floatType, new Type[] {}, null, null);
+        addMethod("Number", "doubleValue", null, false, doubleType, new Type[] {}, null, null);
+
+        addMethod("CharSequence", "charAt", null, false, charType, new Type[] {intType}, null, null);
+        addMethod("CharSequence", "length", null, false, intType, new Type[] {}, null, null);
+
+        addConstructor("String", "new", new Type[] {}, null);
+        addMethod("String", "codePointAt", null, false, intType, new Type[] {intType}, null, null);
+        addMethod("String", "compareTo", null, false, intType, new Type[] {stringType}, null, null);
+        addMethod("String", "concat", null, false, stringType, new Type[] {stringType}, null, null);
+        addMethod("String", "endsWith", null, false, booleanType, new Type[] {stringType}, null, null);
+        addMethod("String", "indexOf", null, false, intType, new Type[] {stringType, intType}, null, null);
+        addMethod("String", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+        addMethod("String", "replace", null, false, stringType, new Type[] {charseqType, charseqType}, null, null);
+        addMethod("String", "startsWith", null, false, booleanType, new Type[] {stringType}, null, null);
+        addMethod("String", "substring", null, false, stringType, new Type[] {intType, intType}, null, null);
+        addMethod("String", "toCharArray", null, false, getType(charType.struct, 1), new Type[] {}, null, null);
+        addMethod("String", "trim", null, false, stringType, new Type[] {}, null, null);
+
+        addMethod("Utility", "NumberToboolean", null, true, booleanType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberTochar", null, true, charType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToBoolean", null, true, booleanobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToByte", null, true, byteobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToShort", null, true, shortobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToCharacter", null, true, charobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToInteger", null, true, intobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToLong", null, true, longobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToFloat", null, true, floatobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "NumberToDouble", null, true, doubleobjType, new Type[] {numberType}, null, null);
+        addMethod("Utility", "booleanTobyte", null, true, byteType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanToshort", null, true, shortType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanTochar", null, true, charType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanToint", null, true, intType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanTolong", null, true, longType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanTofloat", null, true, floatType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanTodouble", null, true, doubleType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "booleanToInteger", null, true, intobjType, new Type[] {booleanType}, null, null);
+        addMethod("Utility", "BooleanTobyte", null, true, byteType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToshort", null, true, shortType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanTochar", null, true, charType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToint", null, true, intType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanTolong", null, true, longType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanTofloat", null, true, floatType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanTodouble", null, true, doubleType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToByte", null, true, byteobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToShort", null, true, shortobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToCharacter", null, true, charobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToInteger", null, true, intobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToLong", null, true, longobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToFloat", null, true, floatobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "BooleanToDouble", null, true, doubleobjType, new Type[] {booleanobjType}, null, null);
+        addMethod("Utility", "byteToboolean", null, true, booleanType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToShort", null, true, shortobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToCharacter", null, true, charobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToInteger", null, true, intobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToLong", null, true, longobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToFloat", null, true, floatobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "byteToDouble", null, true, doubleobjType, new Type[] {byteType}, null, null);
+        addMethod("Utility", "ByteToboolean", null, true, booleanType, new Type[] {byteobjType}, null, null);
+        addMethod("Utility", "ByteTochar", null, true, charType, new Type[] {byteobjType}, null, null);
+        addMethod("Utility", "shortToboolean", null, true, booleanType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToByte", null, true, byteobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToCharacter", null, true, charobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToInteger", null, true, intobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToLong", null, true, longobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToFloat", null, true, floatobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "shortToDouble", null, true, doubleobjType, new Type[] {shortType}, null, null);
+        addMethod("Utility", "ShortToboolean", null, true, booleanType, new Type[] {shortobjType}, null, null);
+        addMethod("Utility", "ShortTochar", null, true, charType, new Type[] {shortobjType}, null, null);
+        addMethod("Utility", "charToboolean", null, true, booleanType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToByte", null, true, byteobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToShort", null, true, shortobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToInteger", null, true, intobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToLong", null, true, longobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToFloat", null, true, floatobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "charToDouble", null, true, doubleobjType, new Type[] {charType}, null, null);
+        addMethod("Utility", "CharacterToboolean", null, true, booleanType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterTobyte", null, true, byteType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToshort", null, true, shortType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToint", null, true, intType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterTolong", null, true, longType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterTofloat", null, true, floatType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterTodouble", null, true, doubleType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToBoolean", null, true, booleanobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToByte", null, true, byteobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToShort", null, true, shortobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToInteger", null, true, intobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToLong", null, true, longobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToFloat", null, true, floatobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "CharacterToDouble", null, true, doubleobjType, new Type[] {charobjType}, null, null);
+        addMethod("Utility", "intToboolean", null, true, booleanType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToByte", null, true, byteobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToShort", null, true, shortobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToCharacter", null, true, charobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToLong", null, true, longobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToFloat", null, true, floatobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "intToDouble", null, true, doubleobjType, new Type[] {intType}, null, null);
+        addMethod("Utility", "IntegerToboolean", null, true, booleanType, new Type[] {intobjType}, null, null);
+        addMethod("Utility", "IntegerTochar", null, true, charType, new Type[] {intobjType}, null, null);
+        addMethod("Utility", "longToboolean", null, true, booleanType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToByte", null, true, byteobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToShort", null, true, shortobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToCharacter", null, true, charobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToInteger", null, true, intobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToFloat", null, true, floatobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "longToDouble", null, true, doubleobjType, new Type[] {longType}, null, null);
+        addMethod("Utility", "LongToboolean", null, true, booleanType, new Type[] {longobjType}, null, null);
+        addMethod("Utility", "LongTochar", null, true, charType, new Type[] {longobjType}, null, null);
+        addMethod("Utility", "floatToboolean", null, true, booleanType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToByte", null, true, byteobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToShort", null, true, shortobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToCharacter", null, true, charobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToInteger", null, true, intobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToLong", null, true, longobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "floatToDouble", null, true, doubleobjType, new Type[] {floatType}, null, null);
+        addMethod("Utility", "FloatToboolean", null, true, booleanType, new Type[] {floatobjType}, null, null);
+        addMethod("Utility", "FloatTochar", null, true, charType, new Type[] {floatobjType}, null, null);
+        addMethod("Utility", "doubleToboolean", null, true, booleanType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToByte", null, true, byteobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToShort", null, true, shortobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToCharacter", null, true, charobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToInteger", null, true, intobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToLong", null, true, longobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "doubleToFloat", null, true, floatobjType, new Type[] {doubleType}, null, null);
+        addMethod("Utility", "DoubleToboolean", null, true, booleanType, new Type[] {doubleobjType}, null, null);
+        addMethod("Utility", "DoubleTochar", null, true, charType, new Type[] {doubleobjType}, null, null);
+
+        addMethod("Math", "dmax", "max", true, doubleType, new Type[] {doubleType, doubleType}, null, null);
+
+        addMethod("Def", "DefToboolean", null, true, booleanType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTobyte", null, true, byteType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToshort", null, true, shortType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTochar", null, true, charType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToint", null, true, intType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTolong", null, true, longType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTofloat", null, true, floatType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefTodouble", null, true, doubleType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToBoolean", null, true, booleanobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToByte", null, true, byteobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToShort", null, true, shortobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToCharacter", null, true, charobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToInteger", null, true, intobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToLong", null, true, longobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToFloat", null, true, floatobjType, new Type[] {defType}, null, null);
+        addMethod("Def", "DefToDouble", null, true, doubleobjType, new Type[] {defType}, null, null);
+        
+        addMethod("List", "addLast", "add", false, booleanType, new Type[] {objectType}, null, new Type[] {defType});
+        addMethod("List", "add", null, false, voidType, new Type[] {intType, objectType}, null, new Type[] {intType, defType});
+        addMethod("List", "get", null, false, objectType, new Type[] {intType}, defType, null);
+        addMethod("List", "remove", null, false, objectType, new Type[] {intType}, defType, null);
+        addMethod("List", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("List", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("ArrayList", "new", new Type[] {}, null);
+
+        addMethod("Map", "put", null, false, objectType, new Type[] {objectType, objectType}, defType, new Type[] {defType, defType});
+        addMethod("Map", "get", null, false, objectType, new Type[] {objectType}, defType, new Type[] {defType});
+        addMethod("Map", "remove", null, false, objectType, new Type[] {objectType}, null, null);
+        addMethod("Map", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("Map", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("HashMap", "new", new Type[] {}, null);
+
+        addMethod("Map<String,def>", "put", null, false, objectType, new Type[] {objectType, objectType}, defType, new Type[] {stringType, defType});
+        addMethod("Map<String,def>", "get", null, false, objectType, new Type[] {objectType}, defType, new Type[] {stringType});
+        addMethod("Map<String,def>", "remove", null, false, objectType, new Type[] {objectType}, defType, new Type[] {stringType});
+        addMethod("Map<String,def>", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("Map<String,def>", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("HashMap<String,def>", "new", new Type[] {}, null);
+
+        addMethod("List<Object>", "addLast", "add", false, booleanType, new Type[] {objectType}, null, null);
+        addMethod("List<Object>", "add", null, false, voidType, new Type[] {intType, objectType}, null, null);
+        addMethod("List<Object>", "get", null, false, objectType, new Type[] {intType}, null, null);
+        addMethod("List<Object>", "remove", null, false, objectType, new Type[] {intType}, null, null);
+        addMethod("List<Object>", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("List<Object>", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("ArrayList<Object>", "new", new Type[] {}, null);
+
+        addMethod("Map<Object,Object>", "put", null, false, objectType, new Type[] {objectType, objectType}, null, null);
+        addMethod("Map<Object,Object>", "get", null, false, objectType, new Type[] {objectType}, null, null);
+        addMethod("Map<Object,Object>", "remove", null, false, objectType, new Type[] {objectType}, null, null);
+        addMethod("Map<Object,Object>", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("Map<Object,Object>", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("HashMap<Object,Object>", "new", new Type[] {}, null);
+
+        addMethod("Map<String,Object>", "put", null, false, objectType, new Type[] {objectType, objectType}, null, new Type[] {stringType, objectType});
+        addMethod("Map<String,Object>", "get", null, false, objectType, new Type[] {objectType}, null, new Type[] {stringType});
+        addMethod("Map<String,Object>", "remove", null, false, objectType, new Type[] {objectType}, null, new Type[] {stringType});
+        addMethod("Map<String,Object>", "size", null, false, intType, new Type[] {}, null, null);
+        addMethod("Map<String,Object>", "isEmpty", null, false, booleanType, new Type[] {}, null, null);
+
+        addConstructor("HashMap<String,Object>", "new", new Type[] {}, null);
+    }
+
+    private void copyDefaultStructs() {
+        copyStruct("Void", "Object");
+        copyStruct("Boolean", "Object");
+        copyStruct("Byte", "Number", "Object");
+        copyStruct("Short", "Number", "Object");
+        copyStruct("Character", "Object");
+        copyStruct("Integer", "Number", "Object");
+        copyStruct("Long", "Number", "Object");
+        copyStruct("Float", "Number", "Object");
+        copyStruct("Double", "Number", "Object");
+
+        copyStruct("Number", "Object");
+        copyStruct("CharSequence", "Object");
+        copyStruct("String", "CharSequence", "Object");
+
+        copyStruct("List", "Object");
+        copyStruct("ArrayList", "List", "Object");
+        copyStruct("Map", "Object");
+        copyStruct("HashMap", "Map", "Object");
+        copyStruct("Map<String,def>", "Object");
+        copyStruct("HashMap<String,def>", "Map<String,def>", "Object");
+
+        copyStruct("List<Object>", "Object");
+        copyStruct("ArrayList<Object>", "List", "Object");
+        copyStruct("Map<Object,Object>", "Object");
+        copyStruct("HashMap<Object,Object>", "Map<Object,Object>", "Object");
+        copyStruct("Map<String,Object>", "Object");
+        copyStruct("HashMap<String,Object>", "Map<String,Object>", "Object");
+
+        copyStruct("Executable", "Object");
+    }
+
+    private void addDefaultTransforms() {
+        addTransform(booleanType, byteType, "Utility", "booleanTobyte", true);
+        addTransform(booleanType, shortType, "Utility", "booleanToshort", true);
+        addTransform(booleanType, charType, "Utility", "booleanTochar", true);
+        addTransform(booleanType, intType, "Utility", "booleanToint", true);
+        addTransform(booleanType, longType, "Utility", "booleanTolong", true);
+        addTransform(booleanType, floatType, "Utility", "booleanTofloat", true);
+        addTransform(booleanType, doubleType, "Utility", "booleanTodouble", true);
+        addTransform(booleanType, objectType, "Boolean", "valueOf", true);
+        addTransform(booleanType, defType, "Boolean", "valueOf", true);
+        addTransform(booleanType, numberType, "Utility", "booleanToInteger", true);
+        addTransform(booleanType, booleanobjType, "Boolean", "valueOf", true);
+
+        addTransform(byteType, booleanType, "Utility", "byteToboolean", true);
+        addTransform(byteType, objectType, "Byte", "valueOf", true);
+        addTransform(byteType, defType, "Byte", "valueOf", true);
+        addTransform(byteType, numberType, "Byte", "valueOf", true);
+        addTransform(byteType, byteobjType, "Byte", "valueOf", true);
+        addTransform(byteType, shortobjType, "Utility", "byteToShort", true);
+        addTransform(byteType, charobjType, "Utility", "byteToCharacter", true);
+        addTransform(byteType, intobjType, "Utility", "byteToInteger", true);
+        addTransform(byteType, longobjType, "Utility", "byteToLong", true);
+        addTransform(byteType, floatobjType, "Utility", "byteToFloat", true);
+        addTransform(byteType, doubleobjType, "Utility", "byteToDouble", true);
+
+        addTransform(shortType, booleanType, "Utility", "shortToboolean", true);
+        addTransform(shortType, objectType, "Short", "valueOf", true);
+        addTransform(shortType, defType, "Short", "valueOf", true);
+        addTransform(shortType, numberType, "Short", "valueOf", true);
+        addTransform(shortType, byteobjType, "Utility", "shortToByte", true);
+        addTransform(shortType, shortobjType, "Short", "valueOf", true);
+        addTransform(shortType, charobjType, "Utility", "shortToCharacter", true);
+        addTransform(shortType, intobjType, "Utility", "shortToInteger", true);
+        addTransform(shortType, longobjType, "Utility", "shortToLong", true);
+        addTransform(shortType, floatobjType, "Utility", "shortToFloat", true);
+        addTransform(shortType, doubleobjType, "Utility", "shortToDouble", true);
+
+        addTransform(charType, booleanType, "Utility", "charToboolean", true);
+        addTransform(charType, objectType, "Character", "valueOf", true);
+        addTransform(charType, defType, "Character", "valueOf", true);
+        addTransform(charType, numberType, "Utility", "charToInteger", true);
+        addTransform(charType, byteobjType, "Utility", "charToByte", true);
+        addTransform(charType, shortobjType, "Utility", "charToShort", true);
+        addTransform(charType, charobjType, "Character", "valueOf", true);
+        addTransform(charType, intobjType, "Utility", "charToInteger", true);
+        addTransform(charType, longobjType, "Utility", "charToLong", true);
+        addTransform(charType, floatobjType, "Utility", "charToFloat", true);
+        addTransform(charType, doubleobjType, "Utility", "charToDouble", true);
+
+        addTransform(intType, booleanType, "Utility", "intToboolean", true);
+        addTransform(intType, objectType, "Integer", "valueOf", true);
+        addTransform(intType, defType, "Integer", "valueOf", true);
+        addTransform(intType, numberType, "Integer", "valueOf", true);
+        addTransform(intType, byteobjType, "Utility", "intToByte", true);
+        addTransform(intType, shortobjType, "Utility", "intToShort", true);
+        addTransform(intType, charobjType, "Utility", "intToCharacter", true);
+        addTransform(intType, intobjType, "Integer", "valueOf", true);
+        addTransform(intType, longobjType, "Utility", "intToLong", true);
+        addTransform(intType, floatobjType, "Utility", "intToFloat", true);
+        addTransform(intType, doubleobjType, "Utility", "intToDouble", true);
+
+        addTransform(longType, booleanType, "Utility", "longToboolean", true);
+        addTransform(longType, objectType, "Long", "valueOf", true);
+        addTransform(longType, defType, "Long", "valueOf", true);
+        addTransform(longType, numberType, "Long", "valueOf", true);
+        addTransform(longType, byteobjType, "Utility", "longToByte", true);
+        addTransform(longType, shortobjType, "Utility", "longToShort", true);
+        addTransform(longType, charobjType, "Utility", "longToCharacter", true);
+        addTransform(longType, intobjType, "Utility", "longToInteger", true);
+        addTransform(longType, longobjType, "Long", "valueOf", true);
+        addTransform(longType, floatobjType, "Utility", "longToFloat", true);
+        addTransform(longType, doubleobjType, "Utility", "longToDouble", true);
+
+        addTransform(floatType, booleanType, "Utility", "floatToboolean", true);
+        addTransform(floatType, objectType, "Float", "valueOf", true);
+        addTransform(floatType, defType, "Float", "valueOf", true);
+        addTransform(floatType, numberType, "Float", "valueOf", true);
+        addTransform(floatType, byteobjType, "Utility", "floatToByte", true);
+        addTransform(floatType, shortobjType, "Utility", "floatToShort", true);
+        addTransform(floatType, charobjType, "Utility", "floatToCharacter", true);
+        addTransform(floatType, intobjType, "Utility", "floatToInteger", true);
+        addTransform(floatType, longobjType, "Utility", "floatToLong", true);
+        addTransform(floatType, floatobjType, "Float", "valueOf", true);
+        addTransform(floatType, doubleobjType, "Utility", "floatToDouble", true);
+
+        addTransform(doubleType, booleanType, "Utility", "doubleToboolean", true);
+        addTransform(doubleType, objectType, "Double", "valueOf", true);
+        addTransform(doubleType, defType, "Double", "valueOf", true);
+        addTransform(doubleType, numberType, "Double", "valueOf", true);
+        addTransform(doubleType, byteobjType, "Utility", "doubleToByte", true);
+        addTransform(doubleType, shortobjType, "Utility", "doubleToShort", true);
+        addTransform(doubleType, charobjType, "Utility", "doubleToCharacter", true);
+        addTransform(doubleType, intobjType, "Utility", "doubleToInteger", true);
+        addTransform(doubleType, longobjType, "Utility", "doubleToLong", true);
+        addTransform(doubleType, floatobjType, "Utility", "doubleToFloat", true);
+        addTransform(doubleType, doubleobjType, "Double", "valueOf", true);
+
+        addTransform(objectType, booleanType, "Boolean", "booleanValue", false);
+        addTransform(objectType, byteType, "Number", "byteValue", false);
+        addTransform(objectType, shortType, "Number", "shortValue", false);
+        addTransform(objectType, charType, "Character", "charValue", false);
+        addTransform(objectType, intType, "Number", "intValue", false);
+        addTransform(objectType, longType, "Number", "longValue", false);
+        addTransform(objectType, floatType, "Number", "floatValue", false);
+        addTransform(objectType, doubleType, "Number", "doubleValue", false);
+
+        addTransform(defType, booleanType, "Def", "DefToboolean", true);
+        addTransform(defType, byteType, "Def", "DefTobyte", true);
+        addTransform(defType, shortType, "Def", "DefToshort", true);
+        addTransform(defType, charType, "Def", "DefTochar", true);
+        addTransform(defType, intType, "Def", "DefToint", true);
+        addTransform(defType, longType, "Def", "DefTolong", true);
+        addTransform(defType, floatType, "Def", "DefTofloat", true);
+        addTransform(defType, doubleType, "Def", "DefTodouble", true);
+        addTransform(defType, booleanobjType, "Def", "DefToBoolean", true);
+        addTransform(defType, byteobjType, "Def", "DefToByte", true);
+        addTransform(defType, shortobjType, "Def", "DefToShort", true);
+        addTransform(defType, charobjType, "Def", "DefToCharacter", true);
+        addTransform(defType, intobjType, "Def", "DefToInteger", true);
+        addTransform(defType, longobjType, "Def", "DefToLong", true);
+        addTransform(defType, floatobjType, "Def", "DefToFloat", true);
+        addTransform(defType, doubleobjType, "Def", "DefToDouble", true);
+        
+        addTransform(numberType, booleanType, "Utility", "NumberToboolean", true);
+        addTransform(numberType, byteType, "Number", "byteValue", false);
+        addTransform(numberType, shortType, "Number", "shortValue", false);
+        addTransform(numberType, charType, "Utility", "NumberTochar", true);
+        addTransform(numberType, intType, "Number", "intValue", false);
+        addTransform(numberType, longType, "Number", "longValue", false);
+        addTransform(numberType, floatType, "Number", "floatValue", false);
+        addTransform(numberType, doubleType, "Number", "doubleValue", false);
+        addTransform(numberType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(numberType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(numberType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(numberType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(numberType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(numberType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(numberType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(numberType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(booleanobjType, booleanType, "Boolean", "booleanValue", false);
+        addTransform(booleanobjType, byteType, "Utility", "BooleanTobyte", true);
+        addTransform(booleanobjType, shortType, "Utility", "BooleanToshort", true);
+        addTransform(booleanobjType, charType, "Utility", "BooleanTochar", true);
+        addTransform(booleanobjType, intType, "Utility", "BooleanToint", true);
+        addTransform(booleanobjType, longType, "Utility", "BooleanTolong", true);
+        addTransform(booleanobjType, floatType, "Utility", "BooleanTofloat", true);
+        addTransform(booleanobjType, doubleType, "Utility", "BooleanTodouble", true);
+        addTransform(booleanobjType, numberType, "Utility", "BooleanToLong", true);
+        addTransform(booleanobjType, byteobjType, "Utility", "BooleanToByte", true);
+        addTransform(booleanobjType, shortobjType, "Utility", "BooleanToShort", true);
+        addTransform(booleanobjType, charobjType, "Utility", "BooleanToCharacter", true);
+        addTransform(booleanobjType, intobjType, "Utility", "BooleanToInteger", true);
+        addTransform(booleanobjType, longobjType, "Utility", "BooleanToLong", true);
+        addTransform(booleanobjType, floatobjType, "Utility", "BooleanToFloat", true);
+        addTransform(booleanobjType, doubleobjType, "Utility", "BooleanToDouble", true);
+
+        addTransform(byteobjType, booleanType, "Utility", "ByteToboolean", true);
+        addTransform(byteobjType, byteType, "Byte", "byteValue", false);
+        addTransform(byteobjType, shortType, "Byte", "shortValue", false);
+        addTransform(byteobjType, charType, "Utility", "ByteTochar", true);
+        addTransform(byteobjType, intType, "Byte", "intValue", false);
+        addTransform(byteobjType, longType, "Byte", "longValue", false);
+        addTransform(byteobjType, floatType, "Byte", "floatValue", false);
+        addTransform(byteobjType, doubleType, "Byte", "doubleValue", false);
+        addTransform(byteobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(byteobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(byteobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(byteobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(byteobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(byteobjType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(byteobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(shortobjType, booleanType, "Utility", "ShortToboolean", true);
+        addTransform(shortobjType, byteType, "Short", "byteValue", false);
+        addTransform(shortobjType, shortType, "Short", "shortValue", false);
+        addTransform(shortobjType, charType, "Utility", "ShortTochar", true);
+        addTransform(shortobjType, intType, "Short", "intValue", false);
+        addTransform(shortobjType, longType, "Short", "longValue", false);
+        addTransform(shortobjType, floatType, "Short", "floatValue", false);
+        addTransform(shortobjType, doubleType, "Short", "doubleValue", false);
+        addTransform(shortobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(shortobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(shortobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(shortobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(shortobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(shortobjType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(shortobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(charobjType, booleanType, "Utility", "CharacterToboolean", true);
+        addTransform(charobjType, byteType, "Utility", "CharacterTobyte", true);
+        addTransform(charobjType, shortType, "Utility", "CharacterToshort", true);
+        addTransform(charobjType, charType, "Character", "charValue", false);
+        addTransform(charobjType, intType, "Utility", "CharacterToint", true);
+        addTransform(charobjType, longType, "Utility", "CharacterTolong", true);
+        addTransform(charobjType, floatType, "Utility", "CharacterTofloat", true);
+        addTransform(charobjType, doubleType, "Utility", "CharacterTodouble", true);
+        addTransform(charobjType, booleanobjType, "Utility", "CharacterToBoolean", true);
+        addTransform(charobjType, byteobjType, "Utility", "CharacterToByte", true);
+        addTransform(charobjType, shortobjType, "Utility", "CharacterToShort", true);
+        addTransform(charobjType, intobjType, "Utility", "CharacterToInteger", true);
+        addTransform(charobjType, longobjType, "Utility", "CharacterToLong", true);
+        addTransform(charobjType, floatobjType, "Utility", "CharacterToFloat", true);
+        addTransform(charobjType, doubleobjType, "Utility", "CharacterToDouble", true);
+
+        addTransform(intobjType, booleanType, "Utility", "IntegerToboolean", true);
+        addTransform(intobjType, byteType, "Integer", "byteValue", false);
+        addTransform(intobjType, shortType, "Integer", "shortValue", false);
+        addTransform(intobjType, charType, "Utility", "IntegerTochar", true);
+        addTransform(intobjType, intType, "Integer", "intValue", false);
+        addTransform(intobjType, longType, "Integer", "longValue", false);
+        addTransform(intobjType, floatType, "Integer", "floatValue", false);
+        addTransform(intobjType, doubleType, "Integer", "doubleValue", false);
+        addTransform(intobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(intobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(intobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(intobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(intobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(intobjType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(intobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(longobjType, booleanType, "Utility", "LongToboolean", true);
+        addTransform(longobjType, byteType, "Long", "byteValue", false);
+        addTransform(longobjType, shortType, "Long", "shortValue", false);
+        addTransform(longobjType, charType, "Utility", "LongTochar", true);
+        addTransform(longobjType, intType, "Long", "intValue", false);
+        addTransform(longobjType, longType, "Long", "longValue", false);
+        addTransform(longobjType, floatType, "Long", "floatValue", false);
+        addTransform(longobjType, doubleType, "Long", "doubleValue", false);
+        addTransform(longobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(longobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(longobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(longobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(longobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(longobjType, floatobjType, "Utility", "NumberToFloat", true);
+        addTransform(longobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(floatobjType, booleanType, "Utility", "FloatToboolean", true);
+        addTransform(floatobjType, byteType, "Float", "byteValue", false);
+        addTransform(floatobjType, shortType, "Float", "shortValue", false);
+        addTransform(floatobjType, charType, "Utility", "FloatTochar", true);
+        addTransform(floatobjType, intType, "Float", "intValue", false);
+        addTransform(floatobjType, longType, "Float", "longValue", false);
+        addTransform(floatobjType, floatType, "Float", "floatValue", false);
+        addTransform(floatobjType, doubleType, "Float", "doubleValue", false);
+        addTransform(floatobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(floatobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(floatobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(floatobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(floatobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(floatobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(floatobjType, doubleobjType, "Utility", "NumberToDouble", true);
+
+        addTransform(doubleobjType, booleanType, "Utility", "DoubleToboolean", true);
+        addTransform(doubleobjType, byteType, "Double", "byteValue", false);
+        addTransform(doubleobjType, shortType, "Double", "shortValue", false);
+        addTransform(doubleobjType, charType, "Utility", "DoubleTochar", true);
+        addTransform(doubleobjType, intType, "Double", "intValue", false);
+        addTransform(doubleobjType, longType, "Double", "longValue", false);
+        addTransform(doubleobjType, floatType, "Double", "floatValue", false);
+        addTransform(doubleobjType, doubleType, "Double", "doubleValue", false);
+        addTransform(doubleobjType, booleanobjType, "Utility", "NumberToBoolean", true);
+        addTransform(doubleobjType, byteobjType, "Utility", "NumberToByte", true);
+        addTransform(doubleobjType, shortobjType, "Utility", "NumberToShort", true);
+        addTransform(doubleobjType, charobjType, "Utility", "NumberToCharacter", true);
+        addTransform(doubleobjType, intobjType, "Utility", "NumberToInteger", true);
+        addTransform(doubleobjType, longobjType, "Utility", "NumberToLong", true);
+        addTransform(doubleobjType, floatobjType, "Utility", "NumberToFloat", true);
+    }
+
+    private void addDefaultBounds() {
+        addBound(byteobjType, numberType, numberType);
+
+        addBound(shortobjType, numberType, numberType);
+        addBound(shortobjType, byteobjType, numberType);
+
+        addBound(intobjType, numberType, numberType);
+        addBound(intobjType, byteobjType, numberType);
+        addBound(intobjType, shortobjType, numberType);
+
+        addBound(longobjType, numberType, numberType);
+        addBound(longobjType, byteobjType, numberType);
+        addBound(longobjType, shortobjType, numberType);
+        addBound(longobjType, intobjType, numberType);
+
+        addBound(floatobjType, numberType, numberType);
+        addBound(floatobjType, byteobjType, numberType);
+        addBound(floatobjType, shortobjType, numberType);
+        addBound(floatobjType, intobjType, numberType);
+        addBound(floatobjType, longobjType, numberType);
+
+        addBound(doubleobjType, numberType, numberType);
+        addBound(doubleobjType, byteobjType, numberType);
+        addBound(doubleobjType, shortobjType, numberType);
+        addBound(doubleobjType, intobjType, numberType);
+        addBound(doubleobjType, longobjType, numberType);
+        addBound(doubleobjType, floatobjType, numberType);
+
+        addBound(stringType, charseqType, charseqType);
+
+        addBound(arraylistType, listType, listType);
+        addBound(olistType, listType, listType);
+        addBound(olistType, arraylistType, listType);
+        addBound(oarraylistType, listType, listType);
+        addBound(oarraylistType, olistType, olistType);
+        addBound(oarraylistType, arraylistType, arraylistType);
+
+        addBound(hashmapType, mapType, mapType);
+        addBound(omapType, mapType, mapType);
+        addBound(omapType, hashmapType, mapType);
+        addBound(ohashmapType, mapType, mapType);
+        addBound(ohashmapType, hashmapType, hashmapType);
+        addBound(ohashmapType, omapType, omapType);
+        addBound(smapType, mapType, mapType);
+        addBound(smapType, hashmapType, mapType);
+        addBound(smapType, omapType, omapType);
+        addBound(smapType, ohashmapType, omapType);
+        addBound(shashmapType, mapType, mapType);
+        addBound(shashmapType, hashmapType, hashmapType);
+        addBound(shashmapType, omapType, omapType);
+        addBound(shashmapType, ohashmapType, ohashmapType);
+        addBound(shashmapType, smapType, smapType);
+        addBound(somapType, mapType, mapType);
+        addBound(somapType, hashmapType, mapType);
+        addBound(somapType, omapType, omapType);
+        addBound(somapType, ohashmapType, omapType);
+        addBound(somapType, smapType, smapType);
+        addBound(somapType, shashmapType, smapType);
+        addBound(sohashmapType, mapType, mapType);
+        addBound(sohashmapType, hashmapType, hashmapType);
+        addBound(sohashmapType, omapType, omapType);
+        addBound(sohashmapType, ohashmapType, ohashmapType);
+        addBound(sohashmapType, smapType, smapType);
+        addBound(sohashmapType, shashmapType, shashmapType);
+        addBound(sohashmapType, somapType, somapType);
+    }
+
+    public final void addStruct(final String name, final Class<?> clazz) {
+        if (!name.matches("^[_a-zA-Z][<>,_a-zA-Z0-9]*$")) {
+            throw new IllegalArgumentException("Invalid struct name [" + name + "].");
+        }
+
+        if (structs.containsKey(name)) {
+            throw new IllegalArgumentException("Duplicate struct name [" + name + "].");
+        }
+
+        final Struct struct = new Struct(name, clazz, org.objectweb.asm.Type.getType(clazz));
+
+        structs.put(name, struct);
+    }
+
+    public final void addClass(final String name) {
+        final Struct struct = structs.get(name);
+
+        if (struct == null) {
+            throw new IllegalArgumentException("Struct [" + name + "] is not defined.");
+        }
+
+        if (classes.containsKey(struct.clazz)) {
+            throw new IllegalArgumentException("Duplicate struct class [" + struct.clazz + "] when defining dynamic.");
+        }
+
+        classes.put(struct.clazz, struct);
+    }
+
+    public final void addConstructor(final String struct, final String name, final Type[] args, final Type[] genargs) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException(
+                    "Owner struct [" + struct + "] not defined for constructor [" + name + "].");
+        }
+
+        if (!name.matches("^[_a-zA-Z][_a-zA-Z0-9]*$")) {
+            throw new IllegalArgumentException(
+                    "Invalid constructor name [" + name + "] with the struct [" + owner.name + "].");
+        }
+
+        if (owner.constructors.containsKey(name)) {
+            throw new IllegalArgumentException(
+                    "Duplicate constructor name [" + name + "] found within the struct [" + owner.name + "].");
+        }
+
+        if (owner.statics.containsKey(name)) {
+            throw new IllegalArgumentException("Constructors and functions may not have the same name" +
+                    " [" + name + "] within the same struct [" + owner.name + "].");
+        }
+
+        if (owner.methods.containsKey(name)) {
+            throw new IllegalArgumentException("Constructors and methods may not have the same name" +
+                    " [" + name + "] within the same struct [" + owner.name + "].");
+        }
+
+        final Class[] classes = new Class[args.length];
+
+        for (int count = 0; count < classes.length; ++count) {
+            if (genargs != null) {
+                try {
+                    genargs[count].clazz.asSubclass(args[count].clazz);
+                } catch (ClassCastException exception) {
+                    throw new ClassCastException("Generic argument [" + genargs[count].name + "]" +
+                            " is not a sub class of [" + args[count].name + "] in the constructor" +
+                            " [" + name + " ] from the struct [" + owner.name + "].");
+                }
+            }
+
+            classes[count] = args[count].clazz;
+        }
+
+        final java.lang.reflect.Constructor<?> reflect;
+
+        try {
+            reflect = owner.clazz.getConstructor(classes);
+        } catch (NoSuchMethodException exception) {
+            throw new IllegalArgumentException("Constructor [" + name + "] not found for class" +
+                    " [" + owner.clazz.getName() + "] with arguments " + Arrays.toString(classes) + ".");
+        }
+
+        final org.objectweb.asm.commons.Method asm = org.objectweb.asm.commons.Method.getMethod(reflect);
+        final Constructor constructor =
+                new Constructor(name, owner, Arrays.asList(genargs != null ? genargs : args), asm, reflect);
+
+        owner.constructors.put(name, constructor);
+    }
+
+    public final void addMethod(final String struct, final String name, final String alias, final boolean statik,
+                                final Type rtn, final Type[] args, final Type genrtn, final Type[] genargs) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException("Owner struct [" + struct + "] not defined" +
+                    " for " + (statik ? "function" : "method") + " [" + name + "].");
+        }
+
+        if (!name.matches("^[_a-zA-Z][_a-zA-Z0-9]*$")) {
+            throw new IllegalArgumentException("Invalid " + (statik ? "function" : "method") +
+                    " name [" + name + "] with the struct [" + owner.name + "].");
+        }
+
+        if (owner.constructors.containsKey(name)) {
+            throw new IllegalArgumentException("Constructors and " + (statik ? "functions" : "methods") +
+                    " may not have the same name [" + name + "] within the same struct" +
+                    " [" + owner.name + "].");
+        }
+
+        if (owner.statics.containsKey(name)) {
+            if (statik) {
+                throw new IllegalArgumentException(
+                        "Duplicate function name [" + name + "] found within the struct [" + owner.name + "].");
+            } else {
+                throw new IllegalArgumentException("Functions and methods may not have the same name" +
+                        " [" + name + "] within the same struct [" + owner.name + "].");
+            }
+        }
+
+        if (owner.methods.containsKey(name)) {
+            if (statik) {
+                throw new IllegalArgumentException("Functions and methods may not have the same name" +
+                        " [" + name + "] within the same struct [" + owner.name + "].");
+            } else {
+                throw new IllegalArgumentException("Duplicate method name [" + name + "]" +
+                        " found within the struct [" + owner.name + "].");
+            }
+        }
+
+        if (genrtn != null) {
+            try {
+                genrtn.clazz.asSubclass(rtn.clazz);
+            } catch (ClassCastException exception) {
+                throw new ClassCastException("Generic return [" + genrtn.clazz.getCanonicalName() + "]" +
+                        " is not a sub class of [" + rtn.clazz.getCanonicalName() + "] in the method" +
+                        " [" + name + " ] from the struct [" + owner.name + "].");
+            }
+        }
+
+        if (genargs != null && genargs.length != args.length) {
+            throw new IllegalArgumentException("Generic arguments arity [" +  genargs.length + "] is not the same as " +
+                    (statik ? "function" : "method") + " [" + name + "] arguments arity" +
+                    " [" + args.length + "] within the struct [" + owner.name + "].");
+        }
+
+        final Class[] classes = new Class[args.length];
+
+        for (int count = 0; count < classes.length; ++count) {
+            if (genargs != null) {
+                try {
+                    genargs[count].clazz.asSubclass(args[count].clazz);
+                } catch (ClassCastException exception) {
+                    throw new ClassCastException("Generic argument [" + genargs[count].name + "] is not a sub class" +
+                            " of [" + args[count].name + "] in the " + (statik ? "function" : "method") +
+                            " [" + name + " ] from the struct [" + owner.name + "].");
+                }
+            }
+
+            classes[count] = args[count].clazz;
+        }
+
+        final java.lang.reflect.Method reflect;
+
+        try {
+            reflect = owner.clazz.getMethod(alias == null ? name : alias, classes);
+        } catch (NoSuchMethodException exception) {
+            throw new IllegalArgumentException((statik ? "Function" : "Method") +
+                    " [" + (alias == null ? name : alias) + "] not found for class [" + owner.clazz.getName() + "]" +
+                    " with arguments " + Arrays.toString(classes) + ".");
+        }
+
+        if (!reflect.getReturnType().equals(rtn.clazz)) {
+            throw new IllegalArgumentException("Specified return type class [" + rtn.clazz + "]" +
+                    " does not match the found return type class [" + reflect.getReturnType() + "] for the " +
+                    (statik ? "function" : "method") + " [" + name + "]" +
+                    " within the struct [" + owner.name + "].");
+        }
+
+        final org.objectweb.asm.commons.Method asm = org.objectweb.asm.commons.Method.getMethod(reflect);
+
+        MethodHandle handle;
+
+        try {
+            if (statik) {
+                handle = MethodHandles.publicLookup().in(owner.clazz).findStatic(
+                        owner.clazz, alias == null ? name : alias, MethodType.methodType(rtn.clazz, classes));
+            } else {
+                handle = MethodHandles.publicLookup().in(owner.clazz).findVirtual(
+                        owner.clazz, alias == null ? name : alias, MethodType.methodType(rtn.clazz, classes));
+            }
+        } catch (NoSuchMethodException | IllegalAccessException exception) {
+            throw new IllegalArgumentException("Method [" + (alias == null ? name : alias) + "]" +
+                    " not found for class [" + owner.clazz.getName() + "]" +
+                    " with arguments " + Arrays.toString(classes) + ".");
+        }
+
+        final Method method = new Method(name, owner, genrtn != null ? genrtn : rtn,
+                Arrays.asList(genargs != null ? genargs : args), asm, reflect, handle);
+        final int modifiers = reflect.getModifiers();
+
+        if (statik) {
+            if (!java.lang.reflect.Modifier.isStatic(modifiers)) {
+                throw new IllegalArgumentException("Function [" + name + "]" +
+                        " within the struct [" + owner.name + "] is not linked to a static Java method.");
+            }
+
+            owner.functions.put(name, method);
+        } else {
+            if (java.lang.reflect.Modifier.isStatic(modifiers)) {
+                throw new IllegalArgumentException("Method [" + name + "]" +
+                        " within the struct [" + owner.name + "] is not linked to a non-static Java method.");
+            }
+
+            owner.methods.put(name, method);
+        }
+    }
+
+    public final void addField(final String struct, final String name, final String alias,
+                               final boolean statik, final Type type, final Type generic) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException("Owner struct [" + struct + "] not defined for " +
+                    (statik ? "static" : "member") + " [" + name + "].");
+        }
+
+        if (!name.matches("^[_a-zA-Z][_a-zA-Z0-9]*$")) {
+            throw new IllegalArgumentException("Invalid " + (statik ? "static" : "member") +
+                    " name [" + name + "] with the struct [" + owner.name + "].");
+        }
+
+        if (owner.statics.containsKey(name)) {
+            if (statik) {
+                throw new IllegalArgumentException("Duplicate static name [" + name + "]" +
+                        " found within the struct [" + owner.name + "].");
+            } else {
+                throw new IllegalArgumentException("Statics and members may not have the same name " +
+                        "[" + name + "] within the same struct [" + owner.name + "].");
+            }
+        }
+
+        if (owner.members.containsKey(name)) {
+            if (statik) {
+                throw new IllegalArgumentException("Statics and members may not have the same name " +
+                        "[" + name + "] within the same struct [" + owner.name + "].");
+            } else {
+                throw new IllegalArgumentException("Duplicate member name [" + name + "]" +
+                        " found within the struct [" + owner.name + "].");
+            }
+        }
+
+        if (generic != null) {
+            try {
+                generic.clazz.asSubclass(type.clazz);
+            } catch (ClassCastException exception) {
+                throw new ClassCastException("Generic type [" + generic.clazz.getCanonicalName() + "]" +
+                        " is not a sub class of [" + type.clazz.getCanonicalName() + "] for the field" +
+                        " [" + name + " ] from the struct [" + owner.name + "].");
+            }
+        }
+
+        java.lang.reflect.Field reflect;
+
+        try {
+            reflect = owner.clazz.getField(alias == null ? name : alias);
+        } catch (NoSuchFieldException exception) {
+            throw new IllegalArgumentException("Field [" + (alias == null ? name : alias) + "]" +
+                    " not found for class [" + owner.clazz.getName() + "].");
+        }
+
+        MethodHandle getter = null;
+        MethodHandle setter = null;
+
+        try {
+            if (!statik) {
+                getter = MethodHandles.publicLookup().in(owner.clazz).findGetter(
+                        owner.clazz, alias == null ? name : alias, type.clazz);
+                setter = MethodHandles.publicLookup().in(owner.clazz).findSetter(
+                        owner.clazz, alias == null ? name : alias, type.clazz);
+            }
+        } catch (NoSuchFieldException | IllegalAccessException exception) {
+            throw new IllegalArgumentException("Getter/Setter [" + (alias == null ? name : alias) + "]" +
+                    " not found for class [" + owner.clazz.getName() + "].");
+        }
+
+        final Field field = new Field(name, owner, generic == null ? type : generic, type, reflect, getter, setter);
+        final int modifiers = reflect.getModifiers();
+
+        if (statik) {
+            if (!java.lang.reflect.Modifier.isStatic(modifiers)) {
+                throw new IllegalArgumentException();
+            }
+
+            if (!java.lang.reflect.Modifier.isFinal(modifiers)) {
+                throw new IllegalArgumentException("Static [" + name + "]" +
+                        " within the struct [" + owner.name + "] is not linked to static Java field.");
+            }
+
+            owner.statics.put(alias == null ? name : alias, field);
+        } else {
+            if (java.lang.reflect.Modifier.isStatic(modifiers)) {
+                throw new IllegalArgumentException("Member [" + name + "]" +
+                        " within the struct [" + owner.name + "] is not linked to non-static Java field.");
+            }
+
+            owner.members.put(alias == null ? name : alias, field);
+        }
+    }
+
+    public final void copyStruct(final String struct, final String... children) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException("Owner struct [" + struct + "] not defined for copy.");
+        }
+
+        for (int count = 0; count < children.length; ++count) {
+            final Struct child = structs.get(children[count]);
+
+            if (struct == null) {
+                throw new IllegalArgumentException("Child struct [" + children[count] + "]" +
+                        " not defined for copy to owner struct [" + owner.name + "].");
+            }
+
+            try {
+                owner.clazz.asSubclass(child.clazz);
+            } catch (ClassCastException exception) {
+                throw new ClassCastException("Child struct [" + child.name + "]" +
+                        " is not a super type of owner struct [" + owner.name + "] in copy.");
+            }
+
+            final boolean object = child.clazz.equals(Object.class) &&
+                    java.lang.reflect.Modifier.isInterface(owner.clazz.getModifiers());
+
+            for (final Method method : child.methods.values()) {
+                if (owner.methods.get(method.name) == null) {
+                    final Class<?> clazz = object ? Object.class : owner.clazz;
+
+                    java.lang.reflect.Method reflect;
+                    MethodHandle handle;
+
+                    try {
+                        reflect = clazz.getMethod(method.method.getName(), method.reflect.getParameterTypes());
+                    } catch (NoSuchMethodException exception) {
+                        throw new IllegalArgumentException("Method [" + method.method.getName() + "] not found for" +
+                                " class [" + owner.clazz.getName() + "] with arguments " +
+                                Arrays.toString(method.reflect.getParameterTypes()) + ".");
+                    }
+
+                    try {
+                        handle = MethodHandles.publicLookup().in(owner.clazz).findVirtual(
+                                owner.clazz, method.method.getName(),
+                                MethodType.methodType(method.reflect.getReturnType(), method.reflect.getParameterTypes()));
+                    } catch (NoSuchMethodException | IllegalAccessException exception) {
+                        throw new IllegalArgumentException("Method [" + method.method.getName() + "] not found for" +
+                                " class [" + owner.clazz.getName() + "] with arguments " +
+                                Arrays.toString(method.reflect.getParameterTypes()) + ".");
+                    }
+
+                    owner.methods.put(method.name,
+                            new Method(method.name, owner, method.rtn, method.arguments, method.method, reflect, handle));
+                }
+            }
+
+            for (final Field field : child.members.values()) {
+                if (owner.members.get(field.name) == null) {
+                    java.lang.reflect.Field reflect;
+                    MethodHandle getter;
+                    MethodHandle setter;
+
+                    try {
+                        reflect = owner.clazz.getField(field.reflect.getName());
+                    } catch (NoSuchFieldException exception) {
+                        throw new IllegalArgumentException("Field [" + field.reflect.getName() + "]" +
+                                " not found for class [" + owner.clazz.getName() + "].");
+                    }
+
+                    try {
+                        getter = MethodHandles.publicLookup().in(owner.clazz).findGetter(
+                                owner.clazz, field.name, field.type.clazz);
+                        setter = MethodHandles.publicLookup().in(owner.clazz).findSetter(
+                                owner.clazz, field.name, field.type.clazz);
+                    } catch (NoSuchFieldException | IllegalAccessException exception) {
+                        throw new IllegalArgumentException("Getter/Setter [" + field.name + "]" +
+                                " not found for class [" + owner.clazz.getName() + "].");
+                    }
+
+                    owner.members.put(field.name,
+                            new Field(field.name, owner, field.type, field.generic, reflect, getter, setter));
+                }
+            }
+        }
+    }
+
+    public final void addTransform(final Type from, final Type to, final String struct,
+                                   final String name, final boolean statik) {
+        final Struct owner = structs.get(struct);
+
+        if (owner == null) {
+            throw new IllegalArgumentException("Owner struct [" + struct + "] not defined for" +
+                    " transform with cast type from [" + from.name + "] and cast type to [" + to.name + "].");
+        }
+
+        if (from.equals(to)) {
+            throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "] cannot" +
+                    " have cast type from [" + from.name + "] be the same as cast type to [" + to.name + "].");
+        }
+
+        final Cast cast = new Cast(from, to);
+
+        if (transforms.containsKey(cast)) {
+            throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                    " and cast type from [" + from.name + "] to cast type to [" + to.name + "] already defined.");
+        }
+
+        Method method;
+        Type upcast = null;
+        Type downcast = null;
+
+        if (statik) {
+            method = owner.functions.get(name);
+
+            if (method == null) {
+                throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                        " and cast type from [" + from.name + "] to cast type to [" + to.name +
+                        "] using a function [" + name + "] that is not defined.");
+            }
+
+            if (method.arguments.size() != 1) {
+                throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                        " and cast type from [" + from.name + "] to cast type to [" + to.name +
+                        "] using function [" + name + "] does not have a single type argument.");
+            }
+
+            Type argument = method.arguments.get(0);
+
+            try {
+                from.clazz.asSubclass(argument.clazz);
+            } catch (ClassCastException cce0) {
+                try {
+                    argument.clazz.asSubclass(from.clazz);
+                    upcast = argument;
+                } catch (ClassCastException cce1) {
+                    throw new ClassCastException("Transform with owner struct [" + owner.name + "]" +
+                            " and cast type from [" + from.name + "] to cast type to [" + to.name + "] using" +
+                            " function [" + name + "] cannot cast from type to the function input argument type.");
+                }
+            }
+
+            final Type rtn = method.rtn;
+
+            try {
+                rtn.clazz.asSubclass(to.clazz);
+            } catch (ClassCastException cce0) {
+                try {
+                    to.clazz.asSubclass(rtn.clazz);
+                    downcast = to;
+                } catch (ClassCastException cce1) {
+                    throw new ClassCastException("Transform with owner struct [" + owner.name + "]" +
+                            " and cast type from [" + from.name + "] to cast type to [" + to.name + "] using" +
+                            " function [" + name + "] cannot cast to type to the function return argument type.");
+                }
+            }
+        } else {
+            method = owner.methods.get(name);
+
+            if (method == null) {
+                throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                        " and cast type from [" + from.name + "] to cast type to [" + to.name +
+                        "] using a method [" + name + "] that is not defined.");
+            }
+
+            if (!method.arguments.isEmpty()) {
+                throw new IllegalArgumentException("Transform with owner struct [" + owner.name + "]" +
+                        " and cast type from [" + from.name + "] to cast type to [" + to.name +
+                        "] using method [" + name + "] does not have a single type argument.");
+            }
+
+            try {
+                from.clazz.asSubclass(owner.clazz);
+            } catch (ClassCastException cce0) {
+                try {
+                    owner.clazz.asSubclass(from.clazz);
+                    upcast = getType(owner.name);
+                } catch (ClassCastException cce1) {
+                    throw new ClassCastException("Transform with owner struct [" + owner.name + "]" +
+                            " and cast type from [" + from.name + "] to cast type to [" + to.name + "] using" +
+                            " method [" + name + "] cannot cast from type to the method input argument type.");
+                }
+            }
+
+            final Type rtn = method.rtn;
+
+            try {
+                rtn.clazz.asSubclass(to.clazz);
+            } catch (ClassCastException cce0) {
+                try {
+                    to.clazz.asSubclass(rtn.clazz);
+                    downcast = to;
+                } catch (ClassCastException cce1) {
+                    throw new ClassCastException("Transform with owner struct [" + owner.name + "]" +
+                            " and cast type from [" + from.name + "] to cast type to [" + to.name + "]" +
+                            " using method [" + name + "] cannot cast to type to the method return argument type.");
+                }
+            }
+        }
+
+        final Transform transform = new Transform(cast, method, upcast, downcast);
+        transforms.put(cast, transform);
+    }
+
+    public final void addBound(final Type type0, final Type type1, final Type bound) {
+        final Pair pair0 = new Pair(type0, type1);
+        final Pair pair1 = new Pair(type1, type0);
+
+        if (bounds.containsKey(pair0)) {
+            throw new IllegalArgumentException(
+                    "Bound already defined for types [" + type0.name + "] and [" + type1.name + "].");
+        }
+
+        if (bounds.containsKey(pair1)) {
+            throw new IllegalArgumentException(
+                    "Bound already defined for types [" + type1.name + "] and [" + type0.name + "].");
+        }
+
+        bounds.put(pair0, bound);
+        bounds.put(pair1, bound);
+    }
+
+    Type getType(final String name) {
+        final int dimensions = getDimensions(name);
+        final String structstr = dimensions == 0 ? name : name.substring(0, name.indexOf('['));
+        final Struct struct = structs.get(structstr);
+
+        if (struct == null) {
+            throw new IllegalArgumentException("The struct with name [" + name + "] has not been defined.");
+        }
+
+        return getType(struct, dimensions);
+    }
+
+    Type getType(final Struct struct, final int dimensions) {
+        String name = struct.name;
+        org.objectweb.asm.Type type = struct.type;
+        Class<?> clazz = struct.clazz;
+        Sort sort;
+
+        if (dimensions > 0) {
+            final StringBuilder builder = new StringBuilder(name);
+            final char[] brackets = new char[dimensions];
+
+            for (int count = 0; count < dimensions; ++count) {
+                builder.append("[]");
+                brackets[count] = '[';
+            }
+
+            final String descriptor = new String(brackets) + struct.type.getDescriptor();
+
+            name = builder.toString();
+            type = org.objectweb.asm.Type.getType(descriptor);
+
+            try {
+                clazz = Class.forName(type.getInternalName().replace('/', '.'));
+            } catch (ClassNotFoundException exception) {
+                throw new IllegalArgumentException("The class [" + type.getInternalName() + "]" +
+                        " could not be found to create type [" + name + "].");
+            }
+
+            sort = Sort.ARRAY;
+        } else if ("def".equals(struct.name)) {
+            sort = Sort.DEF;
+        } else {
+            sort = Sort.OBJECT;
+
+            for (Sort value : Sort.values()) {
+                if (value.clazz == null) {
+                    continue;
+                }
+
+                if (value.clazz.equals(struct.clazz)) {
+                    sort = value;
+
+                    break;
+                }
+            }
+        }
+
+        return new Type(name, struct, clazz, type, sort);
+    }
+
+    private int getDimensions(final String name) {
+        int dimensions = 0;
+        int index = name.indexOf('[');
+
+        if (index != -1) {
+            final int length = name.length();
+
+            while (index < length) {
+                if (name.charAt(index) == '[' && ++index < length && name.charAt(index++) == ']') {
+                    ++dimensions;
+                } else {
+                    throw new IllegalArgumentException("Invalid array braces in canonical name [" + name + "].");
+                }
+            }
+        }
+
+        return dimensions;
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ErrorHandlingLexer.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ErrorHandlingLexer.java
new file mode 100644
index 0000000..95e3c93
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ErrorHandlingLexer.java
@@ -0,0 +1,45 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.text.ParseException;
+
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.LexerNoViableAltException;
+import org.antlr.v4.runtime.misc.Interval;
+
+class ErrorHandlingLexer extends PlanALexer {
+    public ErrorHandlingLexer(CharStream charStream) {
+        super(charStream);
+    }
+
+    @Override
+    public void recover(LexerNoViableAltException lnvae) {
+        CharStream charStream = lnvae.getInputStream();
+        int startIndex = lnvae.getStartIndex();
+        String text = charStream.getText(Interval.of(startIndex, charStream.index()));
+
+        ParseException parseException = new ParseException("Error [" + _tokenStartLine + ":" +
+                _tokenStartCharPositionInLine + "]: unexpected character [" +
+                getErrorDisplay(text) + "].",  _tokenStartCharIndex);
+        parseException.initCause(lnvae);
+        throw new RuntimeException(parseException);
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Executable.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Executable.java
new file mode 100644
index 0000000..09e28cf
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Executable.java
@@ -0,0 +1,50 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.util.Map;
+
+public abstract class Executable {
+    protected final Definition definition;
+
+    private final String name;
+    private final String source;
+
+    public Executable(final Definition definition, final String name, final String source) {
+        this.definition = definition;
+
+        this.name = name;
+        this.source = source;
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public String getSource() {
+        return source;
+    }
+
+    public Definition getDefinition() {
+        return definition;
+    }
+
+    public abstract Object execute(Map<String, Object> input);
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ParserErrorStrategy.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ParserErrorStrategy.java
new file mode 100644
index 0000000..3fe3603
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ParserErrorStrategy.java
@@ -0,0 +1,74 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.text.ParseException;
+
+import org.antlr.v4.runtime.DefaultErrorStrategy;
+import org.antlr.v4.runtime.InputMismatchException;
+import org.antlr.v4.runtime.NoViableAltException;
+import org.antlr.v4.runtime.Parser;
+import org.antlr.v4.runtime.RecognitionException;
+import org.antlr.v4.runtime.Token;
+
+class ParserErrorStrategy extends DefaultErrorStrategy {
+    @Override
+    public void recover(Parser recognizer, RecognitionException re) {
+        Token token = re.getOffendingToken();
+        String message;
+
+        if (token == null) {
+            message = "Error: no parse token found.";
+        } else if (re instanceof InputMismatchException) {
+            message = "Error[" + token.getLine() + ":" + token.getCharPositionInLine() + "]:" +
+                    " unexpected token [" + getTokenErrorDisplay(token) + "]" +
+                    " was expecting one of [" + re.getExpectedTokens().toString(recognizer.getVocabulary()) + "].";
+        } else if (re instanceof NoViableAltException) {
+            if (token.getType() == PlanAParser.EOF) {
+                message = "Error: unexpected end of script.";
+            } else {
+                message = "Error[" + token.getLine() + ":" + token.getCharPositionInLine() + "]:" +
+                        "invalid sequence of tokens near [" + getTokenErrorDisplay(token) + "].";
+            }
+        } else {
+            message = "Error[" + token.getLine() + ":" + token.getCharPositionInLine() + "]:" +
+                    " unexpected token near [" + getTokenErrorDisplay(token) + "].";
+        }
+
+        ParseException parseException = new ParseException(message, token == null ? -1 : token.getStartIndex());
+        parseException.initCause(re);
+
+        throw new RuntimeException(parseException);
+    }
+
+    @Override
+    public Token recoverInline(Parser recognizer) throws RecognitionException {
+        Token token = recognizer.getCurrentToken();
+        String message = "Error[" + token.getLine() + ":" + token.getCharPositionInLine() + "]:" +
+                " unexpected token [" + getTokenErrorDisplay(token) + "]" +
+                " was expecting one of [" + recognizer.getExpectedTokens().toString(recognizer.getVocabulary()) + "].";
+        ParseException parseException = new ParseException(message, token.getStartIndex());
+        throw new RuntimeException(parseException);
+    }
+
+    @Override
+    public void sync(Parser recognizer) {
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanALexer.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanALexer.java
new file mode 100644
index 0000000..a9e5ff6
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanALexer.java
@@ -0,0 +1,390 @@
+// ANTLR GENERATED CODE: DO NOT EDIT
+package org.elasticsearch.plan.a;
+
+    import java.util.Set;
+
+import org.antlr.v4.runtime.Lexer;
+import org.antlr.v4.runtime.CharStream;
+import org.antlr.v4.runtime.Token;
+import org.antlr.v4.runtime.TokenStream;
+import org.antlr.v4.runtime.*;
+import org.antlr.v4.runtime.atn.*;
+import org.antlr.v4.runtime.dfa.DFA;
+import org.antlr.v4.runtime.misc.*;
+
+@SuppressWarnings({"all", "warnings", "unchecked", "unused", "cast"})
+class PlanALexer extends Lexer {
+  static { RuntimeMetaData.checkVersion("4.5.1", RuntimeMetaData.VERSION); }
+
+  protected static final DFA[] _decisionToDFA;
+  protected static final PredictionContextCache _sharedContextCache =
+    new PredictionContextCache();
+  public static final int
+    WS=1, COMMENT=2, LBRACK=3, RBRACK=4, LBRACE=5, RBRACE=6, LP=7, RP=8, DOT=9, 
+    COMMA=10, SEMICOLON=11, IF=12, ELSE=13, WHILE=14, DO=15, FOR=16, CONTINUE=17, 
+    BREAK=18, RETURN=19, NEW=20, TRY=21, CATCH=22, THROW=23, BOOLNOT=24, BWNOT=25, 
+    MUL=26, DIV=27, REM=28, ADD=29, SUB=30, LSH=31, RSH=32, USH=33, LT=34, 
+    LTE=35, GT=36, GTE=37, EQ=38, EQR=39, NE=40, NER=41, BWAND=42, BWXOR=43, 
+    BWOR=44, BOOLAND=45, BOOLOR=46, COND=47, COLON=48, INCR=49, DECR=50, ASSIGN=51, 
+    AADD=52, ASUB=53, AMUL=54, ADIV=55, AREM=56, AAND=57, AXOR=58, AOR=59, 
+    ALSH=60, ARSH=61, AUSH=62, ACAT=63, OCTAL=64, HEX=65, INTEGER=66, DECIMAL=67, 
+    STRING=68, CHAR=69, TRUE=70, FALSE=71, NULL=72, TYPE=73, ID=74, EXTINTEGER=75, 
+    EXTID=76;
+  public static final int EXT = 1;
+  public static String[] modeNames = {
+    "DEFAULT_MODE", "EXT"
+  };
+
+  public static final String[] ruleNames = {
+    "WS", "COMMENT", "LBRACK", "RBRACK", "LBRACE", "RBRACE", "LP", "RP", "DOT", 
+    "COMMA", "SEMICOLON", "IF", "ELSE", "WHILE", "DO", "FOR", "CONTINUE", 
+    "BREAK", "RETURN", "NEW", "TRY", "CATCH", "THROW", "BOOLNOT", "BWNOT", 
+    "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT", 
+    "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND", 
+    "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL", 
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT", 
+    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", 
+    "NULL", "TYPE", "GENERIC", "ID", "EXTINTEGER", "EXTID"
+  };
+
+  private static final String[] _LITERAL_NAMES = {
+    null, null, null, "'{'", "'}'", "'['", "']'", "'('", "')'", "'.'", "','", 
+    "';'", "'if'", "'else'", "'while'", "'do'", "'for'", "'continue'", "'break'", 
+    "'return'", "'new'", "'try'", "'catch'", "'throw'", "'!'", "'~'", "'*'", 
+    "'/'", "'%'", "'+'", "'-'", "'<<'", "'>>'", "'>>>'", "'<'", "'<='", "'>'", 
+    "'>='", "'=='", "'==='", "'!='", "'!=='", "'&'", "'^'", "'|'", "'&&'", 
+    "'||'", "'?'", "':'", "'++'", "'--'", "'='", "'+='", "'-='", "'*='", "'/='", 
+    "'%='", "'&='", "'^='", "'|='", "'<<='", "'>>='", "'>>>='", "'..='", null, 
+    null, null, null, null, null, "'true'", "'false'", "'null'"
+  };
+  private static final String[] _SYMBOLIC_NAMES = {
+    null, "WS", "COMMENT", "LBRACK", "RBRACK", "LBRACE", "RBRACE", "LP", "RP", 
+    "DOT", "COMMA", "SEMICOLON", "IF", "ELSE", "WHILE", "DO", "FOR", "CONTINUE", 
+    "BREAK", "RETURN", "NEW", "TRY", "CATCH", "THROW", "BOOLNOT", "BWNOT", 
+    "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT", 
+    "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND", 
+    "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL", 
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT", 
+    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", 
+    "NULL", "TYPE", "ID", "EXTINTEGER", "EXTID"
+  };
+  public static final Vocabulary VOCABULARY = new VocabularyImpl(_LITERAL_NAMES, _SYMBOLIC_NAMES);
+
+  /**
+   * @deprecated Use {@link #VOCABULARY} instead.
+   */
+  @Deprecated
+  public static final String[] tokenNames;
+  static {
+    tokenNames = new String[_SYMBOLIC_NAMES.length];
+    for (int i = 0; i < tokenNames.length; i++) {
+      tokenNames[i] = VOCABULARY.getLiteralName(i);
+      if (tokenNames[i] == null) {
+        tokenNames[i] = VOCABULARY.getSymbolicName(i);
+      }
+
+      if (tokenNames[i] == null) {
+        tokenNames[i] = "<INVALID>";
+      }
+    }
+  }
+
+  @Override
+  @Deprecated
+  public String[] getTokenNames() {
+    return tokenNames;
+  }
+
+  @Override
+
+  public Vocabulary getVocabulary() {
+    return VOCABULARY;
+  }
+
+
+      private Set<String> types = null;
+
+      void setTypes(Set<String> types) {
+          this.types = types;
+      }
+
+
+  public PlanALexer(CharStream input) {
+    super(input);
+    _interp = new LexerATNSimulator(this,_ATN,_decisionToDFA,_sharedContextCache);
+  }
+
+  @Override
+  public String getGrammarFileName() { return "PlanALexer.g4"; }
+
+  @Override
+  public String[] getRuleNames() { return ruleNames; }
+
+  @Override
+  public String getSerializedATN() { return _serializedATN; }
+
+  @Override
+  public String[] getModeNames() { return modeNames; }
+
+  @Override
+  public ATN getATN() { return _ATN; }
+
+  @Override
+  public void action(RuleContext _localctx, int ruleIndex, int actionIndex) {
+    switch (ruleIndex) {
+    case 67:
+      STRING_action((RuleContext)_localctx, actionIndex);
+      break;
+    case 68:
+      CHAR_action((RuleContext)_localctx, actionIndex);
+      break;
+    case 72:
+      TYPE_action((RuleContext)_localctx, actionIndex);
+      break;
+    }
+  }
+  private void STRING_action(RuleContext _localctx, int actionIndex) {
+    switch (actionIndex) {
+    case 0:
+      setText(getText().substring(1, getText().length() - 1));
+      break;
+    }
+  }
+  private void CHAR_action(RuleContext _localctx, int actionIndex) {
+    switch (actionIndex) {
+    case 1:
+      setText(getText().substring(1, getText().length() - 1));
+      break;
+    }
+  }
+  private void TYPE_action(RuleContext _localctx, int actionIndex) {
+    switch (actionIndex) {
+    case 2:
+      setText(getText().replace(" ", ""));
+      break;
+    }
+  }
+  @Override
+  public boolean sempred(RuleContext _localctx, int ruleIndex, int predIndex) {
+    switch (ruleIndex) {
+    case 72:
+      return TYPE_sempred((RuleContext)_localctx, predIndex);
+    }
+    return true;
+  }
+  private boolean TYPE_sempred(RuleContext _localctx, int predIndex) {
+    switch (predIndex) {
+    case 0:
+      return types.contains(getText().replace(" ", ""));
+    }
+    return true;
+  }
+
+  public static final String _serializedATN =
+    "\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\2N\u0236\b\1\b\1\4"+
+    "\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7\4\b\t\b\4\t\t\t\4\n\t\n"+
+    "\4\13\t\13\4\f\t\f\4\r\t\r\4\16\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22"+
+    "\t\22\4\23\t\23\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\4\31"+
+    "\t\31\4\32\t\32\4\33\t\33\4\34\t\34\4\35\t\35\4\36\t\36\4\37\t\37\4 \t"+
+    " \4!\t!\4\"\t\"\4#\t#\4$\t$\4%\t%\4&\t&\4\'\t\'\4(\t(\4)\t)\4*\t*\4+\t"+
+    "+\4,\t,\4-\t-\4.\t.\4/\t/\4\60\t\60\4\61\t\61\4\62\t\62\4\63\t\63\4\64"+
+    "\t\64\4\65\t\65\4\66\t\66\4\67\t\67\48\t8\49\t9\4:\t:\4;\t;\4<\t<\4=\t"+
+    "=\4>\t>\4?\t?\4@\t@\4A\tA\4B\tB\4C\tC\4D\tD\4E\tE\4F\tF\4G\tG\4H\tH\4"+
+    "I\tI\4J\tJ\4K\tK\4L\tL\4M\tM\4N\tN\3\2\6\2\u00a0\n\2\r\2\16\2\u00a1\3"+
+    "\2\3\2\3\3\3\3\3\3\3\3\7\3\u00aa\n\3\f\3\16\3\u00ad\13\3\3\3\3\3\3\3\3"+
+    "\3\3\3\7\3\u00b4\n\3\f\3\16\3\u00b7\13\3\3\3\3\3\5\3\u00bb\n\3\3\3\3\3"+
+    "\3\4\3\4\3\5\3\5\3\6\3\6\3\7\3\7\3\b\3\b\3\t\3\t\3\n\3\n\3\n\3\n\3\13"+
+    "\3\13\3\f\3\f\3\r\3\r\3\r\3\16\3\16\3\16\3\16\3\16\3\17\3\17\3\17\3\17"+
+    "\3\17\3\17\3\20\3\20\3\20\3\21\3\21\3\21\3\21\3\22\3\22\3\22\3\22\3\22"+
+    "\3\22\3\22\3\22\3\22\3\23\3\23\3\23\3\23\3\23\3\23\3\24\3\24\3\24\3\24"+
+    "\3\24\3\24\3\24\3\25\3\25\3\25\3\25\3\26\3\26\3\26\3\26\3\27\3\27\3\27"+
+    "\3\27\3\27\3\27\3\30\3\30\3\30\3\30\3\30\3\30\3\31\3\31\3\32\3\32\3\33"+
+    "\3\33\3\34\3\34\3\35\3\35\3\36\3\36\3\37\3\37\3 \3 \3 \3!\3!\3!\3\"\3"+
+    "\"\3\"\3\"\3#\3#\3$\3$\3$\3%\3%\3&\3&\3&\3\'\3\'\3\'\3(\3(\3(\3(\3)\3"+
+    ")\3)\3*\3*\3*\3*\3+\3+\3,\3,\3-\3-\3.\3.\3.\3/\3/\3/\3\60\3\60\3\61\3"+
+    "\61\3\62\3\62\3\62\3\63\3\63\3\63\3\64\3\64\3\65\3\65\3\65\3\66\3\66\3"+
+    "\66\3\67\3\67\3\67\38\38\38\39\39\39\3:\3:\3:\3;\3;\3;\3<\3<\3<\3=\3="+
+    "\3=\3=\3>\3>\3>\3>\3?\3?\3?\3?\3?\3@\3@\3@\3@\3A\3A\6A\u0185\nA\rA\16"+
+    "A\u0186\3A\5A\u018a\nA\3B\3B\3B\6B\u018f\nB\rB\16B\u0190\3B\5B\u0194\n"+
+    "B\3C\3C\3C\7C\u0199\nC\fC\16C\u019c\13C\5C\u019e\nC\3C\5C\u01a1\nC\3D"+
+    "\3D\3D\7D\u01a6\nD\fD\16D\u01a9\13D\5D\u01ab\nD\3D\3D\7D\u01af\nD\fD\16"+
+    "D\u01b2\13D\3D\3D\5D\u01b6\nD\3D\6D\u01b9\nD\rD\16D\u01ba\5D\u01bd\nD"+
+    "\3D\5D\u01c0\nD\3E\3E\3E\3E\3E\3E\7E\u01c8\nE\fE\16E\u01cb\13E\3E\3E\3"+
+    "E\3F\3F\3F\3F\3F\3G\3G\3G\3G\3G\3H\3H\3H\3H\3H\3H\3I\3I\3I\3I\3I\3J\3"+
+    "J\5J\u01e7\nJ\3J\3J\3J\3K\7K\u01ed\nK\fK\16K\u01f0\13K\3K\3K\7K\u01f4"+
+    "\nK\fK\16K\u01f7\13K\3K\3K\5K\u01fb\nK\3K\7K\u01fe\nK\fK\16K\u0201\13"+
+    "K\3K\3K\7K\u0205\nK\fK\16K\u0208\13K\3K\3K\5K\u020c\nK\3K\7K\u020f\nK"+
+    "\fK\16K\u0212\13K\7K\u0214\nK\fK\16K\u0217\13K\3K\3K\3L\3L\7L\u021d\n"+
+    "L\fL\16L\u0220\13L\3M\3M\3M\7M\u0225\nM\fM\16M\u0228\13M\5M\u022a\nM\3"+
+    "M\3M\3N\3N\7N\u0230\nN\fN\16N\u0233\13N\3N\3N\5\u00ab\u00b5\u01c9\2O\4"+
+    "\3\6\4\b\5\n\6\f\7\16\b\20\t\22\n\24\13\26\f\30\r\32\16\34\17\36\20 \21"+
+    "\"\22$\23&\24(\25*\26,\27.\30\60\31\62\32\64\33\66\348\35:\36<\37> @!"+
+    "B\"D#F$H%J&L\'N(P)R*T+V,X-Z.\\/^\60`\61b\62d\63f\64h\65j\66l\67n8p9r:"+
+    "t;v<x=z>|?~@\u0080A\u0082B\u0084C\u0086D\u0088E\u008aF\u008cG\u008eH\u0090"+
+    "I\u0092J\u0094K\u0096\2\u0098L\u009aM\u009cN\4\2\3\21\5\2\13\f\17\17\""+
+    "\"\4\2\f\f\17\17\3\2\629\4\2NNnn\4\2ZZzz\5\2\62;CHch\3\2\63;\3\2\62;\b"+
+    "\2FFHHNNffhhnn\4\2GGgg\4\2--//\4\2HHhh\4\2$$^^\5\2C\\aac|\6\2\62;C\\a"+
+    "ac|\u0255\2\4\3\2\2\2\2\6\3\2\2\2\2\b\3\2\2\2\2\n\3\2\2\2\2\f\3\2\2\2"+
+    "\2\16\3\2\2\2\2\20\3\2\2\2\2\22\3\2\2\2\2\24\3\2\2\2\2\26\3\2\2\2\2\30"+
+    "\3\2\2\2\2\32\3\2\2\2\2\34\3\2\2\2\2\36\3\2\2\2\2 \3\2\2\2\2\"\3\2\2\2"+
+    "\2$\3\2\2\2\2&\3\2\2\2\2(\3\2\2\2\2*\3\2\2\2\2,\3\2\2\2\2.\3\2\2\2\2\60"+
+    "\3\2\2\2\2\62\3\2\2\2\2\64\3\2\2\2\2\66\3\2\2\2\28\3\2\2\2\2:\3\2\2\2"+
+    "\2<\3\2\2\2\2>\3\2\2\2\2@\3\2\2\2\2B\3\2\2\2\2D\3\2\2\2\2F\3\2\2\2\2H"+
+    "\3\2\2\2\2J\3\2\2\2\2L\3\2\2\2\2N\3\2\2\2\2P\3\2\2\2\2R\3\2\2\2\2T\3\2"+
+    "\2\2\2V\3\2\2\2\2X\3\2\2\2\2Z\3\2\2\2\2\\\3\2\2\2\2^\3\2\2\2\2`\3\2\2"+
+    "\2\2b\3\2\2\2\2d\3\2\2\2\2f\3\2\2\2\2h\3\2\2\2\2j\3\2\2\2\2l\3\2\2\2\2"+
+    "n\3\2\2\2\2p\3\2\2\2\2r\3\2\2\2\2t\3\2\2\2\2v\3\2\2\2\2x\3\2\2\2\2z\3"+
+    "\2\2\2\2|\3\2\2\2\2~\3\2\2\2\2\u0080\3\2\2\2\2\u0082\3\2\2\2\2\u0084\3"+
+    "\2\2\2\2\u0086\3\2\2\2\2\u0088\3\2\2\2\2\u008a\3\2\2\2\2\u008c\3\2\2\2"+
+    "\2\u008e\3\2\2\2\2\u0090\3\2\2\2\2\u0092\3\2\2\2\2\u0094\3\2\2\2\2\u0098"+
+    "\3\2\2\2\3\u009a\3\2\2\2\3\u009c\3\2\2\2\4\u009f\3\2\2\2\6\u00ba\3\2\2"+
+    "\2\b\u00be\3\2\2\2\n\u00c0\3\2\2\2\f\u00c2\3\2\2\2\16\u00c4\3\2\2\2\20"+
+    "\u00c6\3\2\2\2\22\u00c8\3\2\2\2\24\u00ca\3\2\2\2\26\u00ce\3\2\2\2\30\u00d0"+
+    "\3\2\2\2\32\u00d2\3\2\2\2\34\u00d5\3\2\2\2\36\u00da\3\2\2\2 \u00e0\3\2"+
+    "\2\2\"\u00e3\3\2\2\2$\u00e7\3\2\2\2&\u00f0\3\2\2\2(\u00f6\3\2\2\2*\u00fd"+
+    "\3\2\2\2,\u0101\3\2\2\2.\u0105\3\2\2\2\60\u010b\3\2\2\2\62\u0111\3\2\2"+
+    "\2\64\u0113\3\2\2\2\66\u0115\3\2\2\28\u0117\3\2\2\2:\u0119\3\2\2\2<\u011b"+
+    "\3\2\2\2>\u011d\3\2\2\2@\u011f\3\2\2\2B\u0122\3\2\2\2D\u0125\3\2\2\2F"+
+    "\u0129\3\2\2\2H\u012b\3\2\2\2J\u012e\3\2\2\2L\u0130\3\2\2\2N\u0133\3\2"+
+    "\2\2P\u0136\3\2\2\2R\u013a\3\2\2\2T\u013d\3\2\2\2V\u0141\3\2\2\2X\u0143"+
+    "\3\2\2\2Z\u0145\3\2\2\2\\\u0147\3\2\2\2^\u014a\3\2\2\2`\u014d\3\2\2\2"+
+    "b\u014f\3\2\2\2d\u0151\3\2\2\2f\u0154\3\2\2\2h\u0157\3\2\2\2j\u0159\3"+
+    "\2\2\2l\u015c\3\2\2\2n\u015f\3\2\2\2p\u0162\3\2\2\2r\u0165\3\2\2\2t\u0168"+
+    "\3\2\2\2v\u016b\3\2\2\2x\u016e\3\2\2\2z\u0171\3\2\2\2|\u0175\3\2\2\2~"+
+    "\u0179\3\2\2\2\u0080\u017e\3\2\2\2\u0082\u0182\3\2\2\2\u0084\u018b\3\2"+
+    "\2\2\u0086\u019d\3\2\2\2\u0088\u01aa\3\2\2\2\u008a\u01c1\3\2\2\2\u008c"+
+    "\u01cf\3\2\2\2\u008e\u01d4\3\2\2\2\u0090\u01d9\3\2\2\2\u0092\u01df\3\2"+
+    "\2\2\u0094\u01e4\3\2\2\2\u0096\u01ee\3\2\2\2\u0098\u021a\3\2\2\2\u009a"+
+    "\u0229\3\2\2\2\u009c\u022d\3\2\2\2\u009e\u00a0\t\2\2\2\u009f\u009e\3\2"+
+    "\2\2\u00a0\u00a1\3\2\2\2\u00a1\u009f\3\2\2\2\u00a1\u00a2\3\2\2\2\u00a2"+
+    "\u00a3\3\2\2\2\u00a3\u00a4\b\2\2\2\u00a4\5\3\2\2\2\u00a5\u00a6\7\61\2"+
+    "\2\u00a6\u00a7\7\61\2\2\u00a7\u00ab\3\2\2\2\u00a8\u00aa\13\2\2\2\u00a9"+
+    "\u00a8\3\2\2\2\u00aa\u00ad\3\2\2\2\u00ab\u00ac\3\2\2\2\u00ab\u00a9\3\2"+
+    "\2\2\u00ac\u00ae\3\2\2\2\u00ad\u00ab\3\2\2\2\u00ae\u00bb\t\3\2\2\u00af"+
+    "\u00b0\7\61\2\2\u00b0\u00b1\7,\2\2\u00b1\u00b5\3\2\2\2\u00b2\u00b4\13"+
+    "\2\2\2\u00b3\u00b2\3\2\2\2\u00b4\u00b7\3\2\2\2\u00b5\u00b6\3\2\2\2\u00b5"+
+    "\u00b3\3\2\2\2\u00b6\u00b8\3\2\2\2\u00b7\u00b5\3\2\2\2\u00b8\u00b9\7,"+
+    "\2\2\u00b9\u00bb\7\61\2\2\u00ba\u00a5\3\2\2\2\u00ba\u00af\3\2\2\2\u00bb"+
+    "\u00bc\3\2\2\2\u00bc\u00bd\b\3\2\2\u00bd\7\3\2\2\2\u00be\u00bf\7}\2\2"+
+    "\u00bf\t\3\2\2\2\u00c0\u00c1\7\177\2\2\u00c1\13\3\2\2\2\u00c2\u00c3\7"+
+    "]\2\2\u00c3\r\3\2\2\2\u00c4\u00c5\7_\2\2\u00c5\17\3\2\2\2\u00c6\u00c7"+
+    "\7*\2\2\u00c7\21\3\2\2\2\u00c8\u00c9\7+\2\2\u00c9\23\3\2\2\2\u00ca\u00cb"+
+    "\7\60\2\2\u00cb\u00cc\3\2\2\2\u00cc\u00cd\b\n\3\2\u00cd\25\3\2\2\2\u00ce"+
+    "\u00cf\7.\2\2\u00cf\27\3\2\2\2\u00d0\u00d1\7=\2\2\u00d1\31\3\2\2\2\u00d2"+
+    "\u00d3\7k\2\2\u00d3\u00d4\7h\2\2\u00d4\33\3\2\2\2\u00d5\u00d6\7g\2\2\u00d6"+
+    "\u00d7\7n\2\2\u00d7\u00d8\7u\2\2\u00d8\u00d9\7g\2\2\u00d9\35\3\2\2\2\u00da"+
+    "\u00db\7y\2\2\u00db\u00dc\7j\2\2\u00dc\u00dd\7k\2\2\u00dd\u00de\7n\2\2"+
+    "\u00de\u00df\7g\2\2\u00df\37\3\2\2\2\u00e0\u00e1\7f\2\2\u00e1\u00e2\7"+
+    "q\2\2\u00e2!\3\2\2\2\u00e3\u00e4\7h\2\2\u00e4\u00e5\7q\2\2\u00e5\u00e6"+
+    "\7t\2\2\u00e6#\3\2\2\2\u00e7\u00e8\7e\2\2\u00e8\u00e9\7q\2\2\u00e9\u00ea"+
+    "\7p\2\2\u00ea\u00eb\7v\2\2\u00eb\u00ec\7k\2\2\u00ec\u00ed\7p\2\2\u00ed"+
+    "\u00ee\7w\2\2\u00ee\u00ef\7g\2\2\u00ef%\3\2\2\2\u00f0\u00f1\7d\2\2\u00f1"+
+    "\u00f2\7t\2\2\u00f2\u00f3\7g\2\2\u00f3\u00f4\7c\2\2\u00f4\u00f5\7m\2\2"+
+    "\u00f5\'\3\2\2\2\u00f6\u00f7\7t\2\2\u00f7\u00f8\7g\2\2\u00f8\u00f9\7v"+
+    "\2\2\u00f9\u00fa\7w\2\2\u00fa\u00fb\7t\2\2\u00fb\u00fc\7p\2\2\u00fc)\3"+
+    "\2\2\2\u00fd\u00fe\7p\2\2\u00fe\u00ff\7g\2\2\u00ff\u0100\7y\2\2\u0100"+
+    "+\3\2\2\2\u0101\u0102\7v\2\2\u0102\u0103\7t\2\2\u0103\u0104\7{\2\2\u0104"+
+    "-\3\2\2\2\u0105\u0106\7e\2\2\u0106\u0107\7c\2\2\u0107\u0108\7v\2\2\u0108"+
+    "\u0109\7e\2\2\u0109\u010a\7j\2\2\u010a/\3\2\2\2\u010b\u010c\7v\2\2\u010c"+
+    "\u010d\7j\2\2\u010d\u010e\7t\2\2\u010e\u010f\7q\2\2\u010f\u0110\7y\2\2"+
+    "\u0110\61\3\2\2\2\u0111\u0112\7#\2\2\u0112\63\3\2\2\2\u0113\u0114\7\u0080"+
+    "\2\2\u0114\65\3\2\2\2\u0115\u0116\7,\2\2\u0116\67\3\2\2\2\u0117\u0118"+
+    "\7\61\2\2\u01189\3\2\2\2\u0119\u011a\7\'\2\2\u011a;\3\2\2\2\u011b\u011c"+
+    "\7-\2\2\u011c=\3\2\2\2\u011d\u011e\7/\2\2\u011e?\3\2\2\2\u011f\u0120\7"+
+    ">\2\2\u0120\u0121\7>\2\2\u0121A\3\2\2\2\u0122\u0123\7@\2\2\u0123\u0124"+
+    "\7@\2\2\u0124C\3\2\2\2\u0125\u0126\7@\2\2\u0126\u0127\7@\2\2\u0127\u0128"+
+    "\7@\2\2\u0128E\3\2\2\2\u0129\u012a\7>\2\2\u012aG\3\2\2\2\u012b\u012c\7"+
+    ">\2\2\u012c\u012d\7?\2\2\u012dI\3\2\2\2\u012e\u012f\7@\2\2\u012fK\3\2"+
+    "\2\2\u0130\u0131\7@\2\2\u0131\u0132\7?\2\2\u0132M\3\2\2\2\u0133\u0134"+
+    "\7?\2\2\u0134\u0135\7?\2\2\u0135O\3\2\2\2\u0136\u0137\7?\2\2\u0137\u0138"+
+    "\7?\2\2\u0138\u0139\7?\2\2\u0139Q\3\2\2\2\u013a\u013b\7#\2\2\u013b\u013c"+
+    "\7?\2\2\u013cS\3\2\2\2\u013d\u013e\7#\2\2\u013e\u013f\7?\2\2\u013f\u0140"+
+    "\7?\2\2\u0140U\3\2\2\2\u0141\u0142\7(\2\2\u0142W\3\2\2\2\u0143\u0144\7"+
+    "`\2\2\u0144Y\3\2\2\2\u0145\u0146\7~\2\2\u0146[\3\2\2\2\u0147\u0148\7("+
+    "\2\2\u0148\u0149\7(\2\2\u0149]\3\2\2\2\u014a\u014b\7~\2\2\u014b\u014c"+
+    "\7~\2\2\u014c_\3\2\2\2\u014d\u014e\7A\2\2\u014ea\3\2\2\2\u014f\u0150\7"+
+    "<\2\2\u0150c\3\2\2\2\u0151\u0152\7-\2\2\u0152\u0153\7-\2\2\u0153e\3\2"+
+    "\2\2\u0154\u0155\7/\2\2\u0155\u0156\7/\2\2\u0156g\3\2\2\2\u0157\u0158"+
+    "\7?\2\2\u0158i\3\2\2\2\u0159\u015a\7-\2\2\u015a\u015b\7?\2\2\u015bk\3"+
+    "\2\2\2\u015c\u015d\7/\2\2\u015d\u015e\7?\2\2\u015em\3\2\2\2\u015f\u0160"+
+    "\7,\2\2\u0160\u0161\7?\2\2\u0161o\3\2\2\2\u0162\u0163\7\61\2\2\u0163\u0164"+
+    "\7?\2\2\u0164q\3\2\2\2\u0165\u0166\7\'\2\2\u0166\u0167\7?\2\2\u0167s\3"+
+    "\2\2\2\u0168\u0169\7(\2\2\u0169\u016a\7?\2\2\u016au\3\2\2\2\u016b\u016c"+
+    "\7`\2\2\u016c\u016d\7?\2\2\u016dw\3\2\2\2\u016e\u016f\7~\2\2\u016f\u0170"+
+    "\7?\2\2\u0170y\3\2\2\2\u0171\u0172\7>\2\2\u0172\u0173\7>\2\2\u0173\u0174"+
+    "\7?\2\2\u0174{\3\2\2\2\u0175\u0176\7@\2\2\u0176\u0177\7@\2\2\u0177\u0178"+
+    "\7?\2\2\u0178}\3\2\2\2\u0179\u017a\7@\2\2\u017a\u017b\7@\2\2\u017b\u017c"+
+    "\7@\2\2\u017c\u017d\7?\2\2\u017d\177\3\2\2\2\u017e\u017f\7\60\2\2\u017f"+
+    "\u0180\7\60\2\2\u0180\u0181\7?\2\2\u0181\u0081\3\2\2\2\u0182\u0184\7\62"+
+    "\2\2\u0183\u0185\t\4\2\2\u0184\u0183\3\2\2\2\u0185\u0186\3\2\2\2\u0186"+
+    "\u0184\3\2\2\2\u0186\u0187\3\2\2\2\u0187\u0189\3\2\2\2\u0188\u018a\t\5"+
+    "\2\2\u0189\u0188\3\2\2\2\u0189\u018a\3\2\2\2\u018a\u0083\3\2\2\2\u018b"+
+    "\u018c\7\62\2\2\u018c\u018e\t\6\2\2\u018d\u018f\t\7\2\2\u018e\u018d\3"+
+    "\2\2\2\u018f\u0190\3\2\2\2\u0190\u018e\3\2\2\2\u0190\u0191\3\2\2\2\u0191"+
+    "\u0193\3\2\2\2\u0192\u0194\t\5\2\2\u0193\u0192\3\2\2\2\u0193\u0194\3\2"+
+    "\2\2\u0194\u0085\3\2\2\2\u0195\u019e\7\62\2\2\u0196\u019a\t\b\2\2\u0197"+
+    "\u0199\t\t\2\2\u0198\u0197\3\2\2\2\u0199\u019c\3\2\2\2\u019a\u0198\3\2"+
+    "\2\2\u019a\u019b\3\2\2\2\u019b\u019e\3\2\2\2\u019c\u019a\3\2\2\2\u019d"+
+    "\u0195\3\2\2\2\u019d\u0196\3\2\2\2\u019e\u01a0\3\2\2\2\u019f\u01a1\t\n"+
+    "\2\2\u01a0\u019f\3\2\2\2\u01a0\u01a1\3\2\2\2\u01a1\u0087\3\2\2\2\u01a2"+
+    "\u01ab\7\62\2\2\u01a3\u01a7\t\b\2\2\u01a4\u01a6\t\t\2\2\u01a5\u01a4\3"+
+    "\2\2\2\u01a6\u01a9\3\2\2\2\u01a7\u01a5\3\2\2\2\u01a7\u01a8\3\2\2\2\u01a8"+
+    "\u01ab\3\2\2\2\u01a9\u01a7\3\2\2\2\u01aa\u01a2\3\2\2\2\u01aa\u01a3\3\2"+
+    "\2\2\u01ab\u01ac\3\2\2\2\u01ac\u01b0\5\24\n\2\u01ad\u01af\t\t\2\2\u01ae"+
+    "\u01ad\3\2\2\2\u01af\u01b2\3\2\2\2\u01b0\u01ae\3\2\2\2\u01b0\u01b1\3\2"+
+    "\2\2\u01b1\u01bc\3\2\2\2\u01b2\u01b0\3\2\2\2\u01b3\u01b5\t\13\2\2\u01b4"+
+    "\u01b6\t\f\2\2\u01b5\u01b4\3\2\2\2\u01b5\u01b6\3\2\2\2\u01b6\u01b8\3\2"+
+    "\2\2\u01b7\u01b9\t\t\2\2\u01b8\u01b7\3\2\2\2\u01b9\u01ba\3\2\2\2\u01ba"+
+    "\u01b8\3\2\2\2\u01ba\u01bb\3\2\2\2\u01bb\u01bd\3\2\2\2\u01bc\u01b3\3\2"+
+    "\2\2\u01bc\u01bd\3\2\2\2\u01bd\u01bf\3\2\2\2\u01be\u01c0\t\r\2\2\u01bf"+
+    "\u01be\3\2\2\2\u01bf\u01c0\3\2\2\2\u01c0\u0089\3\2\2\2\u01c1\u01c9\7$"+
+    "\2\2\u01c2\u01c3\7^\2\2\u01c3\u01c8\7$\2\2\u01c4\u01c5\7^\2\2\u01c5\u01c8"+
+    "\7^\2\2\u01c6\u01c8\n\16\2\2\u01c7\u01c2\3\2\2\2\u01c7\u01c4\3\2\2\2\u01c7"+
+    "\u01c6\3\2\2\2\u01c8\u01cb\3\2\2\2\u01c9\u01ca\3\2\2\2\u01c9\u01c7\3\2"+
+    "\2\2\u01ca\u01cc\3\2\2\2\u01cb\u01c9\3\2\2\2\u01cc\u01cd\7$\2\2\u01cd"+
+    "\u01ce\bE\4\2\u01ce\u008b\3\2\2\2\u01cf\u01d0\7)\2\2\u01d0\u01d1\13\2"+
+    "\2\2\u01d1\u01d2\7)\2\2\u01d2\u01d3\bF\5\2\u01d3\u008d\3\2\2\2\u01d4\u01d5"+
+    "\7v\2\2\u01d5\u01d6\7t\2\2\u01d6\u01d7\7w\2\2\u01d7\u01d8\7g\2\2\u01d8"+
+    "\u008f\3\2\2\2\u01d9\u01da\7h\2\2\u01da\u01db\7c\2\2\u01db\u01dc\7n\2"+
+    "\2\u01dc\u01dd\7u\2\2\u01dd\u01de\7g\2\2\u01de\u0091\3\2\2\2\u01df\u01e0"+
+    "\7p\2\2\u01e0\u01e1\7w\2\2\u01e1\u01e2\7n\2\2\u01e2\u01e3\7n\2\2\u01e3"+
+    "\u0093\3\2\2\2\u01e4\u01e6\5\u0098L\2\u01e5\u01e7\5\u0096K\2\u01e6\u01e5"+
+    "\3\2\2\2\u01e6\u01e7\3\2\2\2\u01e7\u01e8\3\2\2\2\u01e8\u01e9\6J\2\2\u01e9"+
+    "\u01ea\bJ\6\2\u01ea\u0095\3\2\2\2\u01eb\u01ed\7\"\2\2\u01ec\u01eb\3\2"+
+    "\2\2\u01ed\u01f0\3\2\2\2\u01ee\u01ec\3\2\2\2\u01ee\u01ef\3\2\2\2\u01ef"+
+    "\u01f1\3\2\2\2\u01f0\u01ee\3\2\2\2\u01f1\u01f5\7>\2\2\u01f2\u01f4\7\""+
+    "\2\2\u01f3\u01f2\3\2\2\2\u01f4\u01f7\3\2\2\2\u01f5\u01f3\3\2\2\2\u01f5"+
+    "\u01f6\3\2\2\2\u01f6\u01f8\3\2\2\2\u01f7\u01f5\3\2\2\2\u01f8\u01fa\5\u0098"+
+    "L\2\u01f9\u01fb\5\u0096K\2\u01fa\u01f9\3\2\2\2\u01fa\u01fb\3\2\2\2\u01fb"+
+    "\u01ff\3\2\2\2\u01fc\u01fe\7\"\2\2\u01fd\u01fc\3\2\2\2\u01fe\u0201\3\2"+
+    "\2\2\u01ff\u01fd\3\2\2\2\u01ff\u0200\3\2\2\2\u0200\u0215\3\2\2\2\u0201"+
+    "\u01ff\3\2\2\2\u0202\u0206\5\26\13\2\u0203\u0205\7\"\2\2\u0204\u0203\3"+
+    "\2\2\2\u0205\u0208\3\2\2\2\u0206\u0204\3\2\2\2\u0206\u0207\3\2\2\2\u0207"+
+    "\u0209\3\2\2\2\u0208\u0206\3\2\2\2\u0209\u020b\5\u0098L\2\u020a\u020c"+
+    "\5\u0096K\2\u020b\u020a\3\2\2\2\u020b\u020c\3\2\2\2\u020c\u0210\3\2\2"+
+    "\2\u020d\u020f\7\"\2\2\u020e\u020d\3\2\2\2\u020f\u0212\3\2\2\2\u0210\u020e"+
+    "\3\2\2\2\u0210\u0211\3\2\2\2\u0211\u0214\3\2\2\2\u0212\u0210\3\2\2\2\u0213"+
+    "\u0202\3\2\2\2\u0214\u0217\3\2\2\2\u0215\u0213\3\2\2\2\u0215\u0216\3\2"+
+    "\2\2\u0216\u0218\3\2\2\2\u0217\u0215\3\2\2\2\u0218\u0219\7@\2\2\u0219"+
+    "\u0097\3\2\2\2\u021a\u021e\t\17\2\2\u021b\u021d\t\20\2\2\u021c\u021b\3"+
+    "\2\2\2\u021d\u0220\3\2\2\2\u021e\u021c\3\2\2\2\u021e\u021f\3\2\2\2\u021f"+
+    "\u0099\3\2\2\2\u0220\u021e\3\2\2\2\u0221\u022a\7\62\2\2\u0222\u0226\t"+
+    "\b\2\2\u0223\u0225\t\t\2\2\u0224\u0223\3\2\2\2\u0225\u0228\3\2\2\2\u0226"+
+    "\u0224\3\2\2\2\u0226\u0227\3\2\2\2\u0227\u022a\3\2\2\2\u0228\u0226\3\2"+
+    "\2\2\u0229\u0221\3\2\2\2\u0229\u0222\3\2\2\2\u022a\u022b\3\2\2\2\u022b"+
+    "\u022c\bM\7\2\u022c\u009b\3\2\2\2\u022d\u0231\t\17\2\2\u022e\u0230\t\20"+
+    "\2\2\u022f\u022e\3\2\2\2\u0230\u0233\3\2\2\2\u0231\u022f\3\2\2\2\u0231"+
+    "\u0232\3\2\2\2\u0232\u0234\3\2\2\2\u0233\u0231\3\2\2\2\u0234\u0235\bN"+
+    "\7\2\u0235\u009d\3\2\2\2%\2\3\u00a1\u00ab\u00b5\u00ba\u0186\u0189\u0190"+
+    "\u0193\u019a\u019d\u01a0\u01a7\u01aa\u01b0\u01b5\u01ba\u01bc\u01bf\u01c7"+
+    "\u01c9\u01e6\u01ee\u01f5\u01fa\u01ff\u0206\u020b\u0210\u0215\u021e\u0226"+
+    "\u0229\u0231\b\b\2\2\4\3\2\3E\2\3F\3\3J\4\4\2\2";
+  public static final ATN _ATN =
+    new ATNDeserializer().deserialize(_serializedATN.toCharArray());
+  static {
+    _decisionToDFA = new DFA[_ATN.getNumberOfDecisions()];
+    for (int i = 0; i < _ATN.getNumberOfDecisions(); i++) {
+      _decisionToDFA[i] = new DFA(_ATN.getDecisionState(i), i);
+    }
+  }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParser.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParser.java
new file mode 100644
index 0000000..13f61ac
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParser.java
@@ -0,0 +1,2884 @@
+// ANTLR GENERATED CODE: DO NOT EDIT
+package org.elasticsearch.plan.a;
+import org.antlr.v4.runtime.atn.*;
+import org.antlr.v4.runtime.dfa.DFA;
+import org.antlr.v4.runtime.*;
+import org.antlr.v4.runtime.misc.*;
+import org.antlr.v4.runtime.tree.*;
+import java.util.List;
+import java.util.Iterator;
+import java.util.ArrayList;
+
+@SuppressWarnings({"all", "warnings", "unchecked", "unused", "cast"})
+class PlanAParser extends Parser {
+  static { RuntimeMetaData.checkVersion("4.5.1", RuntimeMetaData.VERSION); }
+
+  protected static final DFA[] _decisionToDFA;
+  protected static final PredictionContextCache _sharedContextCache =
+    new PredictionContextCache();
+  public static final int
+    WS=1, COMMENT=2, LBRACK=3, RBRACK=4, LBRACE=5, RBRACE=6, LP=7, RP=8, DOT=9, 
+    COMMA=10, SEMICOLON=11, IF=12, ELSE=13, WHILE=14, DO=15, FOR=16, CONTINUE=17, 
+    BREAK=18, RETURN=19, NEW=20, TRY=21, CATCH=22, THROW=23, BOOLNOT=24, BWNOT=25, 
+    MUL=26, DIV=27, REM=28, ADD=29, SUB=30, LSH=31, RSH=32, USH=33, LT=34, 
+    LTE=35, GT=36, GTE=37, EQ=38, EQR=39, NE=40, NER=41, BWAND=42, BWXOR=43, 
+    BWOR=44, BOOLAND=45, BOOLOR=46, COND=47, COLON=48, INCR=49, DECR=50, ASSIGN=51, 
+    AADD=52, ASUB=53, AMUL=54, ADIV=55, AREM=56, AAND=57, AXOR=58, AOR=59, 
+    ALSH=60, ARSH=61, AUSH=62, ACAT=63, OCTAL=64, HEX=65, INTEGER=66, DECIMAL=67, 
+    STRING=68, CHAR=69, TRUE=70, FALSE=71, NULL=72, TYPE=73, ID=74, EXTINTEGER=75, 
+    EXTID=76;
+  public static final int
+    RULE_source = 0, RULE_statement = 1, RULE_block = 2, RULE_empty = 3, RULE_initializer = 4, 
+    RULE_afterthought = 5, RULE_declaration = 6, RULE_decltype = 7, RULE_declvar = 8, 
+    RULE_expression = 9, RULE_extstart = 10, RULE_extprec = 11, RULE_extcast = 12, 
+    RULE_extbrace = 13, RULE_extdot = 14, RULE_exttype = 15, RULE_extcall = 16, 
+    RULE_extvar = 17, RULE_extfield = 18, RULE_extnew = 19, RULE_extstring = 20, 
+    RULE_arguments = 21, RULE_increment = 22;
+  public static final String[] ruleNames = {
+    "source", "statement", "block", "empty", "initializer", "afterthought", 
+    "declaration", "decltype", "declvar", "expression", "extstart", "extprec", 
+    "extcast", "extbrace", "extdot", "exttype", "extcall", "extvar", "extfield", 
+    "extnew", "extstring", "arguments", "increment"
+  };
+
+  private static final String[] _LITERAL_NAMES = {
+    null, null, null, "'{'", "'}'", "'['", "']'", "'('", "')'", "'.'", "','", 
+    "';'", "'if'", "'else'", "'while'", "'do'", "'for'", "'continue'", "'break'", 
+    "'return'", "'new'", "'try'", "'catch'", "'throw'", "'!'", "'~'", "'*'", 
+    "'/'", "'%'", "'+'", "'-'", "'<<'", "'>>'", "'>>>'", "'<'", "'<='", "'>'", 
+    "'>='", "'=='", "'==='", "'!='", "'!=='", "'&'", "'^'", "'|'", "'&&'", 
+    "'||'", "'?'", "':'", "'++'", "'--'", "'='", "'+='", "'-='", "'*='", "'/='", 
+    "'%='", "'&='", "'^='", "'|='", "'<<='", "'>>='", "'>>>='", "'..='", null, 
+    null, null, null, null, null, "'true'", "'false'", "'null'"
+  };
+  private static final String[] _SYMBOLIC_NAMES = {
+    null, "WS", "COMMENT", "LBRACK", "RBRACK", "LBRACE", "RBRACE", "LP", "RP", 
+    "DOT", "COMMA", "SEMICOLON", "IF", "ELSE", "WHILE", "DO", "FOR", "CONTINUE", 
+    "BREAK", "RETURN", "NEW", "TRY", "CATCH", "THROW", "BOOLNOT", "BWNOT", 
+    "MUL", "DIV", "REM", "ADD", "SUB", "LSH", "RSH", "USH", "LT", "LTE", "GT", 
+    "GTE", "EQ", "EQR", "NE", "NER", "BWAND", "BWXOR", "BWOR", "BOOLAND", 
+    "BOOLOR", "COND", "COLON", "INCR", "DECR", "ASSIGN", "AADD", "ASUB", "AMUL", 
+    "ADIV", "AREM", "AAND", "AXOR", "AOR", "ALSH", "ARSH", "AUSH", "ACAT", 
+    "OCTAL", "HEX", "INTEGER", "DECIMAL", "STRING", "CHAR", "TRUE", "FALSE", 
+    "NULL", "TYPE", "ID", "EXTINTEGER", "EXTID"
+  };
+  public static final Vocabulary VOCABULARY = new VocabularyImpl(_LITERAL_NAMES, _SYMBOLIC_NAMES);
+
+  /**
+   * @deprecated Use {@link #VOCABULARY} instead.
+   */
+  @Deprecated
+  public static final String[] tokenNames;
+  static {
+    tokenNames = new String[_SYMBOLIC_NAMES.length];
+    for (int i = 0; i < tokenNames.length; i++) {
+      tokenNames[i] = VOCABULARY.getLiteralName(i);
+      if (tokenNames[i] == null) {
+        tokenNames[i] = VOCABULARY.getSymbolicName(i);
+      }
+
+      if (tokenNames[i] == null) {
+        tokenNames[i] = "<INVALID>";
+      }
+    }
+  }
+
+  @Override
+  @Deprecated
+  public String[] getTokenNames() {
+    return tokenNames;
+  }
+
+  @Override
+
+  public Vocabulary getVocabulary() {
+    return VOCABULARY;
+  }
+
+  @Override
+  public String getGrammarFileName() { return "PlanAParser.g4"; }
+
+  @Override
+  public String[] getRuleNames() { return ruleNames; }
+
+  @Override
+  public String getSerializedATN() { return _serializedATN; }
+
+  @Override
+  public ATN getATN() { return _ATN; }
+
+  public PlanAParser(TokenStream input) {
+    super(input);
+    _interp = new ParserATNSimulator(this,_ATN,_decisionToDFA,_sharedContextCache);
+  }
+  public static class SourceContext extends ParserRuleContext {
+    public TerminalNode EOF() { return getToken(PlanAParser.EOF, 0); }
+    public List<StatementContext> statement() {
+      return getRuleContexts(StatementContext.class);
+    }
+    public StatementContext statement(int i) {
+      return getRuleContext(StatementContext.class,i);
+    }
+    public SourceContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_source; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitSource(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final SourceContext source() throws RecognitionException {
+    SourceContext _localctx = new SourceContext(_ctx, getState());
+    enterRule(_localctx, 0, RULE_source);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(47); 
+      _errHandler.sync(this);
+      _la = _input.LA(1);
+      do {
+        {
+        {
+        setState(46);
+        statement();
+        }
+        }
+        setState(49); 
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+      } while ( (((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << IF) | (1L << WHILE) | (1L << DO) | (1L << FOR) | (1L << CONTINUE) | (1L << BREAK) | (1L << RETURN) | (1L << NEW) | (1L << TRY) | (1L << THROW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0) );
+      setState(51);
+      match(EOF);
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class StatementContext extends ParserRuleContext {
+    public StatementContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_statement; }
+   
+    public StatementContext() { }
+    public void copyFrom(StatementContext ctx) {
+      super.copyFrom(ctx);
+    }
+  }
+  public static class DeclContext extends StatementContext {
+    public DeclarationContext declaration() {
+      return getRuleContext(DeclarationContext.class,0);
+    }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public DeclContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDecl(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class BreakContext extends StatementContext {
+    public TerminalNode BREAK() { return getToken(PlanAParser.BREAK, 0); }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public BreakContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitBreak(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ThrowContext extends StatementContext {
+    public TerminalNode THROW() { return getToken(PlanAParser.THROW, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public ThrowContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitThrow(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ContinueContext extends StatementContext {
+    public TerminalNode CONTINUE() { return getToken(PlanAParser.CONTINUE, 0); }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public ContinueContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitContinue(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ForContext extends StatementContext {
+    public TerminalNode FOR() { return getToken(PlanAParser.FOR, 0); }
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public List<TerminalNode> SEMICOLON() { return getTokens(PlanAParser.SEMICOLON); }
+    public TerminalNode SEMICOLON(int i) {
+      return getToken(PlanAParser.SEMICOLON, i);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public BlockContext block() {
+      return getRuleContext(BlockContext.class,0);
+    }
+    public EmptyContext empty() {
+      return getRuleContext(EmptyContext.class,0);
+    }
+    public InitializerContext initializer() {
+      return getRuleContext(InitializerContext.class,0);
+    }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public AfterthoughtContext afterthought() {
+      return getRuleContext(AfterthoughtContext.class,0);
+    }
+    public ForContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitFor(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class TryContext extends StatementContext {
+    public TerminalNode TRY() { return getToken(PlanAParser.TRY, 0); }
+    public List<BlockContext> block() {
+      return getRuleContexts(BlockContext.class);
+    }
+    public BlockContext block(int i) {
+      return getRuleContext(BlockContext.class,i);
+    }
+    public List<TerminalNode> CATCH() { return getTokens(PlanAParser.CATCH); }
+    public TerminalNode CATCH(int i) {
+      return getToken(PlanAParser.CATCH, i);
+    }
+    public List<TerminalNode> LP() { return getTokens(PlanAParser.LP); }
+    public TerminalNode LP(int i) {
+      return getToken(PlanAParser.LP, i);
+    }
+    public List<TerminalNode> RP() { return getTokens(PlanAParser.RP); }
+    public TerminalNode RP(int i) {
+      return getToken(PlanAParser.RP, i);
+    }
+    public List<TerminalNode> TYPE() { return getTokens(PlanAParser.TYPE); }
+    public TerminalNode TYPE(int i) {
+      return getToken(PlanAParser.TYPE, i);
+    }
+    public List<TerminalNode> ID() { return getTokens(PlanAParser.ID); }
+    public TerminalNode ID(int i) {
+      return getToken(PlanAParser.ID, i);
+    }
+    public TryContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitTry(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ExprContext extends StatementContext {
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public ExprContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExpr(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class DoContext extends StatementContext {
+    public TerminalNode DO() { return getToken(PlanAParser.DO, 0); }
+    public BlockContext block() {
+      return getRuleContext(BlockContext.class,0);
+    }
+    public TerminalNode WHILE() { return getToken(PlanAParser.WHILE, 0); }
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public DoContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDo(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class WhileContext extends StatementContext {
+    public TerminalNode WHILE() { return getToken(PlanAParser.WHILE, 0); }
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public BlockContext block() {
+      return getRuleContext(BlockContext.class,0);
+    }
+    public EmptyContext empty() {
+      return getRuleContext(EmptyContext.class,0);
+    }
+    public WhileContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitWhile(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class IfContext extends StatementContext {
+    public TerminalNode IF() { return getToken(PlanAParser.IF, 0); }
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public List<BlockContext> block() {
+      return getRuleContexts(BlockContext.class);
+    }
+    public BlockContext block(int i) {
+      return getRuleContext(BlockContext.class,i);
+    }
+    public TerminalNode ELSE() { return getToken(PlanAParser.ELSE, 0); }
+    public IfContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitIf(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ReturnContext extends StatementContext {
+    public TerminalNode RETURN() { return getToken(PlanAParser.RETURN, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public ReturnContext(StatementContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitReturn(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final StatementContext statement() throws RecognitionException {
+    StatementContext _localctx = new StatementContext(_ctx, getState());
+    enterRule(_localctx, 2, RULE_statement);
+    int _la;
+    try {
+      int _alt;
+      setState(136);
+      switch ( getInterpreter().adaptivePredict(_input,15,_ctx) ) {
+      case 1:
+        _localctx = new IfContext(_localctx);
+        enterOuterAlt(_localctx, 1);
+        {
+        setState(53);
+        match(IF);
+        setState(54);
+        match(LP);
+        setState(55);
+        expression(0);
+        setState(56);
+        match(RP);
+        setState(57);
+        block();
+        setState(60);
+        switch ( getInterpreter().adaptivePredict(_input,1,_ctx) ) {
+        case 1:
+          {
+          setState(58);
+          match(ELSE);
+          setState(59);
+          block();
+          }
+          break;
+        }
+        }
+        break;
+      case 2:
+        _localctx = new WhileContext(_localctx);
+        enterOuterAlt(_localctx, 2);
+        {
+        setState(62);
+        match(WHILE);
+        setState(63);
+        match(LP);
+        setState(64);
+        expression(0);
+        setState(65);
+        match(RP);
+        setState(68);
+        switch (_input.LA(1)) {
+        case LBRACK:
+        case LP:
+        case IF:
+        case WHILE:
+        case DO:
+        case FOR:
+        case CONTINUE:
+        case BREAK:
+        case RETURN:
+        case NEW:
+        case TRY:
+        case THROW:
+        case BOOLNOT:
+        case BWNOT:
+        case ADD:
+        case SUB:
+        case INCR:
+        case DECR:
+        case OCTAL:
+        case HEX:
+        case INTEGER:
+        case DECIMAL:
+        case STRING:
+        case CHAR:
+        case TRUE:
+        case FALSE:
+        case NULL:
+        case TYPE:
+        case ID:
+          {
+          setState(66);
+          block();
+          }
+          break;
+        case SEMICOLON:
+          {
+          setState(67);
+          empty();
+          }
+          break;
+        default:
+          throw new NoViableAltException(this);
+        }
+        }
+        break;
+      case 3:
+        _localctx = new DoContext(_localctx);
+        enterOuterAlt(_localctx, 3);
+        {
+        setState(70);
+        match(DO);
+        setState(71);
+        block();
+        setState(72);
+        match(WHILE);
+        setState(73);
+        match(LP);
+        setState(74);
+        expression(0);
+        setState(75);
+        match(RP);
+        setState(77);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(76);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 4:
+        _localctx = new ForContext(_localctx);
+        enterOuterAlt(_localctx, 4);
+        {
+        setState(79);
+        match(FOR);
+        setState(80);
+        match(LP);
+        setState(82);
+        _la = _input.LA(1);
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+          {
+          setState(81);
+          initializer();
+          }
+        }
+
+        setState(84);
+        match(SEMICOLON);
+        setState(86);
+        _la = _input.LA(1);
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+          {
+          setState(85);
+          expression(0);
+          }
+        }
+
+        setState(88);
+        match(SEMICOLON);
+        setState(90);
+        _la = _input.LA(1);
+        if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+          {
+          setState(89);
+          afterthought();
+          }
+        }
+
+        setState(92);
+        match(RP);
+        setState(95);
+        switch (_input.LA(1)) {
+        case LBRACK:
+        case LP:
+        case IF:
+        case WHILE:
+        case DO:
+        case FOR:
+        case CONTINUE:
+        case BREAK:
+        case RETURN:
+        case NEW:
+        case TRY:
+        case THROW:
+        case BOOLNOT:
+        case BWNOT:
+        case ADD:
+        case SUB:
+        case INCR:
+        case DECR:
+        case OCTAL:
+        case HEX:
+        case INTEGER:
+        case DECIMAL:
+        case STRING:
+        case CHAR:
+        case TRUE:
+        case FALSE:
+        case NULL:
+        case TYPE:
+        case ID:
+          {
+          setState(93);
+          block();
+          }
+          break;
+        case SEMICOLON:
+          {
+          setState(94);
+          empty();
+          }
+          break;
+        default:
+          throw new NoViableAltException(this);
+        }
+        }
+        break;
+      case 5:
+        _localctx = new DeclContext(_localctx);
+        enterOuterAlt(_localctx, 5);
+        {
+        setState(97);
+        declaration();
+        setState(99);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(98);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 6:
+        _localctx = new ContinueContext(_localctx);
+        enterOuterAlt(_localctx, 6);
+        {
+        setState(101);
+        match(CONTINUE);
+        setState(103);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(102);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 7:
+        _localctx = new BreakContext(_localctx);
+        enterOuterAlt(_localctx, 7);
+        {
+        setState(105);
+        match(BREAK);
+        setState(107);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(106);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 8:
+        _localctx = new ReturnContext(_localctx);
+        enterOuterAlt(_localctx, 8);
+        {
+        setState(109);
+        match(RETURN);
+        setState(110);
+        expression(0);
+        setState(112);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(111);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 9:
+        _localctx = new TryContext(_localctx);
+        enterOuterAlt(_localctx, 9);
+        {
+        setState(114);
+        match(TRY);
+        setState(115);
+        block();
+        setState(123); 
+        _errHandler.sync(this);
+        _alt = 1;
+        do {
+          switch (_alt) {
+          case 1:
+            {
+            {
+            setState(116);
+            match(CATCH);
+            setState(117);
+            match(LP);
+            {
+            setState(118);
+            match(TYPE);
+            setState(119);
+            match(ID);
+            }
+            setState(121);
+            match(RP);
+            setState(122);
+            block();
+            }
+            }
+            break;
+          default:
+            throw new NoViableAltException(this);
+          }
+          setState(125); 
+          _errHandler.sync(this);
+          _alt = getInterpreter().adaptivePredict(_input,12,_ctx);
+        } while ( _alt!=2 && _alt!=org.antlr.v4.runtime.atn.ATN.INVALID_ALT_NUMBER );
+        }
+        break;
+      case 10:
+        _localctx = new ThrowContext(_localctx);
+        enterOuterAlt(_localctx, 10);
+        {
+        setState(127);
+        match(THROW);
+        setState(128);
+        expression(0);
+        setState(130);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(129);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      case 11:
+        _localctx = new ExprContext(_localctx);
+        enterOuterAlt(_localctx, 11);
+        {
+        setState(132);
+        expression(0);
+        setState(134);
+        _la = _input.LA(1);
+        if (_la==SEMICOLON) {
+          {
+          setState(133);
+          match(SEMICOLON);
+          }
+        }
+
+        }
+        break;
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class BlockContext extends ParserRuleContext {
+    public BlockContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_block; }
+   
+    public BlockContext() { }
+    public void copyFrom(BlockContext ctx) {
+      super.copyFrom(ctx);
+    }
+  }
+  public static class SingleContext extends BlockContext {
+    public StatementContext statement() {
+      return getRuleContext(StatementContext.class,0);
+    }
+    public SingleContext(BlockContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitSingle(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class MultipleContext extends BlockContext {
+    public TerminalNode LBRACK() { return getToken(PlanAParser.LBRACK, 0); }
+    public TerminalNode RBRACK() { return getToken(PlanAParser.RBRACK, 0); }
+    public List<StatementContext> statement() {
+      return getRuleContexts(StatementContext.class);
+    }
+    public StatementContext statement(int i) {
+      return getRuleContext(StatementContext.class,i);
+    }
+    public MultipleContext(BlockContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitMultiple(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final BlockContext block() throws RecognitionException {
+    BlockContext _localctx = new BlockContext(_ctx, getState());
+    enterRule(_localctx, 4, RULE_block);
+    int _la;
+    try {
+      setState(147);
+      switch (_input.LA(1)) {
+      case LBRACK:
+        _localctx = new MultipleContext(_localctx);
+        enterOuterAlt(_localctx, 1);
+        {
+        setState(138);
+        match(LBRACK);
+        setState(142);
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+        while ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << IF) | (1L << WHILE) | (1L << DO) | (1L << FOR) | (1L << CONTINUE) | (1L << BREAK) | (1L << RETURN) | (1L << NEW) | (1L << TRY) | (1L << THROW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+          {
+          {
+          setState(139);
+          statement();
+          }
+          }
+          setState(144);
+          _errHandler.sync(this);
+          _la = _input.LA(1);
+        }
+        setState(145);
+        match(RBRACK);
+        }
+        break;
+      case LP:
+      case IF:
+      case WHILE:
+      case DO:
+      case FOR:
+      case CONTINUE:
+      case BREAK:
+      case RETURN:
+      case NEW:
+      case TRY:
+      case THROW:
+      case BOOLNOT:
+      case BWNOT:
+      case ADD:
+      case SUB:
+      case INCR:
+      case DECR:
+      case OCTAL:
+      case HEX:
+      case INTEGER:
+      case DECIMAL:
+      case STRING:
+      case CHAR:
+      case TRUE:
+      case FALSE:
+      case NULL:
+      case TYPE:
+      case ID:
+        _localctx = new SingleContext(_localctx);
+        enterOuterAlt(_localctx, 2);
+        {
+        setState(146);
+        statement();
+        }
+        break;
+      default:
+        throw new NoViableAltException(this);
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class EmptyContext extends ParserRuleContext {
+    public TerminalNode SEMICOLON() { return getToken(PlanAParser.SEMICOLON, 0); }
+    public EmptyContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_empty; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitEmpty(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final EmptyContext empty() throws RecognitionException {
+    EmptyContext _localctx = new EmptyContext(_ctx, getState());
+    enterRule(_localctx, 6, RULE_empty);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(149);
+      match(SEMICOLON);
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class InitializerContext extends ParserRuleContext {
+    public DeclarationContext declaration() {
+      return getRuleContext(DeclarationContext.class,0);
+    }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public InitializerContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_initializer; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitInitializer(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final InitializerContext initializer() throws RecognitionException {
+    InitializerContext _localctx = new InitializerContext(_ctx, getState());
+    enterRule(_localctx, 8, RULE_initializer);
+    try {
+      setState(153);
+      switch ( getInterpreter().adaptivePredict(_input,18,_ctx) ) {
+      case 1:
+        enterOuterAlt(_localctx, 1);
+        {
+        setState(151);
+        declaration();
+        }
+        break;
+      case 2:
+        enterOuterAlt(_localctx, 2);
+        {
+        setState(152);
+        expression(0);
+        }
+        break;
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class AfterthoughtContext extends ParserRuleContext {
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public AfterthoughtContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_afterthought; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitAfterthought(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final AfterthoughtContext afterthought() throws RecognitionException {
+    AfterthoughtContext _localctx = new AfterthoughtContext(_ctx, getState());
+    enterRule(_localctx, 10, RULE_afterthought);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(155);
+      expression(0);
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class DeclarationContext extends ParserRuleContext {
+    public DecltypeContext decltype() {
+      return getRuleContext(DecltypeContext.class,0);
+    }
+    public List<DeclvarContext> declvar() {
+      return getRuleContexts(DeclvarContext.class);
+    }
+    public DeclvarContext declvar(int i) {
+      return getRuleContext(DeclvarContext.class,i);
+    }
+    public List<TerminalNode> COMMA() { return getTokens(PlanAParser.COMMA); }
+    public TerminalNode COMMA(int i) {
+      return getToken(PlanAParser.COMMA, i);
+    }
+    public DeclarationContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_declaration; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDeclaration(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final DeclarationContext declaration() throws RecognitionException {
+    DeclarationContext _localctx = new DeclarationContext(_ctx, getState());
+    enterRule(_localctx, 12, RULE_declaration);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(157);
+      decltype();
+      setState(158);
+      declvar();
+      setState(163);
+      _errHandler.sync(this);
+      _la = _input.LA(1);
+      while (_la==COMMA) {
+        {
+        {
+        setState(159);
+        match(COMMA);
+        setState(160);
+        declvar();
+        }
+        }
+        setState(165);
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class DecltypeContext extends ParserRuleContext {
+    public TerminalNode TYPE() { return getToken(PlanAParser.TYPE, 0); }
+    public List<TerminalNode> LBRACE() { return getTokens(PlanAParser.LBRACE); }
+    public TerminalNode LBRACE(int i) {
+      return getToken(PlanAParser.LBRACE, i);
+    }
+    public List<TerminalNode> RBRACE() { return getTokens(PlanAParser.RBRACE); }
+    public TerminalNode RBRACE(int i) {
+      return getToken(PlanAParser.RBRACE, i);
+    }
+    public DecltypeContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_decltype; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDecltype(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final DecltypeContext decltype() throws RecognitionException {
+    DecltypeContext _localctx = new DecltypeContext(_ctx, getState());
+    enterRule(_localctx, 14, RULE_decltype);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(166);
+      match(TYPE);
+      setState(171);
+      _errHandler.sync(this);
+      _la = _input.LA(1);
+      while (_la==LBRACE) {
+        {
+        {
+        setState(167);
+        match(LBRACE);
+        setState(168);
+        match(RBRACE);
+        }
+        }
+        setState(173);
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class DeclvarContext extends ParserRuleContext {
+    public TerminalNode ID() { return getToken(PlanAParser.ID, 0); }
+    public TerminalNode ASSIGN() { return getToken(PlanAParser.ASSIGN, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public DeclvarContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_declvar; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitDeclvar(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final DeclvarContext declvar() throws RecognitionException {
+    DeclvarContext _localctx = new DeclvarContext(_ctx, getState());
+    enterRule(_localctx, 16, RULE_declvar);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(174);
+      match(ID);
+      setState(177);
+      _la = _input.LA(1);
+      if (_la==ASSIGN) {
+        {
+        setState(175);
+        match(ASSIGN);
+        setState(176);
+        expression(0);
+        }
+      }
+
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExpressionContext extends ParserRuleContext {
+    public ExpressionContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_expression; }
+   
+    public ExpressionContext() { }
+    public void copyFrom(ExpressionContext ctx) {
+      super.copyFrom(ctx);
+    }
+  }
+  public static class CompContext extends ExpressionContext {
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public TerminalNode LT() { return getToken(PlanAParser.LT, 0); }
+    public TerminalNode LTE() { return getToken(PlanAParser.LTE, 0); }
+    public TerminalNode GT() { return getToken(PlanAParser.GT, 0); }
+    public TerminalNode GTE() { return getToken(PlanAParser.GTE, 0); }
+    public TerminalNode EQ() { return getToken(PlanAParser.EQ, 0); }
+    public TerminalNode EQR() { return getToken(PlanAParser.EQR, 0); }
+    public TerminalNode NE() { return getToken(PlanAParser.NE, 0); }
+    public TerminalNode NER() { return getToken(PlanAParser.NER, 0); }
+    public CompContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitComp(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class BoolContext extends ExpressionContext {
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public TerminalNode BOOLAND() { return getToken(PlanAParser.BOOLAND, 0); }
+    public TerminalNode BOOLOR() { return getToken(PlanAParser.BOOLOR, 0); }
+    public BoolContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitBool(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ConditionalContext extends ExpressionContext {
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public TerminalNode COND() { return getToken(PlanAParser.COND, 0); }
+    public TerminalNode COLON() { return getToken(PlanAParser.COLON, 0); }
+    public ConditionalContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitConditional(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class AssignmentContext extends ExpressionContext {
+    public ExtstartContext extstart() {
+      return getRuleContext(ExtstartContext.class,0);
+    }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode ASSIGN() { return getToken(PlanAParser.ASSIGN, 0); }
+    public TerminalNode AADD() { return getToken(PlanAParser.AADD, 0); }
+    public TerminalNode ASUB() { return getToken(PlanAParser.ASUB, 0); }
+    public TerminalNode AMUL() { return getToken(PlanAParser.AMUL, 0); }
+    public TerminalNode ADIV() { return getToken(PlanAParser.ADIV, 0); }
+    public TerminalNode AREM() { return getToken(PlanAParser.AREM, 0); }
+    public TerminalNode AAND() { return getToken(PlanAParser.AAND, 0); }
+    public TerminalNode AXOR() { return getToken(PlanAParser.AXOR, 0); }
+    public TerminalNode AOR() { return getToken(PlanAParser.AOR, 0); }
+    public TerminalNode ALSH() { return getToken(PlanAParser.ALSH, 0); }
+    public TerminalNode ARSH() { return getToken(PlanAParser.ARSH, 0); }
+    public TerminalNode AUSH() { return getToken(PlanAParser.AUSH, 0); }
+    public AssignmentContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitAssignment(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class FalseContext extends ExpressionContext {
+    public TerminalNode FALSE() { return getToken(PlanAParser.FALSE, 0); }
+    public FalseContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitFalse(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class NumericContext extends ExpressionContext {
+    public TerminalNode OCTAL() { return getToken(PlanAParser.OCTAL, 0); }
+    public TerminalNode HEX() { return getToken(PlanAParser.HEX, 0); }
+    public TerminalNode INTEGER() { return getToken(PlanAParser.INTEGER, 0); }
+    public TerminalNode DECIMAL() { return getToken(PlanAParser.DECIMAL, 0); }
+    public NumericContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitNumeric(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class UnaryContext extends ExpressionContext {
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode BOOLNOT() { return getToken(PlanAParser.BOOLNOT, 0); }
+    public TerminalNode BWNOT() { return getToken(PlanAParser.BWNOT, 0); }
+    public TerminalNode ADD() { return getToken(PlanAParser.ADD, 0); }
+    public TerminalNode SUB() { return getToken(PlanAParser.SUB, 0); }
+    public UnaryContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitUnary(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class PrecedenceContext extends ExpressionContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public PrecedenceContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitPrecedence(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class PreincContext extends ExpressionContext {
+    public IncrementContext increment() {
+      return getRuleContext(IncrementContext.class,0);
+    }
+    public ExtstartContext extstart() {
+      return getRuleContext(ExtstartContext.class,0);
+    }
+    public PreincContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitPreinc(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class PostincContext extends ExpressionContext {
+    public ExtstartContext extstart() {
+      return getRuleContext(ExtstartContext.class,0);
+    }
+    public IncrementContext increment() {
+      return getRuleContext(IncrementContext.class,0);
+    }
+    public PostincContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitPostinc(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class CastContext extends ExpressionContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public DecltypeContext decltype() {
+      return getRuleContext(DecltypeContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public CastContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitCast(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class ExternalContext extends ExpressionContext {
+    public ExtstartContext extstart() {
+      return getRuleContext(ExtstartContext.class,0);
+    }
+    public ExternalContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExternal(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class NullContext extends ExpressionContext {
+    public TerminalNode NULL() { return getToken(PlanAParser.NULL, 0); }
+    public NullContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitNull(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class BinaryContext extends ExpressionContext {
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public TerminalNode MUL() { return getToken(PlanAParser.MUL, 0); }
+    public TerminalNode DIV() { return getToken(PlanAParser.DIV, 0); }
+    public TerminalNode REM() { return getToken(PlanAParser.REM, 0); }
+    public TerminalNode ADD() { return getToken(PlanAParser.ADD, 0); }
+    public TerminalNode SUB() { return getToken(PlanAParser.SUB, 0); }
+    public TerminalNode LSH() { return getToken(PlanAParser.LSH, 0); }
+    public TerminalNode RSH() { return getToken(PlanAParser.RSH, 0); }
+    public TerminalNode USH() { return getToken(PlanAParser.USH, 0); }
+    public TerminalNode BWAND() { return getToken(PlanAParser.BWAND, 0); }
+    public TerminalNode BWXOR() { return getToken(PlanAParser.BWXOR, 0); }
+    public TerminalNode BWOR() { return getToken(PlanAParser.BWOR, 0); }
+    public BinaryContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitBinary(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class CharContext extends ExpressionContext {
+    public TerminalNode CHAR() { return getToken(PlanAParser.CHAR, 0); }
+    public CharContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitChar(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+  public static class TrueContext extends ExpressionContext {
+    public TerminalNode TRUE() { return getToken(PlanAParser.TRUE, 0); }
+    public TrueContext(ExpressionContext ctx) { copyFrom(ctx); }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitTrue(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExpressionContext expression() throws RecognitionException {
+    return expression(0);
+  }
+
+  private ExpressionContext expression(int _p) throws RecognitionException {
+    ParserRuleContext _parentctx = _ctx;
+    int _parentState = getState();
+    ExpressionContext _localctx = new ExpressionContext(_ctx, _parentState);
+    ExpressionContext _prevctx = _localctx;
+    int _startState = 18;
+    enterRecursionRule(_localctx, 18, RULE_expression, _p);
+    int _la;
+    try {
+      int _alt;
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(207);
+      switch ( getInterpreter().adaptivePredict(_input,22,_ctx) ) {
+      case 1:
+        {
+        _localctx = new UnaryContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+
+        setState(180);
+        _la = _input.LA(1);
+        if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB))) != 0)) ) {
+        _errHandler.recoverInline(this);
+        } else {
+          consume();
+        }
+        setState(181);
+        expression(14);
+        }
+        break;
+      case 2:
+        {
+        _localctx = new CastContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(182);
+        match(LP);
+        setState(183);
+        decltype();
+        setState(184);
+        match(RP);
+        setState(185);
+        expression(13);
+        }
+        break;
+      case 3:
+        {
+        _localctx = new AssignmentContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(187);
+        extstart();
+        setState(188);
+        _la = _input.LA(1);
+        if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << ASSIGN) | (1L << AADD) | (1L << ASUB) | (1L << AMUL) | (1L << ADIV) | (1L << AREM) | (1L << AAND) | (1L << AXOR) | (1L << AOR) | (1L << ALSH) | (1L << ARSH) | (1L << AUSH))) != 0)) ) {
+        _errHandler.recoverInline(this);
+        } else {
+          consume();
+        }
+        setState(189);
+        expression(1);
+        }
+        break;
+      case 4:
+        {
+        _localctx = new PrecedenceContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(191);
+        match(LP);
+        setState(192);
+        expression(0);
+        setState(193);
+        match(RP);
+        }
+        break;
+      case 5:
+        {
+        _localctx = new NumericContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(195);
+        _la = _input.LA(1);
+        if ( !(((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)))) != 0)) ) {
+        _errHandler.recoverInline(this);
+        } else {
+          consume();
+        }
+        }
+        break;
+      case 6:
+        {
+        _localctx = new CharContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(196);
+        match(CHAR);
+        }
+        break;
+      case 7:
+        {
+        _localctx = new TrueContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(197);
+        match(TRUE);
+        }
+        break;
+      case 8:
+        {
+        _localctx = new FalseContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(198);
+        match(FALSE);
+        }
+        break;
+      case 9:
+        {
+        _localctx = new NullContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(199);
+        match(NULL);
+        }
+        break;
+      case 10:
+        {
+        _localctx = new PostincContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(200);
+        extstart();
+        setState(201);
+        increment();
+        }
+        break;
+      case 11:
+        {
+        _localctx = new PreincContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(203);
+        increment();
+        setState(204);
+        extstart();
+        }
+        break;
+      case 12:
+        {
+        _localctx = new ExternalContext(_localctx);
+        _ctx = _localctx;
+        _prevctx = _localctx;
+        setState(206);
+        extstart();
+        }
+        break;
+      }
+      _ctx.stop = _input.LT(-1);
+      setState(247);
+      _errHandler.sync(this);
+      _alt = getInterpreter().adaptivePredict(_input,24,_ctx);
+      while ( _alt!=2 && _alt!=org.antlr.v4.runtime.atn.ATN.INVALID_ALT_NUMBER ) {
+        if ( _alt==1 ) {
+          if ( _parseListeners!=null ) triggerExitRuleEvent();
+          _prevctx = _localctx;
+          {
+          setState(245);
+          switch ( getInterpreter().adaptivePredict(_input,23,_ctx) ) {
+          case 1:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(209);
+            if (!(precpred(_ctx, 12))) throw new FailedPredicateException(this, "precpred(_ctx, 12)");
+            setState(210);
+            _la = _input.LA(1);
+            if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << MUL) | (1L << DIV) | (1L << REM))) != 0)) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(211);
+            expression(13);
+            }
+            break;
+          case 2:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(212);
+            if (!(precpred(_ctx, 11))) throw new FailedPredicateException(this, "precpred(_ctx, 11)");
+            setState(213);
+            _la = _input.LA(1);
+            if ( !(_la==ADD || _la==SUB) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(214);
+            expression(12);
+            }
+            break;
+          case 3:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(215);
+            if (!(precpred(_ctx, 10))) throw new FailedPredicateException(this, "precpred(_ctx, 10)");
+            setState(216);
+            _la = _input.LA(1);
+            if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LSH) | (1L << RSH) | (1L << USH))) != 0)) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(217);
+            expression(11);
+            }
+            break;
+          case 4:
+            {
+            _localctx = new CompContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(218);
+            if (!(precpred(_ctx, 9))) throw new FailedPredicateException(this, "precpred(_ctx, 9)");
+            setState(219);
+            _la = _input.LA(1);
+            if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LT) | (1L << LTE) | (1L << GT) | (1L << GTE))) != 0)) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(220);
+            expression(10);
+            }
+            break;
+          case 5:
+            {
+            _localctx = new CompContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(221);
+            if (!(precpred(_ctx, 8))) throw new FailedPredicateException(this, "precpred(_ctx, 8)");
+            setState(222);
+            _la = _input.LA(1);
+            if ( !((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << EQ) | (1L << EQR) | (1L << NE) | (1L << NER))) != 0)) ) {
+            _errHandler.recoverInline(this);
+            } else {
+              consume();
+            }
+            setState(223);
+            expression(9);
+            }
+            break;
+          case 6:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(224);
+            if (!(precpred(_ctx, 7))) throw new FailedPredicateException(this, "precpred(_ctx, 7)");
+            setState(225);
+            match(BWAND);
+            setState(226);
+            expression(8);
+            }
+            break;
+          case 7:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(227);
+            if (!(precpred(_ctx, 6))) throw new FailedPredicateException(this, "precpred(_ctx, 6)");
+            setState(228);
+            match(BWXOR);
+            setState(229);
+            expression(7);
+            }
+            break;
+          case 8:
+            {
+            _localctx = new BinaryContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(230);
+            if (!(precpred(_ctx, 5))) throw new FailedPredicateException(this, "precpred(_ctx, 5)");
+            setState(231);
+            match(BWOR);
+            setState(232);
+            expression(6);
+            }
+            break;
+          case 9:
+            {
+            _localctx = new BoolContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(233);
+            if (!(precpred(_ctx, 4))) throw new FailedPredicateException(this, "precpred(_ctx, 4)");
+            setState(234);
+            match(BOOLAND);
+            setState(235);
+            expression(5);
+            }
+            break;
+          case 10:
+            {
+            _localctx = new BoolContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(236);
+            if (!(precpred(_ctx, 3))) throw new FailedPredicateException(this, "precpred(_ctx, 3)");
+            setState(237);
+            match(BOOLOR);
+            setState(238);
+            expression(4);
+            }
+            break;
+          case 11:
+            {
+            _localctx = new ConditionalContext(new ExpressionContext(_parentctx, _parentState));
+            pushNewRecursionContext(_localctx, _startState, RULE_expression);
+            setState(239);
+            if (!(precpred(_ctx, 2))) throw new FailedPredicateException(this, "precpred(_ctx, 2)");
+            setState(240);
+            match(COND);
+            setState(241);
+            expression(0);
+            setState(242);
+            match(COLON);
+            setState(243);
+            expression(2);
+            }
+            break;
+          }
+          } 
+        }
+        setState(249);
+        _errHandler.sync(this);
+        _alt = getInterpreter().adaptivePredict(_input,24,_ctx);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      unrollRecursionContexts(_parentctx);
+    }
+    return _localctx;
+  }
+
+  public static class ExtstartContext extends ParserRuleContext {
+    public ExtprecContext extprec() {
+      return getRuleContext(ExtprecContext.class,0);
+    }
+    public ExtcastContext extcast() {
+      return getRuleContext(ExtcastContext.class,0);
+    }
+    public ExttypeContext exttype() {
+      return getRuleContext(ExttypeContext.class,0);
+    }
+    public ExtvarContext extvar() {
+      return getRuleContext(ExtvarContext.class,0);
+    }
+    public ExtnewContext extnew() {
+      return getRuleContext(ExtnewContext.class,0);
+    }
+    public ExtstringContext extstring() {
+      return getRuleContext(ExtstringContext.class,0);
+    }
+    public ExtstartContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extstart; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtstart(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtstartContext extstart() throws RecognitionException {
+    ExtstartContext _localctx = new ExtstartContext(_ctx, getState());
+    enterRule(_localctx, 20, RULE_extstart);
+    try {
+      setState(256);
+      switch ( getInterpreter().adaptivePredict(_input,25,_ctx) ) {
+      case 1:
+        enterOuterAlt(_localctx, 1);
+        {
+        setState(250);
+        extprec();
+        }
+        break;
+      case 2:
+        enterOuterAlt(_localctx, 2);
+        {
+        setState(251);
+        extcast();
+        }
+        break;
+      case 3:
+        enterOuterAlt(_localctx, 3);
+        {
+        setState(252);
+        exttype();
+        }
+        break;
+      case 4:
+        enterOuterAlt(_localctx, 4);
+        {
+        setState(253);
+        extvar();
+        }
+        break;
+      case 5:
+        enterOuterAlt(_localctx, 5);
+        {
+        setState(254);
+        extnew();
+        }
+        break;
+      case 6:
+        enterOuterAlt(_localctx, 6);
+        {
+        setState(255);
+        extstring();
+        }
+        break;
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtprecContext extends ParserRuleContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public ExtprecContext extprec() {
+      return getRuleContext(ExtprecContext.class,0);
+    }
+    public ExtcastContext extcast() {
+      return getRuleContext(ExtcastContext.class,0);
+    }
+    public ExttypeContext exttype() {
+      return getRuleContext(ExttypeContext.class,0);
+    }
+    public ExtvarContext extvar() {
+      return getRuleContext(ExtvarContext.class,0);
+    }
+    public ExtnewContext extnew() {
+      return getRuleContext(ExtnewContext.class,0);
+    }
+    public ExtstringContext extstring() {
+      return getRuleContext(ExtstringContext.class,0);
+    }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtprecContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extprec; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtprec(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtprecContext extprec() throws RecognitionException {
+    ExtprecContext _localctx = new ExtprecContext(_ctx, getState());
+    enterRule(_localctx, 22, RULE_extprec);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(258);
+      match(LP);
+      setState(265);
+      switch ( getInterpreter().adaptivePredict(_input,26,_ctx) ) {
+      case 1:
+        {
+        setState(259);
+        extprec();
+        }
+        break;
+      case 2:
+        {
+        setState(260);
+        extcast();
+        }
+        break;
+      case 3:
+        {
+        setState(261);
+        exttype();
+        }
+        break;
+      case 4:
+        {
+        setState(262);
+        extvar();
+        }
+        break;
+      case 5:
+        {
+        setState(263);
+        extnew();
+        }
+        break;
+      case 6:
+        {
+        setState(264);
+        extstring();
+        }
+        break;
+      }
+      setState(267);
+      match(RP);
+      setState(270);
+      switch ( getInterpreter().adaptivePredict(_input,27,_ctx) ) {
+      case 1:
+        {
+        setState(268);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(269);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtcastContext extends ParserRuleContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public DecltypeContext decltype() {
+      return getRuleContext(DecltypeContext.class,0);
+    }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public ExtprecContext extprec() {
+      return getRuleContext(ExtprecContext.class,0);
+    }
+    public ExtcastContext extcast() {
+      return getRuleContext(ExtcastContext.class,0);
+    }
+    public ExttypeContext exttype() {
+      return getRuleContext(ExttypeContext.class,0);
+    }
+    public ExtvarContext extvar() {
+      return getRuleContext(ExtvarContext.class,0);
+    }
+    public ExtnewContext extnew() {
+      return getRuleContext(ExtnewContext.class,0);
+    }
+    public ExtstringContext extstring() {
+      return getRuleContext(ExtstringContext.class,0);
+    }
+    public ExtcastContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extcast; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtcast(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtcastContext extcast() throws RecognitionException {
+    ExtcastContext _localctx = new ExtcastContext(_ctx, getState());
+    enterRule(_localctx, 24, RULE_extcast);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(272);
+      match(LP);
+      setState(273);
+      decltype();
+      setState(274);
+      match(RP);
+      setState(281);
+      switch ( getInterpreter().adaptivePredict(_input,28,_ctx) ) {
+      case 1:
+        {
+        setState(275);
+        extprec();
+        }
+        break;
+      case 2:
+        {
+        setState(276);
+        extcast();
+        }
+        break;
+      case 3:
+        {
+        setState(277);
+        exttype();
+        }
+        break;
+      case 4:
+        {
+        setState(278);
+        extvar();
+        }
+        break;
+      case 5:
+        {
+        setState(279);
+        extnew();
+        }
+        break;
+      case 6:
+        {
+        setState(280);
+        extstring();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtbraceContext extends ParserRuleContext {
+    public TerminalNode LBRACE() { return getToken(PlanAParser.LBRACE, 0); }
+    public ExpressionContext expression() {
+      return getRuleContext(ExpressionContext.class,0);
+    }
+    public TerminalNode RBRACE() { return getToken(PlanAParser.RBRACE, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtbraceContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extbrace; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtbrace(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtbraceContext extbrace() throws RecognitionException {
+    ExtbraceContext _localctx = new ExtbraceContext(_ctx, getState());
+    enterRule(_localctx, 26, RULE_extbrace);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(283);
+      match(LBRACE);
+      setState(284);
+      expression(0);
+      setState(285);
+      match(RBRACE);
+      setState(288);
+      switch ( getInterpreter().adaptivePredict(_input,29,_ctx) ) {
+      case 1:
+        {
+        setState(286);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(287);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtdotContext extends ParserRuleContext {
+    public TerminalNode DOT() { return getToken(PlanAParser.DOT, 0); }
+    public ExtcallContext extcall() {
+      return getRuleContext(ExtcallContext.class,0);
+    }
+    public ExtfieldContext extfield() {
+      return getRuleContext(ExtfieldContext.class,0);
+    }
+    public ExtdotContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extdot; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtdot(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtdotContext extdot() throws RecognitionException {
+    ExtdotContext _localctx = new ExtdotContext(_ctx, getState());
+    enterRule(_localctx, 28, RULE_extdot);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(290);
+      match(DOT);
+      setState(293);
+      switch ( getInterpreter().adaptivePredict(_input,30,_ctx) ) {
+      case 1:
+        {
+        setState(291);
+        extcall();
+        }
+        break;
+      case 2:
+        {
+        setState(292);
+        extfield();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExttypeContext extends ParserRuleContext {
+    public TerminalNode TYPE() { return getToken(PlanAParser.TYPE, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExttypeContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_exttype; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExttype(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExttypeContext exttype() throws RecognitionException {
+    ExttypeContext _localctx = new ExttypeContext(_ctx, getState());
+    enterRule(_localctx, 30, RULE_exttype);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(295);
+      match(TYPE);
+      setState(296);
+      extdot();
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtcallContext extends ParserRuleContext {
+    public TerminalNode EXTID() { return getToken(PlanAParser.EXTID, 0); }
+    public ArgumentsContext arguments() {
+      return getRuleContext(ArgumentsContext.class,0);
+    }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtcallContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extcall; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtcall(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtcallContext extcall() throws RecognitionException {
+    ExtcallContext _localctx = new ExtcallContext(_ctx, getState());
+    enterRule(_localctx, 32, RULE_extcall);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(298);
+      match(EXTID);
+      setState(299);
+      arguments();
+      setState(302);
+      switch ( getInterpreter().adaptivePredict(_input,31,_ctx) ) {
+      case 1:
+        {
+        setState(300);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(301);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtvarContext extends ParserRuleContext {
+    public TerminalNode ID() { return getToken(PlanAParser.ID, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtvarContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extvar; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtvar(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtvarContext extvar() throws RecognitionException {
+    ExtvarContext _localctx = new ExtvarContext(_ctx, getState());
+    enterRule(_localctx, 34, RULE_extvar);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(304);
+      match(ID);
+      setState(307);
+      switch ( getInterpreter().adaptivePredict(_input,32,_ctx) ) {
+      case 1:
+        {
+        setState(305);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(306);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtfieldContext extends ParserRuleContext {
+    public TerminalNode EXTID() { return getToken(PlanAParser.EXTID, 0); }
+    public TerminalNode EXTINTEGER() { return getToken(PlanAParser.EXTINTEGER, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtfieldContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extfield; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtfield(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtfieldContext extfield() throws RecognitionException {
+    ExtfieldContext _localctx = new ExtfieldContext(_ctx, getState());
+    enterRule(_localctx, 36, RULE_extfield);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(309);
+      _la = _input.LA(1);
+      if ( !(_la==EXTINTEGER || _la==EXTID) ) {
+      _errHandler.recoverInline(this);
+      } else {
+        consume();
+      }
+      setState(312);
+      switch ( getInterpreter().adaptivePredict(_input,33,_ctx) ) {
+      case 1:
+        {
+        setState(310);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(311);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtnewContext extends ParserRuleContext {
+    public TerminalNode NEW() { return getToken(PlanAParser.NEW, 0); }
+    public TerminalNode TYPE() { return getToken(PlanAParser.TYPE, 0); }
+    public ArgumentsContext arguments() {
+      return getRuleContext(ArgumentsContext.class,0);
+    }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public List<TerminalNode> LBRACE() { return getTokens(PlanAParser.LBRACE); }
+    public TerminalNode LBRACE(int i) {
+      return getToken(PlanAParser.LBRACE, i);
+    }
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public List<TerminalNode> RBRACE() { return getTokens(PlanAParser.RBRACE); }
+    public TerminalNode RBRACE(int i) {
+      return getToken(PlanAParser.RBRACE, i);
+    }
+    public ExtnewContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extnew; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtnew(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtnewContext extnew() throws RecognitionException {
+    ExtnewContext _localctx = new ExtnewContext(_ctx, getState());
+    enterRule(_localctx, 38, RULE_extnew);
+    try {
+      int _alt;
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(314);
+      match(NEW);
+      setState(315);
+      match(TYPE);
+      setState(332);
+      switch (_input.LA(1)) {
+      case LP:
+        {
+        {
+        setState(316);
+        arguments();
+        setState(319);
+        switch ( getInterpreter().adaptivePredict(_input,34,_ctx) ) {
+        case 1:
+          {
+          setState(317);
+          extdot();
+          }
+          break;
+        case 2:
+          {
+          setState(318);
+          extbrace();
+          }
+          break;
+        }
+        }
+        }
+        break;
+      case LBRACE:
+        {
+        {
+        setState(325); 
+        _errHandler.sync(this);
+        _alt = 1;
+        do {
+          switch (_alt) {
+          case 1:
+            {
+            {
+            setState(321);
+            match(LBRACE);
+            setState(322);
+            expression(0);
+            setState(323);
+            match(RBRACE);
+            }
+            }
+            break;
+          default:
+            throw new NoViableAltException(this);
+          }
+          setState(327); 
+          _errHandler.sync(this);
+          _alt = getInterpreter().adaptivePredict(_input,35,_ctx);
+        } while ( _alt!=2 && _alt!=org.antlr.v4.runtime.atn.ATN.INVALID_ALT_NUMBER );
+        setState(330);
+        switch ( getInterpreter().adaptivePredict(_input,36,_ctx) ) {
+        case 1:
+          {
+          setState(329);
+          extdot();
+          }
+          break;
+        }
+        }
+        }
+        break;
+      default:
+        throw new NoViableAltException(this);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ExtstringContext extends ParserRuleContext {
+    public TerminalNode STRING() { return getToken(PlanAParser.STRING, 0); }
+    public ExtdotContext extdot() {
+      return getRuleContext(ExtdotContext.class,0);
+    }
+    public ExtbraceContext extbrace() {
+      return getRuleContext(ExtbraceContext.class,0);
+    }
+    public ExtstringContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_extstring; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitExtstring(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ExtstringContext extstring() throws RecognitionException {
+    ExtstringContext _localctx = new ExtstringContext(_ctx, getState());
+    enterRule(_localctx, 40, RULE_extstring);
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(334);
+      match(STRING);
+      setState(337);
+      switch ( getInterpreter().adaptivePredict(_input,38,_ctx) ) {
+      case 1:
+        {
+        setState(335);
+        extdot();
+        }
+        break;
+      case 2:
+        {
+        setState(336);
+        extbrace();
+        }
+        break;
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class ArgumentsContext extends ParserRuleContext {
+    public TerminalNode LP() { return getToken(PlanAParser.LP, 0); }
+    public TerminalNode RP() { return getToken(PlanAParser.RP, 0); }
+    public List<ExpressionContext> expression() {
+      return getRuleContexts(ExpressionContext.class);
+    }
+    public ExpressionContext expression(int i) {
+      return getRuleContext(ExpressionContext.class,i);
+    }
+    public List<TerminalNode> COMMA() { return getTokens(PlanAParser.COMMA); }
+    public TerminalNode COMMA(int i) {
+      return getToken(PlanAParser.COMMA, i);
+    }
+    public ArgumentsContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_arguments; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitArguments(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final ArgumentsContext arguments() throws RecognitionException {
+    ArgumentsContext _localctx = new ArgumentsContext(_ctx, getState());
+    enterRule(_localctx, 42, RULE_arguments);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      {
+      setState(339);
+      match(LP);
+      setState(348);
+      _la = _input.LA(1);
+      if ((((_la) & ~0x3f) == 0 && ((1L << _la) & ((1L << LP) | (1L << NEW) | (1L << BOOLNOT) | (1L << BWNOT) | (1L << ADD) | (1L << SUB) | (1L << INCR) | (1L << DECR))) != 0) || ((((_la - 64)) & ~0x3f) == 0 && ((1L << (_la - 64)) & ((1L << (OCTAL - 64)) | (1L << (HEX - 64)) | (1L << (INTEGER - 64)) | (1L << (DECIMAL - 64)) | (1L << (STRING - 64)) | (1L << (CHAR - 64)) | (1L << (TRUE - 64)) | (1L << (FALSE - 64)) | (1L << (NULL - 64)) | (1L << (TYPE - 64)) | (1L << (ID - 64)))) != 0)) {
+        {
+        setState(340);
+        expression(0);
+        setState(345);
+        _errHandler.sync(this);
+        _la = _input.LA(1);
+        while (_la==COMMA) {
+          {
+          {
+          setState(341);
+          match(COMMA);
+          setState(342);
+          expression(0);
+          }
+          }
+          setState(347);
+          _errHandler.sync(this);
+          _la = _input.LA(1);
+        }
+        }
+      }
+
+      setState(350);
+      match(RP);
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public static class IncrementContext extends ParserRuleContext {
+    public TerminalNode INCR() { return getToken(PlanAParser.INCR, 0); }
+    public TerminalNode DECR() { return getToken(PlanAParser.DECR, 0); }
+    public IncrementContext(ParserRuleContext parent, int invokingState) {
+      super(parent, invokingState);
+    }
+    @Override public int getRuleIndex() { return RULE_increment; }
+    @Override
+    public <T> T accept(ParseTreeVisitor<? extends T> visitor) {
+      if ( visitor instanceof PlanAParserVisitor ) return ((PlanAParserVisitor<? extends T>)visitor).visitIncrement(this);
+      else return visitor.visitChildren(this);
+    }
+  }
+
+  public final IncrementContext increment() throws RecognitionException {
+    IncrementContext _localctx = new IncrementContext(_ctx, getState());
+    enterRule(_localctx, 44, RULE_increment);
+    int _la;
+    try {
+      enterOuterAlt(_localctx, 1);
+      {
+      setState(352);
+      _la = _input.LA(1);
+      if ( !(_la==INCR || _la==DECR) ) {
+      _errHandler.recoverInline(this);
+      } else {
+        consume();
+      }
+      }
+    }
+    catch (RecognitionException re) {
+      _localctx.exception = re;
+      _errHandler.reportError(this, re);
+      _errHandler.recover(this, re);
+    }
+    finally {
+      exitRule();
+    }
+    return _localctx;
+  }
+
+  public boolean sempred(RuleContext _localctx, int ruleIndex, int predIndex) {
+    switch (ruleIndex) {
+    case 9:
+      return expression_sempred((ExpressionContext)_localctx, predIndex);
+    }
+    return true;
+  }
+  private boolean expression_sempred(ExpressionContext _localctx, int predIndex) {
+    switch (predIndex) {
+    case 0:
+      return precpred(_ctx, 12);
+    case 1:
+      return precpred(_ctx, 11);
+    case 2:
+      return precpred(_ctx, 10);
+    case 3:
+      return precpred(_ctx, 9);
+    case 4:
+      return precpred(_ctx, 8);
+    case 5:
+      return precpred(_ctx, 7);
+    case 6:
+      return precpred(_ctx, 6);
+    case 7:
+      return precpred(_ctx, 5);
+    case 8:
+      return precpred(_ctx, 4);
+    case 9:
+      return precpred(_ctx, 3);
+    case 10:
+      return precpred(_ctx, 2);
+    }
+    return true;
+  }
+
+  public static final String _serializedATN =
+    "\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\3N\u0165\4\2\t\2\4"+
+    "\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7\4\b\t\b\4\t\t\t\4\n\t\n\4\13\t"+
+    "\13\4\f\t\f\4\r\t\r\4\16\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22\t\22"+
+    "\4\23\t\23\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\3\2\6\2\62"+
+    "\n\2\r\2\16\2\63\3\2\3\2\3\3\3\3\3\3\3\3\3\3\3\3\3\3\5\3?\n\3\3\3\3\3"+
+    "\3\3\3\3\3\3\3\3\5\3G\n\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\5\3P\n\3\3\3\3\3"+
+    "\3\3\5\3U\n\3\3\3\3\3\5\3Y\n\3\3\3\3\3\5\3]\n\3\3\3\3\3\3\3\5\3b\n\3\3"+
+    "\3\3\3\5\3f\n\3\3\3\3\3\5\3j\n\3\3\3\3\3\5\3n\n\3\3\3\3\3\3\3\5\3s\n\3"+
+    "\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\3\6\3~\n\3\r\3\16\3\177\3\3\3\3\3\3"+
+    "\5\3\u0085\n\3\3\3\3\3\5\3\u0089\n\3\5\3\u008b\n\3\3\4\3\4\7\4\u008f\n"+
+    "\4\f\4\16\4\u0092\13\4\3\4\3\4\5\4\u0096\n\4\3\5\3\5\3\6\3\6\5\6\u009c"+
+    "\n\6\3\7\3\7\3\b\3\b\3\b\3\b\7\b\u00a4\n\b\f\b\16\b\u00a7\13\b\3\t\3\t"+
+    "\3\t\7\t\u00ac\n\t\f\t\16\t\u00af\13\t\3\n\3\n\3\n\5\n\u00b4\n\n\3\13"+
+    "\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13"+
+    "\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\5\13"+
+    "\u00d2\n\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13"+
+    "\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13"+
+    "\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\3\13\7\13\u00f8\n\13\f\13"+
+    "\16\13\u00fb\13\13\3\f\3\f\3\f\3\f\3\f\3\f\5\f\u0103\n\f\3\r\3\r\3\r\3"+
+    "\r\3\r\3\r\3\r\5\r\u010c\n\r\3\r\3\r\3\r\5\r\u0111\n\r\3\16\3\16\3\16"+
+    "\3\16\3\16\3\16\3\16\3\16\3\16\5\16\u011c\n\16\3\17\3\17\3\17\3\17\3\17"+
+    "\5\17\u0123\n\17\3\20\3\20\3\20\5\20\u0128\n\20\3\21\3\21\3\21\3\22\3"+
+    "\22\3\22\3\22\5\22\u0131\n\22\3\23\3\23\3\23\5\23\u0136\n\23\3\24\3\24"+
+    "\3\24\5\24\u013b\n\24\3\25\3\25\3\25\3\25\3\25\5\25\u0142\n\25\3\25\3"+
+    "\25\3\25\3\25\6\25\u0148\n\25\r\25\16\25\u0149\3\25\5\25\u014d\n\25\5"+
+    "\25\u014f\n\25\3\26\3\26\3\26\5\26\u0154\n\26\3\27\3\27\3\27\3\27\7\27"+
+    "\u015a\n\27\f\27\16\27\u015d\13\27\5\27\u015f\n\27\3\27\3\27\3\30\3\30"+
+    "\3\30\2\3\24\31\2\4\6\b\n\f\16\20\22\24\26\30\32\34\36 \"$&(*,.\2\f\4"+
+    "\2\32\33\37 \3\2\65@\3\2BE\3\2\34\36\3\2\37 \3\2!#\3\2$\'\3\2(+\3\2MN"+
+    "\3\2\63\64\u01a5\2\61\3\2\2\2\4\u008a\3\2\2\2\6\u0095\3\2\2\2\b\u0097"+
+    "\3\2\2\2\n\u009b\3\2\2\2\f\u009d\3\2\2\2\16\u009f\3\2\2\2\20\u00a8\3\2"+
+    "\2\2\22\u00b0\3\2\2\2\24\u00d1\3\2\2\2\26\u0102\3\2\2\2\30\u0104\3\2\2"+
+    "\2\32\u0112\3\2\2\2\34\u011d\3\2\2\2\36\u0124\3\2\2\2 \u0129\3\2\2\2\""+
+    "\u012c\3\2\2\2$\u0132\3\2\2\2&\u0137\3\2\2\2(\u013c\3\2\2\2*\u0150\3\2"+
+    "\2\2,\u0155\3\2\2\2.\u0162\3\2\2\2\60\62\5\4\3\2\61\60\3\2\2\2\62\63\3"+
+    "\2\2\2\63\61\3\2\2\2\63\64\3\2\2\2\64\65\3\2\2\2\65\66\7\2\2\3\66\3\3"+
+    "\2\2\2\678\7\16\2\289\7\t\2\29:\5\24\13\2:;\7\n\2\2;>\5\6\4\2<=\7\17\2"+
+    "\2=?\5\6\4\2><\3\2\2\2>?\3\2\2\2?\u008b\3\2\2\2@A\7\20\2\2AB\7\t\2\2B"+
+    "C\5\24\13\2CF\7\n\2\2DG\5\6\4\2EG\5\b\5\2FD\3\2\2\2FE\3\2\2\2G\u008b\3"+
+    "\2\2\2HI\7\21\2\2IJ\5\6\4\2JK\7\20\2\2KL\7\t\2\2LM\5\24\13\2MO\7\n\2\2"+
+    "NP\7\r\2\2ON\3\2\2\2OP\3\2\2\2P\u008b\3\2\2\2QR\7\22\2\2RT\7\t\2\2SU\5"+
+    "\n\6\2TS\3\2\2\2TU\3\2\2\2UV\3\2\2\2VX\7\r\2\2WY\5\24\13\2XW\3\2\2\2X"+
+    "Y\3\2\2\2YZ\3\2\2\2Z\\\7\r\2\2[]\5\f\7\2\\[\3\2\2\2\\]\3\2\2\2]^\3\2\2"+
+    "\2^a\7\n\2\2_b\5\6\4\2`b\5\b\5\2a_\3\2\2\2a`\3\2\2\2b\u008b\3\2\2\2ce"+
+    "\5\16\b\2df\7\r\2\2ed\3\2\2\2ef\3\2\2\2f\u008b\3\2\2\2gi\7\23\2\2hj\7"+
+    "\r\2\2ih\3\2\2\2ij\3\2\2\2j\u008b\3\2\2\2km\7\24\2\2ln\7\r\2\2ml\3\2\2"+
+    "\2mn\3\2\2\2n\u008b\3\2\2\2op\7\25\2\2pr\5\24\13\2qs\7\r\2\2rq\3\2\2\2"+
+    "rs\3\2\2\2s\u008b\3\2\2\2tu\7\27\2\2u}\5\6\4\2vw\7\30\2\2wx\7\t\2\2xy"+
+    "\7K\2\2yz\7L\2\2z{\3\2\2\2{|\7\n\2\2|~\5\6\4\2}v\3\2\2\2~\177\3\2\2\2"+
+    "\177}\3\2\2\2\177\u0080\3\2\2\2\u0080\u008b\3\2\2\2\u0081\u0082\7\31\2"+
+    "\2\u0082\u0084\5\24\13\2\u0083\u0085\7\r\2\2\u0084\u0083\3\2\2\2\u0084"+
+    "\u0085\3\2\2\2\u0085\u008b\3\2\2\2\u0086\u0088\5\24\13\2\u0087\u0089\7"+
+    "\r\2\2\u0088\u0087\3\2\2\2\u0088\u0089\3\2\2\2\u0089\u008b\3\2\2\2\u008a"+
+    "\67\3\2\2\2\u008a@\3\2\2\2\u008aH\3\2\2\2\u008aQ\3\2\2\2\u008ac\3\2\2"+
+    "\2\u008ag\3\2\2\2\u008ak\3\2\2\2\u008ao\3\2\2\2\u008at\3\2\2\2\u008a\u0081"+
+    "\3\2\2\2\u008a\u0086\3\2\2\2\u008b\5\3\2\2\2\u008c\u0090\7\5\2\2\u008d"+
+    "\u008f\5\4\3\2\u008e\u008d\3\2\2\2\u008f\u0092\3\2\2\2\u0090\u008e\3\2"+
+    "\2\2\u0090\u0091\3\2\2\2\u0091\u0093\3\2\2\2\u0092\u0090\3\2\2\2\u0093"+
+    "\u0096\7\6\2\2\u0094\u0096\5\4\3\2\u0095\u008c\3\2\2\2\u0095\u0094\3\2"+
+    "\2\2\u0096\7\3\2\2\2\u0097\u0098\7\r\2\2\u0098\t\3\2\2\2\u0099\u009c\5"+
+    "\16\b\2\u009a\u009c\5\24\13\2\u009b\u0099\3\2\2\2\u009b\u009a\3\2\2\2"+
+    "\u009c\13\3\2\2\2\u009d\u009e\5\24\13\2\u009e\r\3\2\2\2\u009f\u00a0\5"+
+    "\20\t\2\u00a0\u00a5\5\22\n\2\u00a1\u00a2\7\f\2\2\u00a2\u00a4\5\22\n\2"+
+    "\u00a3\u00a1\3\2\2\2\u00a4\u00a7\3\2\2\2\u00a5\u00a3\3\2\2\2\u00a5\u00a6"+
+    "\3\2\2\2\u00a6\17\3\2\2\2\u00a7\u00a5\3\2\2\2\u00a8\u00ad\7K\2\2\u00a9"+
+    "\u00aa\7\7\2\2\u00aa\u00ac\7\b\2\2\u00ab\u00a9\3\2\2\2\u00ac\u00af\3\2"+
+    "\2\2\u00ad\u00ab\3\2\2\2\u00ad\u00ae\3\2\2\2\u00ae\21\3\2\2\2\u00af\u00ad"+
+    "\3\2\2\2\u00b0\u00b3\7L\2\2\u00b1\u00b2\7\65\2\2\u00b2\u00b4\5\24\13\2"+
+    "\u00b3\u00b1\3\2\2\2\u00b3\u00b4\3\2\2\2\u00b4\23\3\2\2\2\u00b5\u00b6"+
+    "\b\13\1\2\u00b6\u00b7\t\2\2\2\u00b7\u00d2\5\24\13\20\u00b8\u00b9\7\t\2"+
+    "\2\u00b9\u00ba\5\20\t\2\u00ba\u00bb\7\n\2\2\u00bb\u00bc\5\24\13\17\u00bc"+
+    "\u00d2\3\2\2\2\u00bd\u00be\5\26\f\2\u00be\u00bf\t\3\2\2\u00bf\u00c0\5"+
+    "\24\13\3\u00c0\u00d2\3\2\2\2\u00c1\u00c2\7\t\2\2\u00c2\u00c3\5\24\13\2"+
+    "\u00c3\u00c4\7\n\2\2\u00c4\u00d2\3\2\2\2\u00c5\u00d2\t\4\2\2\u00c6\u00d2"+
+    "\7G\2\2\u00c7\u00d2\7H\2\2\u00c8\u00d2\7I\2\2\u00c9\u00d2\7J\2\2\u00ca"+
+    "\u00cb\5\26\f\2\u00cb\u00cc\5.\30\2\u00cc\u00d2\3\2\2\2\u00cd\u00ce\5"+
+    ".\30\2\u00ce\u00cf\5\26\f\2\u00cf\u00d2\3\2\2\2\u00d0\u00d2\5\26\f\2\u00d1"+
+    "\u00b5\3\2\2\2\u00d1\u00b8\3\2\2\2\u00d1\u00bd\3\2\2\2\u00d1\u00c1\3\2"+
+    "\2\2\u00d1\u00c5\3\2\2\2\u00d1\u00c6\3\2\2\2\u00d1\u00c7\3\2\2\2\u00d1"+
+    "\u00c8\3\2\2\2\u00d1\u00c9\3\2\2\2\u00d1\u00ca\3\2\2\2\u00d1\u00cd\3\2"+
+    "\2\2\u00d1\u00d0\3\2\2\2\u00d2\u00f9\3\2\2\2\u00d3\u00d4\f\16\2\2\u00d4"+
+    "\u00d5\t\5\2\2\u00d5\u00f8\5\24\13\17\u00d6\u00d7\f\r\2\2\u00d7\u00d8"+
+    "\t\6\2\2\u00d8\u00f8\5\24\13\16\u00d9\u00da\f\f\2\2\u00da\u00db\t\7\2"+
+    "\2\u00db\u00f8\5\24\13\r\u00dc\u00dd\f\13\2\2\u00dd\u00de\t\b\2\2\u00de"+
+    "\u00f8\5\24\13\f\u00df\u00e0\f\n\2\2\u00e0\u00e1\t\t\2\2\u00e1\u00f8\5"+
+    "\24\13\13\u00e2\u00e3\f\t\2\2\u00e3\u00e4\7,\2\2\u00e4\u00f8\5\24\13\n"+
+    "\u00e5\u00e6\f\b\2\2\u00e6\u00e7\7-\2\2\u00e7\u00f8\5\24\13\t\u00e8\u00e9"+
+    "\f\7\2\2\u00e9\u00ea\7.\2\2\u00ea\u00f8\5\24\13\b\u00eb\u00ec\f\6\2\2"+
+    "\u00ec\u00ed\7/\2\2\u00ed\u00f8\5\24\13\7\u00ee\u00ef\f\5\2\2\u00ef\u00f0"+
+    "\7\60\2\2\u00f0\u00f8\5\24\13\6\u00f1\u00f2\f\4\2\2\u00f2\u00f3\7\61\2"+
+    "\2\u00f3\u00f4\5\24\13\2\u00f4\u00f5\7\62\2\2\u00f5\u00f6\5\24\13\4\u00f6"+
+    "\u00f8\3\2\2\2\u00f7\u00d3\3\2\2\2\u00f7\u00d6\3\2\2\2\u00f7\u00d9\3\2"+
+    "\2\2\u00f7\u00dc\3\2\2\2\u00f7\u00df\3\2\2\2\u00f7\u00e2\3\2\2\2\u00f7"+
+    "\u00e5\3\2\2\2\u00f7\u00e8\3\2\2\2\u00f7\u00eb\3\2\2\2\u00f7\u00ee\3\2"+
+    "\2\2\u00f7\u00f1\3\2\2\2\u00f8\u00fb\3\2\2\2\u00f9\u00f7\3\2\2\2\u00f9"+
+    "\u00fa\3\2\2\2\u00fa\25\3\2\2\2\u00fb\u00f9\3\2\2\2\u00fc\u0103\5\30\r"+
+    "\2\u00fd\u0103\5\32\16\2\u00fe\u0103\5 \21\2\u00ff\u0103\5$\23\2\u0100"+
+    "\u0103\5(\25\2\u0101\u0103\5*\26\2\u0102\u00fc\3\2\2\2\u0102\u00fd\3\2"+
+    "\2\2\u0102\u00fe\3\2\2\2\u0102\u00ff\3\2\2\2\u0102\u0100\3\2\2\2\u0102"+
+    "\u0101\3\2\2\2\u0103\27\3\2\2\2\u0104\u010b\7\t\2\2\u0105\u010c\5\30\r"+
+    "\2\u0106\u010c\5\32\16\2\u0107\u010c\5 \21\2\u0108\u010c\5$\23\2\u0109"+
+    "\u010c\5(\25\2\u010a\u010c\5*\26\2\u010b\u0105\3\2\2\2\u010b\u0106\3\2"+
+    "\2\2\u010b\u0107\3\2\2\2\u010b\u0108\3\2\2\2\u010b\u0109\3\2\2\2\u010b"+
+    "\u010a\3\2\2\2\u010c\u010d\3\2\2\2\u010d\u0110\7\n\2\2\u010e\u0111\5\36"+
+    "\20\2\u010f\u0111\5\34\17\2\u0110\u010e\3\2\2\2\u0110\u010f\3\2\2\2\u0110"+
+    "\u0111\3\2\2\2\u0111\31\3\2\2\2\u0112\u0113\7\t\2\2\u0113\u0114\5\20\t"+
+    "\2\u0114\u011b\7\n\2\2\u0115\u011c\5\30\r\2\u0116\u011c\5\32\16\2\u0117"+
+    "\u011c\5 \21\2\u0118\u011c\5$\23\2\u0119\u011c\5(\25\2\u011a\u011c\5*"+
+    "\26\2\u011b\u0115\3\2\2\2\u011b\u0116\3\2\2\2\u011b\u0117\3\2\2\2\u011b"+
+    "\u0118\3\2\2\2\u011b\u0119\3\2\2\2\u011b\u011a\3\2\2\2\u011c\33\3\2\2"+
+    "\2\u011d\u011e\7\7\2\2\u011e\u011f\5\24\13\2\u011f\u0122\7\b\2\2\u0120"+
+    "\u0123\5\36\20\2\u0121\u0123\5\34\17\2\u0122\u0120\3\2\2\2\u0122\u0121"+
+    "\3\2\2\2\u0122\u0123\3\2\2\2\u0123\35\3\2\2\2\u0124\u0127\7\13\2\2\u0125"+
+    "\u0128\5\"\22\2\u0126\u0128\5&\24\2\u0127\u0125\3\2\2\2\u0127\u0126\3"+
+    "\2\2\2\u0128\37\3\2\2\2\u0129\u012a\7K\2\2\u012a\u012b\5\36\20\2\u012b"+
+    "!\3\2\2\2\u012c\u012d\7N\2\2\u012d\u0130\5,\27\2\u012e\u0131\5\36\20\2"+
+    "\u012f\u0131\5\34\17\2\u0130\u012e\3\2\2\2\u0130\u012f\3\2\2\2\u0130\u0131"+
+    "\3\2\2\2\u0131#\3\2\2\2\u0132\u0135\7L\2\2\u0133\u0136\5\36\20\2\u0134"+
+    "\u0136\5\34\17\2\u0135\u0133\3\2\2\2\u0135\u0134\3\2\2\2\u0135\u0136\3"+
+    "\2\2\2\u0136%\3\2\2\2\u0137\u013a\t\n\2\2\u0138\u013b\5\36\20\2\u0139"+
+    "\u013b\5\34\17\2\u013a\u0138\3\2\2\2\u013a\u0139\3\2\2\2\u013a\u013b\3"+
+    "\2\2\2\u013b\'\3\2\2\2\u013c\u013d\7\26\2\2\u013d\u014e\7K\2\2\u013e\u0141"+
+    "\5,\27\2\u013f\u0142\5\36\20\2\u0140\u0142\5\34\17\2\u0141\u013f\3\2\2"+
+    "\2\u0141\u0140\3\2\2\2\u0141\u0142\3\2\2\2\u0142\u014f\3\2\2\2\u0143\u0144"+
+    "\7\7\2\2\u0144\u0145\5\24\13\2\u0145\u0146\7\b\2\2\u0146\u0148\3\2\2\2"+
+    "\u0147\u0143\3\2\2\2\u0148\u0149\3\2\2\2\u0149\u0147\3\2\2\2\u0149\u014a"+
+    "\3\2\2\2\u014a\u014c\3\2\2\2\u014b\u014d\5\36\20\2\u014c\u014b\3\2\2\2"+
+    "\u014c\u014d\3\2\2\2\u014d\u014f\3\2\2\2\u014e\u013e\3\2\2\2\u014e\u0147"+
+    "\3\2\2\2\u014f)\3\2\2\2\u0150\u0153\7F\2\2\u0151\u0154\5\36\20\2\u0152"+
+    "\u0154\5\34\17\2\u0153\u0151\3\2\2\2\u0153\u0152\3\2\2\2\u0153\u0154\3"+
+    "\2\2\2\u0154+\3\2\2\2\u0155\u015e\7\t\2\2\u0156\u015b\5\24\13\2\u0157"+
+    "\u0158\7\f\2\2\u0158\u015a\5\24\13\2\u0159\u0157\3\2\2\2\u015a\u015d\3"+
+    "\2\2\2\u015b\u0159\3\2\2\2\u015b\u015c\3\2\2\2\u015c\u015f\3\2\2\2\u015d"+
+    "\u015b\3\2\2\2\u015e\u0156\3\2\2\2\u015e\u015f\3\2\2\2\u015f\u0160\3\2"+
+    "\2\2\u0160\u0161\7\n\2\2\u0161-\3\2\2\2\u0162\u0163\t\13\2\2\u0163/\3"+
+    "\2\2\2+\63>FOTX\\aeimr\177\u0084\u0088\u008a\u0090\u0095\u009b\u00a5\u00ad"+
+    "\u00b3\u00d1\u00f7\u00f9\u0102\u010b\u0110\u011b\u0122\u0127\u0130\u0135"+
+    "\u013a\u0141\u0149\u014c\u014e\u0153\u015b\u015e";
+  public static final ATN _ATN =
+    new ATNDeserializer().deserialize(_serializedATN.toCharArray());
+  static {
+    _decisionToDFA = new DFA[_ATN.getNumberOfDecisions()];
+    for (int i = 0; i < _ATN.getNumberOfDecisions(); i++) {
+      _decisionToDFA[i] = new DFA(_ATN.getDecisionState(i), i);
+    }
+  }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserBaseVisitor.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserBaseVisitor.java
new file mode 100644
index 0000000..d731b57
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserBaseVisitor.java
@@ -0,0 +1,357 @@
+// ANTLR GENERATED CODE: DO NOT EDIT
+package org.elasticsearch.plan.a;
+import org.antlr.v4.runtime.tree.AbstractParseTreeVisitor;
+
+/**
+ * This class provides an empty implementation of {@link PlanAParserVisitor},
+ * which can be extended to create a visitor which only needs to handle a subset
+ * of the available methods.
+ *
+ * @param <T> The return type of the visit operation. Use {@link Void} for
+ * operations with no return type.
+ */
+class PlanAParserBaseVisitor<T> extends AbstractParseTreeVisitor<T> implements PlanAParserVisitor<T> {
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitSource(PlanAParser.SourceContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitIf(PlanAParser.IfContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitWhile(PlanAParser.WhileContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDo(PlanAParser.DoContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitFor(PlanAParser.ForContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDecl(PlanAParser.DeclContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitContinue(PlanAParser.ContinueContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitBreak(PlanAParser.BreakContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitReturn(PlanAParser.ReturnContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitTry(PlanAParser.TryContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitThrow(PlanAParser.ThrowContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExpr(PlanAParser.ExprContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitMultiple(PlanAParser.MultipleContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitSingle(PlanAParser.SingleContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitEmpty(PlanAParser.EmptyContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitInitializer(PlanAParser.InitializerContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitAfterthought(PlanAParser.AfterthoughtContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDeclaration(PlanAParser.DeclarationContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDecltype(PlanAParser.DecltypeContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitDeclvar(PlanAParser.DeclvarContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitComp(PlanAParser.CompContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitBool(PlanAParser.BoolContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitConditional(PlanAParser.ConditionalContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitAssignment(PlanAParser.AssignmentContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitFalse(PlanAParser.FalseContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitNumeric(PlanAParser.NumericContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitUnary(PlanAParser.UnaryContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitPrecedence(PlanAParser.PrecedenceContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitPreinc(PlanAParser.PreincContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitPostinc(PlanAParser.PostincContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitCast(PlanAParser.CastContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExternal(PlanAParser.ExternalContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitNull(PlanAParser.NullContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitBinary(PlanAParser.BinaryContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitChar(PlanAParser.CharContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitTrue(PlanAParser.TrueContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtstart(PlanAParser.ExtstartContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtprec(PlanAParser.ExtprecContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtcast(PlanAParser.ExtcastContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtbrace(PlanAParser.ExtbraceContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtdot(PlanAParser.ExtdotContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExttype(PlanAParser.ExttypeContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtcall(PlanAParser.ExtcallContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtvar(PlanAParser.ExtvarContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtfield(PlanAParser.ExtfieldContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtnew(PlanAParser.ExtnewContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitExtstring(PlanAParser.ExtstringContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitArguments(PlanAParser.ArgumentsContext ctx) { return visitChildren(ctx); }
+  /**
+   * {@inheritDoc}
+   *
+   * <p>The default implementation returns the result of calling
+   * {@link #visitChildren} on {@code ctx}.</p>
+   */
+  @Override public T visitIncrement(PlanAParser.IncrementContext ctx) { return visitChildren(ctx); }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserVisitor.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserVisitor.java
new file mode 100644
index 0000000..7470f3b
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAParserVisitor.java
@@ -0,0 +1,336 @@
+// ANTLR GENERATED CODE: DO NOT EDIT
+package org.elasticsearch.plan.a;
+import org.antlr.v4.runtime.tree.ParseTreeVisitor;
+
+/**
+ * This interface defines a complete generic visitor for a parse tree produced
+ * by {@link PlanAParser}.
+ *
+ * @param <T> The return type of the visit operation. Use {@link Void} for
+ * operations with no return type.
+ */
+interface PlanAParserVisitor<T> extends ParseTreeVisitor<T> {
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#source}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitSource(PlanAParser.SourceContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code if}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitIf(PlanAParser.IfContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code while}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitWhile(PlanAParser.WhileContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code do}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDo(PlanAParser.DoContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code for}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitFor(PlanAParser.ForContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code decl}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDecl(PlanAParser.DeclContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code continue}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitContinue(PlanAParser.ContinueContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code break}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitBreak(PlanAParser.BreakContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code return}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitReturn(PlanAParser.ReturnContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code try}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitTry(PlanAParser.TryContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code throw}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitThrow(PlanAParser.ThrowContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code expr}
+   * labeled alternative in {@link PlanAParser#statement}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExpr(PlanAParser.ExprContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code multiple}
+   * labeled alternative in {@link PlanAParser#block}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitMultiple(PlanAParser.MultipleContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code single}
+   * labeled alternative in {@link PlanAParser#block}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitSingle(PlanAParser.SingleContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#empty}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitEmpty(PlanAParser.EmptyContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#initializer}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitInitializer(PlanAParser.InitializerContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#afterthought}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitAfterthought(PlanAParser.AfterthoughtContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#declaration}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDeclaration(PlanAParser.DeclarationContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#decltype}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDecltype(PlanAParser.DecltypeContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#declvar}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitDeclvar(PlanAParser.DeclvarContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code comp}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitComp(PlanAParser.CompContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code bool}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitBool(PlanAParser.BoolContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code conditional}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitConditional(PlanAParser.ConditionalContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code assignment}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitAssignment(PlanAParser.AssignmentContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code false}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitFalse(PlanAParser.FalseContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code numeric}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitNumeric(PlanAParser.NumericContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code unary}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitUnary(PlanAParser.UnaryContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code precedence}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitPrecedence(PlanAParser.PrecedenceContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code preinc}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitPreinc(PlanAParser.PreincContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code postinc}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitPostinc(PlanAParser.PostincContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code cast}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitCast(PlanAParser.CastContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code external}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExternal(PlanAParser.ExternalContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code null}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitNull(PlanAParser.NullContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code binary}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitBinary(PlanAParser.BinaryContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code char}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitChar(PlanAParser.CharContext ctx);
+  /**
+   * Visit a parse tree produced by the {@code true}
+   * labeled alternative in {@link PlanAParser#expression}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitTrue(PlanAParser.TrueContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extstart}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtstart(PlanAParser.ExtstartContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extprec}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtprec(PlanAParser.ExtprecContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extcast}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtcast(PlanAParser.ExtcastContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extbrace}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtbrace(PlanAParser.ExtbraceContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extdot}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtdot(PlanAParser.ExtdotContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#exttype}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExttype(PlanAParser.ExttypeContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extcall}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtcall(PlanAParser.ExtcallContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extvar}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtvar(PlanAParser.ExtvarContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extfield}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtfield(PlanAParser.ExtfieldContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extnew}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtnew(PlanAParser.ExtnewContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#extstring}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitExtstring(PlanAParser.ExtstringContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#arguments}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitArguments(PlanAParser.ArgumentsContext ctx);
+  /**
+   * Visit a parse tree produced by {@link PlanAParser#increment}.
+   * @param ctx the parse tree
+   * @return the visitor result
+   */
+  T visitIncrement(PlanAParser.IncrementContext ctx);
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAPlugin.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAPlugin.java
new file mode 100644
index 0000000..c893cd3
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAPlugin.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.ScriptModule;
+
+public final class PlanAPlugin extends Plugin {
+
+    @Override
+    public String name() {
+        return "lang-plan-a";
+    }
+
+    @Override
+    public String description() {
+        return "Plan A scripting language for Elasticsearch";
+    }
+
+    public void onModule(ScriptModule module) {
+        module.addScriptEngine(PlanAScriptEngineService.class);
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAScriptEngineService.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAScriptEngineService.java
new file mode 100644
index 0000000..6b3cd83
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAScriptEngineService.java
@@ -0,0 +1,140 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.elasticsearch.SpecialPermission;
+import org.elasticsearch.common.component.AbstractComponent;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.LeafSearchScript;
+import org.elasticsearch.script.ScriptEngineService;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
+
+import java.io.IOException;
+import java.security.AccessControlContext;
+import java.security.AccessController;
+import java.security.Permissions;
+import java.security.PrivilegedAction;
+import java.security.ProtectionDomain;
+import java.util.Map;
+
+public class PlanAScriptEngineService extends AbstractComponent implements ScriptEngineService {
+
+    public static final String NAME = "plan-a";
+    // TODO: this should really be per-script since scripts do so many different things?
+    private static final CompilerSettings compilerSettings = new CompilerSettings();
+    
+    public static final String NUMERIC_OVERFLOW = "plan-a.numeric_overflow";
+
+    // TODO: how should custom definitions be specified?
+    private Definition definition = null;
+
+    @Inject
+    public PlanAScriptEngineService(Settings settings) {
+        super(settings);
+        compilerSettings.setNumericOverflow(settings.getAsBoolean(NUMERIC_OVERFLOW, compilerSettings.getNumericOverflow()));
+    }
+
+    public void setDefinition(final Definition definition) {
+        this.definition = new Definition(definition);
+    }
+
+    @Override
+    public String[] types() {
+        return new String[] { NAME };
+    }
+
+    @Override
+    public String[] extensions() {
+        return new String[] { NAME };
+    }
+
+    @Override
+    public boolean sandboxed() {
+        return true;
+    }
+
+    // context used during compilation
+    private static final AccessControlContext COMPILATION_CONTEXT;
+    static {
+        Permissions none = new Permissions();
+        none.setReadOnly();
+        COMPILATION_CONTEXT = new AccessControlContext(new ProtectionDomain[] {
+                new ProtectionDomain(null, none)
+        });
+    }
+
+    @Override
+    public Object compile(String script) {
+        // check we ourselves are not being called by unprivileged code
+        SecurityManager sm = System.getSecurityManager();
+        if (sm != null) {
+            sm.checkPermission(new SpecialPermission());
+        }
+        // create our loader (which loads compiled code with no permissions)
+        Compiler.Loader loader = AccessController.doPrivileged(new PrivilegedAction<Compiler.Loader>() {
+            @Override
+            public Compiler.Loader run() {
+                return new Compiler.Loader(getClass().getClassLoader());
+            }
+        });
+        // drop all permissions to actually compile the code itself
+        return AccessController.doPrivileged(new PrivilegedAction<Executable>() {
+            @Override
+            public Executable run() {
+                return Compiler.compile(loader, "something", script, definition, compilerSettings);
+            }
+        }, COMPILATION_CONTEXT);
+    }
+
+    @Override
+    public ExecutableScript executable(CompiledScript compiledScript, Map<String,Object> vars) {
+        return new ScriptImpl((Executable) compiledScript.compiled(), vars, null);
+    }
+
+    @Override
+    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String,Object> vars) {
+        return new SearchScript() {
+            @Override
+            public LeafSearchScript getLeafSearchScript(LeafReaderContext context) throws IOException {
+                return new ScriptImpl((Executable) compiledScript.compiled(), vars, lookup.getLeafSearchLookup(context));
+            }
+
+            @Override
+            public boolean needsScores() {
+                return true; // TODO: maybe even do these different and more like expressions.
+            }
+        };
+    }
+
+    @Override
+    public void scriptRemoved(CompiledScript script) {
+        // nothing to do
+    }
+
+    @Override
+    public void close() throws IOException {
+        // nothing to do
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ScriptImpl.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ScriptImpl.java
new file mode 100644
index 0000000..3910cdc
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/ScriptImpl.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.apache.lucene.search.Scorer;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.LeafSearchScript;
+import org.elasticsearch.script.ScoreAccessor;
+import org.elasticsearch.search.lookup.LeafSearchLookup;
+
+import java.util.HashMap;
+import java.util.Map;
+
+final class ScriptImpl implements ExecutableScript, LeafSearchScript {
+    final Executable executable;
+    final Map<String,Object> variables;
+    final LeafSearchLookup lookup;
+    
+    ScriptImpl(Executable executable, Map<String,Object> vars, LeafSearchLookup lookup) {
+        this.executable = executable;
+        this.lookup = lookup;
+        this.variables = new HashMap<>();
+        if (vars != null) {
+            variables.putAll(vars);
+        }
+        if (lookup != null) {
+            variables.putAll(lookup.asMap());
+        }
+    }
+    
+    @Override
+    public void setNextVar(String name, Object value) {
+        variables.put(name, value);
+    }
+    
+    @Override
+    public Object run() {
+        return executable.execute(variables);
+    }
+    
+    @Override
+    public float runAsFloat() {
+        return ((Number) run()).floatValue();
+    }
+
+    @Override
+    public long runAsLong() {
+        return ((Number) run()).longValue();
+    }
+
+    @Override
+    public double runAsDouble() {
+        return ((Number) run()).doubleValue();
+    }
+    
+    @Override
+    public Object unwrap(Object value) {
+        return value;
+    }
+
+    @Override
+    public void setScorer(Scorer scorer) {
+        variables.put("_score", new ScoreAccessor(scorer));
+    }
+
+    @Override
+    public void setDocument(int doc) {
+        if (lookup != null) {
+            lookup.setDocument(doc);
+        }
+    }
+
+    @Override
+    public void setSource(Map<String,Object> source) {
+        if (lookup != null) {
+            lookup.source().setSource(source);
+        }
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Utility.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Utility.java
new file mode 100644
index 0000000..3bb5ae4
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Utility.java
@@ -0,0 +1,801 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+public class Utility {
+    public static boolean NumberToboolean(final Number value) {
+        return value.longValue() != 0;
+    }
+
+    public static char NumberTochar(final Number value) {
+        return (char)value.intValue();
+    }
+
+    public static Boolean NumberToBoolean(final Number value) {
+        return value.longValue() != 0;
+    }
+
+    public static Byte NumberToByte(final Number value) {
+        return value == null ? null : value.byteValue();
+    }
+
+    public static Short NumberToShort(final Number value) {
+        return value == null ? null : value.shortValue();
+    }
+
+    public static Character NumberToCharacter(final Number value) {
+        return value == null ? null : (char)value.intValue();
+    }
+
+    public static Integer NumberToInteger(final Number value) {
+        return value == null ? null : value.intValue();
+    }
+
+    public static Long NumberToLong(final Number value) {
+        return value == null ? null : value.longValue();
+    }
+
+    public static Float NumberToFloat(final Number value) {
+        return value == null ? null : value.floatValue();
+    }
+
+    public static Double NumberToDouble(final Number value) {
+        return value == null ? null : value.doubleValue();
+    }
+
+    public static byte booleanTobyte(final boolean value) {
+        return (byte)(value ? 1 : 0);
+    }
+
+    public static short booleanToshort(final boolean value) {
+        return (short)(value ? 1 : 0);
+    }
+
+    public static char booleanTochar(final boolean value) {
+        return (char)(value ? 1 : 0);
+    }
+
+    public static int booleanToint(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static long booleanTolong(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static float booleanTofloat(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static double booleanTodouble(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static Integer booleanToInteger(final boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static byte BooleanTobyte(final Boolean value) {
+        return (byte)(value ? 1 : 0);
+    }
+
+    public static short BooleanToshort(final Boolean value) {
+        return (short)(value ? 1 : 0);
+    }
+
+    public static char BooleanTochar(final Boolean value) {
+        return (char)(value ? 1 : 0);
+    }
+
+    public static int BooleanToint(final Boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static long BooleanTolong(final Boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static float BooleanTofloat(final Boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static double BooleanTodouble(final Boolean value) {
+        return value ? 1 : 0;
+    }
+
+    public static Byte BooleanToByte(final Boolean value) {
+        return value == null ? null : (byte)(value ? 1 : 0);
+    }
+
+    public static Short BooleanToShort(final Boolean value) {
+        return value == null ? null : (short)(value ? 1 : 0);
+    }
+
+    public static Character BooleanToCharacter(final Boolean value) {
+        return value == null ? null : (char)(value ? 1 : 0);
+    }
+
+    public static Integer BooleanToInteger(final Boolean value) {
+        return value == null ? null : value ? 1 : 0;
+    }
+
+    public static Long BooleanToLong(final Boolean value) {
+        return value == null ? null : value ? 1L : 0L;
+    }
+
+    public static Float BooleanToFloat(final Boolean value) {
+        return value == null ? null : value ? 1F : 0F;
+    }
+
+    public static Double BooleanToDouble(final Boolean value) {
+        return value == null ? null : value ? 1D : 0D;
+    }
+
+    public static boolean byteToboolean(final byte value) {
+        return value != 0;
+    }
+
+    public static Short byteToShort(final byte value) {
+        return (short)value;
+    }
+
+    public static Character byteToCharacter(final byte value) {
+        return (char)(byte)value;
+    }
+
+    public static Integer byteToInteger(final byte value) {
+        return (int)value;
+    }
+
+    public static Long byteToLong(final byte value) {
+        return (long)value;
+    }
+
+    public static Float byteToFloat(final byte value) {
+        return (float)value;
+    }
+
+    public static Double byteToDouble(final byte value) {
+        return (double)value;
+    }
+
+    public static boolean ByteToboolean(final Byte value) {
+        return value != 0;
+    }
+
+    public static char ByteTochar(final Byte value) {
+        return (char)value.byteValue();
+    }
+
+    public static boolean shortToboolean(final short value) {
+        return value != 0;
+    }
+
+    public static Byte shortToByte(final short value) {
+        return (byte)value;
+    }
+
+    public static Character shortToCharacter(final short value) {
+        return (char)(short)value;
+    }
+
+    public static Integer shortToInteger(final short value) {
+        return (int)value;
+    }
+
+    public static Long shortToLong(final short value) {
+        return (long)value;
+    }
+
+    public static Float shortToFloat(final short value) {
+        return (float)value;
+    }
+
+    public static Double shortToDouble(final short value) {
+        return (double)value;
+    }
+    
+    public static boolean ShortToboolean(final Short value) {
+        return value != 0;
+    }
+    
+    public static char ShortTochar(final Short value) {
+        return (char)value.shortValue();
+    }
+
+    public static boolean charToboolean(final char value) {
+        return value != 0;
+    }
+
+    public static Byte charToByte(final char value) {
+        return (byte)value;
+    }
+
+    public static Short charToShort(final char value) {
+        return (short)value;
+    }
+
+    public static Integer charToInteger(final char value) {
+        return (int)value;
+    }
+
+    public static Long charToLong(final char value) {
+        return (long)value;
+    }
+
+    public static Float charToFloat(final char value) {
+        return (float)value;
+    }
+
+    public static Double charToDouble(final char value) {
+        return (double)value;
+    }
+
+    public static boolean CharacterToboolean(final Character value) {
+        return value != 0;
+    }
+
+    public static byte CharacterTobyte(final Character value) {
+        return (byte)value.charValue();
+    }
+
+    public static short CharacterToshort(final Character value) {
+        return (short)value.charValue();
+    }
+
+    public static int CharacterToint(final Character value) {
+        return (int)value;
+    }
+
+    public static long CharacterTolong(final Character value) {
+        return (long)value;
+    }
+
+    public static float CharacterTofloat(final Character value) {
+        return (float)value;
+    }
+
+    public static double CharacterTodouble(final Character value) {
+        return (double)value;
+    }
+
+    public static Boolean CharacterToBoolean(final Character value) {
+        return value == null ? null : value != 0;
+    }
+
+    public static Byte CharacterToByte(final Character value) {
+        return value == null ? null : (byte)value.charValue();
+    }
+
+    public static Short CharacterToShort(final Character value) {
+        return value == null ? null : (short)value.charValue();
+    }
+
+    public static Integer CharacterToInteger(final Character value) {
+        return value == null ? null : (int)value;
+    }
+
+    public static Long CharacterToLong(final Character value) {
+        return value == null ? null : (long)value;
+    }
+
+    public static Float CharacterToFloat(final Character value) {
+        return value == null ? null : (float)value;
+    }
+
+    public static Double CharacterToDouble(final Character value) {
+        return value == null ? null : (double)value;
+    }
+
+    public static boolean intToboolean(final int value) {
+        return value != 0;
+    }
+
+    public static Byte intToByte(final int value) {
+        return (byte)value;
+    }
+
+    public static Short intToShort(final int value) {
+        return (short)value;
+    }
+
+    public static Character intToCharacter(final int value) {
+        return (char)(int)value;
+    }
+
+    public static Long intToLong(final int value) {
+        return (long)value;
+    }
+
+    public static Float intToFloat(final int value) {
+        return (float)value;
+    }
+
+    public static Double intToDouble(final int value) {
+        return (double)value;
+    }
+    
+    public static boolean IntegerToboolean(final Integer value) {
+        return value != 0;
+    }
+
+    public static char IntegerTochar(final Integer value) {
+        return (char)value.intValue();
+    }
+
+    public static boolean longToboolean(final long value) {
+        return value != 0;
+    }
+
+    public static Byte longToByte(final long value) {
+        return (byte)value;
+    }
+
+    public static Short longToShort(final long value) {
+        return (short)value;
+    }
+
+    public static Character longToCharacter(final long value) {
+        return (char)(long)value;
+    }
+
+    public static Integer longToInteger(final long value) {
+        return (int)value;
+    }
+
+    public static Float longToFloat(final long value) {
+        return (float)value;
+    }
+
+    public static Double longToDouble(final long value) {
+        return (double)value;
+    }
+    
+    public static boolean LongToboolean(final Long value) {
+        return value != 0;
+    }
+
+    public static char LongTochar(final Long value) {
+        return (char)value.longValue();
+    }
+
+    public static boolean floatToboolean(final float value) {
+        return value != 0;
+    }
+
+    public static Byte floatToByte(final float value) {
+        return (byte)value;
+    }
+
+    public static Short floatToShort(final float value) {
+        return (short)value;
+    }
+
+    public static Character floatToCharacter(final float value) {
+        return (char)(float)value;
+    }
+
+    public static Integer floatToInteger(final float value) {
+        return (int)value;
+    }
+
+    public static Long floatToLong(final float value) {
+        return (long)value;
+    }
+
+    public static Double floatToDouble(final float value) {
+        return (double)value;
+    }
+    
+    public static boolean FloatToboolean(final Float value) {
+        return value != 0;
+    }
+    
+    public static char FloatTochar(final Float value) {
+        return (char)value.floatValue();
+    }
+
+    public static boolean doubleToboolean(final double value) {
+        return value != 0;
+    }
+
+    public static Byte doubleToByte(final double value) {
+        return (byte)value;
+    }
+
+    public static Short doubleToShort(final double value) {
+        return (short)value;
+    }
+
+    public static Character doubleToCharacter(final double value) {
+        return (char)(double)value;
+    }
+
+    public static Integer doubleToInteger(final double value) {
+        return (int)value;
+    }
+
+    public static Long doubleToLong(final double value) {
+        return (long)value;
+    }
+    
+    public static Float doubleToFloat(final double value) {
+        return (float)value;
+    }
+    
+    public static boolean DoubleToboolean(final Double value) {
+        return value != 0;
+    }
+    
+    public static char DoubleTochar(final Double value) {
+        return (char)value.doubleValue();
+    }
+    
+    // although divide by zero is guaranteed, the special overflow case is not caught.
+    // its not needed for remainder because it is not possible there.
+    // see https://docs.oracle.com/javase/specs/jls/se8/html/jls-15.html#jls-15.17.2
+    
+    /**
+     * Integer divide without overflow
+     * @throws ArithmeticException on overflow or divide-by-zero
+     */
+    public static int divideWithoutOverflow(int x, int y) {
+       if (x == Integer.MIN_VALUE && y == -1) {
+           throw new ArithmeticException("integer overflow");
+       }
+       return x / y;
+    }
+    
+    /**
+     * Long divide without overflow
+     * @throws ArithmeticException on overflow or divide-by-zero
+     */
+    public static long divideWithoutOverflow(long x, long y) {
+        if (x == Long.MIN_VALUE && y == -1L) {
+            throw new ArithmeticException("long overflow");
+        }
+        return x / y;
+    }
+
+    // byte, short, and char are promoted to int for normal operations,
+    // so the JDK exact methods are typically used, and the result has a wider range.
+    // but compound assignments and increment/decrement operators (e.g. byte b = Byte.MAX_VALUE; b++;)
+    // implicitly cast back to the original type: so these need to be checked against the original range.
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for byte range.
+     */
+    public static byte toByteExact(int value) {
+        byte s = (byte) value;
+        if (s != value) {
+            throw new ArithmeticException("byte overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for byte range.
+     */
+    public static byte toByteExact(long value) {
+        byte s = (byte) value;
+        if (s != value) {
+            throw new ArithmeticException("byte overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for byte range.
+     */
+    public static byte toByteWithoutOverflow(float value) {
+        if (value < Byte.MIN_VALUE || value > Byte.MAX_VALUE) {
+            throw new ArithmeticException("byte overflow");
+        }
+        return (byte)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for byte range.
+     */
+    public static byte toByteWithoutOverflow(double value) {
+        if (value < Byte.MIN_VALUE || value > Byte.MAX_VALUE) {
+            throw new ArithmeticException("byte overflow");
+        }
+        return (byte)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for short range.
+     */
+    public static short toShortExact(int value) {
+        short s = (short) value;
+        if (s != value) {
+            throw new ArithmeticException("short overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for short range.
+     */
+    public static short toShortExact(long value) {
+        short s = (short) value;
+        if (s != value) {
+            throw new ArithmeticException("short overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for short range.
+     */
+    public static short toShortWithoutOverflow(float value) {
+        if (value < Short.MIN_VALUE || value > Short.MAX_VALUE) {
+            throw new ArithmeticException("short overflow");
+        }
+        return (short)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for short range.
+     */
+    public static short toShortExact(double value) {
+        if (value < Short.MIN_VALUE || value > Short.MAX_VALUE) {
+            throw new ArithmeticException("short overflow");
+        }
+        return (short)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for char range.
+     */
+    public static char toCharExact(int value) {
+        char s = (char) value;
+        if (s != value) {
+            throw new ArithmeticException("char overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for char range.
+     */
+    public static char toCharExact(long value) {
+        char s = (char) value;
+        if (s != value) {
+            throw new ArithmeticException("char overflow");
+        }
+        return s;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for char range.
+     */
+    public static char toCharWithoutOverflow(float value) {
+        if (value < Character.MIN_VALUE || value > Character.MAX_VALUE) {
+            throw new ArithmeticException("char overflow");
+        }
+        return (char)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for char range.
+     */
+    public static char toCharWithoutOverflow(double value) {
+        if (value < Character.MIN_VALUE || value > Character.MAX_VALUE) {
+            throw new ArithmeticException("char overflow");
+        }
+        return (char)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for int range.
+     */
+    public static int toIntWithoutOverflow(float value) {
+        if (value < Integer.MIN_VALUE || value > Integer.MAX_VALUE) {
+            throw new ArithmeticException("int overflow");
+        }
+        return (int)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for int range.
+     */
+    public static int toIntWithoutOverflow(double value) {
+        if (value < Integer.MIN_VALUE || value > Integer.MAX_VALUE) {
+            throw new ArithmeticException("int overflow");
+        }
+        return (int)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for long range.
+     */
+    public static long toLongExactWithoutOverflow(float value) {
+        if (value < Long.MIN_VALUE || value > Long.MAX_VALUE) {
+            throw new ArithmeticException("long overflow");
+        }
+        return (long)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for long range.
+     */
+    public static float toLongExactWithoutOverflow(double value) {
+        if (value < Long.MIN_VALUE || value > Long.MAX_VALUE) {
+            throw new ArithmeticException("long overflow");
+        }
+        return (long)value;
+    }
+
+    /**
+     * Like {@link Math#toIntExact(long)} but for float range.
+     */
+    public static float toFloatWithoutOverflow(double value) {
+        if (value < Float.MIN_VALUE || value > Float.MAX_VALUE) {
+            throw new ArithmeticException("float overflow");
+        }
+        return (float)value;
+    }
+
+    /**
+     * Checks for overflow, result is infinite but operands are finite
+     * @throws ArithmeticException if overflow occurred
+     */
+    private static float checkInfFloat(float x, float y, float z) {
+        if (Float.isInfinite(z)) {
+            if (Float.isFinite(x) && Float.isFinite(y)) {
+                throw new ArithmeticException("float overflow");
+            }
+        }
+        return z;
+    }
+    
+    /**
+     * Checks for NaN, result is NaN but operands are finite
+     * @throws ArithmeticException if overflow occurred
+     */
+    private static float checkNaNFloat(float x, float y, float z) {
+        if (Float.isNaN(z)) {
+            if (Float.isFinite(x) && Float.isFinite(y)) {
+                throw new ArithmeticException("NaN");
+            }
+        }
+        return z;
+    }
+    
+    /**
+     * Checks for NaN, result is infinite but operands are finite
+     * @throws ArithmeticException if overflow occurred
+     */
+    private static double checkInfDouble(double x, double y, double z) {
+        if (Double.isInfinite(z)) {
+            if (Double.isFinite(x) && Double.isFinite(y)) {
+                throw new ArithmeticException("double overflow");
+            }
+        }
+        return z;
+    }
+    
+    /**
+     * Checks for NaN, result is NaN but operands are finite
+     * @throws ArithmeticException if overflow occurred
+     */
+    private static double checkNaNDouble(double x, double y, double z) {
+        if (Double.isNaN(z)) {
+            if (Double.isFinite(x) && Double.isFinite(y)) {
+                throw new ArithmeticException("NaN");
+            }
+        }
+        return z;
+    }
+    
+    /**
+     * Adds two floats but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static float addWithoutOverflow(float x, float y) {
+        return checkInfFloat(x, y, x + y);
+    }
+    
+    /**
+     * Adds two doubles but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static double addWithoutOverflow(double x, double y) {
+        return checkInfDouble(x, y, x + y);
+    }
+    
+    /**
+     * Subtracts two floats but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static float subtractWithoutOverflow(float x, float y) {
+        return checkInfFloat(x, y, x - y);
+    }
+    
+    /**
+     * Subtracts two doubles but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static double subtractWithoutOverflow(double x, double y) {
+        return checkInfDouble(x, y , x - y);
+    }
+    
+    /**
+     * Multiplies two floats but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static float multiplyWithoutOverflow(float x, float y) {
+        return checkInfFloat(x, y, x * y);
+    }
+    
+    /**
+     * Multiplies two doubles but throws {@code ArithmeticException}
+     * if the result overflows.
+     */
+    public static double multiplyWithoutOverflow(double x, double y) {
+        return checkInfDouble(x, y, x * y);
+    }
+    
+    /**
+     * Divides two floats but throws {@code ArithmeticException}
+     * if the result overflows, or would create NaN from finite
+     * inputs ({@code x == 0, y == 0})
+     */
+    public static float divideWithoutOverflow(float x, float y) {
+        return checkNaNFloat(x, y, checkInfFloat(x, y, x / y));
+    }
+    
+    /**
+     * Divides two doubles but throws {@code ArithmeticException}
+     * if the result overflows, or would create NaN from finite
+     * inputs ({@code x == 0, y == 0})
+     */
+    public static double divideWithoutOverflow(double x, double y) {
+        return checkNaNDouble(x, y, checkInfDouble(x, y, x / y));
+    }
+    
+    /**
+     * Takes remainder two floats but throws {@code ArithmeticException}
+     * if the result would create NaN from finite inputs ({@code y == 0})
+     */
+    public static float remainderWithoutOverflow(float x, float y) {
+        return checkNaNFloat(x, y, x % y);
+    }
+    
+    /**
+     * Divides two doubles but throws {@code ArithmeticException}
+     * if the result would create NaN from finite inputs ({@code y == 0})
+     */
+    public static double remainderWithoutOverflow(double x, double y) {
+        return checkNaNDouble(x, y, x % y);
+    }
+
+    public static boolean checkEquals(final Object left, final Object right) {
+        if (left != null && right != null) {
+            return left.equals(right);
+        }
+
+        return left == null && right == null;
+    }
+
+    private Utility() {}
+}
diff --git a/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Writer.java b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Writer.java
new file mode 100644
index 0000000..3756e02
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/Writer.java
@@ -0,0 +1,2224 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.antlr.v4.runtime.ParserRuleContext;
+import org.antlr.v4.runtime.tree.ParseTree;
+import org.objectweb.asm.ClassWriter;
+import org.objectweb.asm.Label;
+import org.objectweb.asm.Opcodes;
+import org.objectweb.asm.commons.GeneratorAdapter;
+
+import java.util.ArrayDeque;
+import java.util.Deque;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import static org.elasticsearch.plan.a.Adapter.*;
+import static org.elasticsearch.plan.a.Definition.*;
+import static org.elasticsearch.plan.a.PlanAParser.*;
+
+class Writer extends PlanAParserBaseVisitor<Void> {
+    private static class Branch {
+        final ParserRuleContext source;
+
+        Label begin;
+        Label end;
+        Label tru;
+        Label fals;
+
+        private Branch(final ParserRuleContext source) {
+            this.source = source;
+
+            begin = null;
+            end = null;
+            tru = null;
+            fals = null;
+        }
+    }
+
+    final static String BASE_CLASS_NAME = Executable.class.getName();
+    final static String CLASS_NAME = BASE_CLASS_NAME + "$CompiledPlanAExecutable";
+    private final static org.objectweb.asm.Type BASE_CLASS_TYPE = org.objectweb.asm.Type.getType(Executable.class);
+    private final static org.objectweb.asm.Type CLASS_TYPE =
+            org.objectweb.asm.Type.getType("L" + CLASS_NAME.replace(".", "/") + ";");
+
+    private final static org.objectweb.asm.commons.Method CONSTRUCTOR = org.objectweb.asm.commons.Method.getMethod(
+            "void <init>(org.elasticsearch.plan.a.Definition, java.lang.String, java.lang.String)");
+    private final static org.objectweb.asm.commons.Method EXECUTE = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object execute(java.util.Map)");
+    private final static String SIGNATURE = "(Ljava/util/Map<Ljava/lang/String;Ljava/lang/Object;>;)Ljava/lang/Object;";
+
+    private final static org.objectweb.asm.Type DEFINITION_TYPE = org.objectweb.asm.Type.getType(Definition.class);
+
+    private final static org.objectweb.asm.commons.Method DEF_METHOD_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object methodCall(java.lang.Object, java.lang.String, " +
+            "org.elasticsearch.plan.a.Definition, java.lang.Object[], boolean[])");
+    private final static org.objectweb.asm.commons.Method DEF_ARRAY_STORE = org.objectweb.asm.commons.Method.getMethod(
+            "void arrayStore(java.lang.Object, java.lang.Object, java.lang.Object, " +
+            "org.elasticsearch.plan.a.Definition, boolean, boolean)");
+    private final static org.objectweb.asm.commons.Method DEF_ARRAY_LOAD = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object arrayLoad(java.lang.Object, java.lang.Object, " +
+            "org.elasticsearch.plan.a.Definition, boolean)");
+    private final static org.objectweb.asm.commons.Method DEF_FIELD_STORE = org.objectweb.asm.commons.Method.getMethod(
+            "void fieldStore(java.lang.Object, java.lang.Object, java.lang.String, " +
+            "org.elasticsearch.plan.a.Definition, boolean)");
+    private final static org.objectweb.asm.commons.Method DEF_FIELD_LOAD = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object fieldLoad(java.lang.Object, java.lang.String, org.elasticsearch.plan.a.Definition)");
+
+    private final static org.objectweb.asm.commons.Method DEF_NOT_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object not(java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_NEG_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object neg(java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_MUL_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object mul(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_DIV_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object div(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_REM_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object rem(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_ADD_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object add(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_SUB_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object sub(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_LSH_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object lsh(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_RSH_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object rsh(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_USH_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object ush(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_AND_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object and(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_XOR_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object xor(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_OR_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "java.lang.Object or(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_EQ_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean eq(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_LT_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean lt(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_LTE_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean lte(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_GT_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean gt(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method DEF_GTE_CALL = org.objectweb.asm.commons.Method.getMethod(
+            "boolean gte(java.lang.Object, java.lang.Object)");
+
+    private final static org.objectweb.asm.Type STRINGBUILDER_TYPE = org.objectweb.asm.Type.getType(StringBuilder.class);
+
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_CONSTRUCTOR =
+            org.objectweb.asm.commons.Method.getMethod("void <init>()");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_BOOLEAN =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(boolean)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_CHAR =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(char)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_INT =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(int)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_LONG =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(long)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(float)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(double)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_STRING =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(java.lang.String)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_APPEND_OBJECT =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.StringBuilder append(java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method STRINGBUILDER_TOSTRING =
+            org.objectweb.asm.commons.Method.getMethod("java.lang.String toString()");
+
+    private final static org.objectweb.asm.commons.Method TOINTEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("int toIntExact(long)");
+    private final static org.objectweb.asm.commons.Method NEGATEEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("int negateExact(int)");
+    private final static org.objectweb.asm.commons.Method NEGATEEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long negateExact(long)");
+    private final static org.objectweb.asm.commons.Method MULEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("int multiplyExact(int, int)");
+    private final static org.objectweb.asm.commons.Method MULEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long multiplyExact(long, long)");
+    private final static org.objectweb.asm.commons.Method ADDEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("int addExact(int, int)");
+    private final static org.objectweb.asm.commons.Method ADDEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long addExact(long, long)");
+    private final static org.objectweb.asm.commons.Method SUBEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("int subtractExact(int, int)");
+    private final static org.objectweb.asm.commons.Method SUBEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long subtractExact(long, long)");
+
+    private final static org.objectweb.asm.commons.Method CHECKEQUALS =
+            org.objectweb.asm.commons.Method.getMethod("boolean checkEquals(java.lang.Object, java.lang.Object)");
+    private final static org.objectweb.asm.commons.Method TOBYTEEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("byte toByteExact(int)");
+    private final static org.objectweb.asm.commons.Method TOBYTEEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("byte toByteExact(long)");
+    private final static org.objectweb.asm.commons.Method TOBYTEWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("byte toByteWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOBYTEWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("byte toByteWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOSHORTEXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("short toShortExact(int)");
+    private final static org.objectweb.asm.commons.Method TOSHORTEXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("short toShortExact(long)");
+    private final static org.objectweb.asm.commons.Method TOSHORTWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("short toShortWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOSHORTWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("short toShortWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOCHAREXACT_INT =
+            org.objectweb.asm.commons.Method.getMethod("char toCharExact(int)");
+    private final static org.objectweb.asm.commons.Method TOCHAREXACT_LONG =
+            org.objectweb.asm.commons.Method.getMethod("char toCharExact(long)");
+    private final static org.objectweb.asm.commons.Method TOCHARWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("char toCharWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOCHARWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("char toCharWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOINTWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("int toIntWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOINTWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("int toIntWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOLONGWOOVERFLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("long toLongExactWithoutOverflow(float)");
+    private final static org.objectweb.asm.commons.Method TOLONGWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("long toLongExactWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method TOFLOATWOOVERFLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("float toFloatWithoutOverflow(double)");
+    private final static org.objectweb.asm.commons.Method MULWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float multiplyWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method MULWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double multiplyWithoutOverflow(double, double)");
+    private final static org.objectweb.asm.commons.Method DIVWOOVERLOW_INT =
+            org.objectweb.asm.commons.Method.getMethod("int divideWithoutOverflow(int, int)");
+    private final static org.objectweb.asm.commons.Method DIVWOOVERLOW_LONG =
+            org.objectweb.asm.commons.Method.getMethod("long divideWithoutOverflow(long, long)");
+    private final static org.objectweb.asm.commons.Method DIVWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float divideWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method DIVWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double divideWithoutOverflow(double, double)");
+    private final static org.objectweb.asm.commons.Method REMWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float remainderWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method REMWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double remainderWithoutOverflow(double, double)");
+    private final static org.objectweb.asm.commons.Method ADDWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float addWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method ADDWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double addWithoutOverflow(double, double)");
+    private final static org.objectweb.asm.commons.Method SUBWOOVERLOW_FLOAT =
+            org.objectweb.asm.commons.Method.getMethod("float subtractWithoutOverflow(float, float)");
+    private final static org.objectweb.asm.commons.Method SUBWOOVERLOW_DOUBLE =
+            org.objectweb.asm.commons.Method.getMethod("double subtractWithoutOverflow(double, double)");
+
+    static byte[] write(Adapter adapter) {
+        Writer writer = new Writer(adapter);
+
+        return writer.getBytes();
+    }
+
+    private final Adapter adapter;
+    private final Definition definition;
+    private final ParseTree root;
+    private final String source;
+    private final CompilerSettings settings;
+
+    private final Map<ParserRuleContext, Branch> branches;
+    private final Deque<Branch> jumps;
+    private final Set<ParserRuleContext> strings;
+
+    private ClassWriter writer;
+    private GeneratorAdapter execute;
+
+    private Writer(final Adapter adapter) {
+        this.adapter = adapter;
+        definition = adapter.definition;
+        root = adapter.root;
+        source = adapter.source;
+        settings = adapter.settings;
+
+        branches = new HashMap<>();
+        jumps = new ArrayDeque<>();
+        strings = new HashSet<>();
+
+        writeBegin();
+        writeConstructor();
+        writeExecute();
+        writeEnd();
+    }
+
+    private Branch markBranch(final ParserRuleContext source, final ParserRuleContext... nodes) {
+        final Branch branch = new Branch(source);
+
+        for (final ParserRuleContext node : nodes) {
+            branches.put(node, branch);
+        }
+
+        return branch;
+    }
+
+    private void copyBranch(final Branch branch, final ParserRuleContext... nodes) {
+        for (final ParserRuleContext node : nodes) {
+            branches.put(node, branch);
+        }
+    }
+
+    private Branch getBranch(final ParserRuleContext source) {
+        return branches.get(source);
+    }
+
+    private void writeBegin() {
+        final int compute = ClassWriter.COMPUTE_FRAMES | ClassWriter.COMPUTE_MAXS;
+        final int version = Opcodes.V1_7;
+        final int access = Opcodes.ACC_PUBLIC | Opcodes.ACC_SUPER | Opcodes.ACC_FINAL | Opcodes.ACC_SYNTHETIC;
+        final String base = BASE_CLASS_TYPE.getInternalName();
+        final String name = CLASS_TYPE.getInternalName();
+
+        writer = new ClassWriter(compute);
+        writer.visit(version, access, name, null, base, null);
+        writer.visitSource(source, null);
+    }
+
+    private void writeConstructor() {
+        final int access = Opcodes.ACC_PUBLIC | Opcodes.ACC_SYNTHETIC;
+        final GeneratorAdapter constructor = new GeneratorAdapter(access, CONSTRUCTOR, null, null, writer);
+        constructor.loadThis();
+        constructor.loadArgs();
+        constructor.invokeConstructor(org.objectweb.asm.Type.getType(Executable.class), CONSTRUCTOR);
+        constructor.returnValue();
+        constructor.endMethod();
+    }
+
+    private void writeExecute() {
+        final int access = Opcodes.ACC_PUBLIC | Opcodes.ACC_SYNTHETIC;
+        execute = new GeneratorAdapter(access, EXECUTE, SIGNATURE, null, writer);
+        visit(root);
+        execute.endMethod();
+    }
+
+    @Override
+    public Void visitSource(final SourceContext ctx) {
+        final StatementMetadata sourcesmd = adapter.getStatementMetadata(ctx);
+
+        for (final StatementContext sctx : ctx.statement()) {
+            visit(sctx);
+        }
+
+        if (!sourcesmd.allReturn) {
+            execute.visitInsn(Opcodes.ACONST_NULL);
+            execute.returnValue();
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitIf(final IfContext ctx) {
+        final ExpressionContext exprctx = ctx.expression();
+        final boolean els = ctx.ELSE() != null;
+        final Branch branch = markBranch(ctx, exprctx);
+        branch.end = new Label();
+        branch.fals = els ? new Label() : branch.end;
+
+        visit(exprctx);
+
+        final BlockContext blockctx0 = ctx.block(0);
+        final StatementMetadata blockmd0 = adapter.getStatementMetadata(blockctx0);
+        visit(blockctx0);
+
+        if (els) {
+            if (!blockmd0.allExit) {
+                execute.goTo(branch.end);
+            }
+
+            execute.mark(branch.fals);
+            visit(ctx.block(1));
+        }
+
+        execute.mark(branch.end);
+
+        return null;
+    }
+
+    @Override
+    public Void visitWhile(final WhileContext ctx) {
+        final ExpressionContext exprctx = ctx.expression();
+        final Branch branch = markBranch(ctx, exprctx);
+        branch.begin = new Label();
+        branch.end = new Label();
+        branch.fals = branch.end;
+
+        jumps.push(branch);
+        execute.mark(branch.begin);
+        visit(exprctx);
+
+        final BlockContext blockctx = ctx.block();
+        boolean allexit = false;
+
+        if (blockctx != null) {
+            StatementMetadata blocksmd = adapter.getStatementMetadata(blockctx);
+            allexit = blocksmd.allExit;
+            visit(blockctx);
+        }
+
+        if (!allexit) {
+            execute.goTo(branch.begin);
+        }
+
+        execute.mark(branch.end);
+        jumps.pop();
+
+        return null;
+    }
+
+    @Override
+    public Void visitDo(final DoContext ctx) {
+        final ExpressionContext exprctx = ctx.expression();
+        final Branch branch = markBranch(ctx, exprctx);
+        branch.begin = new Label();
+        branch.end = new Label();
+        branch.fals = branch.end;
+
+        jumps.push(branch);
+        execute.mark(branch.begin);
+
+        final BlockContext bctx = ctx.block();
+        final StatementMetadata blocksmd = adapter.getStatementMetadata(bctx);
+        visit(bctx);
+
+        visit(exprctx);
+
+        if (!blocksmd.allExit) {
+            execute.goTo(branch.begin);
+        }
+
+        execute.mark(branch.end);
+        jumps.pop();
+
+        return null;
+    }
+
+    @Override
+    public Void visitFor(final ForContext ctx) {
+        final ExpressionContext exprctx = ctx.expression();
+        final AfterthoughtContext atctx = ctx.afterthought();
+        final Branch branch = markBranch(ctx, exprctx);
+        final Label start = new Label();
+        branch.begin = atctx == null ? start : new Label();
+        branch.end = new Label();
+        branch.fals = branch.end;
+
+        jumps.push(branch);
+
+        if (ctx.initializer() != null) {
+            visit(ctx.initializer());
+        }
+
+        execute.mark(start);
+
+        if (exprctx != null) {
+            visit(exprctx);
+        }
+
+        final BlockContext blockctx = ctx.block();
+        boolean allexit = false;
+
+        if (blockctx != null) {
+            StatementMetadata blocksmd = adapter.getStatementMetadata(blockctx);
+            allexit = blocksmd.allExit;
+            visit(blockctx);
+        }
+
+        if (atctx != null) {
+            execute.mark(branch.begin);
+            visit(atctx);
+        }
+
+        if (atctx != null || !allexit) {
+            execute.goTo(start);
+        }
+
+        execute.mark(branch.end);
+        jumps.pop();
+
+        return null;
+    }
+
+    @Override
+    public Void visitDecl(final DeclContext ctx) {
+        visit(ctx.declaration());
+
+        return null;
+    }
+
+    @Override
+    public Void visitContinue(final ContinueContext ctx) {
+        final Branch jump = jumps.peek();
+        execute.goTo(jump.begin);
+
+        return null;
+    }
+
+    @Override
+    public Void visitBreak(final BreakContext ctx) {
+        final Branch jump = jumps.peek();
+        execute.goTo(jump.end);
+
+        return null;
+    }
+
+    @Override
+    public Void visitReturn(final ReturnContext ctx) {
+        visit(ctx.expression());
+        execute.returnValue();
+
+        return null;
+    }
+
+    @Override
+    public Void visitExpr(final ExprContext ctx) {
+        final StatementMetadata exprsmd = adapter.getStatementMetadata(ctx);
+        final ExpressionContext exprctx = ctx.expression();
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(exprctx);
+        visit(exprctx);
+
+        if (exprsmd.allReturn) {
+            execute.returnValue();
+        } else {
+            writePop(expremd.to.type.getSize());
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitMultiple(final MultipleContext ctx) {
+        for (final StatementContext sctx : ctx.statement()) {
+            visit(sctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitSingle(final SingleContext ctx) {
+        visit(ctx.statement());
+
+        return null;
+    }
+
+    @Override
+    public Void visitEmpty(final EmptyContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected writer state.");
+    }
+
+    @Override
+    public Void visitInitializer(InitializerContext ctx) {
+        final DeclarationContext declctx = ctx.declaration();
+        final ExpressionContext exprctx = ctx.expression();
+
+        if (declctx != null) {
+            visit(declctx);
+        } else if (exprctx != null) {
+            visit(exprctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitAfterthought(AfterthoughtContext ctx) {
+        visit(ctx.expression());
+
+        return null;
+    }
+
+    @Override
+    public Void visitDeclaration(DeclarationContext ctx) {
+        for (final DeclvarContext declctx : ctx.declvar()) {
+            visit(declctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitDecltype(final DecltypeContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected writer state.");
+    }
+
+    @Override
+    public Void visitDeclvar(final DeclvarContext ctx) {
+        final ExpressionMetadata declvaremd = adapter.getExpressionMetadata(ctx);
+        final org.objectweb.asm.Type type = declvaremd.to.type;
+        final Sort sort = declvaremd.to.sort;
+        final int slot = (int)declvaremd.postConst;
+
+        final ExpressionContext exprctx = ctx.expression();
+        final boolean initialize = exprctx == null;
+
+        if (!initialize) {
+            visit(exprctx);
+        }
+
+        switch (sort) {
+            case VOID:   throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            case BOOL:
+            case BYTE:
+            case SHORT:
+            case CHAR:
+            case INT:    if (initialize) execute.push(0);    break;
+            case LONG:   if (initialize) execute.push(0L);   break;
+            case FLOAT:  if (initialize) execute.push(0.0F); break;
+            case DOUBLE: if (initialize) execute.push(0.0);  break;
+            default:     if (initialize) execute.visitInsn(Opcodes.ACONST_NULL);
+        }
+
+        execute.visitVarInsn(type.getOpcode(Opcodes.ISTORE), slot);
+
+        return null;
+    }
+
+    @Override
+    public Void visitPrecedence(final PrecedenceContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected writer state.");
+    }
+
+    @Override
+    public Void visitNumeric(final NumericContext ctx) {
+        final ExpressionMetadata numericemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = numericemd.postConst;
+
+        if (postConst == null) {
+            writeNumeric(ctx, numericemd.preConst);
+            checkWriteCast(numericemd);
+        } else {
+            writeConstant(ctx, postConst);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitChar(final CharContext ctx) {
+        final ExpressionMetadata charemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = charemd.postConst;
+
+        if (postConst == null) {
+            writeNumeric(ctx, (int)(char)charemd.preConst);
+            checkWriteCast(charemd);
+        } else {
+            writeConstant(ctx, postConst);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitTrue(final TrueContext ctx) {
+        final ExpressionMetadata trueemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = trueemd.postConst;
+        final Branch branch = getBranch(ctx);
+
+        if (branch == null) {
+            if (postConst == null) {
+                writeBoolean(ctx, true);
+                checkWriteCast(trueemd);
+            } else {
+                writeConstant(ctx, postConst);
+            }
+        } else if (branch.tru != null) {
+            execute.goTo(branch.tru);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitFalse(final FalseContext ctx) {
+        final ExpressionMetadata falseemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = falseemd.postConst;
+        final Branch branch = getBranch(ctx);
+
+        if (branch == null) {
+            if (postConst == null) {
+                writeBoolean(ctx, false);
+                checkWriteCast(falseemd);
+            } else {
+                writeConstant(ctx, postConst);
+            }
+        } else if (branch.fals != null) {
+            execute.goTo(branch.fals);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitNull(final NullContext ctx) {
+        final ExpressionMetadata nullemd = adapter.getExpressionMetadata(ctx);
+
+        execute.visitInsn(Opcodes.ACONST_NULL);
+        checkWriteCast(nullemd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitExternal(final ExternalContext ctx) {
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(ctx);
+        visit(ctx.extstart());
+        checkWriteCast(expremd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+
+    @Override
+    public Void visitPostinc(final PostincContext ctx) {
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(ctx);
+        visit(ctx.extstart());
+        checkWriteCast(expremd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitPreinc(final PreincContext ctx) {
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(ctx);
+        visit(ctx.extstart());
+        checkWriteCast(expremd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitUnary(final UnaryContext ctx) {
+        final ExpressionMetadata unaryemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = unaryemd.postConst;
+        final Object preConst = unaryemd.preConst;
+        final Branch branch = getBranch(ctx);
+
+        if (postConst != null) {
+            if (ctx.BOOLNOT() != null) {
+                if (branch == null) {
+                    writeConstant(ctx, postConst);
+                } else {
+                    if ((boolean)postConst && branch.tru != null) {
+                        execute.goTo(branch.tru);
+                    } else if (!(boolean)postConst && branch.fals != null) {
+                        execute.goTo(branch.fals);
+                    }
+                }
+            } else {
+                writeConstant(ctx, postConst);
+                checkWriteBranch(ctx);
+            }
+        } else if (preConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, preConst);
+                checkWriteCast(unaryemd);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+        } else {
+            final ExpressionContext exprctx = ctx.expression();
+
+            if (ctx.BOOLNOT() != null) {
+                final Branch local = markBranch(ctx, exprctx);
+
+                if (branch == null) {
+                    local.fals = new Label();
+                    final Label aend = new Label();
+
+                    visit(exprctx);
+
+                    execute.push(false);
+                    execute.goTo(aend);
+                    execute.mark(local.fals);
+                    execute.push(true);
+                    execute.mark(aend);
+
+                    checkWriteCast(unaryemd);
+                } else {
+                    local.tru = branch.fals;
+                    local.fals = branch.tru;
+
+                    visit(exprctx);
+                }
+            } else {
+                final org.objectweb.asm.Type type = unaryemd.from.type;
+                final Sort sort = unaryemd.from.sort;
+
+                visit(exprctx);
+
+                if (ctx.BWNOT() != null) {
+                    if (sort == Sort.DEF) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_NOT_CALL);
+                    } else {
+                        if (sort == Sort.INT) {
+                            writeConstant(ctx, -1);
+                        } else if (sort == Sort.LONG) {
+                            writeConstant(ctx, -1L);
+                        } else {
+                            throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                        }
+
+                        execute.math(GeneratorAdapter.XOR, type);
+                    }
+                } else if (ctx.SUB() != null) {
+                    if (sort == Sort.DEF) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_NEG_CALL);
+                    } else {
+                        if (settings.getNumericOverflow()) {
+                            execute.math(GeneratorAdapter.NEG, type);
+                        } else {
+                            if (sort == Sort.INT) {
+                                execute.invokeStatic(definition.mathType.type, NEGATEEXACT_INT);
+                            } else if (sort == Sort.LONG) {
+                                execute.invokeStatic(definition.mathType.type, NEGATEEXACT_LONG);
+                            } else {
+                                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                            }
+                        }
+                    }
+                } else if (ctx.ADD() == null) {
+                    throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                }
+
+                checkWriteCast(unaryemd);
+                checkWriteBranch(ctx);
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitCast(final CastContext ctx) {
+        final ExpressionMetadata castemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = castemd.postConst;
+
+        if (postConst == null) {
+            visit(ctx.expression());
+            checkWriteCast(castemd);
+        } else {
+            writeConstant(ctx, postConst);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitBinary(final BinaryContext ctx) {
+        final ExpressionMetadata binaryemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = binaryemd.postConst;
+        final Object preConst = binaryemd.preConst;
+        final Branch branch = getBranch(ctx);
+
+        if (postConst != null) {
+            writeConstant(ctx, postConst);
+        } else if (preConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, preConst);
+                checkWriteCast(binaryemd);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+        } else if (binaryemd.from.sort == Sort.STRING) {
+            final boolean marked = strings.contains(ctx);
+
+            if (!marked) {
+                writeNewStrings();
+            }
+
+            final ExpressionContext exprctx0 = ctx.expression(0);
+            final ExpressionMetadata expremd0 = adapter.getExpressionMetadata(exprctx0);
+            strings.add(exprctx0);
+            visit(exprctx0);
+
+            if (strings.contains(exprctx0)) {
+                writeAppendStrings(expremd0.from.sort);
+                strings.remove(exprctx0);
+            }
+
+            final ExpressionContext exprctx1 = ctx.expression(1);
+            final ExpressionMetadata expremd1 = adapter.getExpressionMetadata(exprctx1);
+            strings.add(exprctx1);
+            visit(exprctx1);
+
+            if (strings.contains(exprctx1)) {
+                writeAppendStrings(expremd1.from.sort);
+                strings.remove(exprctx1);
+            }
+
+            if (marked) {
+                strings.remove(ctx);
+            } else {
+                writeToStrings();
+            }
+
+            checkWriteCast(binaryemd);
+        } else {
+            final ExpressionContext exprctx0 = ctx.expression(0);
+            final ExpressionContext exprctx1 = ctx.expression(1);
+
+            visit(exprctx0);
+            visit(exprctx1);
+
+            final Type type = binaryemd.from;
+
+            if      (ctx.MUL()   != null) writeBinaryInstruction(ctx, type, MUL);
+            else if (ctx.DIV()   != null) writeBinaryInstruction(ctx, type, DIV);
+            else if (ctx.REM()   != null) writeBinaryInstruction(ctx, type, REM);
+            else if (ctx.ADD()   != null) writeBinaryInstruction(ctx, type, ADD);
+            else if (ctx.SUB()   != null) writeBinaryInstruction(ctx, type, SUB);
+            else if (ctx.LSH()   != null) writeBinaryInstruction(ctx, type, LSH);
+            else if (ctx.USH()   != null) writeBinaryInstruction(ctx, type, USH);
+            else if (ctx.RSH()   != null) writeBinaryInstruction(ctx, type, RSH);
+            else if (ctx.BWAND() != null) writeBinaryInstruction(ctx, type, BWAND);
+            else if (ctx.BWXOR() != null) writeBinaryInstruction(ctx, type, BWXOR);
+            else if (ctx.BWOR()  != null) writeBinaryInstruction(ctx, type, BWOR);
+            else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+
+            checkWriteCast(binaryemd);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitComp(final CompContext ctx) {
+        final ExpressionMetadata compemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = compemd.postConst;
+        final Object preConst = compemd.preConst;
+        final Branch branch = getBranch(ctx);
+
+        if (postConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, postConst);
+            } else {
+                if ((boolean)postConst && branch.tru != null) {
+                    execute.mark(branch.tru);
+                } else if (!(boolean)postConst && branch.fals != null) {
+                    execute.mark(branch.fals);
+                }
+            }
+        } else if (preConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, preConst);
+                checkWriteCast(compemd);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+        } else {
+            final ExpressionContext exprctx0 = ctx.expression(0);
+            final ExpressionMetadata expremd0 = adapter.getExpressionMetadata(exprctx0);
+
+            final ExpressionContext exprctx1 = ctx.expression(1);
+            final ExpressionMetadata expremd1 = adapter.getExpressionMetadata(exprctx1);
+            final org.objectweb.asm.Type type = expremd1.to.type;
+            final Sort sort1 = expremd1.to.sort;
+
+            visit(exprctx0);
+
+            if (!expremd1.isNull) {
+                visit(exprctx1);
+            }
+
+            final boolean tru = branch != null && branch.tru != null;
+            final boolean fals = branch != null && branch.fals != null;
+            final Label jump = tru ? branch.tru : fals ? branch.fals : new Label();
+            final Label end = new Label();
+
+            final boolean eq = (ctx.EQ() != null || ctx.EQR() != null) && (tru || !fals) ||
+                    (ctx.NE() != null || ctx.NER() != null) && fals;
+            final boolean ne = (ctx.NE() != null || ctx.NER() != null) && (tru || !fals) ||
+                    (ctx.EQ() != null || ctx.EQR() != null) && fals;
+            final boolean lt  = ctx.LT()  != null && (tru || !fals) || ctx.GTE() != null && fals;
+            final boolean lte = ctx.LTE() != null && (tru || !fals) || ctx.GT()  != null && fals;
+            final boolean gt  = ctx.GT()  != null && (tru || !fals) || ctx.LTE() != null && fals;
+            final boolean gte = ctx.GTE() != null && (tru || !fals) || ctx.LT()  != null && fals;
+
+            boolean writejump = true;
+
+            switch (sort1) {
+                case VOID:
+                case BYTE:
+                case SHORT:
+                case CHAR:
+                    throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                case BOOL:
+                    if      (eq) execute.ifZCmp(GeneratorAdapter.EQ, jump);
+                    else if (ne) execute.ifZCmp(GeneratorAdapter.NE, jump);
+                    else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case INT:
+                case LONG:
+                case FLOAT:
+                case DOUBLE:
+                    if      (eq)  execute.ifCmp(type, GeneratorAdapter.EQ, jump);
+                    else if (ne)  execute.ifCmp(type, GeneratorAdapter.NE, jump);
+                    else if (lt)  execute.ifCmp(type, GeneratorAdapter.LT, jump);
+                    else if (lte) execute.ifCmp(type, GeneratorAdapter.LE, jump);
+                    else if (gt)  execute.ifCmp(type, GeneratorAdapter.GT, jump);
+                    else if (gte) execute.ifCmp(type, GeneratorAdapter.GE, jump);
+                    else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case DEF:
+                    if (eq) {
+                        if (expremd1.isNull) {
+                            execute.ifNull(jump);
+                        } else if (!expremd0.isNull && ctx.EQ() != null) {
+                            execute.invokeStatic(definition.defobjType.type, DEF_EQ_CALL);
+                        } else {
+                            execute.ifCmp(type, GeneratorAdapter.EQ, jump);
+                        }
+                    } else if (ne) {
+                        if (expremd1.isNull) {
+                            execute.ifNonNull(jump);
+                        } else if (!expremd0.isNull && ctx.NE() != null) {
+                            execute.invokeStatic(definition.defobjType.type, DEF_EQ_CALL);
+                            execute.ifZCmp(GeneratorAdapter.EQ, jump);
+                        } else {
+                            execute.ifCmp(type, GeneratorAdapter.NE, jump);
+                        }
+                    } else if (lt) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_LT_CALL);
+                    } else if (lte) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_LTE_CALL);
+                    } else if (gt) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_GT_CALL);
+                    } else if (gte) {
+                        execute.invokeStatic(definition.defobjType.type, DEF_GTE_CALL);
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                    }
+
+                    writejump = expremd1.isNull || ne || ctx.EQR() != null;
+
+                    if (branch != null && !writejump) {
+                        execute.ifZCmp(GeneratorAdapter.NE, jump);
+                    }
+
+                    break;
+                default:
+                    if (eq) {
+                        if (expremd1.isNull) {
+                            execute.ifNull(jump);
+                        } else if (ctx.EQ() != null) {
+                            execute.invokeStatic(definition.utilityType.type, CHECKEQUALS);
+
+                            if (branch != null) {
+                                execute.ifZCmp(GeneratorAdapter.NE, jump);
+                            }
+
+                            writejump = false;
+                        } else {
+                            execute.ifCmp(type, GeneratorAdapter.EQ, jump);
+                        }
+                    } else if (ne) {
+                        if (expremd1.isNull) {
+                            execute.ifNonNull(jump);
+                        } else if (ctx.NE() != null) {
+                            execute.invokeStatic(definition.utilityType.type, CHECKEQUALS);
+                            execute.ifZCmp(GeneratorAdapter.EQ, jump);
+                        } else {
+                            execute.ifCmp(type, GeneratorAdapter.NE, jump);
+                        }
+                    } else {
+                        throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                    }
+            }
+
+            if (branch == null) {
+                if (writejump) {
+                    execute.push(false);
+                    execute.goTo(end);
+                    execute.mark(jump);
+                    execute.push(true);
+                    execute.mark(end);
+                }
+
+                checkWriteCast(compemd);
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitBool(final BoolContext ctx) {
+        final ExpressionMetadata boolemd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = boolemd.postConst;
+        final Object preConst = boolemd.preConst;
+        final Branch branch = getBranch(ctx);
+
+        if (postConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, postConst);
+            } else {
+                if ((boolean)postConst && branch.tru != null) {
+                    execute.mark(branch.tru);
+                } else if (!(boolean)postConst && branch.fals != null) {
+                    execute.mark(branch.fals);
+                }
+            }
+        } else if (preConst != null) {
+            if (branch == null) {
+                writeConstant(ctx, preConst);
+                checkWriteCast(boolemd);
+            } else {
+                throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+            }
+        } else {
+            final ExpressionContext exprctx0 = ctx.expression(0);
+            final ExpressionContext exprctx1 = ctx.expression(1);
+
+            if (branch == null) {
+                if (ctx.BOOLAND() != null) {
+                    final Branch local = markBranch(ctx, exprctx0, exprctx1);
+                    local.fals = new Label();
+                    final Label end = new Label();
+
+                    visit(exprctx0);
+                    visit(exprctx1);
+
+                    execute.push(true);
+                    execute.goTo(end);
+                    execute.mark(local.fals);
+                    execute.push(false);
+                    execute.mark(end);
+                } else if (ctx.BOOLOR() != null) {
+                    final Branch branch0 = markBranch(ctx, exprctx0);
+                    branch0.tru = new Label();
+                    final Branch branch1 = markBranch(ctx, exprctx1);
+                    branch1.fals = new Label();
+                    final Label aend = new Label();
+
+                    visit(exprctx0);
+                    visit(exprctx1);
+
+                    execute.mark(branch0.tru);
+                    execute.push(true);
+                    execute.goTo(aend);
+                    execute.mark(branch1.fals);
+                    execute.push(false);
+                    execute.mark(aend);
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                }
+
+                checkWriteCast(boolemd);
+            } else {
+                if (ctx.BOOLAND() != null) {
+                    final Branch branch0 = markBranch(ctx, exprctx0);
+                    branch0.fals = branch.fals == null ? new Label() : branch.fals;
+                    final Branch branch1 = markBranch(ctx, exprctx1);
+                    branch1.tru = branch.tru;
+                    branch1.fals = branch.fals;
+
+                    visit(exprctx0);
+                    visit(exprctx1);
+
+                    if (branch.fals == null) {
+                        execute.mark(branch0.fals);
+                    }
+                } else if (ctx.BOOLOR() != null) {
+                    final Branch branch0 = markBranch(ctx, exprctx0);
+                    branch0.tru = branch.tru == null ? new Label() : branch.tru;
+                    final Branch branch1 = markBranch(ctx, exprctx1);
+                    branch1.tru = branch.tru;
+                    branch1.fals = branch.fals;
+
+                    visit(exprctx0);
+                    visit(exprctx1);
+
+                    if (branch.tru == null) {
+                        execute.mark(branch0.tru);
+                    }
+                } else {
+                    throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+                }
+            }
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitConditional(final ConditionalContext ctx) {
+        final ExpressionMetadata condemd = adapter.getExpressionMetadata(ctx);
+        final Branch branch = getBranch(ctx);
+
+        final ExpressionContext expr0 = ctx.expression(0);
+        final ExpressionContext expr1 = ctx.expression(1);
+        final ExpressionContext expr2 = ctx.expression(2);
+
+        final Branch local = markBranch(ctx, expr0);
+        local.fals = new Label();
+        local.end = new Label();
+
+        if (branch != null) {
+            copyBranch(branch, expr1, expr2);
+        }
+
+        visit(expr0);
+        visit(expr1);
+        execute.goTo(local.end);
+        execute.mark(local.fals);
+        visit(expr2);
+        execute.mark(local.end);
+
+        if (branch == null) {
+            checkWriteCast(condemd);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitAssignment(final AssignmentContext ctx) {
+        final ExpressionMetadata expremd = adapter.getExpressionMetadata(ctx);
+        visit(ctx.extstart());
+        checkWriteCast(expremd);
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtstart(ExtstartContext ctx) {
+        final ExternalMetadata startemd = adapter.getExternalMetadata(ctx);
+
+        if (startemd.token == ADD) {
+            final ExpressionMetadata storeemd = adapter.getExpressionMetadata(startemd.storeExpr);
+
+            if (startemd.current.sort == Sort.STRING || storeemd.from.sort == Sort.STRING) {
+                writeNewStrings();
+                strings.add(startemd.storeExpr);
+            }
+        }
+
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            visit(precctx);
+        } else if (castctx != null) {
+            visit(castctx);
+        } else if (typectx != null) {
+            visit(typectx);
+        } else if (varctx != null) {
+            visit(varctx);
+        } else if (newctx != null) {
+            visit(newctx);
+        } else if (stringctx != null) {
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException();
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtprec(final ExtprecContext ctx) {
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            visit(precctx);
+        } else if (castctx != null) {
+            visit(castctx);
+        } else if (typectx != null) {
+            visit(typectx);
+        } else if (varctx != null) {
+            visit(varctx);
+        } else if (newctx != null) {
+            visit(newctx);
+        } else if (stringctx != null) {
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+        }
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtcast(final ExtcastContext ctx) {
+        ExtNodeMetadata castenmd = adapter.getExtNodeMetadata(ctx);
+
+        final ExtprecContext precctx = ctx.extprec();
+        final ExtcastContext castctx = ctx.extcast();
+        final ExttypeContext typectx = ctx.exttype();
+        final ExtvarContext varctx = ctx.extvar();
+        final ExtnewContext newctx = ctx.extnew();
+        final ExtstringContext stringctx = ctx.extstring();
+
+        if (precctx != null) {
+            visit(precctx);
+        } else if (castctx != null) {
+            visit(castctx);
+        } else if (typectx != null) {
+            visit(typectx);
+        } else if (varctx != null) {
+            visit(varctx);
+        } else if (newctx != null) {
+            visit(newctx);
+        } else if (stringctx != null) {
+            visit(stringctx);
+        } else {
+            throw new IllegalStateException(error(ctx) + "Unexpected writer state.");
+        }
+
+        checkWriteCast(ctx, castenmd.castTo);
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtbrace(final ExtbraceContext ctx) {
+        final ExpressionContext exprctx = adapter.updateExpressionTree(ctx.expression());
+
+        visit(exprctx);
+        writeLoadStoreExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtdot(final ExtdotContext ctx) {
+        final ExtcallContext callctx = ctx.extcall();
+        final ExtfieldContext fieldctx = ctx.extfield();
+
+        if (callctx != null) {
+            visit(callctx);
+        } else if (fieldctx != null) {
+            visit(fieldctx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExttype(final ExttypeContext ctx) {
+        visit(ctx.extdot());
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtcall(final ExtcallContext ctx) {
+        writeCallExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtvar(final ExtvarContext ctx) {
+        writeLoadStoreExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtfield(final ExtfieldContext ctx) {
+        writeLoadStoreExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtnew(ExtnewContext ctx) {
+        writeNewExternal(ctx);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitExtstring(ExtstringContext ctx) {
+        final ExtNodeMetadata stringenmd = adapter.getExtNodeMetadata(ctx);
+
+        writeConstant(ctx, stringenmd.target);
+
+        final ExtdotContext dotctx = ctx.extdot();
+        final ExtbraceContext bracectx = ctx.extbrace();
+
+        if (dotctx != null) {
+            visit(dotctx);
+        } else if (bracectx != null) {
+            visit(bracectx);
+        }
+
+        return null;
+    }
+
+    @Override
+    public Void visitArguments(final ArgumentsContext ctx) {
+        throw new UnsupportedOperationException(error(ctx) + "Unexpected writer state.");
+    }
+
+    @Override
+    public Void visitIncrement(IncrementContext ctx) {
+        final ExpressionMetadata incremd = adapter.getExpressionMetadata(ctx);
+        final Object postConst = incremd.postConst;
+
+        if (postConst == null) {
+            writeNumeric(ctx, incremd.preConst);
+            checkWriteCast(incremd);
+        } else {
+            writeConstant(ctx, postConst);
+        }
+
+        checkWriteBranch(ctx);
+
+        return null;
+    }
+
+    private void writeConstant(final ParserRuleContext source, final Object constant) {
+        if (constant instanceof Number) {
+            writeNumeric(source, constant);
+        } else if (constant instanceof Character) {
+            writeNumeric(source, (int)(char)constant);
+        } else if (constant instanceof String) {
+            writeString(source, constant);
+        } else if (constant instanceof Boolean) {
+            writeBoolean(source, constant);
+        } else if (constant != null) {
+            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+        }
+    }
+
+    private void writeNumeric(final ParserRuleContext source, final Object numeric) {
+        if (numeric instanceof Double) {
+            execute.push((double)numeric);
+        } else if (numeric instanceof Float) {
+            execute.push((float)numeric);
+        } else if (numeric instanceof Long) {
+            execute.push((long)numeric);
+        } else if (numeric instanceof Number) {
+            execute.push(((Number)numeric).intValue());
+        } else {
+            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+        }
+    }
+
+    private void writeString(final ParserRuleContext source, final Object string) {
+        if (string instanceof String) {
+            execute.push((String)string);
+        } else {
+            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+        }
+    }
+
+    private void writeBoolean(final ParserRuleContext source, final Object bool) {
+        if (bool instanceof Boolean) {
+            execute.push((boolean)bool);
+        } else {
+            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+        }
+    }
+
+    private void writeNewStrings() {
+        execute.newInstance(STRINGBUILDER_TYPE);
+        execute.dup();
+        execute.invokeConstructor(STRINGBUILDER_TYPE, STRINGBUILDER_CONSTRUCTOR);
+    }
+
+    private void writeAppendStrings(final Sort sort) {
+        switch (sort) {
+            case BOOL:   execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_BOOLEAN); break;
+            case CHAR:   execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_CHAR);    break;
+            case BYTE:
+            case SHORT:
+            case INT:    execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_INT);     break;
+            case LONG:   execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_LONG);    break;
+            case FLOAT:  execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_FLOAT);   break;
+            case DOUBLE: execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_DOUBLE);  break;
+            case STRING: execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_STRING);  break;
+            default:     execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_APPEND_OBJECT);
+        }
+    }
+
+    private void writeToStrings() {
+        execute.invokeVirtual(STRINGBUILDER_TYPE, STRINGBUILDER_TOSTRING);
+    }
+
+    private void writeBinaryInstruction(final ParserRuleContext source, final Type type, final int token) {
+        final Sort sort = type.sort;
+        final boolean exact = !settings.getNumericOverflow() &&
+                ((sort == Sort.INT || sort == Sort.LONG) &&
+                (token == MUL || token == DIV || token == ADD || token == SUB) ||
+                (sort == Sort.FLOAT || sort == Sort.DOUBLE) &&
+                (token == MUL || token == DIV || token == REM || token == ADD || token == SUB));
+
+        // if its a 64-bit shift, fixup the last argument to truncate to 32-bits
+        // note unlike java, this means we still do binary promotion of shifts,
+        // but it keeps things simple -- this check works because we promote shifts.
+        if (sort == Sort.LONG && (token == LSH || token == USH || token == RSH)) {
+            execute.cast(org.objectweb.asm.Type.LONG_TYPE, org.objectweb.asm.Type.INT_TYPE);
+        }
+
+        if (exact) {
+            switch (sort) {
+                case INT:
+                    switch (token) {
+                        case MUL: execute.invokeStatic(definition.mathType.type,    MULEXACT_INT);     break;
+                        case DIV: execute.invokeStatic(definition.utilityType.type, DIVWOOVERLOW_INT); break;
+                        case ADD: execute.invokeStatic(definition.mathType.type,    ADDEXACT_INT);     break;
+                        case SUB: execute.invokeStatic(definition.mathType.type,    SUBEXACT_INT);     break;
+                        default:
+                            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case LONG:
+                    switch (token) {
+                        case MUL: execute.invokeStatic(definition.mathType.type,    MULEXACT_LONG);     break;
+                        case DIV: execute.invokeStatic(definition.utilityType.type, DIVWOOVERLOW_LONG); break;
+                        case ADD: execute.invokeStatic(definition.mathType.type,    ADDEXACT_LONG);     break;
+                        case SUB: execute.invokeStatic(definition.mathType.type,    SUBEXACT_LONG);     break;
+                        default:
+                            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case FLOAT:
+                    switch (token) {
+                        case MUL: execute.invokeStatic(definition.utilityType.type, MULWOOVERLOW_FLOAT); break;
+                        case DIV: execute.invokeStatic(definition.utilityType.type, DIVWOOVERLOW_FLOAT); break;
+                        case REM: execute.invokeStatic(definition.utilityType.type, REMWOOVERLOW_FLOAT); break;
+                        case ADD: execute.invokeStatic(definition.utilityType.type, ADDWOOVERLOW_FLOAT); break;
+                        case SUB: execute.invokeStatic(definition.utilityType.type, SUBWOOVERLOW_FLOAT); break;
+                        default:
+                            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                    }
+
+                    break;
+                case DOUBLE:
+                    switch (token) {
+                        case MUL: execute.invokeStatic(definition.utilityType.type, MULWOOVERLOW_DOUBLE); break;
+                        case DIV: execute.invokeStatic(definition.utilityType.type, DIVWOOVERLOW_DOUBLE); break;
+                        case REM: execute.invokeStatic(definition.utilityType.type, REMWOOVERLOW_DOUBLE); break;
+                        case ADD: execute.invokeStatic(definition.utilityType.type, ADDWOOVERLOW_DOUBLE); break;
+                        case SUB: execute.invokeStatic(definition.utilityType.type, SUBWOOVERLOW_DOUBLE); break;
+                        default:
+                            throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                    }
+
+                    break;
+                default:
+                    throw new IllegalStateException(error(source) + "Unexpected writer state.");
+            }
+        } else {
+            if ((sort == Sort.FLOAT || sort == Sort.DOUBLE) &&
+                    (token == LSH || token == USH || token == RSH || token == BWAND || token == BWXOR || token == BWOR)) {
+                throw new IllegalStateException(error(source) + "Unexpected writer state.");
+            }
+
+            if (sort == Sort.DEF) {
+                switch (token) {
+                    case MUL:   execute.invokeStatic(definition.defobjType.type, DEF_MUL_CALL); break;
+                    case DIV:   execute.invokeStatic(definition.defobjType.type, DEF_DIV_CALL); break;
+                    case REM:   execute.invokeStatic(definition.defobjType.type, DEF_REM_CALL); break;
+                    case ADD:   execute.invokeStatic(definition.defobjType.type, DEF_ADD_CALL); break;
+                    case SUB:   execute.invokeStatic(definition.defobjType.type, DEF_SUB_CALL); break;
+                    case LSH:   execute.invokeStatic(definition.defobjType.type, DEF_LSH_CALL); break;
+                    case USH:   execute.invokeStatic(definition.defobjType.type, DEF_RSH_CALL); break;
+                    case RSH:   execute.invokeStatic(definition.defobjType.type, DEF_USH_CALL); break;
+                    case BWAND: execute.invokeStatic(definition.defobjType.type, DEF_AND_CALL); break;
+                    case BWXOR: execute.invokeStatic(definition.defobjType.type, DEF_XOR_CALL); break;
+                    case BWOR:  execute.invokeStatic(definition.defobjType.type, DEF_OR_CALL);  break;
+                    default:
+                        throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                }
+            } else {
+                switch (token) {
+                    case MUL:   execute.math(GeneratorAdapter.MUL,  type.type); break;
+                    case DIV:   execute.math(GeneratorAdapter.DIV,  type.type); break;
+                    case REM:   execute.math(GeneratorAdapter.REM,  type.type); break;
+                    case ADD:   execute.math(GeneratorAdapter.ADD,  type.type); break;
+                    case SUB:   execute.math(GeneratorAdapter.SUB,  type.type); break;
+                    case LSH:   execute.math(GeneratorAdapter.SHL,  type.type); break;
+                    case USH:   execute.math(GeneratorAdapter.USHR, type.type); break;
+                    case RSH:   execute.math(GeneratorAdapter.SHR,  type.type); break;
+                    case BWAND: execute.math(GeneratorAdapter.AND,  type.type); break;
+                    case BWXOR: execute.math(GeneratorAdapter.XOR,  type.type); break;
+                    case BWOR:  execute.math(GeneratorAdapter.OR,   type.type); break;
+                    default:
+                        throw new IllegalStateException(error(source) + "Unexpected writer state.");
+                }
+            }
+        }
+    }
+
+    /**
+     * Called for any compound assignment (including increment/decrement instructions).
+     * We have to be stricter than writeBinary, and do overflow checks against the original type's size
+     * instead of the promoted type's size, since the result will be implicitly cast back.
+     *
+     * @return true if an instruction is written, false otherwise
+     */
+    private boolean writeExactInstruction(final Sort osort, final Sort psort) {
+            if (psort == Sort.DOUBLE) {
+                if (osort == Sort.FLOAT) {
+                    execute.invokeStatic(definition.utilityType.type, TOFLOATWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.FLOAT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOFLOATWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.floatobjType.type);
+                } else if (osort == Sort.LONG) {
+                    execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.LONG_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.longobjType.type);
+                } else if (osort == Sort.INT) {
+                    execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.INT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.intobjType.type);
+                } else if (osort == Sort.CHAR) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.CHAR_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.charobjType.type);
+                } else if (osort == Sort.SHORT) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.SHORT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.shortobjType.type);
+                } else if (osort == Sort.BYTE) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_DOUBLE);
+                } else if (osort == Sort.BYTE_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_DOUBLE);
+                    execute.checkCast(definition.byteobjType.type);
+                } else {
+                    return false;
+                }
+            } else if (psort == Sort.FLOAT) {
+                if (osort == Sort.LONG) {
+                    execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.LONG_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOLONGWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.longobjType.type);
+                } else if (osort == Sort.INT) {
+                    execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.INT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOINTWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.intobjType.type);
+                } else if (osort == Sort.CHAR) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.CHAR_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHARWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.charobjType.type);
+                } else if (osort == Sort.SHORT) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.SHORT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.shortobjType.type);
+                } else if (osort == Sort.BYTE) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_FLOAT);
+                } else if (osort == Sort.BYTE_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEWOOVERFLOW_FLOAT);
+                    execute.checkCast(definition.byteobjType.type);
+                } else {
+                    return false;
+                }
+            } else if (psort == Sort.LONG) {
+                if (osort == Sort.INT) {
+                    execute.invokeStatic(definition.mathType.type, TOINTEXACT_LONG);
+                } else if (osort == Sort.INT_OBJ) {
+                    execute.invokeStatic(definition.mathType.type, TOINTEXACT_LONG);
+                    execute.checkCast(definition.intobjType.type);
+                } else if (osort == Sort.CHAR) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_LONG);
+                } else if (osort == Sort.CHAR_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_LONG);
+                    execute.checkCast(definition.charobjType.type);
+                } else if (osort == Sort.SHORT) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_LONG);
+                } else if (osort == Sort.SHORT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_LONG);
+                    execute.checkCast(definition.shortobjType.type);
+                } else if (osort == Sort.BYTE) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_LONG);
+                } else if (osort == Sort.BYTE_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_LONG);
+                    execute.checkCast(definition.byteobjType.type);
+                } else {
+                    return false;
+                }
+            } else if (psort == Sort.INT) {
+                if (osort == Sort.CHAR) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_INT);
+                } else if (osort == Sort.CHAR_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOCHAREXACT_INT);
+                    execute.checkCast(definition.charobjType.type);
+                } else if (osort == Sort.SHORT) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_INT);
+                } else if (osort == Sort.SHORT_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOSHORTEXACT_INT);
+                    execute.checkCast(definition.shortobjType.type);
+                } else if (osort == Sort.BYTE) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_INT);
+                } else if (osort == Sort.BYTE_OBJ) {
+                    execute.invokeStatic(definition.utilityType.type, TOBYTEEXACT_INT);
+                    execute.checkCast(definition.byteobjType.type);
+                } else {
+                    return false;
+                }
+            } else {
+                return false;
+            }
+
+        return true;
+    }
+
+    private void writeLoadStoreExternal(final ParserRuleContext source) {
+        final ExtNodeMetadata sourceenmd = adapter.getExtNodeMetadata(source);
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(sourceenmd.parent);
+
+        final boolean length = "#length".equals(sourceenmd.target);
+        final boolean array = "#brace".equals(sourceenmd.target);
+        final boolean name = sourceenmd.target instanceof String && !length && !array;
+        final boolean variable = sourceenmd.target instanceof Integer;
+        final boolean field = sourceenmd.target instanceof Field;
+        final boolean shortcut = sourceenmd.target instanceof Object[];
+
+        if (!length && !variable && !field && !array && !name && !shortcut) {
+            throw new IllegalStateException(error(source) + "Target not found for load/store.");
+        }
+
+        final boolean maplist = shortcut && (boolean)((Object[])sourceenmd.target)[2];
+        final Object constant = shortcut ? ((Object[])sourceenmd.target)[3] : null;
+
+        final boolean x1 = field || name || (shortcut && !maplist);
+        final boolean x2 = array || (shortcut && maplist);
+
+        if (length) {
+            execute.arrayLength();
+        } else if (sourceenmd.last && parentemd.storeExpr != null) {
+            final ExpressionMetadata expremd = adapter.getExpressionMetadata(parentemd.storeExpr);
+            final boolean cat = strings.contains(parentemd.storeExpr);
+
+            if (cat) {
+                if (field || name || shortcut) {
+                    execute.dupX1();
+                } else if (array) {
+                    execute.dup2X1();
+                }
+
+                if (maplist) {
+                    if (constant != null) {
+                        writeConstant(source, constant);
+                    }
+
+                    execute.dupX2();
+                }
+
+                writeLoadStoreInstruction(source, false, variable, field, name, array, shortcut);
+                writeAppendStrings(sourceenmd.type.sort);
+                visit(parentemd.storeExpr);
+
+                if (strings.contains(parentemd.storeExpr)) {
+                    writeAppendStrings(expremd.to.sort);
+                    strings.remove(parentemd.storeExpr);
+                }
+
+                writeToStrings();
+                checkWriteCast(source, sourceenmd.castTo);
+
+                if (parentemd.read) {
+                    writeDup(sourceenmd.type.sort.size, x1, x2);
+                }
+
+                writeLoadStoreInstruction(source, true, variable, field, name, array, shortcut);
+            } else if (parentemd.token > 0) {
+                final int token = parentemd.token;
+
+                if (field || name || shortcut) {
+                    execute.dup();
+                } else if (array) {
+                    execute.dup2();
+                }
+
+                if (maplist) {
+                    if (constant != null) {
+                        writeConstant(source, constant);
+                    }
+
+                    execute.dupX1();
+                }
+
+                writeLoadStoreInstruction(source, false, variable, field, name, array, shortcut);
+
+                if (parentemd.read && parentemd.post) {
+                    writeDup(sourceenmd.type.sort.size, x1, x2);
+                }
+
+                checkWriteCast(source, sourceenmd.castFrom);
+                visit(parentemd.storeExpr);
+
+                writeBinaryInstruction(source, sourceenmd.promote, token);
+
+                boolean exact = false;
+
+                if (!settings.getNumericOverflow() && expremd.typesafe && sourceenmd.type.sort != Sort.DEF &&
+                        (token == MUL || token == DIV || token == REM || token == ADD || token == SUB)) {
+                    exact = writeExactInstruction(sourceenmd.type.sort, sourceenmd.promote.sort);
+                }
+
+                if (!exact) {
+                    checkWriteCast(source, sourceenmd.castTo);
+                }
+
+                if (parentemd.read && !parentemd.post) {
+                    writeDup(sourceenmd.type.sort.size, x1, x2);
+                }
+
+                writeLoadStoreInstruction(source, true, variable, field, name, array, shortcut);
+            } else {
+                if (constant != null) {
+                    writeConstant(source, constant);
+                }
+
+                visit(parentemd.storeExpr);
+
+                if (parentemd.read) {
+                    writeDup(sourceenmd.type.sort.size, x1, x2);
+                }
+
+                writeLoadStoreInstruction(source, true, variable, field, name, array, shortcut);
+            }
+        } else {
+            if (constant != null) {
+                writeConstant(source, constant);
+            }
+
+            writeLoadStoreInstruction(source, false, variable, field, name, array, shortcut);
+        }
+    }
+
+    private void writeLoadStoreInstruction(final ParserRuleContext source,
+                                           final boolean store, final boolean variable,
+                                           final boolean field, final boolean name,
+                                           final boolean array, final boolean shortcut) {
+        final ExtNodeMetadata sourceemd = adapter.getExtNodeMetadata(source);
+
+        if (variable) {
+            writeLoadStoreVariable(source, store, sourceemd.type, (int)sourceemd.target);
+        } else if (field) {
+            writeLoadStoreField(store, (Field)sourceemd.target);
+        } else if (name) {
+            writeLoadStoreField(source, store, (String)sourceemd.target);
+        } else if (array) {
+            writeLoadStoreArray(source, store, sourceemd.type);
+        } else if (shortcut) {
+            Object[] targets = (Object[])sourceemd.target;
+            writeLoadStoreShortcut(store, (Method)targets[0], (Method)targets[1]);
+        } else {
+            throw new IllegalStateException(error(source) + "Load/Store requires a variable, field, or array.");
+        }
+    }
+
+    private void writeLoadStoreVariable(final ParserRuleContext source, final boolean store,
+                                        final Type type, final int slot) {
+        if (type.sort == Sort.VOID) {
+            throw new IllegalStateException(error(source) + "Cannot load/store void type.");
+        }
+
+        if (store) {
+            execute.visitVarInsn(type.type.getOpcode(Opcodes.ISTORE), slot);
+        } else {
+            execute.visitVarInsn(type.type.getOpcode(Opcodes.ILOAD), slot);
+        }
+    }
+
+    private void writeLoadStoreField(final boolean store, final Field field) {
+        if (java.lang.reflect.Modifier.isStatic(field.reflect.getModifiers())) {
+            if (store) {
+                execute.putStatic(field.owner.type, field.reflect.getName(), field.type.type);
+            } else {
+                execute.getStatic(field.owner.type, field.reflect.getName(), field.type.type);
+
+                if (!field.generic.clazz.equals(field.type.clazz)) {
+                    execute.checkCast(field.generic.type);
+                }
+            }
+        } else {
+            if (store) {
+                execute.putField(field.owner.type, field.reflect.getName(), field.type.type);
+            } else {
+                execute.getField(field.owner.type, field.reflect.getName(), field.type.type);
+
+                if (!field.generic.clazz.equals(field.type.clazz)) {
+                    execute.checkCast(field.generic.type);
+                }
+            }
+        }
+    }
+
+    private void writeLoadStoreField(final ParserRuleContext source, final boolean store, final String name) {
+        if (store) {
+            final ExtNodeMetadata sourceemd = adapter.getExtNodeMetadata(source);
+            final ExternalMetadata parentemd = adapter.getExternalMetadata(sourceemd.parent);
+            final ExpressionMetadata expremd = adapter.getExpressionMetadata(parentemd.storeExpr);
+
+            execute.push(name);
+            execute.loadThis();
+            execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+            execute.push(parentemd.token == 0 && expremd.typesafe);
+            execute.invokeStatic(definition.defobjType.type, DEF_FIELD_STORE);
+        } else {
+            execute.push(name);
+            execute.loadThis();
+            execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+            execute.invokeStatic(definition.defobjType.type, DEF_FIELD_LOAD);
+        }
+    }
+
+    private void writeLoadStoreArray(final ParserRuleContext source, final boolean store, final Type type) {
+        if (type.sort == Sort.VOID) {
+            throw new IllegalStateException(error(source) + "Cannot load/store void type.");
+        }
+
+        if (type.sort == Sort.DEF) {
+            final ExtbraceContext bracectx = (ExtbraceContext)source;
+            final ExpressionMetadata expremd0 = adapter.getExpressionMetadata(bracectx.expression());
+
+            if (store) {
+                final ExtNodeMetadata braceenmd = adapter.getExtNodeMetadata(bracectx);
+                final ExternalMetadata parentemd = adapter.getExternalMetadata(braceenmd.parent);
+                final ExpressionMetadata expremd1 = adapter.getExpressionMetadata(parentemd.storeExpr);
+
+                execute.loadThis();
+                execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+                execute.push(expremd0.typesafe);
+                execute.push(parentemd.token == 0 && expremd1.typesafe);
+                execute.invokeStatic(definition.defobjType.type, DEF_ARRAY_STORE);
+            } else {
+                execute.loadThis();
+                execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+                execute.push(expremd0.typesafe);
+                execute.invokeStatic(definition.defobjType.type, DEF_ARRAY_LOAD);
+            }
+        } else {
+            if (store) {
+                execute.arrayStore(type.type);
+            } else {
+                execute.arrayLoad(type.type);
+            }
+        }
+    }
+
+    private void writeLoadStoreShortcut(final boolean store, final Method getter, final Method setter) {
+        final Method method = store ? setter : getter;
+
+        if (java.lang.reflect.Modifier.isInterface(getter.owner.clazz.getModifiers())) {
+            execute.invokeInterface(method.owner.type, method.method);
+        } else {
+            execute.invokeVirtual(method.owner.type, method.method);
+        }
+
+        if (store) {
+            writePop(method.rtn.type.getSize());
+        } else if (!method.rtn.clazz.equals(method.handle.type().returnType())) {
+            execute.checkCast(method.rtn.type);
+        }
+    }
+
+    private void writeDup(final int size, final boolean x1, final boolean x2) {
+        if (size == 1) {
+            if (x2) {
+                execute.dupX2();
+            } else if (x1) {
+                execute.dupX1();
+            } else {
+                execute.dup();
+            }
+        } else if (size == 2) {
+            if (x2) {
+                execute.dup2X2();
+            } else if (x1) {
+                execute.dup2X1();
+            } else {
+                execute.dup2();
+            }
+        }
+    }
+
+    private void writeNewExternal(final ExtnewContext source) {
+        final ExtNodeMetadata sourceenmd = adapter.getExtNodeMetadata(source);
+        final ExternalMetadata parentemd = adapter.getExternalMetadata(sourceenmd.parent);
+
+        final boolean makearray = "#makearray".equals(sourceenmd.target);
+        final boolean constructor = sourceenmd.target instanceof Constructor;
+
+        if (!makearray && !constructor) {
+            throw new IllegalStateException(error(source) + "Target not found for new call.");
+        }
+
+        if (makearray) {
+            for (final ExpressionContext exprctx : source.expression()) {
+                visit(exprctx);
+            }
+
+            if (sourceenmd.type.sort == Sort.ARRAY) {
+                execute.visitMultiANewArrayInsn(sourceenmd.type.type.getDescriptor(), sourceenmd.type.type.getDimensions());
+            } else {
+                execute.newArray(sourceenmd.type.type);
+            }
+        } else {
+            execute.newInstance(sourceenmd.type.type);
+
+            if (parentemd.read) {
+                execute.dup();
+            }
+
+            for (final ExpressionContext exprctx : source.arguments().expression()) {
+                visit(exprctx);
+            }
+
+            final Constructor target = (Constructor)sourceenmd.target;
+            execute.invokeConstructor(target.owner.type, target.method);
+        }
+    }
+
+    private void writeCallExternal(final ExtcallContext source) {
+        final ExtNodeMetadata sourceenmd = adapter.getExtNodeMetadata(source);
+
+        final boolean method = sourceenmd.target instanceof Method;
+        final boolean def = sourceenmd.target instanceof String;
+
+        if (!method && !def) {
+            throw new IllegalStateException(error(source) + "Target not found for call.");
+        }
+
+        final List<ExpressionContext> arguments = source.arguments().expression();
+
+        if (method) {
+            for (final ExpressionContext exprctx : arguments) {
+                visit(exprctx);
+            }
+
+            final Method target = (Method)sourceenmd.target;
+
+            if (java.lang.reflect.Modifier.isStatic(target.reflect.getModifiers())) {
+                execute.invokeStatic(target.owner.type, target.method);
+            } else if (java.lang.reflect.Modifier.isInterface(target.owner.clazz.getModifiers())) {
+                execute.invokeInterface(target.owner.type, target.method);
+            } else {
+                execute.invokeVirtual(target.owner.type, target.method);
+            }
+
+            if (!target.rtn.clazz.equals(target.handle.type().returnType())) {
+                execute.checkCast(target.rtn.type);
+            }
+        } else {
+            execute.push((String)sourceenmd.target);
+            execute.loadThis();
+            execute.getField(CLASS_TYPE, "definition", DEFINITION_TYPE);
+
+            execute.push(arguments.size());
+            execute.newArray(definition.defType.type);
+
+            for (int argument = 0; argument < arguments.size(); ++argument) {
+                execute.dup();
+                execute.push(argument);
+                visit(arguments.get(argument));
+                execute.arrayStore(definition.defType.type);
+            }
+
+            execute.push(arguments.size());
+            execute.newArray(definition.booleanType.type);
+
+            for (int argument = 0; argument < arguments.size(); ++argument) {
+                execute.dup();
+                execute.push(argument);
+                execute.push(adapter.getExpressionMetadata(arguments.get(argument)).typesafe);
+                execute.arrayStore(definition.booleanType.type);
+            }
+
+            execute.invokeStatic(definition.defobjType.type, DEF_METHOD_CALL);
+        }
+    }
+
+    private void writePop(final int size) {
+        if (size == 1) {
+            execute.pop();
+        } else if (size == 2) {
+            execute.pop2();
+        }
+    }
+
+    private void checkWriteCast(final ExpressionMetadata sort) {
+        checkWriteCast(sort.source, sort.cast);
+    }
+
+    private void checkWriteCast(final ParserRuleContext source, final Cast cast) {
+        if (cast instanceof Transform) {
+            writeTransform((Transform)cast);
+        } else if (cast != null) {
+            writeCast(cast);
+        } else {
+            throw new IllegalStateException(error(source) + "Unexpected cast object.");
+        }
+    }
+
+    private void writeCast(final Cast cast) {
+        final Type from = cast.from;
+        final Type to = cast.to;
+
+        if (from.equals(to)) {
+            return;
+        }
+
+        if (from.sort.numeric && from.sort.primitive && to.sort.numeric && to.sort.primitive) {
+            execute.cast(from.type, to.type);
+        } else {
+            try {
+                from.clazz.asSubclass(to.clazz);
+            } catch (ClassCastException exception) {
+                execute.checkCast(to.type);
+            }
+        }
+    }
+
+    private void writeTransform(final Transform transform) {
+        if (transform.upcast != null) {
+            execute.checkCast(transform.upcast.type);
+        }
+
+        if (java.lang.reflect.Modifier.isStatic(transform.method.reflect.getModifiers())) {
+            execute.invokeStatic(transform.method.owner.type, transform.method.method);
+        } else if (java.lang.reflect.Modifier.isInterface(transform.method.owner.clazz.getModifiers())) {
+            execute.invokeInterface(transform.method.owner.type, transform.method.method);
+        } else {
+            execute.invokeVirtual(transform.method.owner.type, transform.method.method);
+        }
+
+        if (transform.downcast != null) {
+            execute.checkCast(transform.downcast.type);
+        }
+    }
+
+    void checkWriteBranch(final ParserRuleContext source) {
+        final Branch branch = getBranch(source);
+
+        if (branch != null) {
+            if (branch.tru != null) {
+                execute.visitJumpInsn(Opcodes.IFNE, branch.tru);
+            } else if (branch.fals != null) {
+                execute.visitJumpInsn(Opcodes.IFEQ, branch.fals);
+            }
+        }
+    }
+
+    private void writeEnd() {
+        writer.visitEnd();
+    }
+
+    private byte[] getBytes() {
+        return writer.toByteArray();
+    }
+}
diff --git a/plugins/lang-plan-a/src/main/plugin-metadata/plugin-security.policy b/plugins/lang-plan-a/src/main/plugin-metadata/plugin-security.policy
new file mode 100644
index 0000000..e45c1b8
--- /dev/null
+++ b/plugins/lang-plan-a/src/main/plugin-metadata/plugin-security.policy
@@ -0,0 +1,23 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+grant {
+  // needed to generate runtime classes
+  permission java.lang.RuntimePermission "createClassLoader";
+};
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AdditionTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AdditionTests.java
new file mode 100644
index 0000000..af7eb25
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AdditionTests.java
@@ -0,0 +1,199 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.lang.invoke.MethodHandles;
+import java.lang.invoke.MethodType;
+import java.util.HashMap;
+import java.util.Map;
+
+/** Tests for addition operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class AdditionTests extends ScriptTestCase {
+
+    public void testInt() throws Exception {
+        assertEquals(1+1, exec("int x = 1; int y = 1; return x+y;"));
+        assertEquals(1+2, exec("int x = 1; int y = 2; return x+y;"));
+        assertEquals(5+10, exec("int x = 5; int y = 10; return x+y;"));
+        assertEquals(1+1+2, exec("int x = 1; int y = 1; int z = 2; return x+y+z;"));
+        assertEquals((1+1)+2, exec("int x = 1; int y = 1; int z = 2; return (x+y)+z;"));
+        assertEquals(1+(1+2), exec("int x = 1; int y = 1; int z = 2; return x+(y+z);"));
+        assertEquals(0+1, exec("int x = 0; int y = 1; return x+y;"));
+        assertEquals(1+0, exec("int x = 1; int y = 0; return x+y;"));
+        assertEquals(0+0, exec("int x = 0; int y = 0; return x+y;"));
+        assertEquals(0+0, exec("int x = 0; int y = 0; return x+y;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1+1, exec("return 1+1;"));
+        assertEquals(1+2, exec("return 1+2;"));
+        assertEquals(5+10, exec("return 5+10;"));
+        assertEquals(1+1+2, exec("return 1+1+2;"));
+        assertEquals((1+1)+2, exec("return (1+1)+2;"));
+        assertEquals(1+(1+2), exec("return 1+(1+2);"));
+        assertEquals(0+1, exec("return 0+1;"));
+        assertEquals(1+0, exec("return 1+0;"));
+        assertEquals(0+0, exec("return 0+0;"));
+    }
+    
+    public void testByte() throws Exception {
+        assertEquals((byte)1+(byte)1, exec("byte x = 1; byte y = 1; return x+y;"));
+        assertEquals((byte)1+(byte)2, exec("byte x = 1; byte y = 2; return x+y;"));
+        assertEquals((byte)5+(byte)10, exec("byte x = 5; byte y = 10; return x+y;"));
+        assertEquals((byte)1+(byte)1+(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return x+y+z;"));
+        assertEquals(((byte)1+(byte)1)+(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return (x+y)+z;"));
+        assertEquals((byte)1+((byte)1+(byte)2), exec("byte x = 1; byte y = 1; byte z = 2; return x+(y+z);"));
+        assertEquals((byte)0+(byte)1, exec("byte x = 0; byte y = 1; return x+y;"));
+        assertEquals((byte)1+(byte)0, exec("byte x = 1; byte y = 0; return x+y;"));
+        assertEquals((byte)0+(byte)0, exec("byte x = 0; byte y = 0; return x+y;"));
+    }
+    
+    public void testByteConst() throws Exception {
+        assertEquals((byte)1+(byte)1, exec("return (byte)1+(byte)1;"));
+        assertEquals((byte)1+(byte)2, exec("return (byte)1+(byte)2;"));
+        assertEquals((byte)5+(byte)10, exec("return (byte)5+(byte)10;"));
+        assertEquals((byte)1+(byte)1+(byte)2, exec("return (byte)1+(byte)1+(byte)2;"));
+        assertEquals(((byte)1+(byte)1)+(byte)2, exec("return ((byte)1+(byte)1)+(byte)2;"));
+        assertEquals((byte)1+((byte)1+(byte)2), exec("return (byte)1+((byte)1+(byte)2);"));
+        assertEquals((byte)0+(byte)1, exec("return (byte)0+(byte)1;"));
+        assertEquals((byte)1+(byte)0, exec("return (byte)1+(byte)0;"));
+        assertEquals((byte)0+(byte)0, exec("return (byte)0+(byte)0;"));
+    }
+    
+    public void testChar() throws Exception {
+        assertEquals((char)1+(char)1, exec("char x = 1; char y = 1; return x+y;"));
+        assertEquals((char)1+(char)2, exec("char x = 1; char y = 2; return x+y;"));
+        assertEquals((char)5+(char)10, exec("char x = 5; char y = 10; return x+y;"));
+        assertEquals((char)1+(char)1+(char)2, exec("char x = 1; char y = 1; char z = 2; return x+y+z;"));
+        assertEquals(((char)1+(char)1)+(char)2, exec("char x = 1; char y = 1; char z = 2; return (x+y)+z;"));
+        assertEquals((char)1+((char)1+(char)2), exec("char x = 1; char y = 1; char z = 2; return x+(y+z);"));
+        assertEquals((char)0+(char)1, exec("char x = 0; char y = 1; return x+y;"));
+        assertEquals((char)1+(char)0, exec("char x = 1; char y = 0; return x+y;"));
+        assertEquals((char)0+(char)0, exec("char x = 0; char y = 0; return x+y;"));
+    }
+    
+    public void testCharConst() throws Exception {
+        assertEquals((char)1+(char)1, exec("return (char)1+(char)1;"));
+        assertEquals((char)1+(char)2, exec("return (char)1+(char)2;"));
+        assertEquals((char)5+(char)10, exec("return (char)5+(char)10;"));
+        assertEquals((char)1+(char)1+(char)2, exec("return (char)1+(char)1+(char)2;"));
+        assertEquals(((char)1+(char)1)+(char)2, exec("return ((char)1+(char)1)+(char)2;"));
+        assertEquals((char)1+((char)1+(char)2), exec("return (char)1+((char)1+(char)2);"));
+        assertEquals((char)0+(char)1, exec("return (char)0+(char)1;"));
+        assertEquals((char)1+(char)0, exec("return (char)1+(char)0;"));
+        assertEquals((char)0+(char)0, exec("return (char)0+(char)0;"));
+    }
+    
+    public void testShort() throws Exception {
+        assertEquals((short)1+(short)1, exec("short x = 1; short y = 1; return x+y;"));
+        assertEquals((short)1+(short)2, exec("short x = 1; short y = 2; return x+y;"));
+        assertEquals((short)5+(short)10, exec("short x = 5; short y = 10; return x+y;"));
+        assertEquals((short)1+(short)1+(short)2, exec("short x = 1; short y = 1; short z = 2; return x+y+z;"));
+        assertEquals(((short)1+(short)1)+(short)2, exec("short x = 1; short y = 1; short z = 2; return (x+y)+z;"));
+        assertEquals((short)1+((short)1+(short)2), exec("short x = 1; short y = 1; short z = 2; return x+(y+z);"));
+        assertEquals((short)0+(short)1, exec("short x = 0; short y = 1; return x+y;"));
+        assertEquals((short)1+(short)0, exec("short x = 1; short y = 0; return x+y;"));
+        assertEquals((short)0+(short)0, exec("short x = 0; short y = 0; return x+y;"));
+    }
+    
+    public void testShortConst() throws Exception {
+        assertEquals((short)1+(short)1, exec("return (short)1+(short)1;"));
+        assertEquals((short)1+(short)2, exec("return (short)1+(short)2;"));
+        assertEquals((short)5+(short)10, exec("return (short)5+(short)10;"));
+        assertEquals((short)1+(short)1+(short)2, exec("return (short)1+(short)1+(short)2;"));
+        assertEquals(((short)1+(short)1)+(short)2, exec("return ((short)1+(short)1)+(short)2;"));
+        assertEquals((short)1+((short)1+(short)2), exec("return (short)1+((short)1+(short)2);"));
+        assertEquals((short)0+(short)1, exec("return (short)0+(short)1;"));
+        assertEquals((short)1+(short)0, exec("return (short)1+(short)0;"));
+        assertEquals((short)0+(short)0, exec("return (short)0+(short)0;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L+1L, exec("long x = 1; long y = 1; return x+y;"));
+        assertEquals(1L+2L, exec("long x = 1; long y = 2; return x+y;"));
+        assertEquals(5L+10L, exec("long x = 5; long y = 10; return x+y;"));
+        assertEquals(1L+1L+2L, exec("long x = 1; long y = 1; long z = 2; return x+y+z;"));
+        assertEquals((1L+1L)+2L, exec("long x = 1; long y = 1; long z = 2; return (x+y)+z;"));
+        assertEquals(1L+(1L+2L), exec("long x = 1; long y = 1; long z = 2; return x+(y+z);"));
+        assertEquals(0L+1L, exec("long x = 0; long y = 1; return x+y;"));
+        assertEquals(1L+0L, exec("long x = 1; long y = 0; return x+y;"));
+        assertEquals(0L+0L, exec("long x = 0; long y = 0; return x+y;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L+1L, exec("return 1L+1L;"));
+        assertEquals(1L+2L, exec("return 1L+2L;"));
+        assertEquals(5L+10L, exec("return 5L+10L;"));
+        assertEquals(1L+1L+2L, exec("return 1L+1L+2L;"));
+        assertEquals((1L+1L)+2L, exec("return (1L+1L)+2L;"));
+        assertEquals(1L+(1L+2L), exec("return 1L+(1L+2L);"));
+        assertEquals(0L+1L, exec("return 0L+1L;"));
+        assertEquals(1L+0L, exec("return 1L+0L;"));
+        assertEquals(0L+0L, exec("return 0L+0L;"));
+    }
+
+    public void testFloat() throws Exception {
+        assertEquals(1F+1F, exec("float x = 1F; float y = 1F; return x+y;"));
+        assertEquals(1F+2F, exec("float x = 1F; float y = 2F; return x+y;"));
+        assertEquals(5F+10F, exec("float x = 5F; float y = 10F; return x+y;"));
+        assertEquals(1F+1F+2F, exec("float x = 1F; float y = 1F; float z = 2F; return x+y+z;"));
+        assertEquals((1F+1F)+2F, exec("float x = 1F; float y = 1F; float z = 2F; return (x+y)+z;"));
+        assertEquals((1F+1F)+2F, exec("float x = 1F; float y = 1F; float z = 2F; return x+(y+z);"));
+        assertEquals(0F+1F, exec("float x = 0F; float y = 1F; return x+y;"));
+        assertEquals(1F+0F, exec("float x = 1F; float y = 0F; return x+y;"));
+        assertEquals(0F+0F, exec("float x = 0F; float y = 0F; return x+y;"));
+    }
+
+    public void testFloatConst() throws Exception {
+        assertEquals(1F+1F, exec("return 1F+1F;"));
+        assertEquals(1F+2F, exec("return 1F+2F;"));
+        assertEquals(5F+10F, exec("return 5F+10F;"));
+        assertEquals(1F+1F+2F, exec("return 1F+1F+2F;"));
+        assertEquals((1F+1F)+2F, exec("return (1F+1F)+2F;"));
+        assertEquals(1F+(1F+2F), exec("return 1F+(1F+2F);"));
+        assertEquals(0F+1F, exec("return 0F+1F;"));
+        assertEquals(1F+0F, exec("return 1F+0F;"));
+        assertEquals(0F+0F, exec("return 0F+0F;"));
+    }
+
+    public void testDouble() throws Exception {
+        assertEquals(1.0+1.0, exec("double x = 1.0; double y = 1.0; return x+y;"));
+        assertEquals(1.0+2.0, exec("double x = 1.0; double y = 2.0; return x+y;"));
+        assertEquals(5.0+10.0, exec("double x = 5.0; double y = 10.0; return x+y;"));
+        assertEquals(1.0+1.0+2.0, exec("double x = 1.0; double y = 1.0; double z = 2.0; return x+y+z;"));
+        assertEquals((1.0+1.0)+2.0, exec("double x = 1.0; double y = 1.0; double z = 2.0; return (x+y)+z;"));
+        assertEquals(1.0+(1.0+2.0), exec("double x = 1.0; double y = 1.0; double z = 2.0; return x+(y+z);"));
+        assertEquals(0.0+1.0, exec("double x = 0.0; double y = 1.0; return x+y;"));
+        assertEquals(1.0+0.0, exec("double x = 1.0; double y = 0.0; return x+y;"));
+        assertEquals(0.0+0.0, exec("double x = 0.0; double y = 0.0; return x+y;"));
+    }
+    
+    public void testDoubleConst() throws Exception {
+        assertEquals(1.0+1.0, exec("return 1.0+1.0;"));
+        assertEquals(1.0+2.0, exec("return 1.0+2.0;"));
+        assertEquals(5.0+10.0, exec("return 5.0+10.0;"));
+        assertEquals(1.0+1.0+2.0, exec("return 1.0+1.0+2.0;"));
+        assertEquals((1.0+1.0)+2.0, exec("return (1.0+1.0)+2.0;"));
+        assertEquals(1.0+(1.0+2.0), exec("return 1.0+(1.0+2.0);"));
+        assertEquals(0.0+1.0, exec("return 0.0+1.0;"));
+        assertEquals(1.0+0.0, exec("return 1.0+0.0;"));
+        assertEquals(0.0+0.0, exec("return 0.0+0.0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AndTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AndTests.java
new file mode 100644
index 0000000..6a41684
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/AndTests.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for and operator across all types */
+public class AndTests extends ScriptTestCase {
+    
+    public void testInt() throws Exception {
+        assertEquals(5 & 12, exec("int x = 5; int y = 12; return x & y;"));
+        assertEquals(5 & -12, exec("int x = 5; int y = -12; return x & y;"));
+        assertEquals(7 & 15 & 3, exec("int x = 7; int y = 15; int z = 3; return x & y & z;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(5 & 12, exec("return 5 & 12;"));
+        assertEquals(5 & -12, exec("return 5 & -12;"));
+        assertEquals(7 & 15 & 3, exec("return 7 & 15 & 3;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(5L & 12L, exec("long x = 5; long y = 12; return x & y;"));
+        assertEquals(5L & -12L, exec("long x = 5; long y = -12; return x & y;"));
+        assertEquals(7L & 15L & 3L, exec("long x = 7; long y = 15; long z = 3; return x & y & z;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(5L & 12L, exec("return 5L & 12L;"));
+        assertEquals(5L & -12L, exec("return 5L & -12L;"));
+        assertEquals(7L & 15L & 3L, exec("return 7L & 15L & 3L;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicExpressionTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicExpressionTests.java
new file mode 100644
index 0000000..6af8ada
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicExpressionTests.java
@@ -0,0 +1,126 @@
+package org.elasticsearch.plan.a;
+
+import java.util.Collections;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+public class BasicExpressionTests extends ScriptTestCase {
+
+    /** simple tests returning a constant value */
+    public void testReturnConstant() {
+        assertEquals(5, exec("return 5;"));
+        assertEquals(7L, exec("return 7L;"));
+        assertEquals(7.0, exec("return 7.0;"));
+        assertEquals(32.0F, exec("return 32.0F;"));
+        assertEquals((byte)255, exec("return (byte)255;"));
+        assertEquals((short)5, exec("return (short)5;"));
+        assertEquals("string", exec("return \"string\";"));
+        assertEquals(true, exec("return true;"));
+        assertEquals(false, exec("return false;"));
+        assertNull(exec("return null;"));
+    }
+
+    public void testReturnConstantChar() {
+        assertEquals('x', exec("return 'x';"));
+    }
+
+    public void testConstantCharTruncation() {
+        assertEquals('蚠', exec("return (char)100000;"));
+    }
+
+    /** declaring variables for primitive types */
+    public void testDeclareVariable() {
+        assertEquals(5, exec("int i = 5; return i;"));
+        assertEquals(7L, exec("long l = 7; return l;"));
+        assertEquals(7.0, exec("double d = 7; return d;"));
+        assertEquals(32.0F, exec("float f = 32F; return f;"));
+        assertEquals((byte)255, exec("byte b = (byte)255; return b;"));
+        assertEquals((short)5, exec("short s = (short)5; return s;"));
+        assertEquals("string", exec("String s = \"string\"; return s;"));
+        assertEquals(true, exec("boolean v = true; return v;"));
+        assertEquals(false, exec("boolean v = false; return v;"));
+    }
+
+    public void testCast() {
+        assertEquals(1, exec("return (int)1.0;"));
+        assertEquals((byte)100, exec("double x = 100; return (byte)x;"));
+
+        assertEquals(3, exec(
+                "Map x = new HashMap();\n" +
+                "Object y = x;\n" +
+                "((Map)y).put(2, 3);\n" +
+                "return x.get(2);\n"));
+    }
+
+    public void testCat() {
+        assertEquals("aaabbb", exec("return \"aaa\" + \"bbb\";"));
+        assertEquals("aaabbb", exec("String aaa = \"aaa\", bbb = \"bbb\"; return aaa + bbb;"));
+
+        assertEquals("aaabbbbbbbbb", exec(
+                "String aaa = \"aaa\", bbb = \"bbb\"; int x;\n" +
+                "for (; x < 3; ++x) \n" +
+                "    aaa += bbb;\n" +
+                "return aaa;"));
+    }
+
+    public void testComp() {
+        assertEquals(true, exec("return 2 < 3;"));
+        assertEquals(false, exec("int x = 4; char y = 2; return x < y;"));
+        assertEquals(true, exec("return 3 <= 3;"));
+        assertEquals(true, exec("int x = 3; char y = 3; return x <= y;"));
+        assertEquals(false, exec("return 2 > 3;"));
+        assertEquals(true, exec("int x = 4; long y = 2; return x > y;"));
+        assertEquals(false, exec("return 3 >= 4;"));
+        assertEquals(true, exec("double x = 3; float y = 3; return x >= y;"));
+        assertEquals(false, exec("return 3 == 4;"));
+        assertEquals(true, exec("double x = 3; float y = 3; return x == y;"));
+        assertEquals(true, exec("return 3 != 4;"));
+        assertEquals(false, exec("double x = 3; float y = 3; return x != y;"));
+    }
+    
+    /** 
+     * Test boxed objects in various places
+     */
+    public void testBoxing() {
+        // return
+        assertEquals(4, exec("return input.get(\"x\");", Collections.singletonMap("x", 4)));
+        // assignment
+        assertEquals(4, exec("int y = (Integer)input.get(\"x\"); return y;", Collections.singletonMap("x", 4)));
+        // comparison
+        assertEquals(true, exec("return 5 > (Integer)input.get(\"x\");", Collections.singletonMap("x", 4)));
+    }
+
+    public void testBool() {
+        assertEquals(true, exec("return true && true;"));
+        assertEquals(false, exec("boolean a = true, b = false; return a && b;"));
+        assertEquals(true, exec("return true || true;"));
+        assertEquals(true, exec("boolean a = true, b = false; return a || b;"));
+    }
+
+    public void testConditional() {
+        assertEquals(1, exec("int x = 5; return x > 3 ? 1 : 0;"));
+        assertEquals(0, exec("String a = null; return a != null ? 1 : 0;"));
+    }
+
+    public void testPrecedence() {
+        assertEquals(2, exec("int x = 5; return (x+x)/x;"));
+        assertEquals(true, exec("boolean t = true, f = false; return t && (f || t);"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicStatementTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicStatementTests.java
new file mode 100644
index 0000000..07ad32d
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BasicStatementTests.java
@@ -0,0 +1,178 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+import java.util.HashMap;
+import java.util.Map;
+
+public class BasicStatementTests extends ScriptTestCase {
+
+    public void testIfStatement() {
+        assertEquals(1, exec("int x = 5; if (x == 5) return 1; return 0;"));
+        assertEquals(0, exec("int x = 4; if (x == 5) return 1; else return 0;"));
+        assertEquals(2, exec("int x = 4; if (x == 5) return 1; else if (x == 4) return 2; else return 0;"));
+        assertEquals(1, exec("int x = 4; if (x == 5) return 1; else if (x == 4) return 1; else return 0;"));
+
+        assertEquals(3, exec(
+                "int x = 5;\n" +
+                "if (x == 5) {\n" +
+                "    int y = 2;\n" +
+                "    \n" +
+                "    if (y == 2) {\n" +
+                "        x = 3;\n" +
+                "    }\n" +
+                "    \n" +
+                "}\n" +
+                "\n" +
+                "return x;\n"));
+    }
+
+    public void testWhileStatement() {
+
+        assertEquals("aaaaaa", exec("String c = \"a\"; int x; while (x < 5) { c += \"a\"; ++x; } return c;"));
+
+        Object value = exec(
+                " byte[][] b = new byte[5][5];       \n" +
+                " byte x = 0, y;                     \n" +
+                "                                    \n" +
+                " while (x < 5) {                    \n" +
+                "     y = 0;                         \n" +
+                "                                    \n" +
+                "     while (y < 5) {                \n" +
+                "         b[x][y] = (byte)(x*y);     \n" +
+                "         ++y;                       \n" +
+                "     }                              \n" +
+                "                                    \n" +
+                "     ++x;                           \n" +
+                " }                                  \n" +
+                "                                    \n" +
+                " return b;                          \n");
+
+        byte[][] b = (byte[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testDoWhileStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\"; int x; do { c += \"a\"; ++x; } while (x < 5); return c;"));
+
+        Object value = exec(
+                " int[][] b = new int[5][5]; \n" +
+                " int x = 0, y;                    \n" +
+                "                                  \n" +
+                " do {                             \n" +
+                "     y = 0;                       \n" +
+                "                                  \n" +
+                "     do {                         \n" +
+                "         b[x][y] = x*y;           \n" +
+                "         ++y;                     \n" +
+                "     } while (y < 5);             \n" +
+                "                                  \n" +
+                "     ++x;                         \n" +
+                " } while (x < 5);                 \n" +
+                "                                  \n" +
+                " return b;                        \n");
+
+        int[][] b = (int[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testForStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\"; for (int x = 0; x < 5; ++x) c += \"a\"; return c;"));
+
+        Object value = exec(
+                " int[][] b = new int[5][5];  \n" +
+                " for (int x = 0; x < 5; ++x) {     \n" +
+                "     for (int y = 0; y < 5; ++y) { \n" +
+                "         b[x][y] = x*y;            \n" +
+                "     }                             \n" +
+                " }                                 \n" +
+                "                                   \n" +
+                " return b;                         \n");
+
+        int[][] b = (int[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testDeclarationStatement() {
+        assertEquals((byte)2, exec("byte a = 2; return a;"));
+        assertEquals((short)2, exec("short a = 2; return a;"));
+        assertEquals((char)2, exec("char a = 2; return a;"));
+        assertEquals(2, exec("int a = 2; return a;"));
+        assertEquals(2L, exec("long a = 2; return a;"));
+        assertEquals(2F, exec("float a = 2; return a;"));
+        assertEquals(2.0, exec("double a = 2; return a;"));
+        assertEquals(false, exec("boolean a = false; return a;"));
+        assertEquals("string", exec("String a = \"string\"; return a;"));
+        assertEquals(HashMap.class, exec("Map<String,Object> a = new HashMap<String,Object>(); return a;").getClass());
+
+        assertEquals(byte[].class, exec("byte[] a = new byte[1]; return a;").getClass());
+        assertEquals(short[].class, exec("short[] a = new short[1]; return a;").getClass());
+        assertEquals(char[].class, exec("char[] a = new char[1]; return a;").getClass());
+        assertEquals(int[].class, exec("int[] a = new int[1]; return a;").getClass());
+        assertEquals(long[].class, exec("long[] a = new long[1]; return a;").getClass());
+        assertEquals(float[].class, exec("float[] a = new float[1]; return a;").getClass());
+        assertEquals(double[].class, exec("double[] a = new double[1]; return a;").getClass());
+        assertEquals(boolean[].class, exec("boolean[] a = new boolean[1]; return a;").getClass());
+        assertEquals(String[].class, exec("String[] a = new String[1]; return a;").getClass());
+        assertEquals(Map[].class, exec("Map<String,Object>[] a = new Map<String,Object>[1]; return a;").getClass());
+
+        assertEquals(byte[][].class, exec("byte[][] a = new byte[1][2]; return a;").getClass());
+        assertEquals(short[][][].class, exec("short[][][] a = new short[1][2][3]; return a;").getClass());
+        assertEquals(char[][][][].class, exec("char[][][][] a = new char[1][2][3][4]; return a;").getClass());
+        assertEquals(int[][][][][].class, exec("int[][][][][] a = new int[1][2][3][4][5]; return a;").getClass());
+        assertEquals(long[][].class, exec("long[][] a = new long[1][2]; return a;").getClass());
+        assertEquals(float[][][].class, exec("float[][][] a = new float[1][2][3]; return a;").getClass());
+        assertEquals(double[][][][].class, exec("double[][][][] a = new double[1][2][3][4]; return a;").getClass());
+        assertEquals(boolean[][][][][].class, exec("boolean[][][][][] a = new boolean[1][2][3][4][5]; return a;").getClass());
+        assertEquals(String[][].class, exec("String[][] a = new String[1][2]; return a;").getClass());
+        assertEquals(Map[][][].class, exec("Map<String,Object>[][][] a = new Map<String,Object>[1][2][3]; return a;").getClass());
+    }
+
+    public void testContinueStatement() {
+        assertEquals(9, exec("int x = 0, y = 0; while (x < 10) { ++x; if (x == 1) continue; ++y; } return y;"));
+    }
+
+    public void testBreakStatement() {
+        assertEquals(4, exec("int x = 0, y = 0; while (x < 10) { ++x; if (x == 5) break; ++y; } return y;"));
+    }
+
+    public void testReturnStatement() {
+        assertEquals(10, exec("return 10;"));
+        assertEquals(5, exec("int x = 5; return x;"));
+        assertEquals(4, exec("int[] x = new int[2]; x[1] = 4; return x[1];"));
+        assertEquals(5, ((short[])exec("short[] s = new short[3]; s[1] = 5; return s;"))[1]);
+        assertEquals(10, ((Map)exec("Map<String, Object> s = new HashMap< String , Object >(); s.put(\"x\", 10); return s;")).get("x"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BinaryOperatorTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BinaryOperatorTests.java
new file mode 100644
index 0000000..032cdcd
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/BinaryOperatorTests.java
@@ -0,0 +1,294 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** 
+ * Tests binary operators across different types
+ */
+// TODO: NaN/Inf/overflow/...
+public class BinaryOperatorTests extends ScriptTestCase {
+    
+    // TODO: move to per-type tests and test for each type
+    public void testBasics() {
+        assertEquals(2.25F / 1.5F, exec("return 2.25F / 1.5F;"));
+        assertEquals(2.25F % 1.5F, exec("return 2.25F % 1.5F;"));
+        assertEquals(2 - 1, exec("return 2 - 1;"));
+        assertEquals(1 << 2, exec("return 1 << 2;"));
+        assertEquals(4 >> 2, exec("return 4 >> 2;"));
+        assertEquals(-1 >>> 29, exec("return -1 >>> 29;"));
+        assertEquals(5 & 3, exec("return 5 & 3;"));
+        assertEquals(5 & 3L, exec("return 5 & 3L;"));
+        assertEquals(5L & 3, exec("return 5L & 3;"));
+        assertEquals(5 | 3, exec("return 5 | 3;"));
+        assertEquals(5L | 3, exec("return 5L | 3;"));
+        assertEquals(5 | 3L, exec("return 5 | 3L;"));
+        assertEquals(9 ^ 3, exec("return 9 ^ 3;"));
+        assertEquals(9L ^ 3, exec("return 9L ^ 3;"));
+        assertEquals(9 ^ 3L, exec("return 9 ^ 3L;"));
+    }
+    
+    public void testLongShifts() {
+        // note: we always promote the results of shifts too (unlike java)
+        assertEquals(1L << 2, exec("long x = 1L; int y = 2; return x << y;"));
+        assertEquals(1L << 2L, exec("long x = 1L; long y = 2L; return x << y;"));
+        assertEquals(4L >> 2L, exec("long x = 4L; long y = 2L; return x >> y;"));
+        assertEquals(4L >> 2, exec("long x = 4L; int y = 2; return x >> y;"));
+        assertEquals(-1L >>> 29, exec("long x = -1L; int y = 29; return x >>> y;"));
+        assertEquals(-1L >>> 29L, exec("long x = -1L; long y = 29L; return x >>> y;"));
+    }
+    
+    public void testLongShiftsConst() {
+        // note: we always promote the results of shifts too (unlike java)
+        assertEquals(1L << 2, exec("return 1L << 2;"));
+        assertEquals(1L << 2L, exec("return 1 << 2L;"));
+        assertEquals(4L >> 2L, exec("return 4 >> 2L;"));
+        assertEquals(4L >> 2, exec("return 4L >> 2;"));
+        assertEquals(-1L >>> 29, exec("return -1L >>> 29;"));
+        assertEquals(-1L >>> 29L, exec("return -1 >>> 29L;"));
+    }
+    
+    public void testMixedTypes() {
+        assertEquals(8, exec("int x = 4; char y = 2; return x*y;"));
+        assertEquals(0.5, exec("double x = 1; float y = 2; return x / y;"));
+        assertEquals(1, exec("int x = 3; int y = 2; return x % y;"));
+        assertEquals(3.0, exec("double x = 1; byte y = 2; return x + y;"));
+        assertEquals(-1, exec("int x = 1; char y = 2; return x - y;"));
+        assertEquals(4, exec("int x = 1; char y = 2; return x << y;"));
+        assertEquals(-1, exec("int x = -1; char y = 29; return x >> y;"));
+        assertEquals(3, exec("int x = -1; char y = 30; return x >>> y;"));
+        assertEquals(1L, exec("int x = 5; long y = 3; return x & y;"));
+        assertEquals(7, exec("short x = 5; byte y = 3; return x | y;"));
+        assertEquals(10, exec("short x = 9; char y = 3; return x ^ y;"));
+    }
+    
+    public void testBinaryPromotion() throws Exception {
+        // byte/byte
+        assertEquals((byte)1 + (byte)1, exec("byte x = 1; byte y = 1; return x+y;"));
+        // byte/char
+        assertEquals((byte)1 + (char)1, exec("byte x = 1; char y = 1; return x+y;"));
+        // byte/short
+        assertEquals((byte)1 + (short)1, exec("byte x = 1; short y = 1; return x+y;"));
+        // byte/int
+        assertEquals((byte)1 + 1, exec("byte x = 1; int y = 1; return x+y;"));
+        // byte/long
+        assertEquals((byte)1 + 1L, exec("byte x = 1; long y = 1; return x+y;"));
+        // byte/float
+        assertEquals((byte)1 + 1F, exec("byte x = 1; float y = 1; return x+y;"));
+        // byte/double
+        assertEquals((byte)1 + 1.0, exec("byte x = 1; double y = 1; return x+y;"));
+        
+        // char/byte
+        assertEquals((char)1 + (byte)1, exec("char x = 1; byte y = 1; return x+y;"));
+        // char/char
+        assertEquals((char)1 + (char)1, exec("char x = 1; char y = 1; return x+y;"));
+        // char/short
+        assertEquals((char)1 + (short)1, exec("char x = 1; short y = 1; return x+y;"));
+        // char/int
+        assertEquals((char)1 + 1, exec("char x = 1; int y = 1; return x+y;"));
+        // char/long
+        assertEquals((char)1 + 1L, exec("char x = 1; long y = 1; return x+y;"));
+        // char/float
+        assertEquals((char)1 + 1F, exec("char x = 1; float y = 1; return x+y;"));
+        // char/double
+        assertEquals((char)1 + 1.0, exec("char x = 1; double y = 1; return x+y;"));
+        
+        // short/byte
+        assertEquals((short)1 + (byte)1, exec("short x = 1; byte y = 1; return x+y;"));
+        // short/char
+        assertEquals((short)1 + (char)1, exec("short x = 1; char y = 1; return x+y;"));
+        // short/short
+        assertEquals((short)1 + (short)1, exec("short x = 1; short y = 1; return x+y;"));
+        // short/int
+        assertEquals((short)1 + 1, exec("short x = 1; int y = 1; return x+y;"));
+        // short/long
+        assertEquals((short)1 + 1L, exec("short x = 1; long y = 1; return x+y;"));
+        // short/float
+        assertEquals((short)1 + 1F, exec("short x = 1; float y = 1; return x+y;"));
+        // short/double
+        assertEquals((short)1 + 1.0, exec("short x = 1; double y = 1; return x+y;"));
+        
+        // int/byte
+        assertEquals(1 + (byte)1, exec("int x = 1; byte y = 1; return x+y;"));
+        // int/char
+        assertEquals(1 + (char)1, exec("int x = 1; char y = 1; return x+y;"));
+        // int/short
+        assertEquals(1 + (short)1, exec("int x = 1; short y = 1; return x+y;"));
+        // int/int
+        assertEquals(1 + 1, exec("int x = 1; int y = 1; return x+y;"));
+        // int/long
+        assertEquals(1 + 1L, exec("int x = 1; long y = 1; return x+y;"));
+        // int/float
+        assertEquals(1 + 1F, exec("int x = 1; float y = 1; return x+y;"));
+        // int/double
+        assertEquals(1 + 1.0, exec("int x = 1; double y = 1; return x+y;"));
+        
+        // long/byte
+        assertEquals(1L + (byte)1, exec("long x = 1; byte y = 1; return x+y;"));
+        // long/char
+        assertEquals(1L + (char)1, exec("long x = 1; char y = 1; return x+y;"));
+        // long/short
+        assertEquals(1L + (short)1, exec("long x = 1; short y = 1; return x+y;"));
+        // long/int
+        assertEquals(1L + 1, exec("long x = 1; int y = 1; return x+y;"));
+        // long/long
+        assertEquals(1L + 1L, exec("long x = 1; long y = 1; return x+y;"));
+        // long/float
+        assertEquals(1L + 1F, exec("long x = 1; float y = 1; return x+y;"));
+        // long/double
+        assertEquals(1L + 1.0, exec("long x = 1; double y = 1; return x+y;"));
+        
+        // float/byte
+        assertEquals(1F + (byte)1, exec("float x = 1; byte y = 1; return x+y;"));
+        // float/char
+        assertEquals(1F + (char)1, exec("float x = 1; char y = 1; return x+y;"));
+        // float/short
+        assertEquals(1F + (short)1, exec("float x = 1; short y = 1; return x+y;"));
+        // float/int
+        assertEquals(1F + 1, exec("float x = 1; int y = 1; return x+y;"));
+        // float/long
+        assertEquals(1F + 1L, exec("float x = 1; long y = 1; return x+y;"));
+        // float/float
+        assertEquals(1F + 1F, exec("float x = 1; float y = 1; return x+y;"));
+        // float/double
+        assertEquals(1F + 1.0, exec("float x = 1; double y = 1; return x+y;"));
+        
+        // double/byte
+        assertEquals(1.0 + (byte)1, exec("double x = 1; byte y = 1; return x+y;"));
+        // double/char
+        assertEquals(1.0 + (char)1, exec("double x = 1; char y = 1; return x+y;"));
+        // double/short
+        assertEquals(1.0 + (short)1, exec("double x = 1; short y = 1; return x+y;"));
+        // double/int
+        assertEquals(1.0 + 1, exec("double x = 1; int y = 1; return x+y;"));
+        // double/long
+        assertEquals(1.0 + 1L, exec("double x = 1; long y = 1; return x+y;"));
+        // double/float
+        assertEquals(1.0 + 1F, exec("double x = 1; float y = 1; return x+y;"));
+        // double/double
+        assertEquals(1.0 + 1.0, exec("double x = 1; double y = 1; return x+y;"));
+    }
+    
+    public void testBinaryPromotionConst() throws Exception {
+        // byte/byte
+        assertEquals((byte)1 + (byte)1, exec("return (byte)1 + (byte)1;"));
+        // byte/char
+        assertEquals((byte)1 + (char)1, exec("return (byte)1 + (char)1;"));
+        // byte/short
+        assertEquals((byte)1 + (short)1, exec("return (byte)1 + (short)1;"));
+        // byte/int
+        assertEquals((byte)1 + 1, exec("return (byte)1 + 1;"));
+        // byte/long
+        assertEquals((byte)1 + 1L, exec("return (byte)1 + 1L;"));
+        // byte/float
+        assertEquals((byte)1 + 1F, exec("return (byte)1 + 1F;"));
+        // byte/double
+        assertEquals((byte)1 + 1.0, exec("return (byte)1 + 1.0;"));
+        
+        // char/byte
+        assertEquals((char)1 + (byte)1, exec("return (char)1 + (byte)1;"));
+        // char/char
+        assertEquals((char)1 + (char)1, exec("return (char)1 + (char)1;"));
+        // char/short
+        assertEquals((char)1 + (short)1, exec("return (char)1 + (short)1;"));
+        // char/int
+        assertEquals((char)1 + 1, exec("return (char)1 + 1;"));
+        // char/long
+        assertEquals((char)1 + 1L, exec("return (char)1 + 1L;"));
+        // char/float
+        assertEquals((char)1 + 1F, exec("return (char)1 + 1F;"));
+        // char/double
+        assertEquals((char)1 + 1.0, exec("return (char)1 + 1.0;"));
+        
+        // short/byte
+        assertEquals((short)1 + (byte)1, exec("return (short)1 + (byte)1;"));
+        // short/char
+        assertEquals((short)1 + (char)1, exec("return (short)1 + (char)1;"));
+        // short/short
+        assertEquals((short)1 + (short)1, exec("return (short)1 + (short)1;"));
+        // short/int
+        assertEquals((short)1 + 1, exec("return (short)1 + 1;"));
+        // short/long
+        assertEquals((short)1 + 1L, exec("return (short)1 + 1L;"));
+        // short/float
+        assertEquals((short)1 + 1F, exec("return (short)1 + 1F;"));
+        // short/double
+        assertEquals((short)1 + 1.0, exec("return (short)1 + 1.0;"));
+        
+        // int/byte
+        assertEquals(1 + (byte)1, exec("return 1 + (byte)1;"));
+        // int/char
+        assertEquals(1 + (char)1, exec("return 1 + (char)1;"));
+        // int/short
+        assertEquals(1 + (short)1, exec("return 1 + (short)1;"));
+        // int/int
+        assertEquals(1 + 1, exec("return 1 + 1;"));
+        // int/long
+        assertEquals(1 + 1L, exec("return 1 + 1L;"));
+        // int/float
+        assertEquals(1 + 1F, exec("return 1 + 1F;"));
+        // int/double
+        assertEquals(1 + 1.0, exec("return 1 + 1.0;"));
+        
+        // long/byte
+        assertEquals(1L + (byte)1, exec("return 1L + (byte)1;"));
+        // long/char
+        assertEquals(1L + (char)1, exec("return 1L + (char)1;"));
+        // long/short
+        assertEquals(1L + (short)1, exec("return 1L + (short)1;"));
+        // long/int
+        assertEquals(1L + 1, exec("return 1L + 1;"));
+        // long/long
+        assertEquals(1L + 1L, exec("return 1L + 1L;"));
+        // long/float
+        assertEquals(1L + 1F, exec("return 1L + 1F;"));
+        // long/double
+        assertEquals(1L + 1.0, exec("return 1L + 1.0;"));
+        
+        // float/byte
+        assertEquals(1F + (byte)1, exec("return 1F + (byte)1;"));
+        // float/char
+        assertEquals(1F + (char)1, exec("return 1F + (char)1;"));
+        // float/short
+        assertEquals(1F + (short)1, exec("return 1F + (short)1;"));
+        // float/int
+        assertEquals(1F + 1, exec("return 1F + 1;"));
+        // float/long
+        assertEquals(1F + 1L, exec("return 1F + 1L;"));
+        // float/float
+        assertEquals(1F + 1F, exec("return 1F + 1F;"));
+        // float/double
+        assertEquals(1F + 1.0, exec("return 1F + 1.0;"));
+        
+        // double/byte
+        assertEquals(1.0 + (byte)1, exec("return 1.0 + (byte)1;"));
+        // double/char
+        assertEquals(1.0 + (char)1, exec("return 1.0 + (char)1;"));
+        // double/short
+        assertEquals(1.0 + (short)1, exec("return 1.0 + (short)1;"));
+        // double/int
+        assertEquals(1.0 + 1, exec("return 1.0 + 1;"));
+        // double/long
+        assertEquals(1.0 + 1L, exec("return 1.0 + 1L;"));
+        // double/float
+        assertEquals(1.0 + 1F, exec("return 1.0 + 1F;"));
+        // double/double
+        assertEquals(1.0 + 1.0, exec("return 1.0 + 1.0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/CompoundAssignmentTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/CompoundAssignmentTests.java
new file mode 100644
index 0000000..3af440a
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/CompoundAssignmentTests.java
@@ -0,0 +1,319 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/**
+ * Tests compound assignments (+=, etc) across all data types
+ */
+public class CompoundAssignmentTests extends ScriptTestCase {
+    public void testAddition() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 5; x += 10; return x;"));
+        assertEquals((byte) -5, exec("byte x = 5; x += -10; return x;"));
+
+        // short
+        assertEquals((short) 15, exec("short x = 5; x += 10; return x;"));
+        assertEquals((short) -5, exec("short x = 5; x += -10; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = 5; x += 10; return x;"));
+        assertEquals((char) 5, exec("char x = 10; x += -5; return x;"));
+        // int
+        assertEquals(15, exec("int x = 5; x += 10; return x;"));
+        assertEquals(-5, exec("int x = 5; x += -10; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 5; x += 10; return x;"));
+        assertEquals(-5L, exec("long x = 5; x += -10; return x;"));
+        // float
+        assertEquals(15F, exec("float x = 5f; x += 10; return x;"));
+        assertEquals(-5F, exec("float x = 5f; x += -10; return x;"));
+        // double
+        assertEquals(15D, exec("double x = 5.0; x += 10; return x;"));
+        assertEquals(-5D, exec("double x = 5.0; x += -10; return x;"));
+    }
+    
+    public void testSubtraction() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 5; x -= -10; return x;"));
+        assertEquals((byte) -5, exec("byte x = 5; x -= 10; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 5; x -= -10; return x;"));
+        assertEquals((short) -5, exec("short x = 5; x -= 10; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = 5; x -= -10; return x;"));
+        assertEquals((char) 5, exec("char x = 10; x -= 5; return x;"));
+        // int
+        assertEquals(15, exec("int x = 5; x -= -10; return x;"));
+        assertEquals(-5, exec("int x = 5; x -= 10; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 5; x -= -10; return x;"));
+        assertEquals(-5L, exec("long x = 5; x -= 10; return x;"));
+        // float
+        assertEquals(15F, exec("float x = 5f; x -= -10; return x;"));
+        assertEquals(-5F, exec("float x = 5f; x -= 10; return x;"));
+        // double
+        assertEquals(15D, exec("double x = 5.0; x -= -10; return x;"));
+        assertEquals(-5D, exec("double x = 5.0; x -= 10; return x;"));
+    }
+    
+    public void testMultiplication() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 5; x *= 3; return x;"));
+        assertEquals((byte) -5, exec("byte x = 5; x *= -1; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 5; x *= 3; return x;"));
+        assertEquals((short) -5, exec("short x = 5; x *= -1; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = 5; x *= 3; return x;"));
+        // int
+        assertEquals(15, exec("int x = 5; x *= 3; return x;"));
+        assertEquals(-5, exec("int x = 5; x *= -1; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 5; x *= 3; return x;"));
+        assertEquals(-5L, exec("long x = 5; x *= -1; return x;"));
+        // float
+        assertEquals(15F, exec("float x = 5f; x *= 3; return x;"));
+        assertEquals(-5F, exec("float x = 5f; x *= -1; return x;"));
+        // double
+        assertEquals(15D, exec("double x = 5.0; x *= 3; return x;"));
+        assertEquals(-5D, exec("double x = 5.0; x *= -1; return x;"));
+    }
+    
+    public void testDivision() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 45; x /= 3; return x;"));
+        assertEquals((byte) -5, exec("byte x = 5; x /= -1; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 45; x /= 3; return x;"));
+        assertEquals((short) -5, exec("short x = 5; x /= -1; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = 45; x /= 3; return x;"));
+        // int
+        assertEquals(15, exec("int x = 45; x /= 3; return x;"));
+        assertEquals(-5, exec("int x = 5; x /= -1; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 45; x /= 3; return x;"));
+        assertEquals(-5L, exec("long x = 5; x /= -1; return x;"));
+        // float
+        assertEquals(15F, exec("float x = 45f; x /= 3; return x;"));
+        assertEquals(-5F, exec("float x = 5f; x /= -1; return x;"));
+        // double
+        assertEquals(15D, exec("double x = 45.0; x /= 3; return x;"));
+        assertEquals(-5D, exec("double x = 5.0; x /= -1; return x;"));
+    }
+    
+    public void testDivisionByZero() {
+        // byte
+        try {
+            exec("byte x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+
+        // short
+        try {
+            exec("short x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 1; x /= 0; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testRemainder() {
+        // byte
+        assertEquals((byte) 3, exec("byte x = 15; x %= 4; return x;"));
+        assertEquals((byte) -3, exec("byte x = (byte) -15; x %= 4; return x;"));
+        // short
+        assertEquals((short) 3, exec("short x = 15; x %= 4; return x;"));
+        assertEquals((short) -3, exec("short x = (short) -15; x %= 4; return x;"));
+        // char
+        assertEquals((char) 3, exec("char x = (char) 15; x %= 4; return x;"));
+        // int
+        assertEquals(3, exec("int x = 15; x %= 4; return x;"));
+        assertEquals(-3, exec("int x = -15; x %= 4; return x;"));
+        // long
+        assertEquals(3L, exec("long x = 15L; x %= 4; return x;"));
+        assertEquals(-3L, exec("long x = -15L; x %= 4; return x;"));
+        // float
+        assertEquals(3F, exec("float x = 15F; x %= 4; return x;"));
+        assertEquals(-3F, exec("float x = -15F; x %= 4; return x;"));
+        // double
+        assertEquals(3D, exec("double x = 15.0; x %= 4; return x;"));
+        assertEquals(-3D, exec("double x = -15.0; x %= 4; return x;"));
+    }
+
+    public void testLeftShift() {
+        // byte
+        assertEquals((byte) 60, exec("byte x = 15; x <<= 2; return x;"));
+        assertEquals((byte) -60, exec("byte x = (byte) -15; x <<= 2; return x;"));
+        // short
+        assertEquals((short) 60, exec("short x = 15; x <<= 2; return x;"));
+        assertEquals((short) -60, exec("short x = (short) -15; x <<= 2; return x;"));
+        // char
+        assertEquals((char) 60, exec("char x = (char) 15; x <<= 2; return x;"));
+        // int
+        assertEquals(60, exec("int x = 15; x <<= 2; return x;"));
+        assertEquals(-60, exec("int x = -15; x <<= 2; return x;"));
+        // long
+        assertEquals(60L, exec("long x = 15L; x <<= 2; return x;"));
+        assertEquals(-60L, exec("long x = -15L; x <<= 2; return x;"));
+    }
+    
+    public void testRightShift() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 60; x >>= 2; return x;"));
+        assertEquals((byte) -15, exec("byte x = (byte) -60; x >>= 2; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 60; x >>= 2; return x;"));
+        assertEquals((short) -15, exec("short x = (short) -60; x >>= 2; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = (char) 60; x >>= 2; return x;"));
+        // int
+        assertEquals(15, exec("int x = 60; x >>= 2; return x;"));
+        assertEquals(-15, exec("int x = -60; x >>= 2; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 60L; x >>= 2; return x;"));
+        assertEquals(-15L, exec("long x = -60L; x >>= 2; return x;"));
+    }
+    
+    public void testUnsignedRightShift() {
+        // byte
+        assertEquals((byte) 15, exec("byte x = 60; x >>>= 2; return x;"));
+        assertEquals((byte) -15, exec("byte x = (byte) -60; x >>>= 2; return x;"));
+        // short
+        assertEquals((short) 15, exec("short x = 60; x >>>= 2; return x;"));
+        assertEquals((short) -15, exec("short x = (short) -60; x >>>= 2; return x;"));
+        // char
+        assertEquals((char) 15, exec("char x = (char) 60; x >>>= 2; return x;"));
+        // int
+        assertEquals(15, exec("int x = 60; x >>>= 2; return x;"));
+        assertEquals(-60 >>> 2, exec("int x = -60; x >>>= 2; return x;"));
+        // long
+        assertEquals(15L, exec("long x = 60L; x >>>= 2; return x;"));
+        assertEquals(-60L >>> 2, exec("long x = -60L; x >>>= 2; return x;"));
+    }
+
+    public void testAnd() {
+        // boolean
+        assertEquals(true, exec("boolean x = true; x &= true; return x;"));
+        assertEquals(false, exec("boolean x = true; x &= false; return x;"));
+        assertEquals(false, exec("boolean x = false; x &= true; return x;"));
+        assertEquals(false, exec("boolean x = false; x &= false; return x;"));
+        assertEquals(true, exec("Boolean x = true; x &= true; return x;"));
+        assertEquals(false, exec("Boolean x = true; x &= false; return x;"));
+        assertEquals(false, exec("Boolean x = false; x &= true; return x;"));
+        assertEquals(false, exec("Boolean x = false; x &= false; return x;"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] &= true; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] &= false; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] &= true; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] &= false; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] &= true; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] &= false; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] &= true; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] &= false; return x[0];"));
+        
+        // byte
+        assertEquals((byte) (13 & 14), exec("byte x = 13; x &= 14; return x;"));
+        // short
+        assertEquals((short) (13 & 14), exec("short x = 13; x &= 14; return x;"));
+        // char
+        assertEquals((char) (13 & 14), exec("char x = 13; x &= 14; return x;"));
+        // int
+        assertEquals(13 & 14, exec("int x = 13; x &= 14; return x;"));
+        // long
+        assertEquals((long) (13 & 14), exec("long x = 13L; x &= 14; return x;"));
+    }
+    
+    public void testOr() {
+        // boolean
+        assertEquals(true, exec("boolean x = true; x |= true; return x;"));
+        assertEquals(true, exec("boolean x = true; x |= false; return x;"));
+        assertEquals(true, exec("boolean x = false; x |= true; return x;"));
+        assertEquals(false, exec("boolean x = false; x |= false; return x;"));
+        assertEquals(true, exec("Boolean x = true; x |= true; return x;"));
+        assertEquals(true, exec("Boolean x = true; x |= false; return x;"));
+        assertEquals(true, exec("Boolean x = false; x |= true; return x;"));
+        assertEquals(false, exec("Boolean x = false; x |= false; return x;"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] |= true; return x[0];"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] |= false; return x[0];"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] |= true; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] |= false; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] |= true; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] |= false; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] |= true; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] |= false; return x[0];"));
+        
+        // byte
+        assertEquals((byte) (13 | 14), exec("byte x = 13; x |= 14; return x;"));
+        // short
+        assertEquals((short) (13 | 14), exec("short x = 13; x |= 14; return x;"));
+        // char
+        assertEquals((char) (13 | 14), exec("char x = 13; x |= 14; return x;"));
+        // int
+        assertEquals(13 | 14, exec("int x = 13; x |= 14; return x;"));
+        // long
+        assertEquals((long) (13 | 14), exec("long x = 13L; x |= 14; return x;"));
+    }
+    
+    public void testXor() {
+        // boolean
+        assertEquals(false, exec("boolean x = true; x ^= true; return x;"));
+        assertEquals(true, exec("boolean x = true; x ^= false; return x;"));
+        assertEquals(true, exec("boolean x = false; x ^= true; return x;"));
+        assertEquals(false, exec("boolean x = false; x ^= false; return x;"));
+        assertEquals(false, exec("Boolean x = true; x ^= true; return x;"));
+        assertEquals(true, exec("Boolean x = true; x ^= false; return x;"));
+        assertEquals(true, exec("Boolean x = false; x ^= true; return x;"));
+        assertEquals(false, exec("Boolean x = false; x ^= false; return x;"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] ^= true; return x[0];"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = true; x[0] ^= false; return x[0];"));
+        assertEquals(true, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] ^= true; return x[0];"));
+        assertEquals(false, exec("boolean[] x = new boolean[1]; x[0] = false; x[0] ^= false; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] ^= true; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = true; x[0] ^= false; return x[0];"));
+        assertEquals(true, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] ^= true; return x[0];"));
+        assertEquals(false, exec("Boolean[] x = new Boolean[1]; x[0] = false; x[0] ^= false; return x[0];"));
+        
+        // byte
+        assertEquals((byte) (13 ^ 14), exec("byte x = 13; x ^= 14; return x;"));
+        // short
+        assertEquals((short) (13 ^ 14), exec("short x = 13; x ^= 14; return x;"));
+        // char
+        assertEquals((char) (13 ^ 14), exec("char x = 13; x ^= 14; return x;"));
+        // int
+        assertEquals(13 ^ 14, exec("int x = 13; x ^= 14; return x;"));
+        // long
+        assertEquals((long) (13 ^ 14), exec("long x = 13L; x ^= 14; return x;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ConditionalTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ConditionalTests.java
new file mode 100644
index 0000000..bc46642
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ConditionalTests.java
@@ -0,0 +1,93 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+
+public class ConditionalTests extends ScriptTestCase {
+    public void testBasic() {
+        assertEquals(2, exec("boolean x = true; return x ? 2 : 3;"));
+        assertEquals(3, exec("boolean x = false; return x ? 2 : 3;"));
+        assertEquals(3, exec("boolean x = false, y = true; return x && y ? 2 : 3;"));
+        assertEquals(2, exec("boolean x = true, y = true; return x && y ? 2 : 3;"));
+        assertEquals(2, exec("boolean x = true, y = false; return x || y ? 2 : 3;"));
+        assertEquals(3, exec("boolean x = false, y = false; return x || y ? 2 : 3;"));
+    }
+
+    public void testPrecedence() {
+        assertEquals(4, exec("boolean x = false, y = true; return x ? (y ? 2 : 3) : 4;"));
+        assertEquals(2, exec("boolean x = true, y = true; return x ? (y ? 2 : 3) : 4;"));
+        assertEquals(3, exec("boolean x = true, y = false; return x ? (y ? 2 : 3) : 4;"));
+        assertEquals(2, exec("boolean x = true, y = true; return x ? y ? 2 : 3 : 4;"));
+        assertEquals(4, exec("boolean x = false, y = true; return x ? y ? 2 : 3 : 4;"));
+        assertEquals(3, exec("boolean x = true, y = false; return x ? y ? 2 : 3 : 4;"));
+        assertEquals(3, exec("boolean x = false, y = true; return x ? 2 : y ? 3 : 4;"));
+        assertEquals(2, exec("boolean x = true, y = false; return x ? 2 : y ? 3 : 4;"));
+        assertEquals(4, exec("boolean x = false, y = false; return x ? 2 : y ? 3 : 4;"));
+        assertEquals(4, exec("boolean x = false, y = false; return (x ? true : y) ? 3 : 4;"));
+        assertEquals(4, exec("boolean x = true, y = false; return (x ? false : y) ? 3 : 4;"));
+        assertEquals(3, exec("boolean x = false, y = true; return (x ? false : y) ? 3 : 4;"));
+        assertEquals(2, exec("boolean x = true, y = false; return (x ? false : y) ? (x ? 3 : 4) : x ? 2 : 1;"));
+        assertEquals(2, exec("boolean x = true, y = false; return (x ? false : y) ? x ? 3 : 4 : x ? 2 : 1;"));
+        assertEquals(4, exec("boolean x = false, y = true; return x ? false : y ? x ? 3 : 4 : x ? 2 : 1;"));
+    }
+
+    public void testAssignment() {
+        assertEquals(4D, exec("boolean x = false; double z = x ? 2 : 4.0F; return z;"));
+        assertEquals((byte)7, exec("boolean x = false; int y = 2; byte z = x ? (byte)y : 7; return z;"));
+        assertEquals((byte)7, exec("boolean x = false; int y = 2; byte z = (byte)(x ? y : 7); return z;"));
+        assertEquals(ArrayList.class, exec("boolean x = false; Object z = x ? new HashMap() : new ArrayList(); return z;").getClass());
+    }
+
+    public void testNullArguments() {
+        assertEquals(null, exec("boolean b = false, c = true; Object x; Map y; return b && c ? x : y;"));
+        assertEquals(HashMap.class, exec("boolean b = false, c = true; Object x; Map y = new HashMap(); return b && c ? x : y;").getClass());
+    }
+
+    public void testPromotion() {
+        assertEquals(false, exec("boolean x = false; boolean y = true; return (x ? 2 : 4.0F) == (y ? 2 : 4.0F);"));
+        assertEquals(false, exec("boolean x = false; boolean y = true; return (x ? 2 : 4.0F) == (y ? new Long(2) : new Float(4.0F));"));
+        assertEquals(false, exec("boolean x = false; boolean y = true; return (x ? new HashMap() : new ArrayList()) == (y ? new Long(2) : new Float(4.0F));"));
+        assertEquals(false, exec("boolean x = false; boolean y = true; return (x ? 2 : 4.0F) == (y ? new HashMap() : new ArrayList());"));
+    }
+
+    public void testIncompatibleAssignment() {
+        try {
+            exec("boolean x = false; byte z = x ? 2 : 4.0F; return z;");
+            fail("expected class cast exception");
+        } catch (ClassCastException expected) {}
+
+        try {
+            exec("boolean x = false; Map z = x ? 4 : (byte)7; return z;");
+            fail("expected class cast exception");
+        } catch (ClassCastException expected) {}
+
+        try {
+            exec("boolean x = false; Map z = x ? new HashMap() : new ArrayList(); return z;");
+            fail("expected class cast exception");
+        } catch (ClassCastException expected) {}
+
+        try {
+            exec("boolean x = false; int y = 2; byte z = x ? y : 7; return z;");
+            fail("expected class cast exception");
+        } catch (ClassCastException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DefTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DefTests.java
new file mode 100644
index 0000000..6ff5113
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DefTests.java
@@ -0,0 +1,914 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+public class DefTests extends ScriptTestCase {
+    public void testNot() {
+        assertEquals(~1, exec("def x = (byte)1 return ~x"));
+        assertEquals(~1, exec("def x = (short)1 return ~x"));
+        assertEquals(~1, exec("def x = (char)1 return ~x"));
+        assertEquals(~1, exec("def x = 1 return ~x"));
+        assertEquals(~1L, exec("def x = 1L return ~x"));
+    }
+
+    public void testNeg() {
+        assertEquals(-1, exec("def x = (byte)1 return -x"));
+        assertEquals(-1, exec("def x = (short)1 return -x"));
+        assertEquals(-1, exec("def x = (char)1 return -x"));
+        assertEquals(-1, exec("def x = 1 return -x"));
+        assertEquals(-1L, exec("def x = 1L return -x"));
+        assertEquals(-1.0F, exec("def x = 1F return -x"));
+        assertEquals(-1.0, exec("def x = 1.0 return -x"));
+    }
+
+    public void testMul() {
+        assertEquals(4, exec("def x = (byte)2 def y = (byte)2 return x * y"));
+        assertEquals(4, exec("def x = (short)2 def y = (byte)2 return x * y"));
+        assertEquals(4, exec("def x = (char)2 def y = (byte)2 return x * y"));
+        assertEquals(4, exec("def x = (int)2 def y = (byte)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (byte)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (byte)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (byte)2 return x * y"));
+
+        assertEquals(4, exec("def x = (byte)2 def y = (short)2 return x * y"));
+        assertEquals(4, exec("def x = (short)2 def y = (short)2 return x * y"));
+        assertEquals(4, exec("def x = (char)2 def y = (short)2 return x * y"));
+        assertEquals(4, exec("def x = (int)2 def y = (short)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (short)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (short)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (short)2 return x * y"));
+
+        assertEquals(4, exec("def x = (byte)2 def y = (char)2 return x * y"));
+        assertEquals(4, exec("def x = (short)2 def y = (char)2 return x * y"));
+        assertEquals(4, exec("def x = (char)2 def y = (char)2 return x * y"));
+        assertEquals(4, exec("def x = (int)2 def y = (char)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (char)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (char)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (char)2 return x * y"));
+
+        assertEquals(4, exec("def x = (byte)2 def y = (int)2 return x * y"));
+        assertEquals(4, exec("def x = (short)2 def y = (int)2 return x * y"));
+        assertEquals(4, exec("def x = (char)2 def y = (int)2 return x * y"));
+        assertEquals(4, exec("def x = (int)2 def y = (int)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (int)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (int)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (int)2 return x * y"));
+
+        assertEquals(4L, exec("def x = (byte)2 def y = (long)2 return x * y"));
+        assertEquals(4L, exec("def x = (short)2 def y = (long)2 return x * y"));
+        assertEquals(4L, exec("def x = (char)2 def y = (long)2 return x * y"));
+        assertEquals(4L, exec("def x = (int)2 def y = (long)2 return x * y"));
+        assertEquals(4L, exec("def x = (long)2 def y = (long)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (long)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (long)2 return x * y"));
+
+        assertEquals(4F, exec("def x = (byte)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (short)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (char)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (int)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (long)2 def y = (float)2 return x * y"));
+        assertEquals(4F, exec("def x = (float)2 def y = (float)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (float)2 return x * y"));
+
+        assertEquals(4D, exec("def x = (byte)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (short)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (char)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (int)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (long)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (float)2 def y = (double)2 return x * y"));
+        assertEquals(4D, exec("def x = (double)2 def y = (double)2 return x * y"));
+
+        assertEquals(4, exec("def x = (Byte)2 def y = (byte)2 return x * y"));
+        assertEquals(4, exec("def x = (Short)2 def y = (short)2 return x * y"));
+        assertEquals(4, exec("def x = (Character)2 def y = (char)2 return x * y"));
+        assertEquals(4, exec("def x = (Integer)2 def y = (int)2 return x * y"));
+        assertEquals(4L, exec("def x = (Long)2 def y = (long)2 return x * y"));
+        assertEquals(4F, exec("def x = (Float)2 def y = (float)2 return x * y"));
+        assertEquals(4D, exec("def x = (Double)2 def y = (double)2 return x * y"));
+    }
+
+    public void testDiv() {
+        assertEquals(1, exec("def x = (byte)2 def y = (byte)2 return x / y"));
+        assertEquals(1, exec("def x = (short)2 def y = (byte)2 return x / y"));
+        assertEquals(1, exec("def x = (char)2 def y = (byte)2 return x / y"));
+        assertEquals(1, exec("def x = (int)2 def y = (byte)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (byte)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (byte)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (byte)2 return x / y"));
+
+        assertEquals(1, exec("def x = (byte)2 def y = (short)2 return x / y"));
+        assertEquals(1, exec("def x = (short)2 def y = (short)2 return x / y"));
+        assertEquals(1, exec("def x = (char)2 def y = (short)2 return x / y"));
+        assertEquals(1, exec("def x = (int)2 def y = (short)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (short)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (short)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (short)2 return x / y"));
+
+        assertEquals(1, exec("def x = (byte)2 def y = (char)2 return x / y"));
+        assertEquals(1, exec("def x = (short)2 def y = (char)2 return x / y"));
+        assertEquals(1, exec("def x = (char)2 def y = (char)2 return x / y"));
+        assertEquals(1, exec("def x = (int)2 def y = (char)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (char)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (char)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (char)2 return x / y"));
+
+        assertEquals(1, exec("def x = (byte)2 def y = (int)2 return x / y"));
+        assertEquals(1, exec("def x = (short)2 def y = (int)2 return x / y"));
+        assertEquals(1, exec("def x = (char)2 def y = (int)2 return x / y"));
+        assertEquals(1, exec("def x = (int)2 def y = (int)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (int)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (int)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (int)2 return x / y"));
+
+        assertEquals(1L, exec("def x = (byte)2 def y = (long)2 return x / y"));
+        assertEquals(1L, exec("def x = (short)2 def y = (long)2 return x / y"));
+        assertEquals(1L, exec("def x = (char)2 def y = (long)2 return x / y"));
+        assertEquals(1L, exec("def x = (int)2 def y = (long)2 return x / y"));
+        assertEquals(1L, exec("def x = (long)2 def y = (long)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (long)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (long)2 return x / y"));
+
+        assertEquals(1F, exec("def x = (byte)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (short)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (char)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (int)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (long)2 def y = (float)2 return x / y"));
+        assertEquals(1F, exec("def x = (float)2 def y = (float)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (float)2 return x / y"));
+
+        assertEquals(1D, exec("def x = (byte)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (short)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (char)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (int)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (long)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (float)2 def y = (double)2 return x / y"));
+        assertEquals(1D, exec("def x = (double)2 def y = (double)2 return x / y"));
+
+        assertEquals(1, exec("def x = (Byte)2 def y = (byte)2 return x / y"));
+        assertEquals(1, exec("def x = (Short)2 def y = (short)2 return x / y"));
+        assertEquals(1, exec("def x = (Character)2 def y = (char)2 return x / y"));
+        assertEquals(1, exec("def x = (Integer)2 def y = (int)2 return x / y"));
+        assertEquals(1L, exec("def x = (Long)2 def y = (long)2 return x / y"));
+        assertEquals(1F, exec("def x = (Float)2 def y = (float)2 return x / y"));
+        assertEquals(1D, exec("def x = (Double)2 def y = (double)2 return x / y"));
+    }
+
+    public void testRem() {
+        assertEquals(0, exec("def x = (byte)2 def y = (byte)2 return x % y"));
+        assertEquals(0, exec("def x = (short)2 def y = (byte)2 return x % y"));
+        assertEquals(0, exec("def x = (char)2 def y = (byte)2 return x % y"));
+        assertEquals(0, exec("def x = (int)2 def y = (byte)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (byte)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (byte)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (byte)2 return x % y"));
+
+        assertEquals(0, exec("def x = (byte)2 def y = (short)2 return x % y"));
+        assertEquals(0, exec("def x = (short)2 def y = (short)2 return x % y"));
+        assertEquals(0, exec("def x = (char)2 def y = (short)2 return x % y"));
+        assertEquals(0, exec("def x = (int)2 def y = (short)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (short)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (short)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (short)2 return x % y"));
+
+        assertEquals(0, exec("def x = (byte)2 def y = (char)2 return x % y"));
+        assertEquals(0, exec("def x = (short)2 def y = (char)2 return x % y"));
+        assertEquals(0, exec("def x = (char)2 def y = (char)2 return x % y"));
+        assertEquals(0, exec("def x = (int)2 def y = (char)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (char)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (char)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (char)2 return x % y"));
+
+        assertEquals(0, exec("def x = (byte)2 def y = (int)2 return x % y"));
+        assertEquals(0, exec("def x = (short)2 def y = (int)2 return x % y"));
+        assertEquals(0, exec("def x = (char)2 def y = (int)2 return x % y"));
+        assertEquals(0, exec("def x = (int)2 def y = (int)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (int)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (int)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (int)2 return x % y"));
+
+        assertEquals(0L, exec("def x = (byte)2 def y = (long)2 return x % y"));
+        assertEquals(0L, exec("def x = (short)2 def y = (long)2 return x % y"));
+        assertEquals(0L, exec("def x = (char)2 def y = (long)2 return x % y"));
+        assertEquals(0L, exec("def x = (int)2 def y = (long)2 return x % y"));
+        assertEquals(0L, exec("def x = (long)2 def y = (long)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (long)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (long)2 return x % y"));
+
+        assertEquals(0F, exec("def x = (byte)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (short)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (char)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (int)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (long)2 def y = (float)2 return x % y"));
+        assertEquals(0F, exec("def x = (float)2 def y = (float)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (float)2 return x % y"));
+
+        assertEquals(0D, exec("def x = (byte)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (short)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (char)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (int)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (long)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (float)2 def y = (double)2 return x % y"));
+        assertEquals(0D, exec("def x = (double)2 def y = (double)2 return x % y"));
+
+        assertEquals(0, exec("def x = (Byte)2 def y = (byte)2 return x % y"));
+        assertEquals(0, exec("def x = (Short)2 def y = (short)2 return x % y"));
+        assertEquals(0, exec("def x = (Character)2 def y = (char)2 return x % y"));
+        assertEquals(0, exec("def x = (Integer)2 def y = (int)2 return x % y"));
+        assertEquals(0L, exec("def x = (Long)2 def y = (long)2 return x % y"));
+        assertEquals(0F, exec("def x = (Float)2 def y = (float)2 return x % y"));
+        assertEquals(0D, exec("def x = (Double)2 def y = (double)2 return x % y"));
+    }
+    
+    public void testAdd() {
+        assertEquals(2, exec("def x = (byte)1 def y = (byte)1 return x + y"));
+        assertEquals(2, exec("def x = (short)1 def y = (byte)1 return x + y"));
+        assertEquals(2, exec("def x = (char)1 def y = (byte)1 return x + y"));
+        assertEquals(2, exec("def x = (int)1 def y = (byte)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (byte)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (byte)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (byte)1 return x + y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (short)1 return x + y"));
+        assertEquals(2, exec("def x = (short)1 def y = (short)1 return x + y"));
+        assertEquals(2, exec("def x = (char)1 def y = (short)1 return x + y"));
+        assertEquals(2, exec("def x = (int)1 def y = (short)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (short)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (short)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (short)1 return x + y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (char)1 return x + y"));
+        assertEquals(2, exec("def x = (short)1 def y = (char)1 return x + y"));
+        assertEquals(2, exec("def x = (char)1 def y = (char)1 return x + y"));
+        assertEquals(2, exec("def x = (int)1 def y = (char)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (char)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (char)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (char)1 return x + y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (int)1 return x + y"));
+        assertEquals(2, exec("def x = (short)1 def y = (int)1 return x + y"));
+        assertEquals(2, exec("def x = (char)1 def y = (int)1 return x + y"));
+        assertEquals(2, exec("def x = (int)1 def y = (int)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (int)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (int)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (int)1 return x + y"));
+
+        assertEquals(2L, exec("def x = (byte)1 def y = (long)1 return x + y"));
+        assertEquals(2L, exec("def x = (short)1 def y = (long)1 return x + y"));
+        assertEquals(2L, exec("def x = (char)1 def y = (long)1 return x + y"));
+        assertEquals(2L, exec("def x = (int)1 def y = (long)1 return x + y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (long)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (long)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (long)1 return x + y"));
+
+        assertEquals(2F, exec("def x = (byte)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (short)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (char)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (int)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (long)1 def y = (float)1 return x + y"));
+        assertEquals(2F, exec("def x = (float)1 def y = (float)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (float)1 return x + y"));
+
+        assertEquals(2D, exec("def x = (byte)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (short)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (char)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (int)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (long)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (float)1 def y = (double)1 return x + y"));
+        assertEquals(2D, exec("def x = (double)1 def y = (double)1 return x + y"));
+
+        assertEquals(2, exec("def x = (Byte)1 def y = (byte)1 return x + y"));
+        assertEquals(2, exec("def x = (Short)1 def y = (short)1 return x + y"));
+        assertEquals(2, exec("def x = (Character)1 def y = (char)1 return x + y"));
+        assertEquals(2, exec("def x = (Integer)1 def y = (int)1 return x + y"));
+        assertEquals(2L, exec("def x = (Long)1 def y = (long)1 return x + y"));
+        assertEquals(2F, exec("def x = (Float)1 def y = (float)1 return x + y"));
+        assertEquals(2D, exec("def x = (Double)1 def y = (double)1 return x + y"));
+    }
+
+    public void testSub() {
+        assertEquals(0, exec("def x = (byte)1 def y = (byte)1 return x - y"));
+        assertEquals(0, exec("def x = (short)1 def y = (byte)1 return x - y"));
+        assertEquals(0, exec("def x = (char)1 def y = (byte)1 return x - y"));
+        assertEquals(0, exec("def x = (int)1 def y = (byte)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (byte)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (byte)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (byte)1 return x - y"));
+
+        assertEquals(0, exec("def x = (byte)1 def y = (short)1 return x - y"));
+        assertEquals(0, exec("def x = (short)1 def y = (short)1 return x - y"));
+        assertEquals(0, exec("def x = (char)1 def y = (short)1 return x - y"));
+        assertEquals(0, exec("def x = (int)1 def y = (short)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (short)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (short)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (short)1 return x - y"));
+
+        assertEquals(0, exec("def x = (byte)1 def y = (char)1 return x - y"));
+        assertEquals(0, exec("def x = (short)1 def y = (char)1 return x - y"));
+        assertEquals(0, exec("def x = (char)1 def y = (char)1 return x - y"));
+        assertEquals(0, exec("def x = (int)1 def y = (char)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (char)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (char)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (char)1 return x - y"));
+
+        assertEquals(0, exec("def x = (byte)1 def y = (int)1 return x - y"));
+        assertEquals(0, exec("def x = (short)1 def y = (int)1 return x - y"));
+        assertEquals(0, exec("def x = (char)1 def y = (int)1 return x - y"));
+        assertEquals(0, exec("def x = (int)1 def y = (int)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (int)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (int)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (int)1 return x - y"));
+
+        assertEquals(0L, exec("def x = (byte)1 def y = (long)1 return x - y"));
+        assertEquals(0L, exec("def x = (short)1 def y = (long)1 return x - y"));
+        assertEquals(0L, exec("def x = (char)1 def y = (long)1 return x - y"));
+        assertEquals(0L, exec("def x = (int)1 def y = (long)1 return x - y"));
+        assertEquals(0L, exec("def x = (long)1 def y = (long)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (long)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (long)1 return x - y"));
+
+        assertEquals(0F, exec("def x = (byte)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (short)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (char)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (int)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (long)1 def y = (float)1 return x - y"));
+        assertEquals(0F, exec("def x = (float)1 def y = (float)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (float)1 return x - y"));
+
+        assertEquals(0D, exec("def x = (byte)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (short)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (char)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (int)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (long)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (float)1 def y = (double)1 return x - y"));
+        assertEquals(0D, exec("def x = (double)1 def y = (double)1 return x - y"));
+
+        assertEquals(0, exec("def x = (Byte)1 def y = (byte)1 return x - y"));
+        assertEquals(0, exec("def x = (Short)1 def y = (short)1 return x - y"));
+        assertEquals(0, exec("def x = (Character)1 def y = (char)1 return x - y"));
+        assertEquals(0, exec("def x = (Integer)1 def y = (int)1 return x - y"));
+        assertEquals(0L, exec("def x = (Long)1 def y = (long)1 return x - y"));
+        assertEquals(0F, exec("def x = (Float)1 def y = (float)1 return x - y"));
+        assertEquals(0D, exec("def x = (Double)1 def y = (double)1 return x - y"));
+    }
+
+    public void testLsh() {
+        assertEquals(2, exec("def x = (byte)1 def y = (byte)1 return x << y"));
+        assertEquals(2, exec("def x = (short)1 def y = (byte)1 return x << y"));
+        assertEquals(2, exec("def x = (char)1 def y = (byte)1 return x << y"));
+        assertEquals(2, exec("def x = (int)1 def y = (byte)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (byte)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (byte)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (byte)1 return x << y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (short)1 return x << y"));
+        assertEquals(2, exec("def x = (short)1 def y = (short)1 return x << y"));
+        assertEquals(2, exec("def x = (char)1 def y = (short)1 return x << y"));
+        assertEquals(2, exec("def x = (int)1 def y = (short)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (short)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (short)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (short)1 return x << y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (char)1 return x << y"));
+        assertEquals(2, exec("def x = (short)1 def y = (char)1 return x << y"));
+        assertEquals(2, exec("def x = (char)1 def y = (char)1 return x << y"));
+        assertEquals(2, exec("def x = (int)1 def y = (char)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (char)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (char)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (char)1 return x << y"));
+
+        assertEquals(2, exec("def x = (byte)1 def y = (int)1 return x << y"));
+        assertEquals(2, exec("def x = (short)1 def y = (int)1 return x << y"));
+        assertEquals(2, exec("def x = (char)1 def y = (int)1 return x << y"));
+        assertEquals(2, exec("def x = (int)1 def y = (int)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (int)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (int)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (int)1 return x << y"));
+
+        assertEquals(2L, exec("def x = (byte)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (short)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (char)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (int)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (long)1 return x << y"));
+
+        assertEquals(2L, exec("def x = (byte)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (short)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (char)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (int)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (float)1 return x << y"));
+
+        assertEquals(2L, exec("def x = (byte)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (short)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (char)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (int)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (long)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (float)1 def y = (double)1 return x << y"));
+        assertEquals(2L, exec("def x = (double)1 def y = (double)1 return x << y"));
+
+        assertEquals(2, exec("def x = (Byte)1 def y = (byte)1 return x << y"));
+        assertEquals(2, exec("def x = (Short)1 def y = (short)1 return x << y"));
+        assertEquals(2, exec("def x = (Character)1 def y = (char)1 return x << y"));
+        assertEquals(2, exec("def x = (Integer)1 def y = (int)1 return x << y"));
+        assertEquals(2L, exec("def x = (Long)1 def y = (long)1 return x << y"));
+        assertEquals(2L, exec("def x = (Float)1 def y = (float)1 return x << y"));
+        assertEquals(2L, exec("def x = (Double)1 def y = (double)1 return x << y"));
+    }
+
+    public void testRsh() {
+        assertEquals(2, exec("def x = (byte)4 def y = (byte)1 return x >> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (byte)1 return x >> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (byte)1 return x >> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (byte)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (byte)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (byte)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (byte)1 return x >> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (short)1 return x >> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (short)1 return x >> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (short)1 return x >> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (short)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (short)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (short)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (short)1 return x >> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (char)1 return x >> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (char)1 return x >> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (char)1 return x >> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (char)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (char)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (char)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (char)1 return x >> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (int)1 return x >> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (int)1 return x >> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (int)1 return x >> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (int)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (int)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (int)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (int)1 return x >> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (long)1 return x >> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (float)1 return x >> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (double)1 return x >> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (double)1 return x >> y"));
+
+        assertEquals(2, exec("def x = (Byte)4 def y = (byte)1 return x >> y"));
+        assertEquals(2, exec("def x = (Short)4 def y = (short)1 return x >> y"));
+        assertEquals(2, exec("def x = (Character)4 def y = (char)1 return x >> y"));
+        assertEquals(2, exec("def x = (Integer)4 def y = (int)1 return x >> y"));
+        assertEquals(2L, exec("def x = (Long)4 def y = (long)1 return x >> y"));
+        assertEquals(2L, exec("def x = (Float)4 def y = (float)1 return x >> y"));
+        assertEquals(2L, exec("def x = (Double)4 def y = (double)1 return x >> y"));
+    }
+
+    public void testUsh() {
+        assertEquals(2, exec("def x = (byte)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (byte)1 return x >>> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (short)1 return x >>> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (short)1 return x >>> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (short)1 return x >>> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (short)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (short)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (short)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (short)1 return x >>> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (char)1 return x >>> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (char)1 return x >>> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (char)1 return x >>> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (char)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (char)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (char)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (char)1 return x >>> y"));
+
+        assertEquals(2, exec("def x = (byte)4 def y = (int)1 return x >>> y"));
+        assertEquals(2, exec("def x = (short)4 def y = (int)1 return x >>> y"));
+        assertEquals(2, exec("def x = (char)4 def y = (int)1 return x >>> y"));
+        assertEquals(2, exec("def x = (int)4 def y = (int)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (int)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (int)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (int)1 return x >>> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (long)1 return x >>> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (float)1 return x >>> y"));
+
+        assertEquals(2L, exec("def x = (byte)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (short)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (char)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (int)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (long)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (float)4 def y = (double)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (double)4 def y = (double)1 return x >>> y"));
+
+        assertEquals(2, exec("def x = (Byte)4 def y = (byte)1 return x >>> y"));
+        assertEquals(2, exec("def x = (Short)4 def y = (short)1 return x >>> y"));
+        assertEquals(2, exec("def x = (Character)4 def y = (char)1 return x >>> y"));
+        assertEquals(2, exec("def x = (Integer)4 def y = (int)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (Long)4 def y = (long)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (Float)4 def y = (float)1 return x >>> y"));
+        assertEquals(2L, exec("def x = (Double)4 def y = (double)1 return x >>> y"));
+    }
+
+    public void testAnd() {
+        assertEquals(0, exec("def x = (byte)4 def y = (byte)1 return x & y"));
+        assertEquals(0, exec("def x = (short)4 def y = (byte)1 return x & y"));
+        assertEquals(0, exec("def x = (char)4 def y = (byte)1 return x & y"));
+        assertEquals(0, exec("def x = (int)4 def y = (byte)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (byte)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (byte)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (byte)1 return x & y"));
+
+        assertEquals(0, exec("def x = (byte)4 def y = (short)1 return x & y"));
+        assertEquals(0, exec("def x = (short)4 def y = (short)1 return x & y"));
+        assertEquals(0, exec("def x = (char)4 def y = (short)1 return x & y"));
+        assertEquals(0, exec("def x = (int)4 def y = (short)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (short)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (short)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (short)1 return x & y"));
+
+        assertEquals(0, exec("def x = (byte)4 def y = (char)1 return x & y"));
+        assertEquals(0, exec("def x = (short)4 def y = (char)1 return x & y"));
+        assertEquals(0, exec("def x = (char)4 def y = (char)1 return x & y"));
+        assertEquals(0, exec("def x = (int)4 def y = (char)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (char)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (char)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (char)1 return x & y"));
+
+        assertEquals(0, exec("def x = (byte)4 def y = (int)1 return x & y"));
+        assertEquals(0, exec("def x = (short)4 def y = (int)1 return x & y"));
+        assertEquals(0, exec("def x = (char)4 def y = (int)1 return x & y"));
+        assertEquals(0, exec("def x = (int)4 def y = (int)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (int)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (int)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (int)1 return x & y"));
+
+        assertEquals(0L, exec("def x = (byte)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (short)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (char)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (int)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (long)1 return x & y"));
+
+        assertEquals(0L, exec("def x = (byte)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (short)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (char)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (int)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (float)1 return x & y"));
+
+        assertEquals(0L, exec("def x = (byte)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (short)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (char)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (int)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (long)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (float)4 def y = (double)1 return x & y"));
+        assertEquals(0L, exec("def x = (double)4 def y = (double)1 return x & y"));
+
+        assertEquals(0, exec("def x = (Byte)4 def y = (byte)1 return x & y"));
+        assertEquals(0, exec("def x = (Short)4 def y = (short)1 return x & y"));
+        assertEquals(0, exec("def x = (Character)4 def y = (char)1 return x & y"));
+        assertEquals(0, exec("def x = (Integer)4 def y = (int)1 return x & y"));
+        assertEquals(0L, exec("def x = (Long)4 def y = (long)1 return x & y"));
+        assertEquals(0L, exec("def x = (Float)4 def y = (float)1 return x & y"));
+        assertEquals(0L, exec("def x = (Double)4 def y = (double)1 return x & y"));
+    }
+
+    public void testXor() {
+        assertEquals(5, exec("def x = (byte)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5, exec("def x = (short)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5, exec("def x = (char)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5, exec("def x = (int)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (byte)1 return x ^ y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (short)1 return x ^ y"));
+        assertEquals(5, exec("def x = (short)4 def y = (short)1 return x ^ y"));
+        assertEquals(5, exec("def x = (char)4 def y = (short)1 return x ^ y"));
+        assertEquals(5, exec("def x = (int)4 def y = (short)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (short)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (short)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (short)1 return x ^ y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (char)1 return x ^ y"));
+        assertEquals(5, exec("def x = (short)4 def y = (char)1 return x ^ y"));
+        assertEquals(5, exec("def x = (char)4 def y = (char)1 return x ^ y"));
+        assertEquals(5, exec("def x = (int)4 def y = (char)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (char)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (char)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (char)1 return x ^ y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (int)1 return x ^ y"));
+        assertEquals(5, exec("def x = (short)4 def y = (int)1 return x ^ y"));
+        assertEquals(5, exec("def x = (char)4 def y = (int)1 return x ^ y"));
+        assertEquals(5, exec("def x = (int)4 def y = (int)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (int)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (int)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (int)1 return x ^ y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (long)1 return x ^ y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (float)1 return x ^ y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (double)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (double)1 return x ^ y"));
+
+        assertEquals(5, exec("def x = (Byte)4 def y = (byte)1 return x ^ y"));
+        assertEquals(5, exec("def x = (Short)4 def y = (short)1 return x ^ y"));
+        assertEquals(5, exec("def x = (Character)4 def y = (char)1 return x ^ y"));
+        assertEquals(5, exec("def x = (Integer)4 def y = (int)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (Long)4 def y = (long)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (Float)4 def y = (float)1 return x ^ y"));
+        assertEquals(5L, exec("def x = (Double)4 def y = (double)1 return x ^ y"));
+    }
+
+    public void testOr() {
+        assertEquals(5, exec("def x = (byte)4 def y = (byte)1 return x | y"));
+        assertEquals(5, exec("def x = (short)4 def y = (byte)1 return x | y"));
+        assertEquals(5, exec("def x = (char)4 def y = (byte)1 return x | y"));
+        assertEquals(5, exec("def x = (int)4 def y = (byte)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (byte)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (byte)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (byte)1 return x | y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (short)1 return x | y"));
+        assertEquals(5, exec("def x = (short)4 def y = (short)1 return x | y"));
+        assertEquals(5, exec("def x = (char)4 def y = (short)1 return x | y"));
+        assertEquals(5, exec("def x = (int)4 def y = (short)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (short)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (short)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (short)1 return x | y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (char)1 return x | y"));
+        assertEquals(5, exec("def x = (short)4 def y = (char)1 return x | y"));
+        assertEquals(5, exec("def x = (char)4 def y = (char)1 return x | y"));
+        assertEquals(5, exec("def x = (int)4 def y = (char)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (char)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (char)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (char)1 return x | y"));
+
+        assertEquals(5, exec("def x = (byte)4 def y = (int)1 return x | y"));
+        assertEquals(5, exec("def x = (short)4 def y = (int)1 return x | y"));
+        assertEquals(5, exec("def x = (char)4 def y = (int)1 return x | y"));
+        assertEquals(5, exec("def x = (int)4 def y = (int)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (int)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (int)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (int)1 return x | y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (long)1 return x | y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (float)1 return x | y"));
+
+        assertEquals(5L, exec("def x = (byte)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (short)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (char)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (int)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (long)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (float)4 def y = (double)1 return x | y"));
+        assertEquals(5L, exec("def x = (double)4 def y = (double)1 return x | y"));
+
+        assertEquals(5, exec("def x = (Byte)4 def y = (byte)1 return x | y"));
+        assertEquals(5, exec("def x = (Short)4 def y = (short)1 return x | y"));
+        assertEquals(5, exec("def x = (Character)4 def y = (char)1 return x | y"));
+        assertEquals(5, exec("def x = (Integer)4 def y = (int)1 return x | y"));
+        assertEquals(5L, exec("def x = (Long)4 def y = (long)1 return x | y"));
+        assertEquals(5L, exec("def x = (Float)4 def y = (float)1 return x | y"));
+        assertEquals(5L, exec("def x = (Double)4 def y = (double)1 return x | y"));
+    }
+
+    public void testEq() {
+        assertEquals(true, exec("def x = (byte)7 def y = (int)7 return x == y"));
+        assertEquals(true, exec("def x = (short)6 def y = (int)6 return x == y"));
+        assertEquals(true, exec("def x = (char)5 def y = (int)5 return x == y"));
+        assertEquals(true, exec("def x = (int)4 def y = (int)4 return x == y"));
+        assertEquals(false, exec("def x = (long)5 def y = (int)3 return x == y"));
+        assertEquals(false, exec("def x = (float)6 def y = (int)2 return x == y"));
+        assertEquals(false, exec("def x = (double)7 def y = (int)1 return x == y"));
+
+        assertEquals(true, exec("def x = (byte)7 def y = (double)7 return x == y"));
+        assertEquals(true, exec("def x = (short)6 def y = (double)6 return x == y"));
+        assertEquals(true, exec("def x = (char)5 def y = (double)5 return x == y"));
+        assertEquals(true, exec("def x = (int)4 def y = (double)4 return x == y"));
+        assertEquals(false, exec("def x = (long)5 def y = (double)3 return x == y"));
+        assertEquals(false, exec("def x = (float)6 def y = (double)2 return x == y"));
+        assertEquals(false, exec("def x = (double)7 def y = (double)1 return x == y"));
+
+        assertEquals(true, exec("def x = new HashMap() def y = new HashMap() return x == y"));
+        assertEquals(false, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() return x == y"));
+        assertEquals(true, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() y.put(3, 3) return x == y"));
+        assertEquals(true, exec("def x = new HashMap() def y = x x.put(3, 3) y.put(3, 3) return x == y"));
+    }
+
+    public void testEqr() {
+        assertEquals(false, exec("def x = (byte)7 def y = (int)7 return x === y"));
+        assertEquals(false, exec("def x = (short)6 def y = (int)6 return x === y"));
+        assertEquals(false, exec("def x = (char)5 def y = (int)5 return x === y"));
+        assertEquals(true, exec("def x = (int)4 def y = (int)4 return x === y"));
+        assertEquals(false, exec("def x = (long)5 def y = (int)3 return x === y"));
+        assertEquals(false, exec("def x = (float)6 def y = (int)2 return x === y"));
+        assertEquals(false, exec("def x = (double)7 def y = (int)1 return x === y"));
+
+        assertEquals(false, exec("def x = new HashMap() def y = new HashMap() return x === y"));
+        assertEquals(false, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() return x === y"));
+        assertEquals(false, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() y.put(3, 3) return x === y"));
+        assertEquals(true, exec("def x = new HashMap() def y = x x.put(3, 3) y.put(3, 3) return x === y"));
+    }
+
+    public void testNe() {
+        assertEquals(false, exec("def x = (byte)7 def y = (int)7 return x != y"));
+        assertEquals(false, exec("def x = (short)6 def y = (int)6 return x != y"));
+        assertEquals(false, exec("def x = (char)5 def y = (int)5 return x != y"));
+        assertEquals(false, exec("def x = (int)4 def y = (int)4 return x != y"));
+        assertEquals(true, exec("def x = (long)5 def y = (int)3 return x != y"));
+        assertEquals(true, exec("def x = (float)6 def y = (int)2 return x != y"));
+        assertEquals(true, exec("def x = (double)7 def y = (int)1 return x != y"));
+
+        assertEquals(false, exec("def x = (byte)7 def y = (double)7 return x != y"));
+        assertEquals(false, exec("def x = (short)6 def y = (double)6 return x != y"));
+        assertEquals(false, exec("def x = (char)5 def y = (double)5 return x != y"));
+        assertEquals(false, exec("def x = (int)4 def y = (double)4 return x != y"));
+        assertEquals(true, exec("def x = (long)5 def y = (double)3 return x != y"));
+        assertEquals(true, exec("def x = (float)6 def y = (double)2 return x != y"));
+        assertEquals(true, exec("def x = (double)7 def y = (double)1 return x != y"));
+
+        assertEquals(false, exec("def x = new HashMap() def y = new HashMap() return x != y"));
+        assertEquals(true, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() return x != y"));
+        assertEquals(false, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() y.put(3, 3) return x != y"));
+        assertEquals(false, exec("def x = new HashMap() def y = x x.put(3, 3) y.put(3, 3) return x != y"));
+    }
+
+    public void testNer() {
+        assertEquals(true, exec("def x = (byte)7 def y = (int)7 return x !== y"));
+        assertEquals(true, exec("def x = (short)6 def y = (int)6 return x !== y"));
+        assertEquals(true, exec("def x = (char)5 def y = (int)5 return x !== y"));
+        assertEquals(false, exec("def x = (int)4 def y = (int)4 return x !== y"));
+        assertEquals(true, exec("def x = (long)5 def y = (int)3 return x !== y"));
+        assertEquals(true, exec("def x = (float)6 def y = (int)2 return x !== y"));
+        assertEquals(true, exec("def x = (double)7 def y = (int)1 return x !== y"));
+
+        assertEquals(true, exec("def x = new HashMap() def y = new HashMap() return x !== y"));
+        assertEquals(true, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() return x !== y"));
+        assertEquals(true, exec("def x = new HashMap() x.put(3, 3) def y = new HashMap() y.put(3, 3) return x !== y"));
+        assertEquals(false, exec("def x = new HashMap() def y = x x.put(3, 3) y.put(3, 3) return x !== y"));
+    }
+
+    public void testLt() {
+        assertEquals(true, exec("def x = (byte)1 def y = (int)7 return x < y"));
+        assertEquals(true, exec("def x = (short)2 def y = (int)6 return x < y"));
+        assertEquals(true, exec("def x = (char)3 def y = (int)5 return x < y"));
+        assertEquals(false, exec("def x = (int)4 def y = (int)4 return x < y"));
+        assertEquals(false, exec("def x = (long)5 def y = (int)3 return x < y"));
+        assertEquals(false, exec("def x = (float)6 def y = (int)2 return x < y"));
+        assertEquals(false, exec("def x = (double)7 def y = (int)1 return x < y"));
+
+        assertEquals(true, exec("def x = (byte)1 def y = (double)7 return x < y"));
+        assertEquals(true, exec("def x = (short)2 def y = (double)6 return x < y"));
+        assertEquals(true, exec("def x = (char)3 def y = (double)5 return x < y"));
+        assertEquals(false, exec("def x = (int)4 def y = (double)4 return x < y"));
+        assertEquals(false, exec("def x = (long)5 def y = (double)3 return x < y"));
+        assertEquals(false, exec("def x = (float)6 def y = (double)2 return x < y"));
+        assertEquals(false, exec("def x = (double)7 def y = (double)1 return x < y"));
+    }
+
+    public void testLte() {
+        assertEquals(true, exec("def x = (byte)1 def y = (int)7 return x <= y"));
+        assertEquals(true, exec("def x = (short)2 def y = (int)6 return x <= y"));
+        assertEquals(true, exec("def x = (char)3 def y = (int)5 return x <= y"));
+        assertEquals(true, exec("def x = (int)4 def y = (int)4 return x <= y"));
+        assertEquals(false, exec("def x = (long)5 def y = (int)3 return x <= y"));
+        assertEquals(false, exec("def x = (float)6 def y = (int)2 return x <= y"));
+        assertEquals(false, exec("def x = (double)7 def y = (int)1 return x <= y"));
+
+        assertEquals(true, exec("def x = (byte)1 def y = (double)7 return x <= y"));
+        assertEquals(true, exec("def x = (short)2 def y = (double)6 return x <= y"));
+        assertEquals(true, exec("def x = (char)3 def y = (double)5 return x <= y"));
+        assertEquals(true, exec("def x = (int)4 def y = (double)4 return x <= y"));
+        assertEquals(false, exec("def x = (long)5 def y = (double)3 return x <= y"));
+        assertEquals(false, exec("def x = (float)6 def y = (double)2 return x <= y"));
+        assertEquals(false, exec("def x = (double)7 def y = (double)1 return x <= y"));
+    }
+
+    public void testGt() {
+        assertEquals(false, exec("def x = (byte)1 def y = (int)7 return x > y"));
+        assertEquals(false, exec("def x = (short)2 def y = (int)6 return x > y"));
+        assertEquals(false, exec("def x = (char)3 def y = (int)5 return x > y"));
+        assertEquals(false, exec("def x = (int)4 def y = (int)4 return x > y"));
+        assertEquals(true, exec("def x = (long)5 def y = (int)3 return x > y"));
+        assertEquals(true, exec("def x = (float)6 def y = (int)2 return x > y"));
+        assertEquals(true, exec("def x = (double)7 def y = (int)1 return x > y"));
+
+        assertEquals(false, exec("def x = (byte)1 def y = (double)7 return x > y"));
+        assertEquals(false, exec("def x = (short)2 def y = (double)6 return x > y"));
+        assertEquals(false, exec("def x = (char)3 def y = (double)5 return x > y"));
+        assertEquals(false, exec("def x = (int)4 def y = (double)4 return x > y"));
+        assertEquals(true, exec("def x = (long)5 def y = (double)3 return x > y"));
+        assertEquals(true, exec("def x = (float)6 def y = (double)2 return x > y"));
+        assertEquals(true, exec("def x = (double)7 def y = (double)1 return x > y"));
+    }
+
+    public void testGte() {
+        assertEquals(false, exec("def x = (byte)1 def y = (int)7 return x >= y"));
+        assertEquals(false, exec("def x = (short)2 def y = (int)6 return x >= y"));
+        assertEquals(false, exec("def x = (char)3 def y = (int)5 return x >= y"));
+        assertEquals(true, exec("def x = (int)4 def y = (int)4 return x >= y"));
+        assertEquals(true, exec("def x = (long)5 def y = (int)3 return x >= y"));
+        assertEquals(true, exec("def x = (float)6 def y = (int)2 return x >= y"));
+        assertEquals(true, exec("def x = (double)7 def y = (int)1 return x >= y"));
+
+        assertEquals(false, exec("def x = (byte)1 def y = (double)7 return x >= y"));
+        assertEquals(false, exec("def x = (short)2 def y = (double)6 return x >= y"));
+        assertEquals(false, exec("def x = (char)3 def y = (double)5 return x >= y"));
+        assertEquals(true, exec("def x = (int)4 def y = (double)4 return x >= y"));
+        assertEquals(true, exec("def x = (long)5 def y = (double)3 return x >= y"));
+        assertEquals(true, exec("def x = (float)6 def y = (double)2 return x >= y"));
+        assertEquals(true, exec("def x = (double)7 def y = (double)1 return x >= y"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DivisionTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DivisionTests.java
new file mode 100644
index 0000000..24849fa
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/DivisionTests.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for division operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class DivisionTests extends ScriptTestCase {
+    
+    // TODO: byte,short,char
+    
+    public void testInt() throws Exception {
+        assertEquals(1/1, exec("int x = 1; int y = 1; return x/y;"));
+        assertEquals(2/3, exec("int x = 2; int y = 3; return x/y;"));
+        assertEquals(5/10, exec("int x = 5; int y = 10; return x/y;"));
+        assertEquals(10/1/2, exec("int x = 10; int y = 1; int z = 2; return x/y/z;"));
+        assertEquals((10/1)/2, exec("int x = 10; int y = 1; int z = 2; return (x/y)/z;"));
+        assertEquals(10/(4/2), exec("int x = 10; int y = 4; int z = 2; return x/(y/z);"));
+        assertEquals(10/1, exec("int x = 10; int y = 1; return x/y;"));
+        assertEquals(0/1, exec("int x = 0; int y = 1; return x/y;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1/1, exec("return 1/1;"));
+        assertEquals(2/3, exec("return 2/3;"));
+        assertEquals(5/10, exec("return 5/10;"));
+        assertEquals(10/1/2, exec("return 10/1/2;"));
+        assertEquals((10/1)/2, exec("return (10/1)/2;"));
+        assertEquals(10/(4/2), exec("return 10/(4/2);"));
+        assertEquals(10/1, exec("return 10/1;"));
+        assertEquals(0/1, exec("return 0/1;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L/1L, exec("long x = 1; long y = 1; return x/y;"));
+        assertEquals(2L/3L, exec("long x = 2; long y = 3; return x/y;"));
+        assertEquals(5L/10L, exec("long x = 5; long y = 10; return x/y;"));
+        assertEquals(10L/1L/2L, exec("long x = 10; long y = 1; long z = 2; return x/y/z;"));
+        assertEquals((10L/1L)/2L, exec("long x = 10; long y = 1; long z = 2; return (x/y)/z;"));
+        assertEquals(10L/(4L/2L), exec("long x = 10; long y = 4; long z = 2; return x/(y/z);"));
+        assertEquals(10L/1L, exec("long x = 10; long y = 1; return x/y;"));
+        assertEquals(0L/1L, exec("long x = 0; long y = 1; return x/y;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L/1L, exec("return 1L/1L;"));
+        assertEquals(2L/3L, exec("return 2L/3L;"));
+        assertEquals(5L/10L, exec("return 5L/10L;"));
+        assertEquals(10L/1L/2L, exec("return 10L/1L/2L;"));
+        assertEquals((10L/1L)/2L, exec("return (10L/1L)/2L;"));
+        assertEquals(10L/(4L/2L), exec("return 10L/(4L/2L);"));
+        assertEquals(10L/1L, exec("return 10L/1L;"));
+        assertEquals(0L/1L, exec("return 0L/1L;"));
+    }
+    
+    public void testFloat() throws Exception {
+        assertEquals(1F/1F, exec("float x = 1; float y = 1; return x/y;"));
+        assertEquals(2F/3F, exec("float x = 2; float y = 3; return x/y;"));
+        assertEquals(5F/10F, exec("float x = 5; float y = 10; return x/y;"));
+        assertEquals(10F/1F/2F, exec("float x = 10; float y = 1; float z = 2; return x/y/z;"));
+        assertEquals((10F/1F)/2F, exec("float x = 10; float y = 1; float z = 2; return (x/y)/z;"));
+        assertEquals(10F/(4F/2F), exec("float x = 10; float y = 4; float z = 2; return x/(y/z);"));
+        assertEquals(10F/1F, exec("float x = 10; float y = 1; return x/y;"));
+        assertEquals(0F/1F, exec("float x = 0; float y = 1; return x/y;"));
+    }
+    
+    public void testFloatConst() throws Exception {
+        assertEquals(1F/1F, exec("return 1F/1F;"));
+        assertEquals(2F/3F, exec("return 2F/3F;"));
+        assertEquals(5F/10F, exec("return 5F/10F;"));
+        assertEquals(10F/1F/2F, exec("return 10F/1F/2F;"));
+        assertEquals((10F/1F)/2F, exec("return (10F/1F)/2F;"));
+        assertEquals(10F/(4F/2F), exec("return 10F/(4F/2F);"));
+        assertEquals(10F/1F, exec("return 10F/1F;"));
+        assertEquals(0F/1F, exec("return 0F/1F;"));
+    }
+    
+    public void testDouble() throws Exception {
+        assertEquals(1.0/1.0, exec("double x = 1; double y = 1; return x/y;"));
+        assertEquals(2.0/3.0, exec("double x = 2; double y = 3; return x/y;"));
+        assertEquals(5.0/10.0, exec("double x = 5; double y = 10; return x/y;"));
+        assertEquals(10.0/1.0/2.0, exec("double x = 10; double y = 1; double z = 2; return x/y/z;"));
+        assertEquals((10.0/1.0)/2.0, exec("double x = 10; double y = 1; double z = 2; return (x/y)/z;"));
+        assertEquals(10.0/(4.0/2.0), exec("double x = 10; double y = 4; double z = 2; return x/(y/z);"));
+        assertEquals(10.0/1.0, exec("double x = 10; double y = 1; return x/y;"));
+        assertEquals(0.0/1.0, exec("double x = 0; double y = 1; return x/y;"));
+    }
+    
+    public void testDoubleConst() throws Exception {
+        assertEquals(1.0/1.0, exec("return 1.0/1.0;"));
+        assertEquals(2.0/3.0, exec("return 2.0/3.0;"));
+        assertEquals(5.0/10.0, exec("return 5.0/10.0;"));
+        assertEquals(10.0/1.0/2.0, exec("return 10.0/1.0/2.0;"));
+        assertEquals((10.0/1.0)/2.0, exec("return (10.0/1.0)/2.0;"));
+        assertEquals(10.0/(4.0/2.0), exec("return 10.0/(4.0/2.0);"));
+        assertEquals(10.0/1.0, exec("return 10.0/1.0;"));
+        assertEquals(0.0/1.0, exec("return 0.0/1.0;"));
+    }
+
+    public void testDivideByZero() throws Exception {
+        try {
+            exec("int x = 1; int y = 0; return x / y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+        
+        try {
+            exec("long x = 1L; long y = 0L; return x / y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+    }
+    
+    public void testDivideByZeroConst() throws Exception {
+        try {
+            exec("return 1/0;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+        
+        try {
+            exec("return 1L/0L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/EqualsTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/EqualsTests.java
new file mode 100644
index 0000000..db83755
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/EqualsTests.java
@@ -0,0 +1,184 @@
+package org.elasticsearch.plan.a;
+
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+// TODO: Figure out a way to test autobox caching properly from methods such as Integer.valueOf(int);
+public class EqualsTests extends ScriptTestCase {
+    public void testTypesEquals() {
+        assertEquals(true, exec("return false === false;"));
+        assertEquals(true, exec("boolean x = false; boolean y = false; return x === y;"));
+        assertEquals(false, exec("return (byte)3 === (byte)4;"));
+        assertEquals(true, exec("byte x = 3; byte y = 3; return x === y;"));
+        assertEquals(false, exec("return (char)3 === (char)4;"));
+        assertEquals(true, exec("char x = 3; char y = 3; return x === y;"));
+        assertEquals(false, exec("return (short)3 === (short)4;"));
+        assertEquals(true, exec("short x = 3; short y = 3; return x === y;"));
+        assertEquals(false, exec("return (int)3 === (int)4;"));
+        assertEquals(true, exec("int x = 3; int y = 3; return x === y;"));
+        assertEquals(false, exec("return (long)3 === (long)4;"));
+        assertEquals(true, exec("long x = 3; long y = 3; return x === y;"));
+        assertEquals(false, exec("return (float)3 === (float)4;"));
+        assertEquals(true, exec("float x = 3; float y = 3; return x === y;"));
+        assertEquals(false, exec("return (double)3 === (double)4;"));
+        assertEquals(true, exec("double x = 3; double y = 3; return x === y;"));
+
+        assertEquals(true, exec("return false == false;"));
+        assertEquals(true, exec("boolean x = false; boolean y = false; return x == y;"));
+        assertEquals(false, exec("return (byte)3 == (byte)4;"));
+        assertEquals(true, exec("byte x = 3; byte y = 3; return x == y;"));
+        assertEquals(false, exec("return (char)3 == (char)4;"));
+        assertEquals(true, exec("char x = 3; char y = 3; return x == y;"));
+        assertEquals(false, exec("return (short)3 == (short)4;"));
+        assertEquals(true, exec("short x = 3; short y = 3; return x == y;"));
+        assertEquals(false, exec("return (int)3 == (int)4;"));
+        assertEquals(true, exec("int x = 3; int y = 3; return x == y;"));
+        assertEquals(false, exec("return (long)3 == (long)4;"));
+        assertEquals(true, exec("long x = 3; long y = 3; return x == y;"));
+        assertEquals(false, exec("return (float)3 == (float)4;"));
+        assertEquals(true, exec("float x = 3; float y = 3; return x == y;"));
+        assertEquals(false, exec("return (double)3 == (double)4;"));
+        assertEquals(true, exec("double x = 3; double y = 3; return x == y;"));
+    }
+
+    public void testTypesNotEquals() {
+        assertEquals(false, exec("return true !== true;"));
+        assertEquals(false, exec("boolean x = false; boolean y = false; return x !== y;"));
+        assertEquals(true, exec("return (byte)3 !== (byte)4;"));
+        assertEquals(false, exec("byte x = 3; byte y = 3; return x !== y;"));
+        assertEquals(true, exec("return (char)3 !== (char)4;"));
+        assertEquals(false, exec("char x = 3; char y = 3; return x !== y;"));
+        assertEquals(true, exec("return (short)3 !== (short)4;"));
+        assertEquals(false, exec("short x = 3; short y = 3; return x !== y;"));
+        assertEquals(true, exec("return (int)3 !== (int)4;"));
+        assertEquals(false, exec("int x = 3; int y = 3; return x !== y;"));
+        assertEquals(true, exec("return (long)3 !== (long)4;"));
+        assertEquals(false, exec("long x = 3; long y = 3; return x !== y;"));
+        assertEquals(true, exec("return (float)3 !== (float)4;"));
+        assertEquals(false, exec("float x = 3; float y = 3; return x !== y;"));
+        assertEquals(true, exec("return (double)3 !== (double)4;"));
+        assertEquals(false, exec("double x = 3; double y = 3; return x !== y;"));
+
+        assertEquals(false, exec("return true != true;"));
+        assertEquals(false, exec("boolean x = false; boolean y = false; return x != y;"));
+        assertEquals(true, exec("return (byte)3 != (byte)4;"));
+        assertEquals(false, exec("byte x = 3; byte y = 3; return x != y;"));
+        assertEquals(true, exec("return (char)3 != (char)4;"));
+        assertEquals(false, exec("char x = 3; char y = 3; return x != y;"));
+        assertEquals(true, exec("return (short)3 != (short)4;"));
+        assertEquals(false, exec("short x = 3; short y = 3; return x != y;"));
+        assertEquals(true, exec("return (int)3 != (int)4;"));
+        assertEquals(false, exec("int x = 3; int y = 3; return x != y;"));
+        assertEquals(true, exec("return (long)3 != (long)4;"));
+        assertEquals(false, exec("long x = 3; long y = 3; return x != y;"));
+        assertEquals(true, exec("return (float)3 != (float)4;"));
+        assertEquals(false, exec("float x = 3; float y = 3; return x != y;"));
+        assertEquals(true, exec("return (double)3 != (double)4;"));
+        assertEquals(false, exec("double x = 3; double y = 3; return x != y;"));
+    }
+
+    public void testEquals() {
+        assertEquals(true, exec("return new Long(3) == new Long(3);"));
+        assertEquals(false, exec("return new Long(3) === new Long(3);"));
+        assertEquals(true, exec("Integer x = new Integer(3); Object y = x; return x == y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); Object y = x; return x === y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); Object y = new Integer(3); return x == y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); Object y = new Integer(3); return x === y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); int y = 3; return x == y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); short y = 3; return x == y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); Short y = (short)3; return x == y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); int y = 3; return x === y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); double y = 3; return x === y;"));
+        assertEquals(true, exec("int[] x = new int[1]; Object y = x; return x == y;"));
+        assertEquals(true, exec("int[] x = new int[1]; Object y = x; return x === y;"));
+        assertEquals(false, exec("int[] x = new int[1]; Object y = new int[1]; return x == y;"));
+        assertEquals(false, exec("int[] x = new int[1]; Object y = new int[1]; return x === y;"));
+        assertEquals(false, exec("Map x = new HashMap(); List y = new ArrayList(); return x == y;"));
+        assertEquals(false, exec("Map x = new HashMap(); List y = new ArrayList(); return x === y;"));
+    }
+
+    public void testNotEquals() {
+        assertEquals(false, exec("return new Long(3) != new Long(3);"));
+        assertEquals(true, exec("return new Long(3) !== new Long(3);"));
+        assertEquals(false, exec("Integer x = new Integer(3); Object y = x; return x != y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); Object y = x; return x !== y;"));
+        assertEquals(false, exec("Integer x = new Integer(3); Object y = new Integer(3); return x != y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); Object y = new Integer(3); return x !== y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); int y = 3; return x !== y;"));
+        assertEquals(true, exec("Integer x = new Integer(3); double y = 3; return x !== y;"));
+        assertEquals(false, exec("int[] x = new int[1]; Object y = x; return x != y;"));
+        assertEquals(false, exec("int[] x = new int[1]; Object y = x; return x !== y;"));
+        assertEquals(true, exec("int[] x = new int[1]; Object y = new int[1]; return x != y;"));
+        assertEquals(true, exec("int[] x = new int[1]; Object y = new int[1]; return x !== y;"));
+        assertEquals(true, exec("Map x = new HashMap(); List y = new ArrayList(); return x != y;"));
+        assertEquals(true, exec("Map x = new HashMap(); List y = new ArrayList(); return x !== y;"));
+    }
+
+    public void testBranchEquals() {
+        assertEquals(0, exec("Character a = 'a'; Character b = 'b'; if (a == b) return 1; else return 0;"));
+        assertEquals(1, exec("Character a = 'a'; Character b = 'a'; if (a == b) return 1; else return 0;"));
+        assertEquals(0, exec("Integer a = new Integer(1); Integer b = 1; if (a === b) return 1; else return 0;"));
+        assertEquals(0, exec("Character a = 'a'; Character b = new Character('a'); if (a === b) return 1; else return 0;"));
+        assertEquals(1, exec("Character a = 'a'; Object b = a; if (a === b) return 1; else return 0;"));
+        assertEquals(1, exec("Integer a = 1; Number b = a; Number c = a; if (c === b) return 1; else return 0;"));
+        assertEquals(0, exec("Integer a = 1; Character b = 'a'; if (a === (Object)b) return 1; else return 0;"));
+    }
+
+    public void testBranchNotEquals() {
+        assertEquals(1, exec("Character a = 'a'; Character b = 'b'; if (a != b) return 1; else return 0;"));
+        assertEquals(0, exec("Character a = 'a'; Character b = 'a'; if (a != b) return 1; else return 0;"));
+        assertEquals(1, exec("Integer a = new Integer(1); Integer b = 1; if (a !== b) return 1; else return 0;"));
+        assertEquals(1, exec("Character a = 'a'; Character b = new Character('a'); if (a !== b) return 1; else return 0;"));
+        assertEquals(0, exec("Character a = 'a'; Object b = a; if (a !== b) return 1; else return 0;"));
+        assertEquals(0, exec("Integer a = 1; Number b = a; Number c = a; if (c !== b) return 1; else return 0;"));
+        assertEquals(1, exec("Integer a = 1; Character b = 'a'; if (a !== (Object)b) return 1; else return 0;"));
+    }
+
+    public void testRightHandNull() {
+        assertEquals(false, exec("Character a = 'a'; return a == null;"));
+        assertEquals(false, exec("Character a = 'a'; return a === null;"));
+        assertEquals(true, exec("Character a = 'a'; return a != null;"));
+        assertEquals(true, exec("Character a = 'a'; return a !== null;"));
+        assertEquals(true, exec("Character a = null; return a == null;"));
+        assertEquals(false, exec("Character a = null; return a != null;"));
+        assertEquals(false, exec("Character a = 'a'; Character b = null; return a == b;"));
+        assertEquals(true, exec("Character a = null; Character b = null; return a === b;"));
+        assertEquals(true, exec("Character a = 'a'; Character b = null; return a != b;"));
+        assertEquals(false, exec("Character a = null; Character b = null; return a !== b;"));
+        assertEquals(false, exec("Integer x = null; double y = 2.0; return x == y;"));
+        assertEquals(true, exec("Integer x = null; Short y = null; return x == y;"));
+    }
+
+    public void testLeftHandNull() {
+        assertEquals(false, exec("Character a = 'a'; return null == a;"));
+        assertEquals(false, exec("Character a = 'a'; return null === a;"));
+        assertEquals(true, exec("Character a = 'a'; return null != a;"));
+        assertEquals(true, exec("Character a = 'a'; return null !== a;"));
+        assertEquals(true, exec("Character a = null; return null == a;"));
+        assertEquals(false, exec("Character a = null; return null != a;"));
+        assertEquals(false, exec("Character a = null; Character b = 'a'; return a == b;"));
+        assertEquals(true, exec("Character a = null; Character b = null; return a == b;"));
+        assertEquals(true, exec("Character a = null; Character b = null; return b === a;"));
+        assertEquals(true, exec("Character a = null; Character b = 'a'; return a != b;"));
+        assertEquals(false, exec("Character a = null; Character b = null; return b != a;"));
+        assertEquals(false, exec("Character a = null; Character b = null; return b !== a;"));
+        assertEquals(false, exec("Integer x = null; double y = 2.0; return y == x;"));
+        assertEquals(true, exec("Integer x = null; Short y = null; return y == x;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FieldTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FieldTests.java
new file mode 100644
index 0000000..7504ed9
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FieldTests.java
@@ -0,0 +1,108 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.junit.Before;
+
+public class FieldTests extends ScriptTestCase {
+    public static class FieldClass {
+        public boolean z = false;
+        public byte b = 0;
+        public short s = 1;
+        public char c = 'c';
+        public int i = 2;
+        public int si = -1;
+        public long j = 3l;
+        public float f = 4.0f;
+        public double d = 5.0;
+        public String t = "s";
+        public Object l = new Object();
+
+        public float test(float a, float b) {
+            return Math.min(a, b);
+        }
+
+        public int getSi() {
+            return si;
+        }
+
+        public void setSi(final int si) {
+            this.si = si;
+        }
+    }
+
+    public static class FieldDefinition extends Definition {
+        FieldDefinition() {
+            super();
+
+            addStruct("FieldClass", FieldClass.class);
+            addConstructor("FieldClass", "new", new Type[] {}, null);
+            addField("FieldClass", "z", null, false, booleanType, null);
+            addField("FieldClass", "b", null, false, byteType, null);
+            addField("FieldClass", "s", null, false, shortType, null);
+            addField("FieldClass", "c", null, false, charType, null);
+            addField("FieldClass", "i", null, false, intType, null);
+            addField("FieldClass", "j", null, false, longType, null);
+            addField("FieldClass", "f", null, false, floatType, null);
+            addField("FieldClass", "d", null, false, doubleType, null);
+            addField("FieldClass", "t", null, false, stringType, null);
+            addField("FieldClass", "l", null, false, objectType, null);
+            addClass("FieldClass");
+            addMethod("FieldClass", "getSi", null, false, intType, new Type[] {}, null, null);
+            addMethod("FieldClass", "setSi", null, false, voidType, new Type[] {intType}, null, null);
+            addMethod("FieldClass", "test", null, false, floatType, new Type[] {floatType, floatType}, null, null);
+        }
+    }
+
+    @Before
+    public void setDefinition() {
+        scriptEngine.setDefinition(new FieldDefinition());
+    }
+
+    public void testIntField() {
+        assertEquals("s5t42", exec("def fc = new FieldClass() return fc.t += 2 + fc.j + \"t\" + 4 + (3 - 1)"));
+        assertEquals(2.0f, exec("def fc = new FieldClass(); def l = new Double(3) Byte b = new Byte((byte)2) return fc.test(l, b)"));
+        assertEquals(4, exec("def fc = new FieldClass() fc.i = 4 return fc.i"));
+        assertEquals(5, exec("FieldClass fc0 = new FieldClass() FieldClass fc1 = new FieldClass() fc0.i = 7 - fc0.i fc1.i = fc0.i return fc1.i"));
+        assertEquals(8, exec("def fc0 = new FieldClass() def fc1 = new FieldClass() fc0.i += fc1.i fc0.i += fc0.i return fc0.i"));
+    }
+
+    public void testExplicitShortcut() {
+        assertEquals(5, exec("FieldClass fc = new FieldClass() fc.setSi(5) return fc.si"));
+        assertEquals(-1, exec("FieldClass fc = new FieldClass() def x = fc.getSi() x"));
+        assertEquals(5, exec("FieldClass fc = new FieldClass() fc.si = 5 return fc.si"));
+        assertEquals(0, exec("FieldClass fc = new FieldClass() fc.si++ return fc.si"));
+        assertEquals(-1, exec("FieldClass fc = new FieldClass() def x = fc.si++ return x"));
+        assertEquals(0, exec("FieldClass fc = new FieldClass() def x = ++fc.si return x"));
+        assertEquals(-2, exec("FieldClass fc = new FieldClass() fc.si *= 2 fc.si"));
+        assertEquals("-1test", exec("FieldClass fc = new FieldClass() fc.si + \"test\""));
+    }
+
+    public void testImplicitShortcut() {
+        assertEquals(5, exec("def fc = new FieldClass() fc.setSi(5) return fc.si"));
+        assertEquals(-1, exec("def fc = new FieldClass() def x = fc.getSi() x"));
+        assertEquals(5, exec("def fc = new FieldClass() fc.si = 5 return fc.si"));
+        assertEquals(0, exec("def fc = new FieldClass() fc.si++ return fc.si"));
+        assertEquals(-1, exec("def fc = new FieldClass() def x = fc.si++ return x"));
+        assertEquals(0, exec("def fc = new FieldClass() def x = ++fc.si return x"));
+        assertEquals(-2, exec("def fc = new FieldClass() fc.si *= 2 fc.si"));
+        assertEquals("-1test", exec("def fc = new FieldClass() fc.si + \"test\""));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowDisabledTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowDisabledTests.java
new file mode 100644
index 0000000..94beac0
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowDisabledTests.java
@@ -0,0 +1,294 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+
+/** Tests floating point overflow with numeric overflow disabled */
+public class FloatOverflowDisabledTests extends ScriptTestCase {
+    
+    @Override
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(super.getSettings());
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, false);
+        return builder.build();
+    }
+
+    public void testAssignmentAdditionOverflow() {        
+        // float
+        try {
+            exec("float x = 3.4028234663852886E38f; x += 3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = -3.4028234663852886E38f; x += -3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double
+        try {
+            exec("double x = 1.7976931348623157E308; x += 1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = -1.7976931348623157E308; x += -1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentSubtractionOverflow() {    
+        // float
+        try {
+            exec("float x = 3.4028234663852886E38f; x -= -3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = -3.4028234663852886E38f; x -= 3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double
+        try {
+            exec("double x = 1.7976931348623157E308; x -= -1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = -1.7976931348623157E308; x -= 1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentMultiplicationOverflow() {
+        // float
+        try {
+            exec("float x = 3.4028234663852886E38f; x *= 3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 3.4028234663852886E38f; x *= -3.4028234663852886E38f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double
+        try {
+            exec("double x = 1.7976931348623157E308; x *= 1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; x *= -1.7976931348623157E308; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentDivisionOverflow() {
+        // float
+        try {
+            exec("float x = 3.4028234663852886E38f; x /= 1.401298464324817E-45f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 3.4028234663852886E38f; x /= -1.401298464324817E-45f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 1.0f; x /= 0.0f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double
+        try {
+            exec("double x = 1.7976931348623157E308; x /= 4.9E-324; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; x /= -4.9E-324; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.0f; x /= 0.0; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+
+    public void testAddition() throws Exception {
+        try {
+            exec("float x = 3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x + y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; double y = 1.7976931348623157E308; return x + y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAdditionConst() throws Exception {
+        try {
+            exec("return 3.4028234663852886E38f + 3.4028234663852886E38f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.7976931348623157E308 + 1.7976931348623157E308;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtraction() throws Exception {
+        try {
+            exec("float x = -3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x - y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = -1.7976931348623157E308; double y = 1.7976931348623157E308; return x - y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtractionConst() throws Exception {
+        try {
+            exec("return -3.4028234663852886E38f - 3.4028234663852886E38f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return -1.7976931348623157E308 - 1.7976931348623157E308;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplication() throws Exception {
+        try {
+            exec("float x = 3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x * y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; double y = 1.7976931348623157E308; return x * y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplicationConst() throws Exception {
+        try {
+            exec("return 3.4028234663852886E38f * 3.4028234663852886E38f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.7976931348623157E308 * 1.7976931348623157E308;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+
+    public void testDivision() throws Exception {
+        try {
+            exec("float x = 3.4028234663852886E38f; float y = 1.401298464324817E-45f; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 1.0f; float y = 0.0f; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.7976931348623157E308; double y = 4.9E-324; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.0; double y = 0.0; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivisionConst() throws Exception {
+        try {
+            exec("return 3.4028234663852886E38f / 1.401298464324817E-45f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.0f / 0.0f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.7976931348623157E308 / 4.9E-324;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.0 / 0.0;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivisionNaN() throws Exception {
+        // float division, constant division, and assignment
+        try {
+            exec("float x = 0f; float y = 0f; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 0f / 0f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 0f; x /= 0f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double division, constant division, and assignment
+        try {
+            exec("double x = 0.0; double y = 0.0; return x / y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 0.0 / 0.0;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 0.0; x /= 0.0; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testRemainderNaN() throws Exception {
+        // float division, constant division, and assignment
+        try {
+            exec("float x = 1f; float y = 0f; return x % y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1f % 0f;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("float x = 1f; x %= 0f; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // double division, constant division, and assignment
+        try {
+            exec("double x = 1.0; double y = 0.0; return x % y;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("return 1.0 % 0.0;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+        try {
+            exec("double x = 1.0; x %= 0.0; return x;");
+            fail("didn't hit expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowEnabledTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowEnabledTests.java
new file mode 100644
index 0000000..ff1c315
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowEnabledTests.java
@@ -0,0 +1,144 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+
+/** Tests floating point overflow with numeric overflow enabled */
+public class FloatOverflowEnabledTests extends ScriptTestCase {
+    
+    @Override
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(super.getSettings());
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, true);
+        return builder.build();
+    }
+
+    public void testAssignmentAdditionOverflow() {        
+        // float
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x += 3.4028234663852886E38f; return x;"));
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = -3.4028234663852886E38f; x += -3.4028234663852886E38f; return x;"));
+        
+        // double
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; x += 1.7976931348623157E308; return x;"));
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = -1.7976931348623157E308; x += -1.7976931348623157E308; return x;"));
+    }
+    
+    public void testAssignmentSubtractionOverflow() {    
+        // float
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x -= -3.4028234663852886E38f; return x;"));
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = -3.4028234663852886E38f; x -= 3.4028234663852886E38f; return x;"));
+        
+        // double
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; x -= -1.7976931348623157E308; return x;"));
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = -1.7976931348623157E308; x -= 1.7976931348623157E308; return x;"));
+    }
+    
+    public void testAssignmentMultiplicationOverflow() {
+        // float
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x *= 3.4028234663852886E38f; return x;"));
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x *= -3.4028234663852886E38f; return x;"));
+        
+        // double
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; x *= 1.7976931348623157E308; return x;"));
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = 1.7976931348623157E308; x *= -1.7976931348623157E308; return x;"));
+    }
+    
+    public void testAssignmentDivisionOverflow() {
+        // float
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x /= 1.401298464324817E-45f; return x;"));
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = 3.4028234663852886E38f; x /= -1.401298464324817E-45f; return x;"));
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 1.0f; x /= 0.0f; return x;"));
+        
+        // double
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; x /= 4.9E-324; return x;"));
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = 1.7976931348623157E308; x /= -4.9E-324; return x;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.0f; x /= 0.0; return x;"));
+    }
+
+    public void testAddition() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x + y;"));        
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; double y = 1.7976931348623157E308; return x + y;"));
+    }
+    
+    public void testAdditionConst() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("return 3.4028234663852886E38f + 3.4028234663852886E38f;"));        
+        assertEquals(Double.POSITIVE_INFINITY, exec("return 1.7976931348623157E308 + 1.7976931348623157E308;"));
+    }
+    
+    public void testSubtraction() throws Exception {
+        assertEquals(Float.NEGATIVE_INFINITY, exec("float x = -3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x - y;"));        
+        assertEquals(Double.NEGATIVE_INFINITY, exec("double x = -1.7976931348623157E308; double y = 1.7976931348623157E308; return x - y;"));
+    }
+    
+    public void testSubtractionConst() throws Exception {
+        assertEquals(Float.NEGATIVE_INFINITY, exec("return -3.4028234663852886E38f - 3.4028234663852886E38f;"));        
+        assertEquals(Double.NEGATIVE_INFINITY, exec("return -1.7976931348623157E308 - 1.7976931348623157E308;"));
+    }
+    
+    public void testMultiplication() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; float y = 3.4028234663852886E38f; return x * y;"));        
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; double y = 1.7976931348623157E308; return x * y;"));
+    }
+    
+    public void testMultiplicationConst() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("return 3.4028234663852886E38f * 3.4028234663852886E38f;"));        
+        assertEquals(Double.POSITIVE_INFINITY, exec("return 1.7976931348623157E308 * 1.7976931348623157E308;"));
+    }
+
+    public void testDivision() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 3.4028234663852886E38f; float y = 1.401298464324817E-45f; return x / y;"));
+        assertEquals(Float.POSITIVE_INFINITY, exec("float x = 1.0f; float y = 0.0f; return x / y;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.7976931348623157E308; double y = 4.9E-324; return x / y;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("double x = 1.0; double y = 0.0; return x / y;"));
+    }
+    
+    public void testDivisionConst() throws Exception {
+        assertEquals(Float.POSITIVE_INFINITY, exec("return 3.4028234663852886E38f / 1.401298464324817E-45f;"));
+        assertEquals(Float.POSITIVE_INFINITY, exec("return 1.0f / 0.0f;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("return 1.7976931348623157E308 / 4.9E-324;"));
+        assertEquals(Double.POSITIVE_INFINITY, exec("return 1.0 / 0.0;"));
+    }
+    
+    public void testDivisionNaN() throws Exception {
+        // float division, constant division, and assignment
+        assertTrue(Float.isNaN((Float) exec("float x = 0f; float y = 0f; return x / y;")));
+        assertTrue(Float.isNaN((Float) exec("return 0f / 0f;")));
+        assertTrue(Float.isNaN((Float) exec("float x = 0f; x /= 0f; return x;")));
+        
+        // double division, constant division, and assignment
+        assertTrue(Double.isNaN((Double) exec("double x = 0.0; double y = 0.0; return x / y;")));
+        assertTrue(Double.isNaN((Double) exec("return 0.0 / 0.0;")));
+        assertTrue(Double.isNaN((Double) exec("double x = 0.0; x /= 0.0; return x;")));
+    }
+    
+    public void testRemainderNaN() throws Exception {
+        // float division, constant division, and assignment
+        assertTrue(Float.isNaN((Float) exec("float x = 1f; float y = 0f; return x % y;")));
+        assertTrue(Float.isNaN((Float) exec("return 1f % 0f;")));
+        assertTrue(Float.isNaN((Float) exec("float x = 1f; x %= 0f; return x;")));
+        
+        // double division, constant division, and assignment
+        assertTrue(Double.isNaN((Double) exec("double x = 1.0; double y = 0.0; return x % y;")));
+        assertTrue(Double.isNaN((Double) exec("return 1.0 % 0.0;")));
+        assertTrue(Double.isNaN((Double) exec("double x = 1.0; x %= 0.0; return x;")));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IncrementTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IncrementTests.java
new file mode 100644
index 0000000..ec4ffd0
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IncrementTests.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for increment/decrement operators across all data types */
+public class IncrementTests extends ScriptTestCase {
+
+    /** incrementing byte values */
+    public void testIncrementByte() {
+        assertEquals((byte)0, exec("byte x = (byte)0; return x++;"));
+        assertEquals((byte)0, exec("byte x = (byte)0; return x--;"));
+        assertEquals((byte)1, exec("byte x = (byte)0; return ++x;"));
+        assertEquals((byte)-1, exec("byte x = (byte)0; return --x;"));
+    }
+    
+    /** incrementing char values */
+    public void testIncrementChar() {
+        assertEquals((char)0, exec("char x = (char)0; return x++;"));
+        assertEquals((char)1, exec("char x = (char)1; return x--;"));
+        assertEquals((char)1, exec("char x = (char)0; return ++x;"));
+    }
+    
+    /** incrementing short values */
+    public void testIncrementShort() {
+        assertEquals((short)0, exec("short x = (short)0; return x++;"));
+        assertEquals((short)0, exec("short x = (short)0; return x--;"));
+        assertEquals((short)1, exec("short x = (short)0; return ++x;"));
+        assertEquals((short)-1, exec("short x = (short)0; return --x;"));
+    }
+
+    /** incrementing integer values */
+    public void testIncrementInt() {
+        assertEquals(0, exec("int x = 0; return x++;"));
+        assertEquals(0, exec("int x = 0; return x--;"));
+        assertEquals(1, exec("int x = 0; return ++x;"));
+        assertEquals(-1, exec("int x = 0; return --x;"));
+    }
+    
+    /** incrementing long values */
+    public void testIncrementLong() {
+        assertEquals(0L, exec("long x = 0; return x++;"));
+        assertEquals(0L, exec("long x = 0; return x--;"));
+        assertEquals(1L, exec("long x = 0; return ++x;"));
+        assertEquals(-1L, exec("long x = 0; return --x;"));
+    }
+    
+    /** incrementing float values */
+    public void testIncrementFloat() {
+        assertEquals(0F, exec("float x = 0F; return x++;"));
+        assertEquals(0F, exec("float x = 0F; return x--;"));
+        assertEquals(1F, exec("float x = 0F; return ++x;"));
+        assertEquals(-1F, exec("float x = 0F; return --x;"));
+    }
+    
+    /** incrementing double values */
+    public void testIncrementDouble() {
+        assertEquals(0D, exec("double x = 0.0; return x++;"));
+        assertEquals(0D, exec("double x = 0.0; return x--;"));
+        assertEquals(1D, exec("double x = 0.0; return ++x;"));
+        assertEquals(-1D, exec("double x = 0.0; return --x;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowDisabledTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowDisabledTests.java
new file mode 100644
index 0000000..279ea06
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowDisabledTests.java
@@ -0,0 +1,445 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+
+/** Tests integer overflow with numeric overflow disabled */
+public class IntegerOverflowDisabledTests extends ScriptTestCase {
+    
+    @Override
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(super.getSettings());
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, false);
+        return builder.build();
+    }
+
+    public void testAssignmentAdditionOverflow() {
+        // byte
+        try {
+            exec("byte x = 0; x += 128; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 0; x += -129; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // short
+        try {
+            exec("short x = 0; x += 32768; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 0; x += -32769; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 0; x += 65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("char x = 0; x += -65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 1; x += 2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("int x = -2; x += -2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 1; x += 9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = -2; x += -9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentSubtractionOverflow() {
+        // byte
+        try {
+            exec("byte x = 0; x -= -128; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 0; x -= 129; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // short
+        try {
+            exec("short x = 0; x -= -32768; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 0; x -= 32769; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 0; x -= -65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("char x = 0; x -= 65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 1; x -= -2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("int x = -2; x -= 2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 1; x -= -9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = -2; x -= 9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentMultiplicationOverflow() {
+        // byte
+        try {
+            exec("byte x = 2; x *= 128; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("byte x = 2; x *= -128; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 2; x *= 65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("char x = 2; x *= -65536; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 2; x *= 2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("int x = 2; x *= -2147483647; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 2; x *= 9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = 2; x *= -9223372036854775807L; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAssignmentDivisionOverflow() {
+        // byte
+        try {
+            exec("byte x = (byte) -128; x /= -1; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+
+        // short
+        try {
+            exec("short x = (short) -32768; x /= -1; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // cannot happen for char: unsigned
+        
+        // int
+        try {
+            exec("int x = -2147483647 - 1; x /= -1; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = -9223372036854775807L - 1L; x /=-1L; return x;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testIncrementOverFlow() throws Exception {
+        // byte
+        try {
+            exec("byte x = 127; ++x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("byte x = 127; x++; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("byte x = (byte) -128; --x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("byte x = (byte) -128; x--; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // short
+        try {
+            exec("short x = 32767; ++x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("short x = 32767; x++; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("short x = (short) -32768; --x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("short x = (short) -32768; x--; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        // char
+        try {
+            exec("char x = 65535; ++x; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("char x = 65535; x++; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("char x = (char) 0; --x; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("char x = (char) 0; x--; return x;");
+        } catch (ArithmeticException expected) {}
+        
+        // int
+        try {
+            exec("int x = 2147483647; ++x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("int x = 2147483647; x++; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("int x = (int) -2147483648L; --x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("int x = (int) -2147483648L; x--; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        // long
+        try {
+            exec("long x = 9223372036854775807L; ++x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = 9223372036854775807L; x++; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = -9223372036854775807L - 1L; --x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("long x = -9223372036854775807L - 1L; x--; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAddition() throws Exception {
+        try {
+            exec("int x = 2147483647; int y = 2147483647; return x + y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = 9223372036854775807L; long y = 9223372036854775807L; return x + y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAdditionConst() throws Exception {
+        try {
+            exec("return 2147483647 + 2147483647;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("return 9223372036854775807L + 9223372036854775807L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    
+    public void testSubtraction() throws Exception {
+        try {
+            exec("int x = -10; int y = 2147483647; return x - y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = -10L; long y = 9223372036854775807L; return x - y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtractionConst() throws Exception {
+        try {
+            exec("return -10 - 2147483647;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("return -10L - 9223372036854775807L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplication() throws Exception {
+        try {
+            exec("int x = 2147483647; int y = 2147483647; return x * y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = 9223372036854775807L; long y = 9223372036854775807L; return x * y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplicationConst() throws Exception {
+        try {
+            exec("return 2147483647 * 2147483647;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("return 9223372036854775807L * 9223372036854775807L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+
+    public void testDivision() throws Exception {
+        try {
+            exec("int x = -2147483647 - 1; int y = -1; return x / y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = -9223372036854775808L; long y = -1L; return x / y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivisionConst() throws Exception {
+        try {
+            exec("return (-2147483648) / -1;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+
+        try {
+            exec("return (-9223372036854775808L) / -1L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testNegationOverflow() throws Exception {
+        try {
+            exec("int x = -2147483648; x = -x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = -9223372036854775808L; x = -x; return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testNegationOverflowConst() throws Exception {
+        try {
+            exec("int x = -(-2147483648); return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            exec("long x = -(-9223372036854775808L); return x;");
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowEnabledTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowEnabledTests.java
new file mode 100644
index 0000000..8abd269
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowEnabledTests.java
@@ -0,0 +1,194 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+
+/** Tests integer overflow with numeric overflow enabled */
+public class IntegerOverflowEnabledTests extends ScriptTestCase {
+    
+    @Override
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(super.getSettings());
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, true);
+        return builder.build();
+    }
+
+    public void testAssignmentAdditionOverflow() {
+        // byte
+        assertEquals((byte)(0 + 128), exec("byte x = 0; x += 128; return x;"));
+        assertEquals((byte)(0 + -129), exec("byte x = 0; x += -129; return x;"));
+        
+        // short
+        assertEquals((short)(0 + 32768), exec("short x = 0; x += 32768; return x;"));
+        assertEquals((short)(0 + -32769), exec("short x = 0; x += -32769; return x;"));
+        
+        // char
+        assertEquals((char)(0 + 65536), exec("char x = 0; x += 65536; return x;"));
+        assertEquals((char)(0 + -65536), exec("char x = 0; x += -65536; return x;"));
+        
+        // int
+        assertEquals(1 + 2147483647, exec("int x = 1; x += 2147483647; return x;"));
+        assertEquals(-2 + -2147483647, exec("int x = -2; x += -2147483647; return x;"));
+        
+        // long
+        assertEquals(1L + 9223372036854775807L, exec("long x = 1; x += 9223372036854775807L; return x;"));
+        assertEquals(-2L + -9223372036854775807L, exec("long x = -2; x += -9223372036854775807L; return x;"));
+    }
+    
+    public void testAssignmentSubtractionOverflow() {
+        // byte
+        assertEquals((byte)(0 - -128), exec("byte x = 0; x -= -128; return x;"));
+        assertEquals((byte)(0 - 129), exec("byte x = 0; x -= 129; return x;"));
+        
+        // short
+        assertEquals((short)(0 - -32768), exec("short x = 0; x -= -32768; return x;"));
+        assertEquals((short)(0 - 32769), exec("short x = 0; x -= 32769; return x;"));
+        
+        // char
+        assertEquals((char)(0 - -65536), exec("char x = 0; x -= -65536; return x;"));
+        assertEquals((char)(0 - 65536), exec("char x = 0; x -= 65536; return x;"));
+        
+        // int
+        assertEquals(1 - -2147483647, exec("int x = 1; x -= -2147483647; return x;"));
+        assertEquals(-2 - 2147483647, exec("int x = -2; x -= 2147483647; return x;"));
+        
+        // long
+        assertEquals(1L - -9223372036854775807L, exec("long x = 1; x -= -9223372036854775807L; return x;"));
+        assertEquals(-2L - 9223372036854775807L, exec("long x = -2; x -= 9223372036854775807L; return x;"));
+    }
+    
+    public void testAssignmentMultiplicationOverflow() {
+        // byte
+        assertEquals((byte) (2 * 128), exec("byte x = 2; x *= 128; return x;"));
+        assertEquals((byte) (2 * -128), exec("byte x = 2; x *= -128; return x;"));
+        
+        // char
+        assertEquals((char) (2 * 65536), exec("char x = 2; x *= 65536; return x;"));
+        assertEquals((char) (2 * -65536), exec("char x = 2; x *= -65536; return x;"));
+        
+        // int
+        assertEquals(2 * 2147483647, exec("int x = 2; x *= 2147483647; return x;"));
+        assertEquals(2 * -2147483647, exec("int x = 2; x *= -2147483647; return x;"));
+        
+        // long
+        assertEquals(2L * 9223372036854775807L, exec("long x = 2; x *= 9223372036854775807L; return x;"));
+        assertEquals(2L * -9223372036854775807L, exec("long x = 2; x *= -9223372036854775807L; return x;"));
+    }
+    
+    public void testAssignmentDivisionOverflow() {
+        // byte
+        assertEquals((byte) (-128 / -1), exec("byte x = (byte) -128; x /= -1; return x;"));
+
+        // short
+        assertEquals((short) (-32768 / -1), exec("short x = (short) -32768; x /= -1; return x;"));
+        
+        // cannot happen for char: unsigned
+        
+        // int
+        assertEquals((-2147483647 - 1) / -1, exec("int x = -2147483647 - 1; x /= -1; return x;"));
+        
+        // long
+        assertEquals((-9223372036854775807L - 1L) / -1L, exec("long x = -9223372036854775807L - 1L; x /=-1L; return x;"));
+    }
+    
+    public void testIncrementOverFlow() throws Exception {
+        // byte
+        assertEquals((byte) 128, exec("byte x = 127; ++x; return x;"));
+        assertEquals((byte) 128, exec("byte x = 127; x++; return x;"));
+        assertEquals((byte) -129, exec("byte x = (byte) -128; --x; return x;"));
+        assertEquals((byte) -129, exec("byte x = (byte) -128; x--; return x;"));
+        
+        // short
+        assertEquals((short) 32768, exec("short x = 32767; ++x; return x;"));
+        assertEquals((short) 32768, exec("short x = 32767; x++; return x;"));
+        assertEquals((short) -32769, exec("short x = (short) -32768; --x; return x;"));
+        assertEquals((short) -32769, exec("short x = (short) -32768; x--; return x;"));
+        
+        // char
+        assertEquals((char) 65536, exec("char x = 65535; ++x; return x;"));
+        assertEquals((char) 65536, exec("char x = 65535; x++; return x;"));
+        assertEquals((char) -1, exec("char x = (char) 0; --x; return x;"));
+        assertEquals((char) -1, exec("char x = (char) 0; x--; return x;"));
+        
+        // int
+        assertEquals(2147483647 + 1, exec("int x = 2147483647; ++x; return x;"));        
+        assertEquals(2147483647 + 1, exec("int x = 2147483647; x++; return x;"));
+        assertEquals(-2147483648 - 1, exec("int x = (int) -2147483648L; --x; return x;"));        
+        assertEquals(-2147483648 - 1, exec("int x = (int) -2147483648L; x--; return x;"));
+        
+        // long
+        assertEquals(9223372036854775807L + 1L, exec("long x = 9223372036854775807L; ++x; return x;"));        
+        assertEquals(9223372036854775807L + 1L, exec("long x = 9223372036854775807L; x++; return x;"));
+        assertEquals(-9223372036854775807L - 1L - 1L, exec("long x = -9223372036854775807L - 1L; --x; return x;"));
+        assertEquals(-9223372036854775807L - 1L - 1L, exec("long x = -9223372036854775807L - 1L; x--; return x;"));
+    }
+    
+    public void testAddition() throws Exception {
+        assertEquals(2147483647 + 2147483647, exec("int x = 2147483647; int y = 2147483647; return x + y;"));        
+        assertEquals(9223372036854775807L + 9223372036854775807L, exec("long x = 9223372036854775807L; long y = 9223372036854775807L; return x + y;"));
+    }
+    
+    public void testAdditionConst() throws Exception {
+        assertEquals(2147483647 + 2147483647, exec("return 2147483647 + 2147483647;"));        
+        assertEquals(9223372036854775807L + 9223372036854775807L, exec("return 9223372036854775807L + 9223372036854775807L;"));
+    }
+    
+    public void testSubtraction() throws Exception {
+        assertEquals(-10 - 2147483647, exec("int x = -10; int y = 2147483647; return x - y;"));        
+        assertEquals(-10L - 9223372036854775807L, exec("long x = -10L; long y = 9223372036854775807L; return x - y;"));
+    }
+    
+    public void testSubtractionConst() throws Exception {
+        assertEquals(-10 - 2147483647, exec("return -10 - 2147483647;"));        
+        assertEquals(-10L - 9223372036854775807L, exec("return -10L - 9223372036854775807L;"));
+    }
+    
+    public void testMultiplication() throws Exception {
+        assertEquals(2147483647 * 2147483647, exec("int x = 2147483647; int y = 2147483647; return x * y;"));        
+        assertEquals(9223372036854775807L * 9223372036854775807L, exec("long x = 9223372036854775807L; long y = 9223372036854775807L; return x * y;"));
+    }
+    
+    public void testMultiplicationConst() throws Exception {
+        assertEquals(2147483647 * 2147483647, exec("return 2147483647 * 2147483647;"));
+        assertEquals(9223372036854775807L * 9223372036854775807L, exec("return 9223372036854775807L * 9223372036854775807L;"));
+    }
+
+    public void testDivision() throws Exception {
+        assertEquals((-2147483647 - 1) / -1, exec("int x = -2147483648; int y = -1; return x / y;"));
+        assertEquals((-9223372036854775807L - 1L) / -1L, exec("long x = -9223372036854775808L; long y = -1L; return x / y;"));
+    }
+    
+    public void testDivisionConst() throws Exception {
+        assertEquals((-2147483647 - 1) / -1, exec("return (-2147483648) / -1;"));
+        assertEquals((-9223372036854775807L - 1L) / -1L, exec("return (-9223372036854775808L) / -1L;"));
+    }
+    
+    public void testNegationOverflow() throws Exception {
+        assertEquals(-(-2147483647 - 1), exec("int x = -2147483648; x = -x; return x;"));
+        assertEquals(-(-9223372036854775807L - 1L), exec("long x = -9223372036854775808L; x = -x; return x;"));
+    }
+    
+    public void testNegationOverflowConst() throws Exception {
+        assertEquals(-(-2147483647 - 1), exec("int x = -(-2147483648); return x;"));
+        assertEquals(-(-9223372036854775807L - 1L), exec("long x = -(-9223372036854775808L); return x;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/MultiplicationTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/MultiplicationTests.java
new file mode 100644
index 0000000..c5fde3b
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/MultiplicationTests.java
@@ -0,0 +1,126 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for multiplication operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class MultiplicationTests extends ScriptTestCase {
+    
+    // TODO: short,byte,char
+    
+    public void testInt() throws Exception {
+        assertEquals(1*1, exec("int x = 1; int y = 1; return x*y;"));
+        assertEquals(2*3, exec("int x = 2; int y = 3; return x*y;"));
+        assertEquals(5*10, exec("int x = 5; int y = 10; return x*y;"));
+        assertEquals(1*1*2, exec("int x = 1; int y = 1; int z = 2; return x*y*z;"));
+        assertEquals((1*1)*2, exec("int x = 1; int y = 1; int z = 2; return (x*y)*z;"));
+        assertEquals(1*(1*2), exec("int x = 1; int y = 1; int z = 2; return x*(y*z);"));
+        assertEquals(10*0, exec("int x = 10; int y = 0; return x*y;"));
+        assertEquals(0*0, exec("int x = 0; int y = 0; return x*x;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1*1, exec("return 1*1;"));
+        assertEquals(2*3, exec("return 2*3;"));
+        assertEquals(5*10, exec("return 5*10;"));
+        assertEquals(1*1*2, exec("return 1*1*2;"));
+        assertEquals((1*1)*2, exec("return (1*1)*2;"));
+        assertEquals(1*(1*2), exec("return 1*(1*2);"));
+        assertEquals(10*0, exec("return 10*0;"));
+        assertEquals(0*0, exec("return 0*0;"));
+    }
+    
+    public void testByte() throws Exception {
+        assertEquals((byte)1*(byte)1, exec("byte x = 1; byte y = 1; return x*y;"));
+        assertEquals((byte)2*(byte)3, exec("byte x = 2; byte y = 3; return x*y;"));
+        assertEquals((byte)5*(byte)10, exec("byte x = 5; byte y = 10; return x*y;"));
+        assertEquals((byte)1*(byte)1*(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return x*y*z;"));
+        assertEquals(((byte)1*(byte)1)*(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return (x*y)*z;"));
+        assertEquals((byte)1*((byte)1*(byte)2), exec("byte x = 1; byte y = 1; byte z = 2; return x*(y*z);"));
+        assertEquals((byte)10*(byte)0, exec("byte x = 10; byte y = 0; return x*y;"));
+        assertEquals((byte)0*(byte)0, exec("byte x = 0; byte y = 0; return x*x;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L*1L, exec("long x = 1; long y = 1; return x*y;"));
+        assertEquals(2L*3L, exec("long x = 2; long y = 3; return x*y;"));
+        assertEquals(5L*10L, exec("long x = 5; long y = 10; return x*y;"));
+        assertEquals(1L*1L*2L, exec("long x = 1; long y = 1; int z = 2; return x*y*z;"));
+        assertEquals((1L*1L)*2L, exec("long x = 1; long y = 1; int z = 2; return (x*y)*z;"));
+        assertEquals(1L*(1L*2L), exec("long x = 1; long y = 1; int z = 2; return x*(y*z);"));
+        assertEquals(10L*0L, exec("long x = 10; long y = 0; return x*y;"));
+        assertEquals(0L*0L, exec("long x = 0; long y = 0; return x*x;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L*1L, exec("return 1L*1L;"));
+        assertEquals(2L*3L, exec("return 2L*3L;"));
+        assertEquals(5L*10L, exec("return 5L*10L;"));
+        assertEquals(1L*1L*2L, exec("return 1L*1L*2L;"));
+        assertEquals((1L*1L)*2L, exec("return (1L*1L)*2L;"));
+        assertEquals(1L*(1L*2L), exec("return 1L*(1L*2L);"));
+        assertEquals(10L*0L, exec("return 10L*0L;"));
+        assertEquals(0L*0L, exec("return 0L*0L;"));
+    }
+    
+    public void testFloat() throws Exception {
+        assertEquals(1F*1F, exec("float x = 1; float y = 1; return x*y;"));
+        assertEquals(2F*3F, exec("float x = 2; float y = 3; return x*y;"));
+        assertEquals(5F*10F, exec("float x = 5; float y = 10; return x*y;"));
+        assertEquals(1F*1F*2F, exec("float x = 1; float y = 1; float z = 2; return x*y*z;"));
+        assertEquals((1F*1F)*2F, exec("float x = 1; float y = 1; float z = 2; return (x*y)*z;"));
+        assertEquals(1F*(1F*2F), exec("float x = 1; float y = 1; float z = 2; return x*(y*z);"));
+        assertEquals(10F*0F, exec("float x = 10; float y = 0; return x*y;"));
+        assertEquals(0F*0F, exec("float x = 0; float y = 0; return x*x;"));
+    }
+    
+    public void testFloatConst() throws Exception {
+        assertEquals(1F*1F, exec("return 1F*1F;"));
+        assertEquals(2F*3F, exec("return 2F*3F;"));
+        assertEquals(5F*10F, exec("return 5F*10F;"));
+        assertEquals(1F*1F*2F, exec("return 1F*1F*2F;"));
+        assertEquals((1F*1F)*2F, exec("return (1F*1F)*2F;"));
+        assertEquals(1F*(1F*2F), exec("return 1F*(1F*2F);"));
+        assertEquals(10F*0F, exec("return 10F*0F;"));
+        assertEquals(0F*0F, exec("return 0F*0F;"));
+    }
+    
+    public void testDouble() throws Exception {
+        assertEquals(1D*1D, exec("double x = 1; double y = 1; return x*y;"));
+        assertEquals(2D*3D, exec("double x = 2; double y = 3; return x*y;"));
+        assertEquals(5D*10D, exec("double x = 5; double y = 10; return x*y;"));
+        assertEquals(1D*1D*2D, exec("double x = 1; double y = 1; double z = 2; return x*y*z;"));
+        assertEquals((1D*1D)*2D, exec("double x = 1; double y = 1; double z = 2; return (x*y)*z;"));
+        assertEquals(1D*(1D*2D), exec("double x = 1; double y = 1; double z = 2; return x*(y*z);"));
+        assertEquals(10D*0D, exec("double x = 10; float y = 0; return x*y;"));
+        assertEquals(0D*0D, exec("double x = 0; float y = 0; return x*x;"));
+    }
+    
+    public void testDoubleConst() throws Exception {
+        assertEquals(1.0*1.0, exec("return 1.0*1.0;"));
+        assertEquals(2.0*3.0, exec("return 2.0*3.0;"));
+        assertEquals(5.0*10.0, exec("return 5.0*10.0;"));
+        assertEquals(1.0*1.0*2.0, exec("return 1.0*1.0*2.0;"));
+        assertEquals((1.0*1.0)*2.0, exec("return (1.0*1.0)*2.0;"));
+        assertEquals(1.0*(1.0*2.0), exec("return 1.0*(1.0*2.0);"));
+        assertEquals(10.0*0.0, exec("return 10.0*0.0;"));
+        assertEquals(0.0*0.0, exec("return 0.0*0.0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/NoSemiColonTest.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/NoSemiColonTest.java
new file mode 100644
index 0000000..ff56ee3
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/NoSemiColonTest.java
@@ -0,0 +1,178 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import java.util.HashMap;
+import java.util.Map;
+
+public class NoSemiColonTest extends ScriptTestCase {
+
+    public void testIfStatement() {
+        assertEquals(1, exec("int x = 5 if (x == 5) return 1 return 0"));
+        assertEquals(0, exec("int x = 4 if (x == 5) return 1 else return 0"));
+        assertEquals(2, exec("int x = 4 if (x == 5) return 1 else if (x == 4) return 2 else return 0"));
+        assertEquals(1, exec("int x = 4 if (x == 5) return 1 else if (x == 4) return 1 else return 0"));
+
+        assertEquals(3, exec(
+                "int x = 5\n" +
+                        "if (x == 5) {\n" +
+                        "    int y = 2\n" +
+                        "    \n" +
+                        "    if (y == 2) {\n" +
+                        "        x = 3\n" +
+                        "    }\n" +
+                        "    \n" +
+                        "}\n" +
+                        "\n" +
+                        "return x\n"));
+    }
+
+    public void testWhileStatement() {
+
+        assertEquals("aaaaaa", exec("String c = \"a\" int x while (x < 5) { c ..= \"a\" ++x } return c"));
+
+        Object value = exec(
+                " byte[][] b = new byte[5][5]       \n" +
+                " byte x = 0, y                     \n" +
+                "                                   \n" +
+                " while (x < 5) {                   \n" +
+                "     y = 0                         \n" +
+                "                                   \n" +
+                "     while (y < 5) {               \n" +
+                "         b[x][y] = (byte)(x*y)     \n" +
+                "         ++y                       \n" +
+                "     }                             \n" +
+                "                                   \n" +
+                "     ++x                           \n" +
+                " }                                 \n" +
+                "                                   \n" +
+                " return b                          \n");
+
+        byte[][] b = (byte[][])value;
+
+        for (byte x = 0; x < 5; ++x) {
+            for (byte y = 0; y < 5; ++y) {
+                assertEquals(x*y, b[x][y]);
+            }
+        }
+    }
+
+    public void testDoWhileStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\" int x do { c ..= \"a\" ++x } while (x < 5) return c"));
+
+        Object value = exec(
+                " long[][] l = new long[5][5]     \n" +
+                " long x = 0, y                   \n" +
+                "                                 \n" +
+                " do {                            \n" +
+                "     y = 0                       \n" +
+                "                                 \n" +
+                "     do {                        \n" +
+                "         l[(int)x][(int)y] = x*y \n" +
+                "         ++y                     \n" +
+                "     } while (y < 5)             \n" +
+                "                                 \n" +
+                "     ++x                         \n" +
+                " } while (x < 5)                 \n" +
+                "                                 \n" +
+                " return l                        \n");
+
+        long[][] l = (long[][])value;
+
+        for (long x = 0; x < 5; ++x) {
+            for (long y = 0; y < 5; ++y) {
+                assertEquals(x*y, l[(int)x][(int)y]);
+            }
+        }
+    }
+
+    public void testForStatement() {
+        assertEquals("aaaaaa", exec("String c = \"a\" for (int x = 0; x < 5; ++x) c ..= \"a\" return c"));
+
+        Object value = exec(
+                " int[][] i = new int[5][5]         \n" +
+                " for (int x = 0; x < 5; ++x) {     \n" +
+                "     for (int y = 0; y < 5; ++y) { \n" +
+                "         i[x][y] = x*y             \n" +
+                "     }                             \n" +
+                " }                                 \n" +
+                "                                   \n" +
+                " return i                          \n");
+
+        int[][] i = (int[][])value;
+
+        for (int x = 0; x < 5; ++x) {
+            for (int y = 0; y < 5; ++y) {
+                assertEquals(x*y, i[x][y]);
+            }
+        }
+    }
+
+    public void testDeclarationStatement() {
+        assertEquals((byte)2, exec("byte a = 2 return a"));
+        assertEquals((short)2, exec("short a = 2 return a"));
+        assertEquals((char)2, exec("char a = 2 return a"));
+        assertEquals(2, exec("int a = 2 return a"));
+        assertEquals(2L, exec("long a = 2 return a"));
+        assertEquals(2F, exec("float a = 2 return a"));
+        assertEquals(2.0, exec("double a = 2 return a"));
+        assertEquals(false, exec("boolean a = false return a"));
+        assertEquals("string", exec("String a = \"string\" return a"));
+        assertEquals(HashMap.class, exec("Map<String, Object> a = new HashMap<String, Object>() return a").getClass());
+
+        assertEquals(byte[].class, exec("byte[] a = new byte[1] return a").getClass());
+        assertEquals(short[].class, exec("short[] a = new short[1] return a").getClass());
+        assertEquals(char[].class, exec("char[] a = new char[1] return a").getClass());
+        assertEquals(int[].class, exec("int[] a = new int[1] return a").getClass());
+        assertEquals(long[].class, exec("long[] a = new long[1] return a").getClass());
+        assertEquals(float[].class, exec("float[] a = new float[1] return a").getClass());
+        assertEquals(double[].class, exec("double[] a = new double[1] return a").getClass());
+        assertEquals(boolean[].class, exec("boolean[] a = new boolean[1] return a").getClass());
+        assertEquals(String[].class, exec("String[] a = new String[1] return a").getClass());
+        assertEquals(Map[].class, exec("Map<String,Object>[] a = new Map<String,Object>[1] return a").getClass());
+
+        assertEquals(byte[][].class, exec("byte[][] a = new byte[1][2] return a").getClass());
+        assertEquals(short[][][].class, exec("short[][][] a = new short[1][2][3] return a").getClass());
+        assertEquals(char[][][][].class, exec("char[][][][] a = new char[1][2][3][4] return a").getClass());
+        assertEquals(int[][][][][].class, exec("int[][][][][] a = new int[1][2][3][4][5] return a").getClass());
+        assertEquals(long[][].class, exec("long[][] a = new long[1][2] return a").getClass());
+        assertEquals(float[][][].class, exec("float[][][] a = new float[1][2][3] return a").getClass());
+        assertEquals(double[][][][].class, exec("double[][][][] a = new double[1][2][3][4] return a").getClass());
+        assertEquals(boolean[][][][][].class, exec("boolean[][][][][] a = new boolean[1][2][3][4][5] return a").getClass());
+        assertEquals(String[][].class, exec("String[][] a = new String[1][2] return a").getClass());
+        assertEquals(Map[][][].class, exec("Map<String,Object>[][][] a = new Map<String,Object>[1][2][3] return a").getClass());
+    }
+
+    public void testContinueStatement() {
+        assertEquals(9, exec("int x = 0, y = 0 while (x < 10) { ++x if (x == 1) continue ++y } return y"));
+    }
+
+    public void testBreakStatement() {
+        assertEquals(4, exec("int x = 0, y = 0 while (x < 10) { ++x if (x == 5) break ++y } return y"));
+    }
+
+    public void testReturnStatement() {
+        assertEquals(10, exec("return 10"));
+        assertEquals(5, exec("int x = 5 return x"));
+        assertEquals(4, exec("int[] x = new int[2] x[1] = 4 return x[1]"));
+        assertEquals(5, ((short[])exec("short[] s = new short[3] s[1] = 5 return s"))[1]);
+        assertEquals(10, ((Map)exec("Map<String,Object> s = new HashMap< String,Object>() s.put(\"x\", 10) return s")).get("x"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/OrTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/OrTests.java
new file mode 100644
index 0000000..f3ba0c8
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/OrTests.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for or operator across all types */
+public class OrTests extends ScriptTestCase {
+    
+    public void testInt() throws Exception {
+        assertEquals(5 | 12, exec("int x = 5; int y = 12; return x | y;"));
+        assertEquals(5 | -12, exec("int x = 5; int y = -12; return x | y;"));
+        assertEquals(7 | 15 | 3, exec("int x = 7; int y = 15; int z = 3; return x | y | z;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(5 | 12, exec("return 5 | 12;"));
+        assertEquals(5 | -12, exec("return 5 | -12;"));
+        assertEquals(7 | 15 | 3, exec("return 7 | 15 | 3;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(5L | 12L, exec("long x = 5; long y = 12; return x | y;"));
+        assertEquals(5L | -12L, exec("long x = 5; long y = -12; return x | y;"));
+        assertEquals(7L | 15L | 3L, exec("long x = 7; long y = 15; long z = 3; return x | y | z;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(5L | 12L, exec("return 5L | 12L;"));
+        assertEquals(5L | -12L, exec("return 5L | -12L;"));
+        assertEquals(7L | 15L | 3L, exec("return 7L | 15L | 3L;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/PlanARestIT.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/PlanARestIT.java
new file mode 100644
index 0000000..c2c19cc
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/PlanARestIT.java
@@ -0,0 +1,49 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import com.carrotsearch.randomizedtesting.annotations.Name;
+import com.carrotsearch.randomizedtesting.annotations.ParametersFactory;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.test.rest.ESRestTestCase;
+import org.elasticsearch.test.rest.RestTestCandidate;
+import org.elasticsearch.test.rest.parser.RestTestParseException;
+
+import java.io.IOException;
+import java.util.Collection;
+
+/** Runs yaml rest tests */
+public class PlanARestIT extends ESRestTestCase {
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return pluginList(PlanAPlugin.class);
+    }
+
+    public PlanARestIT(@Name("yaml") RestTestCandidate testCandidate) {
+        super(testCandidate);
+    }
+
+    @ParametersFactory
+    public static Iterable<Object[]> parameters() throws IOException, RestTestParseException {
+        return ESRestTestCase.createParameters(0, 1);
+    }
+}
+
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/RemainderTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/RemainderTests.java
new file mode 100644
index 0000000..c7b6f7b
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/RemainderTests.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for division operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class RemainderTests extends ScriptTestCase {
+    
+    // TODO: byte,short,char
+    
+    public void testInt() throws Exception {
+        assertEquals(1%1, exec("int x = 1; int y = 1; return x%y;"));
+        assertEquals(2%3, exec("int x = 2; int y = 3; return x%y;"));
+        assertEquals(5%10, exec("int x = 5; int y = 10; return x%y;"));
+        assertEquals(10%1%2, exec("int x = 10; int y = 1; int z = 2; return x%y%z;"));
+        assertEquals((10%1)%2, exec("int x = 10; int y = 1; int z = 2; return (x%y)%z;"));
+        assertEquals(10%(4%3), exec("int x = 10; int y = 4; int z = 3; return x%(y%z);"));
+        assertEquals(10%1, exec("int x = 10; int y = 1; return x%y;"));
+        assertEquals(0%1, exec("int x = 0; int y = 1; return x%y;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1%1, exec("return 1%1;"));
+        assertEquals(2%3, exec("return 2%3;"));
+        assertEquals(5%10, exec("return 5%10;"));
+        assertEquals(10%1%2, exec("return 10%1%2;"));
+        assertEquals((10%1)%2, exec("return (10%1)%2;"));
+        assertEquals(10%(4%3), exec("return 10%(4%3);"));
+        assertEquals(10%1, exec("return 10%1;"));
+        assertEquals(0%1, exec("return 0%1;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L%1L, exec("long x = 1; long y = 1; return x%y;"));
+        assertEquals(2L%3L, exec("long x = 2; long y = 3; return x%y;"));
+        assertEquals(5L%10L, exec("long x = 5; long y = 10; return x%y;"));
+        assertEquals(10L%1L%2L, exec("long x = 10; long y = 1; long z = 2; return x%y%z;"));
+        assertEquals((10L%1L)%2L, exec("long x = 10; long y = 1; long z = 2; return (x%y)%z;"));
+        assertEquals(10L%(4L%3L), exec("long x = 10; long y = 4; long z = 3; return x%(y%z);"));
+        assertEquals(10L%1L, exec("long x = 10; long y = 1; return x%y;"));
+        assertEquals(0L%1L, exec("long x = 0; long y = 1; return x%y;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L%1L, exec("return 1L%1L;"));
+        assertEquals(2L%3L, exec("return 2L%3L;"));
+        assertEquals(5L%10L, exec("return 5L%10L;"));
+        assertEquals(10L%1L%2L, exec("return 10L%1L%2L;"));
+        assertEquals((10L%1L)%2L, exec("return (10L%1L)%2L;"));
+        assertEquals(10L%(4L%3L), exec("return 10L%(4L%3L);"));
+        assertEquals(10L%1L, exec("return 10L%1L;"));
+        assertEquals(0L%1L, exec("return 0L%1L;"));
+    }
+    
+    public void testFloat() throws Exception {
+        assertEquals(1F%1F, exec("float x = 1; float y = 1; return x%y;"));
+        assertEquals(2F%3F, exec("float x = 2; float y = 3; return x%y;"));
+        assertEquals(5F%10F, exec("float x = 5; float y = 10; return x%y;"));
+        assertEquals(10F%1F%2F, exec("float x = 10; float y = 1; float z = 2; return x%y%z;"));
+        assertEquals((10F%1F)%2F, exec("float x = 10; float y = 1; float z = 2; return (x%y)%z;"));
+        assertEquals(10F%(4F%3F), exec("float x = 10; float y = 4; float z = 3; return x%(y%z);"));
+        assertEquals(10F%1F, exec("float x = 10; float y = 1; return x%y;"));
+        assertEquals(0F%1F, exec("float x = 0; float y = 1; return x%y;"));
+    }
+    
+    public void testFloatConst() throws Exception {
+        assertEquals(1F%1F, exec("return 1F%1F;"));
+        assertEquals(2F%3F, exec("return 2F%3F;"));
+        assertEquals(5F%10F, exec("return 5F%10F;"));
+        assertEquals(10F%1F%2F, exec("return 10F%1F%2F;"));
+        assertEquals((10F%1F)%2F, exec("return (10F%1F)%2F;"));
+        assertEquals(10F%(4F%3F), exec("return 10F%(4F%3F);"));
+        assertEquals(10F%1F, exec("return 10F%1F;"));
+        assertEquals(0F%1F, exec("return 0F%1F;"));
+    }
+    
+    public void testDouble() throws Exception {
+        assertEquals(1.0%1.0, exec("double x = 1; double y = 1; return x%y;"));
+        assertEquals(2.0%3.0, exec("double x = 2; double y = 3; return x%y;"));
+        assertEquals(5.0%10.0, exec("double x = 5; double y = 10; return x%y;"));
+        assertEquals(10.0%1.0%2.0, exec("double x = 10; double y = 1; double z = 2; return x%y%z;"));
+        assertEquals((10.0%1.0)%2.0, exec("double x = 10; double y = 1; double z = 2; return (x%y)%z;"));
+        assertEquals(10.0%(4.0%3.0), exec("double x = 10; double y = 4; double z = 3; return x%(y%z);"));
+        assertEquals(10.0%1.0, exec("double x = 10; double y = 1; return x%y;"));
+        assertEquals(0.0%1.0, exec("double x = 0; double y = 1; return x%y;"));
+    }
+    
+    public void testDoubleConst() throws Exception {
+        assertEquals(1.0%1.0, exec("return 1.0%1.0;"));
+        assertEquals(2.0%3.0, exec("return 2.0%3.0;"));
+        assertEquals(5.0%10.0, exec("return 5.0%10.0;"));
+        assertEquals(10.0%1.0%2.0, exec("return 10.0%1.0%2.0;"));
+        assertEquals((10.0%1.0)%2.0, exec("return (10.0%1.0)%2.0;"));
+        assertEquals(10.0%(4.0%3.0), exec("return 10.0%(4.0%3.0);"));
+        assertEquals(10.0%1.0, exec("return 10.0%1.0;"));
+        assertEquals(0.0%1.0, exec("return 0.0%1.0;"));
+    }
+    
+    public void testDivideByZero() throws Exception {
+        try {
+            exec("int x = 1; int y = 0; return x % y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+        
+        try {
+            exec("long x = 1L; long y = 0L; return x % y;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+    }
+    
+    public void testDivideByZeroConst() throws Exception {
+        try {
+            exec("return 1%0;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+        
+        try {
+            exec("return 1L%0L;");
+            fail("should have hit exception");
+        } catch (ArithmeticException expected) {
+            // divide by zero
+        }
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptEngineTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptEngineTests.java
new file mode 100644
index 0000000..d2bbe02
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptEngineTests.java
@@ -0,0 +1,109 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptService;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+public class ScriptEngineTests extends ScriptTestCase {
+
+    public void testSimpleEquation() {
+        final Object value = exec("return 1 + 2;");
+        assertEquals(3, ((Number)value).intValue());
+    }
+
+    public void testMapAccess() {
+        Map<String, Object> vars = new HashMap<>();
+        Map<String, Object> obj2 = new HashMap<>();
+        obj2.put("prop2", "value2");
+        Map<String, Object> obj1 = new HashMap<>();
+        obj1.put("prop1", "value1");
+        obj1.put("obj2", obj2);
+        obj1.put("l", Arrays.asList("2", "1"));
+        vars.put("obj1", obj1);
+
+        Object value = exec("return input.get(\"obj1\");", vars);
+        obj1 = (Map<String, Object>)value;
+        assertEquals("value1", obj1.get("prop1"));
+        assertEquals("value2", ((Map<String, Object>) obj1.get("obj2")).get("prop2"));
+
+        value = exec("return ((List)((Map<String, Object>)input.get(\"obj1\")).get(\"l\")).get(0);", vars);
+        assertEquals("2", value);
+    }
+
+    public void testAccessListInScript() {
+        Map<String, Object> vars = new HashMap<>();
+        Map<String, Object> obj2 = new HashMap<>();
+        obj2.put("prop2", "value2");
+        Map<String, Object> obj1 = new HashMap<>();
+        obj1.put("prop1", "value1");
+        obj1.put("obj2", obj2);
+        vars.put("l", Arrays.asList("1", "2", "3", obj1));
+
+        assertEquals(4, exec("return ((List)input.get(\"l\")).size();", vars));
+        assertEquals("1", exec("return ((List)input.get(\"l\")).get(0);", vars));
+
+        Object value = exec("return ((List)input.get(\"l\")).get(3);", vars);
+        obj1 = (Map<String, Object>)value;
+        assertEquals("value1", obj1.get("prop1"));
+        assertEquals("value2", ((Map<String, Object>)obj1.get("obj2")).get("prop2"));
+
+        assertEquals("value1", exec("return ((Map<String, Object>)((List)input.get(\"l\")).get(3)).get(\"prop1\");", vars));
+    }
+
+    public void testChangingVarsCrossExecution1() {
+        Map<String, Object> vars = new HashMap<>();
+        Map<String, Object> ctx = new HashMap<>();
+        vars.put("ctx", ctx);
+
+        Object compiledScript = scriptEngine.compile("return ((Map<String, Object>)input.get(\"ctx\")).get(\"value\");");
+        ExecutableScript script = scriptEngine.executable(new CompiledScript(ScriptService.ScriptType.INLINE,
+                "testChangingVarsCrossExecution1", "plan-a", compiledScript), vars);
+
+        ctx.put("value", 1);
+        Object o = script.run();
+        assertEquals(1, ((Number) o).intValue());
+
+        ctx.put("value", 2);
+        o = script.run();
+        assertEquals(2, ((Number) o).intValue());
+    }
+
+    public void testChangingVarsCrossExecution2() {
+        Map<String, Object> vars = new HashMap<>();
+        Object compiledScript = scriptEngine.compile("return input.get(\"value\");");
+
+        ExecutableScript script = scriptEngine.executable(new CompiledScript(ScriptService.ScriptType.INLINE,
+                "testChangingVarsCrossExecution2", "plan-a", compiledScript), vars);
+
+        script.setNextVar("value", 1);
+        Object value = script.run();
+        assertEquals(1, ((Number)value).intValue());
+
+        script.setNextVar("value", 2);
+        value = script.run();
+        assertEquals(2, ((Number)value).intValue());
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptTestCase.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptTestCase.java
new file mode 100644
index 0000000..253e371
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptTestCase.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.Map;
+
+/**
+ * Base test case for scripting unit tests.
+ * <p>
+ * Typically just asserts the output of {@code exec()}
+ */
+public abstract class ScriptTestCase extends ESTestCase {
+    protected PlanAScriptEngineService scriptEngine;
+    
+    /** Override to provide different compiler settings */
+    protected Settings getSettings() {
+        Settings.Builder builder = Settings.builder();
+        builder.put(PlanAScriptEngineService.NUMERIC_OVERFLOW, random().nextBoolean());
+        return builder.build();
+    }
+
+    @Before
+    public void setup() {
+        scriptEngine = new PlanAScriptEngineService(getSettings());
+    }
+
+    /** Compiles and returns the result of {@code script} */
+    public Object exec(String script) {
+        return exec(script, null);
+    }
+
+    /** Compiles and returns the result of {@code script} with access to {@code vars} */
+    public Object exec(String script, Map<String, Object> vars) {
+        Object object = scriptEngine.compile(script);
+        CompiledScript compiled = new CompiledScript(ScriptService.ScriptType.INLINE, getTestName(), "plan-a", object);
+        return scriptEngine.executable(compiled, vars).run();
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/StringTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/StringTests.java
new file mode 100644
index 0000000..0fbcaa1
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/StringTests.java
@@ -0,0 +1,75 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+public class StringTests extends ScriptTestCase {
+    
+    public void testAppend() {
+        // boolean
+        assertEquals("cat" + true, exec("String s = \"cat\"; return s + true;"));
+        // byte
+        assertEquals("cat" + (byte)3, exec("String s = \"cat\"; return s + (byte)3;"));
+        // short
+        assertEquals("cat" + (short)3, exec("String s = \"cat\"; return s + (short)3;"));
+        // char
+        assertEquals("cat" + 't', exec("String s = \"cat\"; return s + 't';"));
+        assertEquals("cat" + (char)40, exec("String s = \"cat\"; return s + (char)40;"));
+        // int
+        assertEquals("cat" + 2, exec("String s = \"cat\"; return s + 2;"));
+        // long
+        assertEquals("cat" + 2L, exec("String s = \"cat\"; return s + 2L;"));
+        // float
+        assertEquals("cat" + 2F, exec("String s = \"cat\"; return s + 2F;"));
+        // double
+        assertEquals("cat" + 2.0, exec("String s = \"cat\"; return s + 2.0;"));
+        // String
+        assertEquals("cat" + "cat", exec("String s = \"cat\"; return s + s;"));
+    }
+
+    public void testStringAPI() {
+        assertEquals("", exec("return new String();"));
+        assertEquals('x', exec("String s = \"x\"; return s.charAt(0);"));
+        assertEquals(120, exec("String s = \"x\"; return s.codePointAt(0);"));
+        assertEquals(0, exec("String s = \"x\"; return s.compareTo(\"x\");"));
+        assertEquals("xx", exec("String s = \"x\"; return s.concat(\"x\");"));
+        assertEquals(true, exec("String s = \"xy\"; return s.endsWith(\"y\");"));
+        assertEquals(2, exec("String t = \"abcde\"; return t.indexOf(\"cd\", 1);"));
+        assertEquals(false, exec("String t = \"abcde\"; return t.isEmpty();"));
+        assertEquals(5, exec("String t = \"abcde\"; return t.length();"));
+        assertEquals("cdcde", exec("String t = \"abcde\"; return t.replace(\"ab\", \"cd\");"));
+        assertEquals(false, exec("String s = \"xy\"; return s.startsWith(\"y\");"));
+        assertEquals("e", exec("String t = \"abcde\"; return t.substring(4, 5);"));
+        assertEquals(97, ((char[])exec("String s = \"a\"; return s.toCharArray();"))[0]);
+        assertEquals("a", exec("String s = \" a \"; return s.trim();"));
+        assertEquals('x', exec("return \"x\".charAt(0);"));
+        assertEquals(120, exec("return \"x\".codePointAt(0);"));
+        assertEquals(0, exec("return \"x\".compareTo(\"x\");"));
+        assertEquals("xx", exec("return \"x\".concat(\"x\");"));
+        assertEquals(true, exec("return \"xy\".endsWith(\"y\");"));
+        assertEquals(2, exec("return \"abcde\".indexOf(\"cd\", 1);"));
+        assertEquals(false, exec("return \"abcde\".isEmpty();"));
+        assertEquals(5, exec("return \"abcde\".length();"));
+        assertEquals("cdcde", exec("return \"abcde\".replace(\"ab\", \"cd\");"));
+        assertEquals(false, exec("return \"xy\".startsWith(\"y\");"));
+        assertEquals("e", exec("return \"abcde\".substring(4, 5);"));
+        assertEquals(97, ((char[])exec("return \"a\".toCharArray();"))[0]);
+        assertEquals("a", exec("return \" a \".trim();"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/SubtractionTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/SubtractionTests.java
new file mode 100644
index 0000000..1acd045
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/SubtractionTests.java
@@ -0,0 +1,179 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for subtraction operator across all types */
+//TODO: NaN/Inf/overflow/...
+public class SubtractionTests extends ScriptTestCase {
+    
+    public void testInt() throws Exception {
+        assertEquals(1-1, exec("int x = 1; int y = 1; return x-y;"));
+        assertEquals(2-3, exec("int x = 2; int y = 3; return x-y;"));
+        assertEquals(5-10, exec("int x = 5; int y = 10; return x-y;"));
+        assertEquals(1-1-2, exec("int x = 1; int y = 1; int z = 2; return x-y-z;"));
+        assertEquals((1-1)-2, exec("int x = 1; int y = 1; int z = 2; return (x-y)-z;"));
+        assertEquals(1-(1-2), exec("int x = 1; int y = 1; int z = 2; return x-(y-z);"));
+        assertEquals(10-0, exec("int x = 10; int y = 0; return x-y;"));
+        assertEquals(0-0, exec("int x = 0; int y = 0; return x-x;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(1-1, exec("return 1-1;"));
+        assertEquals(2-3, exec("return 2-3;"));
+        assertEquals(5-10, exec("return 5-10;"));
+        assertEquals(1-1-2, exec("return 1-1-2;"));
+        assertEquals((1-1)-2, exec("return (1-1)-2;"));
+        assertEquals(1-(1-2), exec("return 1-(1-2);"));
+        assertEquals(10-0, exec("return 10-0;"));
+        assertEquals(0-0, exec("return 0-0;"));
+    }
+    
+    public void testByte() throws Exception {
+        assertEquals((byte)1-(byte)1, exec("byte x = 1; byte y = 1; return x-y;"));
+        assertEquals((byte)2-(byte)3, exec("byte x = 2; byte y = 3; return x-y;"));
+        assertEquals((byte)5-(byte)10, exec("byte x = 5; byte y = 10; return x-y;"));
+        assertEquals((byte)1-(byte)1-(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return x-y-z;"));
+        assertEquals(((byte)1-(byte)1)-(byte)2, exec("byte x = 1; byte y = 1; byte z = 2; return (x-y)-z;"));
+        assertEquals((byte)1-((byte)1-(byte)2), exec("byte x = 1; byte y = 1; byte z = 2; return x-(y-z);"));
+        assertEquals((byte)10-(byte)1, exec("byte x = 10; byte y = 1; return x-y;"));
+        assertEquals((byte)0-(byte)0, exec("byte x = 0; byte y = 0; return x-y;"));
+    }
+    
+    public void testByteConst() throws Exception {
+        assertEquals((byte)1-(byte)1, exec("return (byte)1-(byte)1;"));
+        assertEquals((byte)2-(byte)3, exec("return (byte)2-(byte)3;"));
+        assertEquals((byte)5-(byte)10, exec("return (byte)5-(byte)10;"));
+        assertEquals((byte)1-(byte)1-(byte)2, exec("return (byte)1-(byte)1-(byte)2;"));
+        assertEquals(((byte)1-(byte)1)-(byte)2, exec("return ((byte)1-(byte)1)-(byte)2;"));
+        assertEquals((byte)1-((byte)1-(byte)2), exec("return (byte)1-((byte)1-(byte)2);"));
+        assertEquals((byte)10-(byte)1, exec("return (byte)10-(byte)1;"));
+        assertEquals((byte)0-(byte)0, exec("return (byte)0-(byte)0;"));
+    }
+    
+    public void testChar() throws Exception {
+        assertEquals((char)1-(char)1, exec("char x = 1; char y = 1; return x-y;"));
+        assertEquals((char)2-(char)3, exec("char x = 2; char y = 3; return x-y;"));
+        assertEquals((char)5-(char)10, exec("char x = 5; char y = 10; return x-y;"));
+        assertEquals((char)1-(char)1-(char)2, exec("char x = 1; char y = 1; char z = 2; return x-y-z;"));
+        assertEquals(((char)1-(char)1)-(char)2, exec("char x = 1; char y = 1; char z = 2; return (x-y)-z;"));
+        assertEquals((char)1-((char)1-(char)2), exec("char x = 1; char y = 1; char z = 2; return x-(y-z);"));
+        assertEquals((char)10-(char)1, exec("char x = 10; char y = 1; return x-y;"));
+        assertEquals((char)0-(char)0, exec("char x = 0; char y = 0; return x-y;"));
+    }
+    
+    public void testCharConst() throws Exception {
+        assertEquals((char)1-(char)1, exec("return (char)1-(char)1;"));
+        assertEquals((char)2-(char)3, exec("return (char)2-(char)3;"));
+        assertEquals((char)5-(char)10, exec("return (char)5-(char)10;"));
+        assertEquals((char)1-(char)1-(char)2, exec("return (char)1-(char)1-(char)2;"));
+        assertEquals(((char)1-(char)1)-(char)2, exec("return ((char)1-(char)1)-(char)2;"));
+        assertEquals((char)1-((char)1-(char)2), exec("return (char)1-((char)1-(char)2);"));
+        assertEquals((char)10-(char)1, exec("return (char)10-(char)1;"));
+        assertEquals((char)0-(char)0, exec("return (char)0-(char)0;"));
+    }
+    
+    public void testShort() throws Exception {
+        assertEquals((short)1-(short)1, exec("short x = 1; short y = 1; return x-y;"));
+        assertEquals((short)2-(short)3, exec("short x = 2; short y = 3; return x-y;"));
+        assertEquals((short)5-(short)10, exec("short x = 5; short y = 10; return x-y;"));
+        assertEquals((short)1-(short)1-(short)2, exec("short x = 1; short y = 1; short z = 2; return x-y-z;"));
+        assertEquals(((short)1-(short)1)-(short)2, exec("short x = 1; short y = 1; short z = 2; return (x-y)-z;"));
+        assertEquals((short)1-((short)1-(short)2), exec("short x = 1; short y = 1; short z = 2; return x-(y-z);"));
+        assertEquals((short)10-(short)1, exec("short x = 10; short y = 1; return x-y;"));
+        assertEquals((short)0-(short)0, exec("short x = 0; short y = 0; return x-y;"));
+    }
+    
+    public void testShortConst() throws Exception {
+        assertEquals((short)1-(short)1, exec("return (short)1-(short)1;"));
+        assertEquals((short)2-(short)3, exec("return (short)2-(short)3;"));
+        assertEquals((short)5-(short)10, exec("return (short)5-(short)10;"));
+        assertEquals((short)1-(short)1-(short)2, exec("return (short)1-(short)1-(short)2;"));
+        assertEquals(((short)1-(short)1)-(short)2, exec("return ((short)1-(short)1)-(short)2;"));
+        assertEquals((short)1-((short)1-(short)2), exec("return (short)1-((short)1-(short)2);"));
+        assertEquals((short)10-(short)1, exec("return (short)10-(short)1;"));
+        assertEquals((short)0-(short)0, exec("return (short)0-(short)0;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(1L-1L, exec("long x = 1; long y = 1; return x-y;"));
+        assertEquals(2L-3L, exec("long x = 2; long y = 3; return x-y;"));
+        assertEquals(5L-10L, exec("long x = 5; long y = 10; return x-y;"));
+        assertEquals(1L-1L-2L, exec("long x = 1; long y = 1; int z = 2; return x-y-z;"));
+        assertEquals((1L-1L)-2L, exec("long x = 1; long y = 1; int z = 2; return (x-y)-z;"));
+        assertEquals(1L-(1L-2L), exec("long x = 1; long y = 1; int z = 2; return x-(y-z);"));
+        assertEquals(10L-0L, exec("long x = 10; long y = 0; return x-y;"));
+        assertEquals(0L-0L, exec("long x = 0; long y = 0; return x-x;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(1L-1L, exec("return 1L-1L;"));
+        assertEquals(2L-3L, exec("return 2L-3L;"));
+        assertEquals(5L-10L, exec("return 5L-10L;"));
+        assertEquals(1L-1L-2L, exec("return 1L-1L-2L;"));
+        assertEquals((1L-1L)-2L, exec("return (1L-1L)-2L;"));
+        assertEquals(1L-(1L-2L), exec("return 1L-(1L-2L);"));
+        assertEquals(10L-0L, exec("return 10L-0L;"));
+        assertEquals(0L-0L, exec("return 0L-0L;"));
+    }
+    
+    public void testFloat() throws Exception {
+        assertEquals(1F-1F, exec("float x = 1; float y = 1; return x-y;"));
+        assertEquals(2F-3F, exec("float x = 2; float y = 3; return x-y;"));
+        assertEquals(5F-10F, exec("float x = 5; float y = 10; return x-y;"));
+        assertEquals(1F-1F-2F, exec("float x = 1; float y = 1; float z = 2; return x-y-z;"));
+        assertEquals((1F-1F)-2F, exec("float x = 1; float y = 1; float z = 2; return (x-y)-z;"));
+        assertEquals(1F-(1F-2F), exec("float x = 1; float y = 1; float z = 2; return x-(y-z);"));
+        assertEquals(10F-0F, exec("float x = 10; float y = 0; return x-y;"));
+        assertEquals(0F-0F, exec("float x = 0; float y = 0; return x-x;"));
+    }
+    
+    public void testFloatConst() throws Exception {
+        assertEquals(1F-1F, exec("return 1F-1F;"));
+        assertEquals(2F-3F, exec("return 2F-3F;"));
+        assertEquals(5F-10F, exec("return 5F-10F;"));
+        assertEquals(1F-1F-2F, exec("return 1F-1F-2F;"));
+        assertEquals((1F-1F)-2F, exec("return (1F-1F)-2F;"));
+        assertEquals(1F-(1F-2F), exec("return 1F-(1F-2F);"));
+        assertEquals(10F-0F, exec("return 10F-0F;"));
+        assertEquals(0F-0F, exec("return 0F-0F;"));
+    }
+    
+    public void testDouble() throws Exception {
+        assertEquals(1D-1D, exec("double x = 1; double y = 1; return x-y;"));
+        assertEquals(2D-3D, exec("double x = 2; double y = 3; return x-y;"));
+        assertEquals(5D-10D, exec("double x = 5; double y = 10; return x-y;"));
+        assertEquals(1D-1D-2D, exec("double x = 1; double y = 1; double z = 2; return x-y-z;"));
+        assertEquals((1D-1D)-2D, exec("double x = 1; double y = 1; double z = 2; return (x-y)-z;"));
+        assertEquals(1D-(1D-2D), exec("double x = 1; double y = 1; double z = 2; return x-(y-z);"));
+        assertEquals(10D-0D, exec("double x = 10; float y = 0; return x-y;"));
+        assertEquals(0D-0D, exec("double x = 0; float y = 0; return x-x;"));
+    }
+    
+    public void testyDoubleConst() throws Exception {
+        assertEquals(1.0-1.0, exec("return 1.0-1.0;"));
+        assertEquals(2.0-3.0, exec("return 2.0-3.0;"));
+        assertEquals(5.0-10.0, exec("return 5.0-10.0;"));
+        assertEquals(1.0-1.0-2.0, exec("return 1.0-1.0-2.0;"));
+        assertEquals((1.0-1.0)-2.0, exec("return (1.0-1.0)-2.0;"));
+        assertEquals(1.0-(1.0-2.0), exec("return 1.0-(1.0-2.0);"));
+        assertEquals(10.0-0.0, exec("return 10.0-0.0;"));
+        assertEquals(0.0-0.0, exec("return 0.0-0.0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UnaryTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UnaryTests.java
new file mode 100644
index 0000000..c0199ff
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UnaryTests.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for unary operators across different types */
+public class UnaryTests extends ScriptTestCase {
+
+    /** basic tests */
+    public void testBasics() {
+        assertEquals(false, exec("return !true;"));
+        assertEquals(true, exec("boolean x = false; return !x;"));
+        assertEquals(-2, exec("return ~1;"));
+        assertEquals(-2, exec("byte x = 1; return ~x;"));
+        assertEquals(1, exec("return +1;"));
+        assertEquals(1.0, exec("double x = 1; return +x;"));
+        assertEquals(-1, exec("return -1;"));
+        assertEquals(-2, exec("short x = 2; return -x;"));
+    }
+
+    public void testNegationInt() throws Exception {
+        assertEquals(-1, exec("return -1;"));
+        assertEquals(1, exec("return -(-1);"));
+        assertEquals(0, exec("return -0;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UtilityTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UtilityTests.java
new file mode 100644
index 0000000..5c9fe20
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/UtilityTests.java
@@ -0,0 +1,250 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+import org.elasticsearch.test.ESTestCase;
+
+/**
+ * Tests utility methods (typically built-ins)
+ */
+public class UtilityTests extends ESTestCase {
+    
+    public void testDivideWithoutOverflowInt() {
+        assertEquals(5 / 2, Utility.divideWithoutOverflow(5, 2));
+
+        try {
+            Utility.divideWithoutOverflow(Integer.MIN_VALUE, -1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(5, 0);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivideWithoutOverflowLong() {
+        assertEquals(5L / 2L, Utility.divideWithoutOverflow(5L, 2L));
+        
+        try {
+            Utility.divideWithoutOverflow(Long.MIN_VALUE, -1L);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(5L, 0L);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testToByteExact() {
+        for (int b = Byte.MIN_VALUE; b < Byte.MAX_VALUE; b++) {
+            assertEquals((byte)b, Utility.toByteExact(b));
+        }
+        
+        try {
+            Utility.toByteExact(Byte.MIN_VALUE - 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.toByteExact(Byte.MAX_VALUE + 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testToShortExact() {
+        for (int s = Short.MIN_VALUE; s < Short.MAX_VALUE; s++) {
+            assertEquals((short)s, Utility.toShortExact(s));
+        }
+        
+        try {
+            Utility.toShortExact(Short.MIN_VALUE - 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.toShortExact(Short.MAX_VALUE + 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testToCharExact() {
+        for (int c = Character.MIN_VALUE; c < Character.MAX_VALUE; c++) {
+            assertEquals((char)c, Utility.toCharExact(c));
+        }
+        
+        try {
+            Utility.toCharExact(Character.MIN_VALUE - 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.toCharExact(Character.MAX_VALUE + 1);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAddWithoutOverflowFloat() {
+        assertEquals(10F, Utility.addWithoutOverflow(5F, 5F), 0F);
+        assertTrue(Float.isNaN(Utility.addWithoutOverflow(5F, Float.NaN)));
+        assertTrue(Float.isNaN(Utility.addWithoutOverflow(Float.POSITIVE_INFINITY, Float.NEGATIVE_INFINITY)));
+
+        try {
+            Utility.addWithoutOverflow(Float.MAX_VALUE, Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.addWithoutOverflow(-Float.MAX_VALUE, -Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testAddWithoutOverflowDouble() {
+        assertEquals(10D, Utility.addWithoutOverflow(5D, 5D), 0D);
+        assertTrue(Double.isNaN(Utility.addWithoutOverflow(5D, Double.NaN)));
+        assertTrue(Double.isNaN(Utility.addWithoutOverflow(Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY)));
+        
+        try {
+            Utility.addWithoutOverflow(Double.MAX_VALUE, Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.addWithoutOverflow(-Double.MAX_VALUE, -Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtractWithoutOverflowFloat() {
+        assertEquals(5F, Utility.subtractWithoutOverflow(10F, 5F), 0F);
+        assertTrue(Float.isNaN(Utility.subtractWithoutOverflow(5F, Float.NaN)));
+        assertTrue(Float.isNaN(Utility.subtractWithoutOverflow(Float.POSITIVE_INFINITY, Float.POSITIVE_INFINITY)));
+
+        try {
+            Utility.subtractWithoutOverflow(Float.MAX_VALUE, -Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.subtractWithoutOverflow(-Float.MAX_VALUE, Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testSubtractWithoutOverflowDouble() {
+        assertEquals(5D, Utility.subtractWithoutOverflow(10D, 5D), 0D);
+        assertTrue(Double.isNaN(Utility.subtractWithoutOverflow(5D, Double.NaN)));
+        assertTrue(Double.isNaN(Utility.subtractWithoutOverflow(Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY)));
+        
+        try {
+            Utility.subtractWithoutOverflow(Double.MAX_VALUE, -Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.subtractWithoutOverflow(-Double.MAX_VALUE, Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplyWithoutOverflowFloat() {
+        assertEquals(25F, Utility.multiplyWithoutOverflow(5F, 5F), 0F);
+        assertTrue(Float.isNaN(Utility.multiplyWithoutOverflow(5F, Float.NaN)));
+        assertEquals(Float.POSITIVE_INFINITY, Utility.multiplyWithoutOverflow(5F, Float.POSITIVE_INFINITY), 0F);
+
+        try {
+            Utility.multiplyWithoutOverflow(Float.MAX_VALUE, Float.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testMultiplyWithoutOverflowDouble() {
+        assertEquals(25D, Utility.multiplyWithoutOverflow(5D, 5D), 0D);
+        assertTrue(Double.isNaN(Utility.multiplyWithoutOverflow(5D, Double.NaN)));
+        assertEquals(Double.POSITIVE_INFINITY, Utility.multiplyWithoutOverflow(5D, Double.POSITIVE_INFINITY), 0D);
+        
+        try {
+            Utility.multiplyWithoutOverflow(Double.MAX_VALUE, Double.MAX_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivideWithoutOverflowFloat() {
+        assertEquals(5F, Utility.divideWithoutOverflow(25F, 5F), 0F);
+        assertTrue(Float.isNaN(Utility.divideWithoutOverflow(5F, Float.NaN)));
+        assertEquals(Float.POSITIVE_INFINITY, Utility.divideWithoutOverflow(Float.POSITIVE_INFINITY, 5F), 0F);
+
+        try {
+            Utility.divideWithoutOverflow(Float.MAX_VALUE, Float.MIN_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(0F, 0F);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(5F, 0F);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testDivideWithoutOverflowDouble() {
+        assertEquals(5D, Utility.divideWithoutOverflow(25D, 5D), 0D);
+        assertTrue(Double.isNaN(Utility.divideWithoutOverflow(5D, Double.NaN)));
+        assertEquals(Double.POSITIVE_INFINITY, Utility.divideWithoutOverflow(Double.POSITIVE_INFINITY, 5D), 0D);
+        
+        try {
+            Utility.divideWithoutOverflow(Double.MAX_VALUE, Double.MIN_VALUE);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(0D, 0D);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+        
+        try {
+            Utility.divideWithoutOverflow(5D, 0D);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testRemainderWithoutOverflowFloat() {
+        assertEquals(1F, Utility.remainderWithoutOverflow(25F, 4F), 0F);
+        
+        try {
+            Utility.remainderWithoutOverflow(5F, 0F);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+    
+    public void testRemainderWithoutOverflowDouble() {
+        assertEquals(1D, Utility.remainderWithoutOverflow(25D, 4D), 0D);
+        
+        try {
+            Utility.remainderWithoutOverflow(5D, 0D);
+            fail("did not get expected exception");
+        } catch (ArithmeticException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/WhenThingsGoWrongTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/WhenThingsGoWrongTests.java
new file mode 100644
index 0000000..de2c1c9
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/WhenThingsGoWrongTests.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+public class WhenThingsGoWrongTests extends ScriptTestCase {
+    public void testNullPointer() {
+        try {
+            exec("int x = (int) ((Map) input).get(\"missing\"); return x;");
+            fail("should have hit npe");
+        } catch (NullPointerException expected) {}
+    }
+
+    public void testInvalidShift() {
+        try {
+            exec("float x = 15F; x <<= 2; return x;");
+            fail("should have hit cce");
+        } catch (ClassCastException expected) {}
+
+        try {
+            exec("double x = 15F; x <<= 2; return x;");
+            fail("should have hit cce");
+        } catch (ClassCastException expected) {}
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/XorTests.java b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/XorTests.java
new file mode 100644
index 0000000..f10477d
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/XorTests.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.plan.a;
+
+/** Tests for xor operator across all types */
+public class XorTests extends ScriptTestCase {
+    
+    public void testInt() throws Exception {
+        assertEquals(5 ^ 12, exec("int x = 5; int y = 12; return x ^ y;"));
+        assertEquals(5 ^ -12, exec("int x = 5; int y = -12; return x ^ y;"));
+        assertEquals(7 ^ 15 ^ 3, exec("int x = 7; int y = 15; int z = 3; return x ^ y ^ z;"));
+    }
+    
+    public void testIntConst() throws Exception {
+        assertEquals(5 ^ 12, exec("return 5 ^ 12;"));
+        assertEquals(5 ^ -12, exec("return 5 ^ -12;"));
+        assertEquals(7 ^ 15 ^ 3, exec("return 7 ^ 15 ^ 3;"));
+    }
+    
+    public void testLong() throws Exception {
+        assertEquals(5L ^ 12L, exec("long x = 5; long y = 12; return x ^ y;"));
+        assertEquals(5L ^ -12L, exec("long x = 5; long y = -12; return x ^ y;"));
+        assertEquals(7L ^ 15L ^ 3L, exec("long x = 7; long y = 15; long z = 3; return x ^ y ^ z;"));
+    }
+    
+    public void testLongConst() throws Exception {
+        assertEquals(5L ^ 12L, exec("return 5L ^ 12L;"));
+        assertEquals(5L ^ -12L, exec("return 5L ^ -12L;"));
+        assertEquals(7L ^ 15L ^ 3L, exec("return 7L ^ 15L ^ 3L;"));
+    }
+
+    public void testBool() throws Exception {
+        assertEquals(false, exec("boolean x = true; boolean y = true; return x ^ y;"));
+        assertEquals(true, exec("boolean x = true; boolean y = false; return x ^ y;"));
+        assertEquals(true, exec("boolean x = false; boolean y = true; return x ^ y;"));
+        assertEquals(false, exec("boolean x = false; boolean y = false; return x ^ y;"));
+    }
+
+    public void testBoolConst() throws Exception {
+        assertEquals(false, exec("return true ^ true;"));
+        assertEquals(true, exec("return true ^ false;"));
+        assertEquals(true, exec("return false ^ true;"));
+        assertEquals(false, exec("return false ^ false;"));
+    }
+}
diff --git a/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml
new file mode 100644
index 0000000..6259780
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/10_basic.yaml
@@ -0,0 +1,14 @@
+# Integration tests for Plan A Plugin
+#
+"Plan A plugin loaded":
+    - do:
+        cluster.state: {}
+
+    # Get master node id
+    - set: { master_node: master }
+
+    - do:
+        nodes.info: {}
+
+    - match:  { nodes.$master.plugins.0.name: lang-plan-a }
+    - match:  { nodes.$master.plugins.0.jvm: true  }
diff --git a/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/20_scriptfield.yaml b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/20_scriptfield.yaml
new file mode 100644
index 0000000..0a5a3a4
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/20_scriptfield.yaml
@@ -0,0 +1,27 @@
+# Integration tests for using a scripted field
+#
+setup:
+    - do:
+        index:
+            index: test
+            type: test
+            id: 1
+            body: { "foo": "aaa" }
+    - do:
+        indices.refresh: {}
+
+---
+
+"Scripted Field":
+    - do:
+        search:
+            body:
+                script_fields:
+                    bar:
+                        script: 
+                            inline: "input.doc.foo.0 + input.x;"
+                            lang: plan-a
+                            params:
+                                x: "bbb"
+
+    - match: { hits.hits.0.fields.bar.0: "aaabbb"}
diff --git a/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/30_search.yaml b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/30_search.yaml
new file mode 100644
index 0000000..a8d96a0
--- /dev/null
+++ b/plugins/lang-plan-a/src/test/resources/rest-api-spec/test/plan_a/30_search.yaml
@@ -0,0 +1,97 @@
+# Integration tests for Plan-A search scripting
+#
+"Plan-A Query":
+    - do:
+        index:
+            index: test
+            type: test
+            id: 1
+            body: { "test": "value beck", "num1": 1.0 }
+    - do:
+        index:
+            index: test
+            type: test
+            id: 2
+            body: { "test": "value beck", "num1": 2.0 }
+    - do:
+        index:
+            index: test
+            type: test
+            id: 3
+            body: { "test": "value beck", "num1": 3.0 }
+    - do:
+        indices.refresh: {}
+
+    - do:
+        index: test
+        search:
+            body:
+                query:
+                    script:
+                        script:
+                            inline: "input.doc.num1.0 > 1;"
+                            lang: plan-a
+                script_fields:
+                    sNum1:
+                        script: 
+                            inline: "input.doc.num1.0;"
+                            lang: plan-a
+                sort:
+                    num1:
+                        order: asc
+
+    - match: { hits.total: 2 }
+    - match: { hits.hits.0.fields.sNum1.0: 2.0 }
+    - match: { hits.hits.1.fields.sNum1.0: 3.0 }
+
+    - do:
+        index: test
+        search:
+            body:
+                query:
+                    script:
+                        script:
+                            inline: "input.doc.num1.0 > input.param1;"
+                            lang: plan-a
+                            params:
+                                param1: 1
+
+                script_fields:
+                    sNum1:
+                        script:
+                            inline: "return input.doc.num1.0;"
+                            lang: plan-a
+                sort:
+                    num1:
+                        order: asc
+
+    - match: { hits.total: 2 }
+    - match: { hits.hits.0.fields.sNum1.0: 2.0 }
+    - match: { hits.hits.1.fields.sNum1.0: 3.0 }
+
+    - do:
+        index: test
+        search:
+            body:
+                query:
+                    script:
+                        script:
+                            inline: "input.doc.num1.0 > input.param1;"
+                            lang: plan-a
+                            params:
+                                param1: -1
+
+                script_fields:
+                    sNum1:
+                        script: 
+                            inline: "input.doc.num1.0;"
+                            lang: plan-a
+                sort:
+                    num1:
+                        order: asc
+
+    - match: { hits.total: 3 }
+    - match: { hits.hits.0.fields.sNum1.0: 1.0 }
+    - match: { hits.hits.1.fields.sNum1.0: 2.0 }
+    - match: { hits.hits.2.fields.sNum1.0: 3.0 }
+
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java
index 9378f2d..9b7d8af 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/AttachmentUnitTestCase.java
@@ -22,13 +22,20 @@ package org.elasticsearch.mapper.attachments;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Before;
 
 public class AttachmentUnitTestCase extends ESTestCase {
 
     protected Settings testSettings;
-    
+
+    protected static IndicesModule getIndicesModuleWithRegisteredAttachmentMapper() {
+        IndicesModule indicesModule = new IndicesModule();
+        indicesModule.registerMapper(AttachmentMapper.CONTENT_TYPE, new AttachmentMapper.TypeParser());
+        return indicesModule;
+    }
+
     @Before
     public void createSettings() throws Exception {
       testSettings = Settings.builder()
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/DateAttachmentMapperTests.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/DateAttachmentMapperTests.java
index 858ed8a..f93785e 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/DateAttachmentMapperTests.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/DateAttachmentMapperTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.mapper.attachments;
 
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.MapperTestUtils;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
@@ -37,7 +38,7 @@ public class DateAttachmentMapperTests extends AttachmentUnitTestCase {
 
     @Before
     public void setupMapperParser() throws Exception {
-        mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY).documentMapperParser();
+        mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
     }
 
     public void testSimpleMappings() throws Exception {
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/EncryptedDocMapperTests.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/EncryptedDocMapperTests.java
index e086d9b..10e82e2 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/EncryptedDocMapperTests.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/EncryptedDocMapperTests.java
@@ -21,11 +21,11 @@ package org.elasticsearch.mapper.attachments;
 
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.MapperTestUtils;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.mapper.attachments.AttachmentMapper;
 
 import java.io.IOException;
 
@@ -42,7 +42,7 @@ import static org.hamcrest.Matchers.*;
 public class EncryptedDocMapperTests extends AttachmentUnitTestCase {
 
     public void testMultipleDocsEncryptedLast() throws IOException {
-        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY).documentMapperParser();
+        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
 
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/encrypted/test-mapping.json");
         DocumentMapper docMapper = mapperParser.parse(mapping);
@@ -72,7 +72,7 @@ public class EncryptedDocMapperTests extends AttachmentUnitTestCase {
     }
 
     public void testMultipleDocsEncryptedFirst() throws IOException {
-        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY).documentMapperParser();
+        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/encrypted/test-mapping.json");
         DocumentMapper docMapper = mapperParser.parse(mapping);
         byte[] html = copyToBytesFromClasspath("/org/elasticsearch/index/mapper/attachment/test/sample-files/htmlWithValidDateMeta.html");
@@ -103,9 +103,8 @@ public class EncryptedDocMapperTests extends AttachmentUnitTestCase {
     public void testMultipleDocsEncryptedNotIgnoringErrors() throws IOException {
         try {
             DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(),
-                Settings.builder()
-                    .put("index.mapping.attachment.ignore_errors", false)
-                    .build()).documentMapperParser();
+                Settings.builder().put("index.mapping.attachment.ignore_errors", false).build(),
+                getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
 
             String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/encrypted/test-mapping.json");
             DocumentMapper docMapper = mapperParser.parse(mapping);
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/LanguageDetectionAttachmentMapperTests.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/LanguageDetectionAttachmentMapperTests.java
index b2d361f..868ecb3 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/LanguageDetectionAttachmentMapperTests.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/LanguageDetectionAttachmentMapperTests.java
@@ -21,11 +21,11 @@ package org.elasticsearch.mapper.attachments;
 
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.MapperTestUtils;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.mapper.attachments.AttachmentMapper;
 import org.junit.Before;
 
 import java.io.IOException;
@@ -50,9 +50,8 @@ public class LanguageDetectionAttachmentMapperTests extends AttachmentUnitTestCa
 
     public void setupMapperParser(boolean langDetect) throws IOException {
         DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(),
-            Settings.settingsBuilder()
-                .put("index.mapping.attachment.detect_language", langDetect)
-            .build()).documentMapperParser();
+            Settings.settingsBuilder().put("index.mapping.attachment.detect_language", langDetect).build(),
+            getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/language/language-mapping.json");
         docMapper = mapperParser.parse(mapping);
 
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MapperTestUtils.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MapperTestUtils.java
deleted file mode 100644
index 1513f7b..0000000
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MapperTestUtils.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.mapper.attachments;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexSettings;
-import org.elasticsearch.index.analysis.AnalysisRegistry;
-import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.similarity.SimilarityService;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.mapper.MapperRegistry;
-import org.elasticsearch.test.IndexSettingsModule;
-
-import java.io.IOException;
-import java.nio.file.Path;
-import java.util.Collections;
-
-class MapperTestUtils {
-
-    public static MapperService newMapperService(Path tempDir, Settings indexSettings) throws IOException {
-        Settings nodeSettings = Settings.builder()
-            .put("path.home", tempDir)
-            .build();
-        indexSettings = Settings.builder()
-            .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-            .put(indexSettings)
-            .build();
-        IndexSettings idxSettings = IndexSettingsModule.newIndexSettings(new Index("test"), indexSettings);
-        AnalysisService analysisService = new AnalysisRegistry(null, new Environment(nodeSettings)).build(idxSettings);
-        SimilarityService similarityService = new SimilarityService(idxSettings, Collections.emptyMap());
-        IndicesModule indicesModule = new IndicesModule();
-        indicesModule.registerMapper(AttachmentMapper.CONTENT_TYPE, new AttachmentMapper.TypeParser());
-        MapperRegistry mapperRegistry = indicesModule.getMapperRegistry();
-        return new MapperService(idxSettings, analysisService, similarityService, mapperRegistry);
-    }
-}
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MetadataMapperTests.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MetadataMapperTests.java
index cf2a130..acf0163 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MetadataMapperTests.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MetadataMapperTests.java
@@ -21,11 +21,11 @@ package org.elasticsearch.mapper.attachments;
 
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.MapperTestUtils;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperParsingException;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.mapper.attachments.AttachmentMapper;
 
 import java.io.IOException;
 
@@ -44,7 +44,7 @@ public class MetadataMapperTests extends AttachmentUnitTestCase {
                                              .put(this.testSettings)
                                              .put(otherSettings)
                                              .build();
-        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), settings).documentMapperParser();
+        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), settings, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
 
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/metadata/test-mapping.json");
         DocumentMapper docMapper = mapperParser.parse(mapping);
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MultifieldAttachmentMapperTests.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MultifieldAttachmentMapperTests.java
index 4f070bd..40593dd 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MultifieldAttachmentMapperTests.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/MultifieldAttachmentMapperTests.java
@@ -22,13 +22,13 @@ package org.elasticsearch.mapper.attachments;
 import org.elasticsearch.common.Base64;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.MapperTestUtils;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.mapper.attachments.AttachmentMapper;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.junit.After;
 import org.junit.Before;
@@ -48,7 +48,7 @@ public class MultifieldAttachmentMapperTests extends AttachmentUnitTestCase {
 
     @Before
     public void setupMapperParser() throws Exception {
-        mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY).documentMapperParser();
+        mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
 
     }
 
@@ -91,7 +91,7 @@ public class MultifieldAttachmentMapperTests extends AttachmentUnitTestCase {
         String bytes = Base64.encodeBytes(originalText.getBytes(StandardCharsets.ISO_8859_1));
         threadPool = new ThreadPool("testing-only");
 
-        MapperService mapperService = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY);
+        MapperService mapperService = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper());
 
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/multifield/multifield-mapping.json");
 
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java
index 0023fc4..01e87dc 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/SimpleAttachmentMapperTests.java
@@ -25,6 +25,7 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.MapperTestUtils;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.MapperService;
@@ -42,7 +43,7 @@ import static org.hamcrest.Matchers.*;
 public class SimpleAttachmentMapperTests extends AttachmentUnitTestCase {
 
     public void testSimpleMappings() throws Exception {
-        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY).documentMapperParser();
+        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/simple/test-mapping.json");
         DocumentMapper docMapper = mapperParser.parse(mapping);
         byte[] html = copyToBytesFromClasspath("/org/elasticsearch/index/mapper/attachment/test/sample-files/testXHTML.html");
@@ -69,9 +70,8 @@ public class SimpleAttachmentMapperTests extends AttachmentUnitTestCase {
 
     public void testContentBackcompat() throws Exception {
         DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(),
-            Settings.builder()
-                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id)
-            .build()).documentMapperParser();
+            Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_4_2.id).build(),
+            getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/simple/test-mapping.json");
         DocumentMapper docMapper = mapperParser.parse(mapping);
         byte[] html = copyToBytesFromClasspath("/org/elasticsearch/index/mapper/attachment/test/sample-files/testXHTML.html");
@@ -86,7 +86,7 @@ public class SimpleAttachmentMapperTests extends AttachmentUnitTestCase {
      * test for https://github.com/elastic/elasticsearch-mapper-attachments/issues/179
      */
     public void testSimpleMappingsWithAllFields() throws Exception {
-        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY).documentMapperParser();
+        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/simple/test-mapping-all-fields.json");
         DocumentMapper docMapper = mapperParser.parse(mapping);
         byte[] html = copyToBytesFromClasspath("/org/elasticsearch/index/mapper/attachment/test/sample-files/testXHTML.html");
@@ -131,7 +131,7 @@ public class SimpleAttachmentMapperTests extends AttachmentUnitTestCase {
                 .endObject();
 
         byte[] mapping = mappingBuilder.bytes().toBytes();
-        MapperService mapperService = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY);
+        MapperService mapperService = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper());
         DocumentMapper docMapper = mapperService.parse("mail", new CompressedXContent(mapping), true);
         // this should not throw an exception
         mapperService.parse("mail", new CompressedXContent(docMapper.mapping().toString()), true);
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java
index f626433..fcd430d 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/StandaloneRunner.java
@@ -30,10 +30,10 @@ import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.env.Environment;
+import org.elasticsearch.index.MapperTestUtils;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.mapper.attachments.AttachmentMapper;
 
 import java.io.FileNotFoundException;
 import java.io.IOException;
@@ -46,6 +46,7 @@ import static org.elasticsearch.common.cli.CliToolConfig.Builder.cmd;
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.option;
 import static org.elasticsearch.common.io.Streams.copy;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.mapper.attachments.AttachmentUnitTestCase.getIndicesModuleWithRegisteredAttachmentMapper;
 import static org.elasticsearch.test.StreamsUtils.copyToStringFromClasspath;
 
 /**
@@ -88,7 +89,7 @@ public class StandaloneRunner extends CliTool {
             this.size = size;
             this.url = url;
             this.base64text = base64text;
-            DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(PathUtils.get("."), Settings.EMPTY).documentMapperParser(); // use CWD b/c it won't be used
+            DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(PathUtils.get("."), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser(); // use CWD b/c it won't be used
 
             String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/standalone/standalone-mapping.json");
             docMapper = mapperParser.parse(mapping);
diff --git a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/VariousDocTests.java b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/VariousDocTests.java
index c2569fd..5341e03 100644
--- a/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/VariousDocTests.java
+++ b/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/VariousDocTests.java
@@ -23,10 +23,10 @@ import org.apache.tika.io.IOUtils;
 import org.apache.tika.metadata.Metadata;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.MapperTestUtils;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.mapper.attachments.AttachmentMapper;
 import org.junit.Before;
 
 import java.io.IOException;
@@ -48,7 +48,7 @@ public class VariousDocTests extends AttachmentUnitTestCase {
 
     @Before
     public void createMapper() throws IOException {
-        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY).documentMapperParser();
+        DocumentMapperParser mapperParser = MapperTestUtils.newMapperService(createTempDir(), Settings.EMPTY, getIndicesModuleWithRegisteredAttachmentMapper()).documentMapperParser();
 
         String mapping = copyToStringFromClasspath("/org/elasticsearch/index/mapper/attachment/test/unit/various-doc/test-mapping.json");
         docMapper = mapperParser.parse(mapping);
@@ -93,7 +93,7 @@ public class VariousDocTests extends AttachmentUnitTestCase {
         assertParseable("text-in-english.txt");
         testMapper("text-in-english.txt", false);
     }
-    
+
     /**
      * Test for .epub
      */
@@ -129,7 +129,7 @@ public class VariousDocTests extends AttachmentUnitTestCase {
     protected void assertParseable(String filename) throws Exception {
         try (InputStream is = VariousDocTests.class.getResourceAsStream("/org/elasticsearch/index/mapper/attachment/test/sample-files/" + filename)) {
             byte bytes[] = IOUtils.toByteArray(is);
-            String parsedContent = TikaImpl.parse(bytes, new Metadata(), -1);  
+            String parsedContent = TikaImpl.parse(bytes, new Metadata(), -1);
             assertThat(parsedContent, not(isEmptyOrNullString()));
             logger.debug("extracted content: {}", parsedContent);
         }
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
index e5db2ed..711b8db 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java
@@ -26,6 +26,56 @@ import org.elasticsearch.common.component.LifecycleComponent;
  *
  */
 public interface AwsS3Service extends LifecycleComponent<AwsS3Service> {
+
+    final class CLOUD_AWS {
+        public static final String KEY = "cloud.aws.access_key";
+        public static final String SECRET = "cloud.aws.secret_key";
+        public static final String PROTOCOL = "cloud.aws.protocol";
+        public static final String PROXY_HOST = "cloud.aws.proxy.host";
+        public static final String PROXY_PORT = "cloud.aws.proxy.port";
+        public static final String PROXY_USERNAME = "cloud.aws.proxy.username";
+        public static final String PROXY_PASSWORD = "cloud.aws.proxy.password";
+        public static final String SIGNER = "cloud.aws.signer";
+        public static final String REGION = "cloud.aws.region";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.proxy_host";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.proxy_port";
+    }
+
+    final class CLOUD_S3 {
+        public static final String KEY = "cloud.aws.s3.access_key";
+        public static final String SECRET = "cloud.aws.s3.secret_key";
+        public static final String PROTOCOL = "cloud.aws.s3.protocol";
+        public static final String PROXY_HOST = "cloud.aws.s3.proxy.host";
+        public static final String PROXY_PORT = "cloud.aws.s3.proxy.port";
+        public static final String PROXY_USERNAME = "cloud.aws.s3.proxy.username";
+        public static final String PROXY_PASSWORD = "cloud.aws.s3.proxy.password";
+        public static final String SIGNER = "cloud.aws.s3.signer";
+        public static final String ENDPOINT = "cloud.aws.s3.endpoint";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_HOST = "cloud.aws.s3.proxy_host";
+        @Deprecated
+        public static final String DEPRECATED_PROXY_PORT = "cloud.aws.s3.proxy_port";
+    }
+
+    final class REPOSITORY_S3 {
+        public static final String BUCKET = "repositories.s3.bucket";
+        public static final String ENDPOINT = "repositories.s3.endpoint";
+        public static final String PROTOCOL = "repositories.s3.protocol";
+        public static final String REGION = "repositories.s3.region";
+        public static final String SERVER_SIDE_ENCRYPTION = "repositories.s3.server_side_encryption";
+        public static final String BUFFER_SIZE = "repositories.s3.buffer_size";
+        public static final String MAX_RETRIES = "repositories.s3.max_retries";
+        public static final String CHUNK_SIZE = "repositories.s3.chunk_size";
+        public static final String COMPRESS = "repositories.s3.compress";
+        public static final String STORAGE_CLASS = "repositories.s3.storage_class";
+        public static final String CANNED_ACL = "repositories.s3.canned_acl";
+        public static final String BASE_PATH = "repositories.s3.base_path";
+    }
+
+
+
     AmazonS3 client();
 
     AmazonS3 client(String endpoint, String protocol, String region, String account, String key);
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
index 4752a3f..7d0b72c 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java
@@ -50,8 +50,12 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
     @Inject
     public InternalAwsS3Service(Settings settings, SettingsFilter settingsFilter) {
         super(settings);
-        settingsFilter.addFilter("cloud.aws.access_key");
-        settingsFilter.addFilter("cloud.aws.secret_key");
+        settingsFilter.addFilter(CLOUD_AWS.KEY);
+        settingsFilter.addFilter(CLOUD_AWS.SECRET);
+        settingsFilter.addFilter(CLOUD_AWS.PROXY_PASSWORD);
+        settingsFilter.addFilter(CLOUD_S3.KEY);
+        settingsFilter.addFilter(CLOUD_S3.SECRET);
+        settingsFilter.addFilter(CLOUD_S3.PROXY_PASSWORD);
         settingsFilter.addFilter("access_key");
         settingsFilter.addFilter("secret_key");
     }
@@ -59,9 +63,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
     @Override
     public synchronized AmazonS3 client() {
         String endpoint = getDefaultEndpoint();
-        String account = settings.get("cloud.aws.access_key");
-        String key = settings.get("cloud.aws.secret_key");
-
+        String account = settings.get(CLOUD_S3.KEY, settings.get(CLOUD_AWS.KEY));
+        String key = settings.get(CLOUD_S3.SECRET, settings.get(CLOUD_AWS.SECRET));
         return getClient(endpoint, null, account, key, null);
     }
 
@@ -79,8 +82,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
             endpoint = getDefaultEndpoint();
         }
         if (account == null || key == null) {
-            account = settings.get("cloud.aws.access_key");
-            key = settings.get("cloud.aws.secret_key");
+            account = settings.get(CLOUD_S3.KEY, settings.get(CLOUD_AWS.KEY));
+            key = settings.get(CLOUD_S3.SECRET, settings.get(CLOUD_AWS.SECRET));
         }
 
         return getClient(endpoint, protocol, account, key, maxRetries);
@@ -99,8 +102,8 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
         // but can force objects from every response to the old generation.
         clientConfiguration.setResponseMetadataCacheSize(0);
         if (protocol == null) {
-            protocol = settings.get("cloud.aws.protocol", "https").toLowerCase(Locale.ROOT);
-            protocol = settings.get("cloud.aws.s3.protocol", protocol).toLowerCase(Locale.ROOT);
+            protocol = settings.get(CLOUD_AWS.PROTOCOL, "https").toLowerCase(Locale.ROOT);
+            protocol = settings.get(CLOUD_S3.PROTOCOL, protocol).toLowerCase(Locale.ROOT);
         }
 
         if ("http".equals(protocol)) {
@@ -111,18 +114,25 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
             throw new IllegalArgumentException("No protocol supported [" + protocol + "], can either be [http] or [https]");
         }
 
-        String proxyHost = settings.get("cloud.aws.proxy_host");
-        proxyHost = settings.get("cloud.aws.s3.proxy_host", proxyHost);
+        String proxyHost = settings.get(CLOUD_AWS.PROXY_HOST, settings.get(CLOUD_AWS.DEPRECATED_PROXY_HOST));
+        proxyHost = settings.get(CLOUD_S3.PROXY_HOST, settings.get(CLOUD_S3.DEPRECATED_PROXY_HOST, proxyHost));
         if (proxyHost != null) {
-            String portString = settings.get("cloud.aws.proxy_port", "80");
-            portString = settings.get("cloud.aws.s3.proxy_port", portString);
+            String portString = settings.get(CLOUD_AWS.PROXY_PORT, settings.get(CLOUD_AWS.DEPRECATED_PROXY_PORT, "80"));
+            portString = settings.get(CLOUD_S3.PROXY_PORT, settings.get(CLOUD_S3.DEPRECATED_PROXY_PORT, portString));
             Integer proxyPort;
             try {
                 proxyPort = Integer.parseInt(portString, 10);
             } catch (NumberFormatException ex) {
                 throw new IllegalArgumentException("The configured proxy port value [" + portString + "] is invalid", ex);
             }
-            clientConfiguration.withProxyHost(proxyHost).setProxyPort(proxyPort);
+            String proxyUsername = settings.get(CLOUD_S3.PROXY_USERNAME, settings.get(CLOUD_AWS.PROXY_USERNAME));
+            String proxyPassword = settings.get(CLOUD_S3.PROXY_PASSWORD, settings.get(CLOUD_AWS.PROXY_PASSWORD));
+
+            clientConfiguration
+                .withProxyHost(proxyHost)
+                .withProxyPort(proxyPort)
+                .withProxyUsername(proxyUsername)
+                .withProxyPassword(proxyPassword);
         }
 
         if (maxRetries != null) {
@@ -131,7 +141,7 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
         }
 
         // #155: we might have 3rd party users using older S3 API version
-        String awsSigner = settings.get("cloud.aws.s3.signer", settings.get("cloud.aws.signer"));
+        String awsSigner = settings.get(CLOUD_S3.SIGNER, settings.get(CLOUD_AWS.SIGNER));
         if (awsSigner != null) {
             logger.debug("using AWS API signer [{}]", awsSigner);
             AwsSigner.configureSigner(awsSigner, clientConfiguration, endpoint);
@@ -161,11 +171,11 @@ public class InternalAwsS3Service extends AbstractLifecycleComponent<AwsS3Servic
 
     private String getDefaultEndpoint() {
         String endpoint = null;
-        if (settings.get("cloud.aws.s3.endpoint") != null) {
-            endpoint = settings.get("cloud.aws.s3.endpoint");
+        if (settings.get(CLOUD_S3.ENDPOINT) != null) {
+            endpoint = settings.get(CLOUD_S3.ENDPOINT);
             logger.debug("using explicit s3 endpoint [{}]", endpoint);
-        } else if (settings.get("cloud.aws.region") != null) {
-            String region = settings.get("cloud.aws.region").toLowerCase(Locale.ROOT);
+        } else if (settings.get(CLOUD_AWS.REGION) != null) {
+            String region = settings.get(CLOUD_AWS.REGION).toLowerCase(Locale.ROOT);
             endpoint = getEndpoint(region);
             logger.debug("using s3 region [{}], with endpoint [{}]", region, endpoint);
         }
diff --git a/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java b/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
index 23b3fb1..760968b 100644
--- a/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
+++ b/plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3Repository.java
@@ -20,6 +20,8 @@
 package org.elasticsearch.repositories.s3;
 
 import org.elasticsearch.cloud.aws.AwsS3Service;
+import org.elasticsearch.cloud.aws.AwsS3Service.CLOUD_AWS;
+import org.elasticsearch.cloud.aws.AwsS3Service.REPOSITORY_S3;
 import org.elasticsearch.cloud.aws.blobstore.S3BlobStore;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.blobstore.BlobPath;
@@ -73,18 +75,18 @@ public class S3Repository extends BlobStoreRepository {
     public S3Repository(RepositoryName name, RepositorySettings repositorySettings, IndexShardRepository indexShardRepository, AwsS3Service s3Service) throws IOException {
         super(name.getName(), repositorySettings, indexShardRepository);
 
-        String bucket = repositorySettings.settings().get("bucket", settings.get("repositories.s3.bucket"));
+        String bucket = repositorySettings.settings().get("bucket", settings.get(REPOSITORY_S3.BUCKET));
         if (bucket == null) {
             throw new RepositoryException(name.name(), "No bucket defined for s3 gateway");
         }
 
-        String endpoint = repositorySettings.settings().get("endpoint", settings.get("repositories.s3.endpoint"));
-        String protocol = repositorySettings.settings().get("protocol", settings.get("repositories.s3.protocol"));
+        String endpoint = repositorySettings.settings().get("endpoint", settings.get(REPOSITORY_S3.ENDPOINT));
+        String protocol = repositorySettings.settings().get("protocol", settings.get(REPOSITORY_S3.PROTOCOL));
 
-        String region = repositorySettings.settings().get("region", settings.get("repositories.s3.region"));
+        String region = repositorySettings.settings().get("region", settings.get(REPOSITORY_S3.REGION));
         if (region == null) {
             // Bucket setting is not set - use global region setting
-            String regionSetting = repositorySettings.settings().get("cloud.aws.region", settings.get("cloud.aws.region"));
+            String regionSetting = settings.get(CLOUD_AWS.REGION);
             if (regionSetting != null) {
                 regionSetting = regionSetting.toLowerCase(Locale.ENGLISH);
                 if ("us-east".equals(regionSetting) || "us-east-1".equals(regionSetting)) {
@@ -112,15 +114,15 @@ public class S3Repository extends BlobStoreRepository {
             }
         }
 
-        boolean serverSideEncryption = repositorySettings.settings().getAsBoolean("server_side_encryption", settings.getAsBoolean("repositories.s3.server_side_encryption", false));
-        ByteSizeValue bufferSize = repositorySettings.settings().getAsBytesSize("buffer_size", settings.getAsBytesSize("repositories.s3.buffer_size", null));
-        Integer maxRetries = repositorySettings.settings().getAsInt("max_retries", settings.getAsInt("repositories.s3.max_retries", 3));
-        this.chunkSize = repositorySettings.settings().getAsBytesSize("chunk_size", settings.getAsBytesSize("repositories.s3.chunk_size", new ByteSizeValue(100, ByteSizeUnit.MB)));
-        this.compress = repositorySettings.settings().getAsBoolean("compress", settings.getAsBoolean("repositories.s3.compress", false));
+        boolean serverSideEncryption = repositorySettings.settings().getAsBoolean("server_side_encryption", settings.getAsBoolean(REPOSITORY_S3.SERVER_SIDE_ENCRYPTION, false));
+        ByteSizeValue bufferSize = repositorySettings.settings().getAsBytesSize("buffer_size", settings.getAsBytesSize(REPOSITORY_S3.BUFFER_SIZE, null));
+        Integer maxRetries = repositorySettings.settings().getAsInt("max_retries", settings.getAsInt(REPOSITORY_S3.MAX_RETRIES, 3));
+        this.chunkSize = repositorySettings.settings().getAsBytesSize("chunk_size", settings.getAsBytesSize(REPOSITORY_S3.CHUNK_SIZE, new ByteSizeValue(100, ByteSizeUnit.MB)));
+        this.compress = repositorySettings.settings().getAsBoolean("compress", settings.getAsBoolean(REPOSITORY_S3.COMPRESS, false));
 
         // Parse and validate the user's S3 Storage Class setting
-        String storageClass = repositorySettings.settings().get("storage_class", settings.get("repositories.s3.storage_class", null));
-        String cannedACL = repositorySettings.settings().get("canned_acl", settings.get("repositories.s3.canned_acl", null));
+        String storageClass = repositorySettings.settings().get("storage_class", settings.get(REPOSITORY_S3.STORAGE_CLASS, null));
+        String cannedACL = repositorySettings.settings().get("canned_acl", settings.get(REPOSITORY_S3.CANNED_ACL, null));
 
         logger.debug("using bucket [{}], region [{}], endpoint [{}], protocol [{}], chunk_size [{}], server_side_encryption [{}], buffer_size [{}], max_retries [{}], cannedACL [{}], storageClass [{}]",
                 bucket, region, endpoint, protocol, chunkSize, serverSideEncryption, bufferSize, maxRetries, cannedACL, storageClass);
@@ -128,7 +130,7 @@ public class S3Repository extends BlobStoreRepository {
         blobStore = new S3BlobStore(settings, s3Service.client(endpoint, protocol, region, repositorySettings.settings().get("access_key"), repositorySettings.settings().get("secret_key"), maxRetries),
                 bucket, region, serverSideEncryption, bufferSize, maxRetries, cannedACL, storageClass);
 
-        String basePath = repositorySettings.settings().get("base_path", settings.get("repositories.s3.base_path"));
+        String basePath = repositorySettings.settings().get("base_path", settings.get(REPOSITORY_S3.BASE_PATH));
         if (Strings.hasLength(basePath)) {
             BlobPath path = new BlobPath();
             for(String elem : Strings.splitStringToArray(basePath, '/')) {
diff --git a/plugins/repository-s3/src/main/plugin-metadata/plugin-security.policy b/plugins/repository-s3/src/main/plugin-metadata/plugin-security.policy
index 62b29a2..e5f26c3 100644
--- a/plugins/repository-s3/src/main/plugin-metadata/plugin-security.policy
+++ b/plugins/repository-s3/src/main/plugin-metadata/plugin-security.policy
@@ -19,6 +19,7 @@
 
 grant {
   // needed because of problems in ClientConfiguration
-  // TODO: get this fixed in aws sdk
+  // TODO: get these fixed in aws sdk
+  permission java.lang.RuntimePermission "accessDeclaredMembers";
   permission java.lang.RuntimePermission "getClassLoader";
 };
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
index 34a57d7..b2b2c0c 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/plugins/PluginManagerTests.java
@@ -631,6 +631,7 @@ public class PluginManagerTests extends ESIntegTestCase {
         PluginManager.checkForOfficialPlugins("analysis-stempel");
         PluginManager.checkForOfficialPlugins("delete-by-query");
         PluginManager.checkForOfficialPlugins("lang-javascript");
+        PluginManager.checkForOfficialPlugins("lang-plan-a");
         PluginManager.checkForOfficialPlugins("lang-python");
         PluginManager.checkForOfficialPlugins("mapper-attachments");
         PluginManager.checkForOfficialPlugins("mapper-murmur3");
diff --git a/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java b/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
index c13b91b..1ad972e 100644
--- a/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
+++ b/qa/evil-tests/src/test/java/org/elasticsearch/tribe/TribeUnitTests.java
@@ -24,8 +24,8 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.SuppressForbidden;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.discovery.DiscoveryService;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESTestCase;
@@ -54,13 +54,24 @@ public class TribeUnitTests extends ESTestCase {
     @BeforeClass
     public static void createTribes() {
         Settings baseSettings = Settings.builder()
-            .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true)
             .put("http.enabled", false)
             .put("node.mode", NODE_MODE)
             .put("path.home", createTempDir()).build();
 
-        tribe1 = NodeBuilder.nodeBuilder().settings(Settings.builder().put(baseSettings).put("cluster.name", "tribe1").put("node.name", "tribe1_node")).node();
-        tribe2 = NodeBuilder.nodeBuilder().settings(Settings.builder().put(baseSettings).put("cluster.name", "tribe2").put("node.name", "tribe2_node")).node();
+        tribe1 = new TribeClientNode(
+            Settings.builder()
+                .put(baseSettings)
+                .put("cluster.name", "tribe1")
+                .put("name", "tribe1_node")
+                .put(DiscoveryService.SETTING_DISCOVERY_SEED, random().nextLong())
+                .build()).start();
+        tribe2 = new TribeClientNode(
+            Settings.builder()
+                .put(baseSettings)
+                .put("cluster.name", "tribe2")
+                .put("name", "tribe2_node")
+                .put(DiscoveryService.SETTING_DISCOVERY_SEED, random().nextLong())
+                .build()).start();
     }
 
     @AfterClass
@@ -75,6 +86,8 @@ public class TribeUnitTests extends ESTestCase {
         System.setProperty("es.cluster.name", "tribe_node_cluster");
         System.setProperty("es.tribe.t1.cluster.name", "tribe1");
         System.setProperty("es.tribe.t2.cluster.name", "tribe2");
+        System.setProperty("es.tribe.t1.discovery.id.seed", Long.toString(random().nextLong()));
+        System.setProperty("es.tribe.t2.discovery.id.seed", Long.toString(random().nextLong()));
 
         try {
             assertTribeNodeSuccesfullyCreated(Settings.EMPTY);
@@ -82,6 +95,8 @@ public class TribeUnitTests extends ESTestCase {
             System.clearProperty("es.cluster.name");
             System.clearProperty("es.tribe.t1.cluster.name");
             System.clearProperty("es.tribe.t2.cluster.name");
+            System.clearProperty("es.tribe.t1.discovery.id.seed");
+            System.clearProperty("es.tribe.t2.discovery.id.seed");
         }
     }
 
@@ -98,7 +113,7 @@ public class TribeUnitTests extends ESTestCase {
                 .put("tribe.t1.node.mode", NODE_MODE).put("tribe.t2.node.mode", NODE_MODE)
                 .put("path.home", createTempDir()).put(extraSettings).build();
 
-        try (Node node = NodeBuilder.nodeBuilder().settings(settings).node()) {
+        try (Node node = new Node(settings).start()) {
             try (Client client = node.client()) {
                 assertBusy(new Runnable() {
                     @Override
diff --git a/qa/evil-tests/src/test/resources/org/elasticsearch/tribe/elasticsearch.yml b/qa/evil-tests/src/test/resources/org/elasticsearch/tribe/elasticsearch.yml
index 89f4922..ad1b9be 100644
--- a/qa/evil-tests/src/test/resources/org/elasticsearch/tribe/elasticsearch.yml
+++ b/qa/evil-tests/src/test/resources/org/elasticsearch/tribe/elasticsearch.yml
@@ -1,3 +1,5 @@
 cluster.name: tribe_node_cluster
 tribe.t1.cluster.name: tribe1
-tribe.t2.cluster.name: tribe2
\ No newline at end of file
+tribe.t2.cluster.name: tribe2
+tribe.t1.discovery.id.seed: 1
+tribe.t2.discovery.id.seed: 2
diff --git a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
index 6e912cf..95df2d0 100644
--- a/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
+++ b/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/ESSmokeClientTestCase.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.smoketest;
 
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.SuppressForbidden;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClient;
@@ -34,7 +35,10 @@ import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.BeforeClass;
 
+import java.io.IOException;
 import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.net.URL;
 import java.net.UnknownHostException;
 import java.nio.file.Path;
 import java.util.Locale;
@@ -103,20 +107,14 @@ public abstract class ESSmokeClientTestCase extends LuceneTestCase {
         return client;
     }
 
-    private static Client startClient() throws UnknownHostException {
+    private static Client startClient() throws IOException {
         String[] stringAddresses = clusterAddresses.split(",");
         TransportAddress[] transportAddresses = new TransportAddress[stringAddresses.length];
         int i = 0;
         for (String stringAddress : stringAddresses) {
-            String[] split = stringAddress.split(":");
-            if (split.length < 2) {
-                throw new IllegalArgumentException("address [" + clusterAddresses + "] not valid");
-            }
-            try {
-                transportAddresses[i++] = new InetSocketTransportAddress(InetAddress.getByName(split[0]), Integer.valueOf(split[1]));
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("port is not valid, expected number but was [" + split[1] + "]");
-            }
+            URL url = new URL("http://" + stringAddress);
+            InetAddress inetAddress = InetAddress.getByName(url.getHost());
+            transportAddresses[i++] = new InetSocketTransportAddress(new InetSocketAddress(inetAddress, url.getPort()));
         }
         return startClient(createTempDir(), transportAddresses);
     }
@@ -125,7 +123,7 @@ public abstract class ESSmokeClientTestCase extends LuceneTestCase {
         if (client == null) {
             try {
                 client = startClient();
-            } catch (UnknownHostException e) {
+            } catch (IOException e) {
                 logger.error("can not start the client", e);
             }
             assertThat(client, notNullValue());
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
index 48c34fa..54978b3 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash
@@ -223,6 +223,18 @@ fi
     install_and_check_plugin discovery multicast
 }
 
+@test "[$GROUP] install lang-expression plugin" {
+    install_and_check_plugin lang expression
+}
+
+@test "[$GROUP] install lang-groovy plugin" {
+    install_and_check_plugin lang groovy
+}
+
+@test "[$GROUP] install lang-plan-a plugin" {
+    install_and_check_plugin lang plan-a 
+}
+
 @test "[$GROUP] install javascript plugin" {
     install_and_check_plugin lang javascript rhino-*.jar
 }
@@ -323,6 +335,18 @@ fi
     remove_plugin discovery-multicast
 }
 
+@test "[$GROUP] remove lang-expression plugin" {
+    remove_plugin lang-expression
+}
+
+@test "[$GROUP] remove lang-groovy plugin" {
+    remove_plugin lang-groovy
+}
+
+@test "[$GROUP] remove lang-plan-a plugin" {
+    remove_plugin lang-plan-a
+}
+
 @test "[$GROUP] remove javascript plugin" {
     remove_plugin lang-javascript
 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.analyze.json b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.analyze.json
index 00b0ec1..9fe9bfe 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/api/indices.analyze.json
+++ b/rest-api-spec/src/main/resources/rest-api-spec/api/indices.analyze.json
@@ -44,6 +44,14 @@
           "type" : "string",
           "description" : "The name of the tokenizer to use for the analysis"
         },
+        "detail": {
+          "type" : "boolean",
+          "description" : "With `true`, outputs more advanced details. (default: false)"
+        },
+        "attributes": {
+          "type" : "list",
+          "description" : "A comma-separated list of token attributes to output, this parameter works only with `detail=true`"
+        },
         "format": {
           "type": "enum",
           "options" : ["detailed","text"],
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.aliases/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.aliases/10_basic.yaml
index 640d77e..3ee33b0 100755
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.aliases/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.aliases/10_basic.yaml
@@ -148,7 +148,7 @@
                    index           \s+
                    filter          \s+
                    routing.index   \s+
-                   routing.search  \s+
+                   routing.search
                    \n
                    test_1          \s+
                    test            \s+
@@ -185,6 +185,6 @@
   - match:
       $body: |
             /^
-                index \s+ alias  \s+ \n
-                test  \s+ test_1 \s+ \n
+                index \s+ alias \n
+                test  \s+ test_1 \n
             $/
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.allocation/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.allocation/10_basic.yaml
index be25839..3537da7 100755
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.allocation/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.allocation/10_basic.yaml
@@ -71,7 +71,7 @@
               (
                 \s*                          #allow leading spaces to account for right-justified text
                 \d+                    \s+
-                UNASSIGNED             \s+
+                UNASSIGNED
                 \n
               )?
             $/
@@ -134,7 +134,7 @@
                (
                  \s*                          #allow leading spaces to account for right-justified text
                  \d+                    \s+
-                 UNASSIGNED             \s+
+                 UNASSIGNED
                  \n
                )?
              $/
@@ -156,7 +156,7 @@
                disk.percent            \s+
                host                    \s+
                ip                      \s+
-               node                    \s+
+               node
                \n
 
               ( \s*                          #allow leading spaces to account for right-justified text
@@ -199,7 +199,7 @@
       $body: |
             /^
               disk.percent          \s+
-              node                  \s+
+              node
               \n
               (
                 \s+\d*           \s+
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.count/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.count/10_basic.yaml
index 1a62ab0..87ca75a 100755
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.count/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.count/10_basic.yaml
@@ -19,7 +19,7 @@
   - match:
       $body: |
                /# epoch     timestamp              count
-               ^  \d+   \s  \d{2}:\d{2}:\d{2}  \s  0     \s+  \n  $/
+               ^  \d+   \s  \d{2}:\d{2}:\d{2}  \s  0  \n$/
 
   - do:
       index:
@@ -35,7 +35,7 @@
   - match:
       $body: |
                /# epoch     timestamp              count
-               ^  \d+   \s  \d{2}:\d{2}:\d{2}  \s  1     \s+  \n  $/
+               ^  \d+   \s  \d{2}:\d{2}:\d{2}  \s  1  \n  $/
 
   - do:
       index:
@@ -52,7 +52,7 @@
   - match:
       $body: |
                /# count
-               ^  2     \s+  \n  $/
+               ^  2     \n  $/
 
 
   - do:
@@ -62,7 +62,7 @@
   - match:
       $body: |
                /# epoch     timestamp              count
-               ^  \d+   \s  \d{2}:\d{2}:\d{2}  \s  1  \s+  \n  $/
+               ^  \d+   \s  \d{2}:\d{2}:\d{2}  \s  1  \n  $/
 
   - do:
       cat.count:
@@ -71,5 +71,5 @@
 
   - match:
       $body: |
-               /^  epoch  \s+  timestamp          \s+  count  \s+  \n
-                   \d+    \s+  \d{2}:\d{2}:\d{2}  \s+  \d+    \s+  \n  $/
+               /^  epoch  \s+  timestamp          \s+  count  \n
+                   \d+    \s+  \d{2}:\d{2}:\d{2}  \s+  \d+ \n  $/
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.fielddata/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.fielddata/10_basic.yaml
index bc362fa..dfc580d 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.fielddata/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.fielddata/10_basic.yaml
@@ -38,8 +38,8 @@
 
   - match:
       $body: |
-               /^   total               \s \n
-                    (\s*\d+(\.\d+)?[gmk]?b  \s \n)+ $/
+               /^   total               \n
+                    (\s*\d+(\.\d+)?[gmk]?b  \n)+ $/
 
   - do:
       cat.fielddata:
@@ -48,8 +48,8 @@
 
   - match:
       $body: |
-               /^   total \s+              foo \s+ \n
-                    (\s*\d+(\.\d+)?[gmk]?b \s+ \d+(\.\d+)?[gmk]?b \s \n)+ \s*$/
+               /^   total \s+              foo \n
+                    (\s*\d+(\.\d+)?[gmk]?b \s+ \d+(\.\d+)?[gmk]?b \n)+ $/
 
   - do:
       cat.fielddata:
@@ -59,5 +59,5 @@
 
   - match:
       $body: |
-               /^   total \s+              foo \s+ \n
-                    (\s*\d+(\.\d+)?[gmk]?b \s+ \d+(\.\d+)?[gmk]?b \s \n)+ \s*$/
+               /^   total \s+              foo \n
+                    (\s*\d+(\.\d+)?[gmk]?b \s+ \d+(\.\d+)?[gmk]?b \n)+ $/
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.health/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.health/10_basic.yaml
index 9bfde46..0692df2 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.health/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.health/10_basic.yaml
@@ -46,7 +46,7 @@
                 \d+            \s+ # unassign
                 \d+            \s+ # pending_tasks
                 (-|\d+[.]\d+ms|s) \s+ # max task waiting time
-                \d+\.\d+%      \s+ # active shards percent
+                \d+\.\d+%             # active shards percent
                 \n
               )+
             $/
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.nodes/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.nodes/10_basic.yaml
index 5a4cbc8..2531e6e 100755
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.nodes/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.nodes/10_basic.yaml
@@ -7,7 +7,7 @@
   - match:
       $body: |
                /  #host       ip                          heap.percent        ram.percent     cpu     load                node.role        master          name
-               ^  (\S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d+            \s+  \d*         \s+ \d* \s+ (-)?\d*(\.\d+)?    \s+  [-dc]       \s+  [-*mx]    \s+   (\S+\s?)+   \s+  \n)+  $/
+               ^  (\S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d+            \s+  \d*         \s+ \d* \s+ (-)?\d*(\.\d+)?    \s+  [-dc]       \s+  [-*mx]    \s+   (\S+\s?)+     \n)+  $/
 
   - do:
       cat.nodes:
@@ -15,8 +15,8 @@
 
   - match:
       $body: |
-               /^  host  \s+  ip                     \s+  heap\.percent   \s+  ram\.percent \s+ cpu \s+ load           \s+  node\.role   \s+  master   \s+   name        \s+  \n
-                  (\S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d+             \s+  \d*          \s+ \d* \s+ (-)?\d*(\.\d+)?    \s+  [-dc]        \s+  [-*mx]    \s+   (\S+\s?)+   \s+  \n)+  $/
+               /^  host  \s+  ip                     \s+  heap\.percent   \s+  ram\.percent \s+ cpu \s+ load           \s+  node\.role   \s+  master   \s+   name  \n
+                  (\S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d+             \s+  \d*          \s+ \d* \s+ (-)?\d*(\.\d+)?    \s+  [-dc]        \s+  [-*mx]    \s+   (\S+\s?)+     \n)+  $/
 
   - do:
       cat.nodes:
@@ -25,8 +25,8 @@
 
   - match:
       $body: |
-               /^      heap\.current          \s+  heap\.percent  \s+  heap\.max             \s+ \n
-                  (\s+ \d+(\.\d+)?[ptgmk]?b   \s+  \d+            \s+  \d+(\.\d+)?[ptgmk]?b  \s+ \n)+  $/
+               /^      heap\.current          \s+  heap\.percent  \s+  heap\.max              \n
+                  (\s+ \d+(\.\d+)?[ptgmk]?b   \s+  \d+            \s+  \d+(\.\d+)?[ptgmk]?b   \n)+  $/
 
   - do:
       cat.nodes:
@@ -35,8 +35,8 @@
 
   - match:
       $body: |
-               /^      heap\.current          \s+  heap\.percent  \s+  heap\.max             \s+ \n
-                  (\s+ \d+(\.\d+)?[ptgmk]?b   \s+  \d+            \s+  \d+(\.\d+)?[ptgmk]?b  \s+ \n)+  $/
+               /^      heap\.current          \s+  heap\.percent  \s+  heap\.max              \n
+                  (\s+ \d+(\.\d+)?[ptgmk]?b   \s+  \d+            \s+  \d+(\.\d+)?[ptgmk]?b   \n)+  $/
 
   - do:
       cat.nodes:
@@ -46,5 +46,5 @@
   - match:
       # Windows reports -1 for the file descriptor counts.
       $body: |
-               /^      file_desc\.current  \s+  file_desc\.percent  \s+  file_desc\.max  \s+ \n
-                  (\s+ (-1|\d+)            \s+  \d+                 \s+  (-1|\d+)        \s+ \n)+  $/
+               /^      file_desc\.current  \s+  file_desc\.percent  \s+  file_desc\.max   \n
+                  (\s+ (-1|\d+)            \s+  \d+                 \s+  (-1|\d+)         \n)+  $/
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.recovery/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.recovery/10_basic.yaml
index c34437c..b081aa4 100755
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.recovery/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.recovery/10_basic.yaml
@@ -42,7 +42,7 @@
                 \d+         \s+                                 # total_bytes
                 \d+         \s+                                 # translog
                 -?\d+\.\d+% \s+                                 # translog_percent
-                -?\d+       \s+                                 # total_translog
+                -?\d+                                           # total_translog
                 \n
               )+
               $/
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.shards/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.shards/10_basic.yaml
index 5a13923..f264928 100755
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.shards/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.shards/10_basic.yaml
@@ -108,7 +108,7 @@
         h: index,state,sync_id
   - match:
       $body: |
-               /^(sync_id_test\s+STARTED\s+[A-Za-z0-9_\-]{20}\s+\n){5}$/
+               /^(sync_id_test\s+STARTED\s+[A-Za-z0-9_\-]{20}\n){5}$/
 
   - do:
       indices.delete:
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.thread_pool/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.thread_pool/10_basic.yaml
index d362c15..8d59e7c 100755
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cat.thread_pool/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/cat.thread_pool/10_basic.yaml
@@ -7,7 +7,7 @@
   - match:
       $body: |
                /  #host       ip                          bulk.active       bulk.queue       bulk.rejected       index.active       index.queue       index.rejected       search.active       search.queue       search.rejected
-               ^  (\S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d+          \s+  \d+         \s+  \d+            \s+  \d+           \s+  \d+          \s+  \d+             \s+  \d+            \s+  \d+           \s+  \d+              \s+  \n)+  $/
+               ^  (\S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d+          \s+  \d+         \s+  \d+            \s+  \d+           \s+  \d+          \s+  \d+             \s+  \d+            \s+  \d+           \s+  \d+                \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -15,8 +15,8 @@
 
   - match:
       $body: |
-               /^ host  \s+  ip                     \s+  bulk.active  \s+  bulk.queue  \s+  bulk.rejected  \s+  index.active  \s+  index.queue  \s+  index.rejected  \s+  search.active  \s+  search.queue  \s+  search.rejected  \s+  \n
-                 (\S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d+          \s+  \d+         \s+  \d+            \s+  \d+           \s+  \d+          \s+  \d+             \s+  \d+            \s+  \d+           \s+  \d+              \s+  \n)+  $/
+               /^ host  \s+  ip                     \s+  bulk.active  \s+  bulk.queue  \s+  bulk.rejected  \s+  index.active  \s+  index.queue  \s+  index.rejected  \s+  search.active  \s+  search.queue  \s+  search.rejected    \n
+                 (\S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  \d+          \s+  \d+         \s+  \d+            \s+  \d+           \s+  \d+          \s+  \d+             \s+  \d+            \s+  \d+           \s+  \d+                \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -25,7 +25,7 @@
   - match:
       $body: |
                /  #pid       id          host       ip                          port
-               ^  (\d+  \s+  \S{4}  \s+  \S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  (\d+|-) \s+ \n)+  $/
+               ^  (\d+  \s+  \S{4}  \s+  \S+   \s+  (\d{1,3}\.){3}\d{1,3}  \s+  (\d+|-)  \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -35,8 +35,8 @@
 
   - match:
       $body: |
-               /^  id   \s+  ba   \s+  fa   \s+  gea  \s+  ga   \s+  ia   \s+  maa  \s+  fma   \s+  pa  \s+  \n
-                  (\S+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+ \s+  \n)+  $/
+               /^  id   \s+  ba   \s+  fa   \s+  gea  \s+  ga   \s+  ia   \s+  maa  \s+  fma   \s+  pa    \n
+                  (\S+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+  \s+  \d+   \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -45,8 +45,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  bulk.type                \s+  bulk.active  \s+  bulk.size  \s+  bulk.queue  \s+  bulk.queueSize  \s+  bulk.rejected  \s+  bulk.largest  \s+  bulk.completed  \s+  bulk.min  \s+  bulk.max  \s+  bulk.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+          \d+  \s+        \d+  \s+         \d+  \s+             \d*  \s+            \d+  \s+           \d+  \s+             \d+  \s+       \d*  \s+       \d*  \s+             \S*  \s+  \n)+  $/
+               /^  id  \s+  bulk.type                \s+  bulk.active  \s+  bulk.size  \s+  bulk.queue  \s+  bulk.queueSize  \s+  bulk.rejected  \s+  bulk.largest  \s+  bulk.completed  \s+  bulk.min  \s+  bulk.max  \s+  bulk.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+          \d+  \s+        \d+  \s+         \d+  \s+             \d*  \s+            \d+  \s+           \d+  \s+             \d+  \s+       \d*  \s+       \d*  \s+             \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -55,8 +55,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  flush.type               \s+  flush.active  \s+  flush.size  \s+  flush.queue  \s+  flush.queueSize  \s+  flush.rejected  \s+  flush.largest  \s+  flush.completed  \s+  flush.min  \s+  flush.max  \s+  flush.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+           \d+  \s+         \d+  \s+          \d+  \s+              \d*  \s+             \d+  \s+            \d+  \s+              \d+  \s+        \d*  \s+        \d*  \s+              \S*  \s+  \n)+  $/
+               /^  id  \s+  flush.type               \s+  flush.active  \s+  flush.size  \s+  flush.queue  \s+  flush.queueSize  \s+  flush.rejected  \s+  flush.largest  \s+  flush.completed  \s+  flush.min  \s+  flush.max  \s+  flush.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+           \d+  \s+         \d+  \s+          \d+  \s+              \d*  \s+             \d+  \s+            \d+  \s+              \d+  \s+        \d*  \s+        \d*  \s+              \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -65,8 +65,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  generic.type             \s+  generic.active  \s+  generic.size  \s+  generic.queue  \s+  generic.queueSize  \s+  generic.rejected  \s+  generic.largest  \s+  generic.completed  \s+  generic.min  \s+  generic.max  \s+  generic.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+             \d+  \s+           \d+  \s+            \d+  \s+                \d*  \s+               \d+  \s+              \d+  \s+                \d+  \s+          \d*  \s+          \d*  \s+                \S*  \s+  \n)+  $/
+               /^  id  \s+  generic.type             \s+  generic.active  \s+  generic.size  \s+  generic.queue  \s+  generic.queueSize  \s+  generic.rejected  \s+  generic.largest  \s+  generic.completed  \s+  generic.min  \s+  generic.max  \s+  generic.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+             \d+  \s+           \d+  \s+            \d+  \s+                \d*  \s+               \d+  \s+              \d+  \s+                \d+  \s+          \d*  \s+          \d*  \s+                \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -75,8 +75,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  get.type                 \s+  get.active  \s+  get.size  \s+  get.queue  \s+  get.queueSize  \s+  get.rejected  \s+  get.largest  \s+  get.completed  \s+  get.min  \s+  get.max  \s+  get.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+         \d+  \s+       \d+  \s+        \d+  \s+            \d*  \s+           \d+  \s+          \d+  \s+            \d+  \s+      \d*  \s+      \d*  \s+            \S*  \s+  \n)+  $/
+               /^  id  \s+  get.type                 \s+  get.active  \s+  get.size  \s+  get.queue  \s+  get.queueSize  \s+  get.rejected  \s+  get.largest  \s+  get.completed  \s+  get.min  \s+  get.max  \s+  get.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+         \d+  \s+       \d+  \s+        \d+  \s+            \d*  \s+           \d+  \s+          \d+  \s+            \d+  \s+      \d*  \s+      \d*  \s+            \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -85,8 +85,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  index.type               \s+  index.active  \s+  index.size  \s+  index.queue  \s+  index.queueSize  \s+  index.rejected  \s+  index.largest  \s+  index.completed  \s+  index.min  \s+  index.max  \s+  index.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+           \d+  \s+         \d+  \s+          \d+  \s+              \d*  \s+             \d+  \s+            \d+  \s+              \d+  \s+        \d*  \s+        \d*  \s+              \S*  \s+  \n)+  $/
+               /^  id  \s+  index.type               \s+  index.active  \s+  index.size  \s+  index.queue  \s+  index.queueSize  \s+  index.rejected  \s+  index.largest  \s+  index.completed  \s+  index.min  \s+  index.max  \s+  index.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+           \d+  \s+         \d+  \s+          \d+  \s+              \d*  \s+             \d+  \s+            \d+  \s+              \d+  \s+        \d*  \s+        \d*  \s+              \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -95,8 +95,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  management.type          \s+  management.active  \s+  management.size  \s+  management.queue  \s+  management.queueSize  \s+  management.rejected  \s+  management.largest  \s+  management.completed  \s+  management.min  \s+  management.max  \s+  management.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+                \d+  \s+              \d+  \s+               \d+  \s+                   \d*  \s+                  \d+  \s+                 \d+  \s+                   \d+  \s+             \d*  \s+             \d*  \s+                   \S*  \s+  \n)+  $/
+               /^  id  \s+  management.type          \s+  management.active  \s+  management.size  \s+  management.queue  \s+  management.queueSize  \s+  management.rejected  \s+  management.largest  \s+  management.completed  \s+  management.min  \s+  management.max  \s+  management.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+                \d+  \s+              \d+  \s+               \d+  \s+                   \d*  \s+                  \d+  \s+                 \d+  \s+                   \d+  \s+             \d*  \s+             \d*  \s+                   \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -105,8 +105,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  force_merge.type            \s+  force_merge.active  \s+  force_merge.size  \s+  force_merge.queue  \s+  force_merge.queueSize  \s+  force_merge.rejected  \s+  force_merge.largest  \s+  force_merge.completed  \s+  force_merge.min  \s+  force_merge.max  \s+  force_merge.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+              \d+  \s+            \d+  \s+             \d+  \s+                 \d*  \s+                \d+  \s+               \d+  \s+                 \d+  \s+           \d*  \s+           \d*  \s+                 \S*  \s+  \n)+  $/
+               /^  id  \s+  force_merge.type            \s+  force_merge.active  \s+  force_merge.size  \s+  force_merge.queue  \s+  force_merge.queueSize  \s+  force_merge.rejected  \s+  force_merge.largest  \s+  force_merge.completed  \s+  force_merge.min  \s+  force_merge.max  \s+  force_merge.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+              \d+  \s+            \d+  \s+             \d+  \s+                 \d*  \s+                \d+  \s+               \d+  \s+                 \d+  \s+           \d*  \s+           \d*  \s+                 \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -115,8 +115,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  percolate.type           \s+  percolate.active  \s+  percolate.size  \s+  percolate.queue  \s+  percolate.queueSize  \s+  percolate.rejected  \s+  percolate.largest  \s+  percolate.completed  \s+  percolate.min  \s+  percolate.max  \s+  percolate.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+               \d+  \s+             \d+  \s+              \d+  \s+                  \d*  \s+                 \d+  \s+                \d+  \s+                  \d+  \s+            \d*  \s+            \d*  \s+                  \S*  \s+  \n)+  $/
+               /^  id  \s+  percolate.type           \s+  percolate.active  \s+  percolate.size  \s+  percolate.queue  \s+  percolate.queueSize  \s+  percolate.rejected  \s+  percolate.largest  \s+  percolate.completed  \s+  percolate.min  \s+  percolate.max  \s+  percolate.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+               \d+  \s+             \d+  \s+              \d+  \s+                  \d*  \s+                 \d+  \s+                \d+  \s+                  \d+  \s+            \d*  \s+            \d*  \s+                  \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -125,8 +125,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  refresh.type             \s+  refresh.active  \s+  refresh.size  \s+  refresh.queue  \s+  refresh.queueSize  \s+  refresh.rejected  \s+  refresh.largest  \s+  refresh.completed  \s+  refresh.min  \s+  refresh.max  \s+  refresh.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+             \d+  \s+           \d+  \s+            \d+  \s+                \d*  \s+               \d+  \s+              \d+  \s+                \d+  \s+          \d*  \s+          \d*  \s+                \S*  \s+  \n)+  $/
+               /^  id  \s+  refresh.type             \s+  refresh.active  \s+  refresh.size  \s+  refresh.queue  \s+  refresh.queueSize  \s+  refresh.rejected  \s+  refresh.largest  \s+  refresh.completed  \s+  refresh.min  \s+  refresh.max  \s+  refresh.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+             \d+  \s+           \d+  \s+            \d+  \s+                \d*  \s+               \d+  \s+              \d+  \s+                \d+  \s+          \d*  \s+          \d*  \s+                \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -135,8 +135,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  search.type              \s+  search.active  \s+  search.size  \s+  search.queue  \s+  search.queueSize  \s+  search.rejected  \s+  search.largest  \s+  search.completed  \s+  search.min  \s+  search.max  \s+  search.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+            \d+  \s+          \d+  \s+           \d+  \s+               \d*  \s+              \d+  \s+             \d+  \s+               \d+  \s+         \d*  \s+         \d*  \s+               \S*  \s+  \n)+  $/
+               /^  id  \s+  search.type              \s+  search.active  \s+  search.size  \s+  search.queue  \s+  search.queueSize  \s+  search.rejected  \s+  search.largest  \s+  search.completed  \s+  search.min  \s+  search.max  \s+  search.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+            \d+  \s+          \d+  \s+           \d+  \s+               \d*  \s+              \d+  \s+             \d+  \s+               \d+  \s+         \d*  \s+         \d*  \s+               \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -145,8 +145,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  snapshot.type            \s+  snapshot.active  \s+  snapshot.size  \s+  snapshot.queue  \s+  snapshot.queueSize  \s+  snapshot.rejected  \s+  snapshot.largest  \s+  snapshot.completed  \s+  snapshot.min  \s+  snapshot.max  \s+  snapshot.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+              \d+  \s+            \d+  \s+             \d+  \s+                 \d*  \s+                \d+  \s+               \d+  \s+                 \d+  \s+           \d*  \s+           \d*  \s+                 \S*  \s+  \n)+  $/
+               /^  id  \s+  snapshot.type            \s+  snapshot.active  \s+  snapshot.size  \s+  snapshot.queue  \s+  snapshot.queueSize  \s+  snapshot.rejected  \s+  snapshot.largest  \s+  snapshot.completed  \s+  snapshot.min  \s+  snapshot.max  \s+  snapshot.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+              \d+  \s+            \d+  \s+             \d+  \s+                 \d*  \s+                \d+  \s+               \d+  \s+                 \d+  \s+           \d*  \s+           \d*  \s+                 \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -155,8 +155,8 @@
 
   - match:
       $body: |
-               /^  id  \s+  suggest.type             \s+  suggest.active  \s+  suggest.size  \s+  suggest.queue  \s+  suggest.queueSize  \s+  suggest.rejected  \s+  suggest.largest  \s+  suggest.completed  \s+  suggest.min  \s+  suggest.max  \s+  suggest.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+             \d+  \s+           \d+  \s+            \d+  \s+                \d*  \s+               \d+  \s+              \d+  \s+                \d+  \s+          \d*  \s+          \d*  \s+                \S*  \s+  \n)+  $/
+               /^  id  \s+  suggest.type             \s+  suggest.active  \s+  suggest.size  \s+  suggest.queue  \s+  suggest.queueSize  \s+  suggest.rejected  \s+  suggest.largest  \s+  suggest.completed  \s+  suggest.min  \s+  suggest.max  \s+  suggest.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+             \d+  \s+           \d+  \s+            \d+  \s+                \d*  \s+               \d+  \s+              \d+  \s+                \d+  \s+          \d*  \s+          \d*  \s+                \S*    \n)+  $/
 
   - do:
       cat.thread_pool:
@@ -165,5 +165,5 @@
 
   - match:
       $body: |
-               /^  id  \s+  warmer.type              \s+  warmer.active  \s+  warmer.size  \s+  warmer.queue  \s+  warmer.queueSize  \s+  warmer.rejected  \s+  warmer.largest  \s+  warmer.completed  \s+  warmer.min  \s+  warmer.max  \s+  warmer.keepAlive  \s+  \n
-                  (\S+ \s+  (cached|fixed|scaling)?  \s+            \d+  \s+          \d+  \s+           \d+  \s+               \d*  \s+              \d+  \s+             \d+  \s+               \d+  \s+         \d*  \s+         \d*  \s+               \S*  \s+  \n)+  $/
+               /^  id  \s+  warmer.type              \s+  warmer.active  \s+  warmer.size  \s+  warmer.queue  \s+  warmer.queueSize  \s+  warmer.rejected  \s+  warmer.largest  \s+  warmer.completed  \s+  warmer.min  \s+  warmer.max  \s+  warmer.keepAlive    \n
+                  (\S+ \s+  (cached|fixed|scaling)?  \s+            \d+  \s+          \d+  \s+           \d+  \s+               \d*  \s+              \d+  \s+             \d+  \s+               \d+  \s+         \d*  \s+         \d*  \s+               \S*    \n)+  $/
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/cluster.put_settings/11_reset.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/cluster.put_settings/11_reset.yaml
deleted file mode 100644
index 4162296..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/cluster.put_settings/11_reset.yaml
+++ /dev/null
@@ -1,31 +0,0 @@
----
-"Test reset cluster settings":
-  - do:
-      cluster.put_settings:
-        body:
-          persistent:
-            cluster.routing.allocation.disk.threshold_enabled: false
-        flat_settings: true
-
-  - match: {persistent: {cluster.routing.allocation.disk.threshold_enabled: "false"}}
-
-  - do:
-      cluster.get_settings:
-        flat_settings: true
-
-  - match: {persistent: {cluster.routing.allocation.disk.threshold_enabled: "false"}}
-
-  - do:
-      cluster.put_settings:
-        body:
-          persistent:
-            cluster.routing.allocation.disk.threshold_enabled: null
-        flat_settings: true
-
-  - match: {persistent: {}}
-
-  - do:
-      cluster.get_settings:
-        flat_settings: true
-
-  - match: {persistent: {}}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml
index 4942067..0b1a090 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.analyze/10_analyze.yaml
@@ -71,3 +71,31 @@ setup:
     - length: {tokens: 2 }
     - match:     { tokens.0.token: foo bar }
     - match:     { tokens.1.token: baz }
+---
+"Detail response with Analyzer":
+    - do:
+        indices.analyze:
+          body: {"text": "This is troubled", "analyzer": standard, "explain": true}
+    - length: { detail.analyzer.tokens: 3 }
+    - match:     { detail.analyzer.name: standard }
+    - match:     { detail.analyzer.tokens.0.token: this }
+    - match:     { detail.analyzer.tokens.1.token: is }
+    - match:     { detail.analyzer.tokens.2.token: troubled }
+---
+"Detail output spcified attribute":
+    - do:
+        indices.analyze:
+          body: {"text": "<text>This is troubled</text>", "char_filters": ["html_strip"], "filters": ["snowball"], "tokenizer": standard, "explain": true, "attributes": ["keyword"]}
+    - length: { detail.charfilters: 1 }
+    - length: { detail.tokenizer.tokens: 3 }
+    - length: { detail.tokenfilters.0.tokens: 3 }
+    - match:     { detail.tokenizer.name: standard }
+    - match:     { detail.tokenizer.tokens.0.token: This }
+    - match:     { detail.tokenizer.tokens.1.token: is }
+    - match:     { detail.tokenizer.tokens.2.token: troubled }
+    - match:     { detail.tokenfilters.0.name: snowball }
+    - match:     { detail.tokenfilters.0.tokens.0.token: This }
+    - match:     { detail.tokenfilters.0.tokens.1.token: is }
+    - match:     { detail.tokenfilters.0.tokens.2.token: troubl }
+    - match:     { detail.tokenfilters.0.tokens.2.keyword: false }
+
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.update_aliases/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.update_aliases/10_basic.yaml
index 5b45f74..041f6bb 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.update_aliases/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.update_aliases/10_basic.yaml
@@ -32,3 +32,50 @@
         name: test_alias
 
   - match: {test_index.aliases.test_alias: {'index_routing': 'routing_value', 'search_routing': 'routing_value'}}
+
+---
+"Basic test for multiple aliases":
+
+  - do:
+      indices.create:
+        index: test_index
+
+  - do:
+      indices.exists_alias:
+        name: test_alias1
+
+  - is_false: ''
+
+  - do:
+      indices.exists_alias:
+        name: test_alias2
+
+  - is_false: ''
+
+  - do:
+      indices.update_aliases:
+        body:
+          actions:
+            - add:
+                indices: [test_index]
+                aliases: [test_alias1, test_alias2]
+                routing: routing_value
+
+  - do:
+      indices.exists_alias:
+        name: test_alias1
+
+  - is_true: ''
+
+  - do:
+      indices.exists_alias:
+        name: test_alias2
+
+  - is_true: ''
+
+  - do:
+      indices.get_alias:
+        index: test_index
+
+  - match: {test_index.aliases.test_alias1: {'index_routing': 'routing_value', 'search_routing': 'routing_value'}}
+  - match: {test_index.aliases.test_alias2: {'index_routing': 'routing_value', 'search_routing': 'routing_value'}}
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/msearch/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/msearch/10_basic.yaml
deleted file mode 100644
index 49e34fb..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/msearch/10_basic.yaml
+++ /dev/null
@@ -1,53 +0,0 @@
----
-"Basic multi-search":
-  - do:
-      index:
-          index:  test_1
-          type:   test
-          id:     1
-          body:   { foo: bar }
-
-  - do:
-      index:
-          index:  test_1
-          type:   test
-          id:     2
-          body:   { foo: baz }
-
-  - do:
-      index:
-          index:  test_1
-          type:   test
-          id:     3
-          body:   { foo: foo }
-
-  - do:
-      indices.refresh: {}
-
-  - do:
-      msearch:
-        body:
-          - index: test_1
-          - query:
-              match_all: {}
-          - index: test_2
-          - query:
-              match_all: {}
-          - search_type: query_then_fetch
-            index: test_1
-          - query:
-              match: {foo: bar}
-
-  - match:  { responses.0.hits.total:     3  }
-  - match:  { responses.1.error.root_cause.0.type: index_not_found_exception }
-  - match:  { responses.1.error.root_cause.0.reason: "/no.such.index/" }
-  - match:  { responses.1.error.root_cause.0.index: test_2 }
-  - match:  { responses.2.hits.total:     1  }
-
-  - do:
-      msearch:
-        body: 
-          - index: test_1
-          - query:
-              { "template": { "query": { "term": { "foo": { "value": "{{template}}" } } }, "params": { "template": "bar" } } }
-  - match: { responses.0.hits.total: 1 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/search/30_template_query_execution.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/search/30_template_query_execution.yaml
deleted file mode 100644
index d2474b7..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/search/30_template_query_execution.yaml
+++ /dev/null
@@ -1,48 +0,0 @@
----
-"Template query":
-
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     1
-        body:   { "text": "value1" }
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     2
-        body:   { "text": "value2 value3" }
-  - do:
-      indices.refresh: {}
-
-  - do:
-      search:
-        body: { "query": { "template": { "query": { "term": { "text": { "value": "{{template}}" } } }, "params": { "template": "value1" } } } }
-
-  - match: { hits.total: 1 }
-
-  - do:
-      search: 
-        body: { "query": { "template": { "query": {"match_{{template}}": {}}, "params" : { "template" : "all" } } } }
-
-  - match: { hits.total: 2 }
-
-  - do:
-      search:
-        body: { "query": { "template": { "query": "{ \"term\": { \"text\": { \"value\": \"{{template}}\" } } }", "params": { "template": "value1" } } } }
-
-  - match: { hits.total: 1 }
-
-  - do:
-      search:
-        body: { "query": { "template": { "query": "{\"match_{{template}}\": {}}", "params" : { "template" : "all" } } } }
-
-  - match: { hits.total: 2 }
-
-  - do:
-      search:
-        body: { "query": { "template": { "query": "{\"query_string\": { \"query\" : \"{{query}}\" }}", "params" : { "query" : "text:\"value2 value3\"" } } } }
-
-
-  - match: { hits.total: 1 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/search/40_search_request_template.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/search/40_search_request_template.yaml
deleted file mode 100644
index a0a6695..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/search/40_search_request_template.yaml
+++ /dev/null
@@ -1,29 +0,0 @@
----
-"Template search request":
-
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     1
-        body:   { "text": "value1" }
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     2
-        body:   { "text": "value2" }
-  - do:
-      indices.refresh: {}
-
-  - do:
-      search_template:
-        body: { "template" : { "query": { "term": { "text": { "value": "{{template}}" } } } }, "params": { "template": "value1" } }
-
-  - match: { hits.total: 1 }
-
-  - do:
-      search_template:
-        body: { "template" : { "query": { "match_{{template}}": {} } }, "params" : { "template" : "all" } }
-
-  - match: { hits.total: 2 }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/10_basic.yaml
deleted file mode 100644
index bd1fd43..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/10_basic.yaml
+++ /dev/null
@@ -1,56 +0,0 @@
----
-"Indexed template":
-
-  - do:
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match_all": {}}, "size": "{{my_size}}" } }
-  - match: { _id: "1" }
-
-  - do:
-      get_template:
-        id: 1
-  - match: { found: true }
-  - match: { lang: mustache }
-  - match: { _id: "1" }
-  - match: { _version: 1 }
-  - match: { template: /.*query\S\S\S\Smatch_all.*/ }
-
-  - do:
-      catch: missing
-      get_template:
-        id: 2
-  - match: { found: false }
-  - match: { lang: mustache }
-  - match: { _id: "2" }
-  - is_false: _version
-  - is_false: template
-
-  - do:
-      delete_template:
-        id: "1"
-  - match: { found: true }
-  - match: { _index: ".scripts" }
-  - match: { _id: "1" }
-  - match: { _version: 2}
-
-  - do:
-      catch: missing
-      delete_template:
-        id: "non_existing"
-  - match: { found: false }
-  - match: { _index: ".scripts" }
-  - match: { _id: "non_existing" }
-  - match: { _version: 1 }
-
-  - do:
-      catch: request
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match{{}}_all": {}}, "size": "{{my_size}}" } }
-
-  - do:
-      catch: /Unable\sto\sparse.*/
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match{{}}_all": {}}, "size": "{{my_size}}" } }
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
deleted file mode 100644
index 4da748a..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
+++ /dev/null
@@ -1,38 +0,0 @@
----
-"Indexed Template query tests":
-
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     1
-        body:   { "text": "value1_foo" }
-  - do:
-      index:
-        index:  test
-        type:   testtype
-        id:     2
-        body:   { "text": "value2_foo value3_foo" }
-  - do:
-      indices.refresh: {}
-
-  - do:
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match" : { "text": "{{my_value}}" } }, "size": "{{my_size}}" } }
-  - match: { _id: "1" }
-
-  - do:
-      indices.refresh: {}
-
-
-  - do:
-      search_template:
-        body: {  "id" : "1", "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
-  - match: { hits.total: 1 }
-
-  - do:
-      catch: /Unable.to.find.on.disk.file.script.\[simple1\].using.lang.\[mustache\]/
-      search_template:
-        body: { "file" : "simple1"}
-
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/30_render_search_template.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/30_render_search_template.yaml
deleted file mode 100644
index 5d5c3d5..0000000
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/30_render_search_template.yaml
+++ /dev/null
@@ -1,110 +0,0 @@
----
-"Indexed Template validate tests":
-
-  - do:
-      put_template:
-        id: "1"
-        body: { "template": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } } }
-  - match: { _id: "1" }
-
-  - do:
-      indices.refresh: {}
-
-  - do:
-      render_search_template:
-        body: { "id": "1", "params": { "my_value": "foo", "my_field": "field1" } }
-
-  - match: { template_output.query.match.text: "foo" }
-  - match: { template_output.aggs.my_terms.terms.field: "field1" }
-
-  - do:
-      render_search_template:
-        body: { "id": "1", "params": { "my_value": "bar", "my_field": "my_other_field" } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.aggs.my_terms.terms.field: "my_other_field" }
-
-  - do:
-      render_search_template:
-        id: "1"
-        body: { "params": { "my_value": "bar", "my_field": "field1" } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.aggs.my_terms.terms.field: "field1" }
-
----
-"Inline Template validate tests":
-
-  - do:
-      render_search_template:
-        body: { "inline": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "foo", "my_field": "field1" } }
-
-  - match: { template_output.query.match.text: "foo" }
-  - match: { template_output.aggs.my_terms.terms.field: "field1" }
-
-  - do:
-      render_search_template:
-        body: { "inline": { "query": { "match": { "text": "{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "bar", "my_field": "my_other_field" } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.aggs.my_terms.terms.field: "my_other_field" }
-
-  - do:
-      catch: /Improperly.closed.variable.in.query-template/
-      render_search_template:
-        body: { "inline": { "query": { "match": { "text": "{{{my_value}}" } }, "aggs": { "my_terms": { "terms": { "field": "{{my_field}}" } } } }, "params": { "my_value": "bar", "my_field": "field1" } }
----
-"Escaped Indexed Template validate tests":
-
-  - do:
-      put_template:
-        id: "1"
-        body: { "template": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }" }
-  - match: { _id: "1" }
-
-  - do:
-      indices.refresh: {}
-
-  - do:
-      render_search_template:
-        body: { "id": "1", "params": { "my_value": "foo", "my_size": 20 } }
-
-  - match: { template_output.query.match.text: "foo" }
-  - match: { template_output.size: 20 }
-
-  - do:
-      render_search_template:
-        body: { "id": "1", "params": { "my_value": "bar", "my_size": 100 } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.size: 100 }
-
-  - do:
-      render_search_template:
-        id: "1"
-        body: { "params": { "my_value": "bar", "my_size": 100 } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.size: 100 }
-
----
-"Escaped Inline Template validate tests":
-
-  - do:
-      render_search_template:
-        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "foo", "my_size": 20 } }
-
-  - match: { template_output.query.match.text: "foo" }
-  - match: { template_output.size: 20 }
-
-  - do:
-      render_search_template:
-        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "bar", "my_size": 100 } }
-
-  - match: { template_output.query.match.text: "bar" }
-  - match: { template_output.size: 100 }
-
-  - do:
-      catch: /Improperly.closed.variable.in.query-template/
-      render_search_template:
-        body: { "inline": "{ \"query\": { \"match\": { \"text\": \"{{{my_value}}\" } }, \"size\": {{my_size}} }", "params": { "my_value": "bar", "my_size": 100 } }
diff --git a/settings.gradle b/settings.gradle
index 0791c3d..e928e53 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -11,6 +11,7 @@ List projects = [
   'test-framework',
   'modules:lang-expression',
   'modules:lang-groovy',
+  'modules:lang-mustache',
   'plugins:analysis-icu',
   'plugins:analysis-kuromoji',
   'plugins:analysis-phonetic',
@@ -22,6 +23,7 @@ List projects = [
   'plugins:discovery-gce',
   'plugins:discovery-multicast',
   'plugins:lang-javascript',
+  'plugins:lang-plan-a',
   'plugins:lang-python',
   'plugins:mapper-attachments',
   'plugins:mapper-murmur3',
diff --git a/test-framework/src/main/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java b/test-framework/src/main/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
index 3e9b0c0..6ac2101 100644
--- a/test-framework/src/main/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
+++ b/test-framework/src/main/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
@@ -29,11 +29,10 @@ import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.monitor.fs.FsInfo;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.threadpool.ThreadPool;
 
@@ -78,11 +77,11 @@ public class MockInternalClusterInfoService extends InternalClusterInfoService {
     }
 
     @Inject
-    public MockInternalClusterInfoService(Settings settings, ClusterSettings clusterSettings,
+    public MockInternalClusterInfoService(Settings settings, NodeSettingsService nodeSettingsService,
                                           TransportNodesStatsAction transportNodesStatsAction,
                                           TransportIndicesStatsAction transportIndicesStatsAction,
                                           ClusterService clusterService, ThreadPool threadPool) {
-        super(settings, clusterSettings, transportNodesStatsAction, transportIndicesStatsAction, clusterService, threadPool);
+        super(settings, nodeSettingsService, transportNodesStatsAction, transportIndicesStatsAction, clusterService, threadPool);
         this.clusterName = ClusterName.clusterNameFromSettings(settings);
         stats[0] = makeStats("node_t1", new DiskUsage("node_t1", "n1", "/dev/null", 100, 100));
         stats[1] = makeStats("node_t2", new DiskUsage("node_t2", "n2", "/dev/null", 100, 100));
@@ -134,9 +133,4 @@ public class MockInternalClusterInfoService extends InternalClusterInfoService {
             return "/dev/null";
         }
     }
-
-    @Override
-    public void setUpdateFrequency(TimeValue updateFrequency) {
-        super.setUpdateFrequency(updateFrequency);
-    }
 }
diff --git a/test-framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java b/test-framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java
new file mode 100644
index 0000000..8b529f9
--- /dev/null
+++ b/test-framework/src/main/java/org/elasticsearch/index/MapperTestUtils.java
@@ -0,0 +1,63 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index;
+
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.env.Environment;
+import org.elasticsearch.index.analysis.AnalysisRegistry;
+import org.elasticsearch.index.analysis.AnalysisService;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.similarity.SimilarityService;
+import org.elasticsearch.indices.IndicesModule;
+import org.elasticsearch.indices.mapper.MapperRegistry;
+import org.elasticsearch.test.IndexSettingsModule;
+
+import java.io.IOException;
+import java.nio.file.Path;
+import java.util.Collections;
+
+
+public class MapperTestUtils {
+
+    public static MapperService newMapperService(Path tempDir, Settings indexSettings) throws IOException {
+        IndicesModule indicesModule = new IndicesModule();
+        return newMapperService(tempDir, indexSettings, indicesModule);
+    }
+
+    public static MapperService newMapperService(Path tempDir, Settings settings, IndicesModule indicesModule) throws IOException {
+        Settings.Builder settingsBuilder = Settings.builder()
+            .put("path.home", tempDir)
+            .put(settings);
+        if (settings.get(IndexMetaData.SETTING_VERSION_CREATED) == null) {
+            settingsBuilder.put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT);
+        }
+        Settings finalSettings = settingsBuilder.build();
+        MapperRegistry mapperRegistry = indicesModule.getMapperRegistry();
+        IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(new Index("test"), finalSettings);
+        AnalysisService analysisService = new AnalysisRegistry(null, new Environment(finalSettings)).build(indexSettings);
+        SimilarityService similarityService = new SimilarityService(indexSettings, Collections.emptyMap());
+        return new MapperService(indexSettings,
+            analysisService,
+            similarityService,
+            mapperRegistry);
+    }
+}
diff --git a/test-framework/src/main/java/org/elasticsearch/node/MockNode.java b/test-framework/src/main/java/org/elasticsearch/node/MockNode.java
index c5592fe..57dcc08 100644
--- a/test-framework/src/main/java/org/elasticsearch/node/MockNode.java
+++ b/test-framework/src/main/java/org/elasticsearch/node/MockNode.java
@@ -21,6 +21,7 @@ package org.elasticsearch.node;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.plugins.Plugin;
 
 import java.util.Collection;
@@ -39,7 +40,7 @@ public class MockNode extends Node {
     private Collection<Class<? extends Plugin>> plugins;
 
     public MockNode(Settings settings, Version version, Collection<Class<? extends Plugin>> classpathPlugins) {
-        super(settings, version, classpathPlugins);
+        super(InternalSettingsPreparer.prepareEnvironment(settings, null), version, classpathPlugins);
         this.version = version;
         this.plugins = classpathPlugins;
     }
diff --git a/test-framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java b/test-framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java
new file mode 100644
index 0000000..bfd4090
--- /dev/null
+++ b/test-framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java
@@ -0,0 +1,120 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.script;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.search.lookup.SearchLookup;
+
+import java.io.IOException;
+import java.util.Map;
+
+/**
+ * A dummy script engine used for testing. Scripts must be a number. Running the script
+ */
+public class MockScriptEngine implements ScriptEngineService {
+    public static final String NAME = "mockscript";
+
+    public static class TestPlugin extends Plugin {
+
+        public TestPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for integration tests";
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(MockScriptEngine.class);
+        }
+
+    }
+
+    @Override
+    public String[] types() {
+        return new String[]{ NAME };
+    }
+
+    @Override
+    public String[] extensions() {
+        return types();
+    }
+
+    @Override
+    public boolean sandboxed() {
+        return true;
+    }
+
+    @Override
+    public Object compile(String script) {
+        return script;
+    }
+
+    @Override
+    public ExecutableScript executable(CompiledScript compiledScript, @Nullable Map<String, Object> vars) {
+        return new AbstractExecutableScript() {
+            @Override
+            public Object run() {
+                return new BytesArray((String)compiledScript.compiled());
+            }
+        };
+    }
+
+    @Override
+    public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, @Nullable Map<String, Object> vars) {
+        return new SearchScript() {
+            @Override
+            public LeafSearchScript getLeafSearchScript(LeafReaderContext context) throws IOException {
+                AbstractSearchScript leafSearchScript = new AbstractSearchScript() {
+
+                    @Override
+                    public Object run() {
+                        return compiledScript.compiled();
+                    }
+
+                };
+                leafSearchScript.setLookup(lookup.getLeafSearchLookup(context));
+                return leafSearchScript;
+            }
+
+            @Override
+            public boolean needsScores() {
+                return false;
+            }
+        };
+    }
+
+    @Override
+    public void scriptRemoved(@Nullable CompiledScript script) {
+    }
+
+    @Override
+    public void close() throws IOException {
+    }
+}
diff --git a/test-framework/src/main/java/org/elasticsearch/search/MockSearchService.java b/test-framework/src/main/java/org/elasticsearch/search/MockSearchService.java
index 98b5181..9a7a3ef 100644
--- a/test-framework/src/main/java/org/elasticsearch/search/MockSearchService.java
+++ b/test-framework/src/main/java/org/elasticsearch/search/MockSearchService.java
@@ -22,12 +22,12 @@ package org.elasticsearch.search;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.ClusterSettings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.IndicesWarmer;
 import org.elasticsearch.indices.cache.request.IndicesRequestCache;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.dfs.DfsPhase;
@@ -67,13 +67,13 @@ public class MockSearchService extends SearchService {
     }
 
     @Inject
-    public MockSearchService(Settings settings, ClusterSettings clusterSettings, ClusterService clusterService, IndicesService indicesService, IndicesWarmer indicesWarmer,
-                             ThreadPool threadPool, ScriptService scriptService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays,
-                             DfsPhase dfsPhase, QueryPhase queryPhase, FetchPhase fetchPhase, IndicesRequestCache indicesQueryCache) {
-        super(settings, clusterSettings, clusterService, indicesService, indicesWarmer, threadPool, scriptService, pageCacheRecycler, bigArrays, dfsPhase,
+    public MockSearchService(Settings settings, NodeSettingsService nodeSettingsService, ClusterService clusterService, IndicesService indicesService, IndicesWarmer indicesWarmer,
+            ThreadPool threadPool, ScriptService scriptService, PageCacheRecycler pageCacheRecycler, BigArrays bigArrays,
+            DfsPhase dfsPhase, QueryPhase queryPhase, FetchPhase fetchPhase, IndicesRequestCache indicesQueryCache) {
+        super(settings, nodeSettingsService, clusterService, indicesService, indicesWarmer, threadPool, scriptService, pageCacheRecycler, bigArrays, dfsPhase,
                 queryPhase, fetchPhase, indicesQueryCache);
     }
-
+ 
     @Override
     protected void putContext(SearchContext context) {
         super.putContext(context);
diff --git a/test-framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java b/test-framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java
index 091ff23..e82823a 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/ESAllocationTestCase.java
@@ -36,7 +36,7 @@ import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;
 import org.elasticsearch.cluster.routing.allocation.decider.Decision;
-import org.elasticsearch.common.settings.ClusterSettings;
+import org.elasticsearch.common.Randomness;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.common.transport.TransportAddress;
@@ -44,6 +44,7 @@ import org.elasticsearch.gateway.AsyncShardFetch;
 import org.elasticsearch.gateway.GatewayAllocator;
 import org.elasticsearch.gateway.ReplicaShardAllocator;
 import org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.gateway.NoopGatewayAllocator;
 
 import java.lang.reflect.Constructor;
@@ -67,37 +68,37 @@ public abstract class ESAllocationTestCase extends ESTestCase {
     }
 
     public static MockAllocationService createAllocationService(Settings settings, Random random) {
-        return createAllocationService(settings, new ClusterSettings(Settings.Builder.EMPTY_SETTINGS, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), random);
+        return createAllocationService(settings, new NodeSettingsService(Settings.Builder.EMPTY_SETTINGS), random);
     }
 
-    public static MockAllocationService createAllocationService(Settings settings, ClusterSettings clusterSettings, Random random) {
+    public static MockAllocationService createAllocationService(Settings settings, NodeSettingsService nodeSettingsService, Random random) {
         return new MockAllocationService(settings,
-                randomAllocationDeciders(settings, clusterSettings, random),
+                randomAllocationDeciders(settings, nodeSettingsService, random),
                 new ShardsAllocators(settings, NoopGatewayAllocator.INSTANCE), EmptyClusterInfoService.INSTANCE);
     }
 
     public static MockAllocationService createAllocationService(Settings settings, ClusterInfoService clusterInfoService) {
         return new MockAllocationService(settings,
-                randomAllocationDeciders(settings, new ClusterSettings(Settings.Builder.EMPTY_SETTINGS, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), getRandom()),
+                randomAllocationDeciders(settings, new NodeSettingsService(Settings.Builder.EMPTY_SETTINGS), getRandom()),
                 new ShardsAllocators(settings, NoopGatewayAllocator.INSTANCE), clusterInfoService);
     }
 
     public static MockAllocationService createAllocationService(Settings settings, GatewayAllocator allocator) {
         return new MockAllocationService(settings,
-                randomAllocationDeciders(settings, new ClusterSettings(Settings.Builder.EMPTY_SETTINGS, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS), getRandom()),
+                randomAllocationDeciders(settings, new NodeSettingsService(Settings.Builder.EMPTY_SETTINGS), getRandom()),
                 new ShardsAllocators(settings, allocator), EmptyClusterInfoService.INSTANCE);
     }
 
 
 
-    public static AllocationDeciders randomAllocationDeciders(Settings settings, ClusterSettings clusterSettings, Random random) {
+    public static AllocationDeciders randomAllocationDeciders(Settings settings, NodeSettingsService nodeSettingsService, Random random) {
         final List<Class<? extends AllocationDecider>> defaultAllocationDeciders = ClusterModule.DEFAULT_ALLOCATION_DECIDERS;
         final List<AllocationDecider> list = new ArrayList<>();
         for (Class<? extends AllocationDecider> deciderClass : ClusterModule.DEFAULT_ALLOCATION_DECIDERS) {
             try {
                 try {
-                    Constructor<? extends AllocationDecider> constructor = deciderClass.getConstructor(Settings.class, ClusterSettings.class);
-                    list.add(constructor.newInstance(settings, clusterSettings));
+                    Constructor<? extends AllocationDecider> constructor = deciderClass.getConstructor(Settings.class, NodeSettingsService.class);
+                    list.add(constructor.newInstance(settings, nodeSettingsService));
                 } catch (NoSuchMethodException e) {
                     Constructor<? extends AllocationDecider> constructor = null;
                     constructor = deciderClass.getConstructor(Settings.class);
@@ -111,7 +112,7 @@ public abstract class ESAllocationTestCase extends ESTestCase {
         for (AllocationDecider d : list) {
             assertThat(defaultAllocationDeciders.contains(d.getClass()), is(true));
         }
-        Collections.shuffle(list, random);
+        Randomness.shuffle(list);
         return new AllocationDeciders(settings, list.toArray(new AllocationDecider[0]));
 
     }
diff --git a/test-framework/src/main/java/org/elasticsearch/test/ESBackcompatTestCase.java b/test-framework/src/main/java/org/elasticsearch/test/ESBackcompatTestCase.java
index dbd2604..3e5c903 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/ESBackcompatTestCase.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/ESBackcompatTestCase.java
@@ -131,16 +131,6 @@ public abstract class ESBackcompatTestCase extends ESIntegTestCase {
         return file;
     }
 
-    @Override
-    protected Settings.Builder setRandomIndexSettings(Random random, Settings.Builder builder) {
-        if (globalCompatibilityVersion().before(Version.V_1_3_2)) {
-            // if we test against nodes before 1.3.2 we disable all the compression due to a known bug
-            // see #7210
-            builder.put(RecoverySettings.INDICES_RECOVERY_COMPRESS_SETTING.getKey(), false);
-        }
-        return builder;
-    }
-
     /**
      * Retruns the tests compatibility version.
      */
@@ -250,13 +240,6 @@ public abstract class ESBackcompatTestCase extends ESIntegTestCase {
         Settings.Builder builder = Settings.builder().put(requiredSettings());
         builder.put(TransportModule.TRANSPORT_TYPE_KEY, "netty"); // run same transport  / disco as external
         builder.put("node.mode", "network");
-
-        if (compatibilityVersion().before(Version.V_1_3_2)) {
-            // if we test against nodes before 1.3.2 we disable all the compression due to a known bug
-            // see #7210
-            builder.put(Transport.TransportSettings.TRANSPORT_TCP_COMPRESS, false)
-                    .put(RecoverySettings.INDICES_RECOVERY_COMPRESS_SETTING.getKey(), false);
-        }
         return builder.build();
     }
 
diff --git a/test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java b/test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
index 24adcf9..37a8fd3 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -28,6 +28,7 @@ import com.carrotsearch.randomizedtesting.generators.RandomPicks;
 import org.apache.http.impl.client.HttpClients;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.SuppressForbidden;
 import org.apache.lucene.util.TestUtil;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
@@ -136,6 +137,8 @@ import java.lang.annotation.RetentionPolicy;
 import java.lang.annotation.Target;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
+import java.net.MalformedURLException;
+import java.net.URL;
 import java.net.UnknownHostException;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
@@ -450,7 +453,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
                     .setOrder(0)
                     .setSettings(randomSettingsBuilder);
             if (mappings != null) {
-                logger.info("test using _default_ mappings: [{}]", mappings.bytesStream().bytes().toUtf8());
+                logger.info("test using _default_ mappings: [{}]", mappings.bytes().toUtf8());
                 putTemplate.addMapping("_default_", mappings);
             }
             assertAcked(putTemplate.execute().actionGet());
@@ -533,7 +536,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
     }
 
     private TestCluster buildWithPrivateContext(final Scope scope, final long seed) throws Exception {
-        return RandomizedContext.current().runWithPrivateRandomness(new Randomness(seed), new Callable<TestCluster>() {
+        return RandomizedContext.current().runWithPrivateRandomness(new com.carrotsearch.randomizedtesting.Randomness(seed), new Callable<TestCluster>() {
             @Override
             public TestCluster call() throws Exception {
                 return buildTestCluster(scope, seed);
@@ -1039,7 +1042,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
      */
     public void setMinimumMasterNodes(int n) {
         assertTrue(client().admin().cluster().prepareUpdateSettings().setTransientSettings(
-                settingsBuilder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), n))
+                settingsBuilder().put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, n))
                 .get().isAcknowledged());
     }
 
@@ -1388,7 +1391,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
             }
         }
         final String[] indices = indicesSet.toArray(new String[indicesSet.size()]);
-        Collections.shuffle(builders, random);
+        Collections.shuffle(builders, random());
         final CopyOnWriteArrayList<Tuple<IndexRequestBuilder, Throwable>> errors = new CopyOnWriteArrayList<>();
         List<CountDownLatch> inFlightAsyncOperations = new ArrayList<>();
         // If you are indexing just a few documents then frequently do it one at a time.  If many then frequently in bulk.
@@ -1474,7 +1477,7 @@ public abstract class ESIntegTestCase extends ESTestCase {
 
     /** Sets or unsets the cluster read_only mode **/
     public static void setClusterReadOnly(boolean value) {
-        Settings settings = settingsBuilder().put(MetaData.SETTING_READ_ONLY_SETTING.getKey(), value).build();
+        Settings settings = settingsBuilder().put(MetaData.SETTING_READ_ONLY, value).build();
         assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(settings).get());
     }
 
@@ -1687,8 +1690,8 @@ public abstract class ESIntegTestCase extends ESTestCase {
         Settings.Builder builder = settingsBuilder()
                 // Default the watermarks to absurdly low to prevent the tests
                 // from failing on nodes without enough disk space
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "1b")
-                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "1b")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "1b")
+                .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "1b")
                 .put("script.indexed", "on")
                 .put("script.inline", "on")
                         // wait short time for other active shards before actually deleting, default 30s not needed in tests
@@ -1727,20 +1730,14 @@ public abstract class ESIntegTestCase extends ESTestCase {
         return Settings.EMPTY;
     }
 
-    private ExternalTestCluster buildExternalCluster(String clusterAddresses) throws UnknownHostException {
+    private ExternalTestCluster buildExternalCluster(String clusterAddresses) throws IOException {
         String[] stringAddresses = clusterAddresses.split(",");
         TransportAddress[] transportAddresses = new TransportAddress[stringAddresses.length];
         int i = 0;
         for (String stringAddress : stringAddresses) {
-            String[] split = stringAddress.split(":");
-            if (split.length < 2) {
-                throw new IllegalArgumentException("address [" + clusterAddresses + "] not valid");
-            }
-            try {
-                transportAddresses[i++] = new InetSocketTransportAddress(InetAddress.getByName(split[0]), Integer.valueOf(split[1]));
-            } catch (NumberFormatException e) {
-                throw new IllegalArgumentException("port is not valid, expected number but was [" + split[1] + "]");
-            }
+            URL url = new URL("http://" + stringAddress);
+            InetAddress inetAddress = InetAddress.getByName(url.getHost());
+            transportAddresses[i++] = new InetSocketTransportAddress(new InetSocketAddress(inetAddress, url.getPort()));
         }
         return new ExternalTestCluster(createTempDir(), externalClusterClientSettings(), transportClientPlugins(), transportAddresses);
     }
diff --git a/test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java b/test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
index 887d5f9..287bd12 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/ESSingleNodeTestCase.java
@@ -19,12 +19,12 @@
 package org.elasticsearch.test;
 
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -38,7 +38,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.node.Node;
-import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.internal.SearchContext;
@@ -48,7 +47,9 @@ import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.lessThanOrEqualTo;
 
 /**
  * A test that keep a singleton node started for all tests that can be used to get
@@ -119,7 +120,7 @@ public abstract class ESSingleNodeTestCase extends ESTestCase {
     }
 
     private static Node newNode() {
-        Node build = NodeBuilder.nodeBuilder().local(true).data(true).settings(Settings.builder()
+        Node build = new Node(Settings.builder()
                 .put(ClusterName.SETTING, InternalTestCluster.clusterName("single-node-cluster", randomLong()))
                 .put("path.home", createTempDir())
                 // TODO: use a consistent data path for custom paths
@@ -132,8 +133,11 @@ public abstract class ESSingleNodeTestCase extends ESTestCase {
                 .put("script.indexed", "on")
                 .put(EsExecutors.PROCESSORS, 1) // limit the number of threads created
                 .put("http.enabled", false)
+                .put("node.local", true)
+                .put("node.data", true)
                 .put(InternalSettingsPreparer.IGNORE_SYSTEM_PROPERTIES_SETTING, true) // make sure we get what we set :)
-        ).build();
+                .build()
+        );
         build.start();
         assertThat(DiscoveryNode.localNode(build.settings()), is(true));
         return build;
diff --git a/test-framework/src/main/java/org/elasticsearch/test/ESTestCase.java b/test-framework/src/main/java/org/elasticsearch/test/ESTestCase.java
index c59c3ba..e144311 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/ESTestCase.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/ESTestCase.java
@@ -566,7 +566,7 @@ public abstract class ESTestCase extends LuceneTestCase {
             throw new IllegalArgumentException("Can\'t pick " + size + " random objects from a list of " + values.length + " objects");
         }
         List<T> list = arrayAsArrayList(values);
-        Collections.shuffle(list);
+        Collections.shuffle(list, random());
         return list.subList(0, size);
     }
 
@@ -615,7 +615,7 @@ public abstract class ESTestCase extends LuceneTestCase {
         sb.append("]");
         assertThat(count + " files exist that should have been cleaned:\n" + sb.toString(), count, equalTo(0));
     }
-    
+
     /** Returns the suite failure marker: internal use only! */
     public static TestRuleMarkFailure getSuiteFailureMarker() {
         return suiteFailureMarker;
diff --git a/test-framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java b/test-framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
index c8eacc4..7ae3226 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java
@@ -292,19 +292,19 @@ public final class InternalTestCluster extends TestCluster {
         }
         // Default the watermarks to absurdly low to prevent the tests
         // from failing on nodes without enough disk space
-        builder.put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), "1b");
-        builder.put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), "1b");
+        builder.put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "1b");
+        builder.put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "1b");
         if (TEST_NIGHTLY) {
-            builder.put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS_SETTING.getKey(), RandomInts.randomIntBetween(random, 10, 15));
-            builder.put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING.getKey(), RandomInts.randomIntBetween(random, 10, 15));
-            builder.put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), RandomInts.randomIntBetween(random, 5, 10));
+            builder.put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, RandomInts.randomIntBetween(random, 10, 15));
+            builder.put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, RandomInts.randomIntBetween(random, 10, 15));
+            builder.put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, RandomInts.randomIntBetween(random, 5, 10));
         } else if (random.nextInt(100) <= 90) {
-            builder.put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS_SETTING.getKey(), RandomInts.randomIntBetween(random, 3, 6));
-            builder.put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS_SETTING.getKey(), RandomInts.randomIntBetween(random, 3, 6));
-            builder.put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES_SETTING.getKey(), RandomInts.randomIntBetween(random, 2, 5));
+            builder.put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, RandomInts.randomIntBetween(random, 3, 6));
+            builder.put(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, RandomInts.randomIntBetween(random, 3, 6));
+            builder.put(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, RandomInts.randomIntBetween(random, 2, 5));
         }
         // always reduce this - it can make tests really slow
-        builder.put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC_SETTING.getKey(), TimeValue.timeValueMillis(RandomInts.randomIntBetween(random, 20, 50)));
+        builder.put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC, TimeValue.timeValueMillis(RandomInts.randomIntBetween(random, 20, 50)));
         defaultSettings = builder.build();
         executor = EsExecutors.newCached("test runner", 0, TimeUnit.SECONDS, EsExecutors.daemonThreadFactory("test_" + clusterName));
     }
@@ -412,7 +412,7 @@ public final class InternalTestCluster extends TestCluster {
         }
 
         if (random.nextBoolean()) {
-            builder.put(MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT_SETTING.getKey(), new TimeValue(RandomInts.randomIntBetween(random, 10, 30), TimeUnit.SECONDS));
+            builder.put(MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT, new TimeValue(RandomInts.randomIntBetween(random, 10, 30), TimeUnit.SECONDS));
         }
 
         if (random.nextInt(10) == 0) {
@@ -430,28 +430,24 @@ public final class InternalTestCluster extends TestCluster {
 
         if (random.nextBoolean()) {
             if (random.nextInt(10) == 0) { // do something crazy slow here
-                builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 10), ByteSizeUnit.MB));
+                builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC, new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 10), ByteSizeUnit.MB));
             } else {
-                builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 10, 200), ByteSizeUnit.MB));
+                builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC, new ByteSizeValue(RandomInts.randomIntBetween(random, 10, 200), ByteSizeUnit.MB));
             }
         }
         if (random.nextBoolean()) {
-            builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE_SETTING.getKey(), RandomPicks.randomFrom(random, StoreRateLimiting.Type.values()));
+            builder.put(IndexStoreConfig.INDICES_STORE_THROTTLE_TYPE, RandomPicks.randomFrom(random, StoreRateLimiting.Type.values()));
         }
 
         if (random.nextBoolean()) {
             if (random.nextInt(10) == 0) { // do something crazy slow here
-                builder.put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 10), ByteSizeUnit.MB));
+                builder.put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, new ByteSizeValue(RandomInts.randomIntBetween(random, 1, 10), ByteSizeUnit.MB));
             } else {
-                builder.put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC_SETTING.getKey(), new ByteSizeValue(RandomInts.randomIntBetween(random, 10, 200), ByteSizeUnit.MB));
+                builder.put(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, new ByteSizeValue(RandomInts.randomIntBetween(random, 10, 200), ByteSizeUnit.MB));
             }
         }
 
         if (random.nextBoolean()) {
-            builder.put(RecoverySettings.INDICES_RECOVERY_COMPRESS_SETTING.getKey(), random.nextBoolean());
-        }
-
-        if (random.nextBoolean()) {
             builder.put(NettyTransport.PING_SCHEDULE, RandomInts.randomIntBetween(random, 100, 2000) + "ms");
         }
 
@@ -1554,7 +1550,7 @@ public final class InternalTestCluster extends TestCluster {
         for (int i = 0; i < numNodes; i++) {
             asyncs.add(startNodeAsync(settings, version));
         }
-        
+
         return () -> {
             List<String> ids = new ArrayList<>();
             for (Async<String> async : asyncs) {
diff --git a/test-framework/src/main/java/org/elasticsearch/test/VersionUtils.java b/test-framework/src/main/java/org/elasticsearch/test/VersionUtils.java
index 30a89e4..93eef96 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/VersionUtils.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/VersionUtils.java
@@ -35,7 +35,7 @@ public class VersionUtils {
 
     private static final List<Version> SORTED_VERSIONS;
     static {
-        Field[] declaredFields = Version.class.getDeclaredFields();
+        Field[] declaredFields = Version.class.getFields();
         Set<Integer> ids = new HashSet<>();
         for (Field field : declaredFields) {
             final int mod = field.getModifiers();
diff --git a/test-framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/test-framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 7176916..9d8ad7f 100644
--- a/test-framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/test-framework/src/main/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -705,7 +705,7 @@ public class ElasticsearchAssertions {
             IllegalAccessException, InvocationTargetException {
         try {
             Class<? extends Streamable> clazz = streamable.getClass();
-            Constructor<? extends Streamable> constructor = clazz.getDeclaredConstructor();
+            Constructor<? extends Streamable> constructor = clazz.getConstructor();
             assertThat(constructor, Matchers.notNullValue());
             Streamable newInstance = constructor.newInstance();
             return newInstance;
diff --git a/test-framework/src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java b/test-framework/src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java
index 3c8913f..bb07223 100644
--- a/test-framework/src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java
+++ b/test-framework/src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java
@@ -47,7 +47,7 @@ public class LoggingListenerTests extends ESTestCase {
         assertThat(xyzLogger.getLevel(), nullValue());
         assertThat(abcLogger.getLevel(), nullValue());
 
-        Method method = TestClass.class.getDeclaredMethod("annotatedTestMethod");
+        Method method = TestClass.class.getMethod("annotatedTestMethod");
         TestLogging annotation = method.getAnnotation(TestLogging.class);
         Description testDescription = Description.createTestDescription(LoggingListenerTests.class, "annotatedTestMethod", annotation);
         loggingListener.testStarted(testDescription);
@@ -105,7 +105,7 @@ public class LoggingListenerTests extends ESTestCase {
         assertThat(abcLogger.getLevel(), equalTo("ERROR"));
         assertThat(xyzLogger.getLevel(), nullValue());
 
-        Method method = TestClass.class.getDeclaredMethod("annotatedTestMethod");
+        Method method = TestClass.class.getMethod("annotatedTestMethod");
         TestLogging annotation = method.getAnnotation(TestLogging.class);
         Description testDescription = Description.createTestDescription(LoggingListenerTests.class, "annotatedTestMethod", annotation);
         loggingListener.testStarted(testDescription);
@@ -116,7 +116,7 @@ public class LoggingListenerTests extends ESTestCase {
         assertThat(abcLogger.getLevel(), equalTo("ERROR"));
         assertThat(xyzLogger.getLevel(), nullValue());
 
-        Method method2 = TestClass.class.getDeclaredMethod("annotatedTestMethod2");
+        Method method2 = TestClass.class.getMethod("annotatedTestMethod2");
         TestLogging annotation2 = method2.getAnnotation(TestLogging.class);
         Description testDescription2 = Description.createTestDescription(LoggingListenerTests.class, "annotatedTestMethod2", annotation2);
         loggingListener.testStarted(testDescription2);
