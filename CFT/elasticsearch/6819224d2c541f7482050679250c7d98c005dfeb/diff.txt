diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 6c5923e..4491737 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -74,7 +74,7 @@ Then sit back and wait. There will probably be discussion about the pull request
 Contributing to the Elasticsearch codebase
 ------------------------------------------
 
-**Repository:** [https://github.com/elasticsearch/elasticsearch](https://github.com/elastic/elasticsearch)
+**Repository:** [https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch)
 
 Make sure you have [Maven](http://maven.apache.org) installed, as Elasticsearch uses it as its build system. Integration with IntelliJ and Eclipse should work out of the box. Eclipse users can automatically configure their IDE by running `mvn eclipse:eclipse` and then importing the project into their workspace: `File > Import > Existing project into workspace` and make sure to select `Search for nested projects...` option as Elasticsearch is a multi-module maven project. Additionally you will want to ensure that Eclipse is using 2048m of heap by modifying `eclipse.ini` accordingly to avoid GC overhead errors. Please make sure the [m2e-connector](http://marketplace.eclipse.org/content/m2e-connector-maven-dependency-plugin) is not installed in your Eclipse distribution as it will interfere with setup performed by `mvn eclipse:eclipse`.
 
diff --git a/core/.local-3.0.0-SNAPSHOT-test-execution-times.log b/core/.local-3.0.0-SNAPSHOT-test-execution-times.log
new file mode 100644
index 0000000..8b79a35
--- /dev/null
+++ b/core/.local-3.0.0-SNAPSHOT-test-execution-times.log
@@ -0,0 +1,559 @@
+org.apache.lucene.analysis.miscellaneous.TruncateTokenFilterTests=190
+org.apache.lucene.analysis.miscellaneous.UniqueTokenFilterTests=187
+org.apache.lucene.queries.BlendedTermQueryTests=696
+org.apache.lucene.queries.MinDocQueryTests=503
+org.apache.lucene.search.postingshighlight.CustomPassageFormatterTests=69
+org.apache.lucene.search.postingshighlight.CustomPostingsHighlighterTests=599
+org.apache.lucene.search.postingshighlight.CustomSeparatorBreakIteratorTests=99
+org.apache.lucene.util.SloppyMathTests=734
+org.elasticsearch.ESExceptionTests=701
+org.elasticsearch.ExceptionSerializationTests=3740
+org.elasticsearch.NamingConventionTests=1061
+org.elasticsearch.SpecialPermissionTests=90
+org.elasticsearch.VersionTests=179
+org.elasticsearch.action.OriginalIndicesTests=66
+org.elasticsearch.action.admin.cluster.health.ClusterHealthResponsesTests=120
+org.elasticsearch.action.admin.cluster.state.ClusterStateRequestTests=32
+org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilderTests=51
+org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequestTests=60
+org.elasticsearch.action.admin.indices.segments.IndicesSegmentsRequestTests=2294
+org.elasticsearch.action.admin.indices.shards.IndicesShardStoreResponseTests=61
+org.elasticsearch.action.admin.indices.stats.IndicesStatsTests=2832
+org.elasticsearch.action.admin.indices.template.put.MetaDataIndexTemplateServiceTests=72
+org.elasticsearch.action.admin.indices.warmer.put.PutWarmerRequestTests=98
+org.elasticsearch.action.bulk.BulkRequestTests=578
+org.elasticsearch.action.count.CountRequestBuilderTests=495
+org.elasticsearch.action.count.CountRequestTests=21
+org.elasticsearch.action.count.CountResponseTests=63
+org.elasticsearch.action.fieldstats.FieldStatsRequestTests=45
+org.elasticsearch.action.get.MultiGetShardRequestTests=81
+org.elasticsearch.action.index.IndexRequestBuilderTests=372
+org.elasticsearch.action.index.IndexRequestTests=78
+org.elasticsearch.action.indexedscripts.get.GetIndexedScriptRequestTests=58
+org.elasticsearch.action.percolate.MultiPercolatorRequestTests=144
+org.elasticsearch.action.search.MultiSearchRequestTests=57
+org.elasticsearch.action.search.SearchRequestBuilderTests=291
+org.elasticsearch.action.support.IndicesOptionsTests=83
+org.elasticsearch.action.support.ListenableActionFutureTests=55
+org.elasticsearch.action.support.TransportActionFilterChainTests=52
+org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeActionTests=110
+org.elasticsearch.action.support.replication.BroadcastReplicationTests=151
+org.elasticsearch.action.support.replication.ShardReplicationTests=236
+org.elasticsearch.action.termvectors.TermVectorsUnitTests=293
+org.elasticsearch.action.update.UpdateRequestTests=67
+org.elasticsearch.bootstrap.BootstrapCliParserTests=73
+org.elasticsearch.bootstrap.ESPolicyTests=55
+org.elasticsearch.bootstrap.JNANativesTests=77
+org.elasticsearch.bootstrap.JarHellTests=171
+org.elasticsearch.bootstrap.JavaVersionTests=65
+org.elasticsearch.bootstrap.SeccompTests=123
+org.elasticsearch.bootstrap.SecurityTests=238
+org.elasticsearch.client.node.NodeClientHeadersTests=355
+org.elasticsearch.client.transport.TransportClientHeadersTests=640
+org.elasticsearch.client.transport.TransportClientNodesServiceTests=3307
+org.elasticsearch.cluster.ClusterModuleTests=73
+org.elasticsearch.cluster.ClusterStateTests=17
+org.elasticsearch.cluster.DiskUsageTests=107
+org.elasticsearch.cluster.block.ClusterBlockTests=41
+org.elasticsearch.cluster.metadata.DateMathExpressionResolverTests=158
+org.elasticsearch.cluster.metadata.HumanReadableIndexSettingsTests=41
+org.elasticsearch.cluster.metadata.IndexNameExpressionResolverTests=370
+org.elasticsearch.cluster.metadata.MappingMetaDataParserTests=103
+org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeServiceTests=26
+org.elasticsearch.cluster.metadata.ToAndFromJsonMetaDataTests=122
+org.elasticsearch.cluster.metadata.WildcardExpressionResolverTests=80
+org.elasticsearch.cluster.node.DiscoveryNodeFiltersTests=62
+org.elasticsearch.cluster.routing.AllocationIdTests=79
+org.elasticsearch.cluster.routing.RoutingBackwardCompatibilityTests=3477
+org.elasticsearch.cluster.routing.RoutingServiceTests=368
+org.elasticsearch.cluster.routing.RoutingTableTests=123
+org.elasticsearch.cluster.routing.ShardRoutingTests=179
+org.elasticsearch.cluster.routing.UnassignedInfoTests=146
+org.elasticsearch.cluster.routing.allocation.AddIncrementallyTests=97
+org.elasticsearch.cluster.routing.allocation.AllocationCommandsTests=137
+org.elasticsearch.cluster.routing.allocation.AllocationPriorityTests=34
+org.elasticsearch.cluster.routing.allocation.AwarenessAllocationTests=334
+org.elasticsearch.cluster.routing.allocation.BalanceConfigurationTests=426
+org.elasticsearch.cluster.routing.allocation.BalanceUnbalancedClusterTests=9557
+org.elasticsearch.cluster.routing.allocation.ClusterRebalanceRoutingTests=908
+org.elasticsearch.cluster.routing.allocation.ConcurrentRebalanceRoutingTests=157
+org.elasticsearch.cluster.routing.allocation.DeadNodesAllocationTests=72
+org.elasticsearch.cluster.routing.allocation.ElectReplicaAsPrimaryDuringRelocationTests=50
+org.elasticsearch.cluster.routing.allocation.ExpectedShardSizeAllocationTests=127
+org.elasticsearch.cluster.routing.allocation.FailedNodeRoutingTests=48
+org.elasticsearch.cluster.routing.allocation.FailedShardsRoutingTests=151
+org.elasticsearch.cluster.routing.allocation.FilterRoutingTests=53
+org.elasticsearch.cluster.routing.allocation.IndexBalanceTests=118
+org.elasticsearch.cluster.routing.allocation.NodeVersionAllocationDeciderTests=424
+org.elasticsearch.cluster.routing.allocation.PreferLocalPrimariesToRelocatingPrimariesTests=75
+org.elasticsearch.cluster.routing.allocation.PreferPrimaryAllocationTests=97
+org.elasticsearch.cluster.routing.allocation.PrimaryElectionRoutingTests=337
+org.elasticsearch.cluster.routing.allocation.PrimaryNotRelocatedWhileBeingRecoveredTests=2581
+org.elasticsearch.cluster.routing.allocation.RandomAllocationDeciderTests=53
+org.elasticsearch.cluster.routing.allocation.RebalanceAfterActiveTests=79
+org.elasticsearch.cluster.routing.allocation.ReplicaAllocatedAfterPrimaryTests=98
+org.elasticsearch.cluster.routing.allocation.RoutingNodesIntegrityTests=47
+org.elasticsearch.cluster.routing.allocation.SameShardRoutingTests=99
+org.elasticsearch.cluster.routing.allocation.ShardVersioningTests=54
+org.elasticsearch.cluster.routing.allocation.ShardsLimitAllocationTests=69
+org.elasticsearch.cluster.routing.allocation.SingleShardNoReplicasRoutingTests=209
+org.elasticsearch.cluster.routing.allocation.SingleShardOneReplicaRoutingTests=57
+org.elasticsearch.cluster.routing.allocation.StartedShardsRoutingTests=83
+org.elasticsearch.cluster.routing.allocation.TenShardsOneReplicaRoutingTests=39
+org.elasticsearch.cluster.routing.allocation.ThrottlingAllocationTests=14
+org.elasticsearch.cluster.routing.allocation.UpdateNumberOfReplicasTests=30
+org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDeciderTests=75
+org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDeciderUnitTests=107
+org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationTests=108
+org.elasticsearch.cluster.routing.operation.hash.murmur3.Murmur3HashFunctionTests=71
+org.elasticsearch.cluster.serialization.ClusterSerializationTests=25
+org.elasticsearch.cluster.serialization.ClusterStateToStringTests=42
+org.elasticsearch.cluster.serialization.DiffableTests=95
+org.elasticsearch.cluster.settings.SettingsValidatorTests=53
+org.elasticsearch.cluster.structure.RoutingIteratorTests=1016
+org.elasticsearch.codecs.CodecTests=9816
+org.elasticsearch.common.Base64Tests=2127
+org.elasticsearch.common.BooleansTests=54
+org.elasticsearch.common.ChannelsTests=222
+org.elasticsearch.common.ParseFieldTests=92
+org.elasticsearch.common.PidFileTests=2205
+org.elasticsearch.common.StringsTests=96
+org.elasticsearch.common.TableTests=90
+org.elasticsearch.common.UUIDTests=1844
+org.elasticsearch.common.blobstore.BlobStoreTests=44
+org.elasticsearch.common.breaker.MemoryCircuitBreakerTests=187
+org.elasticsearch.common.bytes.BytesReferenceTests=42
+org.elasticsearch.common.bytes.PagedBytesReferenceTests=890
+org.elasticsearch.common.cli.CheckFileCommandTests=462
+org.elasticsearch.common.cli.CliToolTests=195
+org.elasticsearch.common.cli.TerminalTests=111
+org.elasticsearch.common.collect.CopyOnWriteHashMapTests=138
+org.elasticsearch.common.compress.deflate.DeflateCompressedStreamTests=3050
+org.elasticsearch.common.compress.deflate.DeflateXContentTests=1022
+org.elasticsearch.common.compress.lzf.CorruptedCompressorTests=47
+org.elasticsearch.common.compress.lzf.LZFCompressedStreamTests=3845
+org.elasticsearch.common.compress.lzf.LZFXContentTests=738
+org.elasticsearch.common.geo.GeoDistanceTests=183
+org.elasticsearch.common.geo.GeoHashTests=603
+org.elasticsearch.common.geo.GeoJSONShapeParserTests=271
+org.elasticsearch.common.geo.ShapeBuilderTests=649
+org.elasticsearch.common.geo.ShapeRelationTests=63
+org.elasticsearch.common.geo.SpatialStrategyTests=141
+org.elasticsearch.common.hash.MessageDigestsTests=6973
+org.elasticsearch.common.hashing.MurmurHash3Tests=55
+org.elasticsearch.common.hppc.HppcMapsTests=46
+org.elasticsearch.common.io.FileSystemUtilsTests=2105
+org.elasticsearch.common.io.StreamsTests=134
+org.elasticsearch.common.io.stream.BytesStreamsTests=552
+org.elasticsearch.common.joda.DateMathParserTests=127
+org.elasticsearch.common.logging.jdk.JDKESLoggerTests=43
+org.elasticsearch.common.logging.log4j.Log4jESLoggerTests=135
+org.elasticsearch.common.logging.log4j.LoggingConfigurationTests=2414
+org.elasticsearch.common.lucene.IndexCacheableQueryTests=124
+org.elasticsearch.common.lucene.LuceneTests=1704
+org.elasticsearch.common.lucene.ShardCoreKeyMapTests=177
+org.elasticsearch.common.lucene.all.SimpleAllTests=2588
+org.elasticsearch.common.lucene.index.ESDirectoryReaderTests=323
+org.elasticsearch.common.lucene.index.FreqTermsEnumTests=682
+org.elasticsearch.common.lucene.search.MultiPhrasePrefixQueryTests=58
+org.elasticsearch.common.lucene.search.function.ScriptScoreFunctionTests=54
+org.elasticsearch.common.lucene.search.morelikethis.MoreLikeThisQueryTests=69
+org.elasticsearch.common.lucene.search.morelikethis.XMoreLikeThisTests=262
+org.elasticsearch.common.lucene.store.ByteArrayIndexInputTests=112
+org.elasticsearch.common.lucene.store.InputStreamIndexInputTests=89
+org.elasticsearch.common.lucene.uid.VersionsTests=500
+org.elasticsearch.common.math.MathUtilsTests=133
+org.elasticsearch.common.network.NetworkAddressTests=2234
+org.elasticsearch.common.network.NetworkServiceTests=103
+org.elasticsearch.common.network.NetworkUtilsTests=37
+org.elasticsearch.common.path.PathTrieTests=67
+org.elasticsearch.common.property.PropertyPlaceholderTests=22
+org.elasticsearch.common.recycler.ConcurrentRecyclerTests=57
+org.elasticsearch.common.recycler.LockedRecyclerTests=105
+org.elasticsearch.common.recycler.NoneRecyclerTests=15
+org.elasticsearch.common.recycler.QueueRecyclerTests=76
+org.elasticsearch.common.regex.RegexTests=103
+org.elasticsearch.common.rounding.RoundingTests=59
+org.elasticsearch.common.rounding.TimeZoneRoundingTests=180
+org.elasticsearch.common.settings.SettingsFilterTests=9
+org.elasticsearch.common.settings.SettingsTests=137
+org.elasticsearch.common.settings.loader.JsonSettingsLoaderTests=29
+org.elasticsearch.common.settings.loader.PropertiesSettingsLoaderTests=64
+org.elasticsearch.common.settings.loader.YamlSettingsLoaderTests=214
+org.elasticsearch.common.transport.BoundTransportAddressTests=139
+org.elasticsearch.common.unit.ByteSizeUnitTests=77
+org.elasticsearch.common.unit.ByteSizeValueTests=106
+org.elasticsearch.common.unit.DistanceUnitTests=58
+org.elasticsearch.common.unit.FuzzinessTests=55
+org.elasticsearch.common.unit.RatioValueTests=49
+org.elasticsearch.common.unit.SizeValueTests=88
+org.elasticsearch.common.unit.TimeValueTests=120
+org.elasticsearch.common.util.ArrayUtilsTests=43
+org.elasticsearch.common.util.BigArraysTests=4095
+org.elasticsearch.common.util.ByteUtilsTests=103
+org.elasticsearch.common.util.BytesRefHashTests=1372
+org.elasticsearch.common.util.CancellableThreadsTests=37
+org.elasticsearch.common.util.CollectionUtilsTests=219
+org.elasticsearch.common.util.LongHashTests=501
+org.elasticsearch.common.util.LongObjectHashMapTests=820
+org.elasticsearch.common.util.MultiDataPathUpgraderTests=984
+org.elasticsearch.common.util.SingleObjectCacheTests=107
+org.elasticsearch.common.util.URIPatternTests=128
+org.elasticsearch.common.util.concurrent.CountDownTests=266
+org.elasticsearch.common.util.concurrent.EsExecutorsTests=351
+org.elasticsearch.common.util.concurrent.PrioritizedExecutorsTests=436
+org.elasticsearch.common.util.concurrent.RefCountedTests=111
+org.elasticsearch.common.util.iterable.IterablesTests=16
+org.elasticsearch.common.xcontent.ObjectParserTests=110
+org.elasticsearch.common.xcontent.XContentFactoryTests=16
+org.elasticsearch.common.xcontent.builder.BuilderRawFieldTests=93
+org.elasticsearch.common.xcontent.builder.XContentBuilderTests=90
+org.elasticsearch.common.xcontent.cbor.CborXContentParserTests=65
+org.elasticsearch.common.xcontent.cbor.JsonVsCborTests=33
+org.elasticsearch.common.xcontent.smile.JsonVsSmileTests=38
+org.elasticsearch.common.xcontent.support.XContentHelperTests=44
+org.elasticsearch.common.xcontent.support.XContentMapValuesTests=102
+org.elasticsearch.common.xcontent.support.filtering.CborFilteringGeneratorTests=100
+org.elasticsearch.common.xcontent.support.filtering.JsonFilteringGeneratorTests=156
+org.elasticsearch.common.xcontent.support.filtering.SmileFilteringGeneratorTests=177
+org.elasticsearch.common.xcontent.support.filtering.YamlFilteringGeneratorTests=121
+org.elasticsearch.deps.jackson.JacksonLocationTests=20
+org.elasticsearch.deps.joda.SimpleJodaTests=223
+org.elasticsearch.deps.lucene.SimpleLuceneTests=432
+org.elasticsearch.deps.lucene.VectorHighlighterTests=354
+org.elasticsearch.discovery.BlockingClusterStatePublishResponseHandlerTests=21
+org.elasticsearch.discovery.DiscoveryModuleTests=27
+org.elasticsearch.discovery.ZenFaultDetectionTests=270
+org.elasticsearch.discovery.zen.ElectMasterServiceTests=50
+org.elasticsearch.discovery.zen.NodeJoinControllerTests=251
+org.elasticsearch.discovery.zen.ZenDiscoveryUnitTests=65
+org.elasticsearch.discovery.zen.ZenPingTests=16
+org.elasticsearch.discovery.zen.publish.PendingClusterStatesQueueTests=179
+org.elasticsearch.discovery.zen.publish.PublishClusterStateActionTests=887
+org.elasticsearch.env.EnvironmentTests=85
+org.elasticsearch.env.NodeEnvironmentTests=678
+org.elasticsearch.fieldstats.FieldStatsTests=1846
+org.elasticsearch.gateway.AsyncShardFetchTests=400
+org.elasticsearch.gateway.DanglingIndicesStateTests=43
+org.elasticsearch.gateway.GatewayMetaStateTests=202
+org.elasticsearch.gateway.GatewayModuleTests=14
+org.elasticsearch.gateway.GatewayServiceTests=62
+org.elasticsearch.gateway.GatewayTests=66
+org.elasticsearch.gateway.MetaDataStateFormatTests=232
+org.elasticsearch.gateway.MetaStateServiceTests=255
+org.elasticsearch.gateway.PrimaryShardAllocatorTests=85
+org.elasticsearch.gateway.PriorityComparatorTests=49
+org.elasticsearch.gateway.ReplicaShardAllocatorTests=64
+org.elasticsearch.http.netty.NettyHttpChannelTests=100
+org.elasticsearch.http.netty.NettyHttpServerPipeliningTests=3173
+org.elasticsearch.http.netty.pipelining.HttpPipeliningHandlerTests=335
+org.elasticsearch.index.IndexModuleTests=66
+org.elasticsearch.index.IndexServiceTests=15
+org.elasticsearch.index.VersionTypeTests=26
+org.elasticsearch.index.aliases.IndexAliasesServiceTests=318
+org.elasticsearch.index.analysis.ASCIIFoldingTokenFilterFactoryTests=765
+org.elasticsearch.index.analysis.AnalysisFactoryTests=150
+org.elasticsearch.index.analysis.AnalysisModuleTests=371
+org.elasticsearch.index.analysis.AnalysisTests=34
+org.elasticsearch.index.analysis.AnalyzerBackwardsCompatTests=1446
+org.elasticsearch.index.analysis.CJKFilterFactoryTests=39
+org.elasticsearch.index.analysis.CharFilterTests=94
+org.elasticsearch.index.analysis.CompoundAnalysisTests=171
+org.elasticsearch.index.analysis.HunspellTokenFilterFactoryTests=2896
+org.elasticsearch.index.analysis.KeepFilterFactoryTests=53
+org.elasticsearch.index.analysis.KeepTypesFilterFactoryTests=82
+org.elasticsearch.index.analysis.LimitTokenCountFilterFactoryTests=54
+org.elasticsearch.index.analysis.NGramTokenizerFactoryTests=63
+org.elasticsearch.index.analysis.NumericAnalyzerTests=13
+org.elasticsearch.index.analysis.PatternAnalyzerTests=1636
+org.elasticsearch.index.analysis.PatternCaptureTokenFilterTests=147
+org.elasticsearch.index.analysis.PreBuiltAnalyzerProviderFactoryTests=48
+org.elasticsearch.index.analysis.PreBuiltAnalyzerTests=293
+org.elasticsearch.index.analysis.PreBuiltCharFilterFactoryFactoryTests=86
+org.elasticsearch.index.analysis.PreBuiltTokenFilterFactoryFactoryTests=128
+org.elasticsearch.index.analysis.PreBuiltTokenizerFactoryFactoryTests=26
+org.elasticsearch.index.analysis.ShingleTokenFilterFactoryTests=136
+org.elasticsearch.index.analysis.SnowballAnalyzerTests=52
+org.elasticsearch.index.analysis.StemmerTokenFilterFactoryTests=2163
+org.elasticsearch.index.analysis.StopAnalyzerTests=3016
+org.elasticsearch.index.analysis.StopTokenFilterTests=375
+org.elasticsearch.index.analysis.WordDelimiterTokenFilterFactoryTests=336
+org.elasticsearch.index.analysis.commongrams.CommonGramsTokenFilterFactoryTests=359
+org.elasticsearch.index.analysis.synonyms.SynonymsAnalysisTests=166
+org.elasticsearch.index.cache.IndexCacheModuleTests=33
+org.elasticsearch.index.cache.bitset.BitSetFilterCacheTests=657
+org.elasticsearch.index.codec.CodecTests=479
+org.elasticsearch.index.codec.postingformat.PostingsFormatTests=6900
+org.elasticsearch.index.engine.CommitStatsTests=76
+org.elasticsearch.index.engine.InternalEngineSettingsTests=323
+org.elasticsearch.index.engine.InternalEngineTests=13034
+org.elasticsearch.index.engine.ShadowEngineTests=3902
+org.elasticsearch.index.fielddata.BinaryDVFieldDataTests=503
+org.elasticsearch.index.fielddata.DisabledFieldDataFormatTests=1035
+org.elasticsearch.index.fielddata.DoubleFieldDataTests=943
+org.elasticsearch.index.fielddata.DuelFieldDataTests=6036
+org.elasticsearch.index.fielddata.FieldDataTests=152
+org.elasticsearch.index.fielddata.FilterFieldDataTests=650
+org.elasticsearch.index.fielddata.FloatFieldDataTests=1246
+org.elasticsearch.index.fielddata.IndexFieldDataServiceTests=2723
+org.elasticsearch.index.fielddata.LongFieldDataTests=4912
+org.elasticsearch.index.fielddata.NoOrdinalsStringFieldDataTests=5655
+org.elasticsearch.index.fielddata.PagedBytesStringFieldDataTests=5923
+org.elasticsearch.index.fielddata.ParentChildFieldDataTests=1012
+org.elasticsearch.index.fielddata.ScriptDocValuesTests=224
+org.elasticsearch.index.fielddata.SortedSetDVStringFieldDataTests=5307
+org.elasticsearch.index.fielddata.fieldcomparator.ReplaceMissingTests=127
+org.elasticsearch.index.fielddata.ordinals.MultiOrdinalsTests=132
+org.elasticsearch.index.fielddata.ordinals.SingleOrdinalsTests=436
+org.elasticsearch.index.indexing.IndexingSlowLogTests=49
+org.elasticsearch.index.mapper.DocumentParserTests=371
+org.elasticsearch.index.mapper.DynamicMappingTests=1335
+org.elasticsearch.index.mapper.FieldTypeLookupTests=29
+org.elasticsearch.index.mapper.MapperServiceTests=230
+org.elasticsearch.index.mapper.UidTests=57
+org.elasticsearch.index.mapper.all.SimpleAllMapperTests=1376
+org.elasticsearch.index.mapper.binary.BinaryMappingTests=3554
+org.elasticsearch.index.mapper.boost.CustomBoostMappingTests=243
+org.elasticsearch.index.mapper.boost.FieldLevelBoostTests=2704
+org.elasticsearch.index.mapper.camelcase.CamelCaseFieldNameTests=358
+org.elasticsearch.index.mapper.completion.CompletionFieldMapperTests=429
+org.elasticsearch.index.mapper.compound.CompoundTypesTests=332
+org.elasticsearch.index.mapper.copyto.CopyToMapperTests=940
+org.elasticsearch.index.mapper.core.BinaryFieldTypeTests=95
+org.elasticsearch.index.mapper.core.BooleanFieldMapperTests=414
+org.elasticsearch.index.mapper.core.BooleanFieldTypeTests=133
+org.elasticsearch.index.mapper.core.ByteFieldTypeTests=86
+org.elasticsearch.index.mapper.core.CompletionFieldTypeTests=113
+org.elasticsearch.index.mapper.core.DateFieldTypeTests=73
+org.elasticsearch.index.mapper.core.DoubleFieldTypeTests=54
+org.elasticsearch.index.mapper.core.FloatFieldTypeTests=82
+org.elasticsearch.index.mapper.core.IntegerFieldTypeTests=56
+org.elasticsearch.index.mapper.core.LongFieldTypeTests=66
+org.elasticsearch.index.mapper.core.ShortFieldTypeTests=66
+org.elasticsearch.index.mapper.core.StringFieldTypeTests=79
+org.elasticsearch.index.mapper.core.TokenCountFieldMapperTests=300
+org.elasticsearch.index.mapper.date.DateBackwardsCompatibilityTests=1544
+org.elasticsearch.index.mapper.date.SimpleDateMappingTests=378
+org.elasticsearch.index.mapper.dynamictemplate.genericstore.GenericStoreDynamicTemplateTests=3256
+org.elasticsearch.index.mapper.dynamictemplate.pathmatch.PathMatchDynamicTemplateTests=303
+org.elasticsearch.index.mapper.dynamictemplate.simple.SimpleDynamicTemplatesTests=259
+org.elasticsearch.index.mapper.externalvalues.SimpleExternalMappingTests=539
+org.elasticsearch.index.mapper.geo.GeoEncodingTests=113
+org.elasticsearch.index.mapper.geo.GeoPointFieldMapperTests=2345
+org.elasticsearch.index.mapper.geo.GeoPointFieldTypeTests=113
+org.elasticsearch.index.mapper.geo.GeoShapeFieldMapperTests=768
+org.elasticsearch.index.mapper.geo.GeoShapeFieldTypeTests=92
+org.elasticsearch.index.mapper.geo.GeohashMappingGeoPointTests=308
+org.elasticsearch.index.mapper.id.IdMappingTests=712
+org.elasticsearch.index.mapper.index.IndexTypeMapperTests=330
+org.elasticsearch.index.mapper.internal.AllFieldTypeTests=95
+org.elasticsearch.index.mapper.internal.FieldNamesFieldMapperTests=636
+org.elasticsearch.index.mapper.internal.FieldNamesFieldTypeTests=119
+org.elasticsearch.index.mapper.internal.IdFieldTypeTests=54
+org.elasticsearch.index.mapper.internal.IndexFieldTypeTests=49
+org.elasticsearch.index.mapper.internal.ParentFieldMapperTests=83
+org.elasticsearch.index.mapper.internal.ParentFieldTypeTests=75
+org.elasticsearch.index.mapper.internal.RoutingFieldTypeTests=72
+org.elasticsearch.index.mapper.internal.SourceFieldTypeTests=129
+org.elasticsearch.index.mapper.internal.TimestampFieldTypeTests=61
+org.elasticsearch.index.mapper.internal.TypeFieldTypeTests=53
+org.elasticsearch.index.mapper.internal.UidFieldTypeTests=30
+org.elasticsearch.index.mapper.internal.VersionFieldTypeTests=39
+org.elasticsearch.index.mapper.ip.SimpleIpMappingTests=592
+org.elasticsearch.index.mapper.lucene.DoubleIndexingDocTests=142
+org.elasticsearch.index.mapper.lucene.StoredNumericValuesTests=328
+org.elasticsearch.index.mapper.merge.TestMergeMapperTests=1501
+org.elasticsearch.index.mapper.multifield.MultiFieldTests=633
+org.elasticsearch.index.mapper.multifield.merge.JavaMultiFieldMergeTests=218
+org.elasticsearch.index.mapper.nested.NestedMappingTests=749
+org.elasticsearch.index.mapper.null_value.NullValueTests=5152
+org.elasticsearch.index.mapper.numeric.SimpleNumericTests=884
+org.elasticsearch.index.mapper.object.NullValueObjectMappingTests=299
+org.elasticsearch.index.mapper.object.SimpleObjectMappingTests=728
+org.elasticsearch.index.mapper.parent.ParentMappingTests=294
+org.elasticsearch.index.mapper.path.PathMapperTests=515
+org.elasticsearch.index.mapper.routing.RoutingTypeMapperTests=497
+org.elasticsearch.index.mapper.simple.SimpleMapperTests=258
+org.elasticsearch.index.mapper.source.CompressSourceMappingTests=2902
+org.elasticsearch.index.mapper.source.DefaultSourceMappingTests=1323
+org.elasticsearch.index.mapper.string.SimpleStringMappingTests=177
+org.elasticsearch.index.mapper.string.StringFieldMapperPositionIncrementGapTests=702
+org.elasticsearch.index.mapper.timestamp.TimestampMappingTests=2991
+org.elasticsearch.index.mapper.ttl.TTLMappingTests=1822
+org.elasticsearch.index.mapper.typelevels.ParseDocumentTypeLevelsTests=440
+org.elasticsearch.index.mapper.typelevels.ParseMappingTypeLevelTests=187
+org.elasticsearch.index.mapper.update.UpdateMappingTests=843
+org.elasticsearch.index.query.BoolQueryBuilderTests=154
+org.elasticsearch.index.query.BoostingQueryBuilderTests=183
+org.elasticsearch.index.query.CombineFunctionTests=53
+org.elasticsearch.index.query.CommonTermsQueryBuilderTests=95
+org.elasticsearch.index.query.CommonTermsQueryParserTests=709
+org.elasticsearch.index.query.ConstantScoreQueryBuilderTests=285
+org.elasticsearch.index.query.DisMaxQueryBuilderTests=330
+org.elasticsearch.index.query.ExistsQueryBuilderTests=139
+org.elasticsearch.index.query.FieldMaskingSpanQueryBuilderTests=152
+org.elasticsearch.index.query.FuzzyQueryBuilderTests=210
+org.elasticsearch.index.query.GeoBoundingBoxQueryBuilderTests=315
+org.elasticsearch.index.query.GeoDistanceQueryBuilderTests=192
+org.elasticsearch.index.query.GeoDistanceRangeQueryTests=156
+org.elasticsearch.index.query.GeoPolygonQueryBuilderTests=445
+org.elasticsearch.index.query.GeoShapeQueryBuilderTests=246
+org.elasticsearch.index.query.GeohashCellQueryBuilderTests=85
+org.elasticsearch.index.query.HasChildQueryBuilderTests=255
+org.elasticsearch.index.query.HasChildQueryParserTests=82
+org.elasticsearch.index.query.HasParentQueryBuilderTests=336
+org.elasticsearch.index.query.IdsQueryBuilderTests=197
+org.elasticsearch.index.query.IndicesQueryBuilderTests=279
+org.elasticsearch.index.query.MatchAllQueryBuilderTests=188
+org.elasticsearch.index.query.MatchNoneQueryBuilderTests=257
+org.elasticsearch.index.query.MatchQueryBuilderTests=2712
+org.elasticsearch.index.query.MissingQueryBuilderTests=180
+org.elasticsearch.index.query.MoreLikeThisQueryBuilderTests=3351
+org.elasticsearch.index.query.MultiMatchQueryBuilderTests=59
+org.elasticsearch.index.query.NestedQueryBuilderTests=193
+org.elasticsearch.index.query.NotQueryBuilderTests=3071
+org.elasticsearch.index.query.OperatorTests=90
+org.elasticsearch.index.query.PrefixQueryBuilderTests=149
+org.elasticsearch.index.query.QueryFilterBuilderTests=100
+org.elasticsearch.index.query.QueryStringQueryBuilderTests=490
+org.elasticsearch.index.query.RangeQueryBuilderTests=577
+org.elasticsearch.index.query.RegexpQueryBuilderTests=235
+org.elasticsearch.index.query.ScoreModeTests=52
+org.elasticsearch.index.query.ScriptQueryBuilderTests=108
+org.elasticsearch.index.query.SimpleQueryStringBuilderTests=158
+org.elasticsearch.index.query.SpanContainingQueryBuilderTests=213
+org.elasticsearch.index.query.SpanFirstQueryBuilderTests=105
+org.elasticsearch.index.query.SpanMultiTermQueryBuilderTests=1847
+org.elasticsearch.index.query.SpanNearQueryBuilderTests=91
+org.elasticsearch.index.query.SpanNotQueryBuilderTests=589
+org.elasticsearch.index.query.SpanOrQueryBuilderTests=2712
+org.elasticsearch.index.query.SpanTermQueryBuilderTests=85
+org.elasticsearch.index.query.SpanWithinQueryBuilderTests=61
+org.elasticsearch.index.query.TemplateQueryBuilderTests=417
+org.elasticsearch.index.query.TemplateQueryParserTests=288
+org.elasticsearch.index.query.TermQueryBuilderTests=49
+org.elasticsearch.index.query.TermsQueryBuilderTests=69
+org.elasticsearch.index.query.TypeQueryBuilderTests=258
+org.elasticsearch.index.query.WildcardQueryBuilderTests=38
+org.elasticsearch.index.query.WrapperQueryBuilderTests=138
+org.elasticsearch.index.query.functionscore.FieldValueFactorFunctionModifierTests=69
+org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilderTests=227
+org.elasticsearch.index.query.functionscore.ScoreFunctionBuilderTests=53
+org.elasticsearch.index.query.support.QueryInnerHitsTests=53
+org.elasticsearch.index.search.MultiMatchQueryTests=135
+org.elasticsearch.index.search.geo.GeoPointParsingTests=90
+org.elasticsearch.index.search.geo.GeoUtilsTests=142
+org.elasticsearch.index.search.nested.DoubleNestedSortingTests=269
+org.elasticsearch.index.search.nested.FloatNestedSortingTests=575
+org.elasticsearch.index.search.nested.LongNestedSortingTests=305
+org.elasticsearch.index.search.nested.NestedSortingTests=264
+org.elasticsearch.index.shard.CommitPointsTests=58
+org.elasticsearch.index.shard.IndexShardTests=2664
+org.elasticsearch.index.shard.MergePolicySettingsTests=85
+org.elasticsearch.index.shard.NewPathForShardTests=87
+org.elasticsearch.index.shard.ShardPathTests=123
+org.elasticsearch.index.shard.ShardUtilsTests=69
+org.elasticsearch.index.shard.VersionFieldUpgraderTests=27
+org.elasticsearch.index.similarity.SimilarityTests=1282
+org.elasticsearch.index.snapshots.blobstore.FileInfoTests=350
+org.elasticsearch.index.snapshots.blobstore.SlicedInputStreamTests=28
+org.elasticsearch.index.store.DirectoryUtilsTests=179
+org.elasticsearch.index.store.IndexStoreBWCTests=397
+org.elasticsearch.index.store.IndexStoreTests=13
+org.elasticsearch.index.store.LegacyVerificationTests=44
+org.elasticsearch.index.store.StoreTests=433
+org.elasticsearch.index.translog.BufferedTranslogTests=4946
+org.elasticsearch.index.translog.TranslogTests=4070
+org.elasticsearch.index.translog.TranslogVersionTests=42
+org.elasticsearch.indices.IndicesLifecycleListenerSingleNodeTests=611
+org.elasticsearch.indices.IndicesModuleTests=1493
+org.elasticsearch.indices.IndicesServiceTests=5140
+org.elasticsearch.indices.cache.query.terms.TermsLookupTests=22
+org.elasticsearch.indices.flush.SyncedFlushSingleNodeTests=1243
+org.elasticsearch.indices.flush.SyncedFlushUnitTests=64
+org.elasticsearch.indices.memory.IndexingMemoryControllerTests=65
+org.elasticsearch.indices.memory.breaker.CircuitBreakerUnitTests=101
+org.elasticsearch.indices.recovery.RecoverySourceHandlerTests=691
+org.elasticsearch.indices.recovery.RecoveryStateTests=153
+org.elasticsearch.indices.recovery.RecoveryStatusTests=62
+org.elasticsearch.indices.recovery.StartRecoveryRequestTests=84
+org.elasticsearch.indices.store.IndicesStoreTests=83
+org.elasticsearch.monitor.fs.FsProbeTests=41
+org.elasticsearch.monitor.jvm.JvmStatsTests=43
+org.elasticsearch.monitor.os.OsProbeTests=45
+org.elasticsearch.monitor.process.ProcessProbeTests=42
+org.elasticsearch.node.internal.InternalSettingsPreparerTests=140
+org.elasticsearch.plugins.PluginInfoTests=372
+org.elasticsearch.plugins.PluginManagerCliTests=153
+org.elasticsearch.plugins.PluginManagerUnitTests=51
+org.elasticsearch.plugins.PluginsServiceTests=68
+org.elasticsearch.recovery.RecoveriesCollectionTests=430
+org.elasticsearch.recovery.RecoverySettingsTests=569
+org.elasticsearch.rest.BytesRestResponseTests=83
+org.elasticsearch.rest.HeadersAndContextCopyClientTests=194
+org.elasticsearch.rest.RestFilterChainTests=77
+org.elasticsearch.rest.RestRequestTests=39
+org.elasticsearch.rest.action.support.RestTableTests=88
+org.elasticsearch.rest.util.RestUtilsTests=85
+org.elasticsearch.script.FileScriptTests=39
+org.elasticsearch.script.NativeScriptTests=111
+org.elasticsearch.script.ScriptContextRegistryTests=27
+org.elasticsearch.script.ScriptContextTests=85
+org.elasticsearch.script.ScriptModesTests=115
+org.elasticsearch.script.ScriptParameterParserTests=173
+org.elasticsearch.script.ScriptServiceTests=421
+org.elasticsearch.script.mustache.MustacheScriptEngineTests=115
+org.elasticsearch.script.mustache.MustacheTests=65
+org.elasticsearch.search.MultiValueModeTests=149
+org.elasticsearch.search.SearchModuleTests=89
+org.elasticsearch.search.SearchServiceTests=1170
+org.elasticsearch.search.aggregations.AggregationCollectorTests=644
+org.elasticsearch.search.aggregations.bucket.nested.NestedAggregatorTests=419
+org.elasticsearch.search.aggregations.bucket.significant.SignificanceHeuristicTests=120
+org.elasticsearch.search.aggregations.metrics.cardinality.HyperLogLogPlusPlusTests=695
+org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests=27
+org.elasticsearch.search.aggregations.pipeline.moving.avg.MovAvgUnitTests=122
+org.elasticsearch.search.aggregations.support.MissingValuesTests=52
+org.elasticsearch.search.aggregations.support.PathTests=90
+org.elasticsearch.search.aggregations.support.ScriptValuesTests=67
+org.elasticsearch.search.builder.SearchSourceBuilderTests=49
+org.elasticsearch.search.compress.SearchSourceCompressTests=3136
+org.elasticsearch.search.fetch.innerhits.NestedChildrenFilterTests=128
+org.elasticsearch.search.internal.InternalSearchHitTests=46
+org.elasticsearch.search.query.QueryPhaseTests=185
+org.elasticsearch.search.sort.SortParserTests=319
+org.elasticsearch.search.stats.SearchStatsUnitTests=50
+org.elasticsearch.search.suggest.CompletionTokenStreamTests=160
+org.elasticsearch.search.suggest.completion.CompletionPostingsFormatTests=1319
+org.elasticsearch.search.suggest.context.GeoLocationContextMappingTests=109
+org.elasticsearch.search.suggest.phrase.NoisyChannelSpellCheckerTests=1409
+org.elasticsearch.snapshots.SnapshotRequestsTests=30
+org.elasticsearch.snapshots.SnapshotUtilsTests=30
+org.elasticsearch.test.rest.test.AssertionParsersTests=30
+org.elasticsearch.test.rest.test.DoSectionParserTests=198
+org.elasticsearch.test.rest.test.FileUtilsTests=60
+org.elasticsearch.test.rest.test.JsonPathTests=83
+org.elasticsearch.test.rest.test.RestApiParserFailingTests=102
+org.elasticsearch.test.rest.test.RestApiParserTests=73
+org.elasticsearch.test.rest.test.RestTestParserTests=145
+org.elasticsearch.test.rest.test.SetSectionParserTests=116
+org.elasticsearch.test.rest.test.SetupSectionParserTests=248
+org.elasticsearch.test.rest.test.SkipSectionParserTests=95
+org.elasticsearch.test.rest.test.TestSectionParserTests=134
+org.elasticsearch.test.test.InternalTestClusterTests=40
+org.elasticsearch.test.test.LoggingListenerTests=72
+org.elasticsearch.test.test.VersionUtilsTests=28
+org.elasticsearch.threadpool.ThreadPoolSerializationTests=56
+org.elasticsearch.threadpool.ThreadPoolStatsTests=99
+org.elasticsearch.threadpool.UpdateThreadPoolSettingsTests=24
+org.elasticsearch.transport.NettySizeHeaderFrameDecoderTests=183
+org.elasticsearch.transport.TransportMessageTests=51
+org.elasticsearch.transport.local.SimpleLocalTransportTests=1174
+org.elasticsearch.transport.netty.KeyedLockTests=414
+org.elasticsearch.transport.netty.NettyScheduledPingTests=1662
+org.elasticsearch.transport.netty.NettyTransportMultiPortTests=382
+org.elasticsearch.transport.netty.NettyTransportTests=137
+org.elasticsearch.transport.netty.SimpleNettyTransportTests=5528
+org.elasticsearch.tribe.TribeUnitTests=2098
+org.elasticsearch.watcher.FileWatcherTests=203
+org.elasticsearch.watcher.ResourceWatcherServiceTests=101
diff --git a/core/pom.xml b/core/pom.xml
index dc0d644..023d929 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -105,10 +105,6 @@
         <!-- Lucene spatial -->
 
         <dependency>
-            <groupId>com.google.guava</groupId>
-            <artifactId>guava</artifactId>
-        </dependency>
-        <dependency>
             <groupId>com.carrotsearch</groupId>
             <artifactId>hppc</artifactId>
         </dependency>
@@ -333,15 +329,17 @@
                     <excludes>
                         <!-- Guice -->
                         <exclude>src/main/java/org/elasticsearch/common/inject/**</exclude>
-                        <exclude>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</exclude>
-                        <exclude>src/main/java/org/elasticsearch/common/network/InetAddresses.java</exclude>
+                        <!-- Forks of Lucene classes -->
                         <exclude>src/main/java/org/apache/lucene/**/X*.java</exclude>
-                        <!-- t-digest -->
-                        <exclude>src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java</exclude>
                         <!-- netty pipelining -->
                         <exclude>src/main/java/org/elasticsearch/http/netty/pipelining/**</exclude>
+                        <!-- Guava -->
+                        <exclude>src/main/java/org/elasticsearch/common/network/InetAddresses.java</exclude>
                         <exclude>src/test/java/org/elasticsearch/common/network/InetAddressesTests.java</exclude>
                         <exclude>src/test/java/org/elasticsearch/common/collect/EvictingQueueTests.java</exclude>
+                        <!-- Joda -->
+                        <exclude>src/main/java/org/joda/time/base/BaseDateTime.java</exclude>
+                        <exclude>src/main/java/org/joda/time/format/StrictISODateTimeFormat.java</exclude>
                     </excludes>
                 </configuration>
             </plugin>
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
index ca1524f..3ef6e5a 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
@@ -19,14 +19,19 @@
 
 package org.apache.lucene.queryparser.classic;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.*;
-import org.apache.lucene.util.automaton.RegExp;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MatchNoDocsQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.automaton.RegExp;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
@@ -38,9 +43,12 @@ import org.elasticsearch.index.query.support.QueryParsers;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.Objects;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
@@ -52,13 +60,13 @@ import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfN
  */
 public class MapperQueryParser extends QueryParser {
 
-    public static final ImmutableMap<String, FieldQueryExtension> fieldQueryExtensions;
+    public static final Map<String, FieldQueryExtension> FIELD_QUERY_EXTENSIONS;
 
     static {
-        fieldQueryExtensions = ImmutableMap.<String, FieldQueryExtension>builder()
-                .put(ExistsFieldQueryExtension.NAME, new ExistsFieldQueryExtension())
-                .put(MissingFieldQueryExtension.NAME, new MissingFieldQueryExtension())
-                .build();
+        Map<String, FieldQueryExtension> fieldQueryExtensions = new HashMap<>();
+        fieldQueryExtensions.put(ExistsFieldQueryExtension.NAME, new ExistsFieldQueryExtension());
+        fieldQueryExtensions.put(MissingFieldQueryExtension.NAME, new MissingFieldQueryExtension());
+        FIELD_QUERY_EXTENSIONS = unmodifiableMap(fieldQueryExtensions);
     }
 
     private final QueryShardContext context;
@@ -124,7 +132,7 @@ public class MapperQueryParser extends QueryParser {
 
     @Override
     public Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
-        FieldQueryExtension fieldQueryExtension = fieldQueryExtensions.get(field);
+        FieldQueryExtension fieldQueryExtension = FIELD_QUERY_EXTENSIONS.get(field);
         if (fieldQueryExtension != null) {
             return fieldQueryExtension.query(context, queryText);
         }
@@ -540,7 +548,7 @@ public class MapperQueryParser extends QueryParser {
                     return newMatchAllDocsQuery();
                 }
                 // effectively, we check if a field exists or not
-                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(context, actualField);
+                return FIELD_QUERY_EXTENSIONS.get(ExistsFieldQueryExtension.NAME).query(context, actualField);
             }
         }
         if (lowercaseExpandedTerms) {
diff --git a/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java b/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java
index 75ad81b..2f7d538 100644
--- a/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java
+++ b/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java b/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java
index 67373ef..30f57b2 100644
--- a/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java
+++ b/core/src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java b/core/src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java
index a756de6..f3bfa1b 100644
--- a/core/src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java
+++ b/core/src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 4c82c28..62eb374 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -524,7 +524,7 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
         ROUTING_VALIDATION_EXCEPTION(org.elasticsearch.cluster.routing.RoutingValidationException.class, org.elasticsearch.cluster.routing.RoutingValidationException::new, 61),
         NOT_SERIALIZABLE_EXCEPTION_WRAPPER(org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class, org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper::new, 62),
         ALIAS_FILTER_PARSING_EXCEPTION(org.elasticsearch.indices.AliasFilterParsingException.class, org.elasticsearch.indices.AliasFilterParsingException::new, 63),
-        DELETE_BY_QUERY_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class, org.elasticsearch.index.engine.DeleteByQueryFailedEngineException::new, 64),
+        // 64 was DeleteByQueryFailedEngineException, which was removed in 3.0
         GATEWAY_EXCEPTION(org.elasticsearch.gateway.GatewayException.class, org.elasticsearch.gateway.GatewayException::new, 65),
         INDEX_SHARD_NOT_RECOVERING_EXCEPTION(org.elasticsearch.index.shard.IndexShardNotRecoveringException.class, org.elasticsearch.index.shard.IndexShardNotRecoveringException::new, 66),
         HTTP_EXCEPTION(org.elasticsearch.http.HttpException.class, org.elasticsearch.http.HttpException::new, 67),
diff --git a/core/src/main/java/org/elasticsearch/action/ActionModule.java b/core/src/main/java/org/elasticsearch/action/ActionModule.java
index cc41cc0..f8634b1 100644
--- a/core/src/main/java/org/elasticsearch/action/ActionModule.java
+++ b/core/src/main/java/org/elasticsearch/action/ActionModule.java
@@ -121,8 +121,8 @@ import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeAction;
 import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeSettingsAction;
 import org.elasticsearch.action.admin.indices.validate.query.TransportValidateQueryAction;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryAction;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateAction;
-import org.elasticsearch.action.admin.indices.validate.template.TransportRenderSearchTemplateAction;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateAction;
+import org.elasticsearch.action.admin.cluster.validate.template.TransportRenderSearchTemplateAction;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerAction;
 import org.elasticsearch.action.admin.indices.warmer.delete.TransportDeleteWarmerAction;
 import org.elasticsearch.action.admin.indices.warmer.get.GetWarmersAction;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
index 908a25a..2d68385 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.admin.cluster.node.info;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Build;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.support.nodes.BaseNodeResponse;
@@ -36,8 +35,11 @@ import org.elasticsearch.threadpool.ThreadPoolInfo;
 import org.elasticsearch.transport.TransportInfo;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Node information (static, does not change over time).
  */
@@ -75,7 +77,7 @@ public class NodeInfo extends BaseNodeResponse {
     NodeInfo() {
     }
 
-    public NodeInfo(Version version, Build build, DiscoveryNode node, @Nullable ImmutableMap<String, String> serviceAttributes, @Nullable Settings settings,
+    public NodeInfo(Version version, Build build, DiscoveryNode node, @Nullable Map<String, String> serviceAttributes, @Nullable Settings settings,
                     @Nullable OsInfo os, @Nullable ProcessInfo process, @Nullable JvmInfo jvm, @Nullable ThreadPoolInfo threadPool,
                     @Nullable TransportInfo transport, @Nullable HttpInfo http, @Nullable PluginsInfo plugins) {
         super(node);
@@ -186,12 +188,12 @@ public class NodeInfo extends BaseNodeResponse {
         version = Version.readVersion(in);
         build = Build.readBuild(in);
         if (in.readBoolean()) {
-            ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();
+            Map<String, String> builder = new HashMap<>();
             int size = in.readVInt();
             for (int i = 0; i < size; i++) {
                 builder.put(in.readString(), in.readString());
             }
-            serviceAttributes = builder.build();
+            serviceAttributes = unmodifiableMap(builder);
         }
         if (in.readBoolean()) {
             settings = Settings.readSettingsFromStream(in);
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java
index 65033f3..108bb31 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.action.admin.cluster.node.info;
 
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.action.support.nodes.BaseNodesResponse;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -85,8 +87,8 @@ public class NodesInfoResponse extends BaseNodesResponse<NodeInfo> implements To
 
             if (!nodeInfo.getNode().attributes().isEmpty()) {
                 builder.startObject("attributes");
-                for (Map.Entry<String, String> attr : nodeInfo.getNode().attributes().entrySet()) {
-                    builder.field(attr.getKey(), attr.getValue(), XContentBuilder.FieldCaseConversion.NONE);
+                for (ObjectObjectCursor<String, String> attr : nodeInfo.getNode().attributes()) {
+                    builder.field(attr.key, attr.value, XContentBuilder.FieldCaseConversion.NONE);
                 }
                 builder.endObject();
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java
index c437a44..4cd050c 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.action.admin.cluster.node.stats;
 
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.action.support.nodes.BaseNodeResponse;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.Nullable;
@@ -38,7 +40,6 @@ import org.elasticsearch.threadpool.ThreadPoolStats;
 import org.elasticsearch.transport.TransportStats;
 
 import java.io.IOException;
-import java.util.Map;
 
 /**
  * Node statistics (dynamic, changes depending on when created).
@@ -281,8 +282,8 @@ public class NodeStats extends BaseNodeResponse implements ToXContent {
 
             if (!getNode().attributes().isEmpty()) {
                 builder.startObject("attributes");
-                for (Map.Entry<String, String> attr : getNode().attributes().entrySet()) {
-                    builder.field(attr.getKey(), attr.getValue(), XContentBuilder.FieldCaseConversion.NONE);
+                for (ObjectObjectCursor<String, String> attr : getNode().attributes()) {
+                    builder.field(attr.key, attr.value, XContentBuilder.FieldCaseConversion.NONE);
                 }
                 builder.endObject();
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java
index 961914e..5999fc3 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java
@@ -19,16 +19,18 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 
 import java.io.IOException;
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Represents snapshot status of all shards in the index
  */
@@ -45,14 +47,14 @@ public class SnapshotIndexStatus implements Iterable<SnapshotIndexShardStatus>,
     SnapshotIndexStatus(String index, Collection<SnapshotIndexShardStatus> shards) {
         this.index = index;
 
-        ImmutableMap.Builder<Integer, SnapshotIndexShardStatus> builder = ImmutableMap.builder();
+        Map<Integer, SnapshotIndexShardStatus> indexShards = new HashMap<>();
         stats = new SnapshotStats();
         for (SnapshotIndexShardStatus shard : shards) {
-            builder.put(shard.getShardId(), shard);
+            indexShards.put(shard.getShardId(), shard);
             stats.add(shard.getStats());
         }
         shardsStats = new SnapshotShardsStats(shards);
-        indexShards = builder.build();
+        this.indexShards = unmodifiableMap(indexShards);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
index 91b890b..860b414 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.SnapshotsInProgress.State;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -33,11 +32,14 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Status of a snapshot
  */
@@ -49,7 +51,7 @@ public class SnapshotStatus implements ToXContent, Streamable {
 
     private List<SnapshotIndexShardStatus> shards;
 
-    private ImmutableMap<String, SnapshotIndexStatus> indicesStatus;
+    private Map<String, SnapshotIndexStatus> indicesStatus;
 
     private SnapshotShardsStats shardsStats;
 
@@ -100,7 +102,7 @@ public class SnapshotStatus implements ToXContent, Streamable {
             return this.indicesStatus;
         }
 
-        ImmutableMap.Builder<String, SnapshotIndexStatus> indicesStatus = ImmutableMap.builder();
+        Map<String, SnapshotIndexStatus> indicesStatus = new HashMap<>();
 
         Set<String> indices = new HashSet<>();
         for (SnapshotIndexShardStatus shard : shards) {
@@ -116,7 +118,7 @@ public class SnapshotStatus implements ToXContent, Streamable {
             }
             indicesStatus.put(index, new SnapshotIndexStatus(index, shards));
         }
-        this.indicesStatus = indicesStatus.build();
+        this.indicesStatus = unmodifiableMap(indicesStatus);
         return this.indicesStatus;
 
     }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
index 2ecd4fb..8c856be 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.FailedNodeException;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
index 5af92dc..70f4975 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java
@@ -19,7 +19,9 @@
 
 package org.elasticsearch.action.admin.cluster.snapshots.status;
 
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.master.TransportMasterNodeAction;
@@ -95,9 +97,9 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
 
         Set<String> nodesIds = new HashSet<>();
         for (SnapshotsInProgress.Entry entry : currentSnapshots) {
-            for (SnapshotsInProgress.ShardSnapshotStatus status : entry.shards().values()) {
-                if (status.nodeId() != null) {
-                    nodesIds.add(status.nodeId());
+            for (ObjectCursor<SnapshotsInProgress.ShardSnapshotStatus> status : entry.shards().values()) {
+                if (status.value.nodeId() != null) {
+                    nodesIds.add(status.value.nodeId());
                 }
             }
         }
@@ -151,15 +153,15 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
             for (SnapshotsInProgress.Entry entry : currentSnapshots) {
                 currentSnapshotIds.add(entry.snapshotId());
                 List<SnapshotIndexShardStatus> shardStatusBuilder = new ArrayList<>();
-                for (ImmutableMap.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardEntry : entry.shards().entrySet()) {
-                    SnapshotsInProgress.ShardSnapshotStatus status = shardEntry.getValue();
+                for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardEntry : entry.shards()) {
+                    SnapshotsInProgress.ShardSnapshotStatus status = shardEntry.value;
                     if (status.nodeId() != null) {
                         // We should have information about this shard from the shard:
                         TransportNodesSnapshotsStatus.NodeSnapshotStatus nodeStatus = nodeSnapshotStatusMap.get(status.nodeId());
                         if (nodeStatus != null) {
                             Map<ShardId, SnapshotIndexShardStatus> shardStatues = nodeStatus.status().get(entry.snapshotId());
                             if (shardStatues != null) {
-                                SnapshotIndexShardStatus shardStatus = shardStatues.get(shardEntry.getKey());
+                                SnapshotIndexShardStatus shardStatus = shardStatues.get(shardEntry.key);
                                 if (shardStatus != null) {
                                     // We have full information about this shard
                                     shardStatusBuilder.add(shardStatus);
@@ -169,7 +171,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                         }
                     }
                     final SnapshotIndexShardStage stage;
-                    switch (shardEntry.getValue().state()) {
+                    switch (shardEntry.value.state()) {
                         case FAILED:
                         case ABORTED:
                         case MISSING:
@@ -184,9 +186,9 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                             stage = SnapshotIndexShardStage.DONE;
                             break;
                         default:
-                            throw new IllegalArgumentException("Unknown snapshot state " + shardEntry.getValue().state());
+                            throw new IllegalArgumentException("Unknown snapshot state " + shardEntry.value.state());
                     }
-                    SnapshotIndexShardStatus shardStatus = new SnapshotIndexShardStatus(shardEntry.getKey(), stage);
+                    SnapshotIndexShardStatus shardStatus = new SnapshotIndexShardStatus(shardEntry.key, stage);
                     shardStatusBuilder.add(shardStatus);
                 }
                 builder.add(new SnapshotStatus(entry.snapshotId(), entry.state(), Collections.unmodifiableList(shardStatusBuilder)));
@@ -205,7 +207,7 @@ public class TransportSnapshotsStatusAction extends TransportMasterNodeAction<Sn
                     List<SnapshotIndexShardStatus> shardStatusBuilder = new ArrayList<>();
                     if (snapshot.state().completed()) {
                         Map<ShardId, IndexShardSnapshotStatus> shardStatues = snapshotsService.snapshotShards(snapshotId);
-                        for (ImmutableMap.Entry<ShardId, IndexShardSnapshotStatus> shardStatus : shardStatues.entrySet()) {
+                        for (Map.Entry<ShardId, IndexShardSnapshotStatus> shardStatus : shardStatues.entrySet()) {
                             shardStatusBuilder.add(new SnapshotIndexShardStatus(shardStatus.getKey(), shardStatus.getValue()));
                         }
                         final SnapshotsInProgress.State state;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java
index 54ea5a9..a3708f6 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.action.admin.cluster.state;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
@@ -89,7 +90,7 @@ public class TransportClusterStateAction extends TransportMasterNodeReadAction<C
                         routingTableBuilder.add(currentState.routingTable().getIndicesRouting().get(filteredIndex));
                     }
                 }
-                builder.routingTable(routingTableBuilder);
+                builder.routingTable(routingTableBuilder.build());
             } else {
                 builder.routingTable(currentState.routingTable());
             }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateAction.java
new file mode 100644
index 0000000..427d0c4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateAction.java
@@ -0,0 +1,44 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.Action;
+import org.elasticsearch.client.ElasticsearchClient;
+
+public class RenderSearchTemplateAction extends Action<RenderSearchTemplateRequest, RenderSearchTemplateResponse, RenderSearchTemplateRequestBuilder> {
+
+    public static final RenderSearchTemplateAction INSTANCE = new RenderSearchTemplateAction();
+    public static final String NAME = "cluster:admin/render/template/search";
+
+    public RenderSearchTemplateAction() {
+        super(NAME);
+    }
+
+    @Override
+    public RenderSearchTemplateRequestBuilder newRequestBuilder(ElasticsearchClient client) {
+        return new RenderSearchTemplateRequestBuilder(client, this);
+    }
+
+    @Override
+    public RenderSearchTemplateResponse newResponse() {
+        return new RenderSearchTemplateResponse();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java
new file mode 100644
index 0000000..a51090e
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequest.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.ActionRequestValidationException;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.script.Template;
+
+import java.io.IOException;
+
+public class RenderSearchTemplateRequest extends ActionRequest<RenderSearchTemplateRequest> {
+
+    private Template template;
+    
+    public void template(Template template) {
+        this.template = template;
+    }
+    
+    public Template template() {
+        return template;
+    }
+    
+    @Override
+    public ActionRequestValidationException validate() {
+        ActionRequestValidationException exception = null;
+        if (template == null) {
+            exception = new ActionRequestValidationException();
+            exception.addValidationError("template must not be null");
+        }
+        return exception;
+    }
+    
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        boolean hasTemplate = template!= null;
+        out.writeBoolean(hasTemplate);
+        if (hasTemplate) {
+            template.writeTo(out);
+        }
+    }
+    
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        if (in.readBoolean()) {
+            template = Template.readTemplate(in);
+        }
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequestBuilder.java
new file mode 100644
index 0000000..f7e3da1
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateRequestBuilder.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.script.Template;
+
+public class RenderSearchTemplateRequestBuilder extends ActionRequestBuilder<RenderSearchTemplateRequest, RenderSearchTemplateResponse, RenderSearchTemplateRequestBuilder> {
+
+    public RenderSearchTemplateRequestBuilder(ElasticsearchClient client,
+            RenderSearchTemplateAction action) {
+        super(client, action, new RenderSearchTemplateRequest());
+    }
+    
+    public RenderSearchTemplateRequestBuilder template(Template template) {
+        request.template(template);
+        return this;
+    }
+    
+    public Template template() {
+        return request.template();
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateResponse.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateResponse.java
new file mode 100644
index 0000000..d14a9a4
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/RenderSearchTemplateResponse.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
+
+public class RenderSearchTemplateResponse extends ActionResponse implements ToXContent {
+
+    private BytesReference source;
+
+    public BytesReference source() {
+        return source;
+    }
+    
+    public void source(BytesReference source) {
+        this.source = source;
+    }
+    
+    @Override
+    public void writeTo(StreamOutput out) throws IOException {
+        super.writeTo(out);
+        boolean hasSource = source != null;
+        out.writeBoolean(hasSource);
+        if (hasSource) {
+            out.writeBytesReference(source);
+        }
+    }
+    
+    @Override
+    public void readFrom(StreamInput in) throws IOException {
+        super.readFrom(in);
+        if (in.readBoolean()) {
+            source = in.readBytesReference();
+        }
+    }
+
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        builder.rawField("template_output", source);
+        builder.endObject();
+        return builder;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java
new file mode 100644
index 0000000..5fe8297
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.cluster.validate.template;
+
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.support.ActionFilters;
+import org.elasticsearch.action.support.HandledTransportAction;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.util.concurrent.AbstractRunnable;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.ScriptContext;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
+
+public class TransportRenderSearchTemplateAction extends HandledTransportAction<RenderSearchTemplateRequest, RenderSearchTemplateResponse> {
+
+    private final ScriptService scriptService;
+
+    @Inject
+    public TransportRenderSearchTemplateAction(ScriptService scriptService, Settings settings, ThreadPool threadPool,
+            TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
+        super(settings, RenderSearchTemplateAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, RenderSearchTemplateRequest::new);
+        this.scriptService = scriptService;
+    }
+
+    @Override
+    protected void doExecute(final RenderSearchTemplateRequest request, final ActionListener<RenderSearchTemplateResponse> listener) {
+        threadPool.generic().execute(new AbstractRunnable() {
+
+            @Override
+            public void onFailure(Throwable t) {
+                listener.onFailure(t);
+            }
+
+            @Override
+            protected void doRun() throws Exception {
+                ExecutableScript executable = scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, request);
+                BytesReference processedTemplate = (BytesReference) executable.run();
+                RenderSearchTemplateResponse response = new RenderSearchTemplateResponse();
+                response.source(processedTemplate);
+                listener.onResponse(response);
+            }
+        });
+    }
+
+}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
index 6eac403..0930f8f 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexResponse.java
@@ -122,7 +122,7 @@ public class GetIndexResponse extends ActionResponse {
                         in.readString(),
                         in.readStringArray(),
                         in.readOptionalBoolean(),
-                        in.readBoolean() ? new IndexWarmersMetaData.SearchSource(in) : null)
+                        in.readBytesReference())
                 );
             }
             warmersMapBuilder.put(key, Collections.unmodifiableList(warmerEntryBuilder));
@@ -173,11 +173,7 @@ public class GetIndexResponse extends ActionResponse {
                 out.writeString(warmerEntry.name());
                 out.writeStringArray(warmerEntry.types());
                 out.writeOptionalBoolean(warmerEntry.requestCache());
-                boolean hasSource = warmerEntry.source() != null;
-                out.writeBoolean(hasSource);
-                if (hasSource) {
-                    warmerEntry.source().writeTo(out);
-                }
+                out.writeBytesReference(warmerEntry.source());
             }
         }
         out.writeVInt(mappings.size());
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
index 24820ba..12ef72b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.action.admin.indices.mapping.get;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -35,12 +33,13 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
 import static java.util.Collections.unmodifiableMap;
 
 /** Response object for {@link GetFieldMappingsRequest} API */
 public class GetFieldMappingsResponse extends ActionResponse implements ToXContent {
 
-    private Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings = ImmutableMap.of();
+    private Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings = emptyMap();
 
     GetFieldMappingsResponse(Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappings) {
         this.mappings = mappings;
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
index 060d94a..82d6c21 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java
@@ -25,16 +25,18 @@ import org.elasticsearch.action.support.HandledTransportAction;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
+import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReferenceArray;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  */
 public class TransportGetFieldMappingsAction extends HandledTransportAction<GetFieldMappingsRequest, GetFieldMappingsResponse> {
@@ -88,7 +90,7 @@ public class TransportGetFieldMappingsAction extends HandledTransportAction<GetF
     }
 
     private GetFieldMappingsResponse merge(AtomicReferenceArray<Object> indexResponses) {
-        MapBuilder<String, Map<String, Map<String, GetFieldMappingsResponse.FieldMappingMetaData>>> mergedResponses = MapBuilder.newMapBuilder();
+        Map<String, Map<String, Map<String, GetFieldMappingsResponse.FieldMappingMetaData>>> mergedResponses = new HashMap<>();
         for (int i = 0; i < indexResponses.length(); i++) {
             Object element = indexResponses.get(i);
             if (element instanceof GetFieldMappingsResponse) {
@@ -96,6 +98,6 @@ public class TransportGetFieldMappingsAction extends HandledTransportAction<GetF
                 mergedResponses.putAll(response.mappings());
             }
         }
-        return new GetFieldMappingsResponse(mergedResponses.immutableMap());
+        return new GetFieldMappingsResponse(unmodifiableMap(mergedResponses));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
index a7b780a..c71f60e 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.action.admin.indices.mapping.get;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsResponse.FieldMappingMetaData;
 import org.elasticsearch.action.support.ActionFilters;
@@ -56,6 +54,7 @@ import java.util.Iterator;
 import java.util.Map;
 import java.util.stream.Collectors;
 
+import static java.util.Collections.singletonMap;
 import static org.elasticsearch.common.util.CollectionUtils.newLinkedList;
 
 /**
@@ -110,13 +109,13 @@ public class TransportGetFieldMappingsIndexAction extends TransportSingleShardAc
         MapBuilder<String, Map<String, FieldMappingMetaData>> typeMappings = new MapBuilder<>();
         for (String type : typeIntersection) {
             DocumentMapper documentMapper = indexService.mapperService().documentMapper(type);
-            ImmutableMap<String, FieldMappingMetaData> fieldMapping = findFieldMappingsByType(documentMapper, request);
+            Map<String, FieldMappingMetaData> fieldMapping = findFieldMappingsByType(documentMapper, request);
             if (!fieldMapping.isEmpty()) {
                 typeMappings.put(type, fieldMapping);
             }
         }
 
-        return new GetFieldMappingsResponse(ImmutableMap.of(shardId.getIndex(), typeMappings.immutableMap()));
+        return new GetFieldMappingsResponse(singletonMap(shardId.getIndex(), typeMappings.immutableMap()));
     }
 
     @Override
@@ -166,7 +165,7 @@ public class TransportGetFieldMappingsIndexAction extends TransportSingleShardAc
         }
     };
 
-    private ImmutableMap<String, FieldMappingMetaData> findFieldMappingsByType(DocumentMapper documentMapper, GetFieldMappingsIndexRequest request) {
+    private Map<String, FieldMappingMetaData> findFieldMappingsByType(DocumentMapper documentMapper, GetFieldMappingsIndexRequest request) {
         MapBuilder<String, FieldMappingMetaData> fieldMappings = new MapBuilder<>();
         final DocumentFieldMappers allFieldMappers = documentMapper.mappers();
         for (String field : request.fields()) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java
index 5cb94b2..4f427b5 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.admin.indices.stats;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.ShardOperationFailedException;
 import org.elasticsearch.action.support.broadcast.BroadcastResponse;
 import org.elasticsearch.cluster.routing.ShardRouting;
@@ -38,13 +37,15 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  */
 public class IndicesStatsResponse extends BroadcastResponse implements ToXContent {
 
     private ShardStats[] shards;
 
-    private ImmutableMap<ShardRouting, CommonStats> shardStatsMap;
+    private Map<ShardRouting, CommonStats> shardStatsMap;
 
     IndicesStatsResponse() {
 
@@ -55,16 +56,15 @@ public class IndicesStatsResponse extends BroadcastResponse implements ToXConten
         this.shards = shards;
     }
 
-    public ImmutableMap<ShardRouting, CommonStats> asMap() {
-        if (shardStatsMap == null) {
-            ImmutableMap.Builder<ShardRouting, CommonStats> mb = ImmutableMap.builder();
+    public Map<ShardRouting, CommonStats> asMap() {
+        if (this.shardStatsMap == null) {
+            Map<ShardRouting, CommonStats> shardStatsMap = new HashMap<>();
             for (ShardStats ss : shards) {
-                mb.put(ss.getShardRouting(), ss.getStats());
+                shardStatsMap.put(ss.getShardRouting(), ss.getStats());
             }
-
-            shardStatsMap = mb.build();
+            this.shardStatsMap = unmodifiableMap(shardStatsMap);
         }
-        return shardStatsMap;
+        return this.shardStatsMap;
     }
 
     public ShardStats[] getShards() {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateAction.java
deleted file mode 100644
index 0cc7158..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateAction.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.Action;
-import org.elasticsearch.client.ElasticsearchClient;
-
-public class RenderSearchTemplateAction extends Action<RenderSearchTemplateRequest, RenderSearchTemplateResponse, RenderSearchTemplateRequestBuilder> {
-
-    public static final RenderSearchTemplateAction INSTANCE = new RenderSearchTemplateAction();
-    public static final String NAME = "indices:admin/render/template/search";
-
-    public RenderSearchTemplateAction() {
-        super(NAME);
-    }
-
-    @Override
-    public RenderSearchTemplateRequestBuilder newRequestBuilder(ElasticsearchClient client) {
-        return new RenderSearchTemplateRequestBuilder(client, this);
-    }
-
-    @Override
-    public RenderSearchTemplateResponse newResponse() {
-        return new RenderSearchTemplateResponse();
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequest.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequest.java
deleted file mode 100644
index bde255f..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequest.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.ActionRequest;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.script.Template;
-
-import java.io.IOException;
-
-public class RenderSearchTemplateRequest extends ActionRequest<RenderSearchTemplateRequest> {
-
-    private Template template;
-    
-    public void template(Template template) {
-        this.template = template;
-    }
-    
-    public Template template() {
-        return template;
-    }
-    
-    @Override
-    public ActionRequestValidationException validate() {
-        ActionRequestValidationException exception = null;
-        if (template == null) {
-            exception = new ActionRequestValidationException();
-            exception.addValidationError("template must not be null");
-        }
-        return exception;
-    }
-    
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        boolean hasTemplate = template!= null;
-        out.writeBoolean(hasTemplate);
-        if (hasTemplate) {
-            template.writeTo(out);
-        }
-    }
-    
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        if (in.readBoolean()) {
-            template = Template.readTemplate(in);
-        }
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequestBuilder.java
deleted file mode 100644
index 493dc7e..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateRequestBuilder.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.ActionRequestBuilder;
-import org.elasticsearch.client.ElasticsearchClient;
-import org.elasticsearch.script.Template;
-
-public class RenderSearchTemplateRequestBuilder extends ActionRequestBuilder<RenderSearchTemplateRequest, RenderSearchTemplateResponse, RenderSearchTemplateRequestBuilder> {
-
-    public RenderSearchTemplateRequestBuilder(ElasticsearchClient client,
-            RenderSearchTemplateAction action) {
-        super(client, action, new RenderSearchTemplateRequest());
-    }
-    
-    public RenderSearchTemplateRequestBuilder template(Template template) {
-        request.template(template);
-        return this;
-    }
-    
-    public Template template() {
-        return request.template();
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateResponse.java
deleted file mode 100644
index 2d3ca01..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/RenderSearchTemplateResponse.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.ActionResponse;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-
-public class RenderSearchTemplateResponse extends ActionResponse implements ToXContent {
-
-    private BytesReference source;
-
-    public BytesReference source() {
-        return source;
-    }
-    
-    public void source(BytesReference source) {
-        this.source = source;
-    }
-    
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-        boolean hasSource = source != null;
-        out.writeBoolean(hasSource);
-        if (hasSource) {
-            out.writeBytesReference(source);
-        }
-    }
-    
-    @Override
-    public void readFrom(StreamInput in) throws IOException {
-        super.readFrom(in);
-        if (in.readBoolean()) {
-            source = in.readBytesReference();
-        }
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.rawField("template_output", source);
-        builder.endObject();
-        return builder;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java
deleted file mode 100644
index e9208ec..0000000
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/template/TransportRenderSearchTemplateAction.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.validate.template;
-
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.support.ActionFilters;
-import org.elasticsearch.action.support.HandledTransportAction;
-import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.util.concurrent.AbstractRunnable;
-import org.elasticsearch.script.ExecutableScript;
-import org.elasticsearch.script.ScriptContext;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.TransportService;
-
-public class TransportRenderSearchTemplateAction extends HandledTransportAction<RenderSearchTemplateRequest, RenderSearchTemplateResponse> {
-
-    private final ScriptService scriptService;
-
-    @Inject
-    public TransportRenderSearchTemplateAction(ScriptService scriptService, Settings settings, ThreadPool threadPool,
-            TransportService transportService, ActionFilters actionFilters, IndexNameExpressionResolver indexNameExpressionResolver) {
-        super(settings, RenderSearchTemplateAction.NAME, threadPool, transportService, actionFilters, indexNameExpressionResolver, RenderSearchTemplateRequest::new);
-        this.scriptService = scriptService;
-    }
-
-    @Override
-    protected void doExecute(final RenderSearchTemplateRequest request, final ActionListener<RenderSearchTemplateResponse> listener) {
-        threadPool.generic().execute(new AbstractRunnable() {
-
-            @Override
-            public void onFailure(Throwable t) {
-                listener.onFailure(t);
-            }
-
-            @Override
-            protected void doRun() throws Exception {
-                ExecutableScript executable = scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, request);
-                BytesReference processedTemplate = (BytesReference) executable.run();
-                RenderSearchTemplateResponse response = new RenderSearchTemplateResponse();
-                response.source(processedTemplate);
-                listener.onResponse(response);
-            }
-        });
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
index 57e0b74..3ed444c 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersResponse.java
@@ -20,8 +20,9 @@
 package org.elasticsearch.action.admin.indices.warmer.get;
 
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-
+import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -68,10 +69,7 @@ public class GetWarmersResponse extends ActionResponse {
             for (int j = 0; j < valueSize; j++) {
                 String name = in.readString();
                 String[] types = in.readStringArray();
-                IndexWarmersMetaData.SearchSource source = null;
-                if (in.readBoolean()) {
-                    source = new IndexWarmersMetaData.SearchSource(in);
-                }
+                BytesReference source = in.readBytesReference();
                 Boolean queryCache = null;
                 queryCache = in.readOptionalBoolean();
                 warmerEntryBuilder.add(new IndexWarmersMetaData.Entry(
@@ -96,11 +94,7 @@ public class GetWarmersResponse extends ActionResponse {
             for (IndexWarmersMetaData.Entry warmerEntry : indexEntry.value) {
                 out.writeString(warmerEntry.name());
                 out.writeStringArray(warmerEntry.types());
-                boolean hasWarmerSource = warmerEntry != null;
-                out.writeBoolean(hasWarmerSource);
-                if (hasWarmerSource) {
-                    warmerEntry.source().writeTo(out);
-                }
+                out.writeBytesReference(warmerEntry.source());
                 out.writeOptionalBoolean(warmerEntry.requestCache());
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
index d72be81..18246f6 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java
@@ -38,7 +38,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
@@ -115,9 +114,11 @@ public class TransportPutWarmerAction extends TransportMasterNodeAction<PutWarme
                         MetaData metaData = currentState.metaData();
                         String[] concreteIndices = indexNameExpressionResolver.concreteIndices(currentState, request.searchRequest().indicesOptions(), request.searchRequest().indices());
 
-                        IndexWarmersMetaData.SearchSource source = null;
-                        if (request.searchRequest().source() != null) {
-                            source = new IndexWarmersMetaData.SearchSource(request.searchRequest().source());
+                        BytesReference source = null;
+                        if (request.searchRequest().source() != null && request.searchRequest().source().length() > 0) {
+                            source = request.searchRequest().source();
+                        } else if (request.searchRequest().extraSource() != null && request.searchRequest().extraSource().length() > 0) {
+                            source = request.searchRequest().extraSource();
                         }
 
                         // now replace it on the metadata
diff --git a/core/src/main/java/org/elasticsearch/action/count/CountRequest.java b/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
index fd78687..05e193a 100644
--- a/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/count/CountRequest.java
@@ -19,21 +19,34 @@
 
 package org.elasticsearch.action.count;
 
+import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastRequest;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.Map;
+
+import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_AFTER;
 
 /**
  * A request to count the number of documents matching a specific query. Best created with
  * {@link org.elasticsearch.client.Requests#countRequest(String...)}.
+ * <p>
+ * The request requires the query source to be set either using {@link #source(QuerySourceBuilder)},
+ * or {@link #source(byte[])}.
  *
  * @see CountResponse
  * @see org.elasticsearch.client.Client#count(CountRequest)
@@ -41,15 +54,21 @@ import java.util.Arrays;
  */
 public class CountRequest extends BroadcastRequest<CountRequest> {
 
+    public static final float DEFAULT_MIN_SCORE = -1f;
+
+    private float minScore = DEFAULT_MIN_SCORE;
+
     @Nullable
     protected String routing;
 
     @Nullable
     private String preference;
 
+    private BytesReference source;
+
     private String[] types = Strings.EMPTY_ARRAY;
 
-    private SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
+    private int terminateAfter = DEFAULT_TERMINATE_AFTER;
 
     /**
      * Constructs a new count request against the provided indices. No indices provided means it will
@@ -57,14 +76,13 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
      */
     public CountRequest(String... indices) {
         super(indices);
-        searchSourceBuilder.size(0);
     }
 
     /**
      * The minimum score of the documents to include in the count.
      */
-    public Float minScore() {
-        return searchSourceBuilder.minScore();
+    public float minScore() {
+        return minScore;
     }
 
     /**
@@ -72,16 +90,69 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
      * documents will be included in the count.
      */
     public CountRequest minScore(float minScore) {
-        this.searchSourceBuilder.minScore(minScore);
+        this.minScore = minScore;
+        return this;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public BytesReference source() {
+        return source;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public CountRequest source(QuerySourceBuilder sourceBuilder) {
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    /**
+     * The source to execute in the form of a map.
+     */
+    @SuppressWarnings("unchecked")
+    public CountRequest source(Map querySource) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(querySource);
+            return source(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + querySource + "]", e);
+        }
+    }
+
+    public CountRequest source(XContentBuilder builder) {
+        this.source = builder.bytes();
+        return this;
+    }
+
+    /**
+     * The source to execute. It is preferable to use either {@link #source(byte[])}
+     * or {@link #source(QuerySourceBuilder)}.
+     */
+    public CountRequest source(String querySource) {
+        this.source = new BytesArray(querySource);
         return this;
     }
 
+    /**
+     * The source to execute.
+     */
+    public CountRequest source(byte[] querySource) {
+        return source(querySource, 0, querySource.length);
+    }
 
     /**
-     * The query to execute
+     * The source to execute.
      */
-    public CountRequest query(QueryBuilder<?> queryBuilder) {
-        this.searchSourceBuilder.query(queryBuilder);
+    public CountRequest source(byte[] querySource, int offset, int length) {
+        return source(new BytesArray(querySource, offset, length));
+    }
+
+    public CountRequest source(BytesReference querySource) {
+        this.source = querySource;
         return this;
     }
 
@@ -136,12 +207,15 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
      * Upon reaching <code>terminateAfter</code> counts, the count request will early terminate
      */
     public CountRequest terminateAfter(int terminateAfterCount) {
-        this.searchSourceBuilder.terminateAfter(terminateAfterCount);
+        if (terminateAfterCount <= 0) {
+            throw new IllegalArgumentException("terminateAfter must be > 0");
+        }
+        this.terminateAfter = terminateAfterCount;
         return this;
     }
 
     public int terminateAfter() {
-        return this.searchSourceBuilder.terminateAfter();
+        return this.terminateAfter;
     }
 
     @Override
@@ -156,20 +230,31 @@ public class CountRequest extends BroadcastRequest<CountRequest> {
 
     @Override
     public String toString() {
-        return "count request indices:" + Arrays.toString(indices) +
-                ", types:" + Arrays.toString(types) +
-                ", routing: " + routing +
-                ", preference: " + preference +
-                ", source:" + searchSourceBuilder.toString();
+        String sSource = "_na_";
+        try {
+            sSource = XContentHelper.convertToJson(source, false);
+        } catch (Exception e) {
+            // ignore
+        }
+        return "[" + Arrays.toString(indices) + "]" + Arrays.toString(types) + ", source[" + sSource + "]";
     }
 
     public SearchRequest toSearchRequest() {
         SearchRequest searchRequest = new SearchRequest(indices());
-        searchRequest.source(searchSourceBuilder);
         searchRequest.indicesOptions(indicesOptions());
         searchRequest.types(types());
         searchRequest.routing(routing());
         searchRequest.preference(preference());
+        searchRequest.source(source());
+        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
+        searchSourceBuilder.size(0);
+        if (minScore() != DEFAULT_MIN_SCORE) {
+            searchSourceBuilder.minScore(minScore());
+        }
+        if (terminateAfter() != DEFAULT_TERMINATE_AFTER) {
+            searchSourceBuilder.terminateAfter(terminateAfter());
+        }
+        searchRequest.extraSource(searchSourceBuilder);
         return searchRequest;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
index 2f5d914..54c60e5 100644
--- a/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java
@@ -19,12 +19,17 @@
 
 package org.elasticsearch.action.count;
 
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.search.SearchAction;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.support.DelegatingActionListener;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastOperationRequestBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilder;
 
 /**
@@ -32,6 +37,8 @@ import org.elasticsearch.index.query.QueryBuilder;
  */
 public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountRequest, CountResponse, CountRequestBuilder> {
 
+    private QuerySourceBuilder sourceBuilder;
+
     public CountRequestBuilder(ElasticsearchClient client, CountAction action) {
         super(client, action, new CountRequest());
     }
@@ -82,9 +89,43 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
 
     /**
      * The query source to execute.
+     *
+     * @see org.elasticsearch.index.query.QueryBuilders
+     */
+    public CountRequestBuilder setQuery(QueryBuilder queryBuilder) {
+        sourceBuilder().setQuery(queryBuilder);
+        return this;
+    }
+
+    /**
+     * The query binary to execute
+     */
+    public CountRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().setQuery(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new builder with a raw search query.
+     */
+    public CountRequestBuilder setQuery(XContentBuilder query) {
+        return setQuery(query.bytes());
+    }
+
+
+    /**
+     * The source to execute.
+     */
+    public CountRequestBuilder setSource(BytesReference source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The query source to execute.
      */
-    public CountRequestBuilder setQuery(QueryBuilder<?> builder) {
-        request.query(builder);
+    public CountRequestBuilder setSource(byte[] querySource) {
+        request.source(querySource);
         return this;
     }
 
@@ -94,6 +135,21 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
     }
 
     @Override
+    protected CountRequest beforeExecute(CountRequest request) {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder);
+        }
+        return request;
+    }
+
+    private QuerySourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new QuerySourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
+    @Override
     public void execute(ActionListener<CountResponse> listener) {
         CountRequest countRequest = beforeExecute(request);
         client.execute(SearchAction.INSTANCE, countRequest.toSearchRequest(), new DelegatingActionListener<SearchResponse, CountResponse>(listener) {
@@ -106,6 +162,16 @@ public class CountRequestBuilder extends BroadcastOperationRequestBuilder<CountR
 
     @Override
     public String toString() {
-        return request.toString();
+        if (sourceBuilder != null) {
+            return sourceBuilder.toString();
+        }
+        if (request.source() != null) {
+            try {
+                return XContentHelper.convertToJson(request.source().toBytesArray(), false, true);
+            } catch (Exception e) {
+                return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}";
+            }
+        }
+        return new QuerySourceBuilder().toString();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
index 3cc6f06..46f998f 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
@@ -174,7 +174,12 @@ public class TransportExistsAction extends TransportBroadcastAction<ExistsReques
             }
             context.preProcess();
             try {
-                boolean exists = Lucene.exists(context, context.query(), Lucene.createExistsCollector());
+                boolean exists;
+                try {
+                    exists = Lucene.exists(context.searcher(), context.query());
+                } finally {
+                    context.clearReleasables(SearchContext.Lifetime.COLLECTION);
+                }
                 return new ShardExistsResponse(request.shardId(), exists);
             } catch (Exception e) {
                 throw new QueryPhaseExecutionException(context, "failed to execute exists", e);
diff --git a/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java b/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
index ffe4763..3540daa 100644
--- a/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
+++ b/core/src/main/java/org/elasticsearch/action/search/ClearScrollResponse.java
@@ -19,12 +19,12 @@
 
 package org.elasticsearch.action.search;
 
-import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionResponse;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.StatusToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.rest.RestStatus;
 
 import java.io.IOException;
@@ -69,6 +69,8 @@ public class ClearScrollResponse extends ActionResponse implements StatusToXCont
 
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.field(Fields.SUCCEEDED, succeeded);
+        builder.field(Fields.NUMFREED, numFreed);
         return builder;
     }
 
@@ -85,4 +87,10 @@ public class ClearScrollResponse extends ActionResponse implements StatusToXCont
         out.writeBoolean(succeeded);
         out.writeVInt(numFreed);
     }
+
+    static final class Fields {
+        static final XContentBuilderString SUCCEEDED = new XContentBuilderString("succeeded");
+        static final XContentBuilderString NUMFREED = new XContentBuilderString("num_freed");
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java b/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
index a3236e9..d754d96 100644
--- a/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java
@@ -24,14 +24,22 @@ import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.CompositeIndicesRequest;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContent;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Map;
 
 import static org.elasticsearch.action.ValidateActions.addValidationError;
+import static org.elasticsearch.common.xcontent.support.XContentMapValues.*;
 
 /**
  * A multi search API request.
@@ -60,6 +68,107 @@ public class MultiSearchRequest extends ActionRequest<MultiSearchRequest> implem
         return this;
     }
 
+    public MultiSearchRequest add(byte[] data, int from, int length,
+            boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType) throws Exception {
+        return add(new BytesArray(data, from, length), isTemplateRequest, indices, types, searchType, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true);
+    }
+
+    public MultiSearchRequest add(BytesReference data, boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType, IndicesOptions indicesOptions) throws Exception {
+        return add(data, isTemplateRequest, indices, types, searchType, null, indicesOptions, true);
+    }
+
+    public MultiSearchRequest add(BytesReference data, boolean isTemplateRequest, @Nullable String[] indices, @Nullable String[] types, @Nullable String searchType, @Nullable String routing, IndicesOptions indicesOptions, boolean allowExplicitIndex) throws Exception {
+        XContent xContent = XContentFactory.xContent(data);
+        int from = 0;
+        int length = data.length();
+        byte marker = xContent.streamSeparator();
+        while (true) {
+            int nextMarker = findNextMarker(marker, from, data, length);
+            if (nextMarker == -1) {
+                break;
+            }
+            // support first line with \n
+            if (nextMarker == 0) {
+                from = nextMarker + 1;
+                continue;
+            }
+
+            SearchRequest searchRequest = new SearchRequest();
+            if (indices != null) {
+                searchRequest.indices(indices);
+            }
+            if (indicesOptions != null) {
+                searchRequest.indicesOptions(indicesOptions);
+            }
+            if (types != null && types.length > 0) {
+                searchRequest.types(types);
+            }
+            if (routing != null) {
+                searchRequest.routing(routing);
+            }
+            searchRequest.searchType(searchType);
+
+            IndicesOptions defaultOptions = IndicesOptions.strictExpandOpenAndForbidClosed();
+
+
+            // now parse the action
+            if (nextMarker - from > 0) {
+                try (XContentParser parser = xContent.createParser(data.slice(from, nextMarker - from))) {
+                    Map<String, Object> source = parser.map();
+                    for (Map.Entry<String, Object> entry : source.entrySet()) {
+                        Object value = entry.getValue();
+                        if ("index".equals(entry.getKey()) || "indices".equals(entry.getKey())) {
+                            if (!allowExplicitIndex) {
+                                throw new IllegalArgumentException("explicit index in multi percolate is not allowed");
+                            }
+                            searchRequest.indices(nodeStringArrayValue(value));
+                        } else if ("type".equals(entry.getKey()) || "types".equals(entry.getKey())) {
+                            searchRequest.types(nodeStringArrayValue(value));
+                        } else if ("search_type".equals(entry.getKey()) || "searchType".equals(entry.getKey())) {
+                            searchRequest.searchType(nodeStringValue(value, null));
+                        } else if ("request_cache".equals(entry.getKey()) || "requestCache".equals(entry.getKey())) {
+                            searchRequest.requestCache(nodeBooleanValue(value));
+                        } else if ("preference".equals(entry.getKey())) {
+                            searchRequest.preference(nodeStringValue(value, null));
+                        } else if ("routing".equals(entry.getKey())) {
+                            searchRequest.routing(nodeStringValue(value, null));
+                        }
+                    }
+                    defaultOptions = IndicesOptions.fromMap(source, defaultOptions);
+                }
+            }
+            searchRequest.indicesOptions(defaultOptions);
+
+            // move pointers
+            from = nextMarker + 1;
+            // now for the body
+            nextMarker = findNextMarker(marker, from, data, length);
+            if (nextMarker == -1) {
+                break;
+            }
+            if (isTemplateRequest) {
+                searchRequest.templateSource(data.slice(from,  nextMarker - from));
+            } else {
+                searchRequest.source(data.slice(from, nextMarker - from));
+            }
+            // move pointers
+            from = nextMarker + 1;
+
+            add(searchRequest);
+        }
+
+        return this;
+    }
+
+    private int findNextMarker(byte marker, int from, BytesReference data, int length) {
+        for (int i = from; i < length; i++) {
+            if (data.get(i) == marker) {
+                return i;
+            }
+        }
+        return -1;
+    }
+
     public List<SearchRequest> requests() {
         return this.requests;
     }
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
index 60f565b..9348185 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequest.java
@@ -19,21 +19,31 @@
 
 package org.elasticsearch.action.search;
 
+import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
+import java.util.Map;
 
 import static org.elasticsearch.search.Scroll.readScroll;
 
@@ -43,7 +53,9 @@ import static org.elasticsearch.search.Scroll.readScroll;
  * <p>
  * Note, the search {@link #source(org.elasticsearch.search.builder.SearchSourceBuilder)}
  * is required. The search source is the different search options, including aggregations and such.
- * </p>
+ * <p>
+ * There is an option to specify an addition search source using the {@link #extraSource(org.elasticsearch.search.builder.SearchSourceBuilder)}.
+ *
  * @see org.elasticsearch.client.Requests#searchRequest(String...)
  * @see org.elasticsearch.client.Client#search(SearchRequest)
  * @see SearchResponse
@@ -59,8 +71,12 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     @Nullable
     private String preference;
 
-    private SearchSourceBuilder source;
+    private BytesReference templateSource;
+    private Template template;
 
+    private BytesReference source;
+
+    private BytesReference extraSource;
     private Boolean requestCache;
 
     private Scroll scroll;
@@ -71,8 +87,6 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
 
     private IndicesOptions indicesOptions = DEFAULT_INDICES_OPTIONS;
 
-    private Template template;
-
     public SearchRequest() {
     }
 
@@ -86,8 +100,10 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
         this.indices = searchRequest.indices;
         this.routing = searchRequest.routing;
         this.preference = searchRequest.preference;
+        this.templateSource = searchRequest.templateSource;
         this.template = searchRequest.template;
         this.source = searchRequest.source;
+        this.extraSource = searchRequest.extraSource;
         this.requestCache = searchRequest.requestCache;
         this.scroll = searchRequest.scroll;
         this.types = searchRequest.types;
@@ -113,9 +129,9 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     /**
      * Constructs a new search request against the provided indices with the given search source.
      */
-    public SearchRequest(String[] indices, SearchSourceBuilder source) {
+    public SearchRequest(String[] indices, byte[] source) {
         indices(indices);
-        this.source = source;
+        this.source = new BytesArray(source);
     }
 
     @Override
@@ -231,17 +247,60 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
      * The source of the search request.
      */
     public SearchRequest source(SearchSourceBuilder sourceBuilder) {
-        this.source = sourceBuilder;
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    /**
+     * The search source to execute.
+     */
+    public SearchRequest source(BytesReference source) {
+        this.source = source;
         return this;
     }
 
+
     /**
      * The search source to execute.
      */
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return source;
     }
 
+    /**
+     * The search source template to execute.
+     */
+    public BytesReference templateSource() {
+        return templateSource;
+    }
+
+    /**
+     * Allows to provide additional source that will be used as well.
+     */
+    public SearchRequest extraSource(SearchSourceBuilder sourceBuilder) {
+        if (sourceBuilder == null) {
+            extraSource = null;
+            return this;
+        }
+        this.extraSource = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    /**
+     * Allows to provide template as source.
+     */
+    public SearchRequest templateSource(BytesReference template) {
+        this.templateSource = template;
+        return this;
+    }
+
+    /**
+     * The template of the search request.
+     */
+    public SearchRequest templateSource(String template) {
+        this.templateSource = new BytesArray(template);
+        return this;
+    }
 
     /**
      * The stored template
@@ -258,6 +317,88 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
     }
 
     /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateName(String templateName) {
+        updateOrCreateScript(templateName, null, null, null);
+    }
+
+    /**
+     * The type of the stored template
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateType(ScriptService.ScriptType templateType) {
+        updateOrCreateScript(null, templateType, null, null);
+    }
+
+    /**
+     * Template parameters used for rendering
+     * 
+     * @deprecated use {@link #template(Template)} instead.
+     */
+    @Deprecated
+    public void templateParams(Map<String, Object> params) {
+        updateOrCreateScript(null, null, null, params);
+    }
+
+    /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public String templateName() {
+        return template == null ? null : template.getScript();
+    }
+
+    /**
+     * The name of the stored template
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public ScriptService.ScriptType templateType() {
+        return template == null ? null : template.getType();
+    }
+
+    /**
+     * Template parameters used for rendering
+     * 
+     * @deprecated use {@link #template()} instead.
+     */
+    @Deprecated
+    public Map<String, Object> templateParams() {
+        return template == null ? null : template.getParams();
+    }
+
+    private void updateOrCreateScript(String templateContent, ScriptType type, String lang, Map<String, Object> params) {
+        Template template = template();
+        if (template == null) {
+            template = new Template(templateContent == null ? "" : templateContent, type == null ? ScriptType.INLINE : type, lang, null,
+                    params);
+        } else {
+            String newTemplateContent = templateContent == null ? template.getScript() : templateContent;
+            ScriptType newTemplateType = type == null ? template.getType() : type;
+            String newTemplateLang = lang == null ? template.getLang() : lang;
+            Map<String, Object> newTemplateParams = params == null ? template.getParams() : params;
+            template = new Template(newTemplateContent, newTemplateType, MustacheScriptEngineService.NAME, null, newTemplateParams);
+        }
+        template(template);
+    }
+
+    /**
+     * Additional search source to execute.
+     */
+    public BytesReference extraSource() {
+        return this.extraSource;
+    }
+
+    /**
      * The tye of search to execute.
      */
     public SearchType searchType() {
@@ -331,15 +472,18 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
         if (in.readBoolean()) {
             scroll = readScroll(in);
         }
-        if (in.readBoolean()) {
-            source = SearchSourceBuilder.readSearchSourceFrom(in);
-        }
+
+        source = in.readBytesReference();
+        extraSource = in.readBytesReference();
 
         types = in.readStringArray();
         indicesOptions = IndicesOptions.readIndicesOptions(in);
 
+        templateSource = in.readBytesReference();
+        if (in.readBoolean()) {
+            template = Template.readTemplate(in);
+        }
         requestCache = in.readOptionalBoolean();
-        template = in.readOptionalStreamable(new Template());
     }
 
     @Override
@@ -361,15 +505,18 @@ public class SearchRequest extends ActionRequest<SearchRequest> implements Indic
             out.writeBoolean(true);
             scroll.writeTo(out);
         }
-        if (source == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            source.writeTo(out);
-        }
+        out.writeBytesReference(source);
+        out.writeBytesReference(extraSource);
         out.writeStringArray(types);
         indicesOptions.writeIndicesOptions(out);
+
+        out.writeBytesReference(templateSource);
+        boolean hasTemplate = template != null;
+        out.writeBoolean(hasTemplate);
+        if (hasTemplate) {
+            template.writeTo(out);
+        }
+
         out.writeOptionalBoolean(requestCache);
-        out.writeOptionalStreamable(template);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
index 2e3084c..a570080 100644
--- a/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java
@@ -19,13 +19,18 @@
 
 package org.elasticsearch.action.search;
 
+import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionRequestBuilder;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.client.ElasticsearchClient;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
@@ -37,14 +42,15 @@ import org.elasticsearch.search.sort.SortBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.search.suggest.SuggestBuilder;
 
-import java.util.Arrays;
-import java.util.List;
+import java.util.Map;
 
 /**
  * A search action request builder.
  */
 public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, SearchResponse, SearchRequestBuilder> {
 
+    private SearchSourceBuilder sourceBuilder;
+
     public SearchRequestBuilder(ElasticsearchClient client, SearchAction action) {
         super(client, action, new SearchRequest());
     }
@@ -117,6 +123,14 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
+     * An optional timeout to control how long search is allowed to take.
+     */
+    public SearchRequestBuilder setTimeout(String timeout) {
+        sourceBuilder().timeout(timeout);
+        return this;
+    }
+
+    /**
      * An optional document count, upon collecting which the search
      * query will early terminate
      */
@@ -166,16 +180,118 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      *
      * @see org.elasticsearch.index.query.QueryBuilders
      */
-    public SearchRequestBuilder setQuery(QueryBuilder<?> queryBuilder) {
+    public SearchRequestBuilder setQuery(QueryBuilder queryBuilder) {
         sourceBuilder().query(queryBuilder);
         return this;
     }
 
     /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(String query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().query(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(byte[] queryBinary) {
+        sourceBuilder().query(queryBinary);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(byte[] queryBinary, int queryBinaryOffset, int queryBinaryLength) {
+        sourceBuilder().query(queryBinary, queryBinaryOffset, queryBinaryLength);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(XContentBuilder query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchRequestBuilder setQuery(Map query) {
+        sourceBuilder().query(query);
+        return this;
+    }
+
+    /**
      * Sets a filter that will be executed after the query has been executed and only has affect on the search hits
      * (not aggregations). This filter is always executed as last filtering mechanism.
      */
-    public SearchRequestBuilder setPostFilter(QueryBuilder<?> postFilter) {
+    public SearchRequestBuilder setPostFilter(QueryBuilder postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(String postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(BytesReference postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(byte[] postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(byte[] postFilter, int postFilterOffset, int postFilterLength) {
+        sourceBuilder().postFilter(postFilter, postFilterOffset, postFilterLength);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(XContentBuilder postFilter) {
+        sourceBuilder().postFilter(postFilter);
+        return this;
+    }
+
+    /**
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchRequestBuilder setPostFilter(Map postFilter) {
         sourceBuilder().postFilter(postFilter);
         return this;
     }
@@ -237,14 +353,6 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      * The stats groups this request will be aggregated under.
      */
     public SearchRequestBuilder setStats(String... statsGroups) {
-        sourceBuilder().stats(Arrays.asList(statsGroups));
-        return this;
-    }
-
-    /**
-     * The stats groups this request will be aggregated under.
-     */
-    public SearchRequestBuilder setStats(List<String> statsGroups) {
         sourceBuilder().stats(statsGroups);
         return this;
     }
@@ -357,7 +465,7 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
      * the source of the document will be returned.
      */
     public SearchRequestBuilder addFields(String... fields) {
-        sourceBuilder().fields(Arrays.asList(fields));
+        sourceBuilder().fields(fields);
         return this;
     }
 
@@ -369,23 +477,267 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
-    public SearchRequestBuilder highlighter(HighlightBuilder highlightBuilder) {
-        sourceBuilder().highlighter(highlightBuilder);
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(BytesReference aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(byte[] aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(byte[] aggregations, int aggregationsOffset, int aggregationsLength) {
+        sourceBuilder().aggregations(aggregations, aggregationsOffset, aggregationsLength);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(XContentBuilder aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent) binary representation of addAggregation to use.
+     */
+    public SearchRequestBuilder setAggregations(Map aggregations) {
+        sourceBuilder().aggregations(aggregations);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with default fragment size of 100 characters, and
+     * default number of fragments of 5.
+     *
+     * @param name The field to highlight
+     */
+    public SearchRequestBuilder addHighlightedField(String name) {
+        highlightBuilder().field(name);
+        return this;
+    }
+
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * default number of fragments of 5.
+     *
+     * @param name         The field to highlight
+     * @param fragmentSize The size of a fragment in characters
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize) {
+        highlightBuilder().field(name, fragmentSize);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * a provided (maximum) number of fragments.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters),
+     * a provided (maximum) number of fragments and an offset for the highlight.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public SearchRequestBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                                    int fragmentOffset) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+        return this;
+    }
+
+    /**
+     * Adds a highlighted field.
+     */
+    public SearchRequestBuilder addHighlightedField(HighlightBuilder.Field field) {
+        highlightBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+     * are <tt>styled</tt> and <tt>default</tt>.
+     *
+     * @param schemaName The tag scheme name
+     */
+    public SearchRequestBuilder setHighlighterTagsSchema(String schemaName) {
+        highlightBuilder().tagsSchema(schemaName);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterFragmentSize(Integer fragmentSize) {
+        highlightBuilder().fragmentSize(fragmentSize);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterNumOfFragments(Integer numOfFragments) {
+        highlightBuilder().numOfFragments(numOfFragments);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterFilter(Boolean highlightFilter) {
+        highlightBuilder().highlightFilter(highlightFilter);
+        return this;
+    }
+
+    /**
+     * The encoder to set for highlighting
+     */
+    public SearchRequestBuilder setHighlighterEncoder(String encoder) {
+        highlightBuilder().encoder(encoder);
+        return this;
+    }
+
+    /**
+     * Explicitly set the pre tags that will be used for highlighting.
+     */
+    public SearchRequestBuilder setHighlighterPreTags(String... preTags) {
+        highlightBuilder().preTags(preTags);
+        return this;
+    }
+
+    /**
+     * Explicitly set the post tags that will be used for highlighting.
+     */
+    public SearchRequestBuilder setHighlighterPostTags(String... postTags) {
+        highlightBuilder().postTags(postTags);
+        return this;
+    }
+
+    /**
+     * The order of fragments per field. By default, ordered by the order in the
+     * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+     * by score of the fragments.
+     */
+    public SearchRequestBuilder setHighlighterOrder(String order) {
+        highlightBuilder().order(order);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+        highlightBuilder().requireFieldMatch(requireFieldMatch);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+        highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterBoundaryChars(char[] boundaryChars) {
+        highlightBuilder().boundaryChars(boundaryChars);
+        return this;
+    }
+
+    /**
+     * The highlighter type to use.
+     */
+    public SearchRequestBuilder setHighlighterType(String type) {
+        highlightBuilder().highlighterType(type);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterFragmenter(String fragmenter) {
+        highlightBuilder().fragmenter(fragmenter);
+        return this;
+    }
+
+    /**
+     * Sets a query to be used for highlighting all fields instead of the search query.
+     */
+    public SearchRequestBuilder setHighlighterQuery(QueryBuilder highlightQuery) {
+        highlightBuilder().highlightQuery(highlightQuery);
         return this;
     }
 
     /**
-     * Delegates to
-     * {@link org.elasticsearch.search.suggest.SuggestBuilder#addSuggestion(org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder)}
-     * .
+     * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+     * highlight and the field doesn't also define noMatchSize.
+     *
+     * @param noMatchSize integer to set or null to leave out of request.  default is null.
+     * @return this builder for chaining
      */
-    public SearchRequestBuilder suggest(SuggestBuilder suggestBuilder) {
-        sourceBuilder().suggest(suggestBuilder);
+    public SearchRequestBuilder setHighlighterNoMatchSize(Integer noMatchSize) {
+        highlightBuilder().noMatchSize(noMatchSize);
         return this;
     }
 
-    public SearchRequestBuilder innerHits(InnerHitsBuilder innerHitsBuilder) {
-        sourceBuilder().innerHits(innerHitsBuilder);
+    /**
+     * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+     */
+    public SearchRequestBuilder setHighlighterPhraseLimit(Integer phraseLimit) {
+        highlightBuilder().phraseLimit(phraseLimit);
+        return this;
+    }
+
+    public SearchRequestBuilder setHighlighterOptions(Map<String, Object> options) {
+        highlightBuilder().options(options);
+        return this;
+    }
+
+    /**
+     * Forces to highlight fields based on the source even if fields are stored separately.
+     */
+    public SearchRequestBuilder setHighlighterForceSource(Boolean forceSource) {
+        highlightBuilder().forceSource(forceSource);
+        return this;
+    }
+
+    /**
+     * Send the fields to be highlighted using a syntax that is specific about the order in which they should be highlighted.
+     *
+     * @return this for chaining
+     */
+    public SearchRequestBuilder setHighlighterExplicitFieldOrder(boolean explicitFieldOrder) {
+        highlightBuilder().useExplicitFieldOrder(explicitFieldOrder);
+        return this;
+    }
+
+    public SearchRequestBuilder addParentChildInnerHits(String name, String type,  InnerHitsBuilder.InnerHit innerHit) {
+        innerHitsBuilder().addParentChildInnerHits(name, type, innerHit);
+        return this;
+    }
+
+    public SearchRequestBuilder addNestedInnerHits(String name, String path,  InnerHitsBuilder.InnerHit innerHit) {
+        innerHitsBuilder().addNestedInnerHits(name, path, innerHit);
+        return this;
+    }
+
+    /**
+     * Delegates to {@link org.elasticsearch.search.suggest.SuggestBuilder#setText(String)}.
+     */
+    public SearchRequestBuilder setSuggestText(String globalText) {
+        suggestBuilder().setText(globalText);
+        return this;
+    }
+
+    /**
+     * Delegates to {@link org.elasticsearch.search.suggest.SuggestBuilder#addSuggestion(org.elasticsearch.search.suggest.SuggestBuilder.SuggestionBuilder)}.
+     */
+    public SearchRequestBuilder addSuggestion(SuggestBuilder.SuggestionBuilder<?> suggestion) {
+        suggestBuilder().addSuggestion(suggestion);
         return this;
     }
 
@@ -448,7 +800,20 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
-     * Sets the source of the request as a SearchSourceBuilder.
+     * Sets the rescore window for all rescorers that don't specify a window when added.
+     *
+     * @param window rescore window
+     * @return this for chaining
+     */
+    public SearchRequestBuilder setRescoreWindow(int window) {
+        sourceBuilder().defaultRescoreWindowSize(window);
+        return this;
+    }
+
+    /**
+     * Sets the source of the request as a SearchSourceBuilder. Note, settings anything other
+     * than the search type will cause this source to be overridden, consider using
+     * {@link #setExtraSource(SearchSourceBuilder)} instead.
      */
     public SearchRequestBuilder setSource(SearchSourceBuilder source) {
         request.source(source);
@@ -456,6 +821,26 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
     }
 
     /**
+     * Sets the source of the request as a json string. Note, settings anything other
+     * than the search type will cause this source to be overridden, consider using
+     * {@link #setExtraSource(SearchSourceBuilder)} instead.
+     */
+    public SearchRequestBuilder setSource(BytesReference source) {
+        request.source(source);
+        return this;
+    }
+
+    /**
+     * Sets the an addtional source of the request as a SearchSourceBuilder. All values and
+     * settings set on the extra source will override the corresponding settings on the specified
+     * source.
+     */
+    public SearchRequestBuilder setExtraSource(SearchSourceBuilder source) {
+        request.extraSource(source);
+        return this;
+    }
+
+    /**
      * template stuff
      */
     public SearchRequestBuilder setTemplate(Template template) {
@@ -463,6 +848,16 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
+    public SearchRequestBuilder setTemplateSource(String source) {
+        request.templateSource(source);
+        return this;
+    }
+
+    public SearchRequestBuilder setTemplateSource(BytesReference source) {
+        request.templateSource(source);
+        return this;
+    }
+
     /**
      * Sets if this request should use the request cache or not, assuming that it can (for
      * example, if "now" is used, it will never be cached). By default (not set, or null,
@@ -473,18 +868,70 @@ public class SearchRequestBuilder extends ActionRequestBuilder<SearchRequest, Se
         return this;
     }
 
+    /**
+     * Sets the source builder to be used with this request. Note, any operations done
+     * on this require builder before are discarded as this internal builder replaces
+     * what has been built up until this point.
+     */
+    public SearchRequestBuilder internalBuilder(SearchSourceBuilder sourceBuilder) {
+        this.sourceBuilder = sourceBuilder;
+        return this;
+    }
+
+    /**
+     * Returns the internal search source builder used to construct the request.
+     */
+    public SearchSourceBuilder internalBuilder() {
+        return sourceBuilder();
+    }
+
     @Override
     public String toString() {
+        if (sourceBuilder != null) {
+            return sourceBuilder.toString();
+        }
         if (request.source() != null) {
-            return request.source().toString();
+            try {
+                return XContentHelper.convertToJson(request.source().toBytesArray(), false, true);
+            } catch (Exception e) {
+                return "{ \"error\" : \"" + ExceptionsHelper.detailedMessage(e) + "\"}";
+            }
         }
         return new SearchSourceBuilder().toString();
     }
 
+    @Override
+    public SearchRequest request() {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder());
+        }
+        return request;
+    }
+
+    @Override
+    protected SearchRequest beforeExecute(SearchRequest request) {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder());
+        }
+        return request;
+    }
+
     private SearchSourceBuilder sourceBuilder() {
-        if (request.source() == null) {
-            request.source(new SearchSourceBuilder());
+        if (sourceBuilder == null) {
+            sourceBuilder = new SearchSourceBuilder();
         }
-        return request.source();
+        return sourceBuilder;
+    }
+
+    private HighlightBuilder highlightBuilder() {
+        return sourceBuilder().highlighter();
+    }
+
+    private InnerHitsBuilder innerHitsBuilder() {
+        return sourceBuilder().innerHitsBuilder();
+    }
+
+    private SuggestBuilder suggestBuilder() {
+        return sourceBuilder().suggest();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java b/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java
index e4472c7..2b99b6f 100644
--- a/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java
+++ b/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.action.search.type;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.elasticsearch.action.search.SearchRequest;
@@ -38,6 +37,8 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+
 /**
  *
  */
@@ -112,7 +113,7 @@ public abstract class TransportSearchHelper {
         Map<String, String> attributes;
         int attributesSize = Integer.parseInt(elements[index++]);
         if (attributesSize == 0) {
-            attributes = ImmutableMap.of();
+            attributes = emptyMap();
         } else {
             attributes = new HashMap<>(attributesSize);
             for (int i = 0; i < attributesSize; i++) {
diff --git a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
index b21227f..86d575d 100644
--- a/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
+++ b/core/src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java
@@ -27,6 +27,7 @@ import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.broadcast.BroadcastRequest;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -103,8 +104,10 @@ public class DfsOnlyRequest extends BroadcastRequest<DfsOnlyRequest> {
     @Override
     public String toString() {
         String sSource = "_na_";
-        if (searchRequest.source() != null) {
-            sSource = searchRequest.source().toString();
+        try {
+            sSource = XContentHelper.convertToJson(searchRequest.source(), false);
+        } catch (IOException e) {
+            // ignore
         }
         return "[" + Arrays.toString(indices) + "]" + Arrays.toString(types()) + ", source[" + sSource + "]";
     }
diff --git a/core/src/main/java/org/elasticsearch/client/Client.java b/core/src/main/java/org/elasticsearch/client/Client.java
index eafac2b..6e0b0b2 100644
--- a/core/src/main/java/org/elasticsearch/client/Client.java
+++ b/core/src/main/java/org/elasticsearch/client/Client.java
@@ -21,9 +21,6 @@ package org.elasticsearch.client;
 
 import org.elasticsearch.action.ActionFuture;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequest;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequestBuilder;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.action.bulk.BulkRequest;
 import org.elasticsearch.action.bulk.BulkRequestBuilder;
 import org.elasticsearch.action.bulk.BulkResponse;
diff --git a/core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java b/core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java
index c3eb515..1be22b2 100644
--- a/core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java
+++ b/core/src/main/java/org/elasticsearch/client/ClusterAdminClient.java
@@ -77,6 +77,9 @@ import org.elasticsearch.action.admin.cluster.stats.ClusterStatsResponse;
 import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksRequest;
 import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksRequestBuilder;
 import org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksResponse;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequest;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequestBuilder;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
 
 /**
  * Administrative actions/operations against indices.
@@ -423,4 +426,25 @@ public interface ClusterAdminClient extends ElasticsearchClient {
      */
     SnapshotsStatusRequestBuilder prepareSnapshotStatus();
 
+
+    /**
+     * Return the rendered search request for a given search template.
+     *
+     * @param request The request
+     * @return The result future
+     */
+    ActionFuture<RenderSearchTemplateResponse> renderSearchTemplate(RenderSearchTemplateRequest request);
+
+    /**
+     * Return the rendered search request for a given search template.
+     *
+     * @param request  The request
+     * @param listener A listener to be notified of the result
+     */
+    void renderSearchTemplate(RenderSearchTemplateRequest request, ActionListener<RenderSearchTemplateResponse> listener);
+
+    /**
+     * Return the rendered search request for a given search template.
+     */
+    RenderSearchTemplateRequestBuilder prepareRenderSearchTemplate();
 }
diff --git a/core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java b/core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java
index 755bf33..75cae17 100644
--- a/core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java
+++ b/core/src/main/java/org/elasticsearch/client/IndicesAdminClient.java
@@ -105,9 +105,6 @@ import org.elasticsearch.action.admin.indices.upgrade.post.UpgradeResponse;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequest;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequestBuilder;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryResponse;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequest;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequestBuilder;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerRequest;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerRequestBuilder;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerResponse;
@@ -747,27 +744,6 @@ public interface IndicesAdminClient extends ElasticsearchClient {
     ValidateQueryRequestBuilder prepareValidateQuery(String... indices);
 
     /**
-     * Return the rendered search request for a given search template.
-     *
-     * @param request The request
-     * @return The result future
-     */
-    ActionFuture<RenderSearchTemplateResponse> renderSearchTemplate(RenderSearchTemplateRequest request);
-
-    /**
-     * Return the rendered search request for a given search template.
-     *
-     * @param request  The request
-     * @param listener A listener to be notified of the result
-     */
-    void renderSearchTemplate(RenderSearchTemplateRequest request, ActionListener<RenderSearchTemplateResponse> listener);
-
-    /**
-     * Return the rendered search request for a given search template.
-     */
-    RenderSearchTemplateRequestBuilder prepareRenderSearchTemplate();
-
-    /**
      * Puts an index search warmer to be applies when applicable.
      */
     ActionFuture<PutWarmerResponse> putWarmer(PutWarmerRequest request);
diff --git a/core/src/main/java/org/elasticsearch/client/node/NodeClient.java b/core/src/main/java/org/elasticsearch/client/node/NodeClient.java
index deb3e5b..7493887 100644
--- a/core/src/main/java/org/elasticsearch/client/node/NodeClient.java
+++ b/core/src/main/java/org/elasticsearch/client/node/NodeClient.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.client.node;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.*;
 import org.elasticsearch.action.support.TransportAction;
 import org.elasticsearch.client.support.AbstractClient;
@@ -30,17 +29,19 @@ import org.elasticsearch.threadpool.ThreadPool;
 
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
 public class NodeClient extends AbstractClient {
 
-    private final ImmutableMap<GenericAction, TransportAction> actions;
+    private final Map<GenericAction, TransportAction> actions;
 
     @Inject
     public NodeClient(Settings settings, ThreadPool threadPool, Headers headers, Map<GenericAction, TransportAction> actions) {
         super(settings, threadPool, headers);
-        this.actions = ImmutableMap.copyOf(actions);
+        this.actions = unmodifiableMap(actions);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java b/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
index f9abf2f..3fa5d78 100644
--- a/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
+++ b/core/src/main/java/org/elasticsearch/client/support/AbstractClient.java
@@ -208,10 +208,10 @@ import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryAction
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequest;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequestBuilder;
 import org.elasticsearch.action.admin.indices.validate.query.ValidateQueryResponse;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateAction;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequest;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequestBuilder;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateAction;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequest;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequestBuilder;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerAction;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerRequest;
 import org.elasticsearch.action.admin.indices.warmer.delete.DeleteWarmerRequestBuilder;
@@ -1142,6 +1142,21 @@ public abstract class AbstractClient extends AbstractComponent implements Client
         public SnapshotsStatusRequestBuilder prepareSnapshotStatus() {
             return new SnapshotsStatusRequestBuilder(this, SnapshotsStatusAction.INSTANCE);
         }
+
+        @Override
+        public ActionFuture<RenderSearchTemplateResponse> renderSearchTemplate(final RenderSearchTemplateRequest request) {
+            return execute(RenderSearchTemplateAction.INSTANCE, request);
+        }
+
+        @Override
+        public void renderSearchTemplate(final RenderSearchTemplateRequest request, final ActionListener<RenderSearchTemplateResponse> listener) {
+            execute(RenderSearchTemplateAction.INSTANCE, request, listener);
+        }
+
+        @Override
+        public RenderSearchTemplateRequestBuilder prepareRenderSearchTemplate() {
+            return new RenderSearchTemplateRequestBuilder(this, RenderSearchTemplateAction.INSTANCE);
+        }
     }
 
     static class IndicesAdmin implements IndicesAdminClient {
@@ -1618,21 +1633,6 @@ public abstract class AbstractClient extends AbstractComponent implements Client
         }
 
         @Override
-        public ActionFuture<RenderSearchTemplateResponse> renderSearchTemplate(final RenderSearchTemplateRequest request) {
-            return execute(RenderSearchTemplateAction.INSTANCE, request);
-        }
-
-        @Override
-        public void renderSearchTemplate(final RenderSearchTemplateRequest request, final ActionListener<RenderSearchTemplateResponse> listener) {
-            execute(RenderSearchTemplateAction.INSTANCE, request, listener);
-        }
-
-        @Override
-        public RenderSearchTemplateRequestBuilder prepareRenderSearchTemplate() {
-            return new RenderSearchTemplateRequestBuilder(this, RenderSearchTemplateAction.INSTANCE);
-        }
-
-        @Override
         public ActionFuture<PutWarmerResponse> putWarmer(PutWarmerRequest request) {
             return execute(PutWarmerAction.INSTANCE, request);
         }
diff --git a/core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java b/core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java
index 89b3a04..9008764 100644
--- a/core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java
+++ b/core/src/main/java/org/elasticsearch/client/transport/support/TransportProxyClient.java
@@ -19,35 +19,42 @@
 
 package org.elasticsearch.client.transport.support;
 
-import com.google.common.collect.ImmutableMap;
-import org.elasticsearch.action.*;
+import org.elasticsearch.action.Action;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionRequest;
+import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.action.ActionResponse;
+import org.elasticsearch.action.GenericAction;
+import org.elasticsearch.action.TransportActionNodeProxy;
 import org.elasticsearch.client.transport.TransportClientNodesService;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.transport.TransportService;
 
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
 public class TransportProxyClient {
 
     private final TransportClientNodesService nodesService;
-    private final ImmutableMap<Action, TransportActionNodeProxy> proxies;
+    private final Map<Action, TransportActionNodeProxy> proxies;
 
     @Inject
     public TransportProxyClient(Settings settings, TransportService transportService, TransportClientNodesService nodesService, Map<String, GenericAction> actions) {
         this.nodesService = nodesService;
-        MapBuilder<Action, TransportActionNodeProxy> actionsBuilder = new MapBuilder<>();
+        Map<Action, TransportActionNodeProxy> proxies = new HashMap<>();
         for (GenericAction action : actions.values()) {
             if (action instanceof Action) {
-                actionsBuilder.put((Action) action, new TransportActionNodeProxy(settings, action, transportService));
+                proxies.put((Action) action, new TransportActionNodeProxy(settings, action, transportService));
             }
         }
-        this.proxies = actionsBuilder.immutableMap();
+        this.proxies = unmodifiableMap(proxies);
     }
 
     public <Request extends ActionRequest, Response extends ActionResponse, RequestBuilder extends ActionRequestBuilder<Request, Response, RequestBuilder>> void execute(final Action<Request, Response, RequestBuilder> action, final Request request, ActionListener<Response> listener) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java b/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
index f10a40b..265ab0f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterInfo.java
@@ -20,9 +20,7 @@
 package org.elasticsearch.cluster;
 
 import org.elasticsearch.cluster.routing.ShardRouting;
-
-import java.util.Collections;
-import java.util.Map;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 
 /**
  * ClusterInfo is an object representing a map of nodes to {@link DiskUsage}
@@ -31,15 +29,14 @@ import java.util.Map;
  * for the key used in the shardSizes map
  */
 public class ClusterInfo {
-
-    private final Map<String, DiskUsage> leastAvailableSpaceUsage;
-    private final Map<String, DiskUsage> mostAvailableSpaceUsage;
-    final Map<String, Long> shardSizes;
+    private final ImmutableOpenMap<String, DiskUsage> leastAvailableSpaceUsage;
+    private final ImmutableOpenMap<String, DiskUsage> mostAvailableSpaceUsage;
+    final ImmutableOpenMap<String, Long> shardSizes;
     public static final ClusterInfo EMPTY = new ClusterInfo();
-    private final Map<ShardRouting, String> routingToDataPath;
+    private final ImmutableOpenMap<ShardRouting, String> routingToDataPath;
 
     protected ClusterInfo() {
-       this(Collections.EMPTY_MAP, Collections.EMPTY_MAP, Collections.EMPTY_MAP, Collections.EMPTY_MAP);
+       this(ImmutableOpenMap.of(), ImmutableOpenMap.of(), ImmutableOpenMap.of(), ImmutableOpenMap.of());
     }
 
     /**
@@ -51,7 +48,9 @@ public class ClusterInfo {
      * @param routingToDataPath the shard routing to datapath mapping
      * @see #shardIdentifierFromRouting
      */
-    public ClusterInfo(final Map<String, DiskUsage> leastAvailableSpaceUsage, final Map<String, DiskUsage> mostAvailableSpaceUsage, final Map<String, Long> shardSizes, Map<ShardRouting, String> routingToDataPath) {
+    public ClusterInfo(ImmutableOpenMap<String, DiskUsage> leastAvailableSpaceUsage,
+            ImmutableOpenMap<String, DiskUsage> mostAvailableSpaceUsage, ImmutableOpenMap<String, Long> shardSizes,
+            ImmutableOpenMap<ShardRouting, String> routingToDataPath) {
         this.leastAvailableSpaceUsage = leastAvailableSpaceUsage;
         this.shardSizes = shardSizes;
         this.mostAvailableSpaceUsage = mostAvailableSpaceUsage;
@@ -61,14 +60,14 @@ public class ClusterInfo {
     /**
      * Returns a node id to disk usage mapping for the path that has the least available space on the node.
      */
-    public Map<String, DiskUsage> getNodeLeastAvailableDiskUsages() {
+    public ImmutableOpenMap<String, DiskUsage> getNodeLeastAvailableDiskUsages() {
         return this.leastAvailableSpaceUsage;
     }
 
     /**
      * Returns a node id to disk usage mapping for the path that has the most available space on the node.
      */
-    public Map<String, DiskUsage> getNodeMostAvailableDiskUsages() {
+    public ImmutableOpenMap<String, DiskUsage> getNodeMostAvailableDiskUsages() {
         return this.mostAvailableSpaceUsage;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
index 8167ecc..b1bdf52 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterState.java
@@ -389,9 +389,9 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
 
             if (!blocks().indices().isEmpty()) {
                 builder.startObject("indices");
-                for (Map.Entry<String, Set<ClusterBlock>> entry : blocks().indices().entrySet()) {
-                    builder.startObject(entry.getKey());
-                    for (ClusterBlock block : entry.getValue()) {
+                for (ObjectObjectCursor<String, Set<ClusterBlock>> entry : blocks().indices()) {
+                    builder.startObject(entry.key);
+                    for (ClusterBlock block : entry.value) {
                         block.toXContent(builder, params);
                     }
                     builder.endObject();
@@ -591,10 +591,6 @@ public class ClusterState implements ToXContent, Diffable<ClusterState> {
             return this;
         }
 
-        public Builder routingTable(RoutingTable.Builder routingTable) {
-            return routingTable(routingTable.build());
-        }
-
         public Builder routingResult(RoutingAllocation.Result routingResult) {
             this.routingTable = routingResult.routingTable();
             return this;
diff --git a/core/src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java b/core/src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java
index 89a0e91..cc65106 100644
--- a/core/src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.settings.Settings;
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
index 019e245..039868d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java
@@ -33,6 +33,7 @@ import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
@@ -44,10 +45,7 @@ import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.ReceiveTimeoutTransportException;
 
-import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
@@ -70,10 +68,10 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
 
     private volatile TimeValue updateFrequency;
 
-    private volatile Map<String, DiskUsage> leastAvailableSpaceUsages;
-    private volatile Map<String, DiskUsage> mostAvailableSpaceUsages;
-    private volatile Map<ShardRouting, String> shardRoutingToDataPath;
-    private volatile Map<String, Long> shardSizes;
+    private volatile ImmutableOpenMap<String, DiskUsage> leastAvailableSpaceUsages;
+    private volatile ImmutableOpenMap<String, DiskUsage> mostAvailableSpaceUsages;
+    private volatile ImmutableOpenMap<ShardRouting, String> shardRoutingToDataPath;
+    private volatile ImmutableOpenMap<String, Long> shardSizes;
     private volatile boolean isMaster = false;
     private volatile boolean enabled;
     private volatile TimeValue fetchTimeout;
@@ -89,10 +87,10 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
                                       TransportIndicesStatsAction transportIndicesStatsAction, ClusterService clusterService,
                                       ThreadPool threadPool) {
         super(settings);
-        this.leastAvailableSpaceUsages = Collections.emptyMap();
-        this.mostAvailableSpaceUsages = Collections.emptyMap();
-        this.shardRoutingToDataPath = Collections.emptyMap();
-        this.shardSizes = Collections.emptyMap();
+        this.leastAvailableSpaceUsages = ImmutableOpenMap.of();
+        this.mostAvailableSpaceUsages = ImmutableOpenMap.of();
+        this.shardRoutingToDataPath = ImmutableOpenMap.of();
+        this.shardSizes = ImmutableOpenMap.of();
         this.transportNodesStatsAction = transportNodesStatsAction;
         this.transportIndicesStatsAction = transportIndicesStatsAction;
         this.clusterService = clusterService;
@@ -198,14 +196,14 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
                         logger.trace("Removing node from cluster info: {}", removedNode.getId());
                     }
                     if (leastAvailableSpaceUsages.containsKey(removedNode.getId())) {
-                        Map<String, DiskUsage> newMaxUsages = new HashMap<>(leastAvailableSpaceUsages);
+                        ImmutableOpenMap.Builder<String, DiskUsage> newMaxUsages = ImmutableOpenMap.builder(leastAvailableSpaceUsages);
                         newMaxUsages.remove(removedNode.getId());
-                        leastAvailableSpaceUsages = Collections.unmodifiableMap(newMaxUsages);
+                        leastAvailableSpaceUsages = newMaxUsages.build();
                     }
                     if (mostAvailableSpaceUsages.containsKey(removedNode.getId())) {
-                        Map<String, DiskUsage> newMinUsages = new HashMap<>(mostAvailableSpaceUsages);
+                        ImmutableOpenMap.Builder<String, DiskUsage> newMinUsages = ImmutableOpenMap.builder(mostAvailableSpaceUsages);
                         newMinUsages.remove(removedNode.getId());
-                        mostAvailableSpaceUsages = Collections.unmodifiableMap(newMinUsages);
+                        mostAvailableSpaceUsages = newMinUsages.build();
                     }
                 }
             }
@@ -309,11 +307,11 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
         final CountDownLatch nodeLatch = updateNodeStats(new ActionListener<NodesStatsResponse>() {
             @Override
             public void onResponse(NodesStatsResponse nodeStatses) {
-                Map<String, DiskUsage> newLeastAvaiableUsages = new HashMap<>();
-                Map<String, DiskUsage> newMostAvaiableUsages = new HashMap<>();
+                ImmutableOpenMap.Builder<String, DiskUsage> newLeastAvaiableUsages = ImmutableOpenMap.builder();
+                ImmutableOpenMap.Builder<String, DiskUsage> newMostAvaiableUsages = ImmutableOpenMap.builder();
                 fillDiskUsagePerNode(logger, nodeStatses.getNodes(), newLeastAvaiableUsages, newMostAvaiableUsages);
-                leastAvailableSpaceUsages = Collections.unmodifiableMap(newLeastAvaiableUsages);
-                mostAvailableSpaceUsages = Collections.unmodifiableMap(newMostAvaiableUsages);
+                leastAvailableSpaceUsages = newLeastAvaiableUsages.build();
+                mostAvailableSpaceUsages = newMostAvaiableUsages.build();
             }
 
             @Override
@@ -329,8 +327,8 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
                         logger.warn("Failed to execute NodeStatsAction for ClusterInfoUpdateJob", e);
                     }
                     // we empty the usages list, to be safe - we don't know what's going on.
-                    leastAvailableSpaceUsages = Collections.emptyMap();
-                    mostAvailableSpaceUsages = Collections.emptyMap();
+                    leastAvailableSpaceUsages = ImmutableOpenMap.of();
+                    mostAvailableSpaceUsages = ImmutableOpenMap.of();
                 }
             }
         });
@@ -339,11 +337,11 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
             @Override
             public void onResponse(IndicesStatsResponse indicesStatsResponse) {
                 ShardStats[] stats = indicesStatsResponse.getShards();
-                final HashMap<String, Long> newShardSizes = new HashMap<>();
-                final HashMap<ShardRouting, String> newShardRoutingToDataPath = new HashMap<>();
+                ImmutableOpenMap.Builder<String, Long> newShardSizes = ImmutableOpenMap.builder();
+                ImmutableOpenMap.Builder<ShardRouting, String> newShardRoutingToDataPath = ImmutableOpenMap.builder();
                 buildShardLevelInfo(logger, stats, newShardSizes, newShardRoutingToDataPath);
-                shardSizes = Collections.unmodifiableMap(newShardSizes);
-                shardRoutingToDataPath = Collections.unmodifiableMap(newShardRoutingToDataPath);
+                shardSizes = newShardSizes.build();
+                shardRoutingToDataPath = newShardRoutingToDataPath.build();
             }
 
             @Override
@@ -359,8 +357,8 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
                         logger.warn("Failed to execute IndicesStatsAction for ClusterInfoUpdateJob", e);
                     }
                     // we empty the usages list, to be safe - we don't know what's going on.
-                    shardSizes = Collections.emptyMap();
-                    shardRoutingToDataPath = Collections.emptyMap();
+                    shardSizes = ImmutableOpenMap.of();
+                    shardRoutingToDataPath = ImmutableOpenMap.of();
                 }
             }
         });
@@ -389,7 +387,8 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
         return clusterInfo;
     }
 
-    static void buildShardLevelInfo(ESLogger logger, ShardStats[] stats, HashMap<String, Long> newShardSizes, HashMap<ShardRouting, String> newShardRoutingToDataPath) {
+    static void buildShardLevelInfo(ESLogger logger, ShardStats[] stats, ImmutableOpenMap.Builder<String, Long> newShardSizes,
+            ImmutableOpenMap.Builder<ShardRouting, String> newShardRoutingToDataPath) {
         for (ShardStats s : stats) {
             newShardRoutingToDataPath.put(s.getShardRouting(), s.getDataPath());
             long size = s.getStats().getStore().sizeInBytes();
@@ -401,7 +400,9 @@ public class InternalClusterInfoService extends AbstractComponent implements Clu
         }
     }
 
-    static void fillDiskUsagePerNode(ESLogger logger, NodeStats[] nodeStatsArray, Map<String, DiskUsage> newLeastAvaiableUsages, Map<String, DiskUsage> newMostAvaiableUsages) {
+    static void fillDiskUsagePerNode(ESLogger logger, NodeStats[] nodeStatsArray,
+            ImmutableOpenMap.Builder<String, DiskUsage> newLeastAvaiableUsages,
+            ImmutableOpenMap.Builder<String, DiskUsage> newMostAvaiableUsages) {
         for (NodeStats nodeStats : nodeStatsArray) {
             if (nodeStats.getFs() == null) {
                 logger.warn("Unable to retrieve node FS stats for {}", nodeStats.getNode().name());
diff --git a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
index 82ba28d..dd7eb9f 100644
--- a/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java
@@ -19,9 +19,11 @@
 
 package org.elasticsearch.cluster;
 
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.cluster.ClusterState.Custom;
 import org.elasticsearch.cluster.metadata.SnapshotId;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -33,7 +35,6 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 
 /**
  * Meta data about restore processes that are currently executing
@@ -112,7 +113,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
     public static class Entry {
         private final State state;
         private final SnapshotId snapshotId;
-        private final Map<ShardId, ShardRestoreStatus> shards;
+        private final ImmutableOpenMap<ShardId, ShardRestoreStatus> shards;
         private final List<String> indices;
 
         /**
@@ -121,14 +122,14 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          * @param snapshotId snapshot id
          * @param state      current state of the restore process
          * @param indices    list of indices being restored
-         * @param shards     list of shards being restored and thier current restore status
+         * @param shards     map of shards being restored to their current restore status
          */
-        public Entry(SnapshotId snapshotId, State state, List<String> indices, ImmutableMap<ShardId, ShardRestoreStatus> shards) {
+        public Entry(SnapshotId snapshotId, State state, List<String> indices, ImmutableOpenMap<ShardId, ShardRestoreStatus> shards) {
             this.snapshotId = snapshotId;
             this.state = state;
             this.indices = indices;
             if (shards == null) {
-                this.shards = ImmutableMap.of();
+                this.shards = ImmutableOpenMap.of();
             } else {
                 this.shards = shards;
             }
@@ -148,7 +149,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
          *
          * @return list of shards
          */
-        public Map<ShardId, ShardRestoreStatus> shards() {
+        public ImmutableOpenMap<ShardId, ShardRestoreStatus> shards() {
             return this.shards;
         }
 
@@ -416,7 +417,7 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
             for (int j = 0; j < indices; j++) {
                 indexBuilder.add(in.readString());
             }
-            ImmutableMap.Builder<ShardId, ShardRestoreStatus> builder = ImmutableMap.<ShardId, ShardRestoreStatus>builder();
+            ImmutableOpenMap.Builder<ShardId, ShardRestoreStatus> builder = ImmutableOpenMap.builder();
             int shards = in.readVInt();
             for (int j = 0; j < shards; j++) {
                 ShardId shardId = ShardId.readShardId(in);
@@ -442,9 +443,9 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
                 out.writeString(index);
             }
             out.writeVInt(entry.shards().size());
-            for (Map.Entry<ShardId, ShardRestoreStatus> shardEntry : entry.shards().entrySet()) {
-                shardEntry.getKey().writeTo(out);
-                shardEntry.getValue().writeTo(out);
+            for (ObjectObjectCursor<ShardId, ShardRestoreStatus> shardEntry : entry.shards()) {
+                shardEntry.key.writeTo(out);
+                shardEntry.value.writeTo(out);
             }
         }
     }
@@ -483,9 +484,9 @@ public class RestoreInProgress extends AbstractDiffable<Custom> implements Custo
         builder.endArray();
         builder.startArray("shards");
         {
-            for (Map.Entry<ShardId, ShardRestoreStatus> shardEntry : entry.shards.entrySet()) {
-                ShardId shardId = shardEntry.getKey();
-                ShardRestoreStatus status = shardEntry.getValue();
+            for (ObjectObjectCursor<ShardId, ShardRestoreStatus> shardEntry : entry.shards) {
+                ShardId shardId = shardEntry.key;
+                ShardRestoreStatus status = shardEntry.value;
                 builder.startObject();
                 {
                     builder.field("index", shardId.getIndex());
diff --git a/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java b/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
index 83c663a..821ab3c 100644
--- a/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
+++ b/core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java
@@ -19,9 +19,13 @@
 
 package org.elasticsearch.cluster;
 
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.ObjectContainer;
+import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.cluster.ClusterState.Custom;
 import org.elasticsearch.cluster.metadata.SnapshotId;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -32,14 +36,11 @@ import org.elasticsearch.index.shard.ShardId;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Meta data about snapshots that are currently executing
  */
@@ -69,31 +70,31 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
         private final State state;
         private final SnapshotId snapshotId;
         private final boolean includeGlobalState;
-        private final Map<ShardId, ShardSnapshotStatus> shards;
+        private final ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards;
         private final List<String> indices;
-        private final Map<String, List<ShardId>> waitingIndices;
+        private final ImmutableOpenMap<String, List<ShardId>> waitingIndices;
         private final long startTime;
 
-        public Entry(SnapshotId snapshotId, boolean includeGlobalState, State state, List<String> indices, long startTime, Map<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(SnapshotId snapshotId, boolean includeGlobalState, State state, List<String> indices, long startTime, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
             this.state = state;
             this.snapshotId = snapshotId;
             this.includeGlobalState = includeGlobalState;
             this.indices = indices;
             this.startTime = startTime;
             if (shards == null) {
-                this.shards = ImmutableMap.of();
-                this.waitingIndices = ImmutableMap.of();
+                this.shards = ImmutableOpenMap.of();
+                this.waitingIndices = ImmutableOpenMap.of();
             } else {
-                this.shards = unmodifiableMap(shards);
+                this.shards = shards;
                 this.waitingIndices = findWaitingIndices(shards);
             }
         }
 
-        public Entry(Entry entry, State state, Map<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(Entry entry, State state, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
             this(entry.snapshotId, entry.includeGlobalState, state, entry.indices, entry.startTime, shards);
         }
 
-        public Entry(Entry entry, Map<ShardId, ShardSnapshotStatus> shards) {
+        public Entry(Entry entry, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
             this(entry, entry.state, shards);
         }
 
@@ -101,7 +102,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return this.snapshotId;
         }
 
-        public Map<ShardId, ShardSnapshotStatus> shards() {
+        public ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards() {
             return this.shards;
         }
 
@@ -113,7 +114,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return indices;
         }
 
-        public Map<String, List<ShardId>> waitingIndices() {
+        public ImmutableOpenMap<String, List<ShardId>> waitingIndices() {
             return waitingIndices;
         }
 
@@ -155,28 +156,26 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             return result;
         }
 
-        private ImmutableMap<String, List<ShardId>> findWaitingIndices(Map<ShardId, ShardSnapshotStatus> shards) {
+        private ImmutableOpenMap<String, List<ShardId>> findWaitingIndices(ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards) {
             Map<String, List<ShardId>> waitingIndicesMap = new HashMap<>();
-            for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> entry : shards.entrySet()) {
-                if (entry.getValue().state() == State.WAITING) {
-                    List<ShardId> waitingShards = waitingIndicesMap.get(entry.getKey().getIndex());
+            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> entry : shards) {
+                if (entry.value.state() == State.WAITING) {
+                    List<ShardId> waitingShards = waitingIndicesMap.get(entry.key.getIndex());
                     if (waitingShards == null) {
                         waitingShards = new ArrayList<>();
-                        waitingIndicesMap.put(entry.getKey().getIndex(), waitingShards);
+                        waitingIndicesMap.put(entry.key.getIndex(), waitingShards);
                     }
-                    waitingShards.add(entry.getKey());
+                    waitingShards.add(entry.key);
                 }
             }
-            if (!waitingIndicesMap.isEmpty()) {
-                ImmutableMap.Builder<String, List<ShardId>> waitingIndicesBuilder = ImmutableMap.builder();
-                for (Map.Entry<String, List<ShardId>> entry : waitingIndicesMap.entrySet()) {
-                    waitingIndicesBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
-                }
-                return waitingIndicesBuilder.build();
-            } else {
-                return ImmutableMap.of();
+            if (waitingIndicesMap.isEmpty()) {
+                return ImmutableOpenMap.of();
             }
-
+            ImmutableOpenMap.Builder<String, List<ShardId>> waitingIndicesBuilder = ImmutableOpenMap.builder();
+            for (Map.Entry<String, List<ShardId>> entry : waitingIndicesMap.entrySet()) {
+                waitingIndicesBuilder.put(entry.getKey(), Collections.unmodifiableList(entry.getValue()));
+            }
+            return waitingIndicesBuilder.build();
         }
 
     }
@@ -187,9 +186,9 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
      * @param shards list of shard statuses
      * @return true if all shards have completed (either successfully or failed), false otherwise
      */
-    public static boolean completed(Collection<ShardSnapshotStatus> shards) {
-        for (ShardSnapshotStatus status : shards) {
-            if (status.state().completed() == false) {
+    public static boolean completed(ObjectContainer<ShardSnapshotStatus> shards) {
+        for (ObjectCursor<ShardSnapshotStatus> status : shards) {
+            if (status.value.state().completed() == false) {
                 return false;
             }
         }
@@ -369,7 +368,7 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
                 indexBuilder.add(in.readString());
             }
             long startTime = in.readLong();
-            ImmutableMap.Builder<ShardId, ShardSnapshotStatus> builder = ImmutableMap.builder();
+            ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> builder = ImmutableOpenMap.builder();
             int shards = in.readVInt();
             for (int j = 0; j < shards; j++) {
                 ShardId shardId = ShardId.readShardId(in);
@@ -395,10 +394,10 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
             }
             out.writeLong(entry.startTime());
             out.writeVInt(entry.shards().size());
-            for (Map.Entry<ShardId, ShardSnapshotStatus> shardEntry : entry.shards().entrySet()) {
-                shardEntry.getKey().writeTo(out);
-                out.writeOptionalString(shardEntry.getValue().nodeId());
-                out.writeByte(shardEntry.getValue().state().value());
+            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : entry.shards()) {
+                shardEntry.key.writeTo(out);
+                out.writeOptionalString(shardEntry.value.nodeId());
+                out.writeByte(shardEntry.value.state().value());
             }
         }
     }
@@ -444,9 +443,9 @@ public class SnapshotsInProgress extends AbstractDiffable<Custom> implements Cus
         builder.timeValueField(Fields.START_TIME_MILLIS, Fields.START_TIME, entry.startTime());
         builder.startArray(Fields.SHARDS);
         {
-            for (Map.Entry<ShardId, ShardSnapshotStatus> shardEntry : entry.shards.entrySet()) {
-                ShardId shardId = shardEntry.getKey();
-                ShardSnapshotStatus status = shardEntry.getValue();
+            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : entry.shards) {
+                ShardId shardId = shardEntry.key;
+                ShardSnapshotStatus status = shardEntry.value;
                 builder.startObject();
                 {
                     builder.field(Fields.INDEX, shardId.getIndex());
diff --git a/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java b/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
index ab5609c..cfe88af 100644
--- a/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
+++ b/core/src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java
@@ -19,11 +19,12 @@
 
 package org.elasticsearch.cluster.block;
 
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaDataIndexStateService;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.rest.RestStatus;
@@ -37,7 +38,6 @@ import java.util.function.Function;
 import java.util.function.Predicate;
 import java.util.stream.Stream;
 
-import static java.util.Collections.emptyMap;
 import static java.util.Collections.emptySet;
 import static java.util.Collections.unmodifiableSet;
 import static java.util.stream.Collectors.toSet;
@@ -47,17 +47,17 @@ import static java.util.stream.Stream.concat;
  * Represents current cluster level blocks to block dirty operations done against the cluster.
  */
 public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
-    public static final ClusterBlocks EMPTY_CLUSTER_BLOCK = new ClusterBlocks(emptySet(), emptyMap());
+    public static final ClusterBlocks EMPTY_CLUSTER_BLOCK = new ClusterBlocks(emptySet(), ImmutableOpenMap.of());
 
     public static final ClusterBlocks PROTO = EMPTY_CLUSTER_BLOCK;
 
     private final Set<ClusterBlock> global;
 
-    private final Map<String, Set<ClusterBlock>> indicesBlocks;
+    private final ImmutableOpenMap<String, Set<ClusterBlock>> indicesBlocks;
 
     private final ImmutableLevelHolder[] levelHolders;
 
-    ClusterBlocks(Set<ClusterBlock> global, Map<String, Set<ClusterBlock>> indicesBlocks) {
+    ClusterBlocks(Set<ClusterBlock> global, ImmutableOpenMap<String, Set<ClusterBlock>> indicesBlocks) {
         this.global = global;
         this.indicesBlocks = indicesBlocks;
 
@@ -68,9 +68,9 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
                     .filter(containsLevel)
                     .collect(toSet()));
 
-            ImmutableMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableMap.builder();
-            for (Map.Entry<String, Set<ClusterBlock>> entry : indicesBlocks.entrySet()) {
-                indicesBuilder.put(entry.getKey(), unmodifiableSet(entry.getValue().stream()
+            ImmutableOpenMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableOpenMap.builder();
+            for (ObjectObjectCursor<String, Set<ClusterBlock>> entry : indicesBlocks) {
+                indicesBuilder.put(entry.key, unmodifiableSet(entry.value.stream()
                         .filter(containsLevel)
                         .collect(toSet())));
             }
@@ -83,7 +83,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
         return global;
     }
 
-    public Map<String, Set<ClusterBlock>> indices() {
+    public ImmutableOpenMap<String, Set<ClusterBlock>> indices() {
         return indicesBlocks;
     }
 
@@ -91,7 +91,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
         return levelHolders[level.id()].global();
     }
 
-    public Map<String, Set<ClusterBlock>> indices(ClusterBlockLevel level) {
+    public ImmutableOpenMap<String, Set<ClusterBlock>> indices(ClusterBlockLevel level) {
         return levelHolders[level.id()].indices();
     }
 
@@ -203,9 +203,9 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
     public void writeTo(StreamOutput out) throws IOException {
         writeBlockSet(global, out);
         out.writeVInt(indicesBlocks.size());
-        for (Map.Entry<String, Set<ClusterBlock>> entry : indicesBlocks.entrySet()) {
-            out.writeString(entry.getKey());
-            writeBlockSet(entry.getValue(), out);
+        for (ObjectObjectCursor<String, Set<ClusterBlock>> entry : indicesBlocks) {
+            out.writeString(entry.key);
+            writeBlockSet(entry.value, out);
         }
     }
 
@@ -219,8 +219,8 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
     @Override
     public ClusterBlocks readFrom(StreamInput in) throws IOException {
         Set<ClusterBlock> global = readBlockSet(in);
-        ImmutableMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableMap.builder();
         int size = in.readVInt();
+        ImmutableOpenMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableOpenMap.builder(size);
         for (int j = 0; j < size; j++) {
             indicesBuilder.put(in.readString().intern(), readBlockSet(in));
         }
@@ -238,12 +238,12 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
 
     static class ImmutableLevelHolder {
 
-        static final ImmutableLevelHolder EMPTY = new ImmutableLevelHolder(emptySet(), ImmutableMap.of());
+        static final ImmutableLevelHolder EMPTY = new ImmutableLevelHolder(emptySet(), ImmutableOpenMap.of());
 
         private final Set<ClusterBlock> global;
-        private final ImmutableMap<String, Set<ClusterBlock>> indices;
+        private final ImmutableOpenMap<String, Set<ClusterBlock>> indices;
 
-        ImmutableLevelHolder(Set<ClusterBlock> global, ImmutableMap<String, Set<ClusterBlock>> indices) {
+        ImmutableLevelHolder(Set<ClusterBlock> global, ImmutableOpenMap<String, Set<ClusterBlock>> indices) {
             this.global = global;
             this.indices = indices;
         }
@@ -252,7 +252,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
             return global;
         }
 
-        public ImmutableMap<String, Set<ClusterBlock>> indices() {
+        public ImmutableOpenMap<String, Set<ClusterBlock>> indices() {
             return indices;
         }
     }
@@ -272,11 +272,11 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
 
         public Builder blocks(ClusterBlocks blocks) {
             global.addAll(blocks.global());
-            for (Map.Entry<String, Set<ClusterBlock>> entry : blocks.indices().entrySet()) {
-                if (!indices.containsKey(entry.getKey())) {
-                    indices.put(entry.getKey(), new HashSet<>());
+            for (ObjectObjectCursor<String, Set<ClusterBlock>> entry : blocks.indices()) {
+                if (!indices.containsKey(entry.key)) {
+                    indices.put(entry.key, new HashSet<>());
                 }
-                indices.get(entry.getKey()).addAll(entry.getValue());
+                indices.get(entry.key).addAll(entry.value);
             }
             return this;
         }
@@ -339,7 +339,7 @@ public class ClusterBlocks extends AbstractDiffable<ClusterBlocks> {
 
         public ClusterBlocks build() {
             // We copy the block sets here in case of the builder is modified after build is called
-            ImmutableMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableMap.builder();
+            ImmutableOpenMap.Builder<String, Set<ClusterBlock>> indicesBuilder = ImmutableOpenMap.builder(indices.size());
             for (Map.Entry<String, Set<ClusterBlock>> entry : indices.entrySet()) {
                 indicesBuilder.put(entry.getKey(), unmodifiableSet(new HashSet<>(entry.getValue())));
             }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
index 6ea1d0e..9d11017 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java
@@ -21,7 +21,6 @@ package org.elasticsearch.cluster.metadata;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.Diff;
 import org.elasticsearch.cluster.Diffable;
@@ -29,8 +28,6 @@ import org.elasticsearch.cluster.DiffableUtils;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.node.DiscoveryNodeFilters;
-import org.elasticsearch.cluster.routing.HashFunction;
-import org.elasticsearch.cluster.routing.Murmur3HashFunction;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
@@ -167,16 +164,12 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
     public static final String SETTING_PRIORITY = "index.priority";
     public static final String SETTING_CREATION_DATE_STRING = "index.creation_date_string";
     public static final String SETTING_INDEX_UUID = "index.uuid";
-    public static final String SETTING_LEGACY_ROUTING_HASH_FUNCTION = "index.legacy.routing.hash.type";
-    public static final String SETTING_LEGACY_ROUTING_USE_TYPE = "index.legacy.routing.use_type";
     public static final String SETTING_DATA_PATH = "index.data_path";
     public static final String SETTING_SHARED_FS_ALLOW_RECOVERY_ON_ANY_NODE = "index.shared_filesystem.recover_on_any_node";
     public static final String INDEX_UUID_NA_VALUE = "_na_";
 
 
-    // hard-coded hash function as of 2.0
-    // older indices will read which hash function to use in their index settings
-    private static final HashFunction MURMUR3_HASH_FUNCTION = new Murmur3HashFunction();
+
 
     private final String index;
     private final long version;
@@ -200,8 +193,6 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
     private final Version indexCreatedVersion;
     private final Version indexUpgradedVersion;
     private final org.apache.lucene.util.Version minimumCompatibleLuceneVersion;
-    private final HashFunction routingHashFunction;
-    private final boolean useTypeForRouting;
 
     private IndexMetaData(String index, long version, State state, Settings settings, ImmutableOpenMap<String, MappingMetaData> mappings, ImmutableOpenMap<String, AliasMetaData> aliases, ImmutableOpenMap<String, Custom> customs) {
         if (settings.getAsInt(SETTING_NUMBER_OF_SHARDS, null) == null) {
@@ -249,23 +240,6 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
         } else {
             this.minimumCompatibleLuceneVersion = null;
         }
-        final String hashFunction = settings.get(SETTING_LEGACY_ROUTING_HASH_FUNCTION);
-        if (hashFunction == null) {
-            routingHashFunction = MURMUR3_HASH_FUNCTION;
-        } else {
-            final Class<? extends HashFunction> hashFunctionClass;
-            try {
-                hashFunctionClass = Class.forName(hashFunction).asSubclass(HashFunction.class);
-            } catch (ClassNotFoundException|NoClassDefFoundError e) {
-                throw new ElasticsearchException("failed to load custom hash function [" + hashFunction + "]", e);
-            }
-            try {
-                routingHashFunction = hashFunctionClass.newInstance();
-            } catch (InstantiationException | IllegalAccessException e) {
-                throw new IllegalStateException("Cannot instantiate hash function", e);
-            }
-        }
-        useTypeForRouting = settings.getAsBoolean(SETTING_LEGACY_ROUTING_USE_TYPE, false);
     }
 
     public String index() {
@@ -335,29 +309,6 @@ public class IndexMetaData implements Diffable<IndexMetaData>, FromXContentBuild
         return minimumCompatibleLuceneVersion;
     }
 
-    /**
-     * Return the {@link HashFunction} that should be used for routing.
-     */
-    public HashFunction routingHashFunction() {
-        return routingHashFunction;
-    }
-
-    public HashFunction getRoutingHashFunction() {
-        return routingHashFunction();
-    }
-
-    /**
-     * Return whether routing should use the _type in addition to the _id in
-     * order to decide which shard a document should go to.
-     */
-    public boolean routingUseType() {
-        return useTypeForRouting;
-    }
-
-    public boolean getRoutingUseType() {
-        return routingUseType();
-    }
-
     public long creationDate() {
         return settings.getAsLong(SETTING_CREATION_DATE, -1l);
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
index 99ce095..64492d8 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
@@ -21,7 +21,7 @@ package org.elasticsearch.cluster.metadata;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-import java.nio.charset.StandardCharsets;
+
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
@@ -76,6 +76,7 @@ import org.joda.time.DateTimeZone;
 import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.UnsupportedEncodingException;
+import java.nio.charset.StandardCharsets;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
@@ -359,9 +360,9 @@ public class MetaDataCreateIndexService extends AbstractComponent {
                     Settings actualIndexSettings = indexSettingsBuilder.build();
 
                     // Set up everything, now locally create the index to see that things are ok, and apply
-
+                    final IndexMetaData tmpImd = IndexMetaData.builder(request.index()).settings(actualIndexSettings).build();
                     // create the index here (on the master) to validate it can be created, as well as adding the mapping
-                    indicesService.createIndex(request.index(), actualIndexSettings, clusterService.localNode().id());
+                    indicesService.createIndex(tmpImd);
                     indexCreated = true;
                     // now add the mappings
                     IndexService indexService = indicesService.indexServiceSafe(request.index());
@@ -462,7 +463,7 @@ public class MetaDataCreateIndexService extends AbstractComponent {
                     if (request.state() == State.OPEN) {
                         RoutingTable.Builder routingTableBuilder = RoutingTable.builder(updatedState.routingTable())
                                 .addAsNew(updatedState.metaData().index(request.index()));
-                        RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder).build());
+                        RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder.build()).build());
                         updatedState = ClusterState.builder(updatedState).routingResult(routingResult).build();
                     }
                     removalReason = "cleaning up after validating index on master";
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
index d7b2e47..88e1aad 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java
@@ -128,7 +128,7 @@ public class MetaDataDeleteIndexService extends AbstractComponent {
                         .build();
 
                 RoutingAllocation.Result routingResult = allocationService.reroute(
-                        ClusterState.builder(currentState).routingTable(routingTableBuilder).metaData(newMetaData).build());
+                        ClusterState.builder(currentState).routingTable(routingTableBuilder.build()).metaData(newMetaData).build());
 
                 ClusterBlocks blocks = ClusterBlocks.builder().blocks(currentState.blocks()).removeIndexBlocks(request.index).build();
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
index bbe88f4..82d7f55 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java
@@ -98,7 +98,7 @@ public class MetaDataIndexAliasesService extends AbstractComponent {
                                     if (indexService == null) {
                                         // temporarily create the index and add mappings so we can parse the filter
                                         try {
-                                            indexService = indicesService.createIndex(indexMetaData.index(), indexMetaData.settings(), clusterService.localNode().id());
+                                            indexService = indicesService.createIndex(indexMetaData);
                                             if (indexMetaData.mappings().containsKey(MapperService.DEFAULT_MAPPING)) {
                                                 indexService.mapperService().merge(MapperService.DEFAULT_MAPPING, indexMetaData.mappings().get(MapperService.DEFAULT_MAPPING).source(), false, false);
                                             }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java
index b5b3cb6..e4452e4 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java
@@ -124,7 +124,7 @@ public class MetaDataIndexStateService extends AbstractComponent {
                     rtBuilder.remove(index);
                 }
 
-                RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder).build());
+                RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder.build()).build());
                 //no explicit wait for other nodes needed as we use AckedClusterStateUpdateTask
                 return ClusterState.builder(updatedState).routingResult(routingResult).build();
             }
@@ -181,7 +181,7 @@ public class MetaDataIndexStateService extends AbstractComponent {
                     rtBuilder.addAsFromCloseToOpen(updatedState.metaData().index(index));
                 }
 
-                RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder).build());
+                RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder.build()).build());
                 //no explicit wait for other nodes needed as we use AckedClusterStateUpdateTask
                 return ClusterState.builder(updatedState).routingResult(routingResult).build();
             }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
index 6e29f31..cdde491 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeService.java
@@ -21,11 +21,7 @@ package org.elasticsearch.cluster.metadata;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.routing.DjbHashFunction;
-import org.elasticsearch.cluster.routing.HashFunction;
-import org.elasticsearch.cluster.routing.SimpleHashFunction;
 import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -35,7 +31,6 @@ import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.similarity.SimilarityService;
-import org.elasticsearch.index.store.IndexStoreModule;
 import org.elasticsearch.script.ScriptService;
 
 import java.util.Locale;
@@ -54,47 +49,12 @@ import static org.elasticsearch.common.util.set.Sets.newHashSet;
  */
 public class MetaDataIndexUpgradeService extends AbstractComponent {
 
-    private static final String DEPRECATED_SETTING_ROUTING_HASH_FUNCTION = "cluster.routing.operation.hash.type";
-    private static final String DEPRECATED_SETTING_ROUTING_USE_TYPE = "cluster.routing.operation.use_type";
-
-    private final Class<? extends HashFunction> pre20HashFunction;
-    private final Boolean pre20UseType;
     private final ScriptService scriptService;
 
     @Inject
     public MetaDataIndexUpgradeService(Settings settings, ScriptService scriptService) {
         super(settings);
         this.scriptService = scriptService;
-        final String pre20HashFunctionName = settings.get(DEPRECATED_SETTING_ROUTING_HASH_FUNCTION, null);
-        final boolean hasCustomPre20HashFunction = pre20HashFunctionName != null;
-        // the hash function package has changed we replace the two hash functions if their fully qualified name is used.
-        if (hasCustomPre20HashFunction) {
-            switch (pre20HashFunctionName) {
-                case "Simple":
-                case "simple":
-                case "org.elasticsearch.cluster.routing.operation.hash.simple.SimpleHashFunction":
-                    pre20HashFunction = SimpleHashFunction.class;
-                    break;
-                case "Djb":
-                case "djb":
-                case "org.elasticsearch.cluster.routing.operation.hash.djb.DjbHashFunction":
-                    pre20HashFunction = DjbHashFunction.class;
-                    break;
-                default:
-                    try {
-                        pre20HashFunction = Class.forName(pre20HashFunctionName).asSubclass(HashFunction.class);
-                    } catch (ClassNotFoundException|NoClassDefFoundError e) {
-                        throw new ElasticsearchException("failed to load custom hash function [" + pre20HashFunctionName + "]", e);
-                    }
-            }
-        } else {
-            pre20HashFunction = DjbHashFunction.class;
-        }
-        pre20UseType = settings.getAsBoolean(DEPRECATED_SETTING_ROUTING_USE_TYPE, null);
-        if (hasCustomPre20HashFunction || pre20UseType != null) {
-            logger.warn("Settings [{}] and [{}] are deprecated. Index settings from your old indices have been updated to record the fact that they "
-                    + "used some custom routing logic, you can now remove these settings from your `elasticsearch.yml` file", DEPRECATED_SETTING_ROUTING_HASH_FUNCTION, DEPRECATED_SETTING_ROUTING_USE_TYPE);
-        }
     }
 
     /**
@@ -110,68 +70,29 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
             return indexMetaData;
         }
         checkSupportedVersion(indexMetaData);
-        IndexMetaData newMetaData = upgradeLegacyRoutingSettings(indexMetaData);
+        IndexMetaData newMetaData = indexMetaData;
         newMetaData = addDefaultUnitsIfNeeded(newMetaData);
         checkMappingsCompatibility(newMetaData);
-        newMetaData = upgradeSettings(newMetaData);
         newMetaData = markAsUpgraded(newMetaData);
         return newMetaData;
     }
 
-    IndexMetaData upgradeSettings(IndexMetaData indexMetaData) {
-        final String storeType = indexMetaData.getSettings().get(IndexStoreModule.STORE_TYPE);
-        if (storeType != null) {
-            final String upgradeStoreType;
-            switch (storeType.toLowerCase(Locale.ROOT)) {
-                case "nio_fs":
-                case "niofs":
-                    upgradeStoreType = "niofs";
-                    break;
-                case "mmap_fs":
-                case "mmapfs":
-                    upgradeStoreType = "mmapfs";
-                    break;
-                case "simple_fs":
-                case "simplefs":
-                    upgradeStoreType = "simplefs";
-                    break;
-                case "default":
-                    upgradeStoreType = "default";
-                    break;
-                case "fs":
-                    upgradeStoreType = "fs";
-                    break;
-                default:
-                    upgradeStoreType = storeType;
-            }
-            if (storeType.equals(upgradeStoreType) == false) {
-                Settings indexSettings = Settings.builder().put(indexMetaData.settings())
-                        .put(IndexStoreModule.STORE_TYPE, upgradeStoreType)
-                        .build();
-                return IndexMetaData.builder(indexMetaData)
-                        .version(indexMetaData.version())
-                        .settings(indexSettings)
-                        .build();
-            }
-        }
-        return indexMetaData;
-    }
 
     /**
      * Checks if the index was already opened by this version of Elasticsearch and doesn't require any additional checks.
      */
     private boolean isUpgraded(IndexMetaData indexMetaData) {
-        return indexMetaData.upgradeVersion().onOrAfter(Version.V_2_0_0_beta1);
+        return indexMetaData.upgradeVersion().onOrAfter(Version.V_3_0_0);
     }
 
     /**
-     * Elasticsearch 2.0 no longer supports indices with pre Lucene v4.0 (Elasticsearch v 0.90.0) segments. All indices
-     * that were created before Elasticsearch v0.90.0 should be upgraded using upgrade plugin before they can
+     * Elasticsearch 3.0 no longer supports indices with pre Lucene v5.0 (Elasticsearch v2.0.0.beta1) segments. All indices
+     * that were created before Elasticsearch v2.0.0.beta1 should be upgraded using upgrade API before they can
      * be open by this version of elasticsearch.
      */
     private void checkSupportedVersion(IndexMetaData indexMetaData) {
         if (indexMetaData.getState() == IndexMetaData.State.OPEN && isSupportedVersion(indexMetaData) == false) {
-            throw new IllegalStateException("The index [" + indexMetaData.getIndex() + "] was created before v0.90.0 and wasn't upgraded."
+            throw new IllegalStateException("The index [" + indexMetaData.getIndex() + "] was created before v2.0.0.beta1 and wasn't upgraded."
                     + " This index should be open using a version before " + Version.CURRENT.minimumCompatibilityVersion()
                     + " and upgraded using the upgrade API.");
         }
@@ -181,44 +102,18 @@ public class MetaDataIndexUpgradeService extends AbstractComponent {
      * Returns true if this index can be supported by the current version of elasticsearch
      */
     private static boolean isSupportedVersion(IndexMetaData indexMetaData) {
-        if (indexMetaData.creationVersion().onOrAfter(Version.V_0_90_0_Beta1)) {
-            // The index was created with elasticsearch that was using Lucene 4.0
+        if (indexMetaData.creationVersion().onOrAfter(Version.V_2_0_0_beta1)) {
+            // The index was created with elasticsearch that was using Lucene 5.2.1
             return true;
         }
         if (indexMetaData.getMinimumCompatibleVersion() != null &&
-                indexMetaData.getMinimumCompatibleVersion().onOrAfter(org.apache.lucene.util.Version.LUCENE_4_0_0)) {
+                indexMetaData.getMinimumCompatibleVersion().onOrAfter(org.apache.lucene.util.Version.LUCENE_5_0_0)) {
             //The index was upgraded we can work with it
             return true;
         }
         return false;
     }
 
-    /**
-     * Elasticsearch 2.0 deprecated custom routing hash functions. So what we do here is that for old indices, we
-     * move this old and deprecated node setting to an index setting so that we can keep things backward compatible.
-     */
-    private IndexMetaData upgradeLegacyRoutingSettings(IndexMetaData indexMetaData) {
-        if (indexMetaData.settings().get(IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION) == null
-                && indexMetaData.getCreationVersion().before(Version.V_2_0_0_beta1)) {
-            // these settings need an upgrade
-            Settings indexSettings = Settings.builder().put(indexMetaData.settings())
-                    .put(IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION, pre20HashFunction)
-                    .put(IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE, pre20UseType == null ? false : pre20UseType)
-                    .build();
-            return IndexMetaData.builder(indexMetaData)
-                    .version(indexMetaData.version())
-                    .settings(indexSettings)
-                    .build();
-        } else if (indexMetaData.getCreationVersion().onOrAfter(Version.V_2_0_0_beta1)) {
-            if (indexMetaData.getSettings().get(IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION) != null
-                    || indexMetaData.getSettings().get(IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE) != null) {
-                throw new IllegalStateException("Index [" + indexMetaData.getIndex() + "] created on or after 2.0 should NOT contain [" + IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION
-                        + "] + or [" + IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE + "] in its index settings");
-            }
-        }
-        return indexMetaData;
-    }
-
     /** All known byte-sized settings for an index. */
     public static final Set<String> INDEX_BYTES_SIZE_SETTINGS = unmodifiableSet(newHashSet(
                                     "index.merge.policy.floor_segment",
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
index c9662e7..36d727a 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java
@@ -172,7 +172,7 @@ public class MetaDataMappingService extends AbstractComponent {
             IndexService indexService = indicesService.indexService(index);
             if (indexService == null) {
                 // we need to create the index here, and add the current mapping to it, so we can merge
-                indexService = indicesService.createIndex(indexMetaData.index(), indexMetaData.settings(), currentState.nodes().localNode().id());
+                indexService = indicesService.createIndex(indexMetaData);
                 removeIndex = true;
                 Set<String> typesToIntroduce = new HashSet<>();
                 for (MappingTask task : tasks) {
@@ -350,7 +350,7 @@ public class MetaDataMappingService extends AbstractComponent {
                             continue;
                         }
                         final IndexMetaData indexMetaData = currentState.metaData().index(index);
-                        IndexService indexService = indicesService.createIndex(indexMetaData.index(), indexMetaData.settings(), clusterService.localNode().id());
+                        IndexService indexService = indicesService.createIndex(indexMetaData);
                         indicesToClose.add(indexMetaData.index());
                         // make sure to add custom default mapping if exists
                         if (indexMetaData.mappings().containsKey(MapperService.DEFAULT_MAPPING)) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java
index 2f21553..ca482ea 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.cluster.metadata;
 
-import org.elasticsearch.cluster.routing.DjbHashFunction;
+import org.elasticsearch.cluster.routing.Murmur3HashFunction;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.math.MathUtils;
@@ -43,6 +43,6 @@ public class MetaDataService extends AbstractComponent {
     }
 
     public Semaphore indexMetaDataLock(String index) {
-        return indexMdLocks[MathUtils.mod(DjbHashFunction.DJB_HASH(index), indexMdLocks.length)];
+        return indexMdLocks[MathUtils.mod(Murmur3HashFunction.hash(index), indexMdLocks.length)];
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
index 65d862c..58dffd8 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
@@ -320,7 +320,7 @@ public class MetaDataUpdateSettingsService extends AbstractComponent implements
                 }
 
 
-                ClusterState updatedState = ClusterState.builder(currentState).metaData(metaDataBuilder).routingTable(routingTableBuilder).blocks(blocks).build();
+                ClusterState updatedState = ClusterState.builder(currentState).metaData(metaDataBuilder).routingTable(routingTableBuilder.build()).blocks(blocks).build();
 
                 // now, reroute in case things change that require it (like number of replicas)
                 RoutingAllocation.Result routingResult = allocationService.reroute(updatedState);
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
index ebf1bcb..780f511 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java
@@ -19,13 +19,15 @@
 
 package org.elasticsearch.cluster.node;
 
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.*;
-import org.elasticsearch.common.network.NetworkUtils;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
+import org.elasticsearch.common.io.stream.StreamInput;
+import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.transport.TransportAddressSerializers;
@@ -33,7 +35,6 @@ import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.net.InetAddress;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
@@ -100,7 +101,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
     private String hostName;
     private String hostAddress;
     private TransportAddress address;
-    private Map<String, String> attributes;
+    private ImmutableOpenMap<String, String> attributes;
     private Version version = Version.CURRENT;
 
     DiscoveryNode() {
@@ -143,7 +144,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
     }
 
     /**
-     * Creates a new {@link DiscoveryNode}
+     * Creates a new {@link DiscoveryNode}.
      * <p>
      * <b>Note:</b> if the version of the node is unknown {@link #MINIMUM_DISCOVERY_NODE_VERSION} should be used.
      * it corresponds to the minimum version this elasticsearch version can communicate with. If a higher version is used
@@ -163,7 +164,7 @@ public class DiscoveryNode implements Streamable, ToXContent {
         if (nodeName != null) {
             this.nodeName = nodeName.intern();
         }
-        ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();
+        ImmutableOpenMap.Builder<String, String> builder = ImmutableOpenMap.builder();
         for (Map.Entry<String, String> entry : attributes.entrySet()) {
             builder.put(entry.getKey().intern(), entry.getValue().intern());
         }
@@ -176,6 +177,39 @@ public class DiscoveryNode implements Streamable, ToXContent {
     }
 
     /**
+     * Creates a new {@link DiscoveryNode}.
+     * <p>
+     * <b>Note:</b> if the version of the node is unknown {@link #MINIMUM_DISCOVERY_NODE_VERSION} should be used.
+     * it corresponds to the minimum version this elasticsearch version can communicate with. If a higher version is used
+     * the node might not be able to communicate with the remove node. After initial handshakes node versions will be discovered
+     * and updated.
+     * </p>
+     *
+     * @param nodeName    the nodes name
+     * @param nodeId      the nodes unique id.
+     * @param hostName    the nodes hostname
+     * @param hostAddress the nodes host address
+     * @param address     the nodes transport address
+     * @param attributes  node attributes
+     * @param version     the version of the node.
+     */
+    public DiscoveryNode(String nodeName, String nodeId, String hostName, String hostAddress, TransportAddress address, ImmutableOpenMap<String, String> attributes, Version version) {
+        if (nodeName != null) {
+            this.nodeName = nodeName.intern();
+        }
+        ImmutableOpenMap.Builder<String, String> builder = ImmutableOpenMap.builder();
+        for (ObjectObjectCursor<String, String> entry : attributes) {
+            builder.put(entry.key.intern(), entry.value.intern());
+        }
+        this.attributes = builder.build();
+        this.nodeId = nodeId.intern();
+        this.hostName = hostName.intern();
+        this.hostAddress = hostAddress.intern();
+        this.address = address;
+        this.version = version;
+    }
+
+    /**
      * Should this node form a connection to the provided node.
      */
     public boolean shouldConnectTo(DiscoveryNode otherNode) {
@@ -230,14 +264,14 @@ public class DiscoveryNode implements Streamable, ToXContent {
     /**
      * The node attributes.
      */
-    public Map<String, String> attributes() {
+    public ImmutableOpenMap<String, String> attributes() {
         return this.attributes;
     }
 
     /**
      * The node attributes.
      */
-    public Map<String, String> getAttributes() {
+    public ImmutableOpenMap<String, String> getAttributes() {
         return attributes();
     }
 
@@ -319,11 +353,11 @@ public class DiscoveryNode implements Streamable, ToXContent {
         hostAddress = in.readString().intern();
         address = TransportAddressSerializers.addressFromStream(in);
         int size = in.readVInt();
-        ImmutableMap.Builder<String, String> builder = ImmutableMap.builder();
+        ImmutableOpenMap.Builder<String, String> attributes = ImmutableOpenMap.builder(size);
         for (int i = 0; i < size; i++) {
-            builder.put(in.readString().intern(), in.readString().intern());
+            attributes.put(in.readString().intern(), in.readString().intern());
         }
-        attributes = builder.build();
+        this.attributes = attributes.build();
         version = Version.readVersion(in);
     }
 
@@ -335,9 +369,9 @@ public class DiscoveryNode implements Streamable, ToXContent {
         out.writeString(hostAddress);
         addressToStream(out, address);
         out.writeVInt(attributes.size());
-        for (Map.Entry<String, String> entry : attributes.entrySet()) {
-            out.writeString(entry.getKey());
-            out.writeString(entry.getValue());
+        for (ObjectObjectCursor<String, String> entry : attributes) {
+            out.writeString(entry.key);
+            out.writeString(entry.value);
         }
         Version.writeVersion(version, out);
     }
@@ -385,8 +419,8 @@ public class DiscoveryNode implements Streamable, ToXContent {
         builder.field("transport_address", address().toString());
 
         builder.startObject("attributes");
-        for (Map.Entry<String, String> attr : attributes().entrySet()) {
-            builder.field(attr.getKey(), attr.getValue());
+        for (ObjectObjectCursor<String, String> attr : attributes) {
+            builder.field(attr.key, attr.value);
         }
         builder.endObject();
 
diff --git a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
index 13b6471..16b7e9e 100644
--- a/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java
@@ -22,6 +22,7 @@ package org.elasticsearch.cluster.node;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.common.Booleans;
@@ -33,7 +34,12 @@ import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.transport.TransportAddress;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
 
 /**
  * This class holds all {@link DiscoveryNode} in the cluster and provides convenience methods to
@@ -374,9 +380,9 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
                             }
                         } else {
                             for (DiscoveryNode node : this) {
-                                for (Map.Entry<String, String> entry : node.attributes().entrySet()) {
-                                    String attrName = entry.getKey();
-                                    String attrValue = entry.getValue();
+                                for (ObjectObjectCursor<String, String> entry : node.attributes()) {
+                                    String attrName = entry.key;
+                                    String attrValue = entry.value;
                                     if (Regex.simpleMatch(matchAttrName, attrName) && Regex.simpleMatch(matchAttrValue, attrValue)) {
                                         resolvedNodesIds.add(node.id());
                                     }
@@ -563,6 +569,7 @@ public class DiscoveryNodes extends AbstractDiffable<DiscoveryNodes> implements
         }
     }
 
+    @Override
     public void writeTo(StreamOutput out) throws IOException {
         if (masterNodeId == null) {
             out.writeBoolean(false);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/DjbHashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/DjbHashFunction.java
deleted file mode 100644
index 7616bd3..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/routing/DjbHashFunction.java
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-import org.elasticsearch.cluster.routing.HashFunction;
-
-/**
- * This class implements the efficient hash function
- * developed by <i>Daniel J. Bernstein</i>.
- */
-public class DjbHashFunction implements HashFunction {
-
-    public static int DJB_HASH(String value) {
-        long hash = 5381;
-
-        for (int i = 0; i < value.length(); i++) {
-            hash = ((hash << 5) + hash) + value.charAt(i);
-        }
-
-        return (int) hash;
-    }
-
-    public static int DJB_HASH(byte[] value, int offset, int length) {
-        long hash = 5381;
-
-        final int end = offset + length;
-        for (int i = offset; i < end; i++) {
-            hash = ((hash << 5) + hash) + value[i];
-        }
-
-        return (int) hash;
-    }
-
-    @Override
-    public int hash(String routing) {
-        return DJB_HASH(routing);
-    }
-
-    @Override
-    public int hash(String type, String id) {
-        long hash = 5381;
-
-        for (int i = 0; i < type.length(); i++) {
-            hash = ((hash << 5) + hash) + type.charAt(i);
-        }
-
-        for (int i = 0; i < id.length(); i++) {
-            hash = ((hash << 5) + hash) + id.charAt(i);
-        }
-
-        return (int) hash;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java
deleted file mode 100644
index 99977ee..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/routing/HashFunction.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-/**
- * Simple hash function interface used for shard routing.
- */
-public interface HashFunction {
-
-    /**
-     * Calculate a hash value for routing 
-     * @param routing String to calculate the hash value from 
-     * @return hash value of the given routing string
-     */
-    int hash(String routing);
-
-    /**
-     * Calculate a hash value for routing and its type
-     * @param type types name
-     * @param id String to calculate the hash value from 
-     * @return hash value of the given type and routing string
-     */
-    @Deprecated
-    int hash(String type, String id);
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
index e740a4f..6512ee5 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.routing;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.common.collect.MapBuilder;
@@ -39,6 +38,8 @@ import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ThreadLocalRandom;
 
+import static java.util.Collections.emptyMap;
+
 /**
  * {@link IndexShardRoutingTable} encapsulates all instances of a single shard.
  * Each Elasticsearch index consists of multiple shards, each shard encapsulates
@@ -60,6 +61,10 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
     final static List<ShardRouting> NO_SHARDS = Collections.emptyList();
     final boolean allShardsStarted;
 
+    private volatile Map<AttributesKey, AttributesRoutings> activeShardsByAttributes = emptyMap();
+    private volatile Map<AttributesKey, AttributesRoutings> initializingShardsByAttributes = emptyMap();
+    private final Object shardsByAttributeMutex = new Object();
+
     /**
      * The initializing list, including ones that are initializing on a target node because of relocation.
      * If we can come up with a better variable name, it would be nice...
@@ -476,10 +481,6 @@ public class IndexShardRoutingTable implements Iterable<ShardRouting> {
         }
     }
 
-    private volatile Map<AttributesKey, AttributesRoutings> activeShardsByAttributes = ImmutableMap.of();
-    private volatile Map<AttributesKey, AttributesRoutings> initializingShardsByAttributes = ImmutableMap.of();
-    private final Object shardsByAttributeMutex = new Object();
-
     private AttributesRoutings getActiveAttribute(AttributesKey key, DiscoveryNodes nodes) {
         AttributesRoutings shardRoutings = activeShardsByAttributes.get(key);
         if (shardRoutings == null) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/Murmur3HashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/Murmur3HashFunction.java
index 7ca602a..4752271 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/Murmur3HashFunction.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/Murmur3HashFunction.java
@@ -20,15 +20,17 @@
 package org.elasticsearch.cluster.routing;
 
 import org.apache.lucene.util.StringHelper;
-import org.elasticsearch.cluster.routing.HashFunction;
 
 /**
  * Hash function based on the Murmur3 algorithm, which is the default as of Elasticsearch 2.0.
  */
-public class Murmur3HashFunction implements HashFunction {
+public final class Murmur3HashFunction {
 
-    @Override
-    public int hash(String routing) {
+    private Murmur3HashFunction() {
+        //no instance
+    }
+
+    public static int hash(String routing) {
         final byte[] bytesToHash = new byte[routing.length() * 2];
         for (int i = 0; i < routing.length(); ++i) {
             final char c = routing.charAt(i);
@@ -37,12 +39,10 @@ public class Murmur3HashFunction implements HashFunction {
             bytesToHash[i * 2] = b1;
             bytesToHash[i * 2 + 1] = b2;
         }
-        return StringHelper.murmurhash3_x86_32(bytesToHash, 0, bytesToHash.length, 0);
+        return hash(bytesToHash, 0, bytesToHash.length);
     }
 
-    @Override
-    public int hash(String type, String id) {
-        throw new UnsupportedOperationException();
+    public static int hash(byte[] bytes, int offset, int length) {
+        return StringHelper.murmurhash3_x86_32(bytes, offset, length, 0);
     }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java b/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
index 411a1ed..c142b75 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/OperationRouting.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.routing;
 
-import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
@@ -47,7 +46,6 @@ import java.util.Set;
 public class OperationRouting extends AbstractComponent {
 
 
-
     private final AwarenessAllocationDecider awarenessAllocationDecider;
 
     @Inject
@@ -196,9 +194,9 @@ public class OperationRouting extends AbstractComponent {
         // if not, then use it as the index
         String[] awarenessAttributes = awarenessAllocationDecider.awarenessAttributes();
         if (awarenessAttributes.length == 0) {
-            return indexShard.activeInitializingShardsIt(DjbHashFunction.DJB_HASH(preference));
+            return indexShard.activeInitializingShardsIt(Murmur3HashFunction.hash(preference));
         } else {
-            return indexShard.preferAttributesActiveInitializingShardsIt(awarenessAttributes, nodes, DjbHashFunction.DJB_HASH(preference));
+            return indexShard.preferAttributesActiveInitializingShardsIt(awarenessAttributes, nodes, Murmur3HashFunction.hash(preference));
         }
     }
 
@@ -237,37 +235,13 @@ public class OperationRouting extends AbstractComponent {
     @SuppressForbidden(reason = "Math#abs is trappy")
     private int shardId(ClusterState clusterState, String index, String type, String id, @Nullable String routing) {
         final IndexMetaData indexMetaData = indexMetaData(clusterState, index);
-        final Version createdVersion = indexMetaData.getCreationVersion();
-        final HashFunction hashFunction = indexMetaData.getRoutingHashFunction();
-        final boolean useType = indexMetaData.getRoutingUseType();
-
         final int hash;
         if (routing == null) {
-            if (!useType) {
-                hash = hash(hashFunction, id);
-            } else {
-                hash = hash(hashFunction, type, id);
-            }
-        } else {
-            hash = hash(hashFunction, routing);
-        }
-        if (createdVersion.onOrAfter(Version.V_2_0_0_beta1)) {
-            return MathUtils.mod(hash, indexMetaData.numberOfShards());
+            hash = Murmur3HashFunction.hash(id);
         } else {
-            return Math.abs(hash % indexMetaData.numberOfShards());
-        }
-    }
-
-    protected int hash(HashFunction hashFunction, String routing) {
-        return hashFunction.hash(routing);
-    }
-
-    @Deprecated
-    protected int hash(HashFunction hashFunction, String type, String id) {
-        if (type == null || "_all".equals(type)) {
-            throw new IllegalArgumentException("Can't route an operation with no type and having type part of the routing (for backward comp)");
+            hash = Murmur3HashFunction.hash(routing);
         }
-        return hashFunction.hash(type, id);
+        return MathUtils.mod(hash, indexMetaData.numberOfShards());
     }
 
     private void ensureNodeIdExists(DiscoveryNodes nodes, String nodeId) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
index 39c2d03..d5ed922 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java
@@ -21,16 +21,24 @@ package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.ObjectIntHashMap;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
-import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.index.shard.ShardId;
 
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
 import java.util.function.Predicate;
 
 /**
@@ -82,8 +90,8 @@ public class RoutingNodes implements Iterable<RoutingNode> {
 
         // fill in the inverse of node -> shards allocated
         // also fill replicaSet information
-        for (IndexRoutingTable indexRoutingTable : routingTable.indicesRouting().values()) {
-            for (IndexShardRoutingTable indexShard : indexRoutingTable) {
+        for (ObjectCursor<IndexRoutingTable> indexRoutingTable : routingTable.indicesRouting().values()) {
+            for (IndexShardRoutingTable indexShard : indexRoutingTable.value) {
                 for (ShardRouting shard : indexShard) {
                     // to get all the shards belonging to an index, including the replicas,
                     // we define a replica set and keep track of it. A replica set is identified
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
index 7a8c33e..10d7ff9 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java
@@ -20,13 +20,15 @@
 package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.IntSet;
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
 
 import org.elasticsearch.cluster.Diff;
 import org.elasticsearch.cluster.Diffable;
 import org.elasticsearch.cluster.DiffableUtils;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.util.iterable.Iterables;
@@ -41,8 +43,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.function.Predicate;
 
-import static java.util.Collections.unmodifiableMap;
-
 /**
  * Represents a global cluster-wide routing table for all indices including the
  * version of the current routing state.
@@ -58,11 +58,11 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
     private final long version;
 
     // index to IndexRoutingTable map
-    private final Map<String, IndexRoutingTable> indicesRouting;
+    private final ImmutableOpenMap<String, IndexRoutingTable> indicesRouting;
 
-    RoutingTable(long version, Map<String, IndexRoutingTable> indicesRouting) {
+    RoutingTable(long version, ImmutableOpenMap<String, IndexRoutingTable> indicesRouting) {
         this.version = version;
-        this.indicesRouting = unmodifiableMap(indicesRouting);
+        this.indicesRouting = indicesRouting;
     }
 
     /**
@@ -76,7 +76,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
     @Override
     public Iterator<IndexRoutingTable> iterator() {
-        return indicesRouting.values().iterator();
+        return indicesRouting.valuesIt();
     }
 
     public boolean hasIndex(String index) {
@@ -87,11 +87,11 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         return indicesRouting.get(index);
     }
 
-    public Map<String, IndexRoutingTable> indicesRouting() {
+    public ImmutableOpenMap<String, IndexRoutingTable> indicesRouting() {
         return indicesRouting;
     }
 
-    public Map<String, IndexRoutingTable> getIndicesRouting() {
+    public ImmutableOpenMap<String, IndexRoutingTable> getIndicesRouting() {
         return indicesRouting();
     }
 
@@ -126,7 +126,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
      */
     public List<ShardRouting> allShards() {
         List<ShardRouting> shards = new ArrayList<>();
-        String[] indices = indicesRouting.keySet().toArray(new String[indicesRouting.keySet().size()]);
+        String[] indices = indicesRouting.keys().toArray(String.class);
         for (String index : indices) {
             List<ShardRouting> allShardsIndex = allShards(index);
             shards.addAll(allShardsIndex);
@@ -303,8 +303,8 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
     public void writeTo(StreamOutput out) throws IOException {
         out.writeLong(version);
         out.writeVInt(indicesRouting.size());
-        for (IndexRoutingTable index : indicesRouting.values()) {
-            index.writeTo(out);
+        for (ObjectCursor<IndexRoutingTable> index : indicesRouting.values()) {
+            index.value.writeTo(out);
         }
     }
 
@@ -312,7 +312,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
         private final long version;
 
-        private final Diff<Map<String, IndexRoutingTable>> indicesRouting;
+        private final Diff<ImmutableOpenMap<String, IndexRoutingTable>> indicesRouting;
 
         public RoutingTableDiff(RoutingTable before, RoutingTable after) {
             version = after.version;
@@ -321,7 +321,7 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
         public RoutingTableDiff(StreamInput in) throws IOException {
             version = in.readLong();
-            indicesRouting = DiffableUtils.readJdkMapDiff(in, IndexRoutingTable.PROTO);
+            indicesRouting = DiffableUtils.readImmutableOpenMapDiff(in, IndexRoutingTable.PROTO);
         }
 
         @Override
@@ -344,10 +344,13 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         return new Builder(routingTable);
     }
 
+    /**
+     * Builder for the routing table. Note that build can only be called one time.
+     */
     public static class Builder {
 
         private long version;
-        private final Map<String, IndexRoutingTable> indicesRouting = new HashMap<>();
+        private ImmutableOpenMap.Builder<String, IndexRoutingTable> indicesRouting = ImmutableOpenMap.builder();
 
         public Builder() {
 
@@ -403,8 +406,11 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         }
 
         public Builder updateNumberOfReplicas(int numberOfReplicas, String... indices) {
+            if (indicesRouting == null) {
+                throw new IllegalStateException("once build is called the builder cannot be reused");
+            }
             if (indices == null || indices.length == 0) {
-                indices = indicesRouting.keySet().toArray(new String[indicesRouting.keySet().size()]);
+                indices = indicesRouting.keys().toArray(String.class);
             }
             for (String index : indices) {
                 IndexRoutingTable indexRoutingTable = indicesRouting.get(index);
@@ -489,6 +495,9 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
         }
 
         public Builder add(IndexRoutingTable indexRoutingTable) {
+            if (indicesRouting == null) {
+                throw new IllegalStateException("once build is called the builder cannot be reused");
+            }
             indexRoutingTable.validate();
             indicesRouting.put(indexRoutingTable.index(), indexRoutingTable);
             return this;
@@ -499,12 +508,18 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
             return this;
         }
 
-        public Builder indicesRouting(ImmutableMap<String, IndexRoutingTable> indicesRouting) {
+        public Builder indicesRouting(Map<String, IndexRoutingTable> indicesRouting) {
+            if (indicesRouting == null) {
+                throw new IllegalStateException("once build is called the builder cannot be reused");
+            }
             this.indicesRouting.putAll(indicesRouting);
             return this;
         }
 
         public Builder remove(String index) {
+            if (indicesRouting == null) {
+                throw new IllegalStateException("once build is called the builder cannot be reused");
+            }
             indicesRouting.remove(index);
             return this;
         }
@@ -514,12 +529,22 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
             return this;
         }
 
+        /**
+         * Builds the routing table. Note that once this is called the builder
+         * must be thrown away. If you need to build a new RoutingTable as a
+         * copy of this one you'll need to build a new RoutingTable.Builder.
+         */
         public RoutingTable build() {
+            if (indicesRouting == null) {
+                throw new IllegalStateException("once build is called the builder cannot be reused");
+            }
             // normalize the versions right before we build it...
-            for (IndexRoutingTable indexRoutingTable : indicesRouting.values()) {
-                indicesRouting.put(indexRoutingTable.index(), indexRoutingTable.normalizeVersions());
+            for (ObjectCursor<IndexRoutingTable> indexRoutingTable : indicesRouting.values()) {
+                indicesRouting.put(indexRoutingTable.value.index(), indexRoutingTable.value.normalizeVersions());
             }
-            return new RoutingTable(version, indicesRouting);
+            RoutingTable table = new RoutingTable(version, indicesRouting.build());
+            indicesRouting = null;
+            return table;
         }
 
         public static RoutingTable readFrom(StreamInput in) throws IOException {
@@ -529,8 +554,8 @@ public class RoutingTable implements Iterable<IndexRoutingTable>, Diffable<Routi
 
     public String prettyPrint() {
         StringBuilder sb = new StringBuilder("routing_table (version ").append(version).append("):\n");
-        for (Map.Entry<String, IndexRoutingTable> entry : indicesRouting.entrySet()) {
-            sb.append(entry.getValue().prettyPrint()).append('\n');
+        for (ObjectObjectCursor<String, IndexRoutingTable> entry : indicesRouting) {
+            sb.append(entry.value.prettyPrint()).append('\n');
         }
         return sb.toString();
     }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
index aec8bef..472e73b 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/RoutingTableValidation.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.routing;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -31,6 +30,8 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+
 /**
  * Encapsulates the result of a routing table validation and provides access to
  * validation failures.
@@ -72,7 +73,7 @@ public class RoutingTableValidation implements Streamable {
 
     public Map<String, List<String>> indicesFailures() {
         if (indicesFailures == null) {
-            return ImmutableMap.of();
+            return emptyMap();
         }
         return indicesFailures;
     }
@@ -128,7 +129,7 @@ public class RoutingTableValidation implements Streamable {
         }
         size = in.readVInt();
         if (size == 0) {
-            indicesFailures = ImmutableMap.of();
+            indicesFailures = emptyMap();
         } else {
             indicesFailures = new HashMap<>();
             for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/SimpleHashFunction.java b/core/src/main/java/org/elasticsearch/cluster/routing/SimpleHashFunction.java
deleted file mode 100644
index bbb6a61..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/routing/SimpleHashFunction.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-/**
- * This class implements a simple hash function based on Java Build-In {@link Object#hashCode()}
- */
-public class SimpleHashFunction implements HashFunction {
-
-    @Override
-    public int hash(String routing) {
-        return routing.hashCode();
-    }
-
-    @Override
-    public int hash(String type, String id) {
-        return type.hashCode() + 31 * id.hashCode();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
index b0ac162..efb5c96 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java
@@ -19,13 +19,14 @@
 
 package org.elasticsearch.cluster.routing.allocation.allocator;
 
+import com.carrotsearch.hppc.cursors.ObjectCursor;
+
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.IntroSorter;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.RoutingNodes;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.FailedRerouteAllocation;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
@@ -40,7 +41,16 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.gateway.PriorityComparator;
 import org.elasticsearch.node.settings.NodeSettingsService;
 
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.IdentityHashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Set;
 import java.util.function.Predicate;
 
 import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
@@ -284,7 +294,9 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
             if (logger.isTraceEnabled()) {
                 logger.trace("Start distributing Shards");
             }
-            indices.addAll(allocation.routingTable().indicesRouting().keySet());
+            for (ObjectCursor<String> index : allocation.routingTable().indicesRouting().keys()) {
+                indices.add(index.value);
+            }
             buildModelFromAssigned(routing.shards(assignedFilter));
             return allocateUnassigned(unassigned);
         }
@@ -428,7 +440,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                 deltas[i] = sorter.delta();
             }
             new IntroSorter() {
-                
+
                 float pivotWeight;
 
                 @Override
@@ -554,10 +566,10 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                 return false;
             }
             boolean changed = false;
-          
+
             /*
              * TODO: We could be smarter here and group the shards by index and then
-             * use the sorter to save some iterations. 
+             * use the sorter to save some iterations.
              */
             final AllocationDeciders deciders = allocation.deciders();
             final PriorityComparator secondaryComparator = PriorityComparator.getAllocationComparator(allocation);
@@ -768,7 +780,7 @@ public class BalancedShardsAllocator extends AbstractComponent implements Shards
                 }
 
                 if (candidate != null) {
-                  
+
                     /* allocate on the model even if not throttled */
                     maxNode.removeShard(candidate);
                     minNode.addShard(candidate, decision);
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
index e1a0b77..f21ced8 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java
@@ -19,6 +19,10 @@
 
 package org.elasticsearch.cluster.routing.allocation.decider;
 
+import com.carrotsearch.hppc.ObjectLookupContainer;
+import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.cluster.ClusterInfo;
@@ -30,6 +34,7 @@ import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.ByteSizeValue;
@@ -38,7 +43,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.set.Sets;
 import org.elasticsearch.node.settings.NodeSettingsService;
 
-import java.util.Map;
 import java.util.Set;
 
 /**
@@ -164,23 +168,23 @@ public class DiskThresholdDecider extends AllocationDecider {
 
         @Override
         public void onNewInfo(ClusterInfo info) {
-            Map<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages();
+            ImmutableOpenMap<String, DiskUsage> usages = info.getNodeLeastAvailableDiskUsages();
             if (usages != null) {
                 boolean reroute = false;
                 String explanation = "";
 
                 // Garbage collect nodes that have been removed from the cluster
                 // from the map that tracks watermark crossing
-                Set<String> nodes = usages.keySet();
+                ObjectLookupContainer<String> nodes = usages.keys();
                 for (String node : nodeHasPassedWatermark) {
                     if (nodes.contains(node) == false) {
                         nodeHasPassedWatermark.remove(node);
                     }
                 }
 
-                for (Map.Entry<String, DiskUsage> entry : usages.entrySet()) {
-                    String node = entry.getKey();
-                    DiskUsage usage = entry.getValue();
+                for (ObjectObjectCursor<String, DiskUsage> entry : usages) {
+                    String node = entry.key;
+                    DiskUsage usage = entry.value;
                     warnAboutDiskIfNeeded(usage);
                     if (usage.getFreeBytes() < DiskThresholdDecider.this.freeBytesThresholdHigh.bytes() ||
                             usage.getFreeDiskAsPercentage() < DiskThresholdDecider.this.freeDiskThresholdHigh) {
@@ -336,7 +340,7 @@ public class DiskThresholdDecider extends AllocationDecider {
     @Override
     public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {
         ClusterInfo clusterInfo = allocation.clusterInfo();
-        Map<String, DiskUsage> usages = clusterInfo.getNodeMostAvailableDiskUsages();
+        ImmutableOpenMap<String, DiskUsage> usages = clusterInfo.getNodeMostAvailableDiskUsages();
         final Decision decision = earlyTerminate(allocation, usages);
         if (decision != null) {
             return decision;
@@ -451,7 +455,7 @@ public class DiskThresholdDecider extends AllocationDecider {
             throw new IllegalArgumentException("Shard [" + shardRouting + "] is not allocated on node: [" + node.nodeId() + "]");
         }
         final ClusterInfo clusterInfo = allocation.clusterInfo();
-        final Map<String, DiskUsage> usages = clusterInfo.getNodeLeastAvailableDiskUsages();
+        final ImmutableOpenMap<String, DiskUsage> usages = clusterInfo.getNodeLeastAvailableDiskUsages();
         final Decision decision = earlyTerminate(allocation, usages);
         if (decision != null) {
             return decision;
@@ -488,7 +492,7 @@ public class DiskThresholdDecider extends AllocationDecider {
         return allocation.decision(Decision.YES, NAME, "enough disk for shard to remain on node, free: [%s]", new ByteSizeValue(freeBytes));
     }
 
-    private DiskUsage getDiskUsage(RoutingNode node, RoutingAllocation allocation,  Map<String, DiskUsage> usages) {
+    private DiskUsage getDiskUsage(RoutingNode node, RoutingAllocation allocation, ImmutableOpenMap<String, DiskUsage> usages) {
         ClusterInfo clusterInfo = allocation.clusterInfo();
         DiskUsage usage = usages.get(node.nodeId());
         if (usage == null) {
@@ -521,15 +525,15 @@ public class DiskThresholdDecider extends AllocationDecider {
      * @param usages Map of nodeId to DiskUsage for all known nodes
      * @return DiskUsage representing given node using the average disk usage
      */
-    public DiskUsage averageUsage(RoutingNode node, Map<String, DiskUsage> usages) {
+    public DiskUsage averageUsage(RoutingNode node, ImmutableOpenMap<String, DiskUsage> usages) {
         if (usages.size() == 0) {
             return new DiskUsage(node.nodeId(), node.node().name(), "_na_", 0, 0);
         }
         long totalBytes = 0;
         long freeBytes = 0;
-        for (DiskUsage du : usages.values()) {
-            totalBytes += du.getTotalBytes();
-            freeBytes += du.getFreeBytes();
+        for (ObjectCursor<DiskUsage> du : usages.values()) {
+            totalBytes += du.value.getTotalBytes();
+            freeBytes += du.value.getFreeBytes();
         }
         return new DiskUsage(node.nodeId(), node.node().name(), "_na_", totalBytes / usages.size(), freeBytes / usages.size());
     }
@@ -592,7 +596,7 @@ public class DiskThresholdDecider extends AllocationDecider {
         }
     }
 
-    private Decision earlyTerminate(RoutingAllocation allocation, final Map<String, DiskUsage> usages) {
+    private Decision earlyTerminate(RoutingAllocation allocation, ImmutableOpenMap<String, DiskUsage> usages) {
         // Always allow allocation if the decider is disabled
         if (!enabled) {
             return allocation.decision(Decision.YES, NAME, "disk threshold decider disabled");
diff --git a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
index 74bfac0..c230073 100644
--- a/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java
@@ -20,8 +20,16 @@
 package org.elasticsearch.cluster.service;
 
 import org.elasticsearch.Version;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterName;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.ClusterState.Builder;
+import org.elasticsearch.cluster.ClusterStateListener;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
+import org.elasticsearch.cluster.LocalNodeMasterListener;
+import org.elasticsearch.cluster.TimeoutClusterStateListener;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -41,7 +49,13 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.text.StringText;
 import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.util.concurrent.*;
+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
+import org.elasticsearch.common.util.concurrent.CountDown;
+import org.elasticsearch.common.util.concurrent.EsExecutors;
+import org.elasticsearch.common.util.concurrent.EsRejectedExecutionException;
+import org.elasticsearch.common.util.concurrent.FutureUtils;
+import org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor;
+import org.elasticsearch.common.util.concurrent.PrioritizedRunnable;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.discovery.Discovery;
 import org.elasticsearch.discovery.DiscoveryService;
@@ -49,8 +63,18 @@ import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.*;
-import java.util.concurrent.*;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Queue;
+import java.util.concurrent.ConcurrentMap;
+import java.util.concurrent.CopyOnWriteArrayList;
+import java.util.concurrent.Executor;
+import java.util.concurrent.Future;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
 
 import static org.elasticsearch.common.util.concurrent.EsExecutors.daemonThreadFactory;
 
@@ -413,7 +437,7 @@ public class InternalClusterService extends AbstractLifecycleComponent<ClusterSe
                     // only the master controls the version numbers
                     Builder builder = ClusterState.builder(newClusterState).incrementVersion();
                     if (previousClusterState.routingTable() != newClusterState.routingTable()) {
-                        builder.routingTable(RoutingTable.builder(newClusterState.routingTable()).version(newClusterState.routingTable().version() + 1));
+                        builder.routingTable(RoutingTable.builder(newClusterState.routingTable()).version(newClusterState.routingTable().version() + 1).build());
                     }
                     if (previousClusterState.metaData() != newClusterState.metaData()) {
                         builder.metaData(MetaData.builder(newClusterState.metaData()).version(newClusterState.metaData().version() + 1));
diff --git a/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java b/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
index c4137fc..1935a334 100644
--- a/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
+++ b/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
@@ -19,23 +19,21 @@
 
 package org.elasticsearch.cluster.settings;
 
+import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.cluster.ClusterState;
-import org.elasticsearch.common.collect.MapBuilder;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.regex.Regex;
 
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
 /**
  * A container for setting names and validation methods for those settings.
  */
 public class DynamicSettings {
-
-    private final Map<String, Validator> dynamicSettings;
+    private final ImmutableOpenMap<String, Validator> dynamicSettings;
 
     public static class Builder {
-        private Map<String, Validator> settings = new HashMap<>();
+        private ImmutableOpenMap.Builder<String, Validator> settings = ImmutableOpenMap.builder();
 
         public void addSetting(String setting, Validator validator) {
             Validator old = settings.put(setting, validator);
@@ -45,12 +43,12 @@ public class DynamicSettings {
         }
 
         public DynamicSettings build() {
-            return new DynamicSettings(settings);
+            return new DynamicSettings(settings.build());
         }
     }
 
-    private DynamicSettings(Map<String, Validator> settings) {
-        this.dynamicSettings = Collections.unmodifiableMap(settings);
+    private DynamicSettings(ImmutableOpenMap<String, Validator> settings) {
+        this.dynamicSettings = settings;
     }
 
     public boolean isDynamicOrLoggingSetting(String key) {
@@ -58,8 +56,8 @@ public class DynamicSettings {
     }
 
     public boolean hasDynamicSetting(String key) {
-        for (String dynamicSetting : dynamicSettings.keySet()) {
-            if (Regex.simpleMatch(dynamicSetting, key)) {
+        for (ObjectCursor<String> dynamicSetting : dynamicSettings.keys()) {
+            if (Regex.simpleMatch(dynamicSetting.value, key)) {
                 return true;
             }
         }
@@ -67,9 +65,9 @@ public class DynamicSettings {
     }
 
     public String validateDynamicSetting(String dynamicSetting, String value, ClusterState clusterState) {
-        for (Map.Entry<String, Validator> setting : dynamicSettings.entrySet()) {
-            if (Regex.simpleMatch(setting.getKey(), dynamicSetting)) {
-                return setting.getValue().validate(dynamicSetting, value, clusterState);
+        for (ObjectObjectCursor<String, Validator> setting : dynamicSettings) {
+            if (Regex.simpleMatch(setting.key, dynamicSetting)) {
+                return setting.value.validate(dynamicSetting, value, clusterState);
             }
         }
         return null;
diff --git a/core/src/main/java/org/elasticsearch/common/Table.java b/core/src/main/java/org/elasticsearch/common/Table.java
index fd979cf..6156cc2 100644
--- a/core/src/main/java/org/elasticsearch/common/Table.java
+++ b/core/src/main/java/org/elasticsearch/common/Table.java
@@ -19,13 +19,13 @@
 
 package org.elasticsearch.common;
 
-import com.google.common.collect.ImmutableMap;
-
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+
 /**
  */
 public class Table {
@@ -115,7 +115,7 @@ public class Table {
         Map<String, String> mAttr;
         if (attributes.length() == 0) {
             if (inHeaders) {
-                mAttr = ImmutableMap.of();
+                mAttr = emptyMap();
             } else {
                 // get the attributes of the header cell we are going to add to
                 mAttr = headers.get(currentCells.size()).attr;
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
index 5b9a8bd..7039783 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java
@@ -19,22 +19,27 @@
 
 package org.elasticsearch.common.blobstore.fs;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
 import org.elasticsearch.common.blobstore.support.PlainBlobMetaData;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.Streams;
 
-import java.io.*;
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardCopyOption;
 import java.nio.file.attribute.BasicFileAttributes;
+import java.util.HashMap;
+import java.util.Map;
+
+import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -52,14 +57,14 @@ public class FsBlobContainer extends AbstractBlobContainer {
     }
 
     @Override
-    public ImmutableMap<String, BlobMetaData> listBlobs() throws IOException {
+    public Map<String, BlobMetaData> listBlobs() throws IOException {
         return listBlobsByPrefix(null);
     }
 
     @Override
-    public ImmutableMap<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {
-        // using MapBuilder and not ImmutableMap.Builder as it seems like File#listFiles might return duplicate files!
-        MapBuilder<String, BlobMetaData> builder = MapBuilder.newMapBuilder();
+    public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {
+        // If we get duplicate files we should just take the last entry
+        Map<String, BlobMetaData> builder = new HashMap<>();
 
         blobNamePrefix = blobNamePrefix == null ? "" : blobNamePrefix;
         try (DirectoryStream<Path> stream = Files.newDirectoryStream(path, blobNamePrefix + "*")) {
@@ -70,7 +75,7 @@ public class FsBlobContainer extends AbstractBlobContainer {
                 }
             }
         }
-        return builder.immutableMap();
+        return unmodifiableMap(builder);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java b/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
index 9dfaa9c..5bf5521 100644
--- a/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
+++ b/core/src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.common.blobstore.url;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.blobstore.BlobMetaData;
 import org.elasticsearch.common.blobstore.BlobPath;
 import org.elasticsearch.common.blobstore.support.AbstractBlobContainer;
@@ -29,6 +28,7 @@ import java.io.BufferedInputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.net.URL;
+import java.util.Map;
 
 /**
  * URL blob implementation of {@link org.elasticsearch.common.blobstore.BlobContainer}
@@ -65,7 +65,7 @@ public class URLBlobContainer extends AbstractBlobContainer {
      * This operation is not supported by URLBlobContainer
      */
     @Override
-    public ImmutableMap<String, BlobMetaData> listBlobs() throws IOException {
+    public Map<String, BlobMetaData> listBlobs() throws IOException {
         throw new UnsupportedOperationException("URL repository doesn't support this operation");
     }
 
@@ -73,7 +73,7 @@ public class URLBlobContainer extends AbstractBlobContainer {
      * This operation is not supported by URLBlobContainer
      */
     @Override
-    public ImmutableMap<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {
+    public Map<String, BlobMetaData> listBlobsByPrefix(String blobNamePrefix) throws IOException {
         throw new UnsupportedOperationException("URL repository doesn't support this operation");
     }
 
diff --git a/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java b/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
index 1477179..add383b 100644
--- a/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
+++ b/core/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java
@@ -311,10 +311,6 @@ public class PagedBytesReference implements BytesReference {
             return true;
         }
 
-        if (obj == null) {
-            return false;
-        }
-
         if (!(obj instanceof PagedBytesReference)) {
             return BytesReference.Helper.bytesEqual(this, (BytesReference) obj);
         }
diff --git a/core/src/main/java/org/elasticsearch/common/cache/Cache.java b/core/src/main/java/org/elasticsearch/common/cache/Cache.java
new file mode 100644
index 0000000..d2d6970
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/Cache.java
@@ -0,0 +1,690 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+import org.elasticsearch.common.collect.Tuple;
+import org.elasticsearch.common.util.concurrent.ReleasableLock;
+
+import java.util.*;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.LongAdder;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+import java.util.function.ToLongBiFunction;
+
+/**
+ * A simple concurrent cache.
+ * <p>
+ * Cache is a simple concurrent cache that supports time-based and weight-based evictions, with notifications for all
+ * evictions. The design goals for this cache were simplicity and read performance. This means that we are willing to
+ * accept reduced write performance in exchange for easy-to-understand code. Cache statistics for hits, misses and
+ * evictions are exposed.
+ * <p>
+ * The design of the cache is relatively simple. The cache is segmented into 256 segments which are backed by HashMaps.
+ * Each segment is protected by a re-entrant read/write lock. The read/write locks permit multiple concurrent readers
+ * without contention, and the segments gives us write throughput without impacting readers (so readers are blocked only
+ * if they are reading a segment that a writer is writing to).
+ * <p>
+ * The LRU functionality is backed by a single doubly-linked list chaining the entries in order of insertion. This
+ * LRU list is protected by a lock that serializes all writes to it. There are opportunities for improvements
+ * here if write throughput is a concern.
+ * <ol>
+ * <li>LRU list mutations could be inserted into a blocking queue that a single thread is reading from
+ * and applying to the LRU list.</li>
+ * <li>Promotions could be deferred for entries that were "recently" promoted.</li>
+ * <li>Locks on the list could be taken per node being modified instead of globally.</li>
+ * </ol>
+ * <p>
+ * Evictions only occur after a mutation to the cache (meaning an entry promotion, a cache insertion, or a manual
+ * invalidation) or an explicit call to {@link #refresh()}.
+ *
+ * @param <K> The type of the keys
+ * @param <V> The type of the values
+ */
+public class Cache<K, V> {
+    // positive if entries have an expiration
+    private long expireAfterAccess = -1;
+
+    // true if entries can expire after access
+    private boolean entriesExpireAfterAccess;
+
+    // positive if entries have an expiration after write
+    private long expireAfterWrite = -1;
+
+    // true if entries can expire after initial insertion
+    private boolean entriesExpireAfterWrite;
+
+    // the number of entries in the cache
+    private int count = 0;
+
+    // the weight of the entries in the cache
+    private long weight = 0;
+
+    // the maximum weight that this cache supports
+    private long maximumWeight = -1;
+
+    // the weigher of entries
+    private ToLongBiFunction<K, V> weigher = (k, v) -> 1;
+
+    // the removal callback
+    private RemovalListener<K, V> removalListener = notification -> {
+    };
+
+    // use CacheBuilder to construct
+    Cache() {
+    }
+
+    void setExpireAfterAccess(long expireAfterAccess) {
+        if (expireAfterAccess <= 0) {
+            throw new IllegalArgumentException("expireAfterAccess <= 0");
+        }
+        this.expireAfterAccess = expireAfterAccess;
+        this.entriesExpireAfterAccess = true;
+    }
+
+    void setExpireAfterWrite(long expireAfterWrite) {
+        if (expireAfterWrite <= 0) {
+            throw new IllegalArgumentException("expireAfterWrite <= 0");
+        }
+        this.expireAfterWrite = expireAfterWrite;
+        this.entriesExpireAfterWrite = true;
+    }
+
+    void setMaximumWeight(long maximumWeight) {
+        if (maximumWeight < 0) {
+            throw new IllegalArgumentException("maximumWeight < 0");
+        }
+        this.maximumWeight = maximumWeight;
+    }
+
+    void setWeigher(ToLongBiFunction<K, V> weigher) {
+        Objects.requireNonNull(weigher);
+        this.weigher = weigher;
+    }
+
+    void setRemovalListener(RemovalListener<K, V> removalListener) {
+        Objects.requireNonNull(removalListener);
+        this.removalListener = removalListener;
+    }
+
+    /**
+     * The relative time used to track time-based evictions.
+     *
+     * @return the current relative time
+     */
+    protected long now() {
+        // System.nanoTime takes non-negligible time, so we only use it if we need it
+        // use System.nanoTime because we want relative time, not absolute time
+        return entriesExpireAfterAccess || entriesExpireAfterWrite ? System.nanoTime() : 0;
+    }
+
+    // the state of an entry in the LRU list
+    enum State {
+        NEW, EXISTING, DELETED
+    }
+
+    static class Entry<K, V> {
+        final K key;
+        final V value;
+        long writeTime;
+        volatile long accessTime;
+        Entry<K, V> before;
+        Entry<K, V> after;
+        State state = State.NEW;
+
+        public Entry(K key, V value, long writeTime) {
+            this.key = key;
+            this.value = value;
+            this.writeTime = this.accessTime = writeTime;
+        }
+    }
+
+    /**
+     * A cache segment.
+     * <p>
+     * A CacheSegment is backed by a HashMap and is protected by a read/write lock.
+     *
+     * @param <K> the type of the keys
+     * @param <V> the type of the values
+     */
+    private static class CacheSegment<K, V> {
+        // read/write lock protecting mutations to the segment
+        ReadWriteLock segmentLock = new ReentrantReadWriteLock();
+
+        ReleasableLock readLock = new ReleasableLock(segmentLock.readLock());
+        ReleasableLock writeLock = new ReleasableLock(segmentLock.writeLock());
+
+        Map<K, Entry<K, V>> map = new HashMap<>();
+        SegmentStats segmentStats = new SegmentStats();
+
+        /**
+         * get an entry from the segment
+         *
+         * @param key the key of the entry to get from the cache
+         * @param now the access time of this entry
+         * @return the entry if there was one, otherwise null
+         */
+        Entry<K, V> get(K key, long now) {
+            Entry<K, V> entry;
+            try (ReleasableLock ignored = readLock.acquire()) {
+                entry = map.get(key);
+            }
+            if (entry != null) {
+                segmentStats.hit();
+                entry.accessTime = now;
+            } else {
+                segmentStats.miss();
+            }
+            return entry;
+        }
+
+        /**
+         * put an entry into the segment
+         *
+         * @param key   the key of the entry to add to the cache
+         * @param value the value of the entry to add to the cache
+         * @param now   the access time of this entry
+         * @return a tuple of the new entry and the existing entry, if there was one otherwise null
+         */
+        Tuple<Entry<K, V>, Entry<K, V>> put(K key, V value, long now) {
+            Entry<K, V> entry = new Entry<>(key, value, now);
+            Entry<K, V> existing;
+            try (ReleasableLock ignored = writeLock.acquire()) {
+                existing = map.put(key, entry);
+            }
+            return Tuple.tuple(entry, existing);
+        }
+
+        /**
+         * remove an entry from the segment
+         *
+         * @param key the key of the entry to remove from the cache
+         * @return the removed entry if there was one, otherwise null
+         */
+        Entry<K, V> remove(K key) {
+            Entry<K, V> entry;
+            try (ReleasableLock ignored = writeLock.acquire()) {
+                entry = map.remove(key);
+            }
+            if (entry != null) {
+                segmentStats.eviction();
+            }
+            return entry;
+        }
+
+        private static class SegmentStats {
+            private final LongAdder hits = new LongAdder();
+            private final LongAdder misses = new LongAdder();
+            private final LongAdder evictions = new LongAdder();
+
+            void hit() {
+                hits.increment();
+            }
+
+            void miss() {
+                misses.increment();
+            }
+
+            void eviction() {
+                evictions.increment();
+            }
+        }
+    }
+
+    public static final int NUMBER_OF_SEGMENTS = 256;
+    private final CacheSegment<K, V>[] segments = new CacheSegment[NUMBER_OF_SEGMENTS];
+
+    {
+        for (int i = 0; i < segments.length; i++) {
+            segments[i] = new CacheSegment<>();
+        }
+    }
+
+    Entry<K, V> head;
+    Entry<K, V> tail;
+
+    // lock protecting mutations to the LRU list
+    private ReleasableLock lruLock = new ReleasableLock(new ReentrantLock());
+
+    /**
+     * Returns the value to which the specified key is mapped, or null if this map contains no mapping for the key.
+     *
+     * @param key the key whose associated value is to be returned
+     * @return the value to which the specified key is mapped, or null if this map contains no mapping for the key
+     */
+    public V get(K key) {
+        return get(key, now());
+    }
+
+    private V get(K key, long now) {
+        CacheSegment<K, V> segment = getCacheSegment(key);
+        Entry<K, V> entry = segment.get(key, now);
+        if (entry == null || isExpired(entry, now)) {
+            return null;
+        } else {
+            promote(entry, now);
+            return entry.value;
+        }
+    }
+
+    /**
+     * If the specified key is not already associated with a value (or is mapped to null), attempts to compute its
+     * value using the given mapping function and enters it into this map unless null.
+     *
+     * @param key    the key whose associated value is to be returned or computed for if non-existant
+     * @param loader the function to compute a value given a key
+     * @return the current (existing or computed) value associated with the specified key, or null if the computed
+     * value is null
+     * @throws ExecutionException thrown if loader throws an exception
+     */
+    public V computeIfAbsent(K key, CacheLoader<K, V> loader) throws ExecutionException {
+        long now = now();
+        V value = get(key, now);
+        if (value == null) {
+            CacheSegment<K, V> segment = getCacheSegment(key);
+            // we synchronize against the segment lock; this is to avoid a scenario where another thread is inserting
+            // a value for the same key via put which would not be observed on this thread without a mechanism
+            // synchronizing the two threads; it is possible that the segment lock will be too expensive here (it blocks
+            // readers too!) so consider this as a possible place to optimize should contention be observed
+            try (ReleasableLock ignored = segment.writeLock.acquire()) {
+                value = get(key, now);
+                if (value == null) {
+                    try {
+                        value = loader.load(key);
+                    } catch (Exception e) {
+                        throw new ExecutionException(e);
+                    }
+                    if (value == null) {
+                        throw new ExecutionException(new NullPointerException("loader returned a null value"));
+                    }
+                    put(key, value, now);
+                }
+            }
+        }
+        return value;
+    }
+
+    /**
+     * Associates the specified value with the specified key in this map. If the map previously contained a mapping for
+     * the key, the old value is replaced.
+     *
+     * @param key   key with which the specified value is to be associated
+     * @param value value to be associated with the specified key
+     */
+    public void put(K key, V value) {
+        long now = now();
+        put(key, value, now);
+    }
+
+    private void put(K key, V value, long now) {
+        CacheSegment<K, V> segment = getCacheSegment(key);
+        Tuple<Entry<K, V>, Entry<K, V>> tuple = segment.put(key, value, now);
+        boolean replaced = false;
+        try (ReleasableLock ignored = lruLock.acquire()) {
+            if (tuple.v2() != null && tuple.v2().state == State.EXISTING) {
+                if (unlink(tuple.v2())) {
+                    replaced = true;
+                }
+            }
+            promote(tuple.v1(), now);
+        }
+        if (replaced) {
+            removalListener.onRemoval(new RemovalNotification(tuple.v2().key, tuple.v2().value, RemovalNotification.RemovalReason.REPLACED));
+        }
+    }
+
+    /**
+     * Invalidate the association for the specified key. A removal notification will be issued for invalidated
+     * entries with {@link org.elasticsearch.common.cache.RemovalNotification.RemovalReason} INVALIDATED.
+     *
+     * @param key the key whose mapping is to be invalidated from the cache
+     */
+    public void invalidate(K key) {
+        CacheSegment<K, V> segment = getCacheSegment(key);
+        Entry<K, V> entry = segment.remove(key);
+        if (entry != null) {
+            try (ReleasableLock ignored = lruLock.acquire()) {
+                delete(entry, RemovalNotification.RemovalReason.INVALIDATED);
+            }
+        }
+    }
+
+    /**
+     * Invalidate all cache entries. A removal notification will be issued for invalidated entries with
+     * {@link org.elasticsearch.common.cache.RemovalNotification.RemovalReason} INVALIDATED.
+     */
+    public void invalidateAll() {
+        Entry<K, V> h;
+
+        boolean[] haveSegmentLock = new boolean[NUMBER_OF_SEGMENTS];
+        try {
+            for (int i = 0; i < NUMBER_OF_SEGMENTS; i++) {
+                segments[i].segmentLock.writeLock().lock();
+                haveSegmentLock[i] = true;
+            }
+            try (ReleasableLock ignored = lruLock.acquire()) {
+                h = head;
+                Arrays.stream(segments).forEach(segment -> segment.map = new HashMap<>());
+                Entry<K, V> current = head;
+                while (current != null) {
+                    current.state = State.DELETED;
+                    current = current.after;
+                }
+                head = tail = null;
+                count = 0;
+                weight = 0;
+            }
+        } finally {
+            for (int i = NUMBER_OF_SEGMENTS - 1; i >= 0; i--) {
+                if (haveSegmentLock[i]) {
+                    segments[i].segmentLock.writeLock().unlock();
+                }
+            }
+        }
+        while (h != null) {
+            removalListener.onRemoval(new RemovalNotification<>(h.key, h.value, RemovalNotification.RemovalReason.INVALIDATED));
+            h = h.after;
+        }
+    }
+
+    /**
+     * Force any outstanding size-based and time-based evictions to occur
+     */
+    public void refresh() {
+        long now = now();
+        try (ReleasableLock ignored = lruLock.acquire()) {
+            evict(now);
+        }
+    }
+
+    /**
+     * The number of entries in the cache.
+     *
+     * @return the number of entries in the cache
+     */
+    public int count() {
+        return count;
+    }
+
+    /**
+     * The weight of the entries in the cache.
+     *
+     * @return the weight of the entries in the cache
+     */
+    public long weight() {
+        return weight;
+    }
+
+    /**
+     * An LRU sequencing of the keys in the cache that supports removal. This sequence is not protected from mutations
+     * to the cache (except for {@link Iterator#remove()}. The result of iteration under any other mutation is
+     * undefined.
+     *
+     * @return an LRU-ordered {@link Iterable} over the keys in the cache
+     */
+    public Iterable<K> keys() {
+        return () -> new Iterator<K>() {
+            private CacheIterator iterator = new CacheIterator(head);
+
+            @Override
+            public boolean hasNext() {
+                return iterator.hasNext();
+            }
+
+            @Override
+            public K next() {
+                return iterator.next().key;
+            }
+
+            @Override
+            public void remove() {
+                iterator.remove();
+            }
+        };
+    }
+
+    /**
+     * An LRU sequencing of the values in the cache. This sequence is not protected from mutations
+     * to the cache. The result of iteration under mutation is undefined.
+     *
+     * @return an LRU-ordered {@link Iterable} over the values in the cache
+     */
+    public Iterable<V> values() {
+        return () -> new Iterator<V>() {
+            private CacheIterator iterator = new CacheIterator(head);
+
+            @Override
+            public boolean hasNext() {
+                return iterator.hasNext();
+            }
+
+            @Override
+            public V next() {
+                return iterator.next().value;
+            }
+        };
+    }
+
+    private class CacheIterator implements Iterator<Entry<K, V>> {
+        private Entry<K, V> current;
+        private Entry<K, V> next;
+
+        CacheIterator(Entry<K, V> head) {
+            current = null;
+            next = head;
+        }
+
+        @Override
+        public boolean hasNext() {
+            return next != null;
+        }
+
+        @Override
+        public Entry<K, V> next() {
+            current = next;
+            next = next.after;
+            return current;
+        }
+
+        @Override
+        public void remove() {
+            Entry<K, V> entry = current;
+            if (entry != null) {
+                CacheSegment<K, V> segment = getCacheSegment(entry.key);
+                segment.remove(entry.key);
+                try (ReleasableLock ignored = lruLock.acquire()) {
+                    current = null;
+                    delete(entry, RemovalNotification.RemovalReason.INVALIDATED);
+                }
+            }
+        }
+    }
+
+    /**
+     * The cache statistics tracking hits, misses and evictions. These are taken on a best-effort basis meaning that
+     * they could be out-of-date mid-flight.
+     *
+     * @return the current cache statistics
+     */
+    public CacheStats stats() {
+        long hits = 0;
+        long misses = 0;
+        long evictions = 0;
+        for (int i = 0; i < segments.length; i++) {
+            hits += segments[i].segmentStats.hits.longValue();
+            misses += segments[i].segmentStats.misses.longValue();
+            evictions += segments[i].segmentStats.evictions.longValue();
+        }
+        return new CacheStats(hits, misses, evictions);
+    }
+
+    public static class CacheStats {
+        private long hits;
+        private long misses;
+        private long evictions;
+
+        public CacheStats(long hits, long misses, long evictions) {
+            this.hits = hits;
+            this.misses = misses;
+            this.evictions = evictions;
+        }
+
+        public long getHits() {
+            return hits;
+        }
+
+        public long getMisses() {
+            return misses;
+        }
+
+        public long getEvictions() {
+            return evictions;
+        }
+    }
+
+    private boolean promote(Entry<K, V> entry, long now) {
+        boolean promoted = true;
+        try (ReleasableLock ignored = lruLock.acquire()) {
+            switch (entry.state) {
+                case DELETED:
+                    promoted = false;
+                    break;
+                case EXISTING:
+                    relinkAtHead(entry);
+                    break;
+                case NEW:
+                    linkAtHead(entry);
+                    break;
+            }
+            if (promoted) {
+                evict(now);
+            }
+        }
+        return promoted;
+    }
+
+    private void evict(long now) {
+        assert lruLock.isHeldByCurrentThread();
+
+        while (tail != null && shouldPrune(tail, now)) {
+            CacheSegment<K, V> segment = getCacheSegment(tail.key);
+            Entry<K, V> entry = tail;
+            if (segment != null) {
+                segment.remove(tail.key);
+            }
+            delete(entry, RemovalNotification.RemovalReason.EVICTED);
+        }
+    }
+
+    private void delete(Entry<K, V> entry, RemovalNotification.RemovalReason removalReason) {
+        assert lruLock.isHeldByCurrentThread();
+
+        if (unlink(entry)) {
+            removalListener.onRemoval(new RemovalNotification<>(entry.key, entry.value, removalReason));
+        }
+    }
+
+    private boolean shouldPrune(Entry<K, V> entry, long now) {
+        return exceedsWeight() || isExpired(entry, now);
+    }
+
+    private boolean exceedsWeight() {
+        return maximumWeight != -1 && weight > maximumWeight;
+    }
+
+    private boolean isExpired(Entry<K, V> entry, long now) {
+        return (entriesExpireAfterAccess && now - entry.accessTime > expireAfterAccess) ||
+                (entriesExpireAfterWrite && now - entry.writeTime > expireAfterWrite);
+    }
+
+    private boolean unlink(Entry<K, V> entry) {
+        assert lruLock.isHeldByCurrentThread();
+
+        if (entry.state == State.EXISTING) {
+            final Entry<K, V> before = entry.before;
+            final Entry<K, V> after = entry.after;
+
+            if (before == null) {
+                // removing the head
+                assert head == entry;
+                head = after;
+                if (head != null) {
+                    head.before = null;
+                }
+            } else {
+                // removing inner element
+                before.after = after;
+                entry.before = null;
+            }
+
+            if (after == null) {
+                // removing tail
+                assert tail == entry;
+                tail = before;
+                if (tail != null) {
+                    tail.after = null;
+                }
+            } else {
+                // removing inner element
+                after.before = before;
+                entry.after = null;
+            }
+
+            count--;
+            weight -= weigher.applyAsLong(entry.key, entry.value);
+            entry.state = State.DELETED;
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    private void linkAtHead(Entry<K, V> entry) {
+        assert lruLock.isHeldByCurrentThread();
+
+        Entry<K, V> h = head;
+        entry.before = null;
+        entry.after = head;
+        head = entry;
+        if (h == null) {
+            tail = entry;
+        } else {
+            h.before = entry;
+        }
+
+        count++;
+        weight += weigher.applyAsLong(entry.key, entry.value);
+        entry.state = State.EXISTING;
+    }
+
+    private void relinkAtHead(Entry<K, V> entry) {
+        assert lruLock.isHeldByCurrentThread();
+
+        if (head != entry) {
+            unlink(entry);
+            linkAtHead(entry);
+        }
+    }
+
+    private CacheSegment<K, V> getCacheSegment(K key) {
+        return segments[key.hashCode() & 0xff];
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/cache/CacheBuilder.java b/core/src/main/java/org/elasticsearch/common/cache/CacheBuilder.java
new file mode 100644
index 0000000..ffb0e59
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/CacheBuilder.java
@@ -0,0 +1,94 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+import java.util.Objects;
+import java.util.function.ToLongBiFunction;
+
+public class CacheBuilder<K, V> {
+    private long maximumWeight = -1;
+    private long expireAfterAccess = -1;
+    private long expireAfterWrite = -1;
+    private ToLongBiFunction<K, V> weigher;
+    private RemovalListener<K, V> removalListener;
+
+    public static <K, V> CacheBuilder<K, V> builder() {
+        return new CacheBuilder<>();
+    }
+
+    private CacheBuilder() {
+    }
+
+    public CacheBuilder<K, V> setMaximumWeight(long maximumWeight) {
+        if (maximumWeight < 0) {
+            throw new IllegalArgumentException("maximumWeight < 0");
+        }
+        this.maximumWeight = maximumWeight;
+        return this;
+    }
+
+    public CacheBuilder<K, V> setExpireAfterAccess(long expireAfterAccess) {
+        if (expireAfterAccess <= 0) {
+            throw new IllegalArgumentException("expireAfterAccess <= 0");
+        }
+        this.expireAfterAccess = expireAfterAccess;
+        return this;
+    }
+
+    public CacheBuilder<K, V> setExpireAfterWrite(long expireAfterWrite) {
+        if (expireAfterWrite <= 0) {
+            throw new IllegalArgumentException("expireAfterWrite <= 0");
+        }
+        this.expireAfterWrite = expireAfterWrite;
+        return this;
+    }
+
+    public CacheBuilder<K, V> weigher(ToLongBiFunction<K, V> weigher) {
+        Objects.requireNonNull(weigher);
+        this.weigher = weigher;
+        return this;
+    }
+
+    public CacheBuilder<K, V> removalListener(RemovalListener<K, V> removalListener) {
+        Objects.requireNonNull(removalListener);
+        this.removalListener = removalListener;
+        return this;
+    }
+
+    public Cache<K, V> build() {
+        Cache<K, V> cache = new Cache();
+        if (maximumWeight != -1) {
+            cache.setMaximumWeight(maximumWeight);
+        }
+        if (expireAfterAccess != -1) {
+            cache.setExpireAfterAccess(expireAfterAccess);
+        }
+        if (expireAfterWrite != -1) {
+            cache.setExpireAfterWrite(expireAfterWrite);
+        }
+        if (weigher != null) {
+            cache.setWeigher(weigher);
+        }
+        if (removalListener != null) {
+            cache.setRemovalListener(removalListener);
+        }
+        return cache;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/cache/CacheLoader.java b/core/src/main/java/org/elasticsearch/common/cache/CacheLoader.java
new file mode 100644
index 0000000..85636e1
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/CacheLoader.java
@@ -0,0 +1,25 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+@FunctionalInterface
+public interface CacheLoader<K, V> {
+    V load(K key) throws Exception;
+}
diff --git a/core/src/main/java/org/elasticsearch/common/cache/RemovalListener.java b/core/src/main/java/org/elasticsearch/common/cache/RemovalListener.java
new file mode 100644
index 0000000..ae13300
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/RemovalListener.java
@@ -0,0 +1,25 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+@FunctionalInterface
+public interface RemovalListener<K, V> {
+    void onRemoval(RemovalNotification<K, V> notification);
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/cache/RemovalNotification.java b/core/src/main/java/org/elasticsearch/common/cache/RemovalNotification.java
new file mode 100644
index 0000000..afea5a5
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/common/cache/RemovalNotification.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+public class RemovalNotification<K, V> {
+    public enum RemovalReason {REPLACED, INVALIDATED, EVICTED}
+
+    private final K key;
+    private final V value;
+    private final RemovalReason removalReason;
+
+    public RemovalNotification(K key, V value, RemovalReason removalReason) {
+        this.key = key;
+        this.value = value;
+        this.removalReason = removalReason;
+    }
+
+    public K getKey() {
+        return key;
+    }
+
+    public V getValue() {
+        return value;
+    }
+
+    public RemovalReason getRemovalReason() {
+        return removalReason;
+    }
+}
diff --git a/core/src/main/java/org/elasticsearch/common/collect/MapBuilder.java b/core/src/main/java/org/elasticsearch/common/collect/MapBuilder.java
index cafeaad4..bfb0f42 100644
--- a/core/src/main/java/org/elasticsearch/common/collect/MapBuilder.java
+++ b/core/src/main/java/org/elasticsearch/common/collect/MapBuilder.java
@@ -19,11 +19,11 @@
 
 package org.elasticsearch.common.collect;
 
-import com.google.common.collect.ImmutableMap;
-
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
@@ -83,7 +83,12 @@ public class MapBuilder<K, V> {
         return this.map;
     }
 
-    public ImmutableMap<K, V> immutableMap() {
-        return ImmutableMap.copyOf(map);
+    /**
+     * Build an immutable copy of the map under construction. Always copies the map under construction. Prefer building
+     * a HashMap by hand and wrapping it in an unmodifiableMap
+     */
+    public Map<K, V> immutableMap() {
+        // TODO: follow the directions in the Javadoc for this method
+        return unmodifiableMap(new HashMap<>(map));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
index ea5a664..3837de8 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java
@@ -16,8 +16,6 @@
 
 package org.elasticsearch.common.inject.assistedinject;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.common.inject.ConfigurationException;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Injector;
@@ -42,6 +40,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.emptyMap;
 import static java.util.Collections.singleton;
 import static java.util.Collections.unmodifiableSet;
 
@@ -223,7 +222,7 @@ public class FactoryProvider<F> implements Provider<F>, HasDependencies {
         }
 
         if (constructors.isEmpty()) {
-            return ImmutableMap.of();
+            return emptyMap();
         }
 
         Method[] factoryMethods = factoryType.getRawType().getMethods();
diff --git a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
index 0c07e2a..1f0e05f 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider2.java
@@ -16,8 +16,6 @@
 
 package org.elasticsearch.common.inject.assistedinject;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.Binder;
 import org.elasticsearch.common.inject.Binding;
@@ -41,9 +39,11 @@ import java.lang.reflect.Proxy;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.inject.internal.Annotations.getKey;
 
 /**
@@ -91,7 +91,7 @@ public final class FactoryProvider2<F> implements InvocationHandler, Provider<F>
      */
     private final Key<?> producedType;
     private final Map<Method, Key<?>> returnTypesByMethod;
-    private final ImmutableMap<Method, List<Key<?>>> paramTypes;
+    private final Map<Method, List<Key<?>>> paramTypes;
 
     /**
      * the hosting injector, or null if we haven't been initialized yet
@@ -117,9 +117,8 @@ public final class FactoryProvider2<F> implements InvocationHandler, Provider<F>
                 Class<F> factoryRawType = (Class) factoryType.getRawType();
 
         try {
-            ImmutableMap.Builder<Method, Key<?>> returnTypesBuilder = ImmutableMap.builder();
-            ImmutableMap.Builder<Method, List<Key<?>>> paramTypesBuilder
-                    = ImmutableMap.builder();
+            Map<Method, Key<?>> returnTypesBuilder = new HashMap<>();
+            Map<Method, List<Key<?>>> paramTypesBuilder = new HashMap<>();
             // TODO: also grab methods from superinterfaces
             for (Method method : factoryRawType.getMethods()) {
                 Key<?> returnType = getKey(
@@ -135,8 +134,8 @@ public final class FactoryProvider2<F> implements InvocationHandler, Provider<F>
                 }
                 paramTypesBuilder.put(method, Collections.unmodifiableList(keys));
             }
-            returnTypesByMethod = returnTypesBuilder.build();
-            paramTypes = paramTypesBuilder.build();
+            returnTypesByMethod = unmodifiableMap(returnTypesBuilder);
+            paramTypes = unmodifiableMap(paramTypesBuilder);
         } catch (ErrorsException e) {
             throw new ConfigurationException(e.getErrors().getMessages());
         }
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java b/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
index 1b68f2c..63d8e40 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/MoreTypes.java
@@ -17,8 +17,6 @@
 
 package org.elasticsearch.common.inject.internal;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.common.inject.ConfigurationException;
 import org.elasticsearch.common.inject.TypeLiteral;
 import org.elasticsearch.common.inject.spi.Message;
@@ -35,11 +33,13 @@ import java.lang.reflect.Type;
 import java.lang.reflect.TypeVariable;
 import java.lang.reflect.WildcardType;
 import java.util.Arrays;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.NoSuchElementException;
 import java.util.Objects;
 
 import static java.util.Collections.singleton;
+import static java.util.Collections.unmodifiableMap;
 
 /**
  * Static methods for working with types that we aren't publishing in the
@@ -54,18 +54,20 @@ public class MoreTypes {
     private MoreTypes() {
     }
 
-    private static final Map<TypeLiteral<?>, TypeLiteral<?>> PRIMITIVE_TO_WRAPPER
-            = new ImmutableMap.Builder<TypeLiteral<?>, TypeLiteral<?>>()
-            .put(TypeLiteral.get(boolean.class), TypeLiteral.get(Boolean.class))
-            .put(TypeLiteral.get(byte.class), TypeLiteral.get(Byte.class))
-            .put(TypeLiteral.get(short.class), TypeLiteral.get(Short.class))
-            .put(TypeLiteral.get(int.class), TypeLiteral.get(Integer.class))
-            .put(TypeLiteral.get(long.class), TypeLiteral.get(Long.class))
-            .put(TypeLiteral.get(float.class), TypeLiteral.get(Float.class))
-            .put(TypeLiteral.get(double.class), TypeLiteral.get(Double.class))
-            .put(TypeLiteral.get(char.class), TypeLiteral.get(Character.class))
-            .put(TypeLiteral.get(void.class), TypeLiteral.get(Void.class))
-            .build();
+    private static final Map<TypeLiteral<?>, TypeLiteral<?>> PRIMITIVE_TO_WRAPPER;
+    static {
+        Map<TypeLiteral<?>, TypeLiteral<?>> primitiveToWrapper = new HashMap<>();
+        primitiveToWrapper.put(TypeLiteral.get(boolean.class), TypeLiteral.get(Boolean.class));
+        primitiveToWrapper.put(TypeLiteral.get(byte.class), TypeLiteral.get(Byte.class));
+        primitiveToWrapper.put(TypeLiteral.get(short.class), TypeLiteral.get(Short.class));
+        primitiveToWrapper.put(TypeLiteral.get(int.class), TypeLiteral.get(Integer.class));
+        primitiveToWrapper.put(TypeLiteral.get(long.class), TypeLiteral.get(Long.class));
+        primitiveToWrapper.put(TypeLiteral.get(float.class), TypeLiteral.get(Float.class));
+        primitiveToWrapper.put(TypeLiteral.get(double.class), TypeLiteral.get(Double.class));
+        primitiveToWrapper.put(TypeLiteral.get(char.class), TypeLiteral.get(Character.class));
+        primitiveToWrapper.put(TypeLiteral.get(void.class), TypeLiteral.get(Void.class));
+        PRIMITIVE_TO_WRAPPER = unmodifiableMap(primitiveToWrapper);
+    }
 
     /**
      * Returns an equivalent type that's safe for use in a key. The returned type will be free of
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java b/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
index aad0c3a..bb057d6 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/Nullability.java
@@ -1,3 +1,19 @@
+/*
+ * Copyright (C) 2010 Google Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.elasticsearch.common.inject.internal;
 
 import java.lang.annotation.Annotation;
diff --git a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
index 29f7886..34cb541 100644
--- a/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
+++ b/core/src/main/java/org/elasticsearch/common/inject/internal/PrivateElementsImpl.java
@@ -16,7 +16,6 @@
 
 package org.elasticsearch.common.inject.internal;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.inject.Binder;
 import org.elasticsearch.common.inject.Injector;
 import org.elasticsearch.common.inject.Key;
@@ -25,7 +24,15 @@ import org.elasticsearch.common.inject.spi.Element;
 import org.elasticsearch.common.inject.spi.ElementVisitor;
 import org.elasticsearch.common.inject.spi.PrivateElements;
 
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Set;
+
+import static java.util.Collections.unmodifiableMap;
 
 /**
  * @author jessewilson@google.com (Jesse Wilson)
@@ -92,7 +99,7 @@ public final class PrivateElementsImpl implements PrivateElements {
             for (ExposureBuilder<?> exposureBuilder : exposureBuilders) {
                 exposedKeysToSourcesMutable.put(exposureBuilder.getKey(), exposureBuilder.getSource());
             }
-            exposedKeysToSources = ImmutableMap.copyOf(exposedKeysToSourcesMutable);
+            exposedKeysToSources = unmodifiableMap(exposedKeysToSourcesMutable);
             exposureBuilders = null;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
index a715d34..5e7517e 100644
--- a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
+++ b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
@@ -19,10 +19,8 @@
 
 package org.elasticsearch.common.logging.log4j;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.log4j.PropertyConfigurator;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.env.Environment;
@@ -36,10 +34,12 @@ import java.nio.file.SimpleFileVisitor;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.Arrays;
 import java.util.EnumSet;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.Strings.cleanPath;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 
@@ -50,39 +50,42 @@ public class LogConfigurator {
 
     static final List<String> ALLOWED_SUFFIXES = Arrays.asList(".yml", ".yaml", ".json", ".properties");
 
-    private static boolean loaded;
+    private static final Map<String, String> REPLACEMENTS;
+    static {
+        Map<String, String> replacements = new HashMap<>();
+        replacements.put("console", "org.elasticsearch.common.logging.log4j.ConsoleAppender");
+        replacements.put("async", "org.apache.log4j.AsyncAppender");
+        replacements.put("dailyRollingFile", "org.apache.log4j.DailyRollingFileAppender");
+        replacements.put("externallyRolledFile", "org.apache.log4j.ExternallyRolledFileAppender");
+        replacements.put("file", "org.apache.log4j.FileAppender");
+        replacements.put("jdbc", "org.apache.log4j.jdbc.JDBCAppender");
+        replacements.put("jms", "org.apache.log4j.net.JMSAppender");
+        replacements.put("lf5", "org.apache.log4j.lf5.LF5Appender");
+        replacements.put("ntevent", "org.apache.log4j.nt.NTEventLogAppender");
+        replacements.put("null", "org.apache.log4j.NullAppender");
+        replacements.put("rollingFile", "org.apache.log4j.RollingFileAppender");
+        replacements.put("extrasRollingFile", "org.apache.log4j.rolling.RollingFileAppender");
+        replacements.put("smtp", "org.apache.log4j.net.SMTPAppender");
+        replacements.put("socket", "org.apache.log4j.net.SocketAppender");
+        replacements.put("socketHub", "org.apache.log4j.net.SocketHubAppender");
+        replacements.put("syslog", "org.apache.log4j.net.SyslogAppender");
+        replacements.put("telnet", "org.apache.log4j.net.TelnetAppender");
+        replacements.put("terminal", "org.elasticsearch.common.logging.log4j.TerminalAppender");
+                // policies
+        replacements.put("timeBased", "org.apache.log4j.rolling.TimeBasedRollingPolicy");
+        replacements.put("sizeBased", "org.apache.log4j.rolling.SizeBasedTriggeringPolicy");
+                // layouts
+        replacements.put("simple", "org.apache.log4j.SimpleLayout");
+        replacements.put("html", "org.apache.log4j.HTMLLayout");
+        replacements.put("pattern", "org.apache.log4j.PatternLayout");
+        replacements.put("consolePattern", "org.apache.log4j.PatternLayout");
+        replacements.put("enhancedPattern", "org.apache.log4j.EnhancedPatternLayout");
+        replacements.put("ttcc", "org.apache.log4j.TTCCLayout");
+        replacements.put("xml", "org.apache.log4j.XMLLayout");
+        REPLACEMENTS = unmodifiableMap(replacements);
+    }
 
-    private static ImmutableMap<String, String> replacements = new MapBuilder<String, String>()
-            .put("console", "org.elasticsearch.common.logging.log4j.ConsoleAppender")
-            .put("async", "org.apache.log4j.AsyncAppender")
-            .put("dailyRollingFile", "org.apache.log4j.DailyRollingFileAppender")
-            .put("externallyRolledFile", "org.apache.log4j.ExternallyRolledFileAppender")
-            .put("file", "org.apache.log4j.FileAppender")
-            .put("jdbc", "org.apache.log4j.jdbc.JDBCAppender")
-            .put("jms", "org.apache.log4j.net.JMSAppender")
-            .put("lf5", "org.apache.log4j.lf5.LF5Appender")
-            .put("ntevent", "org.apache.log4j.nt.NTEventLogAppender")
-            .put("null", "org.apache.log4j.NullAppender")
-            .put("rollingFile", "org.apache.log4j.RollingFileAppender")
-            .put("extrasRollingFile", "org.apache.log4j.rolling.RollingFileAppender")
-            .put("smtp", "org.apache.log4j.net.SMTPAppender")
-            .put("socket", "org.apache.log4j.net.SocketAppender")
-            .put("socketHub", "org.apache.log4j.net.SocketHubAppender")
-            .put("syslog", "org.apache.log4j.net.SyslogAppender")
-            .put("telnet", "org.apache.log4j.net.TelnetAppender")
-            .put("terminal", "org.elasticsearch.common.logging.log4j.TerminalAppender")
-                    // policies
-            .put("timeBased", "org.apache.log4j.rolling.TimeBasedRollingPolicy")
-            .put("sizeBased", "org.apache.log4j.rolling.SizeBasedTriggeringPolicy")
-                    // layouts
-            .put("simple", "org.apache.log4j.SimpleLayout")
-            .put("html", "org.apache.log4j.HTMLLayout")
-            .put("pattern", "org.apache.log4j.PatternLayout")
-            .put("consolePattern", "org.apache.log4j.PatternLayout")
-            .put("enhancedPattern", "org.apache.log4j.EnhancedPatternLayout")
-            .put("ttcc", "org.apache.log4j.TTCCLayout")
-            .put("xml", "org.apache.log4j.XMLLayout")
-            .immutableMap();
+    private static boolean loaded;
 
     /**
      * Consolidates settings and converts them into actual log4j settings, then initializes loggers and appenders.
@@ -112,9 +115,7 @@ public class LogConfigurator {
         for (Map.Entry<String, String> entry : settingsBuilder.build().getAsMap().entrySet()) {
             String key = "log4j." + entry.getKey();
             String value = entry.getValue();
-            if (replacements.containsKey(value)) {
-                value = replacements.get(value);
-            }
+            value = REPLACEMENTS.getOrDefault(value, value);
             if (key.endsWith(".value")) {
                 props.setProperty(key.substring(0, key.length() - ".value".length()), value);
             } else if (key.endsWith(".type")) {
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java b/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
index 060482e..3aaaf96 100644
--- a/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
+++ b/core/src/main/java/org/elasticsearch/common/lucene/Lucene.java
@@ -46,14 +46,11 @@ import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.index.analysis.AnalyzerScope;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 import java.text.ParseException;
 import java.util.*;
 
-import static org.elasticsearch.common.lucene.search.NoopCollector.NOOP_COLLECTOR;
-
 /**
  *
  */
@@ -229,27 +226,6 @@ public class Lucene {
         }.run();
     }
 
-    public static long count(IndexSearcher searcher, Query query) throws IOException {
-        return searcher.count(query);
-    }
-
-    /**
-     * Performs a count on the <code>searcher</code> for <code>query</code>. Terminates
-     * early when the count has reached <code>terminateAfter</code>
-     */
-    public static long count(IndexSearcher searcher, Query query, int terminateAfterCount) throws IOException {
-        EarlyTerminatingCollector countCollector = createCountBasedEarlyTerminatingCollector(terminateAfterCount);
-        countWithEarlyTermination(searcher, query, countCollector);
-        return countCollector.count();
-    }
-
-    /**
-     * Creates count based early termination collector with a threshold of <code>maxCountHits</code>
-     */
-    public final static EarlyTerminatingCollector createCountBasedEarlyTerminatingCollector(int maxCountHits) {
-        return new EarlyTerminatingCollector(maxCountHits);
-    }
-
     /**
      * Wraps <code>delegate</code> with count based early termination collector with a threshold of <code>maxCountHits</code>
      */
@@ -265,99 +241,27 @@ public class Lucene {
     }
 
     /**
-     * Performs an exists (count &gt; 0) query on the <code>searcher</code> for <code>query</code>
-     * with <code>filter</code> using the given <code>collector</code>
-     *
-     * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
+     * Check whether there is one or more documents matching the provided query.
      */
-    public static boolean exists(IndexSearcher searcher, Query query, Filter filter,
-                                 EarlyTerminatingCollector collector) throws IOException {
-        collector.reset();
-        countWithEarlyTermination(searcher, filter, query, collector);
-        return collector.exists();
-    }
-
-
-    /**
-     * Performs an exists (count &gt; 0) query on the <code>searcher</code> for <code>query</code>
-     * using the given <code>collector</code>
-     *
-     * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
-     */
-    public static boolean exists(IndexSearcher searcher, Query query, EarlyTerminatingCollector collector) throws IOException {
-        collector.reset();
-        countWithEarlyTermination(searcher, query, collector);
-        return collector.exists();
-    }
-
-    /**
-     * Calls <code>countWithEarlyTermination(searcher, null, query, collector)</code>
-     */
-    public static boolean countWithEarlyTermination(IndexSearcher searcher, Query query,
-                                                  EarlyTerminatingCollector collector) throws IOException {
-        return countWithEarlyTermination(searcher, null, query, collector);
-    }
-
-    /**
-     * Performs a count on <code>query</code> and <code>filter</code> with early termination using <code>searcher</code>.
-     * The early termination threshold is specified by the provided <code>collector</code>
-     */
-    public static boolean countWithEarlyTermination(IndexSearcher searcher, Filter filter, Query query,
-                                                        EarlyTerminatingCollector collector) throws IOException {
-        try {
-            if (filter == null) {
-                searcher.search(query, collector);
-            } else {
-                searcher.search(query, filter, collector);
+    public static boolean exists(IndexSearcher searcher, Query query) throws IOException {
+        final Weight weight = searcher.createNormalizedWeight(query, false);
+        // the scorer API should be more efficient at stopping after the first
+        // match than the bulk scorer API
+        for (LeafReaderContext context : searcher.getIndexReader().leaves()) {
+            final Scorer scorer = weight.scorer(context);
+            if (scorer == null) {
+                continue;
+            }
+            final Bits liveDocs = context.reader().getLiveDocs();
+            for (int doc = scorer.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = scorer.nextDoc()) {
+                if (liveDocs == null || liveDocs.get(doc)) {
+                    return true;
+                }
             }
-        } catch (EarlyTerminationException e) {
-            // early termination
-            return true;
         }
         return false;
     }
 
-    /**
-     * Performs an exists (count &gt; 0) query on the searcher from the <code>searchContext</code> for <code>query</code>
-     * using the given <code>collector</code>
-     *
-     * The <code>collector</code> can be instantiated using <code>Lucene.createExistsCollector()</code>
-     */
-    public static boolean exists(SearchContext searchContext, Query query, EarlyTerminatingCollector collector) throws IOException {
-        collector.reset();
-        try {
-            searchContext.searcher().search(query, collector);
-        } catch (EarlyTerminationException e) {
-            // ignore, just early termination...
-        } finally {
-            searchContext.clearReleasables(SearchContext.Lifetime.COLLECTION);
-        }
-        return collector.exists();
-    }
-
-    /**
-     * Creates an {@link org.elasticsearch.common.lucene.Lucene.EarlyTerminatingCollector}
-     * with a threshold of <code>1</code>
-     */
-    public final static EarlyTerminatingCollector createExistsCollector() {
-        return createCountBasedEarlyTerminatingCollector(1);
-    }
-
-    /**
-     * Closes the index writer, returning <tt>false</tt> if it failed to close.
-     */
-    public static boolean safeClose(IndexWriter writer) {
-        if (writer == null) {
-            return true;
-        }
-        try {
-            writer.close();
-            return true;
-        } catch (Throwable e) {
-            return false;
-        }
-    }
-
     public static TopDocs readTopDocs(StreamInput in) throws IOException {
         if (in.readBoolean()) {
             int totalHits = in.readVInt();
@@ -612,19 +516,11 @@ public class Lucene {
         private int count = 0;
         private LeafCollector leafCollector;
 
-        EarlyTerminatingCollector(int maxCountHits) {
-            this.maxCountHits = maxCountHits;
-            this.delegate = NOOP_COLLECTOR;
-        }
-
         EarlyTerminatingCollector(final Collector delegate, int maxCountHits) {
             this.maxCountHits = maxCountHits;
-            this.delegate = (delegate == null) ? NOOP_COLLECTOR : delegate;
+            this.delegate = Objects.requireNonNull(delegate);
         }
 
-        public void reset() {
-            count = 0;
-        }
         public int count() {
             return count;
         }
diff --git a/core/src/main/java/org/elasticsearch/common/lucene/search/NoopCollector.java b/core/src/main/java/org/elasticsearch/common/lucene/search/NoopCollector.java
deleted file mode 100644
index 99845ea..0000000
--- a/core/src/main/java/org/elasticsearch/common/lucene/search/NoopCollector.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common.lucene.search;
-
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.SimpleCollector;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class NoopCollector extends SimpleCollector {
-
-    public static final NoopCollector NOOP_COLLECTOR = new NoopCollector();
-
-    @Override
-    public void setScorer(Scorer scorer) throws IOException {
-    }
-
-    @Override
-    public void collect(int doc) throws IOException {
-    }
-
-    @Override
-    protected void doSetNextReader(LeafReaderContext context) throws IOException {
-    }
-
-    @Override
-    public boolean needsScores() {
-        return false;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/common/path/PathTrie.java b/core/src/main/java/org/elasticsearch/common/path/PathTrie.java
index 3bf2a9b..2bee827 100644
--- a/core/src/main/java/org/elasticsearch/common/path/PathTrie.java
+++ b/core/src/main/java/org/elasticsearch/common/path/PathTrie.java
@@ -19,14 +19,15 @@
 
 package org.elasticsearch.common.path;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.Strings;
 
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -45,7 +46,7 @@ public class PathTrie<T> {
     };
 
     private final Decoder decoder;
-    private final TrieNode<T> root;
+    private final TrieNode root;
     private final char separator;
     private T rootValue;
 
@@ -60,10 +61,10 @@ public class PathTrie<T> {
     public PathTrie(char separator, String wildcard, Decoder decoder) {
         this.decoder = decoder;
         this.separator = separator;
-        root = new TrieNode<>(new String(new char[]{separator}), null, null, wildcard);
+        root = new TrieNode(new String(new char[]{separator}), null, wildcard);
     }
 
-    public class TrieNode<T> {
+    public class TrieNode {
         private transient String key;
         private transient T value;
         private boolean isWildcard;
@@ -71,17 +72,14 @@ public class PathTrie<T> {
 
         private transient String namedWildcard;
 
-        private ImmutableMap<String, TrieNode<T>> children;
+        private Map<String, TrieNode> children;
 
-        private final TrieNode parent;
-
-        public TrieNode(String key, T value, TrieNode parent, String wildcard) {
+        public TrieNode(String key, T value, String wildcard) {
             this.key = key;
             this.wildcard = wildcard;
             this.isWildcard = (key.equals(wildcard));
-            this.parent = parent;
             this.value = value;
-            this.children = ImmutableMap.of();
+            this.children = emptyMap();
             if (isNamedWildcard(key)) {
                 namedWildcard = key.substring(key.indexOf('{') + 1, key.indexOf('}'));
             } else {
@@ -98,8 +96,14 @@ public class PathTrie<T> {
             return isWildcard;
         }
 
-        public synchronized void addChild(TrieNode<T> child) {
-            children = newMapBuilder(children).put(child.key, child).immutableMap();
+        public synchronized void addChild(TrieNode child) {
+            addInnerChild(child.key, child);
+        }
+
+        private void addInnerChild(String key, TrieNode child) {
+            Map<String, TrieNode> newChildren = new HashMap<>(children);
+            newChildren.put(key, child);
+            children = unmodifiableMap(newChildren);
         }
 
         public TrieNode getChild(String key) {
@@ -115,14 +119,11 @@ public class PathTrie<T> {
             if (isNamedWildcard(token)) {
                 key = wildcard;
             }
-            TrieNode<T> node = children.get(key);
+            TrieNode node = children.get(key);
             if (node == null) {
-                if (index == (path.length - 1)) {
-                    node = new TrieNode<>(token, value, this, wildcard);
-                } else {
-                    node = new TrieNode<>(token, null, this, wildcard);
-                }
-                children = newMapBuilder(children).put(key, node).immutableMap();
+                T nodeValue = index == path.length - 1 ? value : null;
+                node = new TrieNode(token, nodeValue, wildcard);
+                addInnerChild(key, node);
             } else {
                 if (isNamedWildcard(token)) {
                     node.updateKeyWithNamedWildcard(token);
@@ -158,7 +159,7 @@ public class PathTrie<T> {
                 return null;
 
             String token = path[index];
-            TrieNode<T> node = children.get(token);
+            TrieNode node = children.get(token);
             boolean usedWildcard;
             if (node == null) {
                 node = children.get(wildcard);
@@ -195,11 +196,16 @@ public class PathTrie<T> {
             return res;
         }
 
-        private void put(Map<String, String> params, TrieNode<T> node, String value) {
+        private void put(Map<String, String> params, TrieNode node, String value) {
             if (params != null && node.isNamedWildcard()) {
                 params.put(node.namedWildcard(), value);
             }
         }
+
+        @Override
+        public String toString() {
+            return key;
+        }
     }
 
     public void insert(String path, T value) {
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Settings.java b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
index e368430..663abd7 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Settings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.common.settings;
 
-import java.nio.charset.StandardCharsets;
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
 import org.elasticsearch.common.Strings;
@@ -30,20 +28,40 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.property.PropertyPlaceholder;
 import org.elasticsearch.common.settings.loader.SettingsLoader;
 import org.elasticsearch.common.settings.loader.SettingsLoaderFactory;
-import org.elasticsearch.common.unit.*;
+import org.elasticsearch.common.unit.ByteSizeUnit;
+import org.elasticsearch.common.unit.ByteSizeValue;
+import org.elasticsearch.common.unit.MemorySizeValue;
+import org.elasticsearch.common.unit.RatioValue;
+import org.elasticsearch.common.unit.SizeValue;
+import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
 import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.Strings.toCamelCase;
 import static org.elasticsearch.common.unit.ByteSizeValue.parseBytesSizeValue;
 import static org.elasticsearch.common.unit.SizeValue.parseSizeValue;
@@ -70,8 +88,8 @@ public final class Settings implements ToXContent {
         return settingsRequireUnits;
     }
 
+    private final Map<String, String> forcedUnderscoreSettings;
     private SortedMap<String, String> settings;
-    private final ImmutableMap<String, String> forcedUnderscoreSettings;
 
     Settings(Map<String, String> settings) {
         // we use a sorted map for consistent serialization when using getAsMap()
@@ -86,7 +104,7 @@ public final class Settings implements ToXContent {
                 forcedUnderscoreSettings.put(toUnderscoreCase, entry.getValue());
             }
         }
-        this.forcedUnderscoreSettings = forcedUnderscoreSettings == null ? ImmutableMap.<String, String>of() : ImmutableMap.copyOf(forcedUnderscoreSettings);
+        this.forcedUnderscoreSettings = forcedUnderscoreSettings == null ? emptyMap() : unmodifiableMap(forcedUnderscoreSettings);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java b/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
index d8e121b..026f849 100644
--- a/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
+++ b/core/src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java
@@ -19,16 +19,16 @@
 
 package org.elasticsearch.common.transport;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 
 import java.io.IOException;
-import java.lang.reflect.Constructor;
+import java.util.HashMap;
+import java.util.Map;
 
-import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
+import static java.util.Collections.unmodifiableMap;
 
 /**
  * A global registry of all different types of {@link org.elasticsearch.common.transport.TransportAddress} allowing
@@ -42,23 +42,25 @@ public abstract class TransportAddressSerializers {
 
     private static final ESLogger logger = Loggers.getLogger(TransportAddressSerializers.class);
 
-    private static ImmutableMap<Short, TransportAddress> ADDRESS_REGISTRY = ImmutableMap.of();
+    private static final Map<Short, TransportAddress> ADDRESS_REGISTRY;
 
     static {
+        Map<Short, TransportAddress> registry = new HashMap<>();
         try {
-            addAddressType(DummyTransportAddress.INSTANCE);
-            addAddressType(InetSocketTransportAddress.PROTO);
-            addAddressType(LocalTransportAddress.PROTO);
+            addAddressType(registry, DummyTransportAddress.INSTANCE);
+            addAddressType(registry, InetSocketTransportAddress.PROTO);
+            addAddressType(registry, LocalTransportAddress.PROTO);
         } catch (Exception e) {
-            logger.warn("Failed to add InetSocketTransportAddress", e);
+            logger.warn("Failed to setup TransportAddresses", e);
         }
+        ADDRESS_REGISTRY = unmodifiableMap(registry);
     }
 
-    public static synchronized void addAddressType(TransportAddress address) throws Exception {
-        if (ADDRESS_REGISTRY.containsKey(address.uniqueAddressTypeId())) {
+    public static synchronized void addAddressType(Map<Short, TransportAddress> registry, TransportAddress address) throws Exception {
+        if (registry.containsKey(address.uniqueAddressTypeId())) {
             throw new IllegalStateException("Address [" + address.uniqueAddressTypeId() + "] already bound");
         }
-        ADDRESS_REGISTRY = newMapBuilder(ADDRESS_REGISTRY).put(address.uniqueAddressTypeId(), address).immutableMap();
+        registry.put(address.uniqueAddressTypeId(), address);
     }
 
     public static TransportAddress addressFromStream(StreamInput input) throws IOException {
diff --git a/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java b/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
index fd0f64d..a36c37b 100644
--- a/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
+++ b/core/src/main/java/org/elasticsearch/common/util/CollectionUtils.java
@@ -447,5 +447,4 @@ public class CollectionUtils {
 
         return result;
     }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java b/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
index cd5789f..6d2216d 100644
--- a/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
+++ b/core/src/main/java/org/elasticsearch/common/util/concurrent/BaseFuture.java
@@ -26,43 +26,6 @@ import java.util.Objects;
 import java.util.concurrent.*;
 import java.util.concurrent.locks.AbstractQueuedSynchronizer;
 
-/**
- * An abstract implementation of the {@link com.google.common.util.concurrent.ListenableFuture} interface. This
- * class is preferable to {@link java.util.concurrent.FutureTask} for two
- * reasons: It implements {@code ListenableFuture}, and it does not implement
- * {@code Runnable}. (If you want a {@code Runnable} implementation of {@code
- * ListenableFuture}, create a {@link com.google.common.util.concurrent.ListenableFutureTask}, or submit your
- * tasks to a {@link com.google.common.util.concurrent.ListeningExecutorService}.)
- * <p>
- * This class implements all methods in {@code ListenableFuture}.
- * Subclasses should provide a way to set the result of the computation through
- * the protected methods {@link #set(Object)} and
- * {@link #setException(Throwable)}. Subclasses may also override {@link
- * #interruptTask()}, which will be invoked automatically if a call to {@link
- * #cancel(boolean) cancel(true)} succeeds in canceling the future.
- * <p>
- * {@code AbstractFuture} uses an {@link AbstractQueuedSynchronizer} to deal
- * with concurrency issues and guarantee thread safety.
- * <p>
- * The state changing methods all return a boolean indicating success or
- * failure in changing the future's state.  Valid states are running,
- * completed, failed, or cancelled.
- * <p>
- * This class uses an {@link com.google.common.util.concurrent.ExecutionList} to guarantee that all registered
- * listeners will be executed, either when the future finishes or, for listeners
- * that are added after the future completes, immediately.
- * {@code Runnable}-{@code Executor} pairs are stored in the execution list but
- * are not necessarily executed in the order in which they were added.  (If a
- * listener is added after the Future is complete, it will be executed
- * immediately, even if earlier listeners have not been executed. Additionally,
- * executors need not guarantee FIFO execution, or different listeners may run
- * in different executors.)
- *
- * @author Sven Mawson
- * @since 1.0
- */
-// Same as AbstractFuture from Guava, but without the listeners and with
-// additional assertions
 public abstract class BaseFuture<V> implements Future<V> {
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
index a84ee83..a17ef93 100644
--- a/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
+++ b/core/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java
@@ -21,7 +21,6 @@ package org.elasticsearch.common.xcontent;
 
 import org.elasticsearch.common.bytes.BytesReference;
 
-import java.io.Closeable;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
@@ -29,7 +28,7 @@ import java.io.OutputStream;
 /**
  *
  */
-public interface XContentGenerator extends Closeable {
+public interface XContentGenerator {
 
     XContentType contentType();
 
diff --git a/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java b/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
index e4285d2..0ebba04 100644
--- a/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
+++ b/core/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.gateway;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -34,6 +33,9 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * The dangling indices state is responsible for finding new dangling indices (indices that have
  * their state written on disk, but don't exists in the metadata of the cluster), and importing
@@ -73,7 +75,8 @@ public class DanglingIndicesState extends AbstractComponent {
      * The current set of dangling indices.
      */
     Map<String, IndexMetaData> getDanglingIndices() {
-        return ImmutableMap.copyOf(danglingIndices);
+        // This might be a good use case for CopyOnWriteHashMap
+        return unmodifiableMap(new HashMap<>(danglingIndices));
     }
 
     /**
@@ -107,7 +110,7 @@ public class DanglingIndicesState extends AbstractComponent {
             indices = nodeEnv.findAllIndices();
         } catch (Throwable e) {
             logger.warn("failed to list dangling indices", e);
-            return ImmutableMap.of();
+            return emptyMap();
         }
 
         Map<String, IndexMetaData>  newIndices = new HashMap<>();
diff --git a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
index 742f789..855b6ce 100644
--- a/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
+++ b/core/src/main/java/org/elasticsearch/gateway/GatewayService.java
@@ -20,7 +20,12 @@
 package org.elasticsearch.gateway;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-import org.elasticsearch.cluster.*;
+
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateListener;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.block.ClusterBlocks;
@@ -253,7 +258,7 @@ public class GatewayService extends AbstractLifecycleComponent<GatewayService> i
                     routingTableBuilder.version(0);
 
                     // now, reroute
-                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder).build());
+                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder.build()).build());
 
                     return ClusterState.builder(updatedState).routingResult(routingResult).build();
                 }
diff --git a/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java b/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
index 1ab7a56..b8491b9 100644
--- a/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
+++ b/core/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java
@@ -37,7 +37,13 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.discovery.MasterNotDiscoveredException;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
+import org.elasticsearch.transport.TransportChannel;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportRequest;
+import org.elasticsearch.transport.TransportRequestHandler;
+import org.elasticsearch.transport.TransportResponse;
+import org.elasticsearch.transport.TransportResponseHandler;
+import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -158,10 +164,11 @@ public class LocalAllocateDangledIndices extends AbstractComponent {
                     }
                     logger.info("auto importing dangled indices {} from [{}]", sb.toString(), request.fromNode);
 
-                    ClusterState updatedState = ClusterState.builder(currentState).metaData(metaData).blocks(blocks).routingTable(routingTableBuilder).build();
+                    RoutingTable routingTable = routingTableBuilder.build();
+                    ClusterState updatedState = ClusterState.builder(currentState).metaData(metaData).blocks(blocks).routingTable(routingTable).build();
 
                     // now, reroute
-                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTableBuilder).build());
+                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(routingTable).build());
 
                     return ClusterState.builder(updatedState).routingResult(routingResult).build();
                 }
diff --git a/core/src/main/java/org/elasticsearch/http/HttpServer.java b/core/src/main/java/org/elasticsearch/http/HttpServer.java
index 168ed06..f3b8c3f 100644
--- a/core/src/main/java/org/elasticsearch/http/HttpServer.java
+++ b/core/src/main/java/org/elasticsearch/http/HttpServer.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.http;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.FileSystemUtils;
@@ -28,18 +26,29 @@ import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.node.service.NodeService;
-import org.elasticsearch.rest.*;
+import org.elasticsearch.rest.BytesRestResponse;
+import org.elasticsearch.rest.RestChannel;
+import org.elasticsearch.rest.RestController;
+import org.elasticsearch.rest.RestFilter;
+import org.elasticsearch.rest.RestFilterChain;
+import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.RestStatus;
 
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.nio.file.*;
+import java.nio.file.Files;
+import java.nio.file.Path;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
 
-import static org.elasticsearch.rest.RestStatus.*;
+import static java.util.Collections.unmodifiableMap;
+import static org.elasticsearch.rest.RestStatus.FORBIDDEN;
+import static org.elasticsearch.rest.RestStatus.INTERNAL_SERVER_ERROR;
+import static org.elasticsearch.rest.RestStatus.NOT_FOUND;
+import static org.elasticsearch.rest.RestStatus.OK;
 
 /**
  *
@@ -204,7 +213,7 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
         final String separator = siteFile.getFileSystem().getSeparator();
         // Convert file separators.
         sitePath = sitePath.replace("/", separator);
-        
+
         Path file = siteFile.resolve(sitePath);
 
         // return not found instead of forbidden to prevent malicious requests to find out if files exist or dont exist
@@ -275,7 +284,7 @@ public class HttpServer extends AbstractLifecycleComponent<HttpServer> {
         mimeTypes.put("svg", "image/svg+xml");
         mimeTypes.put("ico", "image/vnd.microsoft.icon");
         mimeTypes.put("mp3", "audio/mpeg");
-        DEFAULT_MIME_TYPES = ImmutableMap.copyOf(mimeTypes);
+        DEFAULT_MIME_TYPES = unmodifiableMap(mimeTypes);
     }
 
     public static final Map<String, String> DEFAULT_MIME_TYPES;
diff --git a/core/src/main/java/org/elasticsearch/index/IndexModule.java b/core/src/main/java/org/elasticsearch/index/IndexModule.java
index 1929848..dc637cf 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexModule.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexModule.java
@@ -19,9 +19,9 @@
 
 package org.elasticsearch.index;
 
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.index.aliases.IndexAliasesService;
 import org.elasticsearch.index.engine.EngineFactory;
 import org.elasticsearch.index.engine.InternalEngineFactory;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
@@ -33,10 +33,15 @@ import org.elasticsearch.index.shard.IndexSearcherWrapper;
  */
 public class IndexModule extends AbstractModule {
 
+    private final IndexMetaData indexMetaData;
     // pkg private so tests can mock
     Class<? extends EngineFactory> engineFactoryImpl = InternalEngineFactory.class;
     Class<? extends IndexSearcherWrapper> indexSearcherWrapper = null;
-    
+
+    public IndexModule(IndexMetaData indexMetaData) {
+        this.indexMetaData = indexMetaData;
+    }
+
     @Override
     protected void configure() {
         bind(EngineFactory.class).to(engineFactoryImpl).asEagerSingleton();
@@ -45,10 +50,10 @@ public class IndexModule extends AbstractModule {
         } else {
             bind(IndexSearcherWrapper.class).to(indexSearcherWrapper).asEagerSingleton();
         }
+        bind(IndexMetaData.class).toInstance(indexMetaData);
         bind(IndexService.class).asEagerSingleton();
         bind(IndexServicesProvider.class).asEagerSingleton();
         bind(MapperService.class).asEagerSingleton();
-        bind(IndexAliasesService.class).asEagerSingleton();
         bind(IndexFieldDataService.class).asEagerSingleton();
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/IndexService.java b/core/src/main/java/org/elasticsearch/index/IndexService.java
index 2fc7a24..9c932e2 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexService.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexService.java
@@ -19,18 +19,23 @@
 
 package org.elasticsearch.index;
 
-import com.google.common.collect.ImmutableMap;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ElasticsearchException;
+import org.elasticsearch.cluster.metadata.AliasMetaData;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.env.NodeEnvironment;
 import org.elasticsearch.env.ShardLock;
-import org.elasticsearch.index.aliases.IndexAliasesService;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.cache.bitset.BitsetFilterCache;
@@ -40,14 +45,21 @@ import org.elasticsearch.index.fielddata.IndexFieldDataService;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.query.IndexQueryParserService;
+import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.settings.IndexSettingsService;
-import org.elasticsearch.index.shard.*;
+import org.elasticsearch.index.shard.IndexShard;
+import org.elasticsearch.index.shard.ShadowIndexShard;
+import org.elasticsearch.index.shard.ShardId;
+import org.elasticsearch.index.shard.ShardNotFoundException;
+import org.elasticsearch.index.shard.ShardPath;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.index.store.IndexStore;
 import org.elasticsearch.index.store.Store;
+import org.elasticsearch.indices.AliasFilterParsingException;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.indices.InternalIndicesLifecycle;
+import org.elasticsearch.indices.InvalidAliasNameException;
 
 import java.io.Closeable;
 import java.io.IOException;
@@ -59,6 +71,8 @@ import java.util.Set;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
 
 /**
@@ -66,7 +80,6 @@ import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
  */
 public class IndexService extends AbstractIndexComponent implements IndexComponent, Iterable<IndexShard> {
 
-    private final Settings indexSettings;
     private final InternalIndicesLifecycle indicesLifecycle;
     private final AnalysisService analysisService;
     private final IndexFieldDataService indexFieldData;
@@ -76,12 +89,13 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
     private final IndicesService indicesServices;
     private final IndexServicesProvider indexServicesProvider;
     private final IndexStore indexStore;
-    private volatile ImmutableMap<Integer, IndexShard> shards = ImmutableMap.of();
+    private volatile Map<Integer, IndexShard> shards = emptyMap();
     private final AtomicBoolean closed = new AtomicBoolean(false);
     private final AtomicBoolean deleted = new AtomicBoolean(false);
+    private volatile IndexMetaData indexMetaData;
 
     @Inject
-    public IndexService(Index index, @IndexSettings Settings indexSettings, NodeEnvironment nodeEnv,
+    public IndexService(Index index, IndexMetaData indexMetaData, NodeEnvironment nodeEnv,
                         AnalysisService analysisService,
                         IndexSettingsService settingsService,
                         IndexFieldDataService indexFieldData,
@@ -89,8 +103,8 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
                         IndicesService indicesServices,
                         IndexServicesProvider indexServicesProvider,
                         IndexStore indexStore) {
-        super(index, indexSettings);
-        this.indexSettings = indexSettings;
+        super(index, settingsService.indexSettings());
+        assert indexMetaData != null;
         this.analysisService = analysisService;
         this.indexFieldData = indexFieldData;
         this.settingsService = settingsService;
@@ -100,6 +114,7 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
         this.nodeEnv = nodeEnv;
         this.indexServicesProvider = indexServicesProvider;
         this.indexStore = indexStore;
+        this.indexMetaData = indexMetaData;
         indexFieldData.setListener(new FieldDataCacheListener(this));
         bitSetFilterCache.setListener(new BitsetCacheListener(this));
     }
@@ -176,10 +191,6 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
         return indexServicesProvider.getSimilarityService();
     }
 
-    public IndexAliasesService aliasesService() {
-        return indexServicesProvider.getIndexAliasesService();
-    }
-
     public synchronized void close(final String reason, boolean delete) {
         if (closed.compareAndSet(false, true)) {
             deleted.compareAndSet(false, delete);
@@ -224,9 +235,7 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
         if (closed.get()) {
             throw new IllegalStateException("Can't create shard [" + index.name() + "][" + sShardId + "], closed");
         }
-        if (indexSettings.get("index.translog.type") != null) { // TODO remove?
-            throw new IllegalStateException("a custom translog type is no longer supported. got [" + indexSettings.get("index.translog.type") + "]");
-        }
+        final Settings indexSettings = settingsService.getSettings();
         final ShardId shardId = new ShardId(index, sShardId);
         ShardLock lock = null;
         boolean success = false;
@@ -313,15 +322,16 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
             return;
         }
         logger.debug("[{}] closing... (reason: [{}])", shardId, reason);
-        HashMap<Integer, IndexShard> tmpShardsMap = new HashMap<>(shards);
-        indexShard = tmpShardsMap.remove(shardId);
-        shards = ImmutableMap.copyOf(tmpShardsMap);
+        HashMap<Integer, IndexShard> newShards = new HashMap<>(shards);
+        indexShard = newShards.remove(shardId);
+        shards = unmodifiableMap(newShards);
         closeShard(reason, sId, indexShard, indexShard.store());
         logger.debug("[{}] closed (reason: [{}])", shardId, reason);
     }
 
     private void closeShard(String reason, ShardId sId, IndexShard indexShard, Store store) {
         final int shardId = sId.id();
+        final Settings indexSettings = settingsService.getSettings();
         try {
             try {
                 indicesLifecycle.beforeIndexShardClosed(sId, indexShard, indexSettings);
@@ -353,6 +363,7 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
 
     private void onShardClose(ShardLock lock, boolean ownsShard) {
         if (deleted.get()) { // we remove that shards content if this index has been deleted
+            final Settings indexSettings = settingsService.getSettings();
             try {
                 if (ownsShard) {
                     try {
@@ -401,7 +412,7 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
     }
 
     public Settings getIndexSettings() {
-        return indexSettings;
+        return settingsService.getSettings();
     }
 
     private static final class BitsetCacheListener implements BitsetFilterCache.Listener {
@@ -461,4 +472,67 @@ public class IndexService extends AbstractIndexComponent implements IndexCompone
             }
         }
     }
+    /**
+     * Returns the filter associated with listed filtering aliases.
+     * <p>
+     * The list of filtering aliases should be obtained by calling MetaData.filteringAliases.
+     * Returns <tt>null</tt> if no filtering is required.</p>
+     */
+    public Query aliasFilter(String... aliasNames) {
+        if (aliasNames == null || aliasNames.length == 0) {
+            return null;
+        }
+        final IndexQueryParserService indexQueryParser = queryParserService();
+        final ImmutableOpenMap<String, AliasMetaData> aliases = this.indexMetaData.getAliases();
+        if (aliasNames.length == 1) {
+            AliasMetaData alias = aliases.get(aliasNames[0]);
+            if (alias == null) {
+                // This shouldn't happen unless alias disappeared after filteringAliases was called.
+                throw new InvalidAliasNameException(index, aliasNames[0], "Unknown alias name was passed to alias Filter");
+            }
+            return parse(alias, indexQueryParser);
+        } else {
+            // we need to bench here a bit, to see maybe it makes sense to use OrFilter
+            BooleanQuery.Builder combined = new BooleanQuery.Builder();
+            for (String aliasName : aliasNames) {
+                AliasMetaData alias = aliases.get(aliasName);
+                if (alias == null) {
+                    // This shouldn't happen unless alias disappeared after filteringAliases was called.
+                    throw new InvalidAliasNameException(indexQueryParser.index(), aliasNames[0], "Unknown alias name was passed to alias Filter");
+                }
+                Query parsedFilter = parse(alias, indexQueryParser);
+                if (parsedFilter != null) {
+                    combined.add(parsedFilter, BooleanClause.Occur.SHOULD);
+                } else {
+                    // The filter might be null only if filter was removed after filteringAliases was called
+                    return null;
+                }
+            }
+            return combined.build();
+        }
+    }
+
+    private Query parse(AliasMetaData alias, IndexQueryParserService indexQueryParser) {
+        if (alias.filter() == null) {
+            return null;
+        }
+        try {
+            byte[] filterSource = alias.filter().uncompressed();
+            try (XContentParser parser = XContentFactory.xContent(filterSource).createParser(filterSource)) {
+                ParsedQuery parsedFilter = indexQueryParser.parseInnerFilter(parser);
+                return parsedFilter == null ? null : parsedFilter.query();
+            }
+        } catch (IOException ex) {
+            throw new AliasFilterParsingException(indexQueryParser.index(), alias.getAlias(), "Invalid alias filter", ex);
+        }
+    }
+
+    public IndexMetaData getMetaData() {
+        return indexMetaData;
+    }
+
+    public void updateMetaData(IndexMetaData metadata) {
+        this.indexMetaData = metadata;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java b/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
index 0d5c3cb..53b9f06 100644
--- a/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/IndexServicesProvider.java
@@ -22,7 +22,6 @@ package org.elasticsearch.index;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.util.BigArrays;
-import org.elasticsearch.index.aliases.IndexAliasesService;
 import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.codec.CodecService;
 import org.elasticsearch.index.engine.EngineFactory;
@@ -50,7 +49,6 @@ public final class IndexServicesProvider {
     private final MapperService mapperService;
     private final IndexQueryParserService queryParserService;
     private final IndexCache indexCache;
-    private final IndexAliasesService indexAliasesService;
     private final IndicesQueryCache indicesQueryCache;
     private final CodecService codecService;
     private final TermVectorsService termVectorsService;
@@ -63,13 +61,12 @@ public final class IndexServicesProvider {
     private final IndexingMemoryController indexingMemoryController;
 
     @Inject
-    public IndexServicesProvider(IndicesLifecycle indicesLifecycle, ThreadPool threadPool, MapperService mapperService, IndexQueryParserService queryParserService, IndexCache indexCache, IndexAliasesService indexAliasesService, IndicesQueryCache indicesQueryCache, CodecService codecService, TermVectorsService termVectorsService, IndexFieldDataService indexFieldDataService, @Nullable IndicesWarmer warmer, SimilarityService similarityService, EngineFactory factory, BigArrays bigArrays, @Nullable IndexSearcherWrapper indexSearcherWrapper, IndexingMemoryController indexingMemoryController) {
+    public IndexServicesProvider(IndicesLifecycle indicesLifecycle, ThreadPool threadPool, MapperService mapperService, IndexQueryParserService queryParserService, IndexCache indexCache, IndicesQueryCache indicesQueryCache, CodecService codecService, TermVectorsService termVectorsService, IndexFieldDataService indexFieldDataService, @Nullable IndicesWarmer warmer, SimilarityService similarityService, EngineFactory factory, BigArrays bigArrays, @Nullable IndexSearcherWrapper indexSearcherWrapper, IndexingMemoryController indexingMemoryController) {
         this.indicesLifecycle = indicesLifecycle;
         this.threadPool = threadPool;
         this.mapperService = mapperService;
         this.queryParserService = queryParserService;
         this.indexCache = indexCache;
-        this.indexAliasesService = indexAliasesService;
         this.indicesQueryCache = indicesQueryCache;
         this.codecService = codecService;
         this.termVectorsService = termVectorsService;
@@ -102,10 +99,6 @@ public final class IndexServicesProvider {
         return indexCache;
     }
 
-    public IndexAliasesService getIndexAliasesService() {
-        return indexAliasesService;
-    }
-
     public IndicesQueryCache getIndicesQueryCache() {
         return indicesQueryCache;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java b/core/src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java
deleted file mode 100644
index 2b519c2..0000000
--- a/core/src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java
+++ /dev/null
@@ -1,127 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.aliases;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.AliasMetaData;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.collect.ImmutableOpenMap;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.AbstractIndexComponent;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.settings.IndexSettings;
-import org.elasticsearch.indices.AliasFilterParsingException;
-import org.elasticsearch.indices.InvalidAliasNameException;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class IndexAliasesService extends AbstractIndexComponent {
-
-    private final IndexQueryParserService indexQueryParser;
-    private volatile ImmutableOpenMap<String, AliasMetaData> aliases = ImmutableOpenMap.of();
-
-    @Inject
-    public IndexAliasesService(Index index, @IndexSettings Settings indexSettings, IndexQueryParserService indexQueryParser) {
-        super(index, indexSettings);
-        this.indexQueryParser = indexQueryParser;
-    }
-
-    /**
-     * Returns the filter associated with listed filtering aliases.
-     * <p>
-     * The list of filtering aliases should be obtained by calling MetaData.filteringAliases.
-     * Returns <tt>null</tt> if no filtering is required.</p>
-     */
-    public Query aliasFilter(String... aliasNames) {
-        if (aliasNames == null || aliasNames.length == 0) {
-            return null;
-        }
-        if (aliasNames.length == 1) {
-            AliasMetaData alias = this.aliases.get(aliasNames[0]);
-            if (alias == null) {
-                // This shouldn't happen unless alias disappeared after filteringAliases was called.
-                throw new InvalidAliasNameException(index, aliasNames[0], "Unknown alias name was passed to alias Filter");
-            }
-            return parse(alias);
-        } else {
-            // we need to bench here a bit, to see maybe it makes sense to use OrFilter
-            BooleanQuery.Builder combined = new BooleanQuery.Builder();
-            for (String aliasName : aliasNames) {
-                AliasMetaData alias = this.aliases.get(aliasName);
-                if (alias == null) {
-                    // This shouldn't happen unless alias disappeared after filteringAliases was called.
-                    throw new InvalidAliasNameException(index, aliasNames[0], "Unknown alias name was passed to alias Filter");
-                }
-                Query parsedFilter = parse(alias);
-                if (parsedFilter != null) {
-                    combined.add(parsedFilter, BooleanClause.Occur.SHOULD);
-                } else {
-                    // The filter might be null only if filter was removed after filteringAliases was called
-                    return null;
-                }
-            }
-            return combined.build();
-        }
-    }
-
-    public void setAliases(ImmutableOpenMap<String, AliasMetaData> aliases) {
-        this.aliases = aliases;
-    }
-
-    Query parse(AliasMetaData alias) {
-        if (alias.filter() == null) {
-            return null;
-        }
-        try {
-            byte[] filterSource = alias.filter().uncompressed();
-            try (XContentParser parser = XContentFactory.xContent(filterSource).createParser(filterSource)) {
-                ParsedQuery parsedFilter = indexQueryParser.parseInnerFilter(parser);
-                return parsedFilter == null ? null : parsedFilter.query();
-            }
-        } catch (IOException ex) {
-            throw new AliasFilterParsingException(index, alias.getAlias(), "Invalid alias filter", ex);
-        }
-    }
-
-    // Used by tests:
-    void add(String alias, @Nullable CompressedXContent filter) {
-        AliasMetaData aliasMetaData = AliasMetaData.builder(alias).filter(filter).build();
-        aliases = ImmutableOpenMap.builder(aliases).fPut(alias, aliasMetaData).build();
-    }
-
-    boolean hasAlias(String alias) {
-        return aliases.containsKey(alias);
-    }
-
-   void remove(String alias) {
-       aliases = ImmutableOpenMap.builder(aliases).fRemove(alias).build();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java b/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
index 1040a27..861f070 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.index.analysis;
 
-import java.nio.charset.StandardCharsets;
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.NumericTokenStream;
 import org.apache.lucene.analysis.TokenStream;
@@ -60,7 +58,6 @@ import org.apache.lucene.analysis.tr.TurkishAnalyzer;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.Lucene;
@@ -71,15 +68,19 @@ import org.elasticsearch.index.settings.IndexSettings;
 import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.Reader;
+import java.nio.charset.StandardCharsets;
 import java.nio.file.Path;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
@@ -124,40 +125,44 @@ public class Analysis {
         }
     }
 
-    public static final ImmutableMap<String, Set<?>> namedStopWords = MapBuilder.<String, Set<?>>newMapBuilder()
-            .put("_arabic_", ArabicAnalyzer.getDefaultStopSet())
-            .put("_armenian_", ArmenianAnalyzer.getDefaultStopSet())
-            .put("_basque_", BasqueAnalyzer.getDefaultStopSet())
-            .put("_brazilian_", BrazilianAnalyzer.getDefaultStopSet())
-            .put("_bulgarian_", BulgarianAnalyzer.getDefaultStopSet())
-            .put("_catalan_", CatalanAnalyzer.getDefaultStopSet())
-            .put("_czech_", CzechAnalyzer.getDefaultStopSet())
-            .put("_danish_", DanishAnalyzer.getDefaultStopSet())
-            .put("_dutch_", DutchAnalyzer.getDefaultStopSet())
-            .put("_english_", EnglishAnalyzer.getDefaultStopSet())
-            .put("_finnish_", FinnishAnalyzer.getDefaultStopSet())
-            .put("_french_", FrenchAnalyzer.getDefaultStopSet())
-            .put("_galician_", GalicianAnalyzer.getDefaultStopSet())
-            .put("_german_", GermanAnalyzer.getDefaultStopSet())
-            .put("_greek_", GreekAnalyzer.getDefaultStopSet())
-            .put("_hindi_", HindiAnalyzer.getDefaultStopSet())
-            .put("_hungarian_", HungarianAnalyzer.getDefaultStopSet())
-            .put("_indonesian_", IndonesianAnalyzer.getDefaultStopSet())
-            .put("_irish_", IrishAnalyzer.getDefaultStopSet())
-            .put("_italian_", ItalianAnalyzer.getDefaultStopSet())
-            .put("_latvian_", LatvianAnalyzer.getDefaultStopSet())
-            .put("_lithuanian_", LithuanianAnalyzer.getDefaultStopSet())
-            .put("_norwegian_", NorwegianAnalyzer.getDefaultStopSet())
-            .put("_persian_", PersianAnalyzer.getDefaultStopSet())
-            .put("_portuguese_", PortugueseAnalyzer.getDefaultStopSet())
-            .put("_romanian_", RomanianAnalyzer.getDefaultStopSet())
-            .put("_russian_", RussianAnalyzer.getDefaultStopSet())
-            .put("_sorani_", SoraniAnalyzer.getDefaultStopSet())
-            .put("_spanish_", SpanishAnalyzer.getDefaultStopSet())
-            .put("_swedish_", SwedishAnalyzer.getDefaultStopSet())
-            .put("_thai_", ThaiAnalyzer.getDefaultStopSet())
-            .put("_turkish_", TurkishAnalyzer.getDefaultStopSet())
-            .immutableMap();
+    public static final Map<String, Set<?>> NAMED_STOP_WORDS;
+    static {
+        Map<String, Set<?>> namedStopWords = new HashMap<>();
+        namedStopWords.put("_arabic_", ArabicAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_armenian_", ArmenianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_basque_", BasqueAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_brazilian_", BrazilianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_bulgarian_", BulgarianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_catalan_", CatalanAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_czech_", CzechAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_danish_", DanishAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_dutch_", DutchAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_english_", EnglishAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_finnish_", FinnishAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_french_", FrenchAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_galician_", GalicianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_german_", GermanAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_greek_", GreekAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_hindi_", HindiAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_hungarian_", HungarianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_indonesian_", IndonesianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_irish_", IrishAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_italian_", ItalianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_latvian_", LatvianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_lithuanian_", LithuanianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_norwegian_", NorwegianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_persian_", PersianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_portuguese_", PortugueseAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_romanian_", RomanianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_russian_", RussianAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_sorani_", SoraniAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_spanish_", SpanishAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_swedish_", SwedishAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_thai_", ThaiAnalyzer.getDefaultStopSet());
+        namedStopWords.put("_turkish_", TurkishAnalyzer.getDefaultStopSet());
+
+        NAMED_STOP_WORDS = unmodifiableMap(namedStopWords);
+    }
 
     public static CharArraySet parseWords(Environment env, Settings settings, String name, CharArraySet defaultWords, Map<String, Set<?>> namedWords, boolean ignoreCase) {
         String value = settings.get(name);
@@ -176,7 +181,7 @@ public class Analysis {
     }
 
     public static CharArraySet parseCommonWords(Environment env, Settings settings, CharArraySet defaultCommonWords, boolean ignoreCase) {
-        return parseWords(env, settings, "common_words", defaultCommonWords, namedStopWords, ignoreCase);
+        return parseWords(env, settings, "common_words", defaultCommonWords, NAMED_STOP_WORDS, ignoreCase);
     }
 
     public static CharArraySet parseArticles(Environment env, Settings settings) {
@@ -188,7 +193,7 @@ public class Analysis {
     }
 
     public static CharArraySet parseStopWords(Environment env, Settings settings, CharArraySet defaultStopWords, boolean ignoreCase) {
-        return parseWords(env, settings, "stopwords", defaultStopWords, namedStopWords, ignoreCase);
+        return parseWords(env, settings, "stopwords", defaultStopWords, NAMED_STOP_WORDS, ignoreCase);
     }
 
     private static CharArraySet resolveNamedWords(Collection<String> words, Map<String, Set<?>> namedWords, boolean ignoreCase) {
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java b/core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java
index 829f9db..c76446c 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.analysis;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -37,15 +36,17 @@ import java.io.Closeable;
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
 public class AnalysisService extends AbstractIndexComponent implements Closeable {
 
-    private final ImmutableMap<String, NamedAnalyzer> analyzers;
-    private final ImmutableMap<String, TokenizerFactory> tokenizers;
-    private final ImmutableMap<String, CharFilterFactory> charFilters;
-    private final ImmutableMap<String, TokenFilterFactory> tokenFilters;
+    private final Map<String, NamedAnalyzer> analyzers;
+    private final Map<String, TokenizerFactory> tokenizers;
+    private final Map<String, CharFilterFactory> charFilters;
+    private final Map<String, TokenFilterFactory> tokenFilters;
 
     private final NamedAnalyzer defaultAnalyzer;
     private final NamedAnalyzer defaultIndexAnalyzer;
@@ -98,7 +99,7 @@ public class AnalysisService extends AbstractIndexComponent implements Closeable
             }
         }
 
-        this.tokenizers = ImmutableMap.copyOf(tokenizers);
+        this.tokenizers = unmodifiableMap(tokenizers);
 
         Map<String, CharFilterFactory> charFilters = new HashMap<>();
         if (charFilterFactoryFactories != null) {
@@ -133,7 +134,7 @@ public class AnalysisService extends AbstractIndexComponent implements Closeable
             }
         }
 
-        this.charFilters = ImmutableMap.copyOf(charFilters);
+        this.charFilters = unmodifiableMap(charFilters);
 
         Map<String, TokenFilterFactory> tokenFilters = new HashMap<>();
         if (tokenFilterFactoryFactories != null) {
@@ -168,7 +169,7 @@ public class AnalysisService extends AbstractIndexComponent implements Closeable
                 }
             }
         }
-        this.tokenFilters = ImmutableMap.copyOf(tokenFilters);
+        this.tokenFilters = unmodifiableMap(tokenFilters);
 
         Map<String, AnalyzerProvider> analyzerProviders = new HashMap<>();
         if (analyzerFactoryFactories != null) {
@@ -275,7 +276,7 @@ public class AnalysisService extends AbstractIndexComponent implements Closeable
                 throw new IllegalArgumentException("analyzer name must not start with '_'. got \"" + analyzer.getKey() + "\"");
             }
         }
-        this.analyzers = ImmutableMap.copyOf(analyzers);
+        this.analyzers = unmodifiableMap(analyzers);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java b/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
index b884095..f1ad1d5 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java
@@ -19,12 +19,10 @@
 
 package org.elasticsearch.index.analysis;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.ngram.Lucene43NGramTokenizer;
 import org.apache.lucene.analysis.ngram.NGramTokenizer;
 import org.apache.lucene.util.Version;
-
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
@@ -33,9 +31,12 @@ import org.elasticsearch.index.settings.IndexSettings;
 
 import java.lang.reflect.Field;
 import java.lang.reflect.Modifier;
+import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
@@ -49,12 +50,12 @@ public class NGramTokenizerFactory extends AbstractTokenizerFactory {
     static final Map<String, CharMatcher> MATCHERS;
 
     static {
-        ImmutableMap.Builder<String, CharMatcher> builder = ImmutableMap.builder();
-        builder.put("letter", CharMatcher.Basic.LETTER);
-        builder.put("digit", CharMatcher.Basic.DIGIT);
-        builder.put("whitespace", CharMatcher.Basic.WHITESPACE);
-        builder.put("punctuation", CharMatcher.Basic.PUNCTUATION);
-        builder.put("symbol", CharMatcher.Basic.SYMBOL);
+        Map<String, CharMatcher> matchers = new HashMap<>();
+        matchers.put("letter", CharMatcher.Basic.LETTER);
+        matchers.put("digit", CharMatcher.Basic.DIGIT);
+        matchers.put("whitespace", CharMatcher.Basic.WHITESPACE);
+        matchers.put("punctuation", CharMatcher.Basic.PUNCTUATION);
+        matchers.put("symbol", CharMatcher.Basic.SYMBOL);
         // Populate with unicode categories from java.lang.Character
         for (Field field : Character.class.getFields()) {
             if (!field.getName().startsWith("DIRECTIONALITY")
@@ -62,14 +63,14 @@ public class NGramTokenizerFactory extends AbstractTokenizerFactory {
                     && Modifier.isStatic(field.getModifiers())
                     && field.getType() == byte.class) {
                 try {
-                    builder.put(field.getName().toLowerCase(Locale.ROOT), CharMatcher.ByUnicodeCategory.of(field.getByte(null)));
+                    matchers.put(field.getName().toLowerCase(Locale.ROOT), CharMatcher.ByUnicodeCategory.of(field.getByte(null)));
                 } catch (Exception e) {
                     // just ignore
                     continue;
                 }
             }
         }
-        MATCHERS = builder.build();
+        MATCHERS = unmodifiableMap(matchers);
     }
 
     static CharMatcher parseTokenChars(String[] characterClasses) {
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java b/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
index 39cf56b..a3f9004 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java
@@ -18,13 +18,11 @@
  */
 package org.elasticsearch.index.analysis;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.lucene.analysis.de.GermanAnalyzer;
 import org.apache.lucene.analysis.fr.FrenchAnalyzer;
 import org.apache.lucene.analysis.nl.DutchAnalyzer;
 import org.apache.lucene.analysis.util.CharArraySet;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
@@ -32,6 +30,11 @@ import org.elasticsearch.env.Environment;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.settings.IndexSettings;
 
+import java.util.HashMap;
+import java.util.Map;
+
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Creates a SnowballAnalyzer initialized with stopwords and Snowball filter. Only
  * supports Dutch, English (default), French, German and German2 where stopwords
@@ -46,14 +49,17 @@ import org.elasticsearch.index.settings.IndexSettings;
  *
  */
 public class SnowballAnalyzerProvider extends AbstractIndexAnalyzerProvider<SnowballAnalyzer> {
+    private static final Map<String, CharArraySet> DEFAULT_LANGUAGE_STOPWORDS;
 
-    private static final ImmutableMap<String, CharArraySet> defaultLanguageStopwords = MapBuilder.<String, CharArraySet>newMapBuilder()
-            .put("English", StopAnalyzer.ENGLISH_STOP_WORDS_SET)
-            .put("Dutch", DutchAnalyzer.getDefaultStopSet())
-            .put("German", GermanAnalyzer.getDefaultStopSet())
-            .put("German2", GermanAnalyzer.getDefaultStopSet())
-            .put("French", FrenchAnalyzer.getDefaultStopSet())
-            .immutableMap();
+    static {
+        Map<String, CharArraySet> defaultLanguageStopwords = new HashMap<>();
+        defaultLanguageStopwords.put("English", StopAnalyzer.ENGLISH_STOP_WORDS_SET);
+        defaultLanguageStopwords.put("Dutch", DutchAnalyzer.getDefaultStopSet());
+        defaultLanguageStopwords.put("German", GermanAnalyzer.getDefaultStopSet());
+        defaultLanguageStopwords.put("German2", GermanAnalyzer.getDefaultStopSet());
+        defaultLanguageStopwords.put("French", FrenchAnalyzer.getDefaultStopSet());
+        DEFAULT_LANGUAGE_STOPWORDS = unmodifiableMap(defaultLanguageStopwords);
+    }
 
     private final SnowballAnalyzer analyzer;
 
@@ -62,7 +68,7 @@ public class SnowballAnalyzerProvider extends AbstractIndexAnalyzerProvider<Snow
         super(index, indexSettings, name, settings);
 
         String language = settings.get("language", settings.get("name", "English"));
-        CharArraySet defaultStopwords = defaultLanguageStopwords.containsKey(language) ? defaultLanguageStopwords.get(language) : CharArraySet.EMPTY_SET;
+        CharArraySet defaultStopwords = DEFAULT_LANGUAGE_STOPWORDS.getOrDefault(language, CharArraySet.EMPTY_SET);
         CharArraySet stopWords = Analysis.parseStopWords(env, settings, defaultStopwords);
 
         analyzer = new SnowballAnalyzer(language, stopWords);
diff --git a/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java b/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
index 30c0905..f2b7ba8 100644
--- a/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
+++ b/core/src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java
@@ -19,11 +19,6 @@
 
 package org.elasticsearch.index.cache.bitset;
 
-import com.google.common.cache.Cache;
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.RemovalListener;
-import com.google.common.cache.RemovalNotification;
-
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.LeafReaderContext;
@@ -38,6 +33,10 @@ import org.apache.lucene.util.BitDocIdSet;
 import org.apache.lucene.util.BitSet;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.cache.Cache;
+import org.elasticsearch.common.cache.CacheBuilder;
+import org.elasticsearch.common.cache.RemovalListener;
+import org.elasticsearch.common.cache.RemovalNotification;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.settings.Settings;
@@ -58,10 +57,11 @@ import org.elasticsearch.threadpool.ThreadPool;
 import java.io.Closeable;
 import java.io.IOException;
 import java.util.HashSet;
-import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
-import java.util.concurrent.*;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Executor;
 
 /**
  * This is a cache for {@link BitDocIdSet} based filters and is unbounded by size or time.
@@ -94,10 +94,11 @@ public class BitsetFilterCache extends AbstractIndexComponent implements LeafRea
     public BitsetFilterCache(Index index, @IndexSettings Settings indexSettings) {
         super(index, indexSettings);
         this.loadRandomAccessFiltersEagerly = indexSettings.getAsBoolean(LOAD_RANDOM_ACCESS_FILTERS_EAGERLY, true);
-        this.loadedFilters = CacheBuilder.newBuilder().removalListener(this).build();
+        this.loadedFilters = CacheBuilder.<Object, Cache<Query, Value>>builder().removalListener(this).build();
         this.warmer = new BitSetProducerWarmer();
     }
 
+
     @Inject(optional = true)
     public void setIndicesWarmer(IndicesWarmer indicesWarmer) {
         this.indicesWarmer = indicesWarmer;
@@ -144,14 +145,12 @@ public class BitsetFilterCache extends AbstractIndexComponent implements LeafRea
     private BitSet getAndLoadIfNotPresent(final Query query, final LeafReaderContext context) throws IOException, ExecutionException {
         final Object coreCacheReader = context.reader().getCoreCacheKey();
         final ShardId shardId = ShardUtils.extractShardId(context.reader());
-        Cache<Query, Value> filterToFbs = loadedFilters.get(coreCacheReader, new Callable<Cache<Query, Value>>() {
-            @Override
-            public Cache<Query, Value> call() throws Exception {
-                context.reader().addCoreClosedListener(BitsetFilterCache.this);
-                return CacheBuilder.newBuilder().build();
-            }
+        Cache<Query, Value> filterToFbs = loadedFilters.computeIfAbsent(coreCacheReader, key -> {
+            context.reader().addCoreClosedListener(BitsetFilterCache.this);
+            return CacheBuilder.<Query, Value>builder().build();
         });
-        return filterToFbs.get(query, () -> {
+
+        return filterToFbs.computeIfAbsent(query, key -> {
             final IndexReaderContext topLevelContext = ReaderUtil.getTopLevelContext(context);
             final IndexSearcher searcher = new IndexSearcher(topLevelContext);
             searcher.setQueryCache(null);
@@ -172,8 +171,7 @@ public class BitsetFilterCache extends AbstractIndexComponent implements LeafRea
 
     @Override
     public void onRemoval(RemovalNotification<Object, Cache<Query, Value>> notification) {
-        Object key = notification.getKey();
-        if (key == null) {
+        if (notification.getKey() == null) {
             return;
         }
 
@@ -182,7 +180,7 @@ public class BitsetFilterCache extends AbstractIndexComponent implements LeafRea
             return;
         }
 
-        for (Value value : valueCache.asMap().values()) {
+        for (Value value : valueCache.values()) {
             listener.onRemoval(value.shardId, value.bitset);
             // if null then this means the shard has already been removed and the stats are 0 anyway for the shard this key belongs to
         }
diff --git a/core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java b/core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java
index ef82e73..0f594d2 100644
--- a/core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/index/cache/request/ShardRequestCache.java
@@ -19,10 +19,8 @@
 
 package org.elasticsearch.index.cache.request;
 
-import com.google.common.cache.RemovalListener;
-import com.google.common.cache.RemovalNotification;
-
-import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.cache.RemovalListener;
+import org.elasticsearch.common.cache.RemovalNotification;
 import org.elasticsearch.common.metrics.CounterMetric;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.settings.IndexSettings;
@@ -61,7 +59,7 @@ public class ShardRequestCache extends AbstractIndexShardComponent implements Re
 
     @Override
     public void onRemoval(RemovalNotification<IndicesRequestCache.Key, IndicesRequestCache.Value> removalNotification) {
-        if (removalNotification.wasEvicted()) {
+        if (removalNotification.getRemovalReason() == RemovalNotification.RemovalReason.EVICTED) {
             evictionsMetric.inc();
         }
         long dec = 0;
diff --git a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java b/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java
deleted file mode 100644
index 71a52a7..0000000
--- a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java
+++ /dev/null
@@ -1,440 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.codec.postingsformat;
-
-import org.apache.lucene.codecs.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.*;
-import org.elasticsearch.common.util.BloomFilter;
-
-import java.io.IOException;
-import java.util.*;
-import java.util.Map.Entry;
-
-/**
- * <p>
- * A {@link PostingsFormat} useful for low doc-frequency fields such as primary
- * keys. Bloom filters are maintained in a ".blm" file which offers "fast-fail"
- * for reads in segments known to have no record of the key. A choice of
- * delegate PostingsFormat is used to record all other Postings data.
- * </p>
- * <p>
- * This is a special bloom filter version, based on {@link org.elasticsearch.common.util.BloomFilter} and inspired
- * by Lucene {@code org.apache.lucene.codecs.bloom.BloomFilteringPostingsFormat}.
- * @deprecated only for reading old segments
- */
-@Deprecated
-public class BloomFilterPostingsFormat extends PostingsFormat {
-
-    public static final String BLOOM_CODEC_NAME = "XBloomFilter"; // the Lucene one is named BloomFilter
-    public static final int BLOOM_CODEC_VERSION = 1;
-    public static final int BLOOM_CODEC_VERSION_CHECKSUM = 2;
-    public static final int BLOOM_CODEC_VERSION_CURRENT = BLOOM_CODEC_VERSION_CHECKSUM;
-
-    /**
-     * Extension of Bloom Filters file
-     */
-    static final String BLOOM_EXTENSION = "blm";
-
-    private BloomFilter.Factory bloomFilterFactory = BloomFilter.Factory.DEFAULT;
-    private PostingsFormat delegatePostingsFormat;
-
-    /**
-     * Creates Bloom filters for a selection of fields created in the index. This
-     * is recorded as a set of Bitsets held as a segment summary in an additional
-     * "blm" file. This PostingsFormat delegates to a choice of delegate
-     * PostingsFormat for encoding all other postings data.
-     *
-     * @param delegatePostingsFormat The PostingsFormat that records all the non-bloom filter data i.e.
-     *                               postings info.
-     * @param bloomFilterFactory     The {@link org.elasticsearch.common.util.BloomFilter.Factory} responsible for sizing BloomFilters
-     *                               appropriately
-     */
-    public BloomFilterPostingsFormat(PostingsFormat delegatePostingsFormat,
-                                     BloomFilter.Factory bloomFilterFactory) {
-        super(BLOOM_CODEC_NAME);
-        this.delegatePostingsFormat = delegatePostingsFormat;
-        this.bloomFilterFactory = bloomFilterFactory;
-    }
-
-    // Used only by core Lucene at read-time via Service Provider instantiation -
-    // do not use at Write-time in application code.
-    public BloomFilterPostingsFormat() {
-        super(BLOOM_CODEC_NAME);
-    }
-
-    @Override
-    public BloomFilteredFieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-        throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-
-    @Override
-    public BloomFilteredFieldsProducer fieldsProducer(SegmentReadState state)
-            throws IOException {
-        return new BloomFilteredFieldsProducer(state);
-    }
-
-    public PostingsFormat getDelegate() {
-        return delegatePostingsFormat;
-    }
-
-    private final class LazyBloomLoader implements Accountable {
-        private final long offset;
-        private final IndexInput indexInput;
-        private BloomFilter filter;
-
-        private LazyBloomLoader(long offset, IndexInput origial) {
-            this.offset = offset;
-            this.indexInput = origial.clone();
-        }
-
-        synchronized BloomFilter get() throws IOException {
-            if (filter == null) {
-                try (final IndexInput input = indexInput) {
-                    input.seek(offset);
-                    this.filter = BloomFilter.deserialize(input);
-                }
-            }
-            return filter;
-        }
-
-        @Override
-        public long ramBytesUsed() {
-            return filter == null ? 0l : filter.getSizeInBytes();
-        }
-
-        @Override
-        public Collection<Accountable> getChildResources() {
-            return Collections.singleton(Accountables.namedAccountable("bloom", ramBytesUsed()));
-        }
-    }
-
-    public final class BloomFilteredFieldsProducer extends FieldsProducer {
-        private FieldsProducer delegateFieldsProducer;
-        HashMap<String, LazyBloomLoader> bloomsByFieldName = new HashMap<>();
-        private final int version;
-        private final IndexInput data;
-
-        // for internal use only
-        FieldsProducer getDelegate() {
-            return delegateFieldsProducer;
-        }
-
-        public BloomFilteredFieldsProducer(SegmentReadState state)
-                throws IOException {
-
-            final String bloomFileName = IndexFileNames.segmentFileName(
-                    state.segmentInfo.name, state.segmentSuffix, BLOOM_EXTENSION);
-            final Directory directory = state.directory;
-            IndexInput dataInput = directory.openInput(bloomFileName, state.context);
-            try {
-                ChecksumIndexInput bloomIn = new BufferedChecksumIndexInput(dataInput.clone());
-                version = CodecUtil.checkHeader(bloomIn, BLOOM_CODEC_NAME, BLOOM_CODEC_VERSION,
-                        BLOOM_CODEC_VERSION_CURRENT);
-                // // Load the hash function used in the BloomFilter
-                // hashFunction = HashFunction.forName(bloomIn.readString());
-                // Load the delegate postings format
-               final String delegatePostings = bloomIn.readString();
-                this.delegateFieldsProducer = PostingsFormat.forName(delegatePostings)
-                        .fieldsProducer(state);
-                this.data = dataInput;
-                dataInput = null; // null it out such that we don't close it
-            } finally {
-                IOUtils.closeWhileHandlingException(dataInput);
-            }
-        }
-
-        @Override
-        public Iterator<String> iterator() {
-            return delegateFieldsProducer.iterator();
-        }
-
-        @Override
-        public void close() throws IOException {
-            IOUtils.close(data, delegateFieldsProducer);
-        }
-
-        @Override
-        public Terms terms(String field) throws IOException {
-            LazyBloomLoader filter = bloomsByFieldName.get(field);
-            if (filter == null) {
-                return delegateFieldsProducer.terms(field);
-            } else {
-                Terms result = delegateFieldsProducer.terms(field);
-                if (result == null) {
-                    return null;
-                }
-                return new BloomFilteredTerms(result, filter.get());
-            }
-        }
-
-        @Override
-        public int size() {
-            return delegateFieldsProducer.size();
-        }
-
-        @Override
-        public long ramBytesUsed() {
-            long size = delegateFieldsProducer.ramBytesUsed();
-            for (LazyBloomLoader bloomFilter : bloomsByFieldName.values()) {
-                size += bloomFilter.ramBytesUsed();
-            }
-            return size;
-        }
-
-        @Override
-        public Collection<Accountable> getChildResources() {
-            List<Accountable> resources = new ArrayList<>();
-            resources.addAll(Accountables.namedAccountables("field", bloomsByFieldName));
-            if (delegateFieldsProducer != null) {
-                resources.add(Accountables.namedAccountable("delegate", delegateFieldsProducer));
-            }
-            return Collections.unmodifiableList(resources);
-        }
-
-        @Override
-        public void checkIntegrity() throws IOException {
-            delegateFieldsProducer.checkIntegrity();
-            if (version >= BLOOM_CODEC_VERSION_CHECKSUM) {
-                CodecUtil.checksumEntireFile(data);
-            }
-        }
-
-        @Override
-        public FieldsProducer getMergeInstance() throws IOException {
-            return delegateFieldsProducer.getMergeInstance();
-        }
-    }
-
-    public static final class BloomFilteredTerms extends FilterLeafReader.FilterTerms {
-        private BloomFilter filter;
-
-        public BloomFilteredTerms(Terms terms, BloomFilter filter) {
-            super(terms);
-            this.filter = filter;
-        }
-
-        public BloomFilter getFilter() {
-            return filter;
-        }
-
-        @Override
-        public TermsEnum iterator() throws IOException {
-            return new BloomFilteredTermsEnum(this.in, filter);
-        }
-    }
-
-    static final class BloomFilteredTermsEnum extends TermsEnum {
-
-        private Terms delegateTerms;
-        private TermsEnum delegateTermsEnum;
-        private BloomFilter filter;
-
-        public BloomFilteredTermsEnum(Terms other, BloomFilter filter) {
-            this.delegateTerms = other;
-            this.filter = filter;
-        }
-
-        void reset(Terms others) {
-            this.delegateTermsEnum = null;
-            this.delegateTerms = others;
-        }
-
-        private TermsEnum getDelegate() throws IOException {
-            if (delegateTermsEnum == null) {
-                /* pull the iterator only if we really need it -
-                 * this can be a relatively heavy operation depending on the 
-                 * delegate postings format and they underlying directory
-                 * (clone IndexInput) */
-                delegateTermsEnum = delegateTerms.iterator();
-            }
-            return delegateTermsEnum;
-        }
-
-        @Override
-        public final BytesRef next() throws IOException {
-            return getDelegate().next();
-        }
-
-        @Override
-        public final boolean seekExact(BytesRef text)
-                throws IOException {
-            // The magical fail-fast speed up that is the entire point of all of
-            // this code - save a disk seek if there is a match on an in-memory
-            // structure
-            // that may occasionally give a false positive but guaranteed no false
-            // negatives
-            if (!filter.mightContain(text)) {
-                return false;
-            }
-            return getDelegate().seekExact(text);
-        }
-
-        @Override
-        public final SeekStatus seekCeil(BytesRef text)
-                throws IOException {
-            return getDelegate().seekCeil(text);
-        }
-
-        @Override
-        public final void seekExact(long ord) throws IOException {
-            getDelegate().seekExact(ord);
-        }
-
-        @Override
-        public final BytesRef term() throws IOException {
-            return getDelegate().term();
-        }
-
-        @Override
-        public final long ord() throws IOException {
-            return getDelegate().ord();
-        }
-
-        @Override
-        public final int docFreq() throws IOException {
-            return getDelegate().docFreq();
-        }
-
-        @Override
-        public final long totalTermFreq() throws IOException {
-            return getDelegate().totalTermFreq();
-        }
-
-
-        @Override
-        public PostingsEnum postings(PostingsEnum reuse, int flags) throws IOException {
-            return getDelegate().postings(reuse, flags);
-        }
-    }
-
-    // TODO: would be great to move this out to test code, but the interaction between es090 and bloom is complex
-    // at least it is not accessible via SPI
-    public final class BloomFilteredFieldsConsumer extends FieldsConsumer {
-        private final FieldsConsumer delegateFieldsConsumer;
-        private final Map<FieldInfo, BloomFilter> bloomFilters = new HashMap<>();
-        private final SegmentWriteState state;
-        private boolean closed = false;
-
-        // private PostingsFormat delegatePostingsFormat;
-
-        public BloomFilteredFieldsConsumer(FieldsConsumer fieldsConsumer,
-                                           SegmentWriteState state, PostingsFormat delegatePostingsFormat) {
-            this.delegateFieldsConsumer = fieldsConsumer;
-            // this.delegatePostingsFormat=delegatePostingsFormat;
-            this.state = state;
-        }
-
-        // for internal use only
-        public FieldsConsumer getDelegate() {
-            return delegateFieldsConsumer;
-        }
-
-
-        @Override
-        public void write(Fields fields) throws IOException {
-
-            // Delegate must write first: it may have opened files
-            // on creating the class
-            // (e.g. Lucene41PostingsConsumer), and write() will
-            // close them; alternatively, if we delayed pulling
-            // the fields consumer until here, we could do it
-            // afterwards:
-            delegateFieldsConsumer.write(fields);
-
-            for(String field : fields) {
-                Terms terms = fields.terms(field);
-                if (terms == null) {
-                    continue;
-                }
-                FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
-                TermsEnum termsEnum = terms.iterator();
-
-                BloomFilter bloomFilter = null;
-
-                PostingsEnum postings = null;
-                while (true) {
-                    BytesRef term = termsEnum.next();
-                    if (term == null) {
-                        break;
-                    }
-                    if (bloomFilter == null) {
-                        bloomFilter = bloomFilterFactory.createFilter(state.segmentInfo.maxDoc());
-                        assert bloomFilters.containsKey(field) == false;
-                        bloomFilters.put(fieldInfo, bloomFilter);
-                    }
-                    // Make sure there's at least one doc for this term:
-                    postings = termsEnum.postings(postings, 0);
-                    if (postings.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
-                        bloomFilter.put(term);
-                    }
-                }
-            }
-        }
-
-        @Override
-        public void close() throws IOException {
-            if (closed) {
-                return;
-            }
-            closed = true;
-            delegateFieldsConsumer.close();
-            // Now we are done accumulating values for these fields
-            List<Entry<FieldInfo, BloomFilter>> nonSaturatedBlooms = new ArrayList<>();
-
-            for (Entry<FieldInfo, BloomFilter> entry : bloomFilters.entrySet()) {
-                nonSaturatedBlooms.add(entry);
-            }
-            String bloomFileName = IndexFileNames.segmentFileName(
-                    state.segmentInfo.name, state.segmentSuffix, BLOOM_EXTENSION);
-            IndexOutput bloomOutput = null;
-            try {
-                bloomOutput = state.directory
-                        .createOutput(bloomFileName, state.context);
-                CodecUtil.writeHeader(bloomOutput, BLOOM_CODEC_NAME,
-                        BLOOM_CODEC_VERSION_CURRENT);
-                // remember the name of the postings format we will delegate to
-                bloomOutput.writeString(delegatePostingsFormat.getName());
-
-                // First field in the output file is the number of fields+blooms saved
-                bloomOutput.writeInt(nonSaturatedBlooms.size());
-                for (Entry<FieldInfo, BloomFilter> entry : nonSaturatedBlooms) {
-                    FieldInfo fieldInfo = entry.getKey();
-                    BloomFilter bloomFilter = entry.getValue();
-                    bloomOutput.writeInt(fieldInfo.number);
-                    saveAppropriatelySizedBloomFilter(bloomOutput, bloomFilter, fieldInfo);
-                }
-                CodecUtil.writeFooter(bloomOutput);
-            } finally {
-                IOUtils.close(bloomOutput);
-            }
-            //We are done with large bitsets so no need to keep them hanging around
-            bloomFilters.clear();
-        }
-
-        private void saveAppropriatelySizedBloomFilter(IndexOutput bloomOutput,
-                                                       BloomFilter bloomFilter, FieldInfo fieldInfo) throws IOException {
-            BloomFilter.serilaize(bloomFilter, bloomOutput);
-        }
-
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java b/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java
deleted file mode 100644
index f7926c7..0000000
--- a/core/src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.codec.postingsformat;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.util.BloomFilter;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-
-import java.io.IOException;
-import java.util.function.Predicate;
-
-/**
- * This is the old default postings format for Elasticsearch that special cases
- * the <tt>_uid</tt> field to use a bloom filter while all other fields
- * will use a {@link Lucene50PostingsFormat}. This format will reuse the underlying
- * {@link Lucene50PostingsFormat} and its files also for the <tt>_uid</tt> saving up to
- * 5 files per segment in the default case.
- * <p>
- * @deprecated only for reading old segments
- */
-@Deprecated
-public class Elasticsearch090PostingsFormat extends PostingsFormat {
-    protected final BloomFilterPostingsFormat bloomPostings;
-
-    public Elasticsearch090PostingsFormat() {
-        super("es090");
-        Lucene50PostingsFormat delegate = new Lucene50PostingsFormat();
-        assert delegate.getName().equals(Lucene.LATEST_POSTINGS_FORMAT);
-        bloomPostings = new BloomFilterPostingsFormat(delegate, BloomFilter.Factory.DEFAULT);
-    }
-
-    public PostingsFormat getDefaultWrapped() {
-        return bloomPostings.getDelegate();
-    }
-
-    protected static final Predicate<String> UID_FIELD_FILTER = field -> UidFieldMapper.NAME.equals(field);
-
-    @Override
-    public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-        throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-
-    @Override
-    public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-        // we can just return the delegate here since we didn't record bloom filters for 
-        // the other fields. 
-        return bloomPostings.fieldsProducer(state);
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/DeleteByQueryFailedEngineException.java b/core/src/main/java/org/elasticsearch/index/engine/DeleteByQueryFailedEngineException.java
deleted file mode 100644
index 95d57c5..0000000
--- a/core/src/main/java/org/elasticsearch/index/engine/DeleteByQueryFailedEngineException.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.engine;
-
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.index.shard.ShardId;
-
-import java.io.IOException;
-
-/** @deprecated Delete-by-query is removed in 2.0, but we keep this so translog can replay on upgrade. */
-@Deprecated
-public class DeleteByQueryFailedEngineException extends EngineException {
-
-    public DeleteByQueryFailedEngineException(ShardId shardId, Engine.DeleteByQuery deleteByQuery, Throwable cause) {
-        super(shardId, "Delete by query failed for [" + deleteByQuery.query() + "]", cause);
-    }
-
-    public DeleteByQueryFailedEngineException(StreamInput in) throws IOException{
-        super(in);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/engine/Engine.java b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
index e9d61e1..c07be06 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/Engine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/Engine.java
@@ -206,10 +206,6 @@ public abstract class Engine implements Closeable {
 
     public abstract void delete(Delete delete) throws EngineException;
 
-    /** @deprecated This was removed, but we keep this API so translog can replay any DBQs on upgrade. */
-    @Deprecated
-    public abstract void delete(DeleteByQuery delete) throws EngineException;
-
     /**
      * Attempts to do a special commit where the given syncID is put into the commit data. The attempt
      * succeeds if there are not pending writes in lucene and the current point is equal to the expected one.
diff --git a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
index 237857e..3973b47 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java
@@ -21,8 +21,9 @@ package org.elasticsearch.index.engine;
 
 import org.apache.lucene.index.*;
 import org.apache.lucene.index.IndexWriter.IndexReaderWarmer;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.SearcherFactory;
+import org.apache.lucene.search.SearcherManager;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
@@ -31,7 +32,7 @@ import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.InfoStream;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.cluster.routing.DjbHashFunction;
+import org.elasticsearch.cluster.routing.Murmur3HashFunction;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.logging.ESLogger;
@@ -48,7 +49,6 @@ import org.elasticsearch.index.indexing.ShardIndexingService;
 import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.merge.OnGoingMerge;
-import org.elasticsearch.index.search.nested.IncludeNestedDocsQuery;
 import org.elasticsearch.index.shard.ElasticsearchMergePolicy;
 import org.elasticsearch.index.shard.MergeSchedulerConfig;
 import org.elasticsearch.index.shard.ShardId;
@@ -67,7 +67,6 @@ import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 import java.util.function.Function;
-import java.util.function.Supplier;
 
 /**
  *
@@ -182,8 +181,7 @@ public class InternalEngine extends Engine {
             }
             translogConfig.setTranslogGeneration(generation);
             if (generation != null && generation.translogUUID == null) {
-                // only upgrade on pre-2.0 indices...
-                Translog.upgradeLegacyTranslog(logger, translogConfig);
+                throw new IndexFormatTooOldException("trasnlog", "translog has no generation nor a UUID - this might be an index from a previous version consider upgrading to N-1 first");
             }
         }
         final Translog translog = new Translog(translogConfig);
@@ -512,48 +510,6 @@ public class InternalEngine extends Engine {
         }
     }
 
-    /** @deprecated This was removed, but we keep this API so translog can replay any DBQs on upgrade. */
-    @Deprecated
-    @Override
-    public void delete(DeleteByQuery delete) throws EngineException {
-        try (ReleasableLock lock = readLock.acquire()) {
-            ensureOpen();
-            if (delete.origin() == Operation.Origin.RECOVERY) {
-                // Don't throttle recovery operations
-                innerDelete(delete);
-            } else {
-                try (Releasable r = throttle.acquireThrottle()) {
-                    innerDelete(delete);
-                }
-            }
-        }
-    }
-
-    private void innerDelete(DeleteByQuery delete) throws EngineException {
-        try {
-            Query query = delete.query();
-            if (delete.aliasFilter() != null) {
-                query = new BooleanQuery.Builder()
-                        .add(query, Occur.MUST)
-                        .add(delete.aliasFilter(), Occur.FILTER)
-                        .build();
-            }
-            if (delete.nested()) {
-                query = new IncludeNestedDocsQuery(query, delete.parentFilter());
-            }
-
-            indexWriter.deleteDocuments(query);
-            translog.add(new Translog.DeleteByQuery(delete));
-        } catch (Throwable t) {
-            maybeFailEngine("delete_by_query", t);
-            throw new DeleteByQueryFailedEngineException(shardId, delete, t);
-        }
-
-        // TODO: This is heavy, since we refresh, but we must do this because we don't know which documents were in fact deleted (i.e., our
-        // versionMap isn't updated), so we must force a cutover to a new reader to "see" the deletions:
-        refresh("delete_by_query");
-    }
-
     @Override
     public void refresh(String source) throws EngineException {
         // we obtain a read lock here, since we don't want a flush to happen while we are refreshing
@@ -900,7 +856,7 @@ public class InternalEngine extends Engine {
     }
 
     private Object dirtyLock(BytesRef uid) {
-        int hash = DjbHashFunction.DJB_HASH(uid.bytes, uid.offset, uid.length);
+        int hash = Murmur3HashFunction.hash(uid.bytes, uid.offset, uid.length);
         return dirtyLocks[MathUtils.mod(hash, dirtyLocks.length)];
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java b/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
index 3892958..921f116 100644
--- a/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
+++ b/core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java
@@ -112,13 +112,6 @@ public class ShadowEngine extends Engine {
         throw new UnsupportedOperationException(shardId + " delete operation not allowed on shadow engine");
     }
 
-    /** @deprecated This was removed, but we keep this API so translog can replay any DBQs on upgrade. */
-    @Deprecated
-    @Override
-    public void delete(DeleteByQuery delete) throws EngineException {
-        throw new UnsupportedOperationException(shardId + " delete-by-query operation not allowed on shadow engine");
-    }
-
     @Override
     public SyncedFlushResult syncFlush(String syncId, CommitId expectedCommitId) {
         throw new UnsupportedOperationException(shardId + " sync commit operation not allowed on shadow engine");
diff --git a/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java b/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java
index a9713ce..de3adbc 100644
--- a/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java
+++ b/core/src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java
@@ -19,10 +19,8 @@
 
 package org.elasticsearch.index.fielddata;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.Accountable;
 import org.elasticsearch.ExceptionsHelper;
-import org.elasticsearch.Version;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
@@ -41,6 +39,7 @@ import org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData;
 import org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData;
 import org.elasticsearch.index.fielddata.plain.ParentChildIndexFieldData;
 import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.MappedFieldType.Names;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
 import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
@@ -56,7 +55,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static org.elasticsearch.index.mapper.MappedFieldType.Names;
+import static java.util.Collections.unmodifiableMap;
 
 /**
  */
@@ -70,26 +69,28 @@ public class IndexFieldDataService extends AbstractIndexComponent {
     private static final String ARRAY_FORMAT = "array";
     private static final String PAGED_BYTES_FORMAT = "paged_bytes";
 
-    private final static ImmutableMap<String, IndexFieldData.Builder> buildersByType;
-    private final static ImmutableMap<String, IndexFieldData.Builder> docValuesBuildersByType;
-    private final static ImmutableMap<Tuple<String, String>, IndexFieldData.Builder> buildersByTypeAndFormat;
+    private final static Map<String, IndexFieldData.Builder> buildersByType;
+    private final static Map<String, IndexFieldData.Builder> docValuesBuildersByType;
+    private final static Map<Tuple<String, String>, IndexFieldData.Builder> buildersByTypeAndFormat;
     private final CircuitBreakerService circuitBreakerService;
 
     static {
-        buildersByType = MapBuilder.<String, IndexFieldData.Builder>newMapBuilder()
-                .put("string", new PagedBytesIndexFieldData.Builder())
-                .put("float", new FloatArrayIndexFieldData.Builder())
-                .put("double", new DoubleArrayIndexFieldData.Builder())
-                .put("byte", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BYTE))
-                .put("short", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.SHORT))
-                .put("int", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.INT))
-                .put("long", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.LONG))
-                .put("geo_point", new GeoPointDoubleArrayIndexFieldData.Builder())
-                .put(ParentFieldMapper.NAME, new ParentChildIndexFieldData.Builder())
-                .put(IndexFieldMapper.NAME, new IndexIndexFieldData.Builder())
-                .put("binary", new DisabledIndexFieldData.Builder())
-                .put(BooleanFieldMapper.CONTENT_TYPE, new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BOOLEAN))
-                .immutableMap();
+        Map<String, IndexFieldData.Builder> buildersByTypeBuilder = new HashMap<>();
+        buildersByTypeBuilder.put("string", new PagedBytesIndexFieldData.Builder());
+        buildersByTypeBuilder.put("float", new FloatArrayIndexFieldData.Builder());
+        buildersByTypeBuilder.put("double", new DoubleArrayIndexFieldData.Builder());
+        buildersByTypeBuilder.put("byte", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BYTE));
+        buildersByTypeBuilder.put("short", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.SHORT));
+        buildersByTypeBuilder.put("int", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.INT));
+        buildersByTypeBuilder.put("long", new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.LONG));
+        buildersByTypeBuilder.put("geo_point", new GeoPointDoubleArrayIndexFieldData.Builder());
+        buildersByTypeBuilder.put(ParentFieldMapper.NAME, new ParentChildIndexFieldData.Builder());
+        buildersByTypeBuilder.put(IndexFieldMapper.NAME, new IndexIndexFieldData.Builder());
+        buildersByTypeBuilder.put("binary", new DisabledIndexFieldData.Builder());
+        buildersByTypeBuilder.put(BooleanFieldMapper.CONTENT_TYPE,
+                new PackedArrayIndexFieldData.Builder().setNumericType(IndexNumericFieldData.NumericType.BOOLEAN));
+         buildersByType = unmodifiableMap(buildersByTypeBuilder);
+
 
         docValuesBuildersByType = MapBuilder.<String, IndexFieldData.Builder>newMapBuilder()
                 .put("string", new DocValuesIndexFieldData.Builder())
diff --git a/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java b/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
index fa4b587..899da8f 100644
--- a/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
+++ b/core/src/main/java/org/elasticsearch/index/fieldvisitor/FieldsVisitor.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.index.fieldvisitor;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.StoredFieldVisitor;
@@ -47,6 +45,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.emptyMap;
 import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.common.util.set.Sets.newHashSet;
 
@@ -191,9 +190,7 @@ public class FieldsVisitor extends StoredFieldVisitor {
     }
 
     public Map<String, List<Object>> fields() {
-        return fieldsValues != null
-                ? fieldsValues
-                : ImmutableMap.<String, List<Object>>of();
+        return fieldsValues != null ? fieldsValues : emptyMap();
     }
 
     public void reset() {
diff --git a/core/src/main/java/org/elasticsearch/index/get/GetResult.java b/core/src/main/java/org/elasticsearch/index/get/GetResult.java
index c788fcf..d243694 100644
--- a/core/src/main/java/org/elasticsearch/index/get/GetResult.java
+++ b/core/src/main/java/org/elasticsearch/index/get/GetResult.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.get;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.compress.CompressorFactory;
@@ -40,6 +39,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
 import static org.elasticsearch.index.get.GetField.readGetField;
 
 /**
@@ -68,7 +68,7 @@ public class GetResult implements Streamable, Iterable<GetField>, ToXContent {
         this.source = source;
         this.fields = fields;
         if (this.fields == null) {
-            this.fields = ImmutableMap.of();
+            this.fields = emptyMap();
         }
     }
 
@@ -286,7 +286,7 @@ public class GetResult implements Streamable, Iterable<GetField>, ToXContent {
             }
             int size = in.readVInt();
             if (size == 0) {
-                fields = ImmutableMap.of();
+                fields = emptyMap();
             } else {
                 fields = new HashMap<>(size);
                 for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
index d1abbf1..a3a1fa5 100644
--- a/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
+++ b/core/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.indexing;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.metrics.CounterMetric;
 import org.elasticsearch.common.metrics.MeanMetric;
@@ -35,6 +34,8 @@ import java.util.Map;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.TimeUnit;
 
+import static java.util.Collections.emptyMap;
+
 /**
  */
 public class ShardIndexingService extends AbstractIndexShardComponent {
@@ -45,7 +46,7 @@ public class ShardIndexingService extends AbstractIndexShardComponent {
 
     private final CopyOnWriteArrayList<IndexingOperationListener> listeners = new CopyOnWriteArrayList<>();
 
-    private volatile Map<String, StatsHolder> typesStats = ImmutableMap.of();
+    private volatile Map<String, StatsHolder> typesStats = emptyMap();
 
     public ShardIndexingService(ShardId shardId, Settings indexSettings) {
         super(shardId, indexSettings);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
index 54b2c98..0314f8f 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -70,6 +69,8 @@ import java.util.Map;
 import java.util.Objects;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 
+import static java.util.Collections.emptyMap;
+
 /**
  *
  */
@@ -85,7 +86,7 @@ public class DocumentMapper implements ToXContent {
 
         private final RootObjectMapper rootObjectMapper;
 
-        private ImmutableMap<String, Object> meta = ImmutableMap.of();
+        private Map<String, Object> meta = emptyMap();
 
         private final Mapper.BuilderContext builderContext;
 
@@ -115,7 +116,7 @@ public class DocumentMapper implements ToXContent {
             this.rootMappers.put(FieldNamesFieldMapper.class, new FieldNamesFieldMapper(indexSettings, mapperService.fullName(FieldNamesFieldMapper.NAME)));
         }
 
-        public Builder meta(ImmutableMap<String, Object> meta) {
+        public Builder meta(Map<String, Object> meta) {
             this.meta = meta;
             return this;
         }
@@ -169,7 +170,7 @@ public class DocumentMapper implements ToXContent {
 
     public DocumentMapper(MapperService mapperService, @Nullable Settings indexSettings, DocumentMapperParser docMapperParser,
                           RootObjectMapper rootObjectMapper,
-                          ImmutableMap<String, Object> meta,
+                          Map<String, Object> meta,
                           Map<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> rootMappers,
                           List<SourceTransform> sourceTransforms,
                           ReentrantReadWriteLock mappingLock) {
@@ -234,7 +235,7 @@ public class DocumentMapper implements ToXContent {
         return this.typeText;
     }
 
-    public ImmutableMap<String, Object> meta() {
+    public Map<String, Object> meta() {
         return mapping.meta;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
index 82ff5fb..91371a2 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java
@@ -19,12 +19,10 @@
 
 package org.elasticsearch.index.mapper;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.geo.ShapesAvailability;
@@ -35,10 +33,33 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.mapper.core.*;
+import org.elasticsearch.index.mapper.core.BinaryFieldMapper;
+import org.elasticsearch.index.mapper.core.BooleanFieldMapper;
+import org.elasticsearch.index.mapper.core.ByteFieldMapper;
+import org.elasticsearch.index.mapper.core.CompletionFieldMapper;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.elasticsearch.index.mapper.core.DoubleFieldMapper;
+import org.elasticsearch.index.mapper.core.FloatFieldMapper;
+import org.elasticsearch.index.mapper.core.IntegerFieldMapper;
+import org.elasticsearch.index.mapper.core.LongFieldMapper;
+import org.elasticsearch.index.mapper.core.ShortFieldMapper;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
+import org.elasticsearch.index.mapper.core.TokenCountFieldMapper;
+import org.elasticsearch.index.mapper.core.TypeParsers;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper;
-import org.elasticsearch.index.mapper.internal.*;
+import org.elasticsearch.index.mapper.internal.AllFieldMapper;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.internal.IdFieldMapper;
+import org.elasticsearch.index.mapper.internal.IndexFieldMapper;
+import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
+import org.elasticsearch.index.mapper.internal.RoutingFieldMapper;
+import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
+import org.elasticsearch.index.mapper.internal.TTLFieldMapper;
+import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
+import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
+import org.elasticsearch.index.mapper.internal.VersionFieldMapper;
 import org.elasticsearch.index.mapper.ip.IpFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.mapper.object.RootObjectMapper;
@@ -47,8 +68,16 @@ import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
 
-import java.util.*;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
 
+import static java.util.Collections.unmodifiableMap;
+import static java.util.Collections.unmodifiableSortedMap;
 import static org.elasticsearch.index.mapper.MapperBuilders.doc;
 
 public class DocumentMapperParser {
@@ -66,8 +95,8 @@ public class DocumentMapperParser {
     private final Version indexVersionCreated;
     private final ParseFieldMatcher parseFieldMatcher;
 
-    private volatile ImmutableMap<String, Mapper.TypeParser> typeParsers;
-    private volatile ImmutableMap<String, Mapper.TypeParser> rootTypeParsers;
+    private volatile Map<String, Mapper.TypeParser> typeParsers;
+    private volatile Map<String, Mapper.TypeParser> rootTypeParsers;
     private volatile SortedMap<String, Mapper.TypeParser> additionalRootMappers;
 
     public DocumentMapperParser(@IndexSettings Settings indexSettings, MapperService mapperService, AnalysisService analysisService,
@@ -78,66 +107,65 @@ public class DocumentMapperParser {
         this.analysisService = analysisService;
         this.similarityService = similarityService;
         this.scriptService = scriptService;
-        MapBuilder<String, Mapper.TypeParser> typeParsersBuilder = new MapBuilder<String, Mapper.TypeParser>()
-                .put(ByteFieldMapper.CONTENT_TYPE, new ByteFieldMapper.TypeParser())
-                .put(ShortFieldMapper.CONTENT_TYPE, new ShortFieldMapper.TypeParser())
-                .put(IntegerFieldMapper.CONTENT_TYPE, new IntegerFieldMapper.TypeParser())
-                .put(LongFieldMapper.CONTENT_TYPE, new LongFieldMapper.TypeParser())
-                .put(FloatFieldMapper.CONTENT_TYPE, new FloatFieldMapper.TypeParser())
-                .put(DoubleFieldMapper.CONTENT_TYPE, new DoubleFieldMapper.TypeParser())
-                .put(BooleanFieldMapper.CONTENT_TYPE, new BooleanFieldMapper.TypeParser())
-                .put(BinaryFieldMapper.CONTENT_TYPE, new BinaryFieldMapper.TypeParser())
-                .put(DateFieldMapper.CONTENT_TYPE, new DateFieldMapper.TypeParser())
-                .put(IpFieldMapper.CONTENT_TYPE, new IpFieldMapper.TypeParser())
-                .put(StringFieldMapper.CONTENT_TYPE, new StringFieldMapper.TypeParser())
-                .put(TokenCountFieldMapper.CONTENT_TYPE, new TokenCountFieldMapper.TypeParser())
-                .put(ObjectMapper.CONTENT_TYPE, new ObjectMapper.TypeParser())
-                .put(ObjectMapper.NESTED_CONTENT_TYPE, new ObjectMapper.TypeParser())
-                .put(TypeParsers.MULTI_FIELD_CONTENT_TYPE, TypeParsers.multiFieldConverterTypeParser)
-                .put(CompletionFieldMapper.CONTENT_TYPE, new CompletionFieldMapper.TypeParser())
-                .put(GeoPointFieldMapper.CONTENT_TYPE, new GeoPointFieldMapper.TypeParser());
+        Map<String, Mapper.TypeParser> typeParsers = new HashMap<>();
+        typeParsers.put(ByteFieldMapper.CONTENT_TYPE, new ByteFieldMapper.TypeParser());
+        typeParsers.put(ShortFieldMapper.CONTENT_TYPE, new ShortFieldMapper.TypeParser());
+        typeParsers.put(IntegerFieldMapper.CONTENT_TYPE, new IntegerFieldMapper.TypeParser());
+        typeParsers.put(LongFieldMapper.CONTENT_TYPE, new LongFieldMapper.TypeParser());
+        typeParsers.put(FloatFieldMapper.CONTENT_TYPE, new FloatFieldMapper.TypeParser());
+        typeParsers.put(DoubleFieldMapper.CONTENT_TYPE, new DoubleFieldMapper.TypeParser());
+        typeParsers.put(BooleanFieldMapper.CONTENT_TYPE, new BooleanFieldMapper.TypeParser());
+        typeParsers.put(BinaryFieldMapper.CONTENT_TYPE, new BinaryFieldMapper.TypeParser());
+        typeParsers.put(DateFieldMapper.CONTENT_TYPE, new DateFieldMapper.TypeParser());
+        typeParsers.put(IpFieldMapper.CONTENT_TYPE, new IpFieldMapper.TypeParser());
+        typeParsers.put(StringFieldMapper.CONTENT_TYPE, new StringFieldMapper.TypeParser());
+        typeParsers.put(TokenCountFieldMapper.CONTENT_TYPE, new TokenCountFieldMapper.TypeParser());
+        typeParsers.put(ObjectMapper.CONTENT_TYPE, new ObjectMapper.TypeParser());
+        typeParsers.put(ObjectMapper.NESTED_CONTENT_TYPE, new ObjectMapper.TypeParser());
+        typeParsers.put(TypeParsers.MULTI_FIELD_CONTENT_TYPE, TypeParsers.multiFieldConverterTypeParser);
+        typeParsers.put(CompletionFieldMapper.CONTENT_TYPE, new CompletionFieldMapper.TypeParser());
+        typeParsers.put(GeoPointFieldMapper.CONTENT_TYPE, new GeoPointFieldMapper.TypeParser());
 
         if (ShapesAvailability.JTS_AVAILABLE) {
-            typeParsersBuilder.put(GeoShapeFieldMapper.CONTENT_TYPE, new GeoShapeFieldMapper.TypeParser());
+            typeParsers.put(GeoShapeFieldMapper.CONTENT_TYPE, new GeoShapeFieldMapper.TypeParser());
         }
 
-        typeParsers = typeParsersBuilder.immutableMap();
-
-        rootTypeParsers = new MapBuilder<String, Mapper.TypeParser>()
-                .put(IndexFieldMapper.NAME, new IndexFieldMapper.TypeParser())
-                .put(SourceFieldMapper.NAME, new SourceFieldMapper.TypeParser())
-                .put(TypeFieldMapper.NAME, new TypeFieldMapper.TypeParser())
-                .put(AllFieldMapper.NAME, new AllFieldMapper.TypeParser())
-                .put(ParentFieldMapper.NAME, new ParentFieldMapper.TypeParser())
-                .put(RoutingFieldMapper.NAME, new RoutingFieldMapper.TypeParser())
-                .put(TimestampFieldMapper.NAME, new TimestampFieldMapper.TypeParser())
-                .put(TTLFieldMapper.NAME, new TTLFieldMapper.TypeParser())
-                .put(UidFieldMapper.NAME, new UidFieldMapper.TypeParser())
-                .put(VersionFieldMapper.NAME, new VersionFieldMapper.TypeParser())
-                .put(IdFieldMapper.NAME, new IdFieldMapper.TypeParser())
-                .put(FieldNamesFieldMapper.NAME, new FieldNamesFieldMapper.TypeParser())
-                .immutableMap();
+        this.typeParsers = unmodifiableMap(typeParsers);
+
+        Map<String, Mapper.TypeParser> rootTypeParsers = new HashMap<>();
+        rootTypeParsers.put(IndexFieldMapper.NAME, new IndexFieldMapper.TypeParser());
+        rootTypeParsers.put(SourceFieldMapper.NAME, new SourceFieldMapper.TypeParser());
+        rootTypeParsers.put(TypeFieldMapper.NAME, new TypeFieldMapper.TypeParser());
+        rootTypeParsers.put(AllFieldMapper.NAME, new AllFieldMapper.TypeParser());
+        rootTypeParsers.put(ParentFieldMapper.NAME, new ParentFieldMapper.TypeParser());
+        rootTypeParsers.put(RoutingFieldMapper.NAME, new RoutingFieldMapper.TypeParser());
+        rootTypeParsers.put(TimestampFieldMapper.NAME, new TimestampFieldMapper.TypeParser());
+        rootTypeParsers.put(TTLFieldMapper.NAME, new TTLFieldMapper.TypeParser());
+        rootTypeParsers.put(UidFieldMapper.NAME, new UidFieldMapper.TypeParser());
+        rootTypeParsers.put(VersionFieldMapper.NAME, new VersionFieldMapper.TypeParser());
+        rootTypeParsers.put(IdFieldMapper.NAME, new IdFieldMapper.TypeParser());
+        rootTypeParsers.put(FieldNamesFieldMapper.NAME, new FieldNamesFieldMapper.TypeParser());
+        this.rootTypeParsers = unmodifiableMap(rootTypeParsers);
         additionalRootMappers = Collections.emptySortedMap();
         indexVersionCreated = Version.indexCreated(indexSettings);
     }
 
     public void putTypeParser(String type, Mapper.TypeParser typeParser) {
         synchronized (typeParsersMutex) {
-            typeParsers = new MapBuilder<>(typeParsers)
-                    .put(type, typeParser)
-                    .immutableMap();
+            Map<String, Mapper.TypeParser> typeParsers = new HashMap<>(this.typeParsers);
+            typeParsers.put(type, typeParser);
+            this.typeParsers = unmodifiableMap(typeParsers);
         }
     }
 
     public void putRootTypeParser(String type, Mapper.TypeParser typeParser) {
         synchronized (typeParsersMutex) {
-            rootTypeParsers = new MapBuilder<>(rootTypeParsers)
-                    .put(type, typeParser)
-                    .immutableMap();
-            SortedMap<String, Mapper.TypeParser> newAdditionalRootMappers = new TreeMap<>();
-            newAdditionalRootMappers.putAll(additionalRootMappers);
-            newAdditionalRootMappers.put(type, typeParser);
-            additionalRootMappers = Collections.unmodifiableSortedMap(newAdditionalRootMappers);
+            Map<String, Mapper.TypeParser> rootTypeParsers = new HashMap<>(this.rootTypeParsers);
+            rootTypeParsers.put(type, typeParser);
+            this.rootTypeParsers = rootTypeParsers;
+            SortedMap<String, Mapper.TypeParser> additionalRootMappers = new TreeMap<>(this.additionalRootMappers);
+            additionalRootMappers.put(type, typeParser);
+            this.additionalRootMappers = unmodifiableSortedMap(additionalRootMappers);
         }
     }
 
@@ -240,11 +268,12 @@ public class DocumentMapperParser {
             }
         }
 
-        ImmutableMap<String, Object> attributes = ImmutableMap.of();
-        if (mapping.containsKey("_meta")) {
-            attributes = ImmutableMap.copyOf((Map<String, Object>) mapping.remove("_meta"));
+        Map<String, Object> meta = (Map<String, Object>) mapping.remove("_meta");
+        if (meta != null) {
+            // It may not be required to copy meta here to maintain immutability
+            // but the cost is pretty low here.
+            docBuilder.meta(unmodifiableMap(new HashMap<>(meta)));
         }
-        docBuilder.meta(attributes);
 
         checkNoRemainingFields(mapping, parserContext.indexVersionCreated(), "Root mapping definition has unsupported parameters: ");
 
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
index 256a673..bbd96f7 100755
--- a/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.mapper;
 
 import com.carrotsearch.hppc.ObjectHashSet;
-import com.google.common.collect.ImmutableMap;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.DelegatingAnalyzerWrapper;
@@ -38,7 +37,6 @@ import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
-import org.elasticsearch.common.collect.Iterators;
 import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.Queries;
@@ -64,8 +62,8 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.HashSet;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -74,7 +72,9 @@ import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.function.Function;
 import java.util.stream.Collectors;
 
+import static java.util.Collections.emptyMap;
 import static java.util.Collections.emptySet;
+import static java.util.Collections.unmodifiableMap;
 import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
 
@@ -99,7 +99,7 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
     private volatile String defaultMappingSource;
     private volatile String defaultPercolatorMappingSource;
 
-    private volatile Map<String, DocumentMapper> mappers = ImmutableMap.of();
+    private volatile Map<String, DocumentMapper> mappers = emptyMap();
 
     // A lock for mappings: modifications (put mapping) need to be performed
     // under the write lock and read operations (document parsing) need to be
@@ -119,7 +119,7 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
 
     private final List<DocumentTypeListener> typeListeners = new CopyOnWriteArrayList<>();
 
-    private volatile ImmutableMap<String, MappedFieldType> unmappedFieldTypes = ImmutableMap.of();
+    private volatile Map<String, MappedFieldType> unmappedFieldTypes = emptyMap();
 
     private volatile Set<String> parentTypes = emptySet();
 
@@ -539,24 +539,23 @@ public class MapperService extends AbstractIndexComponent implements Closeable {
      * Given a type (eg. long, string, ...), return an anonymous field mapper that can be used for search operations.
      */
     public MappedFieldType unmappedFieldType(String type) {
-        final ImmutableMap<String, MappedFieldType> unmappedFieldMappers = this.unmappedFieldTypes;
-        MappedFieldType fieldType = unmappedFieldMappers.get(type);
+        MappedFieldType fieldType = unmappedFieldTypes.get(type);
         if (fieldType == null) {
             final Mapper.TypeParser.ParserContext parserContext = documentMapperParser().parserContext(type);
             Mapper.TypeParser typeParser = parserContext.typeParser(type);
             if (typeParser == null) {
                 throw new IllegalArgumentException("No mapper found for type [" + type + "]");
             }
-            final Mapper.Builder<?, ?> builder = typeParser.parse("__anonymous_" + type, ImmutableMap.<String, Object>of(), parserContext);
+            final Mapper.Builder<?, ?> builder = typeParser.parse("__anonymous_" + type, emptyMap(), parserContext);
             final BuilderContext builderContext = new BuilderContext(indexSettings, new ContentPath(1));
             fieldType = ((FieldMapper)builder.build(builderContext)).fieldType();
 
             // There is no need to synchronize writes here. In the case of concurrent access, we could just
             // compute some mappers several times, which is not a big deal
-            this.unmappedFieldTypes = ImmutableMap.<String, MappedFieldType>builder()
-                    .putAll(unmappedFieldMappers)
-                    .put(type, fieldType)
-                    .build();
+            Map<String, MappedFieldType> newUnmappedFieldTypes = new HashMap<>();
+            newUnmappedFieldTypes.putAll(unmappedFieldTypes);
+            newUnmappedFieldTypes.put(type, fieldType);
+            unmappedFieldTypes = unmodifiableMap(newUnmappedFieldTypes);
         }
         return fieldType;
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java b/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
index c3b22c6..6eeb520 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/Mapping.java
@@ -19,10 +19,7 @@
 
 package org.elasticsearch.index.mapper;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.Version;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -31,9 +28,13 @@ import org.elasticsearch.index.mapper.object.RootObjectMapper;
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Comparator;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Wrapper around everything that defines a mapping, without references to
  * utility classes like MapperService, ...
@@ -57,20 +58,20 @@ public final class Mapping implements ToXContent {
     final Version indexCreated;
     final RootObjectMapper root;
     final MetadataFieldMapper[] metadataMappers;
-    final ImmutableMap<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> rootMappersMap;
+    final Map<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> rootMappersMap;
     final SourceTransform[] sourceTransforms;
-    volatile ImmutableMap<String, Object> meta;
+    volatile Map<String, Object> meta;
 
-    public Mapping(Version indexCreated, RootObjectMapper rootObjectMapper, MetadataFieldMapper[] metadataMappers, SourceTransform[] sourceTransforms, ImmutableMap<String, Object> meta) {
+    public Mapping(Version indexCreated, RootObjectMapper rootObjectMapper, MetadataFieldMapper[] metadataMappers, SourceTransform[] sourceTransforms, Map<String, Object> meta) {
         this.indexCreated = indexCreated;
         this.root = rootObjectMapper;
         this.metadataMappers = metadataMappers;
-        ImmutableMap.Builder<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> builder = ImmutableMap.builder();
+        Map<Class<? extends MetadataFieldMapper>, MetadataFieldMapper> rootMappersMap = new HashMap<>();
         for (MetadataFieldMapper metadataMapper : metadataMappers) {
             if (indexCreated.before(Version.V_2_0_0_beta1) && LEGACY_INCLUDE_IN_OBJECT.contains(metadataMapper.name())) {
                 root.putMapper(metadataMapper);
             }
-            builder.put(metadataMapper.getClass(), metadataMapper);
+            rootMappersMap.put(metadataMapper.getClass(), metadataMapper);
         }
         // keep root mappers sorted for consistent serialization
         Arrays.sort(metadataMappers, new Comparator<Mapper>() {
@@ -79,7 +80,7 @@ public final class Mapping implements ToXContent {
                 return o1.name().compareTo(o2.name());
             }
         });
-        this.rootMappersMap = builder.build();
+        this.rootMappersMap = unmodifiableMap(rootMappersMap);
         this.sourceTransforms = sourceTransforms;
         this.meta = meta;
     }
@@ -119,7 +120,7 @@ public final class Mapping implements ToXContent {
             meta = mergeWith.meta;
         }
     }
-    
+
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         root.toXContent(builder, params, new ToXContent() {
@@ -150,22 +151,11 @@ public final class Mapping implements ToXContent {
         return builder;
     }
 
-    /** Serialize to a {@link BytesReference}. */
-    public BytesReference toBytes() {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder().startObject();
-            toXContent(builder, new ToXContent.MapParams(ImmutableMap.<String, String>of()));
-            return builder.endObject().bytes();
-        } catch (IOException bogus) {
-            throw new AssertionError(bogus);
-        }
-    }
-
     @Override
     public String toString() {
         try {
             XContentBuilder builder = XContentFactory.jsonBuilder().startObject();
-            toXContent(builder, new ToXContent.MapParams(ImmutableMap.<String, String>of()));
+            toXContent(builder, new ToXContent.MapParams(emptyMap()));
             return builder.endObject().string();
         } catch (IOException bogus) {
             throw new AssertionError(bogus);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java
index 78d0385..7468f4f 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java
@@ -79,7 +79,6 @@ public class BinaryFieldMapper extends FieldMapper {
         @Override
         public BinaryFieldMapper build(BuilderContext context) {
             setupFieldType(context);
-            ((BinaryFieldType)fieldType).setTryUncompressing(context.indexCreatedVersion().before(Version.V_2_0_0_beta1));
             return new BinaryFieldMapper(name, fieldType, defaultFieldType,
                     context.indexSettings(), multiFieldsBuilder.build(this, context), copyTo);
         }
@@ -103,13 +102,11 @@ public class BinaryFieldMapper extends FieldMapper {
     }
 
     static final class BinaryFieldType extends MappedFieldType {
-        private boolean tryUncompressing = false;
 
         public BinaryFieldType() {}
 
         protected BinaryFieldType(BinaryFieldType ref) {
             super(ref);
-            this.tryUncompressing = ref.tryUncompressing;
         }
 
         @Override
@@ -117,40 +114,12 @@ public class BinaryFieldMapper extends FieldMapper {
             return new BinaryFieldType(this);
         }
 
-        @Override
-        public boolean equals(Object o) {
-            if (!super.equals(o)) return false;
-            BinaryFieldType that = (BinaryFieldType) o;
-            return Objects.equals(tryUncompressing, that.tryUncompressing);
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(super.hashCode(), tryUncompressing);
-        }
 
         @Override
         public String typeName() {
             return CONTENT_TYPE;
         }
 
-        @Override
-        public void checkCompatibility(MappedFieldType fieldType, List<String> conflicts, boolean strict) {
-            super.checkCompatibility(fieldType, conflicts, strict);
-            BinaryFieldType other = (BinaryFieldType)fieldType;
-            if (tryUncompressing() != other.tryUncompressing()) {
-                conflicts.add("mapper [" + names().fullName() + "] has different [try_uncompressing] (IMPOSSIBLE)");
-            }
-        }
-
-        public boolean tryUncompressing() {
-            return tryUncompressing;
-        }
-
-        public void setTryUncompressing(boolean tryUncompressing) {
-            checkIfFrozen();
-            this.tryUncompressing = tryUncompressing;
-        }
 
         @Override
         public BytesReference value(Object value) {
@@ -172,15 +141,7 @@ public class BinaryFieldMapper extends FieldMapper {
                     throw new ElasticsearchParseException("failed to convert bytes", e);
                 }
             }
-            try {
-                if (tryUncompressing) { // backcompat behavior
-                    return CompressorFactory.uncompressIfNeeded(bytes);
-                } else {
-                    return bytes;
-                }
-            } catch (IOException e) {
-                throw new ElasticsearchParseException("failed to decompress source", e);
-            }
+            return bytes;
         }
 
         @Override
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java
index 7f06c22..0e512bf 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.index.mapper.core;
 
-import com.carrotsearch.hppc.DoubleArrayList;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Field;
@@ -36,8 +34,6 @@ import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.Numbers;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.util.ByteUtils;
-import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
@@ -286,17 +282,7 @@ public class DoubleFieldMapper extends NumberFieldMapper {
             fields.add(field);
         }
         if (fieldType().hasDocValues()) {
-            if (useSortedNumericDocValues) {
-                addDocValue(context, fields, doubleToSortableLong(value));
-            } else {
-                CustomDoubleNumericDocValuesField field = (CustomDoubleNumericDocValuesField) context.doc().getByKey(fieldType().names().indexName());
-                if (field != null) {
-                    field.add(value);
-                } else {
-                    field = new CustomDoubleNumericDocValuesField(fieldType().names().indexName(), value);
-                    context.doc().addWithKey(fieldType().names().indexName(), field);
-                }
-            }
+            addDocValue(context, fields, doubleToSortableLong(value));
         }
     }
 
@@ -346,30 +332,4 @@ public class DoubleFieldMapper extends NumberFieldMapper {
         }
     }
 
-    public static class CustomDoubleNumericDocValuesField extends CustomNumericDocValuesField {
-
-        private final DoubleArrayList values;
-
-        public CustomDoubleNumericDocValuesField(String  name, double value) {
-            super(name);
-            values = new DoubleArrayList();
-            add(value);
-        }
-
-        public void add(double value) {
-            values.add(value);
-        }
-
-        @Override
-        public BytesRef binaryValue() {
-            CollectionUtils.sortAndDedup(values);
-
-            final byte[] bytes = new byte[values.size() * 8];
-            for (int i = 0; i < values.size(); ++i) {
-                ByteUtils.writeDoubleLE(values.get(i), bytes, i * 8);
-            }
-            return new BytesRef(bytes);
-        }
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java
index caeb2d7..9a607ff 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.index.mapper.core;
 
-import com.carrotsearch.hppc.FloatArrayList;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Field;
@@ -37,8 +35,6 @@ import org.elasticsearch.common.Numbers;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.util.ByteUtils;
-import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
@@ -298,17 +294,7 @@ public class FloatFieldMapper extends NumberFieldMapper {
             fields.add(field);
         }
         if (fieldType().hasDocValues()) {
-            if (useSortedNumericDocValues) {
-                addDocValue(context, fields, floatToSortableInt(value));
-            } else {
-                CustomFloatNumericDocValuesField field = (CustomFloatNumericDocValuesField) context.doc().getByKey(fieldType().names().indexName());
-                if (field != null) {
-                    field.add(value);
-                } else {
-                    field = new CustomFloatNumericDocValuesField(fieldType().names().indexName(), value);
-                    context.doc().addWithKey(fieldType().names().indexName(), field);
-                }
-            }
+            addDocValue(context, fields, floatToSortableInt(value));
         }
     }
 
@@ -357,31 +343,4 @@ public class FloatFieldMapper extends NumberFieldMapper {
             return Float.toString(number);
         }
     }
-
-    public static class CustomFloatNumericDocValuesField extends CustomNumericDocValuesField {
-
-        private final FloatArrayList values;
-
-        public CustomFloatNumericDocValuesField(String  name, float value) {
-            super(name);
-            values = new FloatArrayList();
-            add(value);
-        }
-
-        public void add(float value) {
-            values.add(value);
-        }
-
-        @Override
-        public BytesRef binaryValue() {
-            CollectionUtils.sortAndDedup(values);
-
-            final byte[] bytes = new byte[values.size() * 4];
-            for (int i = 0; i < values.size(); ++i) {
-                ByteUtils.writeFloatLE(values.get(i), bytes, i * 4);
-            }
-            return new BytesRef(bytes);
-        }
-
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
index 78406c2..3fba511 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper.core;
 
-import com.carrotsearch.hppc.LongArrayList;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.NumericTokenStream;
 import org.apache.lucene.analysis.TokenStream;
@@ -31,14 +30,10 @@ import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.IndexableFieldType;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.Version;
 import org.elasticsearch.common.Explicit;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.util.ByteUtils;
-import org.elasticsearch.common.util.CollectionUtils;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.mapper.*;
@@ -170,21 +165,12 @@ public abstract class NumberFieldMapper extends FieldMapper implements AllFieldM
 
     protected Explicit<Boolean> coerce;
     
-    /** 
-     * True if index version is 1.4+
-     * <p>
-     * In this case numerics are encoded with SORTED_NUMERIC docvalues,
-     * otherwise for older indexes we must continue to write BINARY (for now)
-     */
-    protected final boolean useSortedNumericDocValues;
-
     protected NumberFieldMapper(String simpleName, MappedFieldType fieldType, MappedFieldType defaultFieldType,
                                 Explicit<Boolean> ignoreMalformed, Explicit<Boolean> coerce, Settings indexSettings,
                                 MultiFields multiFields, CopyTo copyTo) {
         super(simpleName, fieldType, defaultFieldType, indexSettings, multiFields, copyTo);
         this.ignoreMalformed = ignoreMalformed;
         this.coerce = coerce;
-        this.useSortedNumericDocValues = Version.indexCreated(indexSettings).onOrAfter(Version.V_1_4_0_Beta1);
     }
 
     @Override
@@ -225,17 +211,7 @@ public abstract class NumberFieldMapper extends FieldMapper implements AllFieldM
     protected abstract void innerParseCreateField(ParseContext context, List<Field> fields) throws IOException;
 
     protected final void addDocValue(ParseContext context, List<Field> fields, long value) {
-        if (useSortedNumericDocValues) {
-            fields.add(new SortedNumericDocValuesField(fieldType().names().indexName(), value));
-        } else {
-            CustomLongNumericDocValuesField field = (CustomLongNumericDocValuesField) context.doc().getByKey(fieldType().names().indexName());
-            if (field != null) {
-                field.add(value);
-            } else {
-                field = new CustomLongNumericDocValuesField(fieldType().names().indexName(), value);
-                context.doc().addWithKey(fieldType().names().indexName(), field);
-            }
-        }
+        fields.add(new SortedNumericDocValuesField(fieldType().names().indexName(), value));
     }
 
     /**
@@ -414,40 +390,6 @@ public abstract class NumberFieldMapper extends FieldMapper implements AllFieldM
 
     }
 
-
-    public static class CustomLongNumericDocValuesField extends CustomNumericDocValuesField {
-
-        private final LongArrayList values;
-
-        public CustomLongNumericDocValuesField(String  name, long value) {
-            super(name);
-            values = new LongArrayList();
-            add(value);
-        }
-
-        public void add(long value) {
-            values.add(value);
-        }
-
-        @Override
-        public BytesRef binaryValue() {
-            CollectionUtils.sortAndDedup(values);
-
-            // here is the trick:
-            //  - the first value is zig-zag encoded so that eg. -5 would become positive and would be better compressed by vLong
-            //  - for other values, we only encode deltas using vLong
-            final byte[] bytes = new byte[values.size() * ByteUtils.MAX_BYTES_VLONG];
-            final ByteArrayDataOutput out = new ByteArrayDataOutput(bytes);
-            ByteUtils.writeVLong(out, ByteUtils.zigZagEncode(values.get(0)));
-            for (int i = 1; i < values.size(); ++i) {
-                final long delta = values.get(i) - values.get(i - 1);
-                ByteUtils.writeVLong(out, delta);
-            }
-            return new BytesRef(bytes, 0, out.getPosition());
-        }
-
-    }
-
     @Override
     protected void doXContentBody(XContentBuilder builder, boolean includeDefaults, Params params) throws IOException {
         super.doXContentBody(builder, includeDefaults, params);
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
index 3339c97..1de8db2 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
@@ -30,12 +30,7 @@ import org.elasticsearch.index.mapper.Uid;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.Objects;
-import java.util.Set;
+import java.util.*;
 
 /**
  * A query that will return only documents matching specific ids (and a type).
@@ -133,14 +128,14 @@ public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
 
     @Override
     protected IdsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readOptionalStringArray());
+        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readStringArray());
         idsQueryBuilder.addIds(in.readStringArray());
         return idsQueryBuilder;
     }
 
     @Override
     protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeOptionalStringArray(types);
+        out.writeStringArray(types);
         out.writeStringArray(ids.toArray(new String[ids.size()]));
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
index 5be406e..bbd9f84 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
@@ -149,31 +149,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return this.queryStringLenient;
     }
 
-    public IndicesQueriesRegistry indicesQueriesRegistry() {
+    IndicesQueriesRegistry indicesQueriesRegistry() {
         return indicesQueriesRegistry;
     }
 
-    public ParsedQuery parse(QueryBuilder<?> queryBuilder) {
-        QueryShardContext context = cache.get();
-        context.reset();
-        context.parseFieldMatcher(parseFieldMatcher);
-        try {
-            return innerParse(context, queryBuilder);
-        } catch (ParsingException e) {
-            throw e;
-        } catch (Exception e) {
-            throw new QueryShardException(context, "failed to create query: {}", e, queryBuilder);
-        }
-    }
-
-    private static ParsedQuery innerParse(QueryShardContext context, QueryBuilder<?> queryBuilder) throws IOException, QueryShardException {
-        Query query = queryBuilder.toQuery(context);
-        if (query == null) {
-            query = Queries.newMatchNoDocsQuery();
-        }
-        return new ParsedQuery(query, context.copyNamedQueries());
-    }
-
     public ParsedQuery parse(BytesReference source) {
         QueryShardContext context = cache.get();
         XContentParser parser = null;
@@ -279,7 +258,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
 
     public Query parseInnerQuery(QueryShardContext context) throws IOException {
         return toQuery(context.parseContext().parseInnerQueryBuilder(), context);
-            }
+    }
 
     public ParsedQuery toQuery(QueryBuilder<?> queryBuilder) {
         QueryShardContext context = cache.get();
diff --git a/core/src/main/java/org/elasticsearch/index/query/ParsedQuery.java b/core/src/main/java/org/elasticsearch/index/query/ParsedQuery.java
index d1a4ca5..1c21926 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ParsedQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ParsedQuery.java
@@ -19,22 +19,31 @@
 
 package org.elasticsearch.index.query;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.lucene.search.Queries;
 
+import java.util.Map;
+
+import static java.util.Collections.emptyMap;
+
 /**
  * The result of parsing a query.
- *
- *
  */
 public class ParsedQuery {
-
     private final Query query;
-    private final ImmutableMap<String, Query> namedFilters;
+    private final Map<String, Query> namedFilters;
 
-    public ParsedQuery(Query query, ImmutableMap<String, Query> namedFilters) {
+    /**
+     * Store the query and filters.
+     *
+     * @param query
+     *            the query
+     * @param namedFilters
+     *            an immutable Map containing the named filters. Good callers
+     *            use emptyMap or unmodifiableMap and copy the source to make
+     *            sure this is immutable.
+     */
+    public ParsedQuery(Query query, Map<String, Query> namedFilters) {
         this.query = query;
         this.namedFilters = namedFilters;
     }
@@ -46,7 +55,7 @@ public class ParsedQuery {
 
     public ParsedQuery(Query query) {
         this.query = query;
-        this.namedFilters = ImmutableMap.of();
+        this.namedFilters = emptyMap();
     }
 
     /**
@@ -56,11 +65,11 @@ public class ParsedQuery {
         return this.query;
     }
 
-    public ImmutableMap<String, Query> namedFilters() {
-        return this.namedFilters;
+    public Map<String, Query> namedFilters() {
+        return namedFilters;
     }
 
     public static ParsedQuery parsedMatchAllQuery() {
-        return new ParsedQuery(Queries.newMatchAllQuery(), ImmutableMap.<String, Query>of());
+        return new ParsedQuery(Queries.newMatchAllQuery(), emptyMap());
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
index 5b12b2d..e2a16df 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.queryparser.classic.MapperQueryParser;
 import org.apache.lucene.queryparser.classic.QueryParserSettings;
@@ -37,7 +36,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.mapper.*;
+import org.elasticsearch.index.mapper.ContentPath;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.Mapper;
+import org.elasticsearch.index.mapper.MapperBuilders;
+import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.support.NestedScope;
@@ -54,6 +57,8 @@ import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Context object used to create lucene queries on the shard level.
  */
@@ -186,8 +191,9 @@ public class QueryShardContext {
         }
     }
 
-    public ImmutableMap<String, Query> copyNamedQueries() {
-        return ImmutableMap.copyOf(namedQueries);
+    public Map<String, Query> copyNamedQueries() {
+        // This might be a good use case for CopyOnWriteHashMap
+        return unmodifiableMap(new HashMap<>(namedQueries));
     }
 
     public void combineNamedQueries(QueryShardContext context) {
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index 3c72adf..0df2460 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -18,25 +18,16 @@
  */
 package org.elasticsearch.index.query;
 
-import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.common.HasContextAndHeaders;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.*;
-import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.Template;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import static org.elasticsearch.common.Strings.hasLength;
-
 /**
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
@@ -99,6 +90,4 @@ public class TemplateQueryParser implements QueryParser<TemplateQueryBuilder> {
     public TemplateQueryBuilder getBuilderPrototype() {
         return TemplateQueryBuilder.PROTOTYPE;
     }
-
-
 }
diff --git a/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java b/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
index 108dab4..cfb7402 100644
--- a/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
+++ b/core/src/main/java/org/elasticsearch/index/search/stats/SearchSlowLog.java
@@ -189,11 +189,24 @@ public final class SearchSlowLog{
                 sb.append("], ");
             }
             sb.append("search_type[").append(context.searchType()).append("], total_shards[").append(context.numberOfShards()).append("], ");
-            if (context.request().source() != null) {
-                sb.append("source[").append(context.request().source()).append("], ");
+            if (context.request().source() != null && context.request().source().length() > 0) {
+                try {
+                    sb.append("source[").append(XContentHelper.convertToJson(context.request().source(), reformat)).append("], ");
+                } catch (IOException e) {
+                    sb.append("source[_failed_to_convert_], ");
+                }
             } else {
                 sb.append("source[], ");
             }
+            if (context.request().extraSource() != null && context.request().extraSource().length() > 0) {
+                try {
+                    sb.append("extra_source[").append(XContentHelper.convertToJson(context.request().extraSource(), reformat)).append("], ");
+                } catch (IOException e) {
+                    sb.append("extra_source[_failed_to_convert_], ");
+                }
+            } else {
+                sb.append("extra_source[], ");
+            }
             return sb.toString();
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java b/core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java
index 3ef5652..829db9d 100644
--- a/core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java
+++ b/core/src/main/java/org/elasticsearch/index/search/stats/ShardSearchStats.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.search.stats;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.metrics.CounterMetric;
 import org.elasticsearch.common.metrics.MeanMetric;
@@ -31,6 +30,8 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
 
+import static java.util.Collections.emptyMap;
+
 /**
  */
 public final class ShardSearchStats {
@@ -38,7 +39,7 @@ public final class ShardSearchStats {
     private final SearchSlowLog slowLogSearchService;
     private final StatsHolder totalStats = new StatsHolder();
     private final CounterMetric openContexts = new CounterMetric();
-    private volatile Map<String, StatsHolder> groupsStats = ImmutableMap.of();
+    private volatile Map<String, StatsHolder> groupsStats = emptyMap();
 
     public ShardSearchStats(Settings indexSettings) {
         this.slowLogSearchService = new SearchSlowLog(indexSettings);
diff --git a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
index 2497231..2ff8c37 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java
@@ -55,7 +55,6 @@ import org.elasticsearch.common.util.concurrent.FutureUtils;
 import org.elasticsearch.gateway.MetaDataStateFormat;
 import org.elasticsearch.index.IndexServicesProvider;
 import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.aliases.IndexAliasesService;
 import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.cache.IndexCacheModule;
 import org.elasticsearch.index.cache.bitset.ShardBitsetFilterCache;
@@ -124,12 +123,10 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
 
     private final ThreadPool threadPool;
     private final MapperService mapperService;
-    private final IndexQueryParserService queryParserService;
     private final IndexCache indexCache;
     private final InternalIndicesLifecycle indicesLifecycle;
     private final Store store;
     private final MergeSchedulerConfig mergeSchedulerConfig;
-    private final IndexAliasesService indexAliasesService;
     private final ShardIndexingService indexingService;
     private final ShardSearchStats searchService;
     private final ShardGetService getService;
@@ -211,11 +208,9 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.indicesLifecycle = (InternalIndicesLifecycle) provider.getIndicesLifecycle();
         this.store = store;
         this.mergeSchedulerConfig = new MergeSchedulerConfig(indexSettings);
-        this.threadPool =  provider.getThreadPool();
-        this.mapperService =  provider.getMapperService();
-        this.queryParserService =  provider.getQueryParserService();
-        this.indexCache =  provider.getIndexCache();
-        this.indexAliasesService =  provider.getIndexAliasesService();
+        this.threadPool = provider.getThreadPool();
+        this.mapperService = provider.getMapperService();
+        this.indexCache = provider.getIndexCache();
         this.indexingService = new ShardIndexingService(shardId, indexSettings);
         this.getService = new ShardGetService(this, mapperService);
         this.termVectorsService =  provider.getTermVectorsService();
@@ -254,7 +249,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
         this.indexingMemoryController = provider.getIndexingMemoryController();
 
         this.searcherWrapper = provider.getIndexSearcherWrapper();
-        this.percolatorQueriesRegistry = new PercolatorQueriesRegistry(shardId, indexSettings, queryParserService, indexingService, mapperService, indexFieldDataService);
+        this.percolatorQueriesRegistry = new PercolatorQueriesRegistry(shardId, indexSettings, provider.getQueryParserService(), indexingService, mapperService, indexFieldDataService);
         if (mapperService.hasMapping(PercolatorService.TYPE_NAME)) {
             percolatorQueriesRegistry.enableRealTimePercolator();
         }
@@ -1442,8 +1437,7 @@ public class IndexShard extends AbstractIndexShardComponent implements IndexSett
     }
 
     private final EngineConfig newEngineConfig(TranslogConfig translogConfig, QueryCachingPolicy cachingPolicy) {
-        final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(shardId, mapperService, queryParserService,
-                indexAliasesService, indexCache, logger) {
+        final TranslogRecoveryPerformer translogRecoveryPerformer = new TranslogRecoveryPerformer(shardId, mapperService, logger) {
             @Override
             protected void operationProcessed() {
                 assert recoveryState != null;
diff --git a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
index 23551df..68c552d 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
@@ -18,27 +18,13 @@
  */
 package org.elasticsearch.index.shard;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.BitSetProducer;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParsingException;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.aliases.IndexAliasesService;
-import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.IgnoreOnRecoveryEngineException;
 import org.elasticsearch.index.mapper.*;
-import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.translog.Translog;
 
 import java.io.IOException;
@@ -53,20 +39,13 @@ import static org.elasticsearch.index.mapper.SourceToParse.source;
  */
 public class TranslogRecoveryPerformer {
     private final MapperService mapperService;
-    private final IndexQueryParserService queryParserService;
-    private final IndexAliasesService indexAliasesService;
-    private final IndexCache indexCache;
     private final ESLogger logger;
     private final Map<String, Mapping> recoveredTypes = new HashMap<>();
     private final ShardId shardId;
 
-    protected TranslogRecoveryPerformer(ShardId shardId, MapperService mapperService, IndexQueryParserService queryParserService,
-                                        IndexAliasesService indexAliasesService, IndexCache indexCache, ESLogger logger) {
+    protected TranslogRecoveryPerformer(ShardId shardId, MapperService mapperService, ESLogger logger) {
         this.shardId = shardId;
         this.mapperService = mapperService;
-        this.queryParserService = queryParserService;
-        this.indexAliasesService = indexAliasesService;
-        this.indexCache = indexCache;
         this.logger = logger;
     }
 
@@ -165,11 +144,6 @@ public class TranslogRecoveryPerformer {
                     engine.delete(new Engine.Delete(uid.type(), uid.id(), delete.uid(), delete.version(),
                             delete.versionType().versionTypeForReplicationAndRecovery(), Engine.Operation.Origin.RECOVERY, System.nanoTime(), false));
                     break;
-                case DELETE_BY_QUERY:
-                    Translog.DeleteByQuery deleteByQuery = (Translog.DeleteByQuery) operation;
-                    engine.delete(prepareDeleteByQuery(queryParserService, mapperService, indexAliasesService, indexCache,
-                            deleteByQuery.source(), deleteByQuery.filteringAliases(), Engine.Operation.Origin.RECOVERY, deleteByQuery.types()));
-                    break;
                 default:
                     throw new IllegalStateException("No operation defined for [" + operation + "]");
             }
@@ -194,38 +168,6 @@ public class TranslogRecoveryPerformer {
         operationProcessed();
     }
 
-    private static Engine.DeleteByQuery prepareDeleteByQuery(IndexQueryParserService queryParserService, MapperService mapperService, IndexAliasesService indexAliasesService, IndexCache indexCache, BytesReference source, @Nullable String[] filteringAliases, Engine.Operation.Origin origin, String... types) {
-        long startTime = System.nanoTime();
-        if (types == null) {
-            types = Strings.EMPTY_ARRAY;
-        }
-        Query query;
-        try {
-            query = queryParserService.parseQuery(source).query();
-        } catch (ParsingException ex) {
-            // for BWC we try to parse directly the query since pre 1.0.0.Beta2 we didn't require a top level query field
-            if (queryParserService.getIndexCreatedVersion().onOrBefore(Version.V_1_0_0_Beta2)) {
-                try {
-                    XContentParser parser = XContentHelper.createParser(source);
-                    ParsedQuery parse = queryParserService.parse(parser);
-                    query = parse.query();
-                } catch (Throwable t) {
-                    ex.addSuppressed(t);
-                    throw ex;
-                }
-            } else {
-                throw ex;
-            }
-        }
-        Query searchFilter = mapperService.searchFilter(types);
-        if (searchFilter != null) {
-            query = Queries.filtered(query, searchFilter);
-        }
-
-        Query aliasFilter = indexAliasesService.aliasFilter(filteringAliases);
-        BitSetProducer parentFilter = mapperService.hasNested() ? indexCache.bitsetFilterCache().getBitSetProducer(Queries.newNonNestedFilter()) : null;
-        return new Engine.DeleteByQuery(query, source, filteringAliases, aliasFilter, parentFilter, origin, startTime, types);
-    }
 
     /**
      * Called once for every processed operation by this recovery performer.
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
index 10ba1d4..d5caa4a 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/DFRSimilarityProvider.java
@@ -19,13 +19,29 @@
 
 package org.elasticsearch.index.similarity;
 
-import com.google.common.collect.ImmutableMap;
-import org.apache.lucene.search.similarities.*;
-import org.elasticsearch.common.collect.MapBuilder;
+import org.apache.lucene.search.similarities.AfterEffect;
+import org.apache.lucene.search.similarities.AfterEffectB;
+import org.apache.lucene.search.similarities.AfterEffectL;
+import org.apache.lucene.search.similarities.BasicModel;
+import org.apache.lucene.search.similarities.BasicModelBE;
+import org.apache.lucene.search.similarities.BasicModelD;
+import org.apache.lucene.search.similarities.BasicModelG;
+import org.apache.lucene.search.similarities.BasicModelIF;
+import org.apache.lucene.search.similarities.BasicModelIn;
+import org.apache.lucene.search.similarities.BasicModelIne;
+import org.apache.lucene.search.similarities.BasicModelP;
+import org.apache.lucene.search.similarities.DFRSimilarity;
+import org.apache.lucene.search.similarities.Normalization;
+import org.apache.lucene.search.similarities.Similarity;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
 
+import java.util.HashMap;
+import java.util.Map;
+
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * {@link SimilarityProvider} for {@link DFRSimilarity}.
  * <p>
@@ -38,12 +54,11 @@ import org.elasticsearch.common.settings.Settings;
  * @see DFRSimilarity For more information about configuration
  */
 public class DFRSimilarityProvider extends AbstractSimilarityProvider {
-
-    private static final ImmutableMap<String, BasicModel> MODEL_CACHE;
-    private static final ImmutableMap<String, AfterEffect> EFFECT_CACHE;
+    private static final Map<String, BasicModel> MODEL_CACHE;
+    private static final Map<String, AfterEffect> EFFECT_CACHE;
 
     static {
-        MapBuilder<String, BasicModel> models = MapBuilder.newMapBuilder();
+        Map<String, BasicModel> models = new HashMap<>();
         models.put("be", new BasicModelBE());
         models.put("d", new BasicModelD());
         models.put("g", new BasicModelG());
@@ -51,13 +66,13 @@ public class DFRSimilarityProvider extends AbstractSimilarityProvider {
         models.put("in", new BasicModelIn());
         models.put("ine", new BasicModelIne());
         models.put("p", new BasicModelP());
-        MODEL_CACHE = models.immutableMap();
+        MODEL_CACHE = unmodifiableMap(models);
 
-        MapBuilder<String, AfterEffect> effects = MapBuilder.newMapBuilder();
+        Map<String, AfterEffect> effects = new HashMap<>();
         effects.put("no", new AfterEffect.NoAfterEffect());
         effects.put("b", new AfterEffectB());
         effects.put("l", new AfterEffectL());
-        EFFECT_CACHE = effects.immutableMap();
+        EFFECT_CACHE = unmodifiableMap(effects);
     }
 
     private final DFRSimilarity similarity;
diff --git a/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java b/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
index eb8d20a..4b83bc8 100644
--- a/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
+++ b/core/src/main/java/org/elasticsearch/index/similarity/IBSimilarityProvider.java
@@ -19,11 +19,22 @@
 
 package org.elasticsearch.index.similarity;
 
-import com.google.common.collect.ImmutableMap;
-import org.apache.lucene.search.similarities.*;
-import org.elasticsearch.common.collect.MapBuilder;
+import org.apache.lucene.search.similarities.Distribution;
+import org.apache.lucene.search.similarities.DistributionLL;
+import org.apache.lucene.search.similarities.DistributionSPL;
+import org.apache.lucene.search.similarities.IBSimilarity;
+import org.apache.lucene.search.similarities.Lambda;
+import org.apache.lucene.search.similarities.LambdaDF;
+import org.apache.lucene.search.similarities.LambdaTTF;
+import org.apache.lucene.search.similarities.Normalization;
+import org.apache.lucene.search.similarities.Similarity;
 import org.elasticsearch.common.settings.Settings;
 
+import java.util.HashMap;
+import java.util.Map;
+
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * {@link SimilarityProvider} for {@link IBSimilarity}.
  * <p>
@@ -37,19 +48,19 @@ import org.elasticsearch.common.settings.Settings;
  */
 public class IBSimilarityProvider extends AbstractSimilarityProvider {
 
-    private static final ImmutableMap<String, Distribution> DISTRIBUTION_CACHE;
-    private static final ImmutableMap<String, Lambda> LAMBDA_CACHE;
+    private static final Map<String, Distribution> DISTRIBUTIONS;
+    private static final Map<String, Lambda> LAMBDAS;
 
     static {
-        MapBuilder<String, Distribution> distributions = MapBuilder.newMapBuilder();
+        Map<String, Distribution> distributions = new HashMap<>();
         distributions.put("ll", new DistributionLL());
         distributions.put("spl", new DistributionSPL());
-        DISTRIBUTION_CACHE = distributions.immutableMap();
+        DISTRIBUTIONS = unmodifiableMap(distributions);
 
-        MapBuilder<String, Lambda> lamdas = MapBuilder.newMapBuilder();
+        Map<String, Lambda> lamdas = new HashMap<>();
         lamdas.put("df", new LambdaDF());
         lamdas.put("ttf", new LambdaTTF());
-        LAMBDA_CACHE = lamdas.immutableMap();
+        LAMBDAS = unmodifiableMap(lamdas);
     }
 
     private final IBSimilarity similarity;
@@ -70,7 +81,7 @@ public class IBSimilarityProvider extends AbstractSimilarityProvider {
      */
     protected Distribution parseDistribution(Settings settings) {
         String rawDistribution = settings.get("distribution");
-        Distribution distribution = DISTRIBUTION_CACHE.get(rawDistribution);
+        Distribution distribution = DISTRIBUTIONS.get(rawDistribution);
         if (distribution == null) {
             throw new IllegalArgumentException("Unsupported Distribution [" + rawDistribution + "]");
         }
@@ -85,7 +96,7 @@ public class IBSimilarityProvider extends AbstractSimilarityProvider {
      */
     protected Lambda parseLambda(Settings settings) {
         String rawLambda = settings.get("lambda");
-        Lambda lambda = LAMBDA_CACHE.get(rawLambda);
+        Lambda lambda = LAMBDAS.get(rawLambda);
         if (lambda == null) {
             throw new IllegalArgumentException("Unsupported Lambda [" + rawLambda + "]");
         }
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
index 091985e..d90a869 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java
@@ -19,7 +19,11 @@
 
 package org.elasticsearch.index.snapshots.blobstore;
 
-import org.apache.lucene.index.*;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexCommit;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfos;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
@@ -48,7 +52,11 @@ import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.snapshots.*;
+import org.elasticsearch.index.snapshots.IndexShardRepository;
+import org.elasticsearch.index.snapshots.IndexShardRestoreFailedException;
+import org.elasticsearch.index.snapshots.IndexShardSnapshotException;
+import org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException;
+import org.elasticsearch.index.snapshots.IndexShardSnapshotStatus;
 import org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.FileInfo;
 import org.elasticsearch.index.store.Store;
 import org.elasticsearch.index.store.StoreFileMetaData;
@@ -64,8 +72,15 @@ import org.elasticsearch.repositories.blobstore.LegacyBlobStoreFormat;
 import java.io.FilterInputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.util.*;
-
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.repositories.blobstore.BlobStoreRepository.testBlobPrefix;
 
 /**
@@ -812,7 +827,7 @@ public class BlobStoreIndexShardRepository extends AbstractComponent implements
                     snapshotMetaData.put(fileInfo.metadata().name(), fileInfo.metadata());
                     fileInfos.put(fileInfo.metadata().name(), fileInfo);
                 }
-                final Store.MetadataSnapshot sourceMetaData = new Store.MetadataSnapshot(snapshotMetaData, Collections.EMPTY_MAP, 0);
+                final Store.MetadataSnapshot sourceMetaData = new Store.MetadataSnapshot(unmodifiableMap(snapshotMetaData), emptyMap(), 0);
                 final Store.RecoveryDiff diff = sourceMetaData.recoveryDiff(recoveryTargetMetadata);
                 for (StoreFileMetaData md : diff.identical) {
                     FileInfo fileInfo = fileInfos.get(md.name());
diff --git a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
index ad3b9c9..8f05572 100644
--- a/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
+++ b/core/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.snapshots.blobstore;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -38,6 +37,8 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Contains information about all snapshot for the given shard in repository
  * <p>
@@ -77,15 +78,15 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
                 physicalFileList.add(newFiles.get(fileInfo.name()));
             }
         }
-        ImmutableMap.Builder<String, List<FileInfo>> mapBuilder = ImmutableMap.builder();
+        Map<String, List<FileInfo>> mapBuilder = new HashMap<>();
         for (Map.Entry<String, List<FileInfo>> entry : physicalFiles.entrySet()) {
             mapBuilder.put(entry.getKey(), Collections.unmodifiableList(new ArrayList<>(entry.getValue())));
         }
-        this.physicalFiles = mapBuilder.build();
-        this.files = ImmutableMap.copyOf(newFiles);
+        this.physicalFiles = unmodifiableMap(mapBuilder);
+        this.files = unmodifiableMap(newFiles);
     }
 
-    private BlobStoreIndexShardSnapshots(ImmutableMap<String, FileInfo> files, List<SnapshotFiles> shardSnapshots) {
+    private BlobStoreIndexShardSnapshots(Map<String, FileInfo> files, List<SnapshotFiles> shardSnapshots) {
         this.shardSnapshots = shardSnapshots;
         this.files = files;
         Map<String, List<FileInfo>> physicalFiles = new HashMap<>();
@@ -99,11 +100,11 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
                 physicalFileList.add(files.get(fileInfo.name()));
             }
         }
-        ImmutableMap.Builder<String, List<FileInfo>> mapBuilder = ImmutableMap.builder();
+        Map<String, List<FileInfo>> mapBuilder = new HashMap<>();
         for (Map.Entry<String, List<FileInfo>> entry : physicalFiles.entrySet()) {
             mapBuilder.put(entry.getKey(), Collections.unmodifiableList(new ArrayList<>(entry.getValue())));
         }
-        this.physicalFiles = mapBuilder.build();
+        this.physicalFiles = unmodifiableMap(mapBuilder);
     }
 
     private BlobStoreIndexShardSnapshots() {
@@ -232,13 +233,14 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
         return builder;
     }
 
+    @Override
     public BlobStoreIndexShardSnapshots fromXContent(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException {
         XContentParser.Token token = parser.currentToken();
         if (token == null) { // New parser
             token = parser.nextToken();
         }
         Map<String, List<String>> snapshotsMap = new HashMap<>();
-        ImmutableMap.Builder<String, FileInfo> filesBuilder = ImmutableMap.builder();
+        Map<String, FileInfo> files = new HashMap<>();
         if (token == XContentParser.Token.START_OBJECT) {
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                 if (token != XContentParser.Token.FIELD_NAME) {
@@ -252,7 +254,7 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
                     }
                     while (parser.nextToken() != XContentParser.Token.END_ARRAY) {
                         FileInfo fileInfo = FileInfo.fromXContent(parser);
-                        filesBuilder.put(fileInfo.name(), fileInfo);
+                        files.put(fileInfo.name(), fileInfo);
                     }
                 } else if (token == XContentParser.Token.START_OBJECT) {
                     if (parseFieldMatcher.match(currentFieldName, ParseFields.SNAPSHOTS) == false) {
@@ -288,7 +290,6 @@ public class BlobStoreIndexShardSnapshots implements Iterable<SnapshotFiles>, To
             }
         }
 
-        ImmutableMap<String, FileInfo> files = filesBuilder.build();
         List<SnapshotFiles> snapshots = new ArrayList<>();
         for (Map.Entry<String, List<String>> entry : snapshotsMap.entrySet()) {
             List<FileInfo> fileInfosBuilder = new ArrayList<>();
diff --git a/core/src/main/java/org/elasticsearch/index/store/Store.java b/core/src/main/java/org/elasticsearch/index/store/Store.java
index 7fb1b40..c2b55ac 100644
--- a/core/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/core/src/main/java/org/elasticsearch/index/store/Store.java
@@ -19,14 +19,34 @@
 
 package org.elasticsearch.index.store;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.*;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.*;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexCommit;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.IndexNotFoundException;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.SegmentCommitInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.BufferedChecksum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FilterDirectory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.Lock;
+import org.apache.lucene.store.SimpleFSDirectory;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.Version;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
-import org.apache.lucene.util.Version;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.collect.Tuple;
@@ -44,26 +64,39 @@ import org.elasticsearch.common.lucene.store.InputStreamIndexInput;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.Callback;
-import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.common.util.SingleObjectCache;
 import org.elasticsearch.common.util.concurrent.AbstractRefCounted;
 import org.elasticsearch.common.util.concurrent.RefCounted;
+import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.env.ShardLock;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.AbstractIndexShardComponent;
 import org.elasticsearch.index.shard.ShardId;
 
-import java.io.*;
+import java.io.Closeable;
+import java.io.EOFException;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.InputStream;
 import java.nio.file.NoSuchFileException;
 import java.nio.file.Path;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.zip.Adler32;
 import java.util.zip.CRC32;
 import java.util.zip.Checksum;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * A Store provides plain access to files written by an elasticsearch index shard. Each shard
  * has a dedicated store that is uses to access Lucene's Directory which represents the lowest level
@@ -734,7 +767,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
         private static final ESLogger logger = Loggers.getLogger(MetadataSnapshot.class);
         private static final Version FIRST_LUCENE_CHECKSUM_VERSION = Version.LUCENE_4_8;
 
-        private final ImmutableMap<String, StoreFileMetaData> metadata;
+        private final Map<String, StoreFileMetaData> metadata;
 
         public static final MetadataSnapshot EMPTY = new MetadataSnapshot();
 
@@ -743,16 +776,14 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
         private final long numDocs;
 
         public MetadataSnapshot(Map<String, StoreFileMetaData> metadata, Map<String, String> commitUserData, long numDocs) {
-            ImmutableMap.Builder<String, StoreFileMetaData> metaDataBuilder = ImmutableMap.builder();
-            this.metadata = metaDataBuilder.putAll(metadata).build();
-            ImmutableMap.Builder<String, String> commitUserDataBuilder = ImmutableMap.builder();
-            this.commitUserData = commitUserDataBuilder.putAll(commitUserData).build();
+            this.metadata = metadata;
+            this.commitUserData = commitUserData;
             this.numDocs = numDocs;
         }
 
         MetadataSnapshot() {
-            metadata = ImmutableMap.of();
-            commitUserData = ImmutableMap.of();
+            metadata = emptyMap();
+            commitUserData = emptyMap();
             numDocs = 0;
         }
 
@@ -766,19 +797,19 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
         public MetadataSnapshot(StreamInput in) throws IOException {
             final int size = in.readVInt();
-            final ImmutableMap.Builder<String, StoreFileMetaData> metadataBuilder = ImmutableMap.builder();
+            Map<String, StoreFileMetaData> metadata = new HashMap<>();
             for (int i = 0; i < size; i++) {
                 StoreFileMetaData meta = StoreFileMetaData.readStoreFileMetaData(in);
-                metadataBuilder.put(meta.name(), meta);
+                metadata.put(meta.name(), meta);
             }
-            final ImmutableMap.Builder<String, String> commitUserDataBuilder = ImmutableMap.builder();
+            Map<String, String> commitUserData = new HashMap<>();
             int num = in.readVInt();
             for (int i = num; i > 0; i--) {
-                commitUserDataBuilder.put(in.readString(), in.readString());
+                commitUserData.put(in.readString(), in.readString());
             }
 
-            this.commitUserData = commitUserDataBuilder.build();
-            this.metadata = metadataBuilder.build();
+            this.metadata = unmodifiableMap(metadata);
+            this.commitUserData = unmodifiableMap(commitUserData);
             this.numDocs = in.readLong();
             assert metadata.isEmpty() || numSegmentFiles() == 1 : "numSegmentFiles: " + numSegmentFiles();
         }
@@ -791,11 +822,11 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
         }
 
         static class LoadedMetadata {
-            final ImmutableMap<String, StoreFileMetaData> fileMetadata;
-            final ImmutableMap<String, String> userData;
+            final Map<String, StoreFileMetaData> fileMetadata;
+            final Map<String, String> userData;
             final long numDocs;
 
-            LoadedMetadata(ImmutableMap<String, StoreFileMetaData> fileMetadata, ImmutableMap<String, String> userData, long numDocs) {
+            LoadedMetadata(Map<String, StoreFileMetaData> fileMetadata, Map<String, String> userData, long numDocs) {
                 this.fileMetadata = fileMetadata;
                 this.userData = userData;
                 this.numDocs = numDocs;
@@ -804,9 +835,9 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
         static LoadedMetadata loadMetadata(IndexCommit commit, Directory directory, ESLogger logger) throws IOException {
             long numDocs;
-            ImmutableMap.Builder<String, StoreFileMetaData> builder = ImmutableMap.builder();
+            Map<String, StoreFileMetaData> builder = new HashMap<>();
             Map<String, String> checksumMap = readLegacyChecksums(directory).v1();
-            ImmutableMap.Builder<String, String> commitUserDataBuilder = ImmutableMap.builder();
+            Map<String, String> commitUserDataBuilder = new HashMap<>();
             try {
                 final SegmentInfos segmentCommitInfos = Store.readSegmentsInfo(commit, directory);
                 numDocs = Lucene.getNumDocs(segmentCommitInfos);
@@ -863,7 +894,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
 
                 throw ex;
             }
-            return new LoadedMetadata(builder.build(), commitUserDataBuilder.build(), numDocs);
+            return new LoadedMetadata(unmodifiableMap(builder), unmodifiableMap(commitUserDataBuilder), numDocs);
         }
 
         /**
@@ -920,7 +951,8 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
             }
         }
 
-        private static void checksumFromLuceneFile(Directory directory, String file, ImmutableMap.Builder<String, StoreFileMetaData> builder, ESLogger logger, Version version, boolean readFileAsHash) throws IOException {
+        private static void checksumFromLuceneFile(Directory directory, String file, Map<String, StoreFileMetaData> builder,
+                ESLogger logger, Version version, boolean readFileAsHash) throws IOException {
             final String checksum;
             final BytesRefBuilder fileHash = new BytesRefBuilder();
             try (final IndexInput in = directory.openInput(file, IOContext.READONCE)) {
diff --git a/core/src/main/java/org/elasticsearch/index/translog/Translog.java b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
index a8f2801..4265d61 100644
--- a/core/src/main/java/org/elasticsearch/index/translog/Translog.java
+++ b/core/src/main/java/org/elasticsearch/index/translog/Translog.java
@@ -23,11 +23,9 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TwoPhaseCommit;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.CollectionUtil;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.RamUsageEstimator;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -38,7 +36,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.lease.Releasable;
 import org.elasticsearch.common.lease.Releasables;
-import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.util.BigArrays;
@@ -54,7 +51,6 @@ import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.Closeable;
 import java.io.EOFException;
-import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.channels.FileChannel;
 import java.nio.file.*;
@@ -189,99 +185,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         }
     }
 
-    /**
-     * This method is used to upgarde a pre 2.0 translog structure to the new checkpoint based structure.
-     * The {@link org.elasticsearch.index.translog.Translog.TranslogGeneration} in the given config is
-     * used to determine the smallest file generation to upgrade. The procedure will travers the translog
-     * directory to find all files that have a generation greater or equal to the translog generation and
-     * renames the files to the new <tt>.tlog</tt> file format.
-     * <p>
-     * For each of the files a <tt>${filename}.ckp</tt>
-     * file is written containing the size of the translog in bytes, it's ID and the number of operations. Since
-     * these files are all relying on the pre 2.0 truncation feature where we read operations until hitting an {@link EOFException}
-     * the number of operations are recoreded as <tt>-1</tt>. Later once these files are opened for reading legacy readers will
-     * allow for unknown number of operations and mimic the old behavior.
-     * </p>
-     */
-    public static void upgradeLegacyTranslog(ESLogger logger, TranslogConfig config) throws IOException {
-        Path translogPath = config.getTranslogPath();
-        TranslogGeneration translogGeneration = config.getTranslogGeneration();
-        if (translogGeneration == null) {
-            throw new IllegalArgumentException("TranslogGeneration must be set in order to upgrade");
-        }
-        if (translogGeneration.translogUUID != null) {
-            throw new IllegalArgumentException("TranslogGeneration has a non-null UUID - index must have already been upgraded");
-        }
-        try {
-            if (Checkpoint.read(translogPath.resolve(CHECKPOINT_FILE_NAME)) != null) {
-                throw new IllegalStateException(CHECKPOINT_FILE_NAME + " file already present, translog is already upgraded");
-            }
-        } catch (NoSuchFileException | FileNotFoundException ex) {
-            logger.debug("upgrading translog - no checkpoint found");
-        }
-        final Pattern parseLegacyIdPattern = Pattern.compile("^" + TRANSLOG_FILE_PREFIX + "(\\d+)((\\.recovering))?$"); // here we have to be lenient - nowhere else!
-        try (DirectoryStream<Path> stream = Files.newDirectoryStream(translogPath, new DirectoryStream.Filter<Path>() {
-            @Override
-            public boolean accept(Path entry) throws IOException {
-                Matcher matcher = parseLegacyIdPattern.matcher(entry.getFileName().toString());
-                if (matcher.matches() == false) {
-                    Matcher newIdMatcher = PARSE_STRICT_ID_PATTERN.matcher(entry.getFileName().toString());
-                    return newIdMatcher.matches();
-                } else {
-                    return true;
-                }
-            }
-        })) {
-            long latestGeneration = -1;
-            List<PathWithGeneration> filesToUpgrade = new ArrayList<>();
-            for (Path path : stream) {
-                Matcher matcher = parseLegacyIdPattern.matcher(path.getFileName().toString());
-                if (matcher.matches()) {
-                    long generation = Long.parseLong(matcher.group(1));
-                    if (generation >= translogGeneration.translogFileGeneration) {
-                        latestGeneration = Math.max(translogGeneration.translogFileGeneration, generation);
-                    }
-                    filesToUpgrade.add(new PathWithGeneration(path, generation));
-                } else {
-                    Matcher strict_matcher = PARSE_STRICT_ID_PATTERN.matcher(path.getFileName().toString());
-                    if (strict_matcher.matches()) {
-                        throw new IllegalStateException("non-legacy translog file [" + path.getFileName().toString() + "] found on a translog that wasn't upgraded yet");
-                    }
-                }
-            }
-            if (latestGeneration < translogGeneration.translogFileGeneration) {
-                throw new IllegalStateException("latest found translog has a lower generation that the excepcted uncommitted " + translogGeneration.translogFileGeneration + " > " + latestGeneration);
-            }
-            CollectionUtil.timSort(filesToUpgrade, new Comparator<PathWithGeneration>() {
-                @Override
-                public int compare(PathWithGeneration o1, PathWithGeneration o2) {
-                    long gen1 = o1.getGeneration();
-                    long gen2 = o2.getGeneration();
-                    return Long.compare(gen1, gen2);
-                }
-            });
-            for (PathWithGeneration pathAndGeneration : filesToUpgrade) {
-                final Path path = pathAndGeneration.getPath();
-                final long generation = pathAndGeneration.getGeneration();
-                final Path target = path.resolveSibling(getFilename(generation));
-                logger.debug("upgrading translog copy file from {} to {}", path, target);
-                Files.move(path, target, StandardCopyOption.ATOMIC_MOVE);
-                logger.debug("write commit point for {}", target);
-                if (generation == latestGeneration) {
-                    // for the last one we only write a checkpoint not a real commit
-                    Checkpoint checkpoint = new Checkpoint(Files.size(translogPath.resolve(getFilename(latestGeneration))), -1, latestGeneration);
-                    Checkpoint.write(translogPath.resolve(CHECKPOINT_FILE_NAME), checkpoint, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW);
-                } else {
-                    Checkpoint checkpoint = new Checkpoint(Files.size(target), -1, generation);
-                    Checkpoint.write(translogPath.resolve(getCommitCheckpointFileName(generation)), checkpoint, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW);
-                }
-            }
-
-            IOUtils.fsync(translogPath, true);
-
-        }
-    }
-
     /** recover all translog files found on disk */
     private ArrayList<ImmutableTranslogReader> recoverFromFiles(TranslogGeneration translogGeneration, Checkpoint checkpoint) throws IOException {
         boolean success = false;
@@ -876,8 +779,7 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
             @Deprecated
             CREATE((byte) 1),
             INDEX((byte) 2),
-            DELETE((byte) 3),
-            DELETE_BY_QUERY((byte) 4);
+            DELETE((byte) 3);
 
             private final byte id;
 
@@ -897,8 +799,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
                         return INDEX;
                     case 3:
                         return DELETE;
-                    case 4:
-                        return DELETE_BY_QUERY;
                     default:
                         throw new IllegalArgumentException("No type mapped for [" + id + "]");
                 }
@@ -1232,137 +1132,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         }
     }
 
-    /** @deprecated Delete-by-query is removed in 2.0, but we keep this so translog can replay on upgrade. */
-    @Deprecated
-    public static class DeleteByQuery implements Operation {
-
-        public static final int SERIALIZATION_FORMAT = 2;
-        private BytesReference source;
-        @Nullable
-        private String[] filteringAliases;
-        private String[] types = Strings.EMPTY_ARRAY;
-
-        public DeleteByQuery() {
-        }
-
-        public DeleteByQuery(Engine.DeleteByQuery deleteByQuery) {
-            this(deleteByQuery.source(), deleteByQuery.filteringAliases(), deleteByQuery.types());
-        }
-
-        public DeleteByQuery(BytesReference source, String[] filteringAliases, String... types) {
-            this.source = source;
-            this.types = types == null ? Strings.EMPTY_ARRAY : types;
-            this.filteringAliases = filteringAliases;
-        }
-
-        @Override
-        public Type opType() {
-            return Type.DELETE_BY_QUERY;
-        }
-
-        @Override
-        public long estimateSize() {
-            return source.length() + 8;
-        }
-
-        public BytesReference source() {
-            return this.source;
-        }
-
-        public String[] filteringAliases() {
-            return filteringAliases;
-        }
-
-        public String[] types() {
-            return this.types;
-        }
-
-        @Override
-        public Source getSource() {
-            throw new IllegalStateException("trying to read doc source from delete_by_query operation");
-        }
-
-        @Override
-        public void readFrom(StreamInput in) throws IOException {
-            int version = in.readVInt(); // version
-            source = in.readBytesReference();
-            if (version < 2) {
-                // for query_parser_name, which was removed
-                if (in.readBoolean()) {
-                    in.readString();
-                }
-            }
-            int typesSize = in.readVInt();
-            if (typesSize > 0) {
-                types = new String[typesSize];
-                for (int i = 0; i < typesSize; i++) {
-                    types[i] = in.readString();
-                }
-            }
-            if (version >= 1) {
-                int aliasesSize = in.readVInt();
-                if (aliasesSize > 0) {
-                    filteringAliases = new String[aliasesSize];
-                    for (int i = 0; i < aliasesSize; i++) {
-                        filteringAliases[i] = in.readString();
-                    }
-                }
-            }
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeVInt(SERIALIZATION_FORMAT);
-            out.writeBytesReference(source);
-            out.writeVInt(types.length);
-            for (String type : types) {
-                out.writeString(type);
-            }
-            if (filteringAliases != null) {
-                out.writeVInt(filteringAliases.length);
-                for (String alias : filteringAliases) {
-                    out.writeString(alias);
-                }
-            } else {
-                out.writeVInt(0);
-            }
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) {
-                return true;
-            }
-            if (o == null || getClass() != o.getClass()) {
-                return false;
-            }
-
-            DeleteByQuery that = (DeleteByQuery) o;
-
-            if (!Arrays.equals(filteringAliases, that.filteringAliases)) {
-                return false;
-            }
-            if (!Arrays.equals(types, that.types)) {
-                return false;
-            }
-            return source.equals(that.source);
-        }
-
-        @Override
-        public int hashCode() {
-            int result = source.hashCode();
-            result = 31 * result + (filteringAliases != null ? Arrays.hashCode(filteringAliases) : 0);
-            result = 31 * result + Arrays.hashCode(types);
-            return result;
-        }
-
-        @Override
-        public String toString() {
-            return "DeleteByQuery{" +
-                    "types=" + Arrays.toString(types) +
-                    '}';
-        }
-    }
 
     public enum Durabilty {
         /**
@@ -1478,8 +1247,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
                 return new Index();
             case DELETE:
                 return new Translog.Delete();
-            case DELETE_BY_QUERY:
-                return new Translog.DeleteByQuery();
             case INDEX:
                 return new Index();
             default:
@@ -1589,10 +1356,6 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         return current.getFirstOperationOffset();
     }
 
-    List<ImmutableTranslogReader> getRecoveredReaders() { // for testing
-        return this.recoveredTranslogs;
-    }
-
     private void ensureOpen() {
         if (closed.get()) {
             throw new AlreadyClosedException("translog is already closed");
@@ -1606,21 +1369,4 @@ public class Translog extends AbstractIndexShardComponent implements IndexShardC
         return outstandingViews.size();
     }
 
-    private static class PathWithGeneration {
-        private final Path path;
-        private final long generation;
-
-        public PathWithGeneration(Path path, long generation) {
-            this.path = path;
-            this.generation = generation;
-        }
-
-        public Path getPath() {
-            return path;
-        }
-
-        public long getGeneration() {
-            return generation;
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesService.java b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
index 8601d76..dedfb4b 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesService.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.indices;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.CollectionUtil;
 import org.apache.lucene.util.IOUtils;
@@ -94,6 +92,8 @@ import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 import java.util.stream.Stream;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
@@ -117,8 +117,8 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
     private final NodeEnvironment nodeEnv;
     private final TimeValue shardsClosedTimeout;
 
-    private volatile Map<String, IndexServiceInjectorPair> indices = ImmutableMap.of();
-    
+    private volatile Map<String, IndexServiceInjectorPair> indices = emptyMap();
+
     static class IndexServiceInjectorPair {
         private final IndexService indexService;
         private final Injector injector;
@@ -136,7 +136,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
             return injector;
         }
     }
-    
+
     private final Map<Index, List<PendingDelete>> pendingDeletes = new HashMap<>();
 
     private final OldShardsStats oldShardsStats = new OldShardsStats();
@@ -305,11 +305,12 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         return indexService;
     }
 
-    public synchronized IndexService createIndex(String sIndexName, @IndexSettings Settings settings, String localNodeId) {
+    public synchronized IndexService createIndex(IndexMetaData indexMetaData) {
         if (!lifecycle.started()) {
-            throw new IllegalStateException("Can't create an index [" + sIndexName + "], node is closed");
+            throw new IllegalStateException("Can't create an index [" + indexMetaData.getIndex() + "], node is closed");
         }
-        Index index = new Index(sIndexName);
+        final Settings settings = indexMetaData.getSettings();
+        Index index = new Index(indexMetaData.getIndex());
         if (indices.containsKey(index.name())) {
             throw new IndexAlreadyExistsException(index);
         }
@@ -317,14 +318,14 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         indicesLifecycle.beforeIndexCreated(index, settings);
 
         logger.debug("creating Index [{}], shards [{}]/[{}{}]",
-                sIndexName,
+                indexMetaData.getIndex(),
                 settings.get(SETTING_NUMBER_OF_SHARDS),
                 settings.get(SETTING_NUMBER_OF_REPLICAS),
                 IndexMetaData.isIndexUsingShadowReplicas(settings) ? "s" : "");
 
         Settings indexSettings = settingsBuilder()
                 .put(this.settings)
-                .put(settings)
+                .put(indexMetaData.getSettings())
                 .build();
 
         ModulesBuilder modules = new ModulesBuilder();
@@ -338,8 +339,7 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         modules.add(new AnalysisModule(indexSettings, indicesAnalysisService));
         modules.add(new SimilarityModule(index, indexSettings));
         modules.add(new IndexCacheModule(indexSettings));
-        modules.add(new IndexModule());
-        
+        modules.add(new IndexModule(indexMetaData));
         pluginsService.processModules(modules);
 
         Injector indexInjector;
@@ -356,7 +356,6 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
         indicesLifecycle.afterIndexCreated(indexService);
 
         indices = newMapBuilder(indices).put(index.name(), new IndexServiceInjectorPair(indexService, indexInjector)).immutableMap();
-
         return indexService;
     }
 
@@ -380,11 +379,11 @@ public class IndicesService extends AbstractLifecycleComponent<IndicesService> i
                 }
 
                 logger.debug("[{}] closing ... (reason [{}])", index, reason);
-                Map<String, IndexServiceInjectorPair> tmpMap = new HashMap<>(indices);
-                IndexServiceInjectorPair remove = tmpMap.remove(index);
+                Map<String, IndexServiceInjectorPair> newIndices = new HashMap<>(indices);
+                IndexServiceInjectorPair remove = newIndices.remove(index);
                 indexService = remove.getIndexService();
                 indexInjector = remove.getInjector();
-                indices = ImmutableMap.copyOf(tmpMap);
+                indices = unmodifiableMap(newIndices);
             }
 
             indicesLifecycle.beforeIndexClosed(indexService);
diff --git a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
index ddaf3c8..5fb70b6 100644
--- a/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/cache/request/IndicesRequestCache.java
@@ -21,12 +21,6 @@ package org.elasticsearch.indices.cache.request;
 
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.ObjectSet;
-import com.google.common.cache.Cache;
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.RemovalListener;
-import com.google.common.cache.RemovalNotification;
-import com.google.common.cache.Weigher;
-
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.util.Accountable;
@@ -35,6 +29,7 @@ import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.cache.*;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
@@ -51,14 +46,13 @@ import org.elasticsearch.search.query.QueryPhase;
 import org.elasticsearch.search.query.QuerySearchResult;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import java.util.Collection;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.Iterator;
-import java.util.Set;
-import java.util.concurrent.Callable;
+import java.io.IOException;
+import java.util.*;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.TimeUnit;
+import java.util.function.Function;
+
+import static org.elasticsearch.common.Strings.hasLength;
 
 /**
  * The indices request cache allows to cache a shard level request stage responses, helping with improving
@@ -160,25 +154,17 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
     private void buildCache() {
         long sizeInBytes = MemorySizeValue.parseBytesSizeValueOrHeapRatio(size, INDICES_CACHE_QUERY_SIZE).bytes();
 
-        CacheBuilder<Key, Value> cacheBuilder = CacheBuilder.newBuilder()
-                .maximumWeight(sizeInBytes).weigher(new QueryCacheWeigher()).removalListener(this);
-        cacheBuilder.concurrencyLevel(concurrencyLevel);
+        CacheBuilder<Key, Value> cacheBuilder = CacheBuilder.<Key, Value>builder()
+                .setMaximumWeight(sizeInBytes).weigher((k, v) -> k.ramBytesUsed() + v.ramBytesUsed()).removalListener(this);
+        // cacheBuilder.concurrencyLevel(concurrencyLevel);
 
         if (expire != null) {
-            cacheBuilder.expireAfterAccess(expire.millis(), TimeUnit.MILLISECONDS);
+            cacheBuilder.setExpireAfterAccess(TimeUnit.MILLISECONDS.toNanos(expire.millis()));
         }
 
         cache = cacheBuilder.build();
     }
 
-    private static class QueryCacheWeigher implements Weigher<Key, Value> {
-
-        @Override
-        public int weigh(Key key, Value value) {
-            return (int) (key.ramBytesUsed() + value.ramBytesUsed());
-        }
-    }
-
     public void close() {
         reaper.close();
         cache.invalidateAll();
@@ -195,9 +181,6 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
 
     @Override
     public void onRemoval(RemovalNotification<Key, Value> notification) {
-        if (notification.getKey() == null) {
-            return;
-        }
         notification.getKey().shard.requestCache().onRemoval(notification);
     }
 
@@ -205,7 +188,8 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
      * Can the shard request be cached at all?
      */
     public boolean canCache(ShardSearchRequest request, SearchContext context) {
-        if (request.template() != null) {
+        // TODO: for now, template is not supported, though we could use the generated bytes as the key
+        if (hasLength(request.templateSource())) {
             return false;
         }
 
@@ -255,8 +239,8 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
     public void loadIntoContext(final ShardSearchRequest request, final SearchContext context, final QueryPhase queryPhase) throws Exception {
         assert canCache(request, context);
         Key key = buildKey(request, context);
-        Loader loader = new Loader(queryPhase, context, key);
-        Value value = cache.get(key, loader);
+        Loader loader = new Loader(queryPhase, context);
+        Value value = cache.computeIfAbsent(key, loader);
         if (loader.isLoaded()) {
             key.shard.requestCache().onMiss();
             // see if its the first time we see this reader, and make sure to register a cleanup key
@@ -276,17 +260,15 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
         }
     }
 
-    private static class Loader implements Callable<Value> {
+    private static class Loader implements CacheLoader<Key, Value> {
 
         private final QueryPhase queryPhase;
         private final SearchContext context;
-        private final IndicesRequestCache.Key key;
         private boolean loaded;
 
-        Loader(QueryPhase queryPhase, SearchContext context, IndicesRequestCache.Key key) {
+        Loader(QueryPhase queryPhase, SearchContext context) {
             this.queryPhase = queryPhase;
             this.context = context;
-            this.key = key;
         }
 
         public boolean isLoaded() {
@@ -294,7 +276,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
         }
 
         @Override
-        public Value call() throws Exception {
+        public Value load(Key key) throws Exception {
             queryPhase.execute(context);
 
             /* BytesStreamOutput allows to pass the expected size but by default uses
@@ -470,7 +452,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
 
             if (!currentKeysToClean.isEmpty() || !currentFullClean.isEmpty()) {
                 CleanupKey lookupKey = new CleanupKey(null, -1);
-                for (Iterator<Key> iterator = cache.asMap().keySet().iterator(); iterator.hasNext(); ) {
+                for (Iterator<Key> iterator = cache.keys().iterator(); iterator.hasNext(); ) {
                     Key key = iterator.next();
                     if (currentFullClean.contains(key.shard)) {
                         iterator.remove();
@@ -484,7 +466,7 @@ public class IndicesRequestCache extends AbstractComponent implements RemovalLis
                 }
             }
 
-            cache.cleanUp();
+            cache.refresh();
             currentKeysToClean.clear();
             currentFullClean.clear();
         }
diff --git a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
index 6bce5bc..1da88f7 100644
--- a/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
+++ b/core/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java
@@ -44,7 +44,6 @@ import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.IndexShardAlreadyExistsException;
-import org.elasticsearch.index.aliases.IndexAliasesService;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.MapperService;
@@ -300,7 +299,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                     logger.debug("[{}] creating index", indexMetaData.index());
                 }
                 try {
-                    indicesService.createIndex(indexMetaData.index(), indexMetaData.settings(), event.state().nodes().localNode().id());
+                    indicesService.createIndex(indexMetaData);
                 } catch (Throwable e) {
                     sendFailShard(shard, indexMetaData.getIndexUUID(), "failed to create index", e);
                 }
@@ -458,8 +457,7 @@ public class IndicesClusterStateService extends AbstractLifecycleComponent<Indic
                     // we only create / update here
                     continue;
                 }
-                IndexAliasesService indexAliasesService = indexService.aliasesService();
-                indexAliasesService.setAliases(indexMetaData.getAliases());
+                indexService.updateMetaData(indexMetaData);
             }
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java b/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
index 2a2aef4..6612b9f 100644
--- a/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
+++ b/core/src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java
@@ -19,12 +19,15 @@
 
 package org.elasticsearch.indices.fielddata.cache;
 
-import com.google.common.cache.*;
-import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.SegmentReader;
 import org.apache.lucene.util.Accountable;
 import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.cache.Cache;
+import org.elasticsearch.common.cache.CacheBuilder;
+import org.elasticsearch.common.cache.RemovalListener;
+import org.elasticsearch.common.cache.RemovalNotification;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.logging.ESLogger;
@@ -43,6 +46,7 @@ import org.elasticsearch.threadpool.ThreadPool;
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.function.ToLongBiFunction;
 
 /**
  */
@@ -66,17 +70,11 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
         this.indicesFieldDataCacheListener = indicesFieldDataCacheListener;
         final String size = settings.get(INDICES_FIELDDATA_CACHE_SIZE_KEY, "-1");
         final long sizeInBytes = settings.getAsMemory(INDICES_FIELDDATA_CACHE_SIZE_KEY, "-1").bytes();
-        CacheBuilder<Key, Accountable> cacheBuilder = CacheBuilder.newBuilder()
+        CacheBuilder<Key, Accountable> cacheBuilder = CacheBuilder.<Key, Accountable>builder()
                 .removalListener(this);
         if (sizeInBytes > 0) {
-            cacheBuilder.maximumWeight(sizeInBytes).weigher(new FieldDataWeigher());
+            cacheBuilder.setMaximumWeight(sizeInBytes).weigher(new FieldDataWeigher());
         }
-        // defaults to 4, but this is a busy map for all indices, increase it a bit by default
-        final int concurrencyLevel =  settings.getAsInt(FIELDDATA_CACHE_CONCURRENCY_LEVEL, 16);
-        if (concurrencyLevel <= 0) {
-            throw new IllegalArgumentException("concurrency_level must be > 0 but was: " + concurrencyLevel);
-        }
-        cacheBuilder.concurrencyLevel(concurrencyLevel);
 
         logger.debug("using size [{}] [{}]", size, new ByteSizeValue(sizeInBytes));
         cache = cacheBuilder.build();
@@ -108,7 +106,7 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
         final Accountable value = notification.getValue();
         for (IndexFieldDataCache.Listener listener : key.listeners) {
             try {
-                listener.onRemoval(key.shardId, indexCache.fieldNames, indexCache.fieldDataType, notification.wasEvicted(), value.ramBytesUsed());
+                listener.onRemoval(key.shardId, indexCache.fieldNames, indexCache.fieldDataType, notification.getRemovalReason() == RemovalNotification.RemovalReason.EVICTED, value.ramBytesUsed());
             } catch (Throwable e) {
                 // load anyway since listeners should not throw exceptions
                 logger.error("Failed to call listener on field data cache unloading", e);
@@ -116,10 +114,9 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
         }
     }
 
-    public static class FieldDataWeigher implements Weigher<Key, Accountable> {
-
+    public static class FieldDataWeigher implements ToLongBiFunction<Key, Accountable> {
         @Override
-        public int weigh(Key key, Accountable ramUsage) {
+        public long applyAsLong(Key key, Accountable ramUsage) {
             int weight = (int) Math.min(ramUsage.ramBytesUsed(), Integer.MAX_VALUE);
             return weight == 0 ? 1 : weight;
         }
@@ -150,13 +147,13 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
             final ShardId shardId = ShardUtils.extractShardId(context.reader());
             final Key key = new Key(this, context.reader().getCoreCacheKey(), shardId);
             //noinspection unchecked
-            final Accountable accountable = cache.get(key, () -> {
+            final Accountable accountable = cache.computeIfAbsent(key, k -> {
                 context.reader().addCoreClosedListener(IndexFieldCache.this);
                 for (Listener listener : this.listeners) {
-                    key.listeners.add(listener);
+                    k.listeners.add(listener);
                 }
                 final AtomicFieldData fieldData = indexFieldData.loadDirect(context);
-                for (Listener listener : key.listeners) {
+                for (Listener listener : k.listeners) {
                     try {
                         listener.onCache(shardId, fieldNames, fieldDataType, fieldData);
                     } catch (Throwable e) {
@@ -174,13 +171,13 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
             final ShardId shardId = ShardUtils.extractShardId(indexReader);
             final Key key = new Key(this, indexReader.getCoreCacheKey(), shardId);
             //noinspection unchecked
-            final Accountable accountable = cache.get(key, () -> {
+            final Accountable accountable = cache.computeIfAbsent(key, k -> {
                 indexReader.addReaderClosedListener(IndexFieldCache.this);
                 for (Listener listener : this.listeners) {
-                    key.listeners.add(listener);
+                    k.listeners.add(listener);
                 }
                 final Accountable ifd = (Accountable) indexFieldData.localGlobalDirect(indexReader);
-                for (Listener listener : key.listeners) {
+                for (Listener listener : k.listeners) {
                     try {
                         listener.onCache(shardId, fieldNames, fieldDataType, ifd);
                     } catch (Throwable e) {
@@ -207,38 +204,28 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
 
         @Override
         public void clear() {
-            for (Key key : cache.asMap().keySet()) {
+            for (Key key : cache.keys()) {
                 if (key.indexCache.index.equals(index)) {
                     cache.invalidate(key);
                 }
             }
-            // Note that cache invalidation in Guava does not immediately remove
-            // values from the cache. In the case of a cache with a rare write or
-            // read rate, it's possible for values to persist longer than desired.
-            //
-            // Note this is intended by the Guava developers, see:
-            // https://code.google.com/p/guava-libraries/wiki/CachesExplained#Eviction
-            // (the "When Does Cleanup Happen" section)
-
-            // We call it explicitly here since it should be a "rare" operation, and
-            // if a user runs it he probably wants to see memory returned as soon as
-            // possible
-            cache.cleanUp();
+            // force eviction
+            cache.refresh();
         }
 
         @Override
         public void clear(String fieldName) {
-            for (Key key : cache.asMap().keySet()) {
+            for (Key key : cache.keys()) {
                 if (key.indexCache.index.equals(index)) {
                     if (key.indexCache.fieldNames.fullName().equals(fieldName)) {
                         cache.invalidate(key);
                     }
                 }
             }
-            // we call cleanUp() because this is a manual operation, should happen
+            // we call refresh because this is a manual operation, should happen
             // rarely and probably means the user wants to see memory returned as
             // soon as possible
-            cache.cleanUp();
+            cache.refresh();
         }
 
         @Override
@@ -305,7 +292,7 @@ public class IndicesFieldDataCache extends AbstractComponent implements RemovalL
                 logger.trace("running periodic field data cache cleanup");
             }
             try {
-                this.cache.cleanUp();
+                this.cache.refresh();
             } catch (Exception e) {
                 logger.warn("Exception during periodic field data cache cleanup:", e);
             }
diff --git a/core/src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java b/core/src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java
index 54ec76e..435c0d1 100644
--- a/core/src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java
+++ b/core/src/main/java/org/elasticsearch/indices/flush/IndicesSyncedFlushResult.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.indices.flush;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.util.iterable.Iterables;
 import org.elasticsearch.common.xcontent.ToXContent;
@@ -30,6 +29,8 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * The result of performing a sync flush operation on all shards of multiple indices
  */
@@ -40,7 +41,10 @@ public class IndicesSyncedFlushResult implements ToXContent {
 
 
     public IndicesSyncedFlushResult(Map<String, List<ShardsSyncedFlushResult>> shardsResultPerIndex) {
-        this.shardsResultPerIndex = ImmutableMap.copyOf(shardsResultPerIndex);
+        // shardsResultPerIndex is never modified after it is passed to this
+        // constructor so this is safe even though shardsResultPerIndex is a
+        // ConcurrentHashMap
+        this.shardsResultPerIndex = unmodifiableMap(shardsResultPerIndex);
         this.shardCounts = calculateShardCounts(Iterables.flatten(shardsResultPerIndex.values()));
     }
 
diff --git a/core/src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java b/core/src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java
index 1388373..f7ae5f9 100644
--- a/core/src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java
+++ b/core/src/main/java/org/elasticsearch/indices/flush/ShardsSyncedFlushResult.java
@@ -18,13 +18,15 @@
  */
 package org.elasticsearch.indices.flush;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.index.shard.ShardId;
 
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Result for all copies of a shard
  */
@@ -49,7 +51,7 @@ public class ShardsSyncedFlushResult {
     public ShardsSyncedFlushResult(ShardId shardId, int totalShards, String failureReason) {
         this.syncId = null;
         this.failureReason = failureReason;
-        this.shardResponses = ImmutableMap.of();
+        this.shardResponses = emptyMap();
         this.shardId = shardId;
         this.totalShards = totalShards;
     }
@@ -59,8 +61,7 @@ public class ShardsSyncedFlushResult {
      */
     public ShardsSyncedFlushResult(ShardId shardId, String syncId, int totalShards, Map<ShardRouting, SyncedFlushService.SyncedFlushResponse> shardResponses) {
         this.failureReason = null;
-        ImmutableMap.Builder<ShardRouting, SyncedFlushService.SyncedFlushResponse> builder = ImmutableMap.builder();
-        this.shardResponses = builder.putAll(shardResponses).build();
+        this.shardResponses = unmodifiableMap(new HashMap<>(shardResponses));
         this.syncId = syncId;
         this.totalShards = totalShards;
         this.shardId = shardId;
diff --git a/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java b/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
index b6fc3cd..c0e5dcd 100644
--- a/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
+++ b/core/src/main/java/org/elasticsearch/indices/flush/SyncedFlushService.java
@@ -48,7 +48,13 @@ import org.elasticsearch.indices.IndexClosedException;
 import org.elasticsearch.indices.IndicesLifecycle;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
+import org.elasticsearch.transport.BaseTransportResponseHandler;
+import org.elasticsearch.transport.TransportChannel;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportRequest;
+import org.elasticsearch.transport.TransportRequestHandler;
+import org.elasticsearch.transport.TransportResponse;
+import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -174,7 +180,7 @@ public class SyncedFlushService extends AbstractComponent {
     * be written on a primary if no write operation was executed between step 1 and step 3 and sync id will only be written on
     * the replica if it contains the same changes that the primary contains.
     *
-    * Synced flush is a best effort operation. The sync id may be written on all, some or none of the copies. 
+    * Synced flush is a best effort operation. The sync id may be written on all, some or none of the copies.
     **/
     public void attemptSyncedFlush(final ShardId shardId, final ActionListener<ShardsSyncedFlushResult> actionListener) {
         try {
@@ -341,8 +347,7 @@ public class SyncedFlushService extends AbstractComponent {
     }
 
     private void contDownAndSendResponseIfDone(String syncId, List<ShardRouting> shards, ShardId shardId, int totalShards,
-                                               ActionListener<ShardsSyncedFlushResult> listener, CountDown countDown, Map<ShardRouting,
-            SyncedFlushResponse> results) {
+            ActionListener<ShardsSyncedFlushResult> listener, CountDown countDown, Map<ShardRouting, SyncedFlushResponse> results) {
         if (countDown.countDown()) {
             assert results.size() == shards.size();
             listener.onResponse(new ShardsSyncedFlushResult(shardId, syncId, totalShards, results));
diff --git a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
index b453503..0cec415 100644
--- a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.indices.query;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -32,9 +31,10 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 
-public class IndicesQueriesRegistry extends AbstractComponent {
+import static java.util.Collections.unmodifiableMap;
 
-    private ImmutableMap<String, QueryParser<?>> queryParsers;
+public class IndicesQueriesRegistry extends AbstractComponent {
+    private Map<String, QueryParser<?>> queryParsers;
 
     @Inject
     public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers, NamedWriteableRegistry namedWriteableRegistry) {
@@ -49,13 +49,13 @@ public class IndicesQueriesRegistry extends AbstractComponent {
         // EmptyQueryBuilder is not registered as query parser but used internally.
         // We need to register it with the NamedWriteableRegistry in order to serialize it
         namedWriteableRegistry.registerPrototype(QueryBuilder.class, EmptyQueryBuilder.PROTOTYPE);
-        this.queryParsers = ImmutableMap.copyOf(queryParsers);
+        this.queryParsers = unmodifiableMap(queryParsers);
     }
 
     /**
      * Returns all the registered query parsers
      */
-    public ImmutableMap<String, QueryParser<?>> queryParsers() {
+    public Map<String, QueryParser<?>> queryParsers() {
         return queryParsers;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java b/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
index 97b9730..6956273 100644
--- a/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
+++ b/core/src/main/java/org/elasticsearch/monitor/jvm/DeadlockAnalyzer.java
@@ -19,17 +19,17 @@
 
 package org.elasticsearch.monitor.jvm;
 
-import com.google.common.collect.ImmutableMap;
-
 import java.lang.management.ManagementFactory;
 import java.lang.management.ThreadInfo;
 import java.lang.management.ThreadMXBean;
 import java.util.Arrays;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedHashSet;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.unmodifiableMap;
 import static java.util.Collections.unmodifiableSet;
 
 /**
@@ -55,7 +55,7 @@ public class DeadlockAnalyzer {
         if (deadlockedThreads == null || deadlockedThreads.length == 0) {
             return NULL_RESULT;
         }
-        ImmutableMap<Long, ThreadInfo> threadInfoMap = createThreadInfoMap(deadlockedThreads);
+        Map<Long, ThreadInfo> threadInfoMap = createThreadInfoMap(deadlockedThreads);
         Set<LinkedHashSet<ThreadInfo>> cycles = calculateCycles(threadInfoMap);
         Set<LinkedHashSet<ThreadInfo>> chains = calculateCycleDeadlockChains(threadInfoMap, cycles);
         cycles.addAll(chains);
@@ -89,7 +89,7 @@ public class DeadlockAnalyzer {
     }
 
 
-    private Set<LinkedHashSet<ThreadInfo>> calculateCycleDeadlockChains(ImmutableMap<Long, ThreadInfo> threadInfoMap, Set<LinkedHashSet<ThreadInfo>> cycles) {
+    private Set<LinkedHashSet<ThreadInfo>> calculateCycleDeadlockChains(Map<Long, ThreadInfo> threadInfoMap, Set<LinkedHashSet<ThreadInfo>> cycles) {
         ThreadInfo allThreads[] = threadBean.getThreadInfo(threadBean.getAllThreadIds());
         Set<LinkedHashSet<ThreadInfo>> deadlockChain = new HashSet<>();
         Set<Long> knownDeadlockedThreads = threadInfoMap.keySet();
@@ -113,13 +113,13 @@ public class DeadlockAnalyzer {
     }
 
 
-    private ImmutableMap<Long, ThreadInfo> createThreadInfoMap(long threadIds[]) {
+    private Map<Long, ThreadInfo> createThreadInfoMap(long threadIds[]) {
         ThreadInfo threadInfos[] = threadBean.getThreadInfo(threadIds);
-        ImmutableMap.Builder<Long, ThreadInfo> threadInfoMap = ImmutableMap.builder();
+        Map<Long, ThreadInfo> threadInfoMap = new HashMap<>();
         for (ThreadInfo threadInfo : threadInfos) {
             threadInfoMap.put(threadInfo.getThreadId(), threadInfo);
         }
-        return threadInfoMap.build();
+        return unmodifiableMap(threadInfoMap);
     }
 
     public static class Deadlock {
diff --git a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java
index 1efd3c7..a11fc29 100644
--- a/core/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java
+++ b/core/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java
@@ -19,20 +19,20 @@
 
 package org.elasticsearch.monitor.jvm;
 
-import com.google.common.collect.ImmutableMap;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
+import org.elasticsearch.monitor.jvm.JvmStats.GarbageCollector;
 import org.elasticsearch.threadpool.ThreadPool;
 
+import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.ScheduledFuture;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.unit.TimeValue.timeValueSeconds;
-import static org.elasticsearch.monitor.jvm.JvmStats.GarbageCollector;
 import static org.elasticsearch.monitor.jvm.JvmStats.jvmStats;
 
 /**
@@ -43,7 +43,7 @@ public class JvmMonitorService extends AbstractLifecycleComponent<JvmMonitorServ
     private final ThreadPool threadPool;
     private final boolean enabled;
     private final TimeValue interval;
-    private final ImmutableMap<String, GcThreshold> gcThresholds;
+    private final Map<String, GcThreshold> gcThresholds;
 
     private volatile ScheduledFuture scheduledFuture;
 
@@ -79,7 +79,7 @@ public class JvmMonitorService extends AbstractLifecycleComponent<JvmMonitorServ
         this.enabled = this.settings.getAsBoolean("monitor.jvm.enabled", true);
         this.interval = this.settings.getAsTime("monitor.jvm.interval", timeValueSeconds(1));
 
-        MapBuilder<String, GcThreshold> gcThresholds = MapBuilder.newMapBuilder();
+        Map<String, GcThreshold> gcThresholds = new HashMap<>();
         Map<String, Settings> gcThresholdGroups = this.settings.getGroups("monitor.jvm.gc");
         for (Map.Entry<String, Settings> entry : gcThresholdGroups.entrySet()) {
             String name = entry.getKey();
@@ -92,17 +92,10 @@ public class JvmMonitorService extends AbstractLifecycleComponent<JvmMonitorServ
                 gcThresholds.put(name, new GcThreshold(name, warn.millis(), info.millis(), debug.millis()));
             }
         }
-        if (!gcThresholds.containsKey(GcNames.YOUNG)) {
-            gcThresholds.put(GcNames.YOUNG, new GcThreshold(GcNames.YOUNG, 1000, 700, 400));
-        }
-        if (!gcThresholds.containsKey(GcNames.OLD)) {
-            gcThresholds.put(GcNames.OLD, new GcThreshold(GcNames.OLD, 10000, 5000, 2000));
-        }
-        if (!gcThresholds.containsKey("default")) {
-            gcThresholds.put("default", new GcThreshold("default", 10000, 5000, 2000));
-        }
-
-        this.gcThresholds = gcThresholds.immutableMap();
+        gcThresholds.putIfAbsent(GcNames.YOUNG, new GcThreshold(GcNames.YOUNG, 1000, 700, 400));
+        gcThresholds.putIfAbsent(GcNames.OLD, new GcThreshold(GcNames.OLD, 10000, 5000, 2000));
+        gcThresholds.putIfAbsent("default", new GcThreshold("default", 10000, 5000, 2000));
+        this.gcThresholds = unmodifiableMap(gcThresholds);
 
         logger.debug("enabled [{}], interval [{}], gc_threshold [{}]", enabled, interval, this.gcThresholds);
     }
diff --git a/core/src/main/java/org/elasticsearch/node/service/NodeService.java b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
index 369f7e0..fe57800 100644
--- a/core/src/main/java/org/elasticsearch/node/service/NodeService.java
+++ b/core/src/main/java/org/elasticsearch/node/service/NodeService.java
@@ -19,15 +19,12 @@
 
 package org.elasticsearch.node.service;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.Build;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
 import org.elasticsearch.action.admin.indices.stats.CommonStatsFlags;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -42,8 +39,12 @@ import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
+
 /**
  */
 public class NodeService extends AbstractComponent {
@@ -59,7 +60,7 @@ public class NodeService extends AbstractComponent {
     @Nullable
     private HttpServer httpServer;
 
-    private volatile ImmutableMap<String, String> serviceAttributes = ImmutableMap.of();
+    private volatile Map<String, String> serviceAttributes = emptyMap();
 
     private final Version version;
 
@@ -93,11 +94,15 @@ public class NodeService extends AbstractComponent {
     }
 
     public synchronized void putAttribute(String key, String value) {
-        serviceAttributes = new MapBuilder<>(serviceAttributes).put(key, value).immutableMap();
+        Map<String, String> newServiceAttributes = new HashMap<>(serviceAttributes);
+        newServiceAttributes.put(key, value);
+        serviceAttributes = unmodifiableMap(newServiceAttributes);
     }
 
     public synchronized void removeAttribute(String key) {
-        serviceAttributes = new MapBuilder<>(serviceAttributes).remove(key).immutableMap();
+        Map<String, String> newServiceAttributes = new HashMap<>(serviceAttributes);
+        newServiceAttributes.remove(key);
+        serviceAttributes = unmodifiableMap(newServiceAttributes);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
index b20a54f..86ff604 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolatorService.java
@@ -25,6 +25,7 @@ import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.memory.ExtendedMemoryIndex;
 import org.apache.lucene.index.memory.MemoryIndex;
 import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.MatchAllDocsQuery;
@@ -189,7 +190,7 @@ public class PercolatorService extends AbstractComponent {
                 indexShard.shardId().index().name(),
                 request.indices()
         );
-        Query aliasFilter = percolateIndexService.aliasesService().aliasFilter(filteringAliases);
+        Query aliasFilter = percolateIndexService.aliasFilter(filteringAliases);
 
         SearchShardTarget searchShardTarget = new SearchShardTarget(clusterService.localNode().id(), request.shardId().getIndex(), request.shardId().id());
         final PercolateContext context = new PercolateContext(
@@ -457,22 +458,22 @@ public class PercolatorService extends AbstractComponent {
         @Override
         public PercolateShardResponse doPercolate(PercolateShardRequest request, PercolateContext context, boolean isNested) {
             long count = 0;
-            Lucene.EarlyTerminatingCollector collector = Lucene.createExistsCollector();
             for (Map.Entry<BytesRef, Query> entry : context.percolateQueries().entrySet()) {
                 try {
+                    Query existsQuery = entry.getValue();
                     if (isNested) {
-                        Lucene.exists(context.docSearcher(), entry.getValue(), Queries.newNonNestedFilter(), collector);
-                    } else {
-                        Lucene.exists(context.docSearcher(), entry.getValue(), collector);
+                        existsQuery = new BooleanQuery.Builder()
+                            .add(existsQuery, Occur.MUST)
+                            .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                            .build();
+                    }
+                    if (Lucene.exists(context.docSearcher(), existsQuery)) {
+                        count ++;
                     }
                 } catch (Throwable e) {
                     logger.debug("[" + entry.getKey() + "] failed to execute query", e);
                     throw new PercolateException(context.indexShard().shardId(), "failed to execute", e);
                 }
-
-                if (collector.exists()) {
-                    count++;
-                }
             }
             return new PercolateShardResponse(count, context, request.shardId());
         }
@@ -552,7 +553,6 @@ public class PercolatorService extends AbstractComponent {
             long count = 0;
             List<BytesRef> matches = new ArrayList<>();
             List<Map<String, HighlightField>> hls = new ArrayList<>();
-            Lucene.EarlyTerminatingCollector collector = Lucene.createExistsCollector();
 
             for (Map.Entry<BytesRef, Query> entry : context.percolateQueries().entrySet()) {
                 if (context.highlight() != null) {
@@ -560,26 +560,27 @@ public class PercolatorService extends AbstractComponent {
                     context.hitContext().cache().clear();
                 }
                 try {
+                    Query existsQuery = entry.getValue();
                     if (isNested) {
-                        Lucene.exists(context.docSearcher(), entry.getValue(), Queries.newNonNestedFilter(), collector);
-                    } else {
-                        Lucene.exists(context.docSearcher(), entry.getValue(), collector);
+                        existsQuery = new BooleanQuery.Builder()
+                            .add(existsQuery, Occur.MUST)
+                            .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                            .build();
+                    }
+                    if (Lucene.exists(context.docSearcher(), existsQuery)) {
+                        if (!context.limit || count < context.size()) {
+                            matches.add(entry.getKey());
+                            if (context.highlight() != null) {
+                                highlightPhase.hitExecute(context, context.hitContext());
+                                hls.add(context.hitContext().hit().getHighlightFields());
+                            }
+                        }
+                        count++;
                     }
                 } catch (Throwable e) {
                     logger.debug("[" + entry.getKey() + "] failed to execute query", e);
                     throw new PercolateException(context.indexShard().shardId(), "failed to execute", e);
                 }
-
-                if (collector.exists()) {
-                    if (!context.limit || count < context.size()) {
-                        matches.add(entry.getKey());
-                        if (context.highlight() != null) {
-                            highlightPhase.hitExecute(context, context.hitContext());
-                            hls.add(context.hitContext().hit().getHighlightFields());
-                        }
-                    }
-                    count++;
-                }
             }
 
             BytesRef[] finalMatches = matches.toArray(new BytesRef[matches.size()]);
diff --git a/core/src/main/java/org/elasticsearch/percolator/QueryCollector.java b/core/src/main/java/org/elasticsearch/percolator/QueryCollector.java
index dfa9f4b..094201c 100644
--- a/core/src/main/java/org/elasticsearch/percolator/QueryCollector.java
+++ b/core/src/main/java/org/elasticsearch/percolator/QueryCollector.java
@@ -19,8 +19,10 @@
 package org.elasticsearch.percolator;
 
 import com.carrotsearch.hppc.FloatArrayList;
+
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.lucene.Lucene;
@@ -54,7 +56,6 @@ abstract class QueryCollector extends SimpleCollector {
     final ESLogger logger;
     boolean isNestedDoc = false;
 
-    final Lucene.EarlyTerminatingCollector collector = Lucene.createExistsCollector();
     BytesRef current;
 
     SortedBinaryDocValues values;
@@ -166,6 +167,13 @@ abstract class QueryCollector extends SimpleCollector {
                 // log???
                 return;
             }
+            Query existsQuery = query;
+            if (isNestedDoc) {
+                existsQuery = new BooleanQuery.Builder()
+                    .add(existsQuery, Occur.MUST)
+                    .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                    .build();
+            }
             // run the query
             try {
                 if (context.highlight() != null) {
@@ -173,12 +181,7 @@ abstract class QueryCollector extends SimpleCollector {
                     context.hitContext().cache().clear();
                 }
 
-                if (isNestedDoc) {
-                    Lucene.exists(searcher, query, Queries.newNonNestedFilter(), collector);
-                } else {
-                    Lucene.exists(searcher, query, collector);
-                }
-                if (collector.exists()) {
+                if (Lucene.exists(searcher, existsQuery)) {
                     if (!limit || counter < size) {
                         matches.add(BytesRef.deepCopyOf(current));
                         if (context.highlight() != null) {
@@ -230,14 +233,16 @@ abstract class QueryCollector extends SimpleCollector {
                 // log???
                 return;
             }
+            Query existsQuery = query;
+            if (isNestedDoc) {
+                existsQuery = new BooleanQuery.Builder()
+                    .add(existsQuery, Occur.MUST)
+                    .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                    .build();
+            }
             // run the query
             try {
-                if (isNestedDoc) {
-                    Lucene.exists(searcher, query, Queries.newNonNestedFilter(), collector);
-                } else {
-                    Lucene.exists(searcher, query, collector);
-                }
-                if (collector.exists()) {
+                if (Lucene.exists(searcher, existsQuery)) {
                     topDocsLeafCollector.collect(doc);
                     postMatch(doc);
                 }
@@ -298,18 +303,20 @@ abstract class QueryCollector extends SimpleCollector {
                 // log???
                 return;
             }
+            Query existsQuery = query;
+            if (isNestedDoc) {
+                existsQuery = new BooleanQuery.Builder()
+                    .add(existsQuery, Occur.MUST)
+                    .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                    .build();
+            }
             // run the query
             try {
                 if (context.highlight() != null) {
                     context.parsedQuery(new ParsedQuery(query));
                     context.hitContext().cache().clear();
                 }
-                if (isNestedDoc) {
-                    Lucene.exists(searcher, query, Queries.newNonNestedFilter(), collector);
-                } else {
-                    Lucene.exists(searcher, query, collector);
-                }
-                if (collector.exists()) {
+                if (Lucene.exists(searcher, existsQuery)) {
                     if (!limit || counter < size) {
                         matches.add(BytesRef.deepCopyOf(current));
                         scores.add(scorer.score());
@@ -363,14 +370,16 @@ abstract class QueryCollector extends SimpleCollector {
                 // log???
                 return;
             }
+            Query existsQuery = query;
+            if (isNestedDoc) {
+                existsQuery = new BooleanQuery.Builder()
+                    .add(existsQuery, Occur.MUST)
+                    .add(Queries.newNonNestedFilter(), Occur.FILTER)
+                    .build();
+            }
             // run the query
             try {
-                if (isNestedDoc) {
-                    Lucene.exists(searcher, query, Queries.newNonNestedFilter(), collector);
-                } else {
-                    Lucene.exists(searcher, query, collector);
-                }
-                if (collector.exists()) {
+                if (Lucene.exists(searcher, existsQuery)) {
                     counter++;
                     postMatch(doc);
                 }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 2267d5e..2545deb 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -255,7 +255,7 @@ public class PluginManager {
                 copyBinDirectory(sourcePluginBinDirectory, destPluginBinDirectory, pluginHandle.name, terminal);
             } catch (IOException e) {
                 // rollback and remove potentially before installed leftovers
-                terminal.printError("Error copying bin directory [%s] to [%s], cleaning up, reason: %s", sourcePluginBinDirectory, pluginHandle.binDir(environment), e.getMessage());
+                terminal.printError("Error copying bin directory [%s] to [%s], cleaning up, reason: %s", sourcePluginBinDirectory, destPluginBinDirectory, e.getMessage());
                 tryToDeletePath(terminal, extractLocation, pluginHandle.binDir(environment));
                 throw e;
             }
@@ -267,7 +267,7 @@ public class PluginManager {
         boolean needToCopyConfigDirectory = Files.exists(sourceConfigDirectory);
         if (needToCopyConfigDirectory) {
             if (Files.exists(destConfigDirectory) && !Files.isDirectory(destConfigDirectory)) {
-                tryToDeletePath(terminal, extractLocation, pluginHandle.binDir(environment));
+                tryToDeletePath(terminal, extractLocation, destPluginBinDirectory);
                 throw new IOException("plugin config directory " + destConfigDirectory + " is not a directory");
             }
 
@@ -276,8 +276,8 @@ public class PluginManager {
                 moveFilesWithoutOverwriting(sourceConfigDirectory, destConfigDirectory, ".new");
                 terminal.println(VERBOSE, "Installed %s into %s", pluginHandle.name, destConfigDirectory.toAbsolutePath());
             } catch (IOException e) {
-                terminal.printError("Error copying config directory [%s] to [%s], cleaning up, reason: %s", sourceConfigDirectory, pluginHandle.binDir(environment), e.getMessage());
-                tryToDeletePath(terminal, extractLocation, pluginHandle.binDir(environment), pluginHandle.configDir(environment));
+                terminal.printError("Error copying config directory [%s] to [%s], cleaning up, reason: %s", sourceConfigDirectory, destConfigDirectory, e.getMessage());
+                tryToDeletePath(terminal, extractLocation, destPluginBinDirectory, destConfigDirectory);
                 throw e;
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java b/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
index d485e47..6eb32cf 100644
--- a/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
+++ b/core/src/main/java/org/elasticsearch/repositories/RepositoriesService.java
@@ -19,9 +19,12 @@
 
 package org.elasticsearch.repositories;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateListener;
 import org.elasticsearch.cluster.ack.ClusterStateUpdateRequest;
 import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;
 import org.elasticsearch.cluster.metadata.MetaData;
@@ -40,9 +43,15 @@ import org.elasticsearch.snapshots.SnapshotsService;
 import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 import java.util.stream.Collectors;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 
 /**
@@ -58,7 +67,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
 
     private final VerifyNodeRepositoryAction verifyAction;
 
-    private volatile Map<String, RepositoryHolder> repositories = ImmutableMap.of();
+    private volatile Map<String, RepositoryHolder> repositories = emptyMap();
 
     @Inject
     public RepositoriesService(Settings settings, ClusterService clusterService, TransportService transportService, RepositoryTypesRegistry typesRegistry, Injector injector) {
@@ -272,7 +281,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
                 }
             }
 
-            ImmutableMap.Builder<String, RepositoryHolder> builder = ImmutableMap.builder();
+            Map<String, RepositoryHolder> builder = new HashMap<>();
             if (newMetaData != null) {
                 // Now go through all repositories and update existing or create missing
                 for (RepositoryMetaData repositoryMetaData : newMetaData.repositories()) {
@@ -303,7 +312,7 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
                     }
                 }
             }
-            repositories = builder.build();
+            repositories = unmodifiableMap(builder);
         } catch (Throwable ex) {
             logger.warn("failure updating cluster state ", ex);
         }
@@ -368,7 +377,6 @@ public class RepositoriesService extends AbstractComponent implements ClusterSta
         }
         Map<String, RepositoryHolder> newRepositories = new HashMap<>(repositories);
         newRepositories.put(repositoryMetaData.name(), holder);
-        repositories = ImmutableMap.copyOf(newRepositories);
         return true;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
index 85d6c57..6766196 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java
@@ -29,13 +29,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
@@ -67,10 +61,8 @@ public class RestValidateQueryAction extends BaseRestHandler {
         if (RestActions.hasBodyContent(request)) {
             validateQueryRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
                 validateQueryRequest.source(querySourceBuilder);
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
index 7e75dc1..a25754d 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/validate/template/RestRenderSearchTemplateAction.java
@@ -20,8 +20,8 @@
 package org.elasticsearch.rest.action.admin.indices.validate.template;
 
 import org.elasticsearch.ElasticsearchParseException;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateRequest;
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateRequest;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
@@ -93,7 +93,7 @@ public class RestRenderSearchTemplateAction extends BaseRestHandler {
         }
         renderSearchTemplateRequest = new RenderSearchTemplateRequest();
         renderSearchTemplateRequest.template(template);
-        client.admin().indices().renderSearchTemplate(renderSearchTemplateRequest, new RestBuilderListener<RenderSearchTemplateResponse>(channel) {
+        client.admin().cluster().renderSearchTemplate(renderSearchTemplateRequest, new RestBuilderListener<RenderSearchTemplateResponse>(channel) {
 
             @Override
             public RestResponse buildResponse(RenderSearchTemplateResponse response, XContentBuilder builder) throws Exception {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
index 2a4650b..4c421cc 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java
@@ -19,24 +19,17 @@
 package org.elasticsearch.rest.action.admin.indices.warmer.put;
 
 import org.elasticsearch.action.admin.indices.warmer.put.PutWarmerRequest;
+import org.elasticsearch.action.admin.indices.warmer.put.PutWarmerResponse;
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.AcknowledgedRestListener;
-import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 
-import java.io.IOException;
-
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.RestRequest.Method.PUT;
 
@@ -44,12 +37,9 @@ import static org.elasticsearch.rest.RestRequest.Method.PUT;
  */
 public class RestPutWarmerAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry queryRegistry;
-
     @Inject
-    public RestPutWarmerAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry queryRegistry) {
+    public RestPutWarmerAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.queryRegistry = queryRegistry;
         controller.registerHandler(PUT, "/_warmer/{name}", this);
         controller.registerHandler(PUT, "/{index}/_warmer/{name}", this);
         controller.registerHandler(PUT, "/{index}/{type}/_warmer/{name}", this);
@@ -68,14 +58,12 @@ public class RestPutWarmerAction extends BaseRestHandler {
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         PutWarmerRequest putWarmerRequest = new PutWarmerRequest(request.param("name"));
-
-        BytesReference sourceBytes = RestActions.getRestContent(request);
-        SearchSourceBuilder source = RestActions.getRestSearchSource(sourceBytes, queryRegistry);
         SearchRequest searchRequest = new SearchRequest(Strings.splitStringByCommaToArray(request.param("index")))
                 .types(Strings.splitStringByCommaToArray(request.param("type")))
-                .requestCache(request.paramAsBoolean("request_cache", null)).source(source);
+                .requestCache(request.paramAsBoolean("request_cache", null))
+                .source(request.content());
         searchRequest.indicesOptions(IndicesOptions.fromRequest(request, searchRequest.indicesOptions()));
         putWarmerRequest.searchRequest(searchRequest);
         putWarmerRequest.timeout(request.paramAsTime("timeout", putWarmerRequest.timeout()));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
index 356e81e..72057a9 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java
@@ -25,16 +25,9 @@ import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.Table;
-import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestResponseListener;
 import org.elasticsearch.rest.action.support.RestTable;
@@ -47,14 +40,11 @@ import static org.elasticsearch.rest.RestRequest.Method.GET;
 
 public class RestCountAction extends AbstractCatAction {
 
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestCountAction(Settings settings, RestController restController, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestCountAction(Settings settings, RestController restController, RestController controller, Client client) {
         super(settings, controller, client);
         restController.registerHandler(GET, "/_cat/count", this);
         restController.registerHandler(GET, "/_cat/count/{index}", this);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -69,16 +59,14 @@ public class RestCountAction extends AbstractCatAction {
         CountRequest countRequest = new CountRequest(indices);
         String source = request.param("source");
         if (source != null) {
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-            countRequest.query(RestActions.getQueryContent(new BytesArray(source), context));
+            countRequest.source(source);
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
-                countRequest.query(queryBuilder);
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                countRequest.source(querySourceBuilder);
             }
         }
+
         client.count(countRequest, new RestResponseListener<CountResponse>(channel) {
             @Override
             public RestResponse buildResponse(CountResponse countResponse) throws Exception {
diff --git a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
index 4193208..2ac08fd 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java
@@ -18,6 +18,8 @@
  */
 
 package org.elasticsearch.rest.action.cat;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.action.admin.cluster.node.info.NodeInfo;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoRequest;
 import org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse;
@@ -41,8 +43,6 @@ import org.elasticsearch.rest.action.support.RestActionListener;
 import org.elasticsearch.rest.action.support.RestResponseListener;
 import org.elasticsearch.rest.action.support.RestTable;
 
-import java.util.Map;
-
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 
 public class RestNodeAttrsAction extends AbstractCatAction {
@@ -111,8 +111,7 @@ public class RestNodeAttrsAction extends AbstractCatAction {
 
         for (DiscoveryNode node : nodes) {
             NodeInfo info = nodesInfo.getNodesMap().get(node.id());
-            Map<String, String> attrs = node.getAttributes();
-            for(String att : attrs.keySet()) {
+            for(ObjectObjectCursor<String, String> att : node.attributes()) {
                 table.startRow();
                 table.addCell(node.name());
                 table.addCell(fullId ? node.id() : Strings.substring(node.getId(), 0, 4));
@@ -124,8 +123,8 @@ public class RestNodeAttrsAction extends AbstractCatAction {
                 } else {
                     table.addCell("-");
                 }
-                table.addCell(att);
-                table.addCell(attrs.containsKey(att) ? attrs.get(att) : null);
+                table.addCell(att.key);
+                table.addCell(att.value);
                 table.endRow();
             }
         }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java b/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
index 15bf2bf..677f3af 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java
@@ -22,38 +22,29 @@ package org.elasticsearch.rest.action.count;
 import org.elasticsearch.action.count.CountRequest;
 import org.elasticsearch.action.count.CountResponse;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
+import static org.elasticsearch.action.count.CountRequest.DEFAULT_MIN_SCORE;
+import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_AFTER;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 import static org.elasticsearch.rest.action.support.RestActions.buildBroadcastShardsHeader;
-import static org.elasticsearch.search.internal.SearchContext.DEFAULT_TERMINATE_AFTER;
 
 /**
  *
  */
 public class RestCountAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestCountAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestCountAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
         controller.registerHandler(POST, "/_count", this);
         controller.registerHandler(GET, "/_count", this);
@@ -61,7 +52,6 @@ public class RestCountAction extends BaseRestHandler {
         controller.registerHandler(GET, "/{index}/_count", this);
         controller.registerHandler(POST, "/{index}/{type}/_count", this);
         controller.registerHandler(GET, "/{index}/{type}/_count", this);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -69,20 +59,15 @@ public class RestCountAction extends BaseRestHandler {
         CountRequest countRequest = new CountRequest(Strings.splitStringByCommaToArray(request.param("index")));
         countRequest.indicesOptions(IndicesOptions.fromRequest(request, countRequest.indicesOptions()));
         if (RestActions.hasBodyContent(request)) {
-            BytesReference restContent = RestActions.getRestContent(request);
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-            countRequest.query(RestActions.getQueryContent(restContent, context));
+            countRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-            if (queryBuilder != null) {
-                countRequest.query(queryBuilder);
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                countRequest.source(querySourceBuilder);
             }
         }
         countRequest.routing(request.param("routing"));
-        float minScore = request.paramAsFloat("min_score", -1f);
-        if (minScore != -1f) {
-            countRequest.minScore(minScore);
-        }
+        countRequest.minScore(request.paramAsFloat("min_score", DEFAULT_MIN_SCORE));
         countRequest.types(Strings.splitStringByCommaToArray(request.param("type")));
         countRequest.preference(request.param("preference"));
 
diff --git a/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java b/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
index 04f548b..7cfe7ca 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/exists/RestExistsAction.java
@@ -27,14 +27,7 @@ import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.BytesRestResponse;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.rest.RestResponse;
-import org.elasticsearch.rest.RestStatus;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestBuilderListener;
 
@@ -58,11 +51,9 @@ public class RestExistsAction extends BaseRestHandler {
         if (RestActions.hasBodyContent(request)) {
             existsRequest.source(RestActions.getRestContent(request));
         } else {
-            QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-            if (queryBuilder != null) {
-                QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
-                querySourceBuilder.setQuery(queryBuilder);
-                existsRequest.source(querySourceBuilder.buildAsBytes());
+            QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+            if (querySourceBuilder != null) {
+                existsRequest.source(querySourceBuilder);
             }
         }
         existsRequest.routing(request.param("routing"));
diff --git a/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java b/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
index 2921e91..af1f2f4 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java
@@ -20,34 +20,16 @@
 package org.elasticsearch.rest.action.search;
 
 import org.elasticsearch.action.search.MultiSearchRequest;
-import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.search.MultiSearchResponse;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.client.Client;
-import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContent;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.TemplateQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.BaseRestHandler;
-import org.elasticsearch.rest.RestChannel;
-import org.elasticsearch.rest.RestController;
-import org.elasticsearch.rest.RestRequest;
+import org.elasticsearch.rest.*;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestToXContentListener;
-import org.elasticsearch.script.Template;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
-import java.util.Map;
-
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBooleanValue;
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeStringArrayValue;
-import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeStringValue;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
 import static org.elasticsearch.rest.RestRequest.Method.POST;
 
@@ -56,11 +38,9 @@ import static org.elasticsearch.rest.RestRequest.Method.POST;
 public class RestMultiSearchAction extends BaseRestHandler {
 
     private final boolean allowExplicitIndex;
-    private final IndicesQueriesRegistry indicesQueriesRegistry;
-
 
     @Inject
-    public RestMultiSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestMultiSearchAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
 
         controller.registerHandler(GET, "/_msearch", this);
@@ -78,7 +58,6 @@ public class RestMultiSearchAction extends BaseRestHandler {
         controller.registerHandler(POST, "/{index}/{type}/_msearch/template", this);
 
         this.allowExplicitIndex = settings.getAsBoolean("rest.action.multi.allow_explicit_index", true);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
     }
 
     @Override
@@ -90,117 +69,12 @@ public class RestMultiSearchAction extends BaseRestHandler {
         String path = request.path();
         boolean isTemplateRequest = isTemplateRequest(path);
         IndicesOptions indicesOptions = IndicesOptions.fromRequest(request, multiSearchRequest.indicesOptions());
-        parseRequest(multiSearchRequest, RestActions.getRestContent(request), isTemplateRequest, indices, types, request.param("search_type"), request.param("routing"), indicesOptions, allowExplicitIndex, indicesQueriesRegistry);
-        client.multiSearch(multiSearchRequest, new RestToXContentListener<>(channel));
+        multiSearchRequest.add(RestActions.getRestContent(request), isTemplateRequest, indices, types, request.param("search_type"), request.param("routing"), indicesOptions, allowExplicitIndex);
+
+        client.multiSearch(multiSearchRequest, new RestToXContentListener<MultiSearchResponse>(channel));
     }
 
     private boolean isTemplateRequest(String path) {
         return (path != null && path.endsWith("/template"));
     }
-
-    public static MultiSearchRequest parseRequest(MultiSearchRequest msr, BytesReference data, boolean isTemplateRequest,
-                                                   @Nullable String[] indices,
-                                                   @Nullable String[] types,
-                                                   @Nullable String searchType,
-                                                   @Nullable String routing,
-                                                   IndicesOptions indicesOptions,
-                                                   boolean allowExplicitIndex, IndicesQueriesRegistry indicesQueriesRegistry) throws Exception {
-        XContent xContent = XContentFactory.xContent(data);
-        int from = 0;
-        int length = data.length();
-        byte marker = xContent.streamSeparator();
-        final QueryParseContext queryParseContext = new QueryParseContext(indicesQueriesRegistry);
-        while (true) {
-            int nextMarker = findNextMarker(marker, from, data, length);
-            if (nextMarker == -1) {
-                break;
-            }
-            // support first line with \n
-            if (nextMarker == 0) {
-                from = nextMarker + 1;
-                continue;
-            }
-
-            SearchRequest searchRequest = new SearchRequest();
-            if (indices != null) {
-                searchRequest.indices(indices);
-            }
-            if (indicesOptions != null) {
-                searchRequest.indicesOptions(indicesOptions);
-            }
-            if (types != null && types.length > 0) {
-                searchRequest.types(types);
-            }
-            if (routing != null) {
-                searchRequest.routing(routing);
-            }
-            searchRequest.searchType(searchType);
-
-            IndicesOptions defaultOptions = IndicesOptions.strictExpandOpenAndForbidClosed();
-
-
-            // now parse the action
-            if (nextMarker - from > 0) {
-                try (XContentParser parser = xContent.createParser(data.slice(from, nextMarker - from))) {
-                    Map<String, Object> source = parser.map();
-                    for (Map.Entry<String, Object> entry : source.entrySet()) {
-                        Object value = entry.getValue();
-                        if ("index".equals(entry.getKey()) || "indices".equals(entry.getKey())) {
-                            if (!allowExplicitIndex) {
-                                throw new IllegalArgumentException("explicit index in multi percolate is not allowed");
-                            }
-                            searchRequest.indices(nodeStringArrayValue(value));
-                        } else if ("type".equals(entry.getKey()) || "types".equals(entry.getKey())) {
-                            searchRequest.types(nodeStringArrayValue(value));
-                        } else if ("search_type".equals(entry.getKey()) || "searchType".equals(entry.getKey())) {
-                            searchRequest.searchType(nodeStringValue(value, null));
-                        } else if ("request_cache".equals(entry.getKey()) || "requestCache".equals(entry.getKey())) {
-                            searchRequest.requestCache(nodeBooleanValue(value));
-                        } else if ("preference".equals(entry.getKey())) {
-                            searchRequest.preference(nodeStringValue(value, null));
-                        } else if ("routing".equals(entry.getKey())) {
-                            searchRequest.routing(nodeStringValue(value, null));
-                        }
-                    }
-                    defaultOptions = IndicesOptions.fromMap(source, defaultOptions);
-                }
-            }
-            searchRequest.indicesOptions(defaultOptions);
-
-            // move pointers
-            from = nextMarker + 1;
-            // now for the body
-            nextMarker = findNextMarker(marker, from, data, length);
-            if (nextMarker == -1) {
-                break;
-            }
-            final BytesReference slice = data.slice(from, nextMarker - from);
-            if (isTemplateRequest) {
-                try (XContentParser parser = XContentFactory.xContent(slice).createParser(slice)) {
-                    queryParseContext.reset(parser);
-                    Template template = TemplateQueryParser.parse(parser, queryParseContext.parseFieldMatcher(), "params", "template");
-                    searchRequest.template(template);
-                }
-            } else {
-                try (XContentParser requestParser = XContentFactory.xContent(slice).createParser(slice)) {
-                    queryParseContext.reset(requestParser);
-                    searchRequest.source(SearchSourceBuilder.parseSearchSource(requestParser, queryParseContext));
-                }
-            }
-            // move pointers
-            from = nextMarker + 1;
-
-            msr.add(searchRequest);
-        }
-        return msr;
-    }
-
-    private static int findNextMarker(byte marker, int from, BytesReference data, int length) {
-        for (int i = from; i < length; i++) {
-            if (data.get(i) == marker) {
-                return i;
-            }
-        }
-        return -1;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java b/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
index 0d8caa5..03a33e0 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
@@ -20,20 +20,15 @@
 package org.elasticsearch.rest.action.search;
 
 import org.elasticsearch.action.search.SearchRequest;
+import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.TemplateQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.RestChannel;
 import org.elasticsearch.rest.RestController;
@@ -41,16 +36,11 @@ import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.exists.RestExistsAction;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestStatusToXContentListener;
-import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-
-import java.io.IOException;
-import java.util.Arrays;
 
 import static org.elasticsearch.common.unit.TimeValue.parseTimeValue;
 import static org.elasticsearch.rest.RestRequest.Method.GET;
@@ -62,12 +52,9 @@ import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
  */
 public class RestSearchAction extends BaseRestHandler {
 
-    private final IndicesQueriesRegistry queryRegistry;
-
     @Inject
-    public RestSearchAction(Settings settings, RestController controller, Client client, IndicesQueriesRegistry queryRegistry) {
+    public RestSearchAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.queryRegistry = queryRegistry;
         controller.registerHandler(GET, "/_search", this);
         controller.registerHandler(POST, "/_search", this);
         controller.registerHandler(GET, "/{index}/_search", this);
@@ -91,34 +78,24 @@ public class RestSearchAction extends BaseRestHandler {
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         SearchRequest searchRequest;
-        searchRequest = RestSearchAction.parseSearchRequest(queryRegistry, request, parseFieldMatcher);
-        client.search(searchRequest, new RestStatusToXContentListener<>(channel));
+        searchRequest = RestSearchAction.parseSearchRequest(request, parseFieldMatcher);
+        client.search(searchRequest, new RestStatusToXContentListener<SearchResponse>(channel));
     }
 
-    public static SearchRequest parseSearchRequest(IndicesQueriesRegistry indicesQueriesRegistry,  RestRequest request, ParseFieldMatcher parseFieldMatcher) throws IOException {
+    public static SearchRequest parseSearchRequest(RestRequest request, ParseFieldMatcher parseFieldMatcher) {
         String[] indices = Strings.splitStringByCommaToArray(request.param("index"));
         SearchRequest searchRequest = new SearchRequest(indices);
         // get the content, and put it in the body
         // add content/source as template if template flag is set
         boolean isTemplateRequest = request.path().endsWith("/template");
-        final SearchSourceBuilder builder;
         if (RestActions.hasBodyContent(request)) {
-            BytesReference restContent = RestActions.getRestContent(request);
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
             if (isTemplateRequest) {
-                try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                    context.reset(parser);
-                    Template template = TemplateQueryParser.parse(parser, context.parseFieldMatcher(), "params", "template");
-                    searchRequest.template(template);
-                }
-                builder = null;
+                searchRequest.templateSource(RestActions.getRestContent(request));
             } else {
-                builder = RestActions.getRestSearchSource(restContent, indicesQueriesRegistry);
+                searchRequest.source(RestActions.getRestContent(request));
             }
-        } else {
-            builder = null;
         }
 
         // do not allow 'query_and_fetch' or 'dfs_query_and_fetch' search types
@@ -131,15 +108,8 @@ public class RestSearchAction extends BaseRestHandler {
         } else {
             searchRequest.searchType(searchType);
         }
-        if (builder == null) {
-            SearchSourceBuilder extraBuilder = new SearchSourceBuilder();
-            if (parseSearchSource(extraBuilder, request)) {
-                searchRequest.source(extraBuilder);
-            }
-        } else {
-            parseSearchSource(builder, request);
-            searchRequest.source(builder);
-        }
+
+        searchRequest.extraSource(parseSearchSource(request));
         searchRequest.requestCache(request.paramAsBoolean("request_cache", null));
 
         String scroll = request.param("scroll");
@@ -155,89 +125,111 @@ public class RestSearchAction extends BaseRestHandler {
         return searchRequest;
     }
 
-    public static boolean parseSearchSource(final SearchSourceBuilder searchSourceBuilder, RestRequest request) {
+    public static SearchSourceBuilder parseSearchSource(RestRequest request) {
+        SearchSourceBuilder searchSourceBuilder = null;
 
-        boolean modified = false;
-        QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-        if (queryBuilder != null) {
-            searchSourceBuilder.query(queryBuilder);
-            modified = true;
+        QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+        if (querySourceBuilder != null) {
+            searchSourceBuilder = new SearchSourceBuilder();
+            searchSourceBuilder.query(querySourceBuilder);
         }
 
         int from = request.paramAsInt("from", -1);
         if (from != -1) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.from(from);
-            modified = true;
         }
         int size = request.paramAsInt("size", -1);
         if (size != -1) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.size(size);
-            modified = true;
         }
 
         if (request.hasParam("explain")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.explain(request.paramAsBoolean("explain", null));
-            modified = true;
         }
         if (request.hasParam("version")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.version(request.paramAsBoolean("version", null));
-            modified = true;
         }
         if (request.hasParam("timeout")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.timeout(request.paramAsTime("timeout", null));
-            modified = true;
         }
         if (request.hasParam("terminate_after")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             int terminateAfter = request.paramAsInt("terminate_after",
                     SearchContext.DEFAULT_TERMINATE_AFTER);
             if (terminateAfter < 0) {
                 throw new IllegalArgumentException("terminateAfter must be > 0");
             } else if (terminateAfter > 0) {
                 searchSourceBuilder.terminateAfter(terminateAfter);
-                modified = true;
             }
         }
 
         String sField = request.param("fields");
         if (sField != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             if (!Strings.hasText(sField)) {
                 searchSourceBuilder.noFields();
-                modified = true;
             } else {
                 String[] sFields = Strings.splitStringByCommaToArray(sField);
                 if (sFields != null) {
                     for (String field : sFields) {
                         searchSourceBuilder.field(field);
-                        modified = true;
                     }
                 }
             }
         }
         String sFieldDataFields = request.param("fielddata_fields");
         if (sFieldDataFields != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             if (Strings.hasText(sFieldDataFields)) {
                 String[] sFields = Strings.splitStringByCommaToArray(sFieldDataFields);
                 if (sFields != null) {
                     for (String field : sFields) {
                         searchSourceBuilder.fieldDataField(field);
-                        modified = true;
                     }
                 }
             }
         }
         FetchSourceContext fetchSourceContext = FetchSourceContext.parseFromRestRequest(request);
         if (fetchSourceContext != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.fetchSource(fetchSourceContext);
-            modified = true;
         }
 
         if (request.hasParam("track_scores")) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             searchSourceBuilder.trackScores(request.paramAsBoolean("track_scores", false));
-            modified = true;
         }
 
         String sSorts = request.param("sort");
         if (sSorts != null) {
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             String[] sorts = Strings.splitStringByCommaToArray(sSorts);
             for (String sort : sorts) {
                 int delimiter = sort.lastIndexOf(":");
@@ -246,33 +238,37 @@ public class RestSearchAction extends BaseRestHandler {
                     String reverse = sort.substring(delimiter + 1);
                     if ("asc".equals(reverse)) {
                         searchSourceBuilder.sort(sortField, SortOrder.ASC);
-                        modified = true;
                     } else if ("desc".equals(reverse)) {
                         searchSourceBuilder.sort(sortField, SortOrder.DESC);
-                        modified = true;
                     }
                 } else {
                     searchSourceBuilder.sort(sort);
-                    modified = true;
                 }
             }
         }
 
         String sStats = request.param("stats");
         if (sStats != null) {
-            searchSourceBuilder.stats(Arrays.asList(Strings.splitStringByCommaToArray(sStats)));
-            modified = true;
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
+            searchSourceBuilder.stats(Strings.splitStringByCommaToArray(sStats));
         }
 
         String suggestField = request.param("suggest_field");
         if (suggestField != null) {
             String suggestText = request.param("suggest_text", request.param("q"));
             int suggestSize = request.paramAsInt("suggest_size", 5);
+            if (searchSourceBuilder == null) {
+                searchSourceBuilder = new SearchSourceBuilder();
+            }
             String suggestMode = request.param("suggest_mode");
-            searchSourceBuilder.suggest(new SuggestBuilder().addSuggestion(
-                    termSuggestion(suggestField).field(suggestField).text(suggestText).size(suggestSize).suggestMode(suggestMode)));
-            modified = true;
+            searchSourceBuilder.suggest().addSuggestion(
+                    termSuggestion(suggestField).field(suggestField).text(suggestText).size(suggestSize)
+                            .suggestMode(suggestMode)
+            );
         }
-        return modified;
+
+        return searchSourceBuilder;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
index 8610879..674aa69 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
@@ -19,27 +19,18 @@
 
 package org.elasticsearch.rest.action.support;
 
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ShardOperationFailedException;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentBuilderString;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.query.Operator;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.RestRequest;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 
@@ -94,7 +85,7 @@ public class RestActions {
         builder.endObject();
     }
 
-    public static QueryBuilder<?> urlParamsToQueryBuilder(RestRequest request) {
+    public static QuerySourceBuilder parseQuerySource(RestRequest request) {
         String queryString = request.param("q");
         if (queryString == null) {
             return null;
@@ -109,16 +100,7 @@ public class RestActions {
         if (defaultOperator != null) {
             queryBuilder.defaultOperator(Operator.fromString(defaultOperator));
         }
-        return queryBuilder;
-    }
-
-    public static SearchSourceBuilder getRestSearchSource(BytesReference sourceBytes, IndicesQueriesRegistry queryRegistry)
-            throws IOException {
-        XContentParser parser = XContentFactory.xContent(sourceBytes).createParser(sourceBytes);
-        QueryParseContext queryParseContext = new QueryParseContext(queryRegistry);
-        queryParseContext.reset(parser);
-        SearchSourceBuilder source = SearchSourceBuilder.parseSearchSource(parser, queryParseContext);
-        return source;
+        return new QuerySourceBuilder().setQuery(queryBuilder);
     }
 
     /**
@@ -140,17 +122,6 @@ public class RestActions {
         return content;
     }
 
-    public static QueryBuilder<?> getQueryContent(BytesReference source, QueryParseContext context) {
-        try (XContentParser requestParser = XContentFactory.xContent(source).createParser(source)) {
-            context.reset(requestParser);
-            return context.parseInnerQueryBuilder();
-        } catch (IOException e) {
-            throw new ElasticsearchException("failed to parse source", e);
-        } finally {
-            context.reset(null);
-        }
-    }
-
     /**
      * guesses the content type from either payload or source parameter
      * @param request Rest request
diff --git a/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java b/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
index 71154a5..073a4eb 100644
--- a/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
+++ b/core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.script;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -31,6 +29,8 @@ import org.elasticsearch.search.lookup.SearchLookup;
 import java.io.IOException;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * A native script engine service.
  */
@@ -38,12 +38,12 @@ public class NativeScriptEngineService extends AbstractComponent implements Scri
 
     public static final String NAME = "native";
 
-    private final ImmutableMap<String, NativeScriptFactory> scripts;
+    private final Map<String, NativeScriptFactory> scripts;
 
     @Inject
     public NativeScriptEngineService(Settings settings, Map<String, NativeScriptFactory> scripts) {
         super(settings);
-        this.scripts = ImmutableMap.copyOf(scripts);
+        this.scripts = unmodifiableMap(scripts);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java b/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
index 10a1c42..929575c 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptContextRegistry.java
@@ -19,10 +19,13 @@
 
 package org.elasticsearch.script;
 
-import com.google.common.collect.ImmutableMap;
-
-import java.util.*;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
 
+import static java.util.Collections.unmodifiableMap;
 import static java.util.Collections.unmodifiableSet;
 
 /**
@@ -33,7 +36,7 @@ import static java.util.Collections.unmodifiableSet;
 public final class ScriptContextRegistry {
     static final Set<String> RESERVED_SCRIPT_CONTEXTS = reservedScriptContexts();
 
-    private final ImmutableMap<String, ScriptContext> scriptContexts;
+    private final Map<String, ScriptContext> scriptContexts;
 
     public ScriptContextRegistry(Collection<ScriptContext.Plugin> customScriptContexts) {
         Map<String, ScriptContext> scriptContexts = new HashMap<>();
@@ -47,7 +50,7 @@ public final class ScriptContextRegistry {
                 throw new IllegalArgumentException("script context [" + customScriptContext.getKey() + "] cannot be registered twice");
             }
         }
-        this.scriptContexts = ImmutableMap.copyOf(scriptContexts);
+        this.scriptContexts = unmodifiableMap(scriptContexts);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptModes.java b/core/src/main/java/org/elasticsearch/script/ScriptModes.java
index 897e69b..cfa3a59 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptModes.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptModes.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.script;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.ScriptService.ScriptType;
@@ -29,6 +28,8 @@ import java.util.Map;
 import java.util.Set;
 import java.util.TreeMap;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Holds the {@link org.elasticsearch.script.ScriptMode}s for each of the different scripting languages available,
  * each script source and each scripted operation.
@@ -38,7 +39,7 @@ public class ScriptModes {
     static final String SCRIPT_SETTINGS_PREFIX = "script.";
     static final String ENGINE_SETTINGS_PREFIX = "script.engine";
 
-    final ImmutableMap<String, ScriptMode> scriptModes;
+    final Map<String, ScriptMode> scriptModes;
 
     ScriptModes(Map<String, ScriptEngineService> scriptEngines, ScriptContextRegistry scriptContextRegistry, Settings settings) {
         //filter out the native engine as we don't want to apply fine grained settings to it.
@@ -48,7 +49,7 @@ public class ScriptModes {
         this.scriptModes = buildScriptModeSettingsMap(settings, filteredEngines, scriptContextRegistry);
     }
 
-    private static ImmutableMap<String, ScriptMode> buildScriptModeSettingsMap(Settings settings, Map<String, ScriptEngineService> scriptEngines, ScriptContextRegistry scriptContextRegistry) {
+    private static Map<String, ScriptMode> buildScriptModeSettingsMap(Settings settings, Map<String, ScriptEngineService> scriptEngines, ScriptContextRegistry scriptContextRegistry) {
         HashMap<String, ScriptMode> scriptModesMap = new HashMap<>();
 
         //file scripts are enabled by default, for any language
@@ -61,7 +62,7 @@ public class ScriptModes {
         processSourceBasedGlobalSettings(settings, scriptEngines, scriptContextRegistry, scriptModesMap);
         processOperationBasedGlobalSettings(settings, scriptEngines, scriptContextRegistry, scriptModesMap);
         processEngineSpecificSettings(settings, scriptEngines, scriptContextRegistry, scriptModesMap);
-        return ImmutableMap.copyOf(scriptModesMap);
+        return unmodifiableMap(scriptModesMap);
     }
 
     private static void processSourceBasedGlobalSettings(Settings settings, Map<String, ScriptEngineService> scriptEngines, ScriptContextRegistry scriptContextRegistry, Map<String, ScriptMode> scriptModes) {
diff --git a/core/src/main/java/org/elasticsearch/script/ScriptService.java b/core/src/main/java/org/elasticsearch/script/ScriptService.java
index 85769bc..87a5a9a 100644
--- a/core/src/main/java/org/elasticsearch/script/ScriptService.java
+++ b/core/src/main/java/org/elasticsearch/script/ScriptService.java
@@ -19,13 +19,6 @@
 
 package org.elasticsearch.script;
 
-import java.nio.charset.StandardCharsets;
-import com.google.common.cache.Cache;
-import com.google.common.cache.CacheBuilder;
-import com.google.common.cache.RemovalListener;
-import com.google.common.cache.RemovalNotification;
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.delete.DeleteRequest;
@@ -43,6 +36,10 @@ import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesReference;
+import org.elasticsearch.common.cache.Cache;
+import org.elasticsearch.common.cache.CacheBuilder;
+import org.elasticsearch.common.cache.RemovalListener;
+import org.elasticsearch.common.cache.RemovalNotification;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -67,13 +64,16 @@ import org.elasticsearch.watcher.ResourceWatcherService;
 import java.io.Closeable;
 import java.io.IOException;
 import java.io.InputStreamReader;
+import java.nio.charset.StandardCharsets;
 import java.nio.file.Files;
 import java.nio.file.Path;
+import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.TimeUnit;
+
+import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -93,8 +93,8 @@ public class ScriptService extends AbstractComponent implements Closeable {
     private final String defaultLang;
 
     private final Set<ScriptEngineService> scriptEngines;
-    private final ImmutableMap<String, ScriptEngineService> scriptEnginesByLang;
-    private final ImmutableMap<String, ScriptEngineService> scriptEnginesByExt;
+    private final Map<String, ScriptEngineService> scriptEnginesByLang;
+    private final Map<String, ScriptEngineService> scriptEnginesByExt;
 
     private final ConcurrentMap<String, CompiledScript> staticCache = ConcurrentCollections.newConcurrentMap();
 
@@ -153,17 +153,17 @@ public class ScriptService extends AbstractComponent implements Closeable {
 
         this.defaultLang = settings.get(DEFAULT_SCRIPTING_LANGUAGE_SETTING, DEFAULT_LANG);
 
-        CacheBuilder cacheBuilder = CacheBuilder.newBuilder();
+        CacheBuilder<String, CompiledScript> cacheBuilder = CacheBuilder.builder();
         if (cacheMaxSize >= 0) {
-            cacheBuilder.maximumSize(cacheMaxSize);
+            cacheBuilder.setMaximumWeight(cacheMaxSize);
         }
         if (cacheExpire != null) {
-            cacheBuilder.expireAfterAccess(cacheExpire.nanos(), TimeUnit.NANOSECONDS);
+            cacheBuilder.setExpireAfterAccess(cacheExpire.nanos());
         }
         this.cache = cacheBuilder.removalListener(new ScriptCacheRemovalListener()).build();
 
-        ImmutableMap.Builder<String, ScriptEngineService> enginesByLangBuilder = ImmutableMap.builder();
-        ImmutableMap.Builder<String, ScriptEngineService> enginesByExtBuilder = ImmutableMap.builder();
+        Map<String, ScriptEngineService> enginesByLangBuilder = new HashMap<>();
+        Map<String, ScriptEngineService> enginesByExtBuilder = new HashMap<>();
         for (ScriptEngineService scriptEngine : scriptEngines) {
             for (String type : scriptEngine.types()) {
                 enginesByLangBuilder.put(type, scriptEngine);
@@ -172,8 +172,8 @@ public class ScriptService extends AbstractComponent implements Closeable {
                 enginesByExtBuilder.put(ext, scriptEngine);
             }
         }
-        this.scriptEnginesByLang = enginesByLangBuilder.build();
-        this.scriptEnginesByExt = enginesByExtBuilder.build();
+        this.scriptEnginesByLang = unmodifiableMap(enginesByLangBuilder);
+        this.scriptEnginesByExt = unmodifiableMap(enginesByExtBuilder);
 
         this.scriptModes = new ScriptModes(this.scriptEnginesByLang, scriptContextRegistry, settings);
 
@@ -301,7 +301,7 @@ public class ScriptService extends AbstractComponent implements Closeable {
         }
 
         String cacheKey = getCacheKey(scriptEngineService, type == ScriptType.INLINE ? null : name, code);
-        CompiledScript compiledScript = cache.getIfPresent(cacheKey);
+        CompiledScript compiledScript = cache.get(cacheKey);
 
         if (compiledScript == null) {
             //Either an un-cached inline script or indexed script
@@ -493,12 +493,8 @@ public class ScriptService extends AbstractComponent implements Closeable {
      * script has been removed from the cache
      */
     private class ScriptCacheRemovalListener implements RemovalListener<String, CompiledScript> {
-
         @Override
         public void onRemoval(RemovalNotification<String, CompiledScript> notification) {
-            if (logger.isDebugEnabled()) {
-                logger.debug("notifying script services of script removal due to: [{}]", notification.getCause());
-            }
             scriptMetrics.onCacheEviction();
             for (ScriptEngineService service : scriptEngines) {
                 try {
diff --git a/core/src/main/java/org/elasticsearch/script/Template.java b/core/src/main/java/org/elasticsearch/script/Template.java
index babe488..293a8b3 100644
--- a/core/src/main/java/org/elasticsearch/script/Template.java
+++ b/core/src/main/java/org/elasticsearch/script/Template.java
@@ -46,7 +46,7 @@ public class Template extends Script {
     /**
      * Constructor for simple inline template. The template will have no lang,
      * content type or params set.
-     *
+     * 
      * @param template
      *            The inline template.
      */
@@ -56,7 +56,7 @@ public class Template extends Script {
 
     /**
      * Constructor for Template.
-     *
+     * 
      * @param template
      *            The cache key of the template to be compiled/executed. For
      *            inline templates this is the actual templates source code. For
@@ -73,13 +73,13 @@ public class Template extends Script {
      */
     public Template(String template, ScriptType type, @Nullable String lang, @Nullable XContentType xContentType,
             @Nullable Map<String, Object> params) {
-        super(template, type, lang == null ? MustacheScriptEngineService.NAME : lang, params);
+        super(template, type, lang, params);
         this.contentType = xContentType;
     }
 
     /**
      * Method for getting the {@link XContentType} of the template.
-     *
+     * 
      * @return The {@link XContentType} of the template.
      */
     public XContentType getContentType() {
diff --git a/core/src/main/java/org/elasticsearch/search/SearchService.java b/core/src/main/java/org/elasticsearch/search/SearchService.java
index 2e1d99c..57cecfc 100644
--- a/core/src/main/java/org/elasticsearch/search/SearchService.java
+++ b/core/src/main/java/org/elasticsearch/search/SearchService.java
@@ -19,16 +19,15 @@
 
 package org.elasticsearch.search;
 
-import com.carrotsearch.hppc.ObjectFloatHashMap;
 import com.carrotsearch.hppc.ObjectHashSet;
 import com.carrotsearch.hppc.ObjectSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.google.common.collect.ImmutableMap;
 
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.TopDocs;
+import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
@@ -39,6 +38,7 @@ import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lease.Releasables;
 import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
@@ -46,8 +46,8 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
 import org.elasticsearch.common.util.concurrent.ConcurrentMapLong;
 import org.elasticsearch.common.util.concurrent.FutureUtils;
-import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
@@ -62,7 +62,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MappedFieldType.Loading;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.ParentFieldMapper;
-import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.TemplateQueryParser;
 import org.elasticsearch.index.search.stats.ShardSearchStats;
 import org.elasticsearch.index.search.stats.StatsGroupsParseElement;
 import org.elasticsearch.index.settings.IndexSettings;
@@ -75,10 +75,11 @@ import org.elasticsearch.indices.IndicesWarmer.WarmerContext;
 import org.elasticsearch.indices.cache.request.IndicesRequestCache;
 import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.Script.ScriptParseException;
 import org.elasticsearch.script.ScriptContext;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.SearchScript;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.script.Template;
+import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.dfs.DfsPhase;
 import org.elasticsearch.search.dfs.DfsSearchResult;
 import org.elasticsearch.search.fetch.FetchPhase;
@@ -86,10 +87,6 @@ import org.elasticsearch.search.fetch.FetchSearchResult;
 import org.elasticsearch.search.fetch.QueryFetchSearchResult;
 import org.elasticsearch.search.fetch.ScrollQueryFetchSearchResult;
 import org.elasticsearch.search.fetch.ShardFetchRequest;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsContext.FieldDataField;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase;
-import org.elasticsearch.search.fetch.script.ScriptFieldsContext.ScriptField;
 import org.elasticsearch.search.internal.DefaultSearchContext;
 import org.elasticsearch.search.internal.InternalScrollSearchRequest;
 import org.elasticsearch.search.internal.ScrollContext;
@@ -105,6 +102,7 @@ import org.elasticsearch.search.query.ScrollQuerySearchResult;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.threadpool.ThreadPool;
 
+import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.CountDownLatch;
@@ -113,6 +111,8 @@ import java.util.concurrent.Executor;
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.atomic.AtomicLong;
 
+import static java.util.Collections.unmodifiableMap;
+import static org.elasticsearch.common.Strings.hasLength;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMillis;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMinutes;
 
@@ -160,7 +160,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
 
     private final ConcurrentMapLong<SearchContext> activeContexts = ConcurrentCollections.newConcurrentMapLongWithAggressiveConcurrency();
 
-    private final ImmutableMap<String, SearchParseElement> elementParsers;
+    private final Map<String, SearchParseElement> elementParsers;
 
     private final ParseFieldMatcher parseFieldMatcher;
 
@@ -212,7 +212,7 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         elementParsers.putAll(queryPhase.parseElements());
         elementParsers.putAll(fetchPhase.parseElements());
         elementParsers.put("stats", new StatsGroupsParseElement());
-        this.elementParsers = ImmutableMap.copyOf(elementParsers);
+        this.elementParsers = unmodifiableMap(elementParsers);
 
         this.keepAliveReaper = threadPool.scheduleWithFixedDelay(new Reaper(), keepAliveInterval);
 
@@ -572,16 +572,10 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
                 context.scrollContext(new ScrollContext());
                 context.scrollContext().scroll = request.scroll();
             }
-            if (request.template() != null) {
-                ExecutableScript executable = this.scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, context);
-                BytesReference run = (BytesReference) executable.run();
-                try (XContentParser parser = XContentFactory.xContent(run).createParser(run)) {
-                    QueryParseContext queryParseContext = new QueryParseContext(indexService.queryParserService().indicesQueriesRegistry());
-                    queryParseContext.reset(parser);
-                    parseSource(context, SearchSourceBuilder.parseSearchSource(parser, queryParseContext));
-                }
-            }
+
+            parseTemplate(request, context);
             parseSource(context, request.source());
+            parseSource(context, request.extraSource());
 
             // if the from and size are still not set, default them
             if (context.from() == -1) {
@@ -670,230 +664,114 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
         }
     }
 
-    private void parseSource(SearchContext context, SearchSourceBuilder source) throws SearchParseException {
-        // nothing to parse...
-        if (source == null) {
-            return;
-        }
+    private void parseTemplate(ShardSearchRequest request, SearchContext searchContext) {
 
-        context.from(source.from());
-        context.size(source.size());
-        ObjectFloatHashMap<String> indexBoostMap = source.indexBoost();
-        if (indexBoostMap != null) {
-            Float indexBoost = indexBoostMap.get(context.shardTarget().index());
-            if (indexBoost != null) {
-                context.queryBoost(indexBoost);
-            }
-        }
-        if (source.query() != null) {
-            context.parsedQuery(context.queryParserService().parse(source.query()));
-        }
-        if (source.postFilter() != null) {
-            context.parsedPostFilter(context.queryParserService().parse(source.postFilter()));
-        }
-        if (source.sorts() != null) {
-            XContentParser completeSortParser = null;
-            try {
-                XContentBuilder completeSortBuilder = XContentFactory.jsonBuilder();
-                completeSortBuilder.startObject();
-                completeSortBuilder.startArray("sort");
-                for (BytesReference sort : source.sorts()) {
-                    XContentParser parser = XContentFactory.xContent(sort).createParser(sort);
-                    parser.nextToken();
-                    completeSortBuilder.copyCurrentStructure(parser);
-                }
-                completeSortBuilder.endArray();
-                completeSortBuilder.endObject();
-                BytesReference completeSortBytes = completeSortBuilder.bytes();
-                completeSortParser = XContentFactory.xContent(completeSortBytes).createParser(completeSortBytes);
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                completeSortParser.nextToken();
-                this.elementParsers.get("sort").parse(completeSortParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = completeSortParser != null ? completeSortParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse sort source [" + sSource + "]", location, e);
-            }
-        }
-        context.trackScores(source.trackScores());
-        if (source.minScore() != null) {
-            context.minimumScore(source.minScore());
-        }
-        context.timeoutInMillis(source.timeoutInMillis());
-        context.terminateAfter(source.terminateAfter());
-        if (source.aggregations() != null) {
-            XContentParser completeAggregationsParser = null;
-            try {
-                XContentBuilder completeAggregationsBuilder = XContentFactory.jsonBuilder();
-                completeAggregationsBuilder.startObject();
-                for (BytesReference agg : source.aggregations()) {
-                    XContentParser parser = XContentFactory.xContent(agg).createParser(agg);
-                    parser.nextToken();
-                    parser.nextToken();
-                    completeAggregationsBuilder.field(parser.currentName());
-                    parser.nextToken();
-                    completeAggregationsBuilder.copyCurrentStructure(parser);
-                }
-                completeAggregationsBuilder.endObject();
-                BytesReference completeAggregationsBytes = completeAggregationsBuilder.bytes();
-                completeAggregationsParser = XContentFactory.xContent(completeAggregationsBytes).createParser(completeAggregationsBytes);
-                completeAggregationsParser.nextToken();
-                this.elementParsers.get("aggregations").parse(completeAggregationsParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = completeAggregationsParser != null ? completeAggregationsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
+        BytesReference processedQuery;
+        if (request.template() != null) {
+            ExecutableScript executable = this.scriptService.executable(request.template(), ScriptContext.Standard.SEARCH, searchContext);
+            processedQuery = (BytesReference) executable.run();
+        } else {
+            if (!hasLength(request.templateSource())) {
+                return;
             }
-        }
-        if (source.suggest() != null) {
-            XContentParser suggestParser = null;
+            XContentParser parser = null;
+            Template template = null;
+
             try {
-                suggestParser = XContentFactory.xContent(source.suggest()).createParser(source.suggest());
-                suggestParser.nextToken();
-                this.elementParsers.get("suggest").parse(suggestParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
+                parser = XContentFactory.xContent(request.templateSource()).createParser(request.templateSource());
+                template = TemplateQueryParser.parse(parser, searchContext.parseFieldMatcher(), "params", "template");
+
+                if (template.getType() == ScriptService.ScriptType.INLINE) {
+                    //Try to double parse for nested template id/file
+                    parser = null;
+                    try {
+                        ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                        processedQuery = (BytesReference) executable.run();
+                        parser = XContentFactory.xContent(processedQuery).createParser(processedQuery);
+                    } catch (ElasticsearchParseException epe) {
+                        //This was an non-nested template, the parse failure was due to this, it is safe to assume this refers to a file
+                        //for backwards compatibility and keep going
+                        template = new Template(template.getScript(), ScriptService.ScriptType.FILE, MustacheScriptEngineService.NAME,
+                                null, template.getParams());
+                        ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                        processedQuery = (BytesReference) executable.run();
+                    }
+                    if (parser != null) {
+                        try {
+                            Template innerTemplate = TemplateQueryParser.parse(parser, searchContext.parseFieldMatcher());
+                            if (hasLength(innerTemplate.getScript()) && !innerTemplate.getType().equals(ScriptService.ScriptType.INLINE)) {
+                                //An inner template referring to a filename or id
+                                template = new Template(innerTemplate.getScript(), innerTemplate.getType(),
+                                        MustacheScriptEngineService.NAME, null, template.getParams());
+                                ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH,
+                                        searchContext);
+                                processedQuery = (BytesReference) executable.run();
+                            }
+                        } catch (ScriptParseException e) {
+                            // No inner template found, use original template from above
+                        }
+                    }
+                } else {
+                    ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH, searchContext);
+                    processedQuery = (BytesReference) executable.run();
                 }
-                XContentLocation location = suggestParser != null ? suggestParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
+            } catch (IOException e) {
+                throw new ElasticsearchParseException("Failed to parse template", e);
+            } finally {
+                Releasables.closeWhileHandlingException(parser);
             }
-        }
-        if (source.rescores() != null) {
-            XContentParser completeRescoreParser = null;
-            try {
-                XContentBuilder completeRescoreBuilder = XContentFactory.jsonBuilder();
-                completeRescoreBuilder.startObject();
-                completeRescoreBuilder.startArray("rescore");
-                for (BytesReference rescore : source.rescores()) {
-                    XContentParser parser = XContentFactory.xContent(rescore).createParser(rescore);
-                    parser.nextToken();
-                    completeRescoreBuilder.copyCurrentStructure(parser);
-                }
-                completeRescoreBuilder.endArray();
-                completeRescoreBuilder.endObject();
-                BytesReference completeRescoreBytes = completeRescoreBuilder.bytes();
-                completeRescoreParser = XContentFactory.xContent(completeRescoreBytes).createParser(completeRescoreBytes);
-                completeRescoreParser.nextToken();
-                completeRescoreParser.nextToken();
-                completeRescoreParser.nextToken();
-                this.elementParsers.get("rescore").parse(completeRescoreParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = completeRescoreParser != null ? completeRescoreParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse rescore source [" + sSource + "]", location, e);
+
+            if (!hasLength(template.getScript())) {
+                throw new ElasticsearchParseException("Template must have [template] field configured");
             }
         }
-        if (source.fields() != null) {
-            context.fieldNames().addAll(source.fields());
-        }
-        if (source.explain() != null) {
-            context.explain(source.explain());
-        }
-        if (source.fetchSource() != null) {
-            context.fetchSourceContext(source.fetchSource());
+        request.source(processedQuery);
+    }
+
+    private void parseSource(SearchContext context, BytesReference source) throws SearchParseException {
+        // nothing to parse...
+        if (source == null || source.length() == 0) {
+            return;
         }
-        if (source.fieldDataFields() != null) {
-            FieldDataFieldsContext fieldDataFieldsContext = context.getFetchSubPhaseContext(FieldDataFieldsFetchSubPhase.CONTEXT_FACTORY);
-            for (String field : source.fieldDataFields()) {
-                fieldDataFieldsContext.add(new FieldDataField(field));
+        XContentParser parser = null;
+        try {
+            parser = XContentFactory.xContent(source).createParser(source);
+            XContentParser.Token token;
+            token = parser.nextToken();
+            if (token != XContentParser.Token.START_OBJECT) {
+                throw new ElasticsearchParseException("failed to parse search source. source must be an object, but found [{}] instead", token.name());
             }
-            fieldDataFieldsContext.setHitExecutionNeeded(true);
-        }
-        if (source.highlighter() != null) {
-            XContentParser highlighterParser = null;
-            try {
-                highlighterParser = XContentFactory.xContent(source.highlighter()).createParser(source.highlighter());
-                this.elementParsers.get("highlight").parse(highlighterParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
+            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
+                if (token == XContentParser.Token.FIELD_NAME) {
+                    String fieldName = parser.currentName();
+                    parser.nextToken();
+                    SearchParseElement element = elementParsers.get(fieldName);
+                    if (element == null) {
+                        throw new SearchParseException(context, "failed to parse search source. unknown search element [" + fieldName + "]", parser.getTokenLocation());
+                    }
+                    element.parse(parser, context);
+                } else {
+                    if (token == null) {
+                        throw new ElasticsearchParseException("failed to parse search source. end of query source reached but query is not complete.");
+                    } else {
+                        throw new ElasticsearchParseException("failed to parse search source. expected field name but got [{}]", token);
+                    }
                 }
-                XContentLocation location = highlighterParser != null ? highlighterParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
             }
-        }
-        if (source.innerHits() != null) {
-            XContentParser innerHitsParser = null;
+        } catch (Throwable e) {
+            String sSource = "_na_";
             try {
-                innerHitsParser = XContentFactory.xContent(source.innerHits()).createParser(source.innerHits());
-                innerHitsParser.nextToken();
-                this.elementParsers.get("inner_hits").parse(innerHitsParser, context);
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = innerHitsParser != null ? innerHitsParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse suggest source [" + sSource + "]", location, e);
+                sSource = XContentHelper.convertToJson(source, false);
+            } catch (Throwable e1) {
+                // ignore
             }
-        }
-        if (source.scriptFields() != null) {
-            for (org.elasticsearch.search.builder.SearchSourceBuilder.ScriptField field : source.scriptFields()) {
-                SearchScript searchScript = context.scriptService().search(context.lookup(), field.script(), ScriptContext.Standard.SEARCH);
-                context.scriptFields().add(new ScriptField(field.fieldName(), searchScript, field.ignoreFailure()));
-            }
-        }
-        if (source.ext() != null) {
-            XContentParser extParser = null;
-            try {
-                extParser = XContentFactory.xContent(source.ext()).createParser(source.ext());
-                XContentParser.Token token = extParser.nextToken();
-                String currentFieldName = null;
-                while ((token = extParser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                    if (token == XContentParser.Token.FIELD_NAME) {
-                        currentFieldName = extParser.currentName();
-                    } else {
-                        SearchParseElement parseElement = this.elementParsers.get(currentFieldName);
-                        if (parseElement == null) {
-                            throw new SearchParseException(context, "Unknown element [" + currentFieldName + "] in [ext]",
-                                    extParser.getTokenLocation());
-                        } else {
-                            parseElement.parse(extParser, context);
-                        }
-                    }
-                }
-            } catch (Exception e) {
-                String sSource = "_na_";
-                try {
-                    sSource = source.toString();
-                } catch (Throwable e1) {
-                    // ignore
-                }
-                XContentLocation location = extParser != null ? extParser.getTokenLocation() : null;
-                throw new SearchParseException(context, "failed to parse ext source [" + sSource + "]", location, e);
+            XContentLocation location = parser != null ? parser.getTokenLocation() : null;
+            throw new SearchParseException(context, "failed to parse search source [" + sSource + "]", location, e);
+        } finally {
+            if (parser != null) {
+                parser.close();
             }
         }
-        if (source.version() != null) {
-            context.version(source.version());
-        }
-        if (source.stats() != null) {
-            context.groupStats(source.stats());
-        }
     }
 
     private static final int[] EMPTY_DOC_IDS = new int[0];
@@ -1186,23 +1064,17 @@ public class SearchService extends AbstractLifecycleComponent<SearchService> {
                         SearchContext context = null;
                         try {
                             long now = System.nanoTime();
-                            final IndexService indexService = indicesService.indexServiceSafe(indexShard.shardId().index().name());
                             ShardSearchRequest request = new ShardSearchLocalRequest(indexShard.shardId(), indexMetaData.numberOfShards(),
-                                    SearchType.QUERY_THEN_FETCH, entry.source().build(new QueryParseContext(indexService.queryParserService().indicesQueriesRegistry())), entry.types(), entry.requestCache());
+                                    SearchType.QUERY_THEN_FETCH, entry.source(), entry.types(), entry.requestCache());
                             context = createContext(request, warmerContext.searcher());
-                            // if we use sort, we need to do query to sort on
-                            // it and load relevant field data
-                            // if not, we might as well set size=0 (and cache
-                            // if needed)
+                            // if we use sort, we need to do query to sort on it and load relevant field data
+                            // if not, we might as well set size=0 (and cache if needed)
                             if (context.sort() == null) {
                                 context.size(0);
                             }
                             boolean canCache = indicesQueryCache.canCache(request, context);
-                            // early terminate when we can cache, since we
-                            // can only do proper caching on top level searcher
-                            // also, if we can't cache, and its top, we don't
-                            // need to execute it, since we already did when its
-                            // not top
+                            // early terminate when we can cache, since we can only do proper caching on top level searcher
+                            // also, if we can't cache, and its top, we don't need to execute it, since we already did when its not top
                             if (canCache != top) {
                                 return;
                             }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
index 123da5a..742f678 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
@@ -35,34 +34,33 @@ import org.elasticsearch.search.query.QueryPhaseExecutionException;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
 public class AggregationPhase implements SearchPhase {
-
-    private final AggregationParseElement parseElement;
-
-    private final AggregationBinaryParseElement binaryParseElement;
+    private final Map<String, SearchParseElement> parseElements;
 
     @Inject
     public AggregationPhase(AggregationParseElement parseElement, AggregationBinaryParseElement binaryParseElement) {
-        this.parseElement = parseElement;
-        this.binaryParseElement = binaryParseElement;
+        Map<String, SearchParseElement> parseElements = new HashMap<>();
+        parseElements.put("aggregations", parseElement);
+        parseElements.put("aggs", parseElement);
+        parseElements.put("aggregations_binary", binaryParseElement);
+        parseElements.put("aggregationsBinary", binaryParseElement);
+        parseElements.put("aggs_binary", binaryParseElement);
+        parseElements.put("aggsBinary", binaryParseElement);
+        this.parseElements = unmodifiableMap(parseElements);
     }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return ImmutableMap.<String, SearchParseElement>builder()
-                .put("aggregations", parseElement)
-                .put("aggs", parseElement)
-                .put("aggregations_binary", binaryParseElement)
-                .put("aggregationsBinary", binaryParseElement)
-                .put("aggs_binary", binaryParseElement)
-                .put("aggsBinary", binaryParseElement)
-                .build();
+        return parseElements;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationStreams.java
index 97985f3..2ebe2dd 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationStreams.java
@@ -18,20 +18,22 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.stream.StreamInput;
 
 import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 
 /**
  * A registry for all the dedicated streams in the aggregation module. This is to support dynamic addAggregation that
  * know how to stream themselves.
  */
 public class AggregationStreams {
-
-    private static ImmutableMap<BytesReference, Stream> streams = ImmutableMap.of();
+    private static Map<BytesReference, Stream> streams = emptyMap();
 
     /**
      * A stream that knows how to read an aggregation from the input.
@@ -47,11 +49,11 @@ public class AggregationStreams {
      * @param types     The types associated with the streams
      */
     public static synchronized void registerStream(Stream stream, BytesReference... types) {
-        MapBuilder<BytesReference, Stream> uStreams = MapBuilder.newMapBuilder(streams);
+        Map<BytesReference, Stream> newStreams = new HashMap<>(streams);
         for (BytesReference type : types) {
-            uStreams.put(type, stream);
+            newStreams.put(type, stream);
         }
-        streams = uStreams.immutableMap();
+        streams = unmodifiableMap(newStreams);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
index 257fef8..f38138f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java
@@ -18,9 +18,6 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import com.google.common.collect.ImmutableMap;
-
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -30,19 +27,22 @@ import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorFactory;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * A registry for all the aggregator parser, also servers as the main parser for the aggregations module
  */
 public class AggregatorParsers {
-
     public static final Pattern VALID_AGG_NAME = Pattern.compile("[^\\[\\]>]+");
-    private final ImmutableMap<String, Aggregator.Parser> aggParsers;
-    private final ImmutableMap<String, PipelineAggregator.Parser> pipelineAggregatorParsers;
+
+    private final Map<String, Aggregator.Parser> aggParsers;
+    private final Map<String, PipelineAggregator.Parser> pipelineAggregatorParsers;
 
 
     /**
@@ -55,16 +55,16 @@ public class AggregatorParsers {
      */
     @Inject
     public AggregatorParsers(Set<Aggregator.Parser> aggParsers, Set<PipelineAggregator.Parser> pipelineAggregatorParsers) {
-        MapBuilder<String, Aggregator.Parser> aggParsersBuilder = MapBuilder.newMapBuilder();
+        Map<String, Aggregator.Parser> aggParsersBuilder = new HashMap<>(aggParsers.size());
         for (Aggregator.Parser parser : aggParsers) {
             aggParsersBuilder.put(parser.type(), parser);
         }
-        this.aggParsers = aggParsersBuilder.immutableMap();
-        MapBuilder<String, PipelineAggregator.Parser> pipelineAggregatorParsersBuilder = MapBuilder.newMapBuilder();
+        this.aggParsers = unmodifiableMap(aggParsersBuilder);
+        Map<String, PipelineAggregator.Parser> pipelineAggregatorParsersBuilder = new HashMap<>(pipelineAggregatorParsers.size());
         for (PipelineAggregator.Parser parser : pipelineAggregatorParsers) {
             pipelineAggregatorParsersBuilder.put(parser.type(), parser);
         }
-        this.pipelineAggregatorParsers = pipelineAggregatorParsersBuilder.immutableMap();
+        this.pipelineAggregatorParsers = unmodifiableMap(pipelineAggregatorParsersBuilder);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java b/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
index 4297680..3841030 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.search.aggregations;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
@@ -37,6 +36,9 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
+
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 /**
  * An internal implementation of {@link Aggregations}.
  */
@@ -46,7 +48,7 @@ public class InternalAggregations implements Aggregations, ToXContent, Streamabl
 
     private List<InternalAggregation> aggregations = Collections.emptyList();
 
-    private Map<String, InternalAggregation> aggregationsAsMap;
+    private Map<String, Aggregation> aggregationsAsMap;
 
     private InternalAggregations() {
     }
@@ -88,13 +90,13 @@ public class InternalAggregations implements Aggregations, ToXContent, Streamabl
     @Override
     public Map<String, Aggregation> getAsMap() {
         if (aggregationsAsMap == null) {
-            Map<String, InternalAggregation> aggregationsAsMap = new HashMap<>();
+            Map<String, InternalAggregation> newAggregationsAsMap = new HashMap<>();
             for (InternalAggregation aggregation : aggregations) {
-                aggregationsAsMap.put(aggregation.getName(), aggregation);
+                newAggregationsAsMap.put(aggregation.getName(), aggregation);
             }
-            this.aggregationsAsMap = aggregationsAsMap;
+            this.aggregationsAsMap = unmodifiableMap(newAggregationsAsMap);
         }
-        return new HashMap<>(aggregationsAsMap);
+        return aggregationsAsMap;
     }
 
     /**
@@ -200,7 +202,7 @@ public class InternalAggregations implements Aggregations, ToXContent, Streamabl
         int size = in.readVInt();
         if (size == 0) {
             aggregations = Collections.emptyList();
-            aggregationsAsMap = ImmutableMap.of();
+            aggregationsAsMap = emptyMap();
         } else {
             aggregations = new ArrayList<>(size);
             for (int i = 0; i < size; i++) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java
index aa489e0..ffbf826 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java
@@ -19,16 +19,18 @@
 
 package org.elasticsearch.search.aggregations.bucket;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.stream.StreamInput;
 
 import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
 
-public class BucketStreams {
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 
-    private static ImmutableMap<BytesReference, Stream> STREAMS = ImmutableMap.of();
+public class BucketStreams {
+    private static Map<BytesReference, Stream> streams = emptyMap();
 
     /**
      * A stream that knows how to read a bucket from the input.
@@ -45,11 +47,11 @@ public class BucketStreams {
      * @param types     The types associated with the streams
      */
     public static synchronized void registerStream(Stream stream, BytesReference... types) {
-        MapBuilder<BytesReference, Stream> uStreams = MapBuilder.newMapBuilder(STREAMS);
+        Map<BytesReference, Stream> newStreams = new HashMap<>(streams);
         for (BytesReference type : types) {
-            uStreams.put(type, stream);
+            newStreams.put(type, stream);
         }
-        STREAMS = uStreams.immutableMap();
+        streams = unmodifiableMap(newStreams);
     }
 
     /**
@@ -59,7 +61,7 @@ public class BucketStreams {
      * @return  The associated stream
      */
     public static Stream stream(BytesReference type) {
-        return STREAMS.get(type);
+        return streams.get(type);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
index ae2ab8a..694abf2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java
@@ -18,10 +18,7 @@
  */
 package org.elasticsearch.search.aggregations.bucket.histogram;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.rounding.DateTimeUnit;
 import org.elasticsearch.common.rounding.Rounding;
 import org.elasticsearch.common.rounding.TimeZoneRounding;
@@ -34,7 +31,12 @@ import org.elasticsearch.search.aggregations.support.ValueType;
 import org.elasticsearch.search.aggregations.support.ValuesSourceConfig;
 import org.elasticsearch.search.aggregations.support.ValuesSourceParser;
 import org.elasticsearch.search.internal.SearchContext;
+
 import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -45,27 +47,27 @@ public class DateHistogramParser implements Aggregator.Parser {
     static final ParseField OFFSET = new ParseField("offset");
     static final ParseField INTERVAL = new ParseField("interval");
 
-    public static final ImmutableMap<String, DateTimeUnit> DATE_FIELD_UNITS;
+    public static final Map<String, DateTimeUnit> DATE_FIELD_UNITS;
 
     static {
-        DATE_FIELD_UNITS = MapBuilder.<String, DateTimeUnit>newMapBuilder()
-                .put("year", DateTimeUnit.YEAR_OF_CENTURY)
-                .put("1y", DateTimeUnit.YEAR_OF_CENTURY)
-                .put("quarter", DateTimeUnit.QUARTER)
-                .put("1q", DateTimeUnit.QUARTER)
-                .put("month", DateTimeUnit.MONTH_OF_YEAR)
-                .put("1M", DateTimeUnit.MONTH_OF_YEAR)
-                .put("week", DateTimeUnit.WEEK_OF_WEEKYEAR)
-                .put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR)
-                .put("day", DateTimeUnit.DAY_OF_MONTH)
-                .put("1d", DateTimeUnit.DAY_OF_MONTH)
-                .put("hour", DateTimeUnit.HOUR_OF_DAY)
-                .put("1h", DateTimeUnit.HOUR_OF_DAY)
-                .put("minute", DateTimeUnit.MINUTES_OF_HOUR)
-                .put("1m", DateTimeUnit.MINUTES_OF_HOUR)
-                .put("second", DateTimeUnit.SECOND_OF_MINUTE)
-                .put("1s", DateTimeUnit.SECOND_OF_MINUTE)
-                .immutableMap();
+        Map<String, DateTimeUnit> dateFieldUnits = new HashMap<>();
+        dateFieldUnits.put("year", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("1y", DateTimeUnit.YEAR_OF_CENTURY);
+        dateFieldUnits.put("quarter", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("1q", DateTimeUnit.QUARTER);
+        dateFieldUnits.put("month", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("1M", DateTimeUnit.MONTH_OF_YEAR);
+        dateFieldUnits.put("week", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("1w", DateTimeUnit.WEEK_OF_WEEKYEAR);
+        dateFieldUnits.put("day", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("1d", DateTimeUnit.DAY_OF_MONTH);
+        dateFieldUnits.put("hour", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("1h", DateTimeUnit.HOUR_OF_DAY);
+        dateFieldUnits.put("minute", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("1m", DateTimeUnit.MINUTES_OF_HOUR);
+        dateFieldUnits.put("second", DateTimeUnit.SECOND_OF_MINUTE);
+        dateFieldUnits.put("1s", DateTimeUnit.SECOND_OF_MINUTE);
+        DATE_FIELD_UNITS = unmodifiableMap(dateFieldUnits);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java
index 25c875b..5b3182d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestState.java
@@ -1,20 +1,21 @@
 /*
-* Licensed to the Apache Software Foundation (ASF) under one or more
-* contributor license agreements. See the NOTICE file distributed with
-* this work for additional information regarding copyright ownership.
-* The ASF licenses this file to You under the Apache License, Version 2.0
-* (the "License"); you may not use this file except in compliance with
-* the License. You may obtain a copy of the License at
-*
-* http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing, software
-* distributed under the License is distributed on an "AS IS" BASIS,
-* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-* See the License for the specific language governing permissions and
-* limitations under the License.
-*/
-
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
 package org.elasticsearch.search.aggregations.metrics.percentiles.tdigest;
 
 import com.tdunning.math.stats.AVLTreeDigest;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
index 0614cd7..4bbb407 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricBuilder.java
@@ -30,7 +30,7 @@ import java.util.Map;
 /**
  * Builder for the {@link ScriptedMetric} aggregation.
  */
-public class ScriptedMetricBuilder extends MetricsAggregationBuilder {
+public class ScriptedMetricBuilder extends MetricsAggregationBuilder<ScriptedMetricBuilder> {
 
     private Script initScript = null;
     private Script mapScript = null;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
index a5202c6..62bd22a 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsBuilder.java
@@ -19,8 +19,8 @@
 package org.elasticsearch.search.aggregations.metrics.tophits;
 
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -29,6 +29,7 @@ import org.elasticsearch.search.sort.SortBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 
 import java.io.IOException;
+import java.util.Map;
 
 /**
  * Builder for the {@link TopHits} aggregation.
@@ -172,6 +173,185 @@ public class TopHitsBuilder extends AbstractAggregationBuilder {
         return this;
     }
 
+    /**
+     * Adds a field to be highlighted with default fragment size of 100 characters, and
+     * default number of fragments of 5.
+     *
+     * @param name The field to highlight
+     */
+    public TopHitsBuilder addHighlightedField(String name) {
+        highlightBuilder().field(name);
+        return this;
+    }
+
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * default number of fragments of 5.
+     *
+     * @param name         The field to highlight
+     * @param fragmentSize The size of a fragment in characters
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize) {
+        highlightBuilder().field(name, fragmentSize);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters), and
+     * a provided (maximum) number of fragments.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments);
+        return this;
+    }
+
+    /**
+     * Adds a field to be highlighted with a provided fragment size (in characters),
+     * a provided (maximum) number of fragments and an offset for the highlight.
+     *
+     * @param name              The field to highlight
+     * @param fragmentSize      The size of a fragment in characters
+     * @param numberOfFragments The (maximum) number of fragments
+     */
+    public TopHitsBuilder addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                                    int fragmentOffset) {
+        highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+        return this;
+    }
+
+    /**
+     * Adds a highlighted field.
+     */
+    public TopHitsBuilder addHighlightedField(HighlightBuilder.Field field) {
+        highlightBuilder().field(field);
+        return this;
+    }
+
+    /**
+     * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+     * are <tt>styled</tt> and <tt>default</tt>.
+     *
+     * @param schemaName The tag scheme name
+     */
+    public TopHitsBuilder setHighlighterTagsSchema(String schemaName) {
+        highlightBuilder().tagsSchema(schemaName);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFragmentSize(Integer fragmentSize) {
+        highlightBuilder().fragmentSize(fragmentSize);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterNumOfFragments(Integer numOfFragments) {
+        highlightBuilder().numOfFragments(numOfFragments);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFilter(Boolean highlightFilter) {
+        highlightBuilder().highlightFilter(highlightFilter);
+        return this;
+    }
+
+    /**
+     * The encoder to set for highlighting
+     */
+    public TopHitsBuilder setHighlighterEncoder(String encoder) {
+        highlightBuilder().encoder(encoder);
+        return this;
+    }
+
+    /**
+     * Explicitly set the pre tags that will be used for highlighting.
+     */
+    public TopHitsBuilder setHighlighterPreTags(String... preTags) {
+        highlightBuilder().preTags(preTags);
+        return this;
+    }
+
+    /**
+     * Explicitly set the post tags that will be used for highlighting.
+     */
+    public TopHitsBuilder setHighlighterPostTags(String... postTags) {
+        highlightBuilder().postTags(postTags);
+        return this;
+    }
+
+    /**
+     * The order of fragments per field. By default, ordered by the order in the
+     * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+     * by score of the fragments.
+     */
+    public TopHitsBuilder setHighlighterOrder(String order) {
+        highlightBuilder().order(order);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+        highlightBuilder().requireFieldMatch(requireFieldMatch);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+        highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterBoundaryChars(char[] boundaryChars) {
+        highlightBuilder().boundaryChars(boundaryChars);
+        return this;
+    }
+
+    /**
+     * The highlighter type to use.
+     */
+    public TopHitsBuilder setHighlighterType(String type) {
+        highlightBuilder().highlighterType(type);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterFragmenter(String fragmenter) {
+        highlightBuilder().fragmenter(fragmenter);
+        return this;
+    }
+
+    /**
+     * Sets a query to be used for highlighting all fields instead of the search query.
+     */
+    public TopHitsBuilder setHighlighterQuery(QueryBuilder highlightQuery) {
+        highlightBuilder().highlightQuery(highlightQuery);
+        return this;
+    }
+
+    /**
+     * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+     * highlight and the field doesn't also define noMatchSize.
+     * @param noMatchSize integer to set or null to leave out of request.  default is null.
+     * @return this builder for chaining
+     */
+    public TopHitsBuilder setHighlighterNoMatchSize(Integer noMatchSize) {
+        highlightBuilder().noMatchSize(noMatchSize);
+        return this;
+    }
+
+    /**
+     * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+     */
+    public TopHitsBuilder setHighlighterPhraseLimit(Integer phraseLimit) {
+        highlightBuilder().phraseLimit(phraseLimit);
+        return this;
+    }
+
+    public TopHitsBuilder setHighlighterOptions(Map<String, Object> options) {
+        highlightBuilder().options(options);
+        return this;
+    }
+
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(getName()).field(type);
@@ -186,12 +366,7 @@ public class TopHitsBuilder extends AbstractAggregationBuilder {
         return sourceBuilder;
     }
 
-    public BytesReference highlighter() {
+    public HighlightBuilder highlightBuilder() {
         return sourceBuilder().highlighter();
     }
-
-    public TopHitsBuilder highlighter(HighlightBuilder highlightBuilder) {
-        sourceBuilder().highlighter(highlightBuilder);
-        return this;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorStreams.java b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorStreams.java
index a633a3c..7104609 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorStreams.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/PipelineAggregatorStreams.java
@@ -18,21 +18,22 @@
  */
 package org.elasticsearch.search.aggregations.pipeline;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.stream.StreamInput;
 
 import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 
 /**
  * A registry for all the dedicated streams in the aggregation module. This is to support dynamic addAggregation that
  * know how to stream themselves.
  */
 public class PipelineAggregatorStreams {
-
-    private static ImmutableMap<BytesReference, Stream> streams = ImmutableMap.of();
+    private static Map<BytesReference, Stream> streams = emptyMap();
 
     /**
      * A stream that knows how to read an aggregation from the input.
@@ -48,11 +49,11 @@ public class PipelineAggregatorStreams {
      * @param types     The types associated with the streams
      */
     public static synchronized void registerStream(Stream stream, BytesReference... types) {
-        MapBuilder<BytesReference, Stream> uStreams = MapBuilder.newMapBuilder(streams);
+        Map<BytesReference, Stream> newStreams = new HashMap<>(streams);
         for (BytesReference type : types) {
-            uStreams.put(type, stream);
+            newStreams.put(type, stream);
         }
-        streams = uStreams.immutableMap();
+        streams = unmodifiableMap(newStreams);
     }
 
     /**
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index 47d8a7f..3b87030 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -20,25 +20,19 @@
 package org.elasticsearch.search.builder;
 
 import com.carrotsearch.hppc.ObjectFloatHashMap;
-import com.carrotsearch.hppc.cursors.ObjectCursor;
-
+import java.nio.charset.StandardCharsets;
+import org.elasticsearch.ElasticsearchGenerationException;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.ToXContentToBytes;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
@@ -54,8 +48,9 @@ import org.elasticsearch.search.suggest.SuggestBuilder;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.List;
-import java.util.Objects;
+import java.util.Map;
 
 /**
  * A search source builder allowing to easily build search source. Simple
@@ -64,43 +59,7 @@ import java.util.Objects;
  *
  * @see org.elasticsearch.action.search.SearchRequest#source(SearchSourceBuilder)
  */
-public final class SearchSourceBuilder extends ToXContentToBytes implements Writeable<SearchSourceBuilder> {
-
-    public static final ParseField FROM_FIELD = new ParseField("from");
-    public static final ParseField SIZE_FIELD = new ParseField("size");
-    public static final ParseField TIMEOUT_FIELD = new ParseField("timeout");
-    public static final ParseField TERMINATE_AFTER_FIELD = new ParseField("terminate_after");
-    public static final ParseField QUERY_FIELD = new ParseField("query");
-    public static final ParseField POST_FILTER_FIELD = new ParseField("post_filter");
-    public static final ParseField MIN_SCORE_FIELD = new ParseField("min_score");
-    public static final ParseField VERSION_FIELD = new ParseField("version");
-    public static final ParseField EXPLAIN_FIELD = new ParseField("explain");
-    public static final ParseField _SOURCE_FIELD = new ParseField("_source");
-    public static final ParseField FIELDS_FIELD = new ParseField("fields");
-    public static final ParseField FIELDDATA_FIELDS_FIELD = new ParseField("fielddata_fields");
-    public static final ParseField SCRIPT_FIELDS_FIELD = new ParseField("script_fields");
-    public static final ParseField SCRIPT_FIELD = new ParseField("script");
-    public static final ParseField IGNORE_FAILURE_FIELD = new ParseField("ignore_failure");
-    public static final ParseField SORT_FIELD = new ParseField("sort");
-    public static final ParseField TRACK_SCORES_FIELD = new ParseField("track_scores");
-    public static final ParseField INDICES_BOOST_FIELD = new ParseField("indices_boost");
-    public static final ParseField AGGREGATIONS_FIELD = new ParseField("aggregations", "aggs");
-    public static final ParseField HIGHLIGHT_FIELD = new ParseField("highlight");
-    public static final ParseField INNER_HITS_FIELD = new ParseField("inner_hits");
-    public static final ParseField SUGGEST_FIELD = new ParseField("suggest");
-    public static final ParseField RESCORE_FIELD = new ParseField("rescore");
-    public static final ParseField STATS_FIELD = new ParseField("stats");
-    public static final ParseField EXT_FIELD = new ParseField("ext");
-
-    private static final SearchSourceBuilder PROTOTYPE = new SearchSourceBuilder();
-
-    public static SearchSourceBuilder readSearchSourceFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    public static SearchSourceBuilder parseSearchSource(XContentParser parser, QueryParseContext context) throws IOException {
-        return PROTOTYPE.fromXContent(parser, context);
-    }
+public class SearchSourceBuilder extends ToXContentToBytes {
 
     /**
      * A static factory method to construct a new search source.
@@ -116,9 +75,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         return new HighlightBuilder();
     }
 
-    private QueryBuilder<?> queryBuilder;
+    private QuerySourceBuilder querySourceBuilder;
+
+    private QueryBuilder postQueryBuilder;
 
-    private QueryBuilder<?> postQueryBuilder;
+    private BytesReference filterBinary;
 
     private int from = -1;
 
@@ -128,7 +89,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     private Boolean version;
 
-    private List<BytesReference> sorts;
+    private List<SortBuilder> sorts;
 
     private boolean trackScores = false;
 
@@ -142,21 +103,21 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     private List<ScriptField> scriptFields;
     private FetchSourceContext fetchSourceContext;
 
-    private List<BytesReference> aggregations;
+    private List<AbstractAggregationBuilder> aggregations;
+    private BytesReference aggregationsBinary;
 
-    private BytesReference highlightBuilder;
+    private HighlightBuilder highlightBuilder;
 
-    private BytesReference suggestBuilder;
+    private SuggestBuilder suggestBuilder;
 
-    private BytesReference innerHitsBuilder;
+    private InnerHitsBuilder innerHitsBuilder;
 
-    private List<BytesReference> rescoreBuilders;
+    private List<RescoreBuilder> rescoreBuilders;
+    private Integer defaultRescoreWindowSize;
 
     private ObjectFloatHashMap<String> indexBoost = null;
 
-    private List<String> stats;
-
-    private BytesReference ext = null;
+    private String[] stats;
 
     /**
      * Constructs a new search source builder.
@@ -165,20 +126,77 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Sets the search query for this request.
+     * Sets the query provided as a {@link QuerySourceBuilder}
+     */
+    public SearchSourceBuilder query(QuerySourceBuilder querySourceBuilder) {
+        this.querySourceBuilder = querySourceBuilder;
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a search query.
      *
      * @see org.elasticsearch.index.query.QueryBuilders
      */
-    public SearchSourceBuilder query(QueryBuilder<?> query) {
-        this.queryBuilder = query;
+    public SearchSourceBuilder query(QueryBuilder query) {
+        if (this.querySourceBuilder == null) {
+            this.querySourceBuilder = new QuerySourceBuilder();
+        }
+        this.querySourceBuilder.setQuery(query);
+        return this;
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(byte[] queryBinary) {
+        return query(queryBinary, 0, queryBinary.length);
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(byte[] queryBinary, int queryBinaryOffset, int queryBinaryLength) {
+        return query(new BytesArray(queryBinary, queryBinaryOffset, queryBinaryLength));
+    }
+
+    /**
+     * Constructs a new search source builder with a raw search query.
+     */
+    public SearchSourceBuilder query(BytesReference queryBinary) {
+        if (this.querySourceBuilder == null) {
+            this.querySourceBuilder = new QuerySourceBuilder();
+        }
+        this.querySourceBuilder.setQuery(queryBinary);
         return this;
     }
 
     /**
-     * Gets the query for this request
+     * Constructs a new search source builder with a raw search query.
      */
-    public QueryBuilder<?> query() {
-        return queryBuilder;
+    public SearchSourceBuilder query(String queryString) {
+        return query(queryString.getBytes(StandardCharsets.UTF_8));
+    }
+
+    /**
+     * Constructs a new search source builder with a query from a builder.
+     */
+    public SearchSourceBuilder query(XContentBuilder query) {
+        return query(query.bytes());
+    }
+
+    /**
+     * Constructs a new search source builder with a query from a map.
+     */
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder query(Map query) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(query);
+            return query(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + query + "]", e);
+        }
     }
 
     /**
@@ -186,78 +204,96 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      * only has affect on the search hits (not aggregations). This filter is
      * always executed as last filtering mechanism.
      */
-    public SearchSourceBuilder postFilter(QueryBuilder<?> postFilter) {
+    public SearchSourceBuilder postFilter(QueryBuilder postFilter) {
         this.postQueryBuilder = postFilter;
         return this;
     }
 
     /**
-     * Gets the post filter for this request
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public QueryBuilder<?> postFilter() {
-        return postQueryBuilder;
+    public SearchSourceBuilder postFilter(String postFilterString) {
+        return postFilter(postFilterString.getBytes(StandardCharsets.UTF_8));
     }
 
     /**
-     * From index to start the search from. Defaults to <tt>0</tt>.
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public SearchSourceBuilder from(int from) {
-        this.from = from;
-        return this;
+    public SearchSourceBuilder postFilter(byte[] postFilter) {
+        return postFilter(postFilter, 0, postFilter.length);
     }
 
     /**
-     * Gets the from index to start the search from.
-     **/
-    public int from() {
-        return from;
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
+     */
+    public SearchSourceBuilder postFilter(byte[] postFilterBinary, int postFilterBinaryOffset, int postFilterBinaryLength) {
+        return postFilter(new BytesArray(postFilterBinary, postFilterBinaryOffset, postFilterBinaryLength));
     }
 
     /**
-     * The number of search hits to return. Defaults to <tt>10</tt>.
+     * Sets a filter on the query executed that only applies to the search query
+     * (and not aggs for example).
      */
-    public SearchSourceBuilder size(int size) {
-        this.size = size;
+    public SearchSourceBuilder postFilter(BytesReference postFilterBinary) {
+        this.filterBinary = postFilterBinary;
         return this;
     }
 
     /**
-     * Gets the number of search hits to return.
+     * Constructs a new search source builder with a query from a builder.
      */
-    public int size() {
-        return size;
+    public SearchSourceBuilder postFilter(XContentBuilder postFilter) {
+        return postFilter(postFilter.bytes());
     }
 
     /**
-     * Sets the minimum score below which docs will be filtered out.
+     * Constructs a new search source builder with a query from a map.
      */
-    public SearchSourceBuilder minScore(float minScore) {
-        this.minScore = minScore;
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder postFilter(Map postFilter) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(postFilter);
+            return postFilter(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + postFilter + "]", e);
+        }
+    }
+
+    /**
+     * From index to start the search from. Defaults to <tt>0</tt>.
+     */
+    public SearchSourceBuilder from(int from) {
+        this.from = from;
         return this;
     }
 
     /**
-     * Gets the minimum score below which docs will be filtered out.
+     * The number of search hits to return. Defaults to <tt>10</tt>.
      */
-    public Float minScore() {
-        return minScore;
+    public SearchSourceBuilder size(int size) {
+        this.size = size;
+        return this;
     }
 
     /**
-     * Should each {@link org.elasticsearch.search.SearchHit} be returned with
-     * an explanation of the hit (ranking).
+     * Sets the minimum score below which docs will be filtered out.
      */
-    public SearchSourceBuilder explain(Boolean explain) {
-        this.explain = explain;
+    public SearchSourceBuilder minScore(float minScore) {
+        this.minScore = minScore;
         return this;
     }
 
     /**
-     * Indicates whether each search hit will be returned with an explanation of
-     * the hit (ranking)
+     * Should each {@link org.elasticsearch.search.SearchHit} be returned with
+     * an explanation of the hit (ranking).
      */
-    public Boolean explain() {
-        return explain;
+    public SearchSourceBuilder explain(Boolean explain) {
+        this.explain = explain;
+        return this;
     }
 
     /**
@@ -270,14 +306,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Indicates whether the document's version will be included in the search
-     * hits.
-     */
-    public Boolean version() {
-        return version;
-    }
-
-    /**
      * An optional timeout to control how long search is allowed to take.
      */
     public SearchSourceBuilder timeout(TimeValue timeout) {
@@ -286,10 +314,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the timeout to control how long search is allowed to take.
+     * An optional timeout to control how long search is allowed to take.
      */
-    public long timeoutInMillis() {
-        return timeoutInMillis;
+    public SearchSourceBuilder timeout(String timeout) {
+        this.timeoutInMillis = TimeValue.parseTimeValue(timeout, null, getClass().getSimpleName() + ".timeout").millis();
+        return this;
     }
 
     /**
@@ -297,7 +326,7 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      * <code>terminateAfter</code> documents
      */
     public  SearchSourceBuilder terminateAfter(int terminateAfter) {
-        if (terminateAfter < 0) {
+        if (terminateAfter <= 0) {
             throw new IllegalArgumentException("terminateAfter must be > 0");
         }
         this.terminateAfter = terminateAfter;
@@ -305,13 +334,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the number of documents to terminate after collecting.
-     */
-    public int terminateAfter() {
-        return terminateAfter;
-    }
-
-    /**
      * Adds a sort against the given field name and the sort ordering.
      *
      * @param name
@@ -337,26 +359,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      * Adds a sort builder.
      */
     public SearchSourceBuilder sort(SortBuilder sort) {
-        try {
-            if (sorts == null) {
-                sorts = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            sort.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            sorts.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+        if (sorts == null) {
+            sorts = new ArrayList<>();
         }
-    }
-
-    /**
-     * Gets the bytes representing the sort builders for this request.
-     */
-    public List<BytesReference> sorts() {
-        return sorts;
+        sorts.add(sort);
+        return this;
     }
 
     /**
@@ -369,113 +376,102 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Indicates whether scores will be tracked for this request.
+     * Add an get to perform as part of the search.
      */
-    public boolean trackScores() {
-        return trackScores;
+    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
+        if (aggregations == null) {
+            aggregations = new ArrayList<>();
+        }
+        aggregations.add(aggregation);
+        return this;
     }
 
     /**
-     * Add an aggregation to perform as part of the search.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public SearchSourceBuilder aggregation(AbstractAggregationBuilder aggregation) {
-        try {
-            if (aggregations == null) {
-                aggregations = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            aggregation.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            aggregations.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
+    public SearchSourceBuilder aggregations(byte[] aggregationsBinary) {
+        return aggregations(aggregationsBinary, 0, aggregationsBinary.length);
     }
 
     /**
-     * Gets the bytes representing the aggregation builders for this request.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public List<BytesReference> aggregations() {
-        return aggregations;
+    public SearchSourceBuilder aggregations(byte[] aggregationsBinary, int aggregationsBinaryOffset, int aggregationsBinaryLength) {
+        return aggregations(new BytesArray(aggregationsBinary, aggregationsBinaryOffset, aggregationsBinaryLength));
     }
 
     /**
-     * Adds highlight to perform as part of the search.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public SearchSourceBuilder highlighter(HighlightBuilder highlightBuilder) {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            highlightBuilder.innerXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            this.highlightBuilder = builder.bytes();
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
-        }
+    public SearchSourceBuilder aggregations(BytesReference aggregationsBinary) {
+        this.aggregationsBinary = aggregationsBinary;
+        return this;
     }
 
     /**
-     * Gets the bytes representing the hightlighter builder for this request.
+     * Sets a raw (xcontent / json) addAggregation.
      */
-    public BytesReference highlighter() {
-        return highlightBuilder;
+    public SearchSourceBuilder aggregations(XContentBuilder aggs) {
+        return aggregations(aggs.bytes());
     }
 
-    public SearchSourceBuilder innerHits(InnerHitsBuilder innerHitsBuilder) {
+    /**
+     * Set the rescore window size for rescores that don't specify their window.
+     */
+    public SearchSourceBuilder defaultRescoreWindowSize(int defaultRescoreWindowSize) {
+        this.defaultRescoreWindowSize = defaultRescoreWindowSize;
+        return this;
+    }
+
+    /**
+     * Sets a raw (xcontent / json) addAggregation.
+     */
+    @SuppressWarnings("unchecked")
+    public SearchSourceBuilder aggregations(Map aggregations) {
         try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            innerHitsBuilder.innerXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            this.innerHitsBuilder = builder.bytes();
-            return this;
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(aggregations);
+            return aggregations(builder);
         } catch (IOException e) {
-            throw new RuntimeException(e);
+            throw new ElasticsearchGenerationException("Failed to generate [" + aggregations + "]", e);
         }
     }
 
+    public HighlightBuilder highlighter() {
+        if (highlightBuilder == null) {
+            highlightBuilder = new HighlightBuilder();
+        }
+        return highlightBuilder;
+    }
+
     /**
-     * Gets the bytes representing the inner hits builder for this request.
+     * Adds highlight to perform as part of the search.
      */
-    public BytesReference innerHits() {
-        return innerHitsBuilder;
+    public SearchSourceBuilder highlight(HighlightBuilder highlightBuilder) {
+        this.highlightBuilder = highlightBuilder;
+        return this;
     }
 
-    public SearchSourceBuilder suggest(SuggestBuilder suggestBuilder) {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            suggestBuilder.toXContent(builder, EMPTY_PARAMS);
-            this.suggestBuilder = builder.bytes();
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+    public InnerHitsBuilder innerHitsBuilder() {
+        if (innerHitsBuilder == null) {
+            innerHitsBuilder = new InnerHitsBuilder();
         }
+        return innerHitsBuilder;
     }
 
-    /**
-     * Gets the bytes representing the suggester builder for this request.
-     */
-    public BytesReference suggest() {
+    public SuggestBuilder suggest() {
+        if (suggestBuilder == null) {
+            suggestBuilder = new SuggestBuilder("suggest");
+        }
         return suggestBuilder;
     }
 
     public SearchSourceBuilder addRescorer(RescoreBuilder rescoreBuilder) {
-        try {
-            if (rescoreBuilders == null) {
-                rescoreBuilders = new ArrayList<>();
-            }
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            rescoreBuilder.toXContent(builder, EMPTY_PARAMS);
-            builder.endObject();
-            rescoreBuilders.add(builder.bytes());
-            return this;
-        } catch (IOException e) {
-            throw new RuntimeException(e);
+        if (rescoreBuilders == null) {
+            rescoreBuilders = new ArrayList<>();
         }
+        rescoreBuilders.add(rescoreBuilder);
+        return this;
     }
 
     public SearchSourceBuilder clearRescorers() {
@@ -484,13 +480,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the bytes representing the rescore builders for this request.
-     */
-    public List<BytesReference> rescores() {
-        return rescoreBuilders;
-    }
-
-    /**
      * Indicates whether the response should contain the stored _source for
      * every hit
      */
@@ -546,23 +535,11 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the {@link FetchSourceContext} which defines how the _source should
-     * be fetched.
-     */
-    public FetchSourceContext fetchSource() {
-        return fetchSourceContext;
-    }
-
-    /**
-     * Adds a field to load and return (note, it must be stored) as part of the
-     * search request. If none are specified, the source of the document will be
-     * return.
+     * Sets no fields to be loaded, resulting in only id and type to be returned
+     * per field.
      */
-    public SearchSourceBuilder field(String name) {
-        if (fieldNames == null) {
-            fieldNames = new ArrayList<>();
-        }
-        fieldNames.add(name);
+    public SearchSourceBuilder noFields() {
+        this.fieldNames = Collections.emptyList();
         return this;
     }
 
@@ -576,19 +553,28 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Sets no fields to be loaded, resulting in only id and type to be returned
-     * per field.
+     * Adds the fields to load and return as part of the search request. If none
+     * are specified, the source of the document will be returned.
      */
-    public SearchSourceBuilder noFields() {
-        this.fieldNames = Collections.emptyList();
+    public SearchSourceBuilder fields(String... fields) {
+        if (fieldNames == null) {
+            fieldNames = new ArrayList<>();
+        }
+        Collections.addAll(fieldNames, fields);
         return this;
     }
 
     /**
-     * Gets the fields to load and return as part of the search request.
+     * Adds a field to load and return (note, it must be stored) as part of the
+     * search request. If none are specified, the source of the document will be
+     * return.
      */
-    public List<String> fields() {
-        return fieldNames;
+    public SearchSourceBuilder field(String name) {
+        if (fieldNames == null) {
+            fieldNames = new ArrayList<>();
+        }
+        fieldNames.add(name);
+        return this;
     }
 
     /**
@@ -604,13 +590,6 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the field-data fields.
-     */
-    public List<String> fieldDataFields() {
-        return fieldDataFields;
-    }
-
-    /**
      * Adds a script field under the given name with the provided script.
      *
      * @param name
@@ -619,34 +598,14 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
      *            The script
      */
     public SearchSourceBuilder scriptField(String name, Script script) {
-        scriptField(name, script, false);
-        return this;
-    }
-
-    /**
-     * Adds a script field under the given name with the provided script.
-     *
-     * @param name
-     *            The name of the field
-     * @param script
-     *            The script
-     */
-    public SearchSourceBuilder scriptField(String name, Script script, boolean ignoreFailure) {
         if (scriptFields == null) {
             scriptFields = new ArrayList<>();
         }
-        scriptFields.add(new ScriptField(name, script, ignoreFailure));
+        scriptFields.add(new ScriptField(name, script));
         return this;
     }
 
     /**
-     * Gets the script fields.
-     */
-    public List<ScriptField> scriptFields() {
-        return scriptFields;
-    }
-
-    /**
      * Sets the boost a specific index will receive when the query is executeed
      * against it.
      *
@@ -664,242 +623,13 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
     }
 
     /**
-     * Gets the boost a specific indices will receive when the query is
-     * executeed against them.
-     */
-    public ObjectFloatHashMap<String> indexBoost() {
-        return indexBoost;
-    }
-
-    /**
      * The stats groups this request will be aggregated under.
      */
-    public SearchSourceBuilder stats(List<String> statsGroups) {
+    public SearchSourceBuilder stats(String... statsGroups) {
         this.stats = statsGroups;
         return this;
     }
 
-    /**
-     * The stats groups this request will be aggregated under.
-     */
-    public List<String> stats() {
-        return stats;
-    }
-
-    public SearchSourceBuilder ext(XContentBuilder ext) {
-        this.ext = ext.bytes();
-        return this;
-    }
-
-    public BytesReference ext() {
-        return ext;
-    }
-
-    public SearchSourceBuilder fromXContent(XContentParser parser, QueryParseContext context) throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        XContentParser.Token token;
-        String currentFieldName = null;
-        if ((token = parser.nextToken()) != XContentParser.Token.START_OBJECT) {
-            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] but found [" + token + "]",
-                    parser.getTokenLocation());
-        }
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if (context.parseFieldMatcher().match(currentFieldName, FROM_FIELD)) {
-                    builder.from = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SIZE_FIELD)) {
-                    builder.size = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TIMEOUT_FIELD)) {
-                    builder.timeoutInMillis = parser.longValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TERMINATE_AFTER_FIELD)) {
-                    builder.terminateAfter = parser.intValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, MIN_SCORE_FIELD)) {
-                    builder.minScore = parser.floatValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, VERSION_FIELD)) {
-                    builder.version = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, EXPLAIN_FIELD)) {
-                    builder.explain = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, TRACK_SCORES_FIELD)) {
-                    builder.trackScores = parser.booleanValue();
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else if (context.parseFieldMatcher().match(currentFieldName, FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    fieldNames.add(parser.text());
-                    builder.fieldNames = fieldNames;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    builder.sort(parser.text());
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (context.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    builder.queryBuilder = context.parseInnerQueryBuilder();
-                } else if (context.parseFieldMatcher().match(currentFieldName, POST_FILTER_FIELD)) {
-                    builder.postQueryBuilder = context.parseInnerQueryBuilder();
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELDS_FIELD)) {
-                    List<ScriptField> scriptFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        String scriptFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            Script script = null;
-                            boolean ignoreFailure = false;
-                            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                                if (token == XContentParser.Token.FIELD_NAME) {
-                                    currentFieldName = parser.currentName();
-                                } else if (token.isValue()) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else if (context.parseFieldMatcher().match(currentFieldName, IGNORE_FAILURE_FIELD)) {
-                                        ignoreFailure = parser.booleanValue();
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                                + "].", parser.getTokenLocation());
-                                    }
-                                } else if (token == XContentParser.Token.START_OBJECT) {
-                                    if (context.parseFieldMatcher().match(currentFieldName, SCRIPT_FIELD)) {
-                                        script = Script.parse(parser, context.parseFieldMatcher());
-                                    } else {
-                                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                                + "].", parser.getTokenLocation());
-                                    }
-                                } else {
-                                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName
-                                            + "].", parser.getTokenLocation());
-                                }
-                            }
-                            scriptFields.add(new ScriptField(scriptFieldName, script, ignoreFailure));
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.START_OBJECT + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.scriptFields = scriptFields;
-                } else if (context.parseFieldMatcher().match(currentFieldName, INDICES_BOOST_FIELD)) {
-                    ObjectFloatHashMap<String> indexBoost = new ObjectFloatHashMap<String>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        if (token == XContentParser.Token.FIELD_NAME) {
-                            currentFieldName = parser.currentName();
-                        } else if (token.isValue()) {
-                            indexBoost.put(currentFieldName, parser.floatValue());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                    parser.getTokenLocation());
-                        }
-                    }
-                    builder.indexBoost = indexBoost;
-                } else if (context.parseFieldMatcher().match(currentFieldName, AGGREGATIONS_FIELD)) {
-                    List<BytesReference> aggregations = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                        currentFieldName = parser.currentName();
-                        token = parser.nextToken();
-                        if (token == XContentParser.Token.START_OBJECT) {
-                            XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType());
-                            xContentBuilder.startObject();
-                            xContentBuilder.field(currentFieldName);
-                            xContentBuilder.copyCurrentStructure(parser);
-                            xContentBuilder.endObject();
-                            aggregations.add(xContentBuilder.bytes());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                    parser.getTokenLocation());
-                        }
-                    }
-                    builder.aggregations = aggregations;
-                } else if (context.parseFieldMatcher().match(currentFieldName, HIGHLIGHT_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.highlightBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, INNER_HITS_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.innerHitsBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SUGGEST_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType());
-                    xContentBuilder.copyCurrentStructure(parser);
-                    builder.suggestBuilder = xContentBuilder.bytes();
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    sorts.add(xContentBuilder.bytes());
-                    builder.sorts = sorts;
-                } else if (context.parseFieldMatcher().match(currentFieldName, EXT_FIELD)) {
-                    XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                    builder.ext = xContentBuilder.bytes();
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else if (token == XContentParser.Token.START_ARRAY) {
-
-                if (context.parseFieldMatcher().match(currentFieldName, FIELDS_FIELD)) {
-                    List<String> fieldNames = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldNames.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.fieldNames = fieldNames;
-                } else if (context.parseFieldMatcher().match(currentFieldName, FIELDDATA_FIELDS_FIELD)) {
-                    List<String> fieldDataFields = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            fieldDataFields.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.fieldDataFields = fieldDataFields;
-                } else if (context.parseFieldMatcher().match(currentFieldName, SORT_FIELD)) {
-                    List<BytesReference> sorts = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                        sorts.add(xContentBuilder.bytes());
-                    }
-                    builder.sorts = sorts;
-                } else if (context.parseFieldMatcher().match(currentFieldName, RESCORE_FIELD)) {
-                    List<BytesReference> rescoreBuilders = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(parser.contentType()).copyCurrentStructure(parser);
-                        rescoreBuilders.add(xContentBuilder.bytes());
-                    }
-                    builder.rescoreBuilders = rescoreBuilders;
-                } else if (context.parseFieldMatcher().match(currentFieldName, STATS_FIELD)) {
-                    List<String> stats = new ArrayList<>();
-                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        if (token == XContentParser.Token.VALUE_STRING) {
-                            stats.add(parser.text());
-                        } else {
-                            throw new ParsingException(parser.getTokenLocation(), "Expected [" + XContentParser.Token.VALUE_STRING + "] in ["
-                                    + currentFieldName + "] but found [" + token + "]", parser.getTokenLocation());
-                        }
-                    }
-                    builder.stats = stats;
-                } else if (context.parseFieldMatcher().match(currentFieldName, _SOURCE_FIELD)) {
-                    FetchSourceContext fetchSourceContext = FetchSourceContext.parse(parser, context);
-                    builder.fetchSourceContext = fetchSourceContext;
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            } else {
-                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                        parser.getTokenLocation());
-            }
-        }
-        return builder;
-    }
-
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject();
@@ -910,49 +640,65 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
 
     public void innerToXContent(XContentBuilder builder, Params params) throws IOException {
         if (from != -1) {
-            builder.field(FROM_FIELD.getPreferredName(), from);
+            builder.field("from", from);
         }
         if (size != -1) {
-            builder.field(SIZE_FIELD.getPreferredName(), size);
+            builder.field("size", size);
         }
 
         if (timeoutInMillis != -1) {
-            builder.field(TIMEOUT_FIELD.getPreferredName(), timeoutInMillis);
+            builder.field("timeout", timeoutInMillis);
         }
 
         if (terminateAfter != SearchContext.DEFAULT_TERMINATE_AFTER) {
-            builder.field(TERMINATE_AFTER_FIELD.getPreferredName(), terminateAfter);
+            builder.field("terminate_after", terminateAfter);
         }
 
-        if (queryBuilder != null) {
-            builder.field(QUERY_FIELD.getPreferredName(), queryBuilder);
+        if (querySourceBuilder != null) {
+            querySourceBuilder.innerToXContent(builder, params);
         }
 
         if (postQueryBuilder != null) {
-            builder.field(POST_FILTER_FIELD.getPreferredName(), postQueryBuilder);
+            builder.field("post_filter");
+            postQueryBuilder.toXContent(builder, params);
+        }
+
+        if (filterBinary != null) {
+            if (XContentFactory.xContentType(filterBinary) == builder.contentType()) {
+                builder.rawField("filter", filterBinary);
+            } else {
+                builder.field("filter_binary", filterBinary);
+            }
         }
 
         if (minScore != null) {
-            builder.field(MIN_SCORE_FIELD.getPreferredName(), minScore);
+            builder.field("min_score", minScore);
         }
 
         if (version != null) {
-            builder.field(VERSION_FIELD.getPreferredName(), version);
+            builder.field("version", version);
         }
 
         if (explain != null) {
-            builder.field(EXPLAIN_FIELD.getPreferredName(), explain);
+            builder.field("explain", explain);
         }
 
         if (fetchSourceContext != null) {
-            builder.field(_SOURCE_FIELD.getPreferredName(), fetchSourceContext);
+            if (!fetchSourceContext.fetchSource()) {
+                builder.field("_source", false);
+            } else {
+                builder.startObject("_source");
+                builder.array("includes", fetchSourceContext.includes());
+                builder.array("excludes", fetchSourceContext.excludes());
+                builder.endObject();
+            }
         }
 
         if (fieldNames != null) {
             if (fieldNames.size() == 1) {
-                builder.field(FIELDS_FIELD.getPreferredName(), fieldNames.get(0));
+                builder.field("fields", fieldNames.get(0));
             } else {
-                builder.startArray(FIELDS_FIELD.getPreferredName());
+                builder.startArray("fields");
                 for (String fieldName : fieldNames) {
                     builder.value(fieldName);
                 }
@@ -961,37 +707,39 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
 
         if (fieldDataFields != null) {
-            builder.startArray(FIELDDATA_FIELDS_FIELD.getPreferredName());
-            for (String fieldDataField : fieldDataFields) {
-                builder.value(fieldDataField);
+            builder.startArray("fielddata_fields");
+            for (String fieldName : fieldDataFields) {
+                builder.value(fieldName);
             }
             builder.endArray();
         }
 
         if (scriptFields != null) {
-            builder.startObject(SCRIPT_FIELDS_FIELD.getPreferredName());
+            builder.startObject("script_fields");
             for (ScriptField scriptField : scriptFields) {
-                scriptField.toXContent(builder, params);
+                builder.startObject(scriptField.fieldName());
+                builder.field("script", scriptField.script());
+                builder.endObject();
             }
             builder.endObject();
         }
 
         if (sorts != null) {
-            builder.startArray(SORT_FIELD.getPreferredName());
-            for (BytesReference sort : sorts) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(sort);
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            builder.startArray("sort");
+            for (SortBuilder sort : sorts) {
+                builder.startObject();
+                sort.toXContent(builder, params);
+                builder.endObject();
             }
             builder.endArray();
         }
 
         if (trackScores) {
-            builder.field(TRACK_SCORES_FIELD.getPreferredName(), true);
+            builder.field("track_scores", true);
         }
 
         if (indexBoost != null) {
-            builder.startObject(INDICES_BOOST_FIELD.getPreferredName());
+            builder.startObject("indices_boost");
             assert !indexBoost.containsKey(null);
             final Object[] keys = indexBoost.keys;
             final float[] values = indexBoost.values;
@@ -1004,76 +752,82 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         }
 
         if (aggregations != null) {
-            builder.field(AGGREGATIONS_FIELD.getPreferredName());
+            builder.field("aggregations");
             builder.startObject();
-            for (BytesReference aggregation : aggregations) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(aggregation);
-                parser.nextToken();
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            for (AbstractAggregationBuilder aggregation : aggregations) {
+                aggregation.toXContent(builder, params);
             }
             builder.endObject();
         }
 
+        if (aggregationsBinary != null) {
+            if (XContentFactory.xContentType(aggregationsBinary) == builder.contentType()) {
+                builder.rawField("aggregations", aggregationsBinary);
+            } else {
+                builder.field("aggregations_binary", aggregationsBinary);
+            }
+        }
+
         if (highlightBuilder != null) {
-            builder.field(HIGHLIGHT_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(highlightBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            highlightBuilder.toXContent(builder, params);
         }
 
         if (innerHitsBuilder != null) {
-            builder.field(INNER_HITS_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(innerHitsBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            innerHitsBuilder.toXContent(builder, params);
         }
 
         if (suggestBuilder != null) {
-            builder.field(SUGGEST_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(suggestBuilder);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+            suggestBuilder.toXContent(builder, params);
         }
 
         if (rescoreBuilders != null) {
-            builder.startArray(RESCORE_FIELD.getPreferredName());
-            for (BytesReference rescoreBuilder : rescoreBuilders) {
-                XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(rescoreBuilder);
-                parser.nextToken();
-                builder.copyCurrentStructure(parser);
+            // Strip empty rescoreBuilders from the request
+            Iterator<RescoreBuilder> itr = rescoreBuilders.iterator();
+            while (itr.hasNext()) {
+                if (itr.next().isEmpty()) {
+                    itr.remove();
+                }
             }
-            builder.endArray();
-        }
 
-        if (stats != null) {
-            builder.field(STATS_FIELD.getPreferredName(), stats);
+            // Now build the request taking care to skip empty lists and only send the object form
+            // if there is just one builder.
+            if (rescoreBuilders.size() == 1) {
+                builder.startObject("rescore");
+                rescoreBuilders.get(0).toXContent(builder, params);
+                if (rescoreBuilders.get(0).windowSize() == null && defaultRescoreWindowSize != null) {
+                    builder.field("window_size", defaultRescoreWindowSize);
+                }
+                builder.endObject();
+            } else if (!rescoreBuilders.isEmpty()) {
+                builder.startArray("rescore");
+                for (RescoreBuilder rescoreBuilder : rescoreBuilders) {
+                    builder.startObject();
+                    rescoreBuilder.toXContent(builder, params);
+                    if (rescoreBuilder.windowSize() == null && defaultRescoreWindowSize != null) {
+                        builder.field("window_size", defaultRescoreWindowSize);
+                    }
+                    builder.endObject();
+                }
+                builder.endArray();
+            }
         }
 
-        if (ext != null) {
-            builder.field(EXT_FIELD.getPreferredName());
-            XContentParser parser = XContentFactory.xContent(XContentType.JSON).createParser(ext);
-            parser.nextToken();
-            builder.copyCurrentStructure(parser);
+        if (stats != null) {
+            builder.startArray("stats");
+            for (String stat : stats) {
+                builder.value(stat);
+            }
+            builder.endArray();
         }
     }
 
-    public static class ScriptField implements Writeable<ScriptField>, ToXContent {
-
-        public static final ScriptField PROTOTYPE = new ScriptField(null, null);
-
-        private final boolean ignoreFailure;
+    private static class ScriptField {
         private final String fieldName;
         private final Script script;
 
         private ScriptField(String fieldName, Script script) {
-            this(fieldName, script, false);
-        }
-
-        private ScriptField(String fieldName, Script script, boolean ignoreFailure) {
             this.fieldName = fieldName;
             this.script = script;
-            this.ignoreFailure = ignoreFailure;
         }
 
         public String fieldName() {
@@ -1083,303 +837,5 @@ public final class SearchSourceBuilder extends ToXContentToBytes implements Writ
         public Script script() {
             return script;
         }
-
-        public boolean ignoreFailure() {
-            return ignoreFailure;
-        }
-
-        @Override
-        public ScriptField readFrom(StreamInput in) throws IOException {
-            return new ScriptField(in.readString(), Script.readScript(in), in.readBoolean());
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeString(fieldName);
-            script.writeTo(out);
-            out.writeBoolean(ignoreFailure);
-        }
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(fieldName);
-            builder.field(SCRIPT_FIELD.getPreferredName(), script);
-            builder.field(IGNORE_FAILURE_FIELD.getPreferredName(), ignoreFailure);
-            builder.endObject();
-            return builder;
-        }
-
-        @Override
-        public int hashCode() {
-            return Objects.hash(fieldName, script, ignoreFailure);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (obj == null) {
-                return false;
-            }
-            if (getClass() != obj.getClass()) {
-                return false;
-            }
-            ScriptField other = (ScriptField) obj;
-            return Objects.equals(fieldName, other.fieldName)
-                    && Objects.equals(script, other.script)
-                    && Objects.equals(ignoreFailure, other.ignoreFailure);
-        }
-    }
-
-    @Override
-    public SearchSourceBuilder readFrom(StreamInput in) throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> aggregations = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                aggregations.add(in.readBytesReference());
-            }
-            builder.aggregations = aggregations;
-        }
-        builder.explain = in.readOptionalBoolean();
-        builder.fetchSourceContext = FetchSourceContext.optionalReadFromStream(in);
-        boolean hasFieldDataFields = in.readBoolean();
-        if (hasFieldDataFields) {
-            int size = in.readVInt();
-            List<String> fieldDataFields = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                fieldDataFields.add(in.readString());
-            }
-            builder.fieldDataFields = fieldDataFields;
-        }
-        boolean hasFieldNames = in.readBoolean();
-        if (hasFieldNames) {
-            int size = in.readVInt();
-            List<String> fieldNames = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                fieldNames.add(in.readString());
-            }
-            builder.fieldNames = fieldNames;
-        }
-        builder.from = in.readVInt();
-        if (in.readBoolean()) {
-            builder.highlightBuilder = in.readBytesReference();
-        }
-        boolean hasIndexBoost = in.readBoolean();
-        if (hasIndexBoost) {
-            int size = in.readVInt();
-            ObjectFloatHashMap<String> indexBoost = new ObjectFloatHashMap<String>(size);
-            for (int i = 0; i < size; i++) {
-                indexBoost.put(in.readString(), in.readFloat());
-            }
-            builder.indexBoost = indexBoost;
-        }
-        if (in.readBoolean()) {
-            builder.innerHitsBuilder = in.readBytesReference();
-        }
-        if (in.readBoolean()) {
-            builder.minScore = in.readFloat();
-        }
-        if (in.readBoolean()) {
-            builder.postQueryBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            builder.queryBuilder = in.readQuery();
-        }
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> rescoreBuilders = new ArrayList<>();
-            for (int i = 0; i < size; i++) {
-                rescoreBuilders.add(in.readBytesReference());
-            }
-            builder.rescoreBuilders = rescoreBuilders;
-        }
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<ScriptField> scriptFields = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                scriptFields.add(ScriptField.PROTOTYPE.readFrom(in));
-            }
-            builder.scriptFields = scriptFields;
-        }
-        builder.size = in.readVInt();
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<BytesReference> sorts = new ArrayList<>();
-            for (int i = 0; i < size; i++) {
-                sorts.add(in.readBytesReference());
-            }
-            builder.sorts = sorts;
-        }
-        if (in.readBoolean()) {
-            int size = in.readVInt();
-            List<String> stats = new ArrayList<>();
-            for (int i = 0; i < size; i++) {
-                stats.add(in.readString());
-            }
-            builder.stats = stats;
-        }
-        if (in.readBoolean()) {
-            builder.suggestBuilder = in.readBytesReference();
-        }
-        builder.terminateAfter = in.readVInt();
-        builder.timeoutInMillis = in.readLong();
-        builder.trackScores = in.readBoolean();
-        builder.version = in.readOptionalBoolean();
-        if (in.readBoolean()) {
-            builder.ext = in.readBytesReference();
-        }
-        return builder;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        boolean hasAggregations = aggregations != null;
-        out.writeBoolean(hasAggregations);
-        if (hasAggregations) {
-            out.writeVInt(aggregations.size());
-            for (BytesReference aggregation : aggregations) {
-                out.writeBytesReference(aggregation);
-            }
-        }
-        out.writeOptionalBoolean(explain);
-        FetchSourceContext.optionalWriteToStream(fetchSourceContext, out);
-        boolean hasFieldDataFields = fieldDataFields != null;
-        out.writeBoolean(hasFieldDataFields);
-        if (hasFieldDataFields) {
-            out.writeVInt(fieldDataFields.size());
-            for (String field : fieldDataFields) {
-                out.writeString(field);
-            }
-        }
-        boolean hasFieldNames = fieldNames != null;
-        out.writeBoolean(hasFieldNames);
-        if (hasFieldNames) {
-            out.writeVInt(fieldNames.size());
-            for (String field : fieldNames) {
-                out.writeString(field);
-            }
-        }
-        out.writeVInt(from);
-        boolean hasHighlightBuilder = highlightBuilder != null;
-        out.writeBoolean(hasHighlightBuilder);
-        if (hasHighlightBuilder) {
-            out.writeBytesReference(highlightBuilder);
-        }
-        boolean hasIndexBoost = indexBoost != null;
-        out.writeBoolean(hasIndexBoost);
-        if (hasIndexBoost) {
-            out.writeVInt(indexBoost.size());
-            for (ObjectCursor<String> key : indexBoost.keys()) {
-                out.writeString(key.value);
-                out.writeFloat(indexBoost.get(key.value));
-            }
-        }
-        boolean hasInnerHitsBuilder = innerHitsBuilder != null;
-        out.writeBoolean(hasInnerHitsBuilder);
-        if (hasInnerHitsBuilder) {
-            out.writeBytesReference(innerHitsBuilder);
-        }
-        boolean hasMinScore = minScore != null;
-        out.writeBoolean(hasMinScore);
-        if (hasMinScore) {
-            out.writeFloat(minScore);
-        }
-        boolean hasPostQuery = postQueryBuilder != null;
-        out.writeBoolean(hasPostQuery);
-        if (hasPostQuery) {
-            out.writeQuery(postQueryBuilder);
-        }
-        boolean hasQuery = queryBuilder != null;
-        out.writeBoolean(hasQuery);
-        if (hasQuery) {
-            out.writeQuery(queryBuilder);
-        }
-        boolean hasRescoreBuilders = rescoreBuilders != null;
-        out.writeBoolean(hasRescoreBuilders);
-        if (hasRescoreBuilders) {
-            out.writeVInt(rescoreBuilders.size());
-            for (BytesReference rescoreBuilder : rescoreBuilders) {
-                out.writeBytesReference(rescoreBuilder);
-            }
-        }
-        boolean hasScriptFields = scriptFields != null;
-        out.writeBoolean(hasScriptFields);
-        if (hasScriptFields) {
-            out.writeVInt(scriptFields.size());
-            for (ScriptField scriptField : scriptFields) {
-                scriptField.writeTo(out);
-            }
-        }
-        out.writeVInt(size);
-        boolean hasSorts = sorts != null;
-        out.writeBoolean(hasSorts);
-        if (hasSorts) {
-            out.writeVInt(sorts.size());
-            for (BytesReference sort : sorts) {
-                out.writeBytesReference(sort);
-            }
-        }
-        boolean hasStats = stats != null;
-        out.writeBoolean(hasStats);
-        if (hasStats) {
-            out.writeVInt(stats.size());
-            for (String stat : stats) {
-                out.writeString(stat);
-            }
-        }
-        boolean hasSuggestBuilder = suggestBuilder != null;
-        out.writeBoolean(hasSuggestBuilder);
-        if (hasSuggestBuilder) {
-            out.writeBytesReference(suggestBuilder);
-        }
-        out.writeVInt(terminateAfter);
-        out.writeLong(timeoutInMillis);
-        out.writeBoolean(trackScores);
-        out.writeOptionalBoolean(version);
-        boolean hasExt = ext != null;
-        out.writeBoolean(hasExt);
-        if (hasExt) {
-            out.writeBytesReference(ext);
-        }
-    }
-
-    @Override
-    public int hashCode() {
-        return Objects.hash(aggregations, explain, fetchSourceContext, fieldDataFields, fieldNames, from,
-                highlightBuilder, indexBoost, innerHitsBuilder, minScore, postQueryBuilder, queryBuilder, rescoreBuilders, scriptFields,
-                size, sorts, stats, suggestBuilder, terminateAfter, timeoutInMillis, trackScores, version);
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (obj == null) {
-            return false;
-        }
-        if (obj.getClass() != getClass()) {
-            return false;
-        }
-        SearchSourceBuilder other = (SearchSourceBuilder) obj;
-        return Objects.equals(aggregations, other.aggregations)
-                && Objects.equals(explain, other.explain)
-                && Objects.equals(fetchSourceContext, other.fetchSourceContext)
-                && Objects.equals(fieldDataFields, other.fieldDataFields)
-                && Objects.equals(fieldNames, other.fieldNames)
-                && Objects.equals(from, other.from)
-                && Objects.equals(highlightBuilder, other.highlightBuilder)
-                && Objects.equals(indexBoost, other.indexBoost)
-                && Objects.equals(innerHitsBuilder, other.innerHitsBuilder)
-                && Objects.equals(minScore, other.minScore)
-                && Objects.equals(postQueryBuilder, other.postQueryBuilder)
-                && Objects.equals(queryBuilder, other.queryBuilder)
-                && Objects.equals(rescoreBuilders, other.rescoreBuilders)
-                && Objects.equals(scriptFields, other.scriptFields)
-                && Objects.equals(size, other.size)
-                && Objects.equals(sorts, other.sorts)
-                && Objects.equals(stats, other.stats)
-                && Objects.equals(suggestBuilder, other.suggestBuilder)
-                && Objects.equals(terminateAfter, other.terminateAfter)
-                && Objects.equals(timeoutInMillis, other.timeoutInMillis)
-                && Objects.equals(trackScores, other.trackScores)
-                && Objects.equals(version, other.version);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java b/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
index f552292..e1b98c4 100644
--- a/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
@@ -19,10 +19,10 @@
 
 package org.elasticsearch.search.dfs;
 
-import com.carrotsearch.hppc.ObjectObjectHashMap;
 import com.carrotsearch.hppc.ObjectHashSet;
+import com.carrotsearch.hppc.ObjectObjectHashMap;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.google.common.collect.ImmutableMap;
+
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -39,6 +39,8 @@ import java.util.Collection;
 import java.util.Iterator;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+
 /**
  *
  */
@@ -46,7 +48,7 @@ public class DfsPhase implements SearchPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return ImmutableMap.of();
+        return emptyMap();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
index e8e2e0e..be3798e 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.fetch;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.search.DocIdSet;
@@ -64,6 +63,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.xcontent.XContentFactory.contentBuilder;
 
 /**
@@ -82,12 +82,12 @@ public class FetchPhase implements SearchPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
+        Map<String, SearchParseElement> parseElements = new HashMap<>();
         parseElements.put("fields", new FieldsParseElement());
         for (FetchSubPhase fetchSubPhase : fetchSubPhases) {
             parseElements.putAll(fetchSubPhase.parseElements());
         }
-        return parseElements.build();
+        return unmodifiableMap(parseElements);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java b/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
index 981408b..237f435 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/FetchSubPhaseContext.java
@@ -33,7 +33,7 @@ public class FetchSubPhaseContext {
     /**
      * Set if this phase should be executed at all.
      */
-    public void setHitExecutionNeeded(boolean hitExecutionNeeded) {
+    void setHitExecutionNeeded(boolean hitExecutionNeeded) {
         this.hitExecutionNeeded = hitExecutionNeeded;
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java
index 1c0eeaa..42eecb5 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.search.fetch.explain;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Explanation;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.FetchPhaseExecutionException;
@@ -30,6 +29,8 @@ import org.elasticsearch.search.rescore.RescoreSearchContext;
 import java.io.IOException;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+
 /**
  *
  */
@@ -37,7 +38,7 @@ public class ExplainFetchSubPhase implements FetchSubPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return ImmutableMap.of("explain", new ExplainParseElement());
+        return singletonMap("explain", new ExplainParseElement());
     }
 
     @Override
@@ -59,7 +60,7 @@ public class ExplainFetchSubPhase implements FetchSubPhase {
         try {
             final int topLevelDocId = hitContext.hit().docId();
             Explanation explanation = context.searcher().explain(context.query(), topLevelDocId);
-            
+
             for (RescoreSearchContext rescore : context.rescore()) {
                 explanation = rescore.rescorer().explain(topLevelDocId, context, rescore, explanation);
             }
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java
index 1ec0a98..c74ef7b 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java
@@ -18,16 +18,13 @@
  */
 package org.elasticsearch.search.fetch.fielddata;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.index.fielddata.AtomicFieldData;
 import org.elasticsearch.index.fielddata.ScriptDocValues;
-import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.FetchSubPhase;
-import org.elasticsearch.search.fetch.FetchSubPhaseContext;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHitField;
 import org.elasticsearch.search.internal.SearchContext;
@@ -36,6 +33,8 @@ import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * Query sub phase which pulls data from field data (using the cache if
  * available, building it if not).
@@ -64,10 +63,10 @@ public class FieldDataFieldsFetchSubPhase implements FetchSubPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
-        parseElements.put("fielddata_fields", new FieldDataFieldsParseElement())
-                .put("fielddataFields", new FieldDataFieldsParseElement());
-        return parseElements.build();
+        Map<String, SearchParseElement> parseElements = new HashMap<>();
+        parseElements.put("fielddata_fields", new FieldDataFieldsParseElement());
+        parseElements.put("fielddataFields", new FieldDataFieldsParseElement());
+        return unmodifiableMap(parseElements);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
index 7941e17..a14fdfe 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.search.fetch.innerhits;
 
 import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
@@ -43,16 +42,12 @@ public class InnerHitsBuilder implements ToXContent {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject("inner_hits");
-        innerXContent(builder, params);
-        return builder.endObject();
-    }
-
-    public void innerXContent(XContentBuilder builder, Params params) throws IOException {
         for (Map.Entry<String, InnerHitsHolder> entry : innerHits.entrySet()) {
             builder.startObject(entry.getKey());
             entry.getValue().toXContent(builder, params);
             builder.endObject();
         }
+        return builder.endObject();
     }
 
     /**
@@ -266,12 +261,187 @@ public class InnerHitsBuilder implements ToXContent {
             return this;
         }
 
-        public BytesReference highlighter() {
+        public HighlightBuilder highlightBuilder() {
             return sourceBuilder().highlighter();
         }
 
-        public InnerHit highlighter(HighlightBuilder highlightBuilder) {
-            sourceBuilder().highlighter(highlightBuilder);
+        /**
+         * Adds a field to be highlighted with default fragment size of 100 characters, and
+         * default number of fragments of 5.
+         *
+         * @param name The field to highlight
+         */
+        public InnerHit addHighlightedField(String name) {
+            highlightBuilder().field(name);
+            return this;
+        }
+
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters), and
+         * default number of fragments of 5.
+         *
+         * @param name         The field to highlight
+         * @param fragmentSize The size of a fragment in characters
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize) {
+            highlightBuilder().field(name, fragmentSize);
+            return this;
+        }
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters), and
+         * a provided (maximum) number of fragments.
+         *
+         * @param name              The field to highlight
+         * @param fragmentSize      The size of a fragment in characters
+         * @param numberOfFragments The (maximum) number of fragments
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize, int numberOfFragments) {
+            highlightBuilder().field(name, fragmentSize, numberOfFragments);
+            return this;
+        }
+
+        /**
+         * Adds a field to be highlighted with a provided fragment size (in characters),
+         * a provided (maximum) number of fragments and an offset for the highlight.
+         *
+         * @param name              The field to highlight
+         * @param fragmentSize      The size of a fragment in characters
+         * @param numberOfFragments The (maximum) number of fragments
+         */
+        public InnerHit addHighlightedField(String name, int fragmentSize, int numberOfFragments,
+                                            int fragmentOffset) {
+            highlightBuilder().field(name, fragmentSize, numberOfFragments, fragmentOffset);
+            return this;
+        }
+
+        /**
+         * Adds a highlighted field.
+         */
+        public InnerHit addHighlightedField(HighlightBuilder.Field field) {
+            highlightBuilder().field(field);
+            return this;
+        }
+
+        /**
+         * Set a tag scheme that encapsulates a built in pre and post tags. The allows schemes
+         * are <tt>styled</tt> and <tt>default</tt>.
+         *
+         * @param schemaName The tag scheme name
+         */
+        public InnerHit setHighlighterTagsSchema(String schemaName) {
+            highlightBuilder().tagsSchema(schemaName);
+            return this;
+        }
+
+        public InnerHit setHighlighterFragmentSize(Integer fragmentSize) {
+            highlightBuilder().fragmentSize(fragmentSize);
+            return this;
+        }
+
+        public InnerHit setHighlighterNumOfFragments(Integer numOfFragments) {
+            highlightBuilder().numOfFragments(numOfFragments);
+            return this;
+        }
+
+        public InnerHit setHighlighterFilter(Boolean highlightFilter) {
+            highlightBuilder().highlightFilter(highlightFilter);
+            return this;
+        }
+
+        /**
+         * The encoder to set for highlighting
+         */
+        public InnerHit setHighlighterEncoder(String encoder) {
+            highlightBuilder().encoder(encoder);
+            return this;
+        }
+
+        /**
+         * Explicitly set the pre tags that will be used for highlighting.
+         */
+        public InnerHit setHighlighterPreTags(String... preTags) {
+            highlightBuilder().preTags(preTags);
+            return this;
+        }
+
+        /**
+         * Explicitly set the post tags that will be used for highlighting.
+         */
+        public InnerHit setHighlighterPostTags(String... postTags) {
+            highlightBuilder().postTags(postTags);
+            return this;
+        }
+
+        /**
+         * The order of fragments per field. By default, ordered by the order in the
+         * highlighted text. Can be <tt>score</tt>, which then it will be ordered
+         * by score of the fragments.
+         */
+        public InnerHit setHighlighterOrder(String order) {
+            highlightBuilder().order(order);
+            return this;
+        }
+
+        public InnerHit setHighlighterRequireFieldMatch(boolean requireFieldMatch) {
+            highlightBuilder().requireFieldMatch(requireFieldMatch);
+            return this;
+        }
+
+        public InnerHit setHighlighterBoundaryMaxScan(Integer boundaryMaxScan) {
+            highlightBuilder().boundaryMaxScan(boundaryMaxScan);
+            return this;
+        }
+
+        public InnerHit setHighlighterBoundaryChars(char[] boundaryChars) {
+            highlightBuilder().boundaryChars(boundaryChars);
+            return this;
+        }
+
+        /**
+         * The highlighter type to use.
+         */
+        public InnerHit setHighlighterType(String type) {
+            highlightBuilder().highlighterType(type);
+            return this;
+        }
+
+        public InnerHit setHighlighterFragmenter(String fragmenter) {
+            highlightBuilder().fragmenter(fragmenter);
+            return this;
+        }
+
+        /**
+         * Sets a query to be used for highlighting all fields instead of the search query.
+         */
+        public InnerHit setHighlighterQuery(QueryBuilder highlightQuery) {
+            highlightBuilder().highlightQuery(highlightQuery);
+            return this;
+        }
+
+        /**
+         * Sets the size of the fragment to return from the beginning of the field if there are no matches to
+         * highlight and the field doesn't also define noMatchSize.
+         *
+         * @param noMatchSize integer to set or null to leave out of request.  default is null.
+         * @return this builder for chaining
+         */
+        public InnerHit setHighlighterNoMatchSize(Integer noMatchSize) {
+            highlightBuilder().noMatchSize(noMatchSize);
+            return this;
+        }
+
+        /**
+         * Sets the maximum number of phrases the fvh will consider if the field doesn't also define phraseLimit.
+         */
+        public InnerHit setHighlighterPhraseLimit(Integer phraseLimit) {
+            highlightBuilder().phraseLimit(phraseLimit);
+            return this;
+        }
+
+        public InnerHit setHighlighterOptions(Map<String, Object> options) {
+            highlightBuilder().options(options);
             return this;
         }
 
@@ -290,8 +460,24 @@ public class InnerHitsBuilder implements ToXContent {
             return this;
         }
 
-        public InnerHit innerHits(InnerHitsBuilder innerHitsBuilder) {
-            sourceBuilder().innerHits(innerHitsBuilder);
+
+
+
+        /**
+         * Adds a nested inner hit definition that collects inner hits for hits
+         * on this inner hit level.
+         */
+        public InnerHit addNestedInnerHits(String name, String path, InnerHit innerHit) {
+            sourceBuilder().innerHitsBuilder().addNestedInnerHits(name, path, innerHit);
+            return this;
+        }
+
+        /**
+         * Adds a nested inner hit definition that collects inner hits for hits
+         * on this inner hit level.
+         */
+        public InnerHit addParentChildInnerHits(String name, String type, InnerHit innerHit) {
+            sourceBuilder().innerHitsBuilder().addParentChildInnerHits(name, type, innerHit);
             return this;
         }
 
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java
index 2a36797..3557e55 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java
@@ -19,11 +19,9 @@
 
 package org.elasticsearch.search.fetch.innerhits;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
-import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.search.SearchParseElement;
@@ -43,32 +41,24 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+
 /**
  */
 public class InnerHitsFetchSubPhase implements FetchSubPhase {
-
-    private final SortParseElement sortParseElement;
-    private final FetchSourceParseElement sourceParseElement;
-    private final HighlighterParseElement highlighterParseElement;
-    private final FieldDataFieldsParseElement fieldDataFieldsParseElement;
-    private final ScriptFieldsParseElement scriptFieldsParseElement;
+    private final Map<String, ? extends SearchParseElement> parseElements;
 
     private FetchPhase fetchPhase;
 
     @Inject
     public InnerHitsFetchSubPhase(SortParseElement sortParseElement, FetchSourceParseElement sourceParseElement, HighlighterParseElement highlighterParseElement, FieldDataFieldsParseElement fieldDataFieldsParseElement, ScriptFieldsParseElement scriptFieldsParseElement) {
-        this.sortParseElement = sortParseElement;
-        this.sourceParseElement = sourceParseElement;
-        this.highlighterParseElement = highlighterParseElement;
-        this.fieldDataFieldsParseElement = fieldDataFieldsParseElement;
-        this.scriptFieldsParseElement = scriptFieldsParseElement;
+        parseElements = singletonMap("inner_hits", new InnerHitsParseElement(sortParseElement, sourceParseElement, highlighterParseElement,
+                fieldDataFieldsParseElement, scriptFieldsParseElement));
     }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return ImmutableMap.of("inner_hits", new InnerHitsParseElement(
-                sortParseElement, sourceParseElement, highlighterParseElement, fieldDataFieldsParseElement, scriptFieldsParseElement
-        ));
+        return parseElements;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
index 2824bc1..de5294f 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.search.fetch.matchedqueries;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.TwoPhaseIterator;
@@ -35,6 +34,8 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+
 /**
  *
  */
@@ -42,7 +43,7 @@ public class MatchedQueriesFetchSubPhase implements FetchSubPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return ImmutableMap.of();
+        return emptyMap();
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java
index 05ec51e..8abcdfb 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java
@@ -18,10 +18,6 @@
  */
 package org.elasticsearch.search.fetch.script;
 
-import com.google.common.collect.ImmutableMap;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.script.LeafSearchScript;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchParseElement;
@@ -38,21 +34,23 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
 public class ScriptFieldsFetchSubPhase implements FetchSubPhase {
-
-    @Inject
-    public ScriptFieldsFetchSubPhase() {
+    private static final Map<String, SearchParseElement> PARSE_ELEMENTS;
+    static {
+        Map<String, SearchParseElement> parseElements = new HashMap<>();
+        parseElements.put("script_fields", new ScriptFieldsParseElement());
+        parseElements.put("scriptFields", new ScriptFieldsParseElement());
+        PARSE_ELEMENTS = unmodifiableMap(parseElements);
     }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
-        parseElements.put("script_fields", new ScriptFieldsParseElement())
-                .put("scriptFields", new ScriptFieldsParseElement());
-        return parseElements.build();
+        return PARSE_ELEMENTS;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
index ae0a71d..9db7aea 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceContext.java
@@ -19,30 +19,20 @@
 
 package org.elasticsearch.search.fetch.source;
 
+import org.elasticsearch.Version;
 import org.elasticsearch.common.Booleans;
-import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.ParsingException;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.rest.RestRequest;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.List;
 
 /**
  */
-public class FetchSourceContext implements Streamable, ToXContent {
-
-    public static final ParseField INCLUDES_FIELD = new ParseField("includes", "include");
-    public static final ParseField EXCLUDES_FIELD = new ParseField("excludes", "exclude");
+public class FetchSourceContext implements Streamable {
 
     public static final FetchSourceContext FETCH_SOURCE = new FetchSourceContext(true);
     public static final FetchSourceContext DO_NOT_FETCH_SOURCE = new FetchSourceContext(false);
@@ -51,11 +41,6 @@ public class FetchSourceContext implements Streamable, ToXContent {
     private String[] includes;
     private String[] excludes;
 
-    public static FetchSourceContext parse(XContentParser parser, QueryParseContext context) throws IOException {
-        FetchSourceContext fetchSourceContext = new FetchSourceContext();
-        fetchSourceContext.fromXContent(parser, context);
-        return fetchSourceContext;
-    }
 
     FetchSourceContext() {
 
@@ -187,86 +172,6 @@ public class FetchSourceContext implements Streamable, ToXContent {
         return null;
     }
 
-    public void fromXContent(XContentParser parser, QueryParseContext context) throws IOException {
-        XContentParser.Token token = parser.currentToken();
-        boolean fetchSource = true;
-        String[] includes = Strings.EMPTY_ARRAY;
-        String[] excludes = Strings.EMPTY_ARRAY;
-        if (token == XContentParser.Token.VALUE_BOOLEAN) {
-            fetchSource = parser.booleanValue();
-        } else if (token == XContentParser.Token.VALUE_STRING) {
-            includes = new String[]{parser.text()};
-        } else if (token == XContentParser.Token.START_ARRAY) {
-            ArrayList<String> list = new ArrayList<>();
-            while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                list.add(parser.text());
-            }
-            includes = list.toArray(new String[list.size()]);
-        } else if (token == XContentParser.Token.START_OBJECT) {
-            String currentFieldName = null;
-            while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-                if (token == XContentParser.Token.FIELD_NAME) {
-                    currentFieldName = parser.currentName();
-                } else if (token == XContentParser.Token.START_ARRAY) {
-                    if (context.parseFieldMatcher().match(currentFieldName, INCLUDES_FIELD)) {
-                        List<String> includesList = new ArrayList<>();
-                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            if (token == XContentParser.Token.VALUE_STRING) {
-                                includesList.add(parser.text());
-                            } else {
-                                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                        parser.getTokenLocation());
-                            }
-                        }
-                        includes = includesList.toArray(new String[includesList.size()]);
-                    } else if (context.parseFieldMatcher().match(currentFieldName, EXCLUDES_FIELD)) {
-                        List<String> excludesList = new ArrayList<>();
-                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            if (token == XContentParser.Token.VALUE_STRING) {
-                                excludesList.add(parser.text());
-                            } else {
-                                throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                        parser.getTokenLocation());
-                            }
-                        }
-                        excludes = excludesList.toArray(new String[excludesList.size()]);
-                    } else {
-                        throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                                parser.getTokenLocation());
-                    }
-                } else if (token == XContentParser.Token.VALUE_STRING) {
-                    if (context.parseFieldMatcher().match(currentFieldName, INCLUDES_FIELD)) {
-                        includes = new String[] {parser.text()};
-                    } else if (context.parseFieldMatcher().match(currentFieldName, EXCLUDES_FIELD)) {
-                        excludes = new String[] {parser.text()};
-                    }
-                } else {
-                    throw new ParsingException(parser.getTokenLocation(), "Unknown key for a " + token + " in [" + currentFieldName + "].",
-                            parser.getTokenLocation());
-                }
-            }
-        } else {
-            throw new ParsingException(parser.getTokenLocation(), "Expected one of [" + XContentParser.Token.VALUE_BOOLEAN + ", "
-                    + XContentParser.Token.START_OBJECT + "] but found [" + token + "]", parser.getTokenLocation());
-        }
-        this.fetchSource = fetchSource;
-        this.includes = includes;
-        this.excludes = excludes;
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        if (fetchSource) {
-            builder.startObject();
-            builder.array(INCLUDES_FIELD.getPreferredName(), includes);
-            builder.array(EXCLUDES_FIELD.getPreferredName(), excludes);
-            builder.endObject();
-        } else {
-            builder.value(false);
-        }
-        return builder;
-    }
-
     @Override
     public void readFrom(StreamInput in) throws IOException {
         fetchSource = in.readBoolean();
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java
index 445d680..1ed4738 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java
@@ -19,12 +19,9 @@
 
 package org.elasticsearch.search.fetch.source;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.FetchSubPhase;
 import org.elasticsearch.search.internal.InternalSearchHit;
@@ -34,20 +31,16 @@ import org.elasticsearch.search.lookup.SourceLookup;
 import java.io.IOException;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+
 /**
  */
 public class FetchSourceSubPhase implements FetchSubPhase {
-
-    @Inject
-    public FetchSourceSubPhase() {
-
-    }
+    private static final Map<String, SearchParseElement> PARSE_ELEMENTS = singletonMap("_source", new FetchSourceParseElement());
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
-        parseElements.put("_source", new FetchSourceParseElement());
-        return parseElements.build();
+        return PARSE_ELEMENTS;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java b/core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java
index 6a5264d..ec36b78 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/version/VersionFetchSubPhase.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.search.fetch.version;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchException;
@@ -33,14 +32,17 @@ import org.elasticsearch.search.internal.SearchContext;
 import java.io.IOException;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+
 /**
  *
  */
 public class VersionFetchSubPhase implements FetchSubPhase {
+    private static final Map<String, ? extends SearchParseElement> PARSE_ELEMENTS = singletonMap("version", new VersionParseElement());
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return ImmutableMap.of("version", new VersionParseElement());
+        return PARSE_ELEMENTS;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
index 7f1e19b..695598e 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java
@@ -227,9 +227,9 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * Set to true to cause a field to be highlighted only if a query matches that field.
-     * Default is false meaning that terms are highlighted on all requested fields regardless
-     * if the query matches specifically on them.
+     * Set to true to cause a field to be highlighted only if a query matches that field. 
+     * Default is false meaning that terms are highlighted on all requested fields regardless 
+     * if the query matches specifically on them. 
      */
     public HighlightBuilder requireFieldMatch(boolean requireFieldMatch) {
         this.requireFieldMatch = requireFieldMatch;
@@ -237,7 +237,7 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting
+     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting 
      * controls how far to look for boundary characters, and defaults to 20.
      */
     public HighlightBuilder boundaryMaxScan(Integer boundaryMaxScan) {
@@ -246,8 +246,8 @@ public class HighlightBuilder implements ToXContent {
     }
 
     /**
-     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting
-     * defines what constitutes a boundary for highlighting. It’s a single string with
+     * When using the highlighterType <tt>fast-vector-highlighter</tt> this setting 
+     * defines what constitutes a boundary for highlighting. It’s a single string with 
      * each boundary character defined in it. It defaults to .,!? \t\n
      */
     public HighlightBuilder boundaryChars(char[] boundaryChars) {
@@ -258,7 +258,7 @@ public class HighlightBuilder implements ToXContent {
     /**
      * Set type of highlighter to use. Supported types
      * are <tt>highlighter</tt>, <tt>fast-vector-highlighter</tt> and <tt>postings-highlighter</tt>.
-     * The default option selected is dependent on the mappings defined for your index.
+     * The default option selected is dependent on the mappings defined for your index. 
      * Details of the different highlighter types are covered in the reference guide.
      */
     public HighlightBuilder highlighterType(String highlighterType) {
@@ -334,13 +334,6 @@ public class HighlightBuilder implements ToXContent {
     @Override
     public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject("highlight");
-        innerXContent(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-
-    public void innerXContent(XContentBuilder builder, Params params) throws IOException {
         if (tagsSchema != null) {
             builder.field("tags_schema", tagsSchema);
         }
@@ -472,6 +465,8 @@ public class HighlightBuilder implements ToXContent {
                 builder.endObject();
             }
         }
+        builder.endObject();
+        return builder;
     }
 
     public static class Field {
diff --git a/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java b/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
index 96e1988..5352af7 100644
--- a/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.highlight;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -40,12 +39,15 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+
 /**
  *
  */
 public class HighlightPhase extends AbstractComponent implements FetchSubPhase {
-
     private static final List<String> STANDARD_HIGHLIGHTERS_BY_PRECEDENCE = Arrays.asList("fvh", "postings", "plain");
+    private static final Map<String, ? extends SearchParseElement> PARSE_ELEMENTS = singletonMap("highlight",
+            new HighlighterParseElement());
 
     private final Highlighters highlighters;
 
@@ -57,7 +59,7 @@ public class HighlightPhase extends AbstractComponent implements FetchSubPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        return ImmutableMap.of("highlight", new HighlighterParseElement());
+        return PARSE_ELEMENTS;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
index d95b1ae..64d7659 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
@@ -193,7 +193,7 @@ public class DefaultSearchContext extends SearchContext {
         }
 
         // initialize the filtering alias based on the provided filters
-        aliasFilter = indexService.aliasesService().aliasFilter(request.filteringAliases());
+        aliasFilter = indexService.aliasFilter(request.filteringAliases());
 
         if (query() == null) {
             parsedQuery(ParsedQuery.parsedMatchAllQuery());
diff --git a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
index 7c334a9..c1194f1 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.internal;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.ElasticsearchParseException;
@@ -52,6 +51,9 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.lucene.Lucene.readExplanation;
 import static org.elasticsearch.common.lucene.Lucene.writeExplanation;
 import static org.elasticsearch.search.SearchShardTarget.readSearchShardTarget;
@@ -78,7 +80,7 @@ public class InternalSearchHit implements SearchHit {
 
     private BytesReference source;
 
-    private Map<String, SearchHitField> fields = ImmutableMap.of();
+    private Map<String, SearchHitField> fields = emptyMap();
 
     private Map<String, HighlightField> highlightFields = null;
 
@@ -292,15 +294,12 @@ public class InternalSearchHit implements SearchHit {
 
     @Override
     public Map<String, SearchHitField> fields() {
-        if (fields == null) {
-            return ImmutableMap.of();
-        }
-        return fields;
+        return fields == null ? emptyMap() : fields;
     }
 
     // returns the fields without handling null cases
     public Map<String, SearchHitField> fieldsOrNull() {
-        return this.fields;
+        return fields;
     }
 
     @Override
@@ -318,10 +317,7 @@ public class InternalSearchHit implements SearchHit {
 
     @Override
     public Map<String, HighlightField> highlightFields() {
-        if (highlightFields == null) {
-            return ImmutableMap.of();
-        }
-        return this.highlightFields;
+        return highlightFields == null ? emptyMap() : highlightFields;
     }
 
     @Override
@@ -574,69 +570,32 @@ public class InternalSearchHit implements SearchHit {
         }
         int size = in.readVInt();
         if (size == 0) {
-            fields = ImmutableMap.of();
+            fields = emptyMap();
         } else if (size == 1) {
             SearchHitField hitField = readSearchHitField(in);
-            fields = ImmutableMap.of(hitField.name(), hitField);
-        } else if (size == 2) {
-            SearchHitField hitField1 = readSearchHitField(in);
-            SearchHitField hitField2 = readSearchHitField(in);
-            fields = ImmutableMap.of(hitField1.name(), hitField1, hitField2.name(), hitField2);
-        } else if (size == 3) {
-            SearchHitField hitField1 = readSearchHitField(in);
-            SearchHitField hitField2 = readSearchHitField(in);
-            SearchHitField hitField3 = readSearchHitField(in);
-            fields = ImmutableMap.of(hitField1.name(), hitField1, hitField2.name(), hitField2, hitField3.name(), hitField3);
-        } else if (size == 4) {
-            SearchHitField hitField1 = readSearchHitField(in);
-            SearchHitField hitField2 = readSearchHitField(in);
-            SearchHitField hitField3 = readSearchHitField(in);
-            SearchHitField hitField4 = readSearchHitField(in);
-            fields = ImmutableMap.of(hitField1.name(), hitField1, hitField2.name(), hitField2, hitField3.name(), hitField3, hitField4.name(), hitField4);
-        } else if (size == 5) {
-            SearchHitField hitField1 = readSearchHitField(in);
-            SearchHitField hitField2 = readSearchHitField(in);
-            SearchHitField hitField3 = readSearchHitField(in);
-            SearchHitField hitField4 = readSearchHitField(in);
-            SearchHitField hitField5 = readSearchHitField(in);
-            fields = ImmutableMap.of(hitField1.name(), hitField1, hitField2.name(), hitField2, hitField3.name(), hitField3, hitField4.name(), hitField4, hitField5.name(), hitField5);
+            fields = singletonMap(hitField.name(), hitField);
         } else {
-            ImmutableMap.Builder<String, SearchHitField> builder = ImmutableMap.builder();
+            Map<String, SearchHitField> fields = new HashMap<>();
             for (int i = 0; i < size; i++) {
                 SearchHitField hitField = readSearchHitField(in);
-                builder.put(hitField.name(), hitField);
+                fields.put(hitField.name(), hitField);
             }
-            fields = builder.build();
+            this.fields = unmodifiableMap(fields);
         }
 
         size = in.readVInt();
         if (size == 0) {
-            highlightFields = ImmutableMap.of();
+            highlightFields = emptyMap();
         } else if (size == 1) {
             HighlightField field = readHighlightField(in);
-            highlightFields = ImmutableMap.of(field.name(), field);
-        } else if (size == 2) {
-            HighlightField field1 = readHighlightField(in);
-            HighlightField field2 = readHighlightField(in);
-            highlightFields = ImmutableMap.of(field1.name(), field1, field2.name(), field2);
-        } else if (size == 3) {
-            HighlightField field1 = readHighlightField(in);
-            HighlightField field2 = readHighlightField(in);
-            HighlightField field3 = readHighlightField(in);
-            highlightFields = ImmutableMap.of(field1.name(), field1, field2.name(), field2, field3.name(), field3);
-        } else if (size == 4) {
-            HighlightField field1 = readHighlightField(in);
-            HighlightField field2 = readHighlightField(in);
-            HighlightField field3 = readHighlightField(in);
-            HighlightField field4 = readHighlightField(in);
-            highlightFields = ImmutableMap.of(field1.name(), field1, field2.name(), field2, field3.name(), field3, field4.name(), field4);
+            highlightFields = singletonMap(field.name(), field);
         } else {
-            ImmutableMap.Builder<String, HighlightField> builder = ImmutableMap.builder();
+            Map<String, HighlightField> highlightFields = new HashMap<>();
             for (int i = 0; i < size; i++) {
                 HighlightField field = readHighlightField(in);
-                builder.put(field.name(), field);
+                highlightFields.put(field.name(), field);
             }
-            highlightFields = builder.build();
+            this.highlightFields = unmodifiableMap(highlightFields);
         }
 
         size = in.readVInt();
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
index 8f0cd98..ca8c074 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java
@@ -31,7 +31,6 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 
@@ -66,7 +65,9 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     private Scroll scroll;
     private String[] types = Strings.EMPTY_ARRAY;
     private String[] filteringAliases;
-    private SearchSourceBuilder source;
+    private BytesReference source;
+    private BytesReference extraSource;
+    private BytesReference templateSource;
     private Template template;
     private Boolean requestCache;
     private long nowInMillis;
@@ -78,6 +79,8 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
                             String[] filteringAliases, long nowInMillis) {
         this(shardRouting.shardId(), numberOfShards, searchRequest.searchType(),
                 searchRequest.source(), searchRequest.types(), searchRequest.requestCache());
+        this.extraSource = searchRequest.extraSource();
+        this.templateSource = searchRequest.templateSource();
         this.template = searchRequest.template();
         this.scroll = searchRequest.scroll();
         this.filteringAliases = filteringAliases;
@@ -95,8 +98,8 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
         this.filteringAliases = filteringAliases;
     }
 
-    public ShardSearchLocalRequest(ShardId shardId, int numberOfShards, SearchType searchType, SearchSourceBuilder source, String[] types,
-            Boolean requestCache) {
+    public ShardSearchLocalRequest(ShardId shardId, int numberOfShards, SearchType searchType,
+                                   BytesReference source, String[] types, Boolean requestCache) {
         this.index = shardId.getIndex();
         this.shardId = shardId.id();
         this.numberOfShards = numberOfShards;
@@ -122,16 +125,21 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     }
 
     @Override
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return source;
     }
 
     @Override
-    public void source(SearchSourceBuilder source) {
+    public void source(BytesReference source) {
         this.source = source;
     }
 
     @Override
+    public BytesReference extraSource() {
+        return extraSource;
+    }
+
+    @Override
     public int numberOfShards() {
         return numberOfShards;
     }
@@ -150,12 +158,18 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
     public long nowInMillis() {
         return nowInMillis;
     }
+
     @Override
     public Template template() {
         return template;
     }
 
     @Override
+    public BytesReference templateSource() {
+        return templateSource;
+    }
+
+    @Override
     public Boolean requestCache() {
         return requestCache;
     }
@@ -174,13 +188,18 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
         if (in.readBoolean()) {
             scroll = readScroll(in);
         }
-        if (in.readBoolean()) {
-            source = SearchSourceBuilder.readSearchSourceFrom(in);
-        }
+
+        source = in.readBytesReference();
+        extraSource = in.readBytesReference();
+
         types = in.readStringArray();
         filteringAliases = in.readStringArray();
         nowInMillis = in.readVLong();
-        template = in.readOptionalStreamable(new Template());
+
+        templateSource = in.readBytesReference();
+        if (in.readBoolean()) {
+            template = Template.readTemplate(in);
+        }
         requestCache = in.readOptionalBoolean();
     }
 
@@ -197,20 +216,20 @@ public class ShardSearchLocalRequest extends ContextAndHeaderHolder implements S
             out.writeBoolean(true);
             scroll.writeTo(out);
         }
-        if (source == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            source.writeTo(out);
-
-        }
+        out.writeBytesReference(source);
+        out.writeBytesReference(extraSource);
         out.writeStringArray(types);
         out.writeStringArrayNullable(filteringAliases);
         if (!asKey) {
             out.writeVLong(nowInMillis);
         }
 
-        out.writeOptionalStreamable(template);
+        out.writeBytesReference(templateSource);
+        boolean hasTemplate = template != null;
+        out.writeBoolean(hasTemplate);
+        if (hasTemplate) {
+            template.writeTo(out);
+        }
         out.writeOptionalBoolean(requestCache);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
index fb631b0..6d9734f 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java
@@ -20,11 +20,12 @@
 package org.elasticsearch.search.internal;
 
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.HasContext;
 import org.elasticsearch.common.HasContextAndHeaders;
+import org.elasticsearch.common.HasHeaders;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
 import java.io.IOException;
 
@@ -41,9 +42,11 @@ public interface ShardSearchRequest extends HasContextAndHeaders {
 
     String[] types();
 
-    SearchSourceBuilder source();
+    BytesReference source();
 
-    void source(SearchSourceBuilder source);
+    void source(BytesReference source);
+
+    BytesReference extraSource();
 
     int numberOfShards();
 
@@ -55,6 +58,8 @@ public interface ShardSearchRequest extends HasContextAndHeaders {
 
     Template template();
 
+    BytesReference templateSource();
+
     Boolean requestCache();
 
     Scroll scroll();
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
index 279d9d6..e7b1e2f 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java
@@ -30,7 +30,6 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.search.Scroll;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.transport.TransportRequest;
 
 import java.io.IOException;
@@ -88,16 +87,21 @@ public class ShardSearchTransportRequest extends TransportRequest implements Sha
     }
 
     @Override
-    public SearchSourceBuilder source() {
+    public BytesReference source() {
         return shardSearchLocalRequest.source();
     }
 
     @Override
-    public void source(SearchSourceBuilder source) {
+    public void source(BytesReference source) {
         shardSearchLocalRequest.source(source);
     }
 
     @Override
+    public BytesReference extraSource() {
+        return shardSearchLocalRequest.extraSource();
+    }
+
+    @Override
     public int numberOfShards() {
         return shardSearchLocalRequest.numberOfShards();
     }
@@ -116,12 +120,18 @@ public class ShardSearchTransportRequest extends TransportRequest implements Sha
     public long nowInMillis() {
         return shardSearchLocalRequest.nowInMillis();
     }
+
     @Override
     public Template template() {
         return shardSearchLocalRequest.template();
     }
 
     @Override
+    public BytesReference templateSource() {
+        return shardSearchLocalRequest.templateSource();
+    }
+
+    @Override
     public Boolean requestCache() {
         return shardSearchLocalRequest.requestCache();
     }
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
index 0150ef7..485c690 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/IndexLookup.java
@@ -18,12 +18,24 @@
  */
 package org.elasticsearch.search.lookup;
 
-import com.google.common.collect.ImmutableMap.Builder;
-
 import org.apache.lucene.index.LeafReaderContext;
 
-public class IndexLookup {
+import java.util.HashMap;
+import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
+
+public class IndexLookup {
+    public static final Map<String, Object> NAMES;
+    static {
+        Map<String, Object> names = new HashMap<>();
+        names.put("_FREQUENCIES", IndexLookup.FLAG_FREQUENCIES);
+        names.put("_POSITIONS", IndexLookup.FLAG_POSITIONS);
+        names.put("_OFFSETS", IndexLookup.FLAG_OFFSETS);
+        names.put("_PAYLOADS", IndexLookup.FLAG_PAYLOADS);
+        names.put("_CACHE", IndexLookup.FLAG_CACHE);
+        NAMES = unmodifiableMap(names);
+    }
     /**
      * Flag to pass to {@link IndexField#get(Object, int)} if you require
      * offsets in the returned {@link IndexFieldTerm}.
@@ -55,15 +67,7 @@ public class IndexLookup {
      */
     public static final int FLAG_CACHE = 32;
 
-    public IndexLookup(Builder<String, Object> builder) {
-        builder.put("_FREQUENCIES", IndexLookup.FLAG_FREQUENCIES);
-        builder.put("_POSITIONS", IndexLookup.FLAG_POSITIONS);
-        builder.put("_OFFSETS", IndexLookup.FLAG_OFFSETS);
-        builder.put("_PAYLOADS", IndexLookup.FLAG_PAYLOADS);
-        builder.put("_CACHE", IndexLookup.FLAG_CACHE);
-    }
-
-    public LeafIndexLookup getLeafIndexLookup(LeafReaderContext context) {
+    public static LeafIndexLookup getLeafIndexLookup(LeafReaderContext context) {
         return new LeafIndexLookup(context);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java
index d45067f..e5295e8 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/LeafFieldsLookup.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.search.lookup;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReader;
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.Nullable;
@@ -33,6 +32,8 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.singletonMap;
+
 /**
  *
  */
@@ -148,7 +149,7 @@ public class LeafFieldsLookup implements Map {
             try {
                 reader.document(docId, fieldVisitor);
                 fieldVisitor.postProcess(data.fieldType());
-                data.fields(ImmutableMap.of(name, fieldVisitor.fields().get(data.fieldType().names().indexName())));
+                data.fields(singletonMap(name, fieldVisitor.fields().get(data.fieldType().names().indexName())));
             } catch (IOException e) {
                 throw new ElasticsearchParseException("failed to load field [{}]", e, name);
             }
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java
index 091a368..c9438fd 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/SearchLookup.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.lookup;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.index.fielddata.IndexFieldDataService;
@@ -36,17 +35,10 @@ public class SearchLookup {
 
     final FieldsLookup fieldsLookup;
 
-    final IndexLookup indexLookup;
-
-    final ImmutableMap<String, Object> asMap;
-
     public SearchLookup(MapperService mapperService, IndexFieldDataService fieldDataService, @Nullable String[] types) {
-        ImmutableMap.Builder<String, Object> builder = ImmutableMap.builder();
         docMap = new DocLookup(mapperService, fieldDataService, types);
         sourceLookup = new SourceLookup();
         fieldsLookup = new FieldsLookup(mapperService, types);
-        indexLookup = new IndexLookup(builder);
-        asMap = builder.build();
     }
 
     public LeafSearchLookup getLeafSearchLookup(LeafReaderContext context) {
@@ -54,8 +46,8 @@ public class SearchLookup {
                 docMap.getLeafDocLookup(context),
                 sourceLookup,
                 fieldsLookup.getLeafFieldsLookup(context),
-                indexLookup.getLeafIndexLookup(context),
-                asMap);
+                IndexLookup.getLeafIndexLookup(context),
+                IndexLookup.NAMES);
     }
 
     public DocLookup doc() {
diff --git a/core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java b/core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java
index c0ca2eb..910f5da 100644
--- a/core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java
+++ b/core/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.search.lookup;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.LeafReaderContext;
 import org.elasticsearch.ElasticsearchParseException;
@@ -35,6 +33,8 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import static java.util.Collections.emptyMap;
+
 /**
  *
  */
@@ -71,7 +71,7 @@ public class SourceLookup implements Map {
             reader.document(docId, sourceFieldVisitor);
             BytesReference source = sourceFieldVisitor.source();
             if (source == null) {
-                this.source = ImmutableMap.of();
+                this.source = emptyMap();
                 this.sourceContentType = null;
             } else {
                 Tuple<XContentType, Map<String, Object>> tuple = sourceAsMapAndType(source);
diff --git a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
index d347897..c4aa23f 100644
--- a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
@@ -19,12 +19,28 @@
 
 package org.elasticsearch.search.query;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.MinDocQuery;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TimeLimitingCollector;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopDocsCollector;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.search.TotalHitCountCollector;
+import org.apache.lucene.search.Weight;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.Lucene;
@@ -43,10 +59,13 @@ import org.elasticsearch.search.sort.TrackScoresParseElement;
 import org.elasticsearch.search.suggest.SuggestPhase;
 
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.Callable;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  *
  */
@@ -65,29 +84,30 @@ public class QueryPhase implements SearchPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
-        parseElements.put("from", new FromParseElement()).put("size", new SizeParseElement())
-                .put("indices_boost", new IndicesBoostParseElement())
-                .put("indicesBoost", new IndicesBoostParseElement())
-                .put("query", new QueryParseElement())
-                .put("queryBinary", new QueryBinaryParseElement())
-                .put("query_binary", new QueryBinaryParseElement())
-                .put("filter", new PostFilterParseElement()) // For bw comp reason, should be removed in version 1.1
-                .put("post_filter", new PostFilterParseElement())
-                .put("postFilter", new PostFilterParseElement())
-                .put("filterBinary", new FilterBinaryParseElement())
-                .put("filter_binary", new FilterBinaryParseElement())
-                .put("sort", new SortParseElement())
-                .put("trackScores", new TrackScoresParseElement())
-                .put("track_scores", new TrackScoresParseElement())
-                .put("min_score", new MinScoreParseElement())
-                .put("minScore", new MinScoreParseElement())
-                .put("timeout", new TimeoutParseElement())
-                .put("terminate_after", new TerminateAfterParseElement())
-                .putAll(aggregationPhase.parseElements())
-                .putAll(suggestPhase.parseElements())
-                .putAll(rescorePhase.parseElements());
-        return parseElements.build();
+        Map<String, SearchParseElement> parseElements = new HashMap<>();
+        parseElements.put("from", new FromParseElement());
+        parseElements.put("size", new SizeParseElement());
+        parseElements.put("indices_boost", new IndicesBoostParseElement());
+        parseElements.put("indicesBoost", new IndicesBoostParseElement());
+        parseElements.put("query", new QueryParseElement());
+        parseElements.put("queryBinary", new QueryBinaryParseElement());
+        parseElements.put("query_binary", new QueryBinaryParseElement());
+        parseElements.put("filter", new PostFilterParseElement()); // For bw comp reason, should be removed in version 1.1
+        parseElements.put("post_filter", new PostFilterParseElement());
+        parseElements.put("postFilter", new PostFilterParseElement());
+        parseElements.put("filterBinary", new FilterBinaryParseElement());
+        parseElements.put("filter_binary", new FilterBinaryParseElement());
+        parseElements.put("sort", new SortParseElement());
+        parseElements.put("trackScores", new TrackScoresParseElement());
+        parseElements.put("track_scores", new TrackScoresParseElement());
+        parseElements.put("min_score", new MinScoreParseElement());
+        parseElements.put("minScore", new MinScoreParseElement());
+        parseElements.put("timeout", new TimeoutParseElement());
+        parseElements.put("terminate_after", new TerminateAfterParseElement());
+        parseElements.putAll(aggregationPhase.parseElements());
+        parseElements.putAll(suggestPhase.parseElements());
+        parseElements.putAll(rescorePhase.parseElements());
+        return unmodifiableMap(parseElements);
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java b/core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java
index 48d8407..d1592aa 100644
--- a/core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java
+++ b/core/src/main/java/org/elasticsearch/search/rescore/RescorePhase.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.search.rescore;
 
-import com.google.common.collect.ImmutableMap;
-import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -33,10 +31,13 @@ import org.elasticsearch.search.internal.SearchContext;
 import java.io.IOException;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+
 /**
  */
 public class RescorePhase extends AbstractComponent implements SearchPhase {
-    
+    private static final Map<String, SearchParseElement> PARSE_ELEMENTS = singletonMap("rescore", new RescoreParseElement());
+
     @Inject
     public RescorePhase(Settings settings) {
         super(settings);
@@ -44,9 +45,7 @@ public class RescorePhase extends AbstractComponent implements SearchPhase {
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
-        parseElements.put("rescore", new RescoreParseElement());
-        return parseElements.build();
+        return PARSE_ELEMENTS;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
index f03ca7c..8454537 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.search.sort;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.QueryWrapperFilter;
 import org.apache.lucene.search.Sort;
@@ -34,17 +32,19 @@ import org.elasticsearch.index.fielddata.IndexFieldData;
 import org.elasticsearch.index.fielddata.IndexFieldData.XFieldComparatorSource.Nested;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.core.LongFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.support.NestedInnerQueryParseSupport;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.SearchParseException;
 import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.internal.SubSearchContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
+
+import static java.util.Collections.unmodifiableMap;
 
 /**
  *
@@ -62,16 +62,16 @@ public class SortParseElement implements SearchParseElement {
     public static final String SCORE_FIELD_NAME = "_score";
     public static final String DOC_FIELD_NAME = "_doc";
 
-    private final ImmutableMap<String, SortParser> parsers;
+    private static final Map<String, SortParser> PARSERS;
 
-    public SortParseElement() {
-        ImmutableMap.Builder<String, SortParser> builder = ImmutableMap.builder();
-        addParser(builder, new ScriptSortParser());
-        addParser(builder, new GeoDistanceSortParser());
-        this.parsers = builder.build();
+    static {
+        Map<String, SortParser> parsers = new HashMap<>();
+        addParser(parsers, new ScriptSortParser());
+        addParser(parsers, new GeoDistanceSortParser());
+        PARSERS = unmodifiableMap(parsers);
     }
 
-    private void addParser(ImmutableMap.Builder<String, SortParser> parsers, SortParser parser) {
+    private static void addParser(Map<String, SortParser> parsers, SortParser parser) {
         for (String name : parser.names()) {
             parsers.put(name, parser);
         }
@@ -140,8 +140,8 @@ public class SortParseElement implements SearchParseElement {
                     }
                     addSortField(context, sortFields, fieldName, reverse, unmappedType, missing, sortMode, nestedFilterParseHelper);
                 } else {
-                    if (parsers.containsKey(fieldName)) {
-                        sortFields.add(parsers.get(fieldName).parse(parser, context));
+                    if (PARSERS.containsKey(fieldName)) {
+                        sortFields.add(PARSERS.get(fieldName).parse(parser, context));
                     } else {
                         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                             if (token == XContentParser.Token.FIELD_NAME) {
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java b/core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java
index 58a4502..541efa7 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/SuggestPhase.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.search.suggest;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.elasticsearch.ElasticsearchException;
@@ -38,23 +37,24 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+
 /**
  */
 public class SuggestPhase extends AbstractComponent implements SearchPhase {
-
+    private final Map<String, SearchParseElement> parseElements;
     private final SuggestParseElement parseElement;
 
     @Inject
     public SuggestPhase(Settings settings, SuggestParseElement suggestParseElement) {
         super(settings);
         this.parseElement = suggestParseElement;
+        parseElements = singletonMap("suggest", parseElement);
     }
 
     @Override
     public Map<String, ? extends SearchParseElement> parseElements() {
-        ImmutableMap.Builder<String, SearchParseElement> parseElements = ImmutableMap.builder();
-        parseElements.put("suggest", parseElement);
-        return parseElements.build();
+        return parseElements;
     }
 
     public SuggestParseElement parseElement() {
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java b/core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java
index bbb3340..447b3fd 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.search.suggest.completion;
 
-import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.ImmutableMap.Builder;
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
@@ -59,6 +57,8 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+
 /**
  * This {@link PostingsFormat} is basically a T-Sink for a default postings
  * format that is used to store postings on disk fitting the lucene APIs and
@@ -75,18 +75,12 @@ public class Completion090PostingsFormat extends PostingsFormat {
     public static final int SUGGEST_VERSION_CURRENT = SUGGEST_CODEC_VERSION;
     public static final String EXTENSION = "cmp";
 
-    private final static ESLogger logger = Loggers.getLogger(Completion090PostingsFormat.class);
+    private static final ESLogger logger = Loggers.getLogger(Completion090PostingsFormat.class);
+    private static final CompletionLookupProvider LOOKUP_PROVIDER = new AnalyzingCompletionLookupProvider(true, false, true, false);
+    private static final Map<String, CompletionLookupProvider> PROVIDERS = singletonMap(LOOKUP_PROVIDER.getName(), LOOKUP_PROVIDER);
     private PostingsFormat delegatePostingsFormat;
-    private final static Map<String, CompletionLookupProvider> providers;
     private CompletionLookupProvider writeProvider;
 
-
-    static {
-        final CompletionLookupProvider provider = new AnalyzingCompletionLookupProvider(true, false, true, false);
-        final Builder<String, CompletionLookupProvider> builder = ImmutableMap.builder();
-        providers = builder.put(provider.getName(), provider).build();
-    }
-
     public Completion090PostingsFormat(PostingsFormat delegatePostingsFormat, CompletionLookupProvider provider) {
         super(CODEC_NAME);
         this.delegatePostingsFormat = delegatePostingsFormat;
@@ -173,11 +167,11 @@ public class Completion090PostingsFormat extends PostingsFormat {
             try {
                 PostingsFormat delegatePostingsFormat = PostingsFormat.forName(input.readString());
                 String providerName = input.readString();
-                CompletionLookupProvider completionLookupProvider = providers.get(providerName);
+                CompletionLookupProvider completionLookupProvider = PROVIDERS.get(providerName);
                 if (completionLookupProvider == null) {
                     throw new IllegalStateException("no provider with name [" + providerName + "] registered");
                 }
-                // TODO: we could clone the ReadState and make it always forward IOContext.MERGE to prevent unecessary heap usage? 
+                // TODO: we could clone the ReadState and make it always forward IOContext.MERGE to prevent unecessary heap usage?
                 delegateProducer = delegatePostingsFormat.fieldsProducer(state);
                 /*
                  * If we are merging we don't load the FSTs at all such that we
diff --git a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
index 724e3d4..6e7a91d 100644
--- a/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
+++ b/core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggester.java
@@ -104,7 +104,6 @@ public final class PhraseSuggester extends Suggester<PhraseSuggestionContext> {
             response.addTerm(resultEntry);
 
             final BytesRefBuilder byteSpare = new BytesRefBuilder();
-            final EarlyTerminatingCollector collector = Lucene.createExistsCollector();
             final CompiledScript collateScript = suggestion.getCollateQueryScript();
             final boolean collatePrune = (collateScript != null) && suggestion.collatePrune();
             for (int i = 0; i < checkerResult.corrections.length; i++) {
@@ -119,7 +118,7 @@ public final class PhraseSuggester extends Suggester<PhraseSuggestionContext> {
                     final ExecutableScript executable = scriptService.executable(collateScript, vars);
                     final BytesReference querySource = (BytesReference) executable.run();
                     final ParsedQuery parsedQuery = suggestion.getQueryParserService().parse(querySource);
-                    collateMatch = Lucene.exists(searcher, parsedQuery.query(), collector);
+                    collateMatch = Lucene.exists(searcher, parsedQuery.query());
                 }
                 if (!collateMatch && !collatePrune) {
                     continue;
diff --git a/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java b/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
index 1ce27f9..6e881cb 100644
--- a/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
+++ b/core/src/main/java/org/elasticsearch/search/warmer/IndexWarmersMetaData.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.search.warmer;
 
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.action.support.ToXContentToBytes;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
@@ -29,17 +27,12 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentGenerator;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -74,10 +67,10 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
     public static class Entry {
         private final String name;
         private final String[] types;
-        private final SearchSource source;
+        private final BytesReference source;
         private final Boolean requestCache;
 
-        public Entry(String name, String[] types, Boolean requestCache, SearchSource source) {
+        public Entry(String name, String[] types, Boolean requestCache, BytesReference source) {
             this.name = name;
             this.types = types == null ? Strings.EMPTY_ARRAY : types;
             this.source = source;
@@ -93,7 +86,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         }
 
         @Nullable
-        public SearchSource source() {
+        public BytesReference source() {
             return this.source;
         }
 
@@ -148,9 +141,9 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         for (int i = 0; i < entries.length; i++) {
             String name = in.readString();
             String[] types = in.readStringArray();
-            SearchSource source = null;
+            BytesReference source = null;
             if (in.readBoolean()) {
-                source = new SearchSource(in);
+                source = in.readBytesReference();
             }
             Boolean queryCache;
             queryCache = in.readOptionalBoolean();
@@ -169,7 +162,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
                 out.writeBoolean(false);
             } else {
                 out.writeBoolean(true);
-                entry.source.writeTo(out);
+                out.writeBytesReference(entry.source());
             }
             out.writeOptionalBoolean(entry.requestCache());
         }
@@ -201,7 +194,7 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
             } else if (token == XContentParser.Token.START_OBJECT) {
                 String name = currentFieldName;
                 List<String> types = new ArrayList<>(2);
-                SearchSource source = null;
+                BytesReference source = null;
                 Boolean queryCache = null;
                 while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
                     if (token == XContentParser.Token.FIELD_NAME) {
@@ -214,15 +207,12 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
                         }
                     } else if (token == XContentParser.Token.START_OBJECT) {
                         if ("source".equals(currentFieldName)) {
-                            ByteArrayOutputStream out = new ByteArrayOutputStream();
-                            try (XContentGenerator generator = XContentType.JSON.xContent().createGenerator(out)) {
-                                generator.copyCurrentStructure(parser);
-                            }
-                            source = new SearchSource(new BytesArray(out.toByteArray()));
+                            XContentBuilder builder = XContentFactory.jsonBuilder().map(parser.mapOrdered());
+                            source = builder.bytes();
                         }
                     } else if (token == XContentParser.Token.VALUE_EMBEDDED_OBJECT) {
                         if ("source".equals(currentFieldName)) {
-                            source = new SearchSource(new BytesArray(parser.binaryValue()));
+                            source = new BytesArray(parser.binaryValue());
                         }
                     } else if (token.isValue()) {
                         if ("requestCache".equals(currentFieldName) || "request_cache".equals(currentFieldName)) {
@@ -249,12 +239,22 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
     }
 
     public static void toXContent(Entry entry, XContentBuilder builder, ToXContent.Params params) throws IOException {
+        boolean binary = params.paramAsBoolean("binary", false);
         builder.startObject(entry.name(), XContentBuilder.FieldCaseConversion.NONE);
         builder.field("types", entry.types());
         if (entry.requestCache() != null) {
             builder.field("requestCache", entry.requestCache());
         }
-        builder.field("source", entry.source());
+        builder.field("source");
+        if (binary) {
+            builder.value(entry.source());
+        } else {
+            Map<String, Object> mapping;
+            try (XContentParser parser = XContentFactory.xContent(entry.source()).createParser(entry.source())) {
+                mapping = parser.mapOrdered();
+            }
+            builder.map(mapping);
+        }
         builder.endObject();
     }
 
@@ -277,78 +277,4 @@ public class IndexWarmersMetaData extends AbstractDiffable<IndexMetaData.Custom>
         }
         return new IndexWarmersMetaData(entries.toArray(new Entry[entries.size()]));
     }
-
-    public static class SearchSource extends ToXContentToBytes implements Writeable<SearchSource> {
-        private final BytesReference binary;
-        private SearchSourceBuilder cached;
-
-        public SearchSource(BytesReference bytesArray) {
-            if (bytesArray == null) {
-                throw new IllegalArgumentException("bytesArray must not be null");
-            }
-            this.binary = bytesArray;
-        }
-
-        public SearchSource(StreamInput input) throws IOException {
-            this(input.readBytesReference());
-        }
-
-        public SearchSource(SearchSourceBuilder source) {
-            try (XContentBuilder builder = XContentBuilder.builder(XContentType.JSON.xContent())) {
-                source.toXContent(builder, ToXContent.EMPTY_PARAMS);
-                binary = builder.bytes();
-            } catch (IOException ex) {
-                throw new ElasticsearchException("failed to generate XContent", ex);
-            }
-        }
-
-        public SearchSourceBuilder build(QueryParseContext ctx) throws IOException {
-            if (cached == null) {
-                try (XContentParser parser = XContentFactory.xContent(binary).createParser(binary)) {
-                    ctx.reset(parser);
-                    cached = SearchSourceBuilder.parseSearchSource(parser, ctx);
-                }
-            }
-            return cached;
-        }
-
-
-        @Override
-        public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-            if (binary == null) {
-                cached.toXContent(builder, params);
-            } else {
-                try (XContentParser parser = XContentFactory.xContent(binary).createParser(binary)) {
-                    builder.copyCurrentStructure(parser);
-                }
-            }
-            return builder;
-        }
-
-        @Override
-        public void writeTo(StreamOutput out) throws IOException {
-            out.writeBytesReference(binary);
-        }
-
-        @Override
-        public SearchSource readFrom(StreamInput in) throws IOException {
-            return new SearchSource(in);
-        }
-
-        @Override
-        public boolean equals(Object o) {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-
-            SearchSource that = (SearchSource) o;
-
-            return binary.equals(that.binary);
-
-        }
-
-        @Override
-        public int hashCode() {
-            return binary.hashCode();
-        }
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
index c4f379b..9db3ef9 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
@@ -22,7 +22,6 @@ import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.hppc.IntSet;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
-import com.google.common.collect.ImmutableMap;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionListener;
@@ -55,6 +54,7 @@ import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
 import org.elasticsearch.cluster.settings.DynamicSettings;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -94,8 +94,6 @@ import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_AUTO_EXPAND_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_CREATION_DATE;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_INDEX_UUID;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
@@ -131,8 +129,6 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
     private static final Set<String> UNMODIFIABLE_SETTINGS = unmodifiableSet(newHashSet(
             SETTING_NUMBER_OF_SHARDS,
             SETTING_VERSION_CREATED,
-            SETTING_LEGACY_ROUTING_HASH_FUNCTION,
-            SETTING_LEGACY_ROUTING_USE_TYPE,
             SETTING_INDEX_UUID,
             SETTING_CREATION_DATE));
 
@@ -232,11 +228,11 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                     MetaData.Builder mdBuilder = MetaData.builder(currentState.metaData());
                     ClusterBlocks.Builder blocks = ClusterBlocks.builder().blocks(currentState.blocks());
                     RoutingTable.Builder rtBuilder = RoutingTable.builder(currentState.routingTable());
-                    final ImmutableMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards;
+                    ImmutableOpenMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards;
                     Set<String> aliases = new HashSet<>();
                     if (!renamedIndices.isEmpty()) {
                         // We have some indices to restore
-                        ImmutableMap.Builder<ShardId, RestoreInProgress.ShardRestoreStatus> shardsBuilder = ImmutableMap.builder();
+                        ImmutableOpenMap.Builder<ShardId, RestoreInProgress.ShardRestoreStatus> shardsBuilder = ImmutableOpenMap.builder();
                         for (Map.Entry<String, String> indexEntry : renamedIndices.entrySet()) {
                             String index = indexEntry.getValue();
                             boolean partial = checkPartial(index);
@@ -311,7 +307,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                         RestoreInProgress.Entry restoreEntry = new RestoreInProgress.Entry(snapshotId, RestoreInProgress.State.INIT, Collections.unmodifiableList(new ArrayList<>(renamedIndices.keySet())), shards);
                         builder.putCustom(RestoreInProgress.TYPE, new RestoreInProgress(restoreEntry));
                     } else {
-                        shards = ImmutableMap.of();
+                        shards = ImmutableOpenMap.of();
                     }
 
                     checkAliasNameConflicts(renamedIndices, aliases);
@@ -325,8 +321,9 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                                 shards.size(), shards.size() - failedShards(shards));
                     }
 
-                    ClusterState updatedState = builder.metaData(mdBuilder).blocks(blocks).routingTable(rtBuilder).build();
-                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rtBuilder).build());
+                    RoutingTable rt = rtBuilder.build();
+                    ClusterState updatedState = builder.metaData(mdBuilder).blocks(blocks).routingTable(rt).build();
+                    RoutingAllocation.Result routingResult = allocationService.reroute(ClusterState.builder(updatedState).routingTable(rt).build());
                     return ClusterState.builder(updatedState).routingResult(routingResult).build();
                 }
 
@@ -531,7 +528,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
         clusterService.submitStateUpdateTask("update snapshot state", new ClusterStateUpdateTask() {
             private final List<UpdateIndexShardRestoreStatusRequest> drainedRequests = new ArrayList<>();
-            private Map<SnapshotId, Tuple<RestoreInfo, Map<ShardId, ShardRestoreStatus>>> batchedRestoreInfo = null;
+            private Map<SnapshotId, Tuple<RestoreInfo, ImmutableOpenMap<ShardId, ShardRestoreStatus>>> batchedRestoreInfo = null;
 
             @Override
             public ClusterState execute(ClusterState currentState) {
@@ -554,7 +551,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                     int changedCount = 0;
                     final List<RestoreInProgress.Entry> entries = new ArrayList<>();
                     for (RestoreInProgress.Entry entry : restore.entries()) {
-                        Map<ShardId, ShardRestoreStatus> shards = null;
+                        ImmutableOpenMap.Builder<ShardId, ShardRestoreStatus> shardsBuilder = null;
 
                         for (int i = 0; i < batchSize; i++) {
                             final UpdateIndexShardRestoreStatusRequest updateSnapshotState = drainedRequests.get(i);
@@ -562,17 +559,18 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
 
                             if (entry.snapshotId().equals(updateSnapshotState.snapshotId())) {
                                 logger.trace("[{}] Updating shard [{}] with status [{}]", updateSnapshotState.snapshotId(), updateSnapshotState.shardId(), updateSnapshotState.status().state());
-                                if (shards == null) {
-                                    shards = new HashMap<>(entry.shards());
+                                if (shardsBuilder == null) {
+                                    shardsBuilder = ImmutableOpenMap.builder(entry.shards());
                                 }
-                                shards.put(updateSnapshotState.shardId(), updateSnapshotState.status());
+                                shardsBuilder.put(updateSnapshotState.shardId(), updateSnapshotState.status());
                                 changedCount++;
                             }
                         }
 
-                        if (shards != null) {
+                        if (shardsBuilder != null) {
+                            ImmutableOpenMap<ShardId, ShardRestoreStatus> shards = shardsBuilder.build();
                             if (!completed(shards)) {
-                                entries.add(new RestoreInProgress.Entry(entry.snapshotId(), RestoreInProgress.State.STARTED, entry.indices(), ImmutableMap.copyOf(shards)));
+                                entries.add(new RestoreInProgress.Entry(entry.snapshotId(), RestoreInProgress.State.STARTED, entry.indices(), shards));
                             } else {
                                 logger.info("restore [{}] is done", entry.snapshotId());
                                 if (batchedRestoreInfo == null) {
@@ -609,15 +607,15 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
             @Override
             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
                 if (batchedRestoreInfo != null) {
-                    for (final Entry<SnapshotId, Tuple<RestoreInfo, Map<ShardId, ShardRestoreStatus>>> entry : batchedRestoreInfo.entrySet()) {
+                    for (final Entry<SnapshotId, Tuple<RestoreInfo, ImmutableOpenMap<ShardId, ShardRestoreStatus>>> entry : batchedRestoreInfo.entrySet()) {
                         final SnapshotId snapshotId = entry.getKey();
                         final RestoreInfo restoreInfo = entry.getValue().v1();
-                        final Map<ShardId, ShardRestoreStatus> shards = entry.getValue().v2();
+                        final ImmutableOpenMap<ShardId, ShardRestoreStatus> shards = entry.getValue().v2();
                         RoutingTable routingTable = newState.getRoutingTable();
                         final List<ShardId> waitForStarted = new ArrayList<>();
-                        for (Map.Entry<ShardId, ShardRestoreStatus> shard : shards.entrySet()) {
-                            if (shard.getValue().state() == RestoreInProgress.State.SUCCESS ) {
-                                ShardId shardId = shard.getKey();
+                        for (ObjectObjectCursor<ShardId, ShardRestoreStatus> shard : shards) {
+                            if (shard.value.state() == RestoreInProgress.State.SUCCESS ) {
+                                ShardId shardId = shard.key;
                                 ShardRouting shardRouting = findPrimaryShard(routingTable, shardId);
                                 if (shardRouting != null && !shardRouting.active()) {
                                     logger.trace("[{}][{}] waiting for the shard to start", snapshotId, shardId);
@@ -677,19 +675,19 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
         });
     }
 
-    private boolean completed(Map<ShardId, RestoreInProgress.ShardRestoreStatus> shards) {
-        for (RestoreInProgress.ShardRestoreStatus status : shards.values()) {
-            if (!status.state().completed()) {
+    private boolean completed(ImmutableOpenMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards) {
+        for (ObjectCursor<RestoreInProgress.ShardRestoreStatus> status : shards.values()) {
+            if (!status.value.state().completed()) {
                 return false;
             }
         }
         return true;
     }
 
-    private int failedShards(Map<ShardId, RestoreInProgress.ShardRestoreStatus> shards) {
+    private int failedShards(ImmutableOpenMap<ShardId, RestoreInProgress.ShardRestoreStatus> shards) {
         int failedShards = 0;
-        for (RestoreInProgress.ShardRestoreStatus status : shards.values()) {
-            if (status.state() == RestoreInProgress.State.FAILURE) {
+        for (ObjectCursor<RestoreInProgress.ShardRestoreStatus> status : shards.values()) {
+            if (status.value.state() == RestoreInProgress.State.FAILURE) {
                 failedShards++;
             }
         }
@@ -744,13 +742,13 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
             // Some indices were deleted, let's make sure all indices that we are restoring still exist
             for (RestoreInProgress.Entry entry : restore.entries()) {
                 List<ShardId> shardsToFail = null;
-                for (ImmutableMap.Entry<ShardId, ShardRestoreStatus> shard : entry.shards().entrySet()) {
-                    if (!shard.getValue().state().completed()) {
-                        if (!event.state().metaData().hasIndex(shard.getKey().getIndex())) {
+                for (ObjectObjectCursor<ShardId, ShardRestoreStatus> shard : entry.shards()) {
+                    if (!shard.value.state().completed()) {
+                        if (!event.state().metaData().hasIndex(shard.key.getIndex())) {
                             if (shardsToFail == null) {
                                 shardsToFail = new ArrayList<>();
                             }
-                            shardsToFail.add(shard.getKey());
+                            shardsToFail.add(shard.key);
                         }
                     }
                 }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
index c751895..91cf2af 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java
@@ -19,7 +19,8 @@
 
 package org.elasticsearch.snapshots;
 
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.apache.lucene.index.IndexCommit;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.cluster.ClusterChangedEvent;
@@ -30,6 +31,7 @@ import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.SnapshotsInProgress;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.io.stream.StreamInput;
@@ -67,6 +69,8 @@ import java.util.concurrent.locks.Condition;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.cluster.SnapshotsInProgress.completed;
 
 /**
@@ -91,7 +95,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
 
     private final Condition shutdownCondition = shutdownLock.newCondition();
 
-    private volatile Map<SnapshotId, SnapshotShards> shardSnapshots = ImmutableMap.of();
+    private volatile Map<SnapshotId, SnapshotShards> shardSnapshots = emptyMap();
 
     private final BlockingQueue<UpdateIndexShardSnapshotStatusRequest> updatedSnapshotStateQueue = ConcurrentCollections.newBlockingQueue();
 
@@ -210,12 +214,12 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
                 if (entry.state() == SnapshotsInProgress.State.STARTED) {
                     Map<ShardId, IndexShardSnapshotStatus> startedShards = new HashMap<>();
                     SnapshotShards snapshotShards = shardSnapshots.get(entry.snapshotId());
-                    for (Map.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shard : entry.shards().entrySet()) {
+                    for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shard : entry.shards()) {
                         // Add all new shards to start processing on
-                        if (localNodeId.equals(shard.getValue().nodeId())) {
-                            if (shard.getValue().state() == SnapshotsInProgress.State.INIT && (snapshotShards == null || !snapshotShards.shards.containsKey(shard.getKey()))) {
-                                logger.trace("[{}] - Adding shard to the queue", shard.getKey());
-                                startedShards.put(shard.getKey(), new IndexShardSnapshotStatus());
+                        if (localNodeId.equals(shard.value.nodeId())) {
+                            if (shard.value.state() == SnapshotsInProgress.State.INIT && (snapshotShards == null || !snapshotShards.shards.containsKey(shard.key))) {
+                                logger.trace("[{}] - Adding shard to the queue", shard.key);
+                                startedShards.put(shard.key, new IndexShardSnapshotStatus());
                             }
                         }
                     }
@@ -223,23 +227,23 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
                         newSnapshots.put(entry.snapshotId(), startedShards);
                         if (snapshotShards != null) {
                             // We already saw this snapshot but we need to add more started shards
-                            ImmutableMap.Builder<ShardId, IndexShardSnapshotStatus> shards = ImmutableMap.builder();
+                            Map<ShardId, IndexShardSnapshotStatus> shards = new HashMap<>();
                             // Put all shards that were already running on this node
                             shards.putAll(snapshotShards.shards);
                             // Put all newly started shards
                             shards.putAll(startedShards);
-                            survivors.put(entry.snapshotId(), new SnapshotShards(shards.build()));
+                            survivors.put(entry.snapshotId(), new SnapshotShards(unmodifiableMap(shards)));
                         } else {
                             // Brand new snapshot that we haven't seen before
-                            survivors.put(entry.snapshotId(), new SnapshotShards(ImmutableMap.copyOf(startedShards)));
+                            survivors.put(entry.snapshotId(), new SnapshotShards(unmodifiableMap(startedShards)));
                         }
                     }
                 } else if (entry.state() == SnapshotsInProgress.State.ABORTED) {
                     // Abort all running shards for this snapshot
                     SnapshotShards snapshotShards = shardSnapshots.get(entry.snapshotId());
                     if (snapshotShards != null) {
-                        for (Map.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shard : entry.shards().entrySet()) {
-                            IndexShardSnapshotStatus snapshotStatus = snapshotShards.shards.get(shard.getKey());
+                        for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shard : entry.shards()) {
+                            IndexShardSnapshotStatus snapshotStatus = snapshotShards.shards.get(shard.key);
                             if (snapshotStatus != null) {
                                 switch (snapshotStatus.stage()) {
                                     case INIT:
@@ -247,16 +251,16 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
                                         snapshotStatus.abort();
                                         break;
                                     case FINALIZE:
-                                        logger.debug("[{}] trying to cancel snapshot on shard [{}] that is finalizing, letting it finish", entry.snapshotId(), shard.getKey());
+                                        logger.debug("[{}] trying to cancel snapshot on shard [{}] that is finalizing, letting it finish", entry.snapshotId(), shard.key);
                                         break;
                                     case DONE:
-                                        logger.debug("[{}] trying to cancel snapshot on the shard [{}] that is already done, updating status on the master", entry.snapshotId(), shard.getKey());
-                                        updateIndexShardSnapshotStatus(entry.snapshotId(), shard.getKey(),
+                                        logger.debug("[{}] trying to cancel snapshot on the shard [{}] that is already done, updating status on the master", entry.snapshotId(), shard.key);
+                                        updateIndexShardSnapshotStatus(entry.snapshotId(), shard.key,
                                                 new SnapshotsInProgress.ShardSnapshotStatus(event.state().nodes().localNodeId(), SnapshotsInProgress.State.SUCCESS));
                                         break;
                                     case FAILURE:
-                                        logger.debug("[{}] trying to cancel snapshot on the shard [{}] that has already failed, updating status on the master", entry.snapshotId(), shard.getKey());
-                                        updateIndexShardSnapshotStatus(entry.snapshotId(), shard.getKey(),
+                                        logger.debug("[{}] trying to cancel snapshot on the shard [{}] that has already failed, updating status on the master", entry.snapshotId(), shard.key);
+                                        updateIndexShardSnapshotStatus(entry.snapshotId(), shard.key,
                                                 new SnapshotsInProgress.ShardSnapshotStatus(event.state().nodes().localNodeId(), SnapshotsInProgress.State.FAILED, snapshotStatus.failure()));
                                         break;
                                     default:
@@ -273,7 +277,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
         // If startup of these shards fails later, we don't want to try starting these shards again
         shutdownLock.lock();
         try {
-            shardSnapshots = ImmutableMap.copyOf(survivors);
+            shardSnapshots = unmodifiableMap(survivors);
             if (shardSnapshots.isEmpty()) {
                 // Notify all waiting threads that no more snapshots
                 shutdownCondition.signalAll();
@@ -368,7 +372,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
             if (snapshot.state() == SnapshotsInProgress.State.STARTED || snapshot.state() == SnapshotsInProgress.State.ABORTED) {
                 Map<ShardId, IndexShardSnapshotStatus> localShards = currentSnapshotShards(snapshot.snapshotId());
                 if (localShards != null) {
-                    Map<ShardId, SnapshotsInProgress.ShardSnapshotStatus> masterShards = snapshot.shards();
+                    ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> masterShards = snapshot.shards();
                     for(Map.Entry<ShardId, IndexShardSnapshotStatus> localShard : localShards.entrySet()) {
                         ShardId shardId = localShard.getKey();
                         IndexShardSnapshotStatus localShardStatus = localShard.getValue();
@@ -400,7 +404,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
     private static class SnapshotShards {
         private final Map<ShardId, IndexShardSnapshotStatus> shards;
 
-        private SnapshotShards(ImmutableMap<ShardId, IndexShardSnapshotStatus> shards) {
+        private SnapshotShards(Map<ShardId, IndexShardSnapshotStatus> shards) {
             this.shards = shards;
         }
     }
@@ -518,7 +522,7 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
                     int changedCount = 0;
                     final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();
                     for (SnapshotsInProgress.Entry entry : snapshots.entries()) {
-                        final Map<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = new HashMap<>();
+                        ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = ImmutableOpenMap.builder();
                         boolean updated = false;
 
                         for (int i = 0; i < batchSize; i++) {
@@ -538,11 +542,11 @@ public class SnapshotShardsService extends AbstractLifecycleComponent<SnapshotSh
 
                         if (updated) {
                             if (completed(shards.values()) == false) {
-                                entries.add(new SnapshotsInProgress.Entry(entry, ImmutableMap.copyOf(shards)));
+                                entries.add(new SnapshotsInProgress.Entry(entry, shards.build()));
                             } else {
                                 // Snapshot is finished - mark it as done
                                 // TODO: Add PARTIAL_SUCCESS status?
-                                SnapshotsInProgress.Entry updatedEntry = new SnapshotsInProgress.Entry(entry, SnapshotsInProgress.State.SUCCESS, ImmutableMap.copyOf(shards));
+                                SnapshotsInProgress.Entry updatedEntry = new SnapshotsInProgress.Entry(entry, SnapshotsInProgress.State.SUCCESS, shards.build());
                                 entries.add(updatedEntry);
                                 // Finalize snapshot in the repository
                                 snapshotsService.endSnapshot(updatedEntry);
diff --git a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
index d89d260..0b4d041 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java
@@ -19,15 +19,26 @@
 
 package org.elasticsearch.snapshots;
 
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.cursors.ObjectCursor;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateListener;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
+import org.elasticsearch.cluster.SnapshotsInProgress;
 import org.elasticsearch.cluster.SnapshotsInProgress.ShardSnapshotStatus;
 import org.elasticsearch.cluster.SnapshotsInProgress.State;
-import org.elasticsearch.cluster.metadata.*;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.metadata.RepositoriesMetaData;
+import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.IndexRoutingTable;
@@ -35,6 +46,7 @@ import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
@@ -50,9 +62,18 @@ import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.threadpool.ThreadPool;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.CopyOnWriteArrayList;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.cluster.SnapshotsInProgress.completed;
 
 /**
@@ -226,7 +247,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
         }
         validate(new SnapshotId(request.repository(), request.name()));
     }
-    
+
     private static void validate(SnapshotId snapshotId) {
         String name = snapshotId.getSnapshot();
         if (!Strings.hasLength(name)) {
@@ -297,7 +318,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                     for (SnapshotsInProgress.Entry entry : snapshots.entries()) {
                         if (entry.snapshotId().equals(snapshot.snapshotId())) {
                             // Replace the snapshot that was just created
-                            ImmutableMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = shards(currentState, entry.indices());
+                            ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = shards(currentState, entry.indices());
                             if (!partial) {
                                 Tuple<Set<String>, Set<String>> indicesWithMissingShards = indicesWithMissingShards(shards, currentState.metaData());
                                 Set<String> missing = indicesWithMissingShards.v1();
@@ -447,9 +468,9 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param snapshotId snapshot id
      * @return map of shard id to snapshot status
      */
-    public ImmutableMap<ShardId, IndexShardSnapshotStatus> snapshotShards(SnapshotId snapshotId) throws IOException {
+    public Map<ShardId, IndexShardSnapshotStatus> snapshotShards(SnapshotId snapshotId) throws IOException {
         validate(snapshotId);
-        ImmutableMap.Builder<ShardId, IndexShardSnapshotStatus> shardStatusBuilder = ImmutableMap.builder();
+        Map<ShardId, IndexShardSnapshotStatus> shardStatus = new HashMap<>();
         Repository repository = repositoriesService.repository(snapshotId.getRepository());
         IndexShardRepository indexShardRepository = repositoriesService.indexShardRepository(snapshotId.getRepository());
         Snapshot snapshot = repository.readSnapshot(snapshotId);
@@ -465,15 +486,15 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         IndexShardSnapshotStatus shardSnapshotStatus = new IndexShardSnapshotStatus();
                         shardSnapshotStatus.updateStage(IndexShardSnapshotStatus.Stage.FAILURE);
                         shardSnapshotStatus.failure(shardFailure.reason());
-                        shardStatusBuilder.put(shardId, shardSnapshotStatus);
+                        shardStatus.put(shardId, shardSnapshotStatus);
                     } else {
                         IndexShardSnapshotStatus shardSnapshotStatus = indexShardRepository.snapshotStatus(snapshotId, snapshot.version(), shardId);
-                        shardStatusBuilder.put(shardId, shardSnapshotStatus);
+                        shardStatus.put(shardId, shardSnapshotStatus);
                     }
                 }
             }
         }
-        return shardStatusBuilder.build();
+        return unmodifiableMap(shardStatus);
     }
 
 
@@ -525,23 +546,23 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         SnapshotsInProgress.Entry updatedSnapshot = snapshot;
                         boolean snapshotChanged = false;
                         if (snapshot.state() == State.STARTED || snapshot.state() == State.ABORTED) {
-                            ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableMap.builder();
-                            for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards().entrySet()) {
-                                ShardSnapshotStatus shardStatus = shardEntry.getValue();
+                            ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();
+                            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards()) {
+                                ShardSnapshotStatus shardStatus = shardEntry.value;
                                 if (!shardStatus.state().completed() && shardStatus.nodeId() != null) {
                                     if (nodes.nodeExists(shardStatus.nodeId())) {
-                                        shards.put(shardEntry);
+                                        shards.put(shardEntry.key, shardEntry.value);
                                     } else {
                                         // TODO: Restart snapshot on another node?
                                         snapshotChanged = true;
-                                        logger.warn("failing snapshot of shard [{}] on closed node [{}]", shardEntry.getKey(), shardStatus.nodeId());
-                                        shards.put(shardEntry.getKey(), new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "node shutdown"));
+                                        logger.warn("failing snapshot of shard [{}] on closed node [{}]", shardEntry.key, shardStatus.nodeId());
+                                        shards.put(shardEntry.key, new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "node shutdown"));
                                     }
                                 }
                             }
                             if (snapshotChanged) {
                                 changed = true;
-                                ImmutableMap<ShardId, ShardSnapshotStatus> shardsMap = shards.build();
+                                ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardsMap = shards.build();
                                 if (!snapshot.state().completed() && completed(shardsMap.values())) {
                                     updatedSnapshot = new SnapshotsInProgress.Entry(snapshot, State.SUCCESS, shardsMap);
                                     endSnapshot(updatedSnapshot);
@@ -596,7 +617,7 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         for (final SnapshotsInProgress.Entry snapshot : snapshots.entries()) {
                             SnapshotsInProgress.Entry updatedSnapshot = snapshot;
                             if (snapshot.state() == State.STARTED) {
-                                Map<ShardId, ShardSnapshotStatus> shards = processWaitingShards(snapshot.shards(), routingTable);
+                                ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards = processWaitingShards(snapshot.shards(), routingTable);
                                 if (shards != null) {
                                     changed = true;
                                     if (!snapshot.state().completed() && completed(shards.values())) {
@@ -625,13 +646,14 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
         }
     }
 
-    private Map<ShardId, ShardSnapshotStatus> processWaitingShards(Map<ShardId, ShardSnapshotStatus> snapshotShards, RoutingTable routingTable) {
+    private ImmutableOpenMap<ShardId, ShardSnapshotStatus> processWaitingShards(
+            ImmutableOpenMap<ShardId, ShardSnapshotStatus> snapshotShards, RoutingTable routingTable) {
         boolean snapshotChanged = false;
-        ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableMap.builder();
-        for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> shardEntry : snapshotShards.entrySet()) {
-            ShardSnapshotStatus shardStatus = shardEntry.getValue();
+        ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();
+        for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshotShards) {
+            ShardSnapshotStatus shardStatus = shardEntry.value;
+            ShardId shardId = shardEntry.key;
             if (shardStatus.state() == State.WAITING) {
-                ShardId shardId = shardEntry.getKey();
                 IndexRoutingTable indexShardRoutingTable = routingTable.index(shardId.getIndex());
                 if (indexShardRoutingTable != null) {
                     IndexShardRoutingTable shardRouting = indexShardRoutingTable.shard(shardId.id());
@@ -639,22 +661,22 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                         if (shardRouting.primaryShard().started()) {
                             // Shard that we were waiting for has started on a node, let's process it
                             snapshotChanged = true;
-                            logger.trace("starting shard that we were waiting for [{}] on node [{}]", shardEntry.getKey(), shardStatus.nodeId());
-                            shards.put(shardEntry.getKey(), new ShardSnapshotStatus(shardRouting.primaryShard().currentNodeId()));
+                            logger.trace("starting shard that we were waiting for [{}] on node [{}]", shardId, shardStatus.nodeId());
+                            shards.put(shardId, new ShardSnapshotStatus(shardRouting.primaryShard().currentNodeId()));
                             continue;
                         } else if (shardRouting.primaryShard().initializing() || shardRouting.primaryShard().relocating()) {
                             // Shard that we were waiting for hasn't started yet or still relocating - will continue to wait
-                            shards.put(shardEntry);
+                            shards.put(shardId, shardStatus);
                             continue;
                         }
                     }
                 }
                 // Shard that we were waiting for went into unassigned state or disappeared - giving up
                 snapshotChanged = true;
-                logger.warn("failing snapshot of shard [{}] on unassigned shard [{}]", shardEntry.getKey(), shardStatus.nodeId());
-                shards.put(shardEntry.getKey(), new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "shard is unassigned"));
+                logger.warn("failing snapshot of shard [{}] on unassigned shard [{}]", shardId, shardStatus.nodeId());
+                shards.put(shardId, new ShardSnapshotStatus(shardStatus.nodeId(), State.FAILED, "shard is unassigned"));
             } else {
-                shards.put(shardEntry);
+                shards.put(shardId, shardStatus);
             }
         }
         if (snapshotChanged) {
@@ -669,10 +691,10 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
         if (curr != null) {
             for (SnapshotsInProgress.Entry entry : curr.entries()) {
                 if (entry.state() == State.STARTED && !entry.waitingIndices().isEmpty()) {
-                    for (String index : entry.waitingIndices().keySet()) {
-                        if (event.indexRoutingTableChanged(index)) {
-                            IndexRoutingTable indexShardRoutingTable = event.state().getRoutingTable().index(index);
-                            for (ShardId shardId : entry.waitingIndices().get(index)) {
+                    for (ObjectCursor<String> index : entry.waitingIndices().keys()) {
+                        if (event.indexRoutingTableChanged(index.value)) {
+                            IndexRoutingTable indexShardRoutingTable = event.state().getRoutingTable().index(index.value);
+                            for (ShardId shardId : entry.waitingIndices().get(index.value)) {
                                 ShardRouting shardRouting = indexShardRoutingTable.shard(shardId.id()).primaryShard();
                                 if (shardRouting != null && (shardRouting.started() || shardRouting.unassigned())) {
                                     return true;
@@ -699,8 +721,8 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 return true;
             }
             for (DiscoveryNode node : event.nodesDelta().removedNodes()) {
-                for (ShardSnapshotStatus shardStatus : snapshot.shards().values()) {
-                    if (!shardStatus.state().completed() && node.getId().equals(shardStatus.nodeId())) {
+                for (ObjectCursor<ShardSnapshotStatus> shardStatus : snapshot.shards().values()) {
+                    if (!shardStatus.value.state().completed() && node.getId().equals(shardStatus.value.nodeId())) {
                         // At least one shard was running on the removed node - we need to fail it
                         return true;
                     }
@@ -716,15 +738,15 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param shards list of shard statuses
      * @return list of failed and closed indices
      */
-    private Tuple<Set<String>, Set<String>> indicesWithMissingShards(Map<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards, MetaData metaData) {
+    private Tuple<Set<String>, Set<String>> indicesWithMissingShards(ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards, MetaData metaData) {
         Set<String> missing = new HashSet<>();
         Set<String> closed = new HashSet<>();
-        for (Map.Entry<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards.entrySet()) {
-            if (entry.getValue().state() == State.MISSING) {
-                if (metaData.hasIndex(entry.getKey().getIndex()) && metaData.index(entry.getKey().getIndex()).getState() == IndexMetaData.State.CLOSE) {
-                    closed.add(entry.getKey().getIndex());
+        for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards) {
+            if (entry.value.state() == State.MISSING) {
+                if (metaData.hasIndex(entry.key.getIndex()) && metaData.index(entry.key.getIndex()).getState() == IndexMetaData.State.CLOSE) {
+                    closed.add(entry.key.getIndex());
                 } else {
-                    missing.add(entry.getKey().getIndex());
+                    missing.add(entry.key.getIndex());
                 }
             }
         }
@@ -761,9 +783,9 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                     logger.trace("[{}] finalizing snapshot in repository, state: [{}], failure[{}]", snapshotId, entry.state(), failure);
                     ArrayList<ShardSearchFailure> failures = new ArrayList<>();
                     ArrayList<SnapshotShardFailure> shardFailures = new ArrayList<>();
-                    for (Map.Entry<ShardId, ShardSnapshotStatus> shardStatus : entry.shards().entrySet()) {
-                        ShardId shardId = shardStatus.getKey();
-                        ShardSnapshotStatus status = shardStatus.getValue();
+                    for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardStatus : entry.shards()) {
+                        ShardId shardId = shardStatus.key;
+                        ShardSnapshotStatus status = shardStatus.value;
                         if (status.state().failed()) {
                             failures.add(new ShardSearchFailure(status.reason(), new SearchShardTarget(status.nodeId(), shardId.getIndex(), shardId.id())));
                             shardFailures.add(new SnapshotShardFailure(status.nodeId(), shardId.getIndex(), shardId.id(), status.reason()));
@@ -864,16 +886,16 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                 } else {
                     // This snapshot is currently running - stopping shards first
                     waitForSnapshot = true;
-                    Map<ShardId, ShardSnapshotStatus> shards;
+                    ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards;
                     if (snapshot.state() == State.STARTED && snapshot.shards() != null) {
                         // snapshot is currently running - stop started shards
-                        ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shardsBuilder = ImmutableMap.builder();
-                        for (ImmutableMap.Entry<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards().entrySet()) {
-                            ShardSnapshotStatus status = shardEntry.getValue();
+                        ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shardsBuilder = ImmutableOpenMap.builder();
+                        for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shardEntry : snapshot.shards()) {
+                            ShardSnapshotStatus status = shardEntry.value;
                             if (!status.state().completed()) {
-                                shardsBuilder.put(shardEntry.getKey(), new ShardSnapshotStatus(status.nodeId(), State.ABORTED));
+                                shardsBuilder.put(shardEntry.key, new ShardSnapshotStatus(status.nodeId(), State.ABORTED));
                             } else {
-                                shardsBuilder.put(shardEntry.getKey(), status);
+                                shardsBuilder.put(shardEntry.key, status);
                             }
                         }
                         shards = shardsBuilder.build();
@@ -884,9 +906,10 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
                     } else {
                         boolean hasUncompletedShards = false;
                         // Cleanup in case a node gone missing and snapshot wasn't updated for some reason
-                        for (ShardSnapshotStatus shardStatus : snapshot.shards().values()) {
+                        for (ObjectCursor<ShardSnapshotStatus> shardStatus : snapshot.shards().values()) {
                             // Check if we still have shard running on existing nodes
-                            if (shardStatus.state().completed() == false && shardStatus.nodeId() != null && currentState.nodes().get(shardStatus.nodeId()) != null) {
+                            if (shardStatus.value.state().completed() == false && shardStatus.value.nodeId() != null
+                                    && currentState.nodes().get(shardStatus.value.nodeId()) != null) {
                                 hasUncompletedShards = true;
                                 break;
                             }
@@ -991,8 +1014,8 @@ public class SnapshotsService extends AbstractLifecycleComponent<SnapshotsServic
      * @param indices      list of indices to be snapshotted
      * @return list of shard to be included into current snapshot
      */
-    private ImmutableMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards(ClusterState clusterState, List<String> indices) {
-        ImmutableMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> builder = ImmutableMap.builder();
+    private ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards(ClusterState clusterState, List<String> indices) {
+        ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> builder = ImmutableOpenMap.builder();
         MetaData metaData = clusterState.metaData();
         for (String index : indices) {
             IndexMetaData indexMetaData = metaData.index(index);
diff --git a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
index 85de5f6..d20f600 100644
--- a/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
+++ b/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.threadpool;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.Counter;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.component.AbstractComponent;
@@ -57,7 +56,7 @@ import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
 
-import static org.elasticsearch.common.collect.MapBuilder.newMapBuilder;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.unit.SizeValue.parseSizeValue;
 import static org.elasticsearch.common.unit.TimeValue.timeValueMinutes;
@@ -89,7 +88,7 @@ public class ThreadPool extends AbstractComponent {
 
     public static final String THREADPOOL_GROUP = "threadpool.";
 
-    private volatile ImmutableMap<String, ExecutorHolder> executors;
+    private volatile Map<String, ExecutorHolder> executors;
 
     private final Map<String, Settings> defaultExecutorTypeSettings;
 
@@ -118,26 +117,38 @@ public class ThreadPool extends AbstractComponent {
         int availableProcessors = EsExecutors.boundedNumberOfProcessors(settings);
         int halfProcMaxAt5 = Math.min(((availableProcessors + 1) / 2), 5);
         int halfProcMaxAt10 = Math.min(((availableProcessors + 1) / 2), 10);
-        defaultExecutorTypeSettings = ImmutableMap.<String, Settings>builder()
-                .put(Names.GENERIC, settingsBuilder().put("type", "cached").put("keep_alive", "30s").build())
-                .put(Names.INDEX, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 200).build())
-                .put(Names.BULK, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 50).build())
-                .put(Names.GET, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build())
-                .put(Names.SEARCH, settingsBuilder().put("type", "fixed").put("size", ((availableProcessors * 3) / 2) + 1).put("queue_size", 1000).build())
-                .put(Names.SUGGEST, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build())
-                .put(Names.PERCOLATE, settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build())
-                .put(Names.MANAGEMENT, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", 5).build())
-                // no queue as this means clients will need to handle rejections on listener queue even if the operation succeeded
-                // the assumption here is that the listeners should be very lightweight on the listeners side
-                .put(Names.LISTENER, settingsBuilder().put("type", "fixed").put("size", halfProcMaxAt10).build())
-                .put(Names.FLUSH, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build())
-                .put(Names.REFRESH, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt10).build())
-                .put(Names.WARMER, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build())
-                .put(Names.SNAPSHOT, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build())
-                .put(Names.OPTIMIZE, settingsBuilder().put("type", "fixed").put("size", 1).build())
-                .put(Names.FETCH_SHARD_STARTED, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", availableProcessors * 2).build())
-                .put(Names.FETCH_SHARD_STORE, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", availableProcessors * 2).build())
-                .build();
+        Map<String, Settings> defaultExecutorTypeSettings = new HashMap<>();
+        defaultExecutorTypeSettings.put(Names.GENERIC, settingsBuilder().put("type", "cached").put("keep_alive", "30s").build());
+        defaultExecutorTypeSettings.put(Names.INDEX,
+                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 200).build());
+        defaultExecutorTypeSettings.put(Names.BULK,
+                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 50).build());
+        defaultExecutorTypeSettings.put(Names.GET,
+                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build());
+        defaultExecutorTypeSettings.put(Names.SEARCH,
+                settingsBuilder().put("type", "fixed").put("size", ((availableProcessors * 3) / 2) + 1).put("queue_size", 1000).build());
+        defaultExecutorTypeSettings.put(Names.SUGGEST,
+                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build());
+        defaultExecutorTypeSettings.put(Names.PERCOLATE,
+                settingsBuilder().put("type", "fixed").put("size", availableProcessors).put("queue_size", 1000).build());
+        defaultExecutorTypeSettings  .put(Names.MANAGEMENT, settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", 5).build());
+        // no queue as this means clients will need to handle rejections on listener queue even if the operation succeeded
+        // the assumption here is that the listeners should be very lightweight on the listeners side
+        defaultExecutorTypeSettings.put(Names.LISTENER, settingsBuilder().put("type", "fixed").put("size", halfProcMaxAt10).build());
+        defaultExecutorTypeSettings.put(Names.FLUSH,
+                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build());
+        defaultExecutorTypeSettings.put(Names.REFRESH,
+                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt10).build());
+        defaultExecutorTypeSettings.put(Names.WARMER,
+                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build());
+        defaultExecutorTypeSettings.put(Names.SNAPSHOT,
+                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", halfProcMaxAt5).build());
+        defaultExecutorTypeSettings.put(Names.OPTIMIZE, settingsBuilder().put("type", "fixed").put("size", 1).build());
+        defaultExecutorTypeSettings.put(Names.FETCH_SHARD_STARTED,
+                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", availableProcessors * 2).build());
+        defaultExecutorTypeSettings.put(Names.FETCH_SHARD_STORE,
+                settingsBuilder().put("type", "scaling").put("keep_alive", "5m").put("size", availableProcessors * 2).build());
+        this.defaultExecutorTypeSettings = unmodifiableMap(defaultExecutorTypeSettings);
 
         Map<String, ExecutorHolder> executors = new HashMap<>();
         for (Map.Entry<String, Settings> executor : defaultExecutorTypeSettings.entrySet()) {
@@ -156,7 +167,7 @@ public class ThreadPool extends AbstractComponent {
         if (!executors.get(Names.GENERIC).info.getType().equals("cached")) {
             throw new IllegalArgumentException("generic thread pool must be of type cached");
         }
-        this.executors = ImmutableMap.copyOf(executors);
+        this.executors = unmodifiableMap(executors);
         this.scheduler = new ScheduledThreadPoolExecutor(1, EsExecutors.daemonThreadFactory(settings, "scheduler"), new EsAbortPolicy());
         this.scheduler.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);
         this.scheduler.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);
@@ -446,7 +457,9 @@ public class ThreadPool extends AbstractComponent {
             ExecutorHolder oldExecutorHolder = executors.get(executor.getKey());
             ExecutorHolder newExecutorHolder = rebuild(executor.getKey(), oldExecutorHolder, updatedSettings, executor.getValue());
             if (!oldExecutorHolder.equals(newExecutorHolder)) {
-                executors = newMapBuilder(executors).put(executor.getKey(), newExecutorHolder).immutableMap();
+                Map<String, ExecutorHolder> newExecutors = new HashMap<>(executors);
+                newExecutors.put(executor.getKey(), newExecutorHolder);
+                executors = unmodifiableMap(newExecutors);
                 if (!oldExecutorHolder.executor().equals(newExecutorHolder.executor()) && oldExecutorHolder.executor() instanceof EsThreadPoolExecutor) {
                     retiredExecutors.add(oldExecutorHolder);
                     ((EsThreadPoolExecutor) oldExecutorHolder.executor()).shutdown(new ExecutorShutdownListener(oldExecutorHolder));
@@ -466,7 +479,9 @@ public class ThreadPool extends AbstractComponent {
             // case the settings contains a thread pool not defined in the initial settings in the constructor. The if
             // statement will then fail and so this prevents the addition of new thread groups at runtime, which is desired.
             if (!newExecutorHolder.equals(oldExecutorHolder)) {
-                executors = newMapBuilder(executors).put(entry.getKey(), newExecutorHolder).immutableMap();
+                Map<String, ExecutorHolder> newExecutors = new HashMap<>(executors);
+                newExecutors.put(entry.getKey(), newExecutorHolder);
+                executors = unmodifiableMap(newExecutors);
                 if (!oldExecutorHolder.executor().equals(newExecutorHolder.executor()) && oldExecutorHolder.executor() instanceof EsThreadPoolExecutor) {
                     retiredExecutors.add(oldExecutorHolder);
                     ((EsThreadPoolExecutor) oldExecutorHolder.executor()).shutdown(new ExecutorShutdownListener(oldExecutorHolder));
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportService.java b/core/src/main/java/org/elasticsearch/transport/TransportService.java
index 1d24853..964cfac 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportService.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportService.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.transport;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.action.admin.cluster.node.liveness.TransportLivenessAction;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.Strings;
diff --git a/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java b/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
index 2dca60c..8d2eb15 100644
--- a/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/local/LocalTransport.java
@@ -37,27 +37,11 @@ import org.elasticsearch.common.transport.TransportAddress;
 import org.elasticsearch.common.util.concurrent.AbstractRunnable;
 import org.elasticsearch.common.util.concurrent.EsExecutors;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.ActionNotFoundTransportException;
-import org.elasticsearch.transport.ConnectTransportException;
-import org.elasticsearch.transport.NodeNotConnectedException;
-import org.elasticsearch.transport.RemoteTransportException;
-import org.elasticsearch.transport.RequestHandlerRegistry;
-import org.elasticsearch.transport.ResponseHandlerFailureTransportException;
-import org.elasticsearch.transport.Transport;
-import org.elasticsearch.transport.TransportException;
-import org.elasticsearch.transport.TransportRequest;
-import org.elasticsearch.transport.TransportRequestOptions;
-import org.elasticsearch.transport.TransportResponse;
-import org.elasticsearch.transport.TransportResponseHandler;
-import org.elasticsearch.transport.TransportSerializationException;
-import org.elasticsearch.transport.TransportServiceAdapter;
-import org.elasticsearch.transport.Transports;
+import org.elasticsearch.transport.*;
 import org.elasticsearch.transport.support.TransportStatus;
 
 import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
@@ -81,7 +65,7 @@ public class LocalTransport extends AbstractLifecycleComponent<Transport> implem
     private final static ConcurrentMap<LocalTransportAddress, LocalTransport> transports = newConcurrentMap();
     private static final AtomicLong transportAddressIdGenerator = new AtomicLong();
     private final ConcurrentMap<DiscoveryNode, LocalTransport> connectedNodes = newConcurrentMap();
-    protected final NamedWriteableRegistry namedWriteableRegistry;
+    private final NamedWriteableRegistry namedWriteableRegistry;
 
     public static final String TRANSPORT_LOCAL_ADDRESS = "transport.local.address";
     public static final String TRANSPORT_LOCAL_WORKERS = "transport.local.workers";
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index 5cf9ed3..853497d 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.transport.netty;
 
-import java.nio.charset.StandardCharsets;
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -93,6 +91,7 @@ import java.net.InetSocketAddress;
 import java.net.SocketAddress;
 import java.net.UnknownHostException;
 import java.nio.channels.CancelledKeyException;
+import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -115,6 +114,7 @@ import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING_CLIENT;
 import static org.elasticsearch.common.network.NetworkService.TcpSettings.TCP_BLOCKING_SERVER;
@@ -340,7 +340,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
 
     @Override
     public Map<String, BoundTransportAddress> profileBoundAddresses() {
-        return ImmutableMap.copyOf(profileBoundAddresses);
+        return unmodifiableMap(new HashMap<>(profileBoundAddresses));
     }
 
     private InetSocketAddress createPublishAddress(String publishHost, int publishPort) {
@@ -453,7 +453,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             bindServerBootstrap(name, hostAddress, settings);
         }
     }
-        
+
     private void bindServerBootstrap(final String name, final InetAddress hostAddress, Settings profileSettings) {
 
         String port = profileSettings.get("port");
@@ -657,15 +657,15 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
 
     @Override
     public TransportAddress[] addressesFromString(String address, int perAddressLimit) throws Exception {
-        return parse(address, settings.get("transport.profiles.default.port", 
-                              settings.get("transport.netty.port", 
-                              settings.get("transport.tcp.port", 
+        return parse(address, settings.get("transport.profiles.default.port",
+                              settings.get("transport.netty.port",
+                              settings.get("transport.tcp.port",
                               DEFAULT_PORT_RANGE))), perAddressLimit);
     }
-    
+
     // this code is a take on guava's HostAndPort, like a HostAndPortRange
-    
-    // pattern for validating ipv6 bracked addresses. 
+
+    // pattern for validating ipv6 bracked addresses.
     // not perfect, but PortsRange should take care of any port range validation, not a regex
     private static final Pattern BRACKET_PATTERN = Pattern.compile("^\\[(.*:.*)\\](?::([\\d\\-]*))?$");
 
@@ -698,12 +698,12 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
             }
           }
         }
-        
+
         // if port isn't specified, fill with the default
         if (portString == null || portString.isEmpty()) {
             portString = defaultPortRange;
         }
-        
+
         // generate address for each port in the range
         Set<InetAddress> addresses = new HashSet<>(Arrays.asList(InetAddress.getAllByName(host)));
         List<TransportAddress> transportAddresses = new ArrayList<>();
diff --git a/core/src/main/java/org/elasticsearch/tribe/TribeService.java b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
index 36ea7fc..75b8176 100644
--- a/core/src/main/java/org/elasticsearch/tribe/TribeService.java
+++ b/core/src/main/java/org/elasticsearch/tribe/TribeService.java
@@ -19,10 +19,15 @@
 
 package org.elasticsearch.tribe;
 
-import com.google.common.collect.ImmutableMap;
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.support.master.TransportMasterNodeReadAction;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.ClusterChangedEvent;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateListener;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.block.ClusterBlocks;
@@ -33,7 +38,6 @@ import org.elasticsearch.cluster.node.DiscoveryNodes;
 import org.elasticsearch.cluster.routing.IndexRoutingTable;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.component.AbstractLifecycleComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.regex.Regex;
@@ -46,9 +50,15 @@ import org.elasticsearch.node.NodeBuilder;
 import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.elasticsearch.rest.RestStatus;
 
-import java.util.*;
+import java.util.EnumSet;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.CopyOnWriteArrayList;
 
+import static java.util.Collections.unmodifiableMap;
+
 /**
  * The tribe service holds a list of node clients connected to a list of tribe members, and uses their
  * cluster state events to update this local node cluster state with the merged view of it.
@@ -234,8 +244,12 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
                     for (DiscoveryNode tribe : tribeState.nodes()) {
                         if (currentState.nodes().get(tribe.id()) == null) {
                             // a new node, add it, but also add the tribe name to the attributes
-                            ImmutableMap<String, String> tribeAttr = MapBuilder.newMapBuilder(tribe.attributes()).put(TRIBE_NAME, tribeName).immutableMap();
-                            DiscoveryNode discoNode = new DiscoveryNode(tribe.name(), tribe.id(), tribe.getHostName(), tribe.getHostAddress(), tribe.address(), tribeAttr, tribe.version());
+                            Map<String, String> tribeAttr = new HashMap<>();
+                            for (ObjectObjectCursor<String, String> attr : tribe.attributes()) {
+                                tribeAttr.put(attr.key, attr.value);
+                            }
+                            tribeAttr.put(TRIBE_NAME, tribeName);
+                            DiscoveryNode discoNode = new DiscoveryNode(tribe.name(), tribe.id(), tribe.getHostName(), tribe.getHostAddress(), tribe.address(), unmodifiableMap(tribeAttr), tribe.version());
                             logger.info("[{}] adding node [{}]", tribeName, discoNode);
                             nodes.put(discoNode);
                         }
@@ -301,7 +315,7 @@ public class TribeService extends AbstractLifecycleComponent<TribeService> {
                         }
                     }
 
-                    return ClusterState.builder(currentState).incrementVersion().blocks(blocks).nodes(nodes).metaData(metaData).routingTable(routingTable).build();
+                    return ClusterState.builder(currentState).incrementVersion().blocks(blocks).nodes(nodes).metaData(metaData).routingTable(routingTable.build()).build();
                 }
 
                 private void removeIndex(ClusterBlocks.Builder blocks, MetaData.Builder metaData, RoutingTable.Builder routingTable, IndexMetaData index) {
diff --git a/core/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/core/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index 52134e4..06b50d3 100644
--- a/core/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/core/src/main/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -1,3 +1 @@
-org.elasticsearch.index.codec.postingsformat.Elasticsearch090PostingsFormat
-org.elasticsearch.search.suggest.completion.Completion090PostingsFormat
-org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat
+org.elasticsearch.search.suggest.completion.Completion090PostingsFormat
\ No newline at end of file
diff --git a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java
index ef7a9ac..dc176ae 100644
--- a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java
+++ b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatterTests.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java
index 450382c..58728d8 100644
--- a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java
+++ b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighterTests.java
@@ -1,19 +1,20 @@
 /*
- * Licensed to Elasticsearch under one
- * or more contributor license agreements. See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership. Elasticsearch licenses this
- * file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
- *     http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java
index 3b63f76..1be578f 100644
--- a/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java
+++ b/core/src/test/java/org/apache/lucene/search/postingshighlight/CustomSeparatorBreakIteratorTests.java
@@ -1,20 +1,20 @@
 /*
-Licensed to Elasticsearch under one or more contributor
-license agreements. See the NOTICE file distributed with
-this work for additional information regarding copyright
-ownership. Elasticsearch licenses this file to you under
-the Apache License, Version 2.0 (the "License"); you may
-not use this file except in compliance with the License.
-You may obtain a copy of the License at
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
  *
-   http://www.apache.org/licenses/LICENSE-2.0
+ *    http://www.apache.org/licenses/LICENSE-2.0
  *
-Unless required by applicable law or agreed to in writing,
-software distributed under the License is distributed on an
-"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-KIND, either express or implied.  See the License for the
-specific language governing permissions and limitations
-under the License.
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
  */
 
 package org.apache.lucene.search.postingshighlight;
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index 9297c6b..55dc2e4 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -705,7 +705,7 @@ public class ExceptionSerializationTests extends ESTestCase {
         ids.put(61, org.elasticsearch.cluster.routing.RoutingValidationException.class);
         ids.put(62, org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class);
         ids.put(63, org.elasticsearch.indices.AliasFilterParsingException.class);
-        ids.put(64, org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class);
+        ids.put(64, null); // DeleteByQueryFailedEngineException was removed in 3.0
         ids.put(65, org.elasticsearch.gateway.GatewayException.class);
         ids.put(66, org.elasticsearch.index.shard.IndexShardNotRecoveringException.class);
         ids.put(67, org.elasticsearch.http.HttpException.class);
diff --git a/core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java b/core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java
index d66c1bc..86ead20 100644
--- a/core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java
+++ b/core/src/test/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponsesTests.java
@@ -21,17 +21,18 @@ package org.elasticsearch.action.admin.cluster.health;
 
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse;
-import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
-import org.elasticsearch.action.admin.cluster.health.ClusterIndexHealth;
-import org.elasticsearch.action.admin.cluster.health.ClusterShardHealth;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
 import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.cluster.routing.IndexRoutingTable;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.TestShardRouting;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.settings.Settings;
@@ -46,7 +47,10 @@ import java.io.IOException;
 
 import static org.hamcrest.CoreMatchers.allOf;
 import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.empty;
+import static org.hamcrest.Matchers.greaterThanOrEqualTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.lessThanOrEqualTo;
 
 public class ClusterHealthResponsesTests extends ESTestCase {
 
@@ -209,7 +213,7 @@ public class ClusterHealthResponsesTests extends ESTestCase {
             metaData.put(indexMetaData, true);
             routingTable.add(indexRoutingTable);
         }
-        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
+        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable.build()).build();
         int pendingTasks = randomIntBetween(0, 200);
         int inFlight = randomIntBetween(0, 200);
         int delayedUnassigned = randomIntBetween(0, 200);
@@ -249,7 +253,7 @@ public class ClusterHealthResponsesTests extends ESTestCase {
         MetaData.Builder metaData = MetaData.builder();
         metaData.put(indexMetaData, true);
         routingTable.add(indexRoutingTable);
-        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
+        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable.build()).build();
         ClusterHealthResponse clusterHealth = new ClusterHealthResponse("bla", indexNameExpressionResolver.concreteIndices(clusterState, IndicesOptions.strictExpand(), (String[]) null), clusterState, 0, 0, 0, TimeValue.timeValueMillis(0));
         clusterHealth = maybeSerialize(clusterHealth);
         // currently we have no cluster level validation failures as index validation issues are reported per index.
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeReallyOldIndexIT.java b/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeReallyOldIndexIT.java
deleted file mode 100644
index d365f5b..0000000
--- a/core/src/test/java/org/elasticsearch/action/admin/indices/upgrade/UpgradeReallyOldIndexIT.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.action.admin.indices.upgrade;
-
-import org.elasticsearch.Version;
-import org.elasticsearch.bwcompat.StaticIndexBackwardCompatibilityIT;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.indices.IndicesService;
-
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.hamcrest.Matchers.containsString;
-
-public class UpgradeReallyOldIndexIT extends StaticIndexBackwardCompatibilityIT {
-
-    public void testUpgrade_0_90_6() throws Exception {
-        String indexName = "index-0.90.6";
-
-        loadIndex(indexName);
-        assertMinVersion(indexName, org.apache.lucene.util.Version.parse("4.5.1"));
-        UpgradeIT.assertNotUpgraded(client(), indexName);
-        assertTrue(UpgradeIT.hasAncientSegments(client(), indexName));
-        assertNoFailures(client().admin().indices().prepareUpgrade(indexName).setUpgradeOnlyAncientSegments(true).get());
-
-        assertFalse(UpgradeIT.hasAncientSegments(client(), indexName));
-        // This index has only ancient segments, so it should now be fully upgraded:
-        UpgradeIT.assertUpgraded(client(), indexName);
-        assertEquals(Version.CURRENT.luceneVersion.toString(), client().admin().indices().prepareGetSettings(indexName).get().getSetting(indexName, IndexMetaData.SETTING_VERSION_MINIMUM_COMPATIBLE));
-        assertMinVersion(indexName, Version.CURRENT.luceneVersion);
-
-        assertEquals(client().admin().indices().prepareGetSettings(indexName).get().getSetting(indexName, IndexMetaData.SETTING_VERSION_UPGRADED), Integer.toString(Version.CURRENT.id));
-    }
-
-    public void testUpgradeConflictingMapping() throws Exception {
-        String indexName = "index-conflicting-mappings-1.7.0";
-        logger.info("Checking static index " + indexName);
-        Settings nodeSettings = prepareBackwardsDataDir(getDataPath(indexName + ".zip"));
-        try {
-            internalCluster().startNode(nodeSettings);
-            fail("Should have failed to start the node");
-        } catch (Exception ex) {
-            assertThat(ex.getMessage(), containsString("conflicts with existing mapping in other types"));
-        }
-    }
-
-    private void assertMinVersion(String index, org.apache.lucene.util.Version version) {
-        for (IndicesService services : internalCluster().getInstances(IndicesService.class)) {
-            IndexService indexService = services.indexService(index);
-            if (indexService != null) {
-                assertEquals(version, indexService.getShardOrNull(0).minimumCompatibleVersion());
-            }
-        }
-
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
index f3058c0..5d77247 100644
--- a/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/action/count/CountRequestBuilderTests.java
@@ -19,16 +19,23 @@
 
 package org.elasticsearch.action.count;
 
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClient;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.MatchAllQueryBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import java.io.IOException;
+
 import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.equalTo;
 
@@ -55,29 +62,71 @@ public class CountRequestBuilderTests extends ESTestCase {
     @Test
     public void testEmptySourceToString() {
         CountRequestBuilder countRequestBuilder = client.prepareCount();
-        assertThat(countRequestBuilder.toString(), equalTo(new CountRequest().toString()));
+        assertThat(countRequestBuilder.toString(), equalTo(new QuerySourceBuilder().toString()));
     }
 
     @Test
     public void testQueryBuilderQueryToString() {
         CountRequestBuilder countRequestBuilder = client.prepareCount();
         countRequestBuilder.setQuery(QueryBuilders.matchAllQuery());
-        assertThat(countRequestBuilder.toString(), equalTo(new CountRequest().query(QueryBuilders.matchAllQuery()).toString()));
+        assertThat(countRequestBuilder.toString(), equalTo(new QuerySourceBuilder().setQuery(QueryBuilders.matchAllQuery()).toString()));
     }
 
     @Test
     public void testStringQueryToString() {
         CountRequestBuilder countRequestBuilder = client.prepareCount();
-        countRequestBuilder.setQuery(new MatchAllQueryBuilder());
-        assertThat(countRequestBuilder.toString(), containsString("match_all"));
+        String query = "{ \"match_all\" : {} }";
+        countRequestBuilder.setQuery(new BytesArray(query));
+        assertThat(countRequestBuilder.toString(), containsString("\"query\":{ \"match_all\" : {} }"));
+    }
+
+    @Test
+    public void testXContentBuilderQueryToString() throws IOException {
+        CountRequestBuilder countRequestBuilder = client.prepareCount();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        countRequestBuilder.setQuery(xContentBuilder);
+        assertThat(countRequestBuilder.toString(), equalTo(new QuerySourceBuilder().setQuery(xContentBuilder.bytes()).toString()));
+    }
+
+    @Test
+    public void testStringSourceToString() {
+        CountRequestBuilder countRequestBuilder = client.prepareCount();
+        String query = "{ \"query\": { \"match_all\" : {} } }";
+        countRequestBuilder.setSource(new BytesArray(query));
+        assertThat(countRequestBuilder.toString(), equalTo("{ \"query\": { \"match_all\" : {} } }"));
+    }
+
+    @Test
+    public void testXContentBuilderSourceToString() throws IOException {
+        CountRequestBuilder countRequestBuilder = client.prepareCount();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        countRequestBuilder.setSource(xContentBuilder.bytes());
+        assertThat(countRequestBuilder.toString(), equalTo(XContentHelper.convertToJson(xContentBuilder.bytes(), false, true)));
     }
 
     @Test
     public void testThatToStringDoesntWipeSource() {
-        CountRequestBuilder countRequestBuilder = client.prepareCount().setQuery(QueryBuilders.termQuery("field", "value"));
-        String preToString = countRequestBuilder.request().toString();
-        assertThat(countRequestBuilder.toString(), equalTo(new CountRequest().query(QueryBuilders.termQuery("field", "value")).toString()));
-        String postToString = countRequestBuilder.request().toString();
+        String source = "{\n" +
+                "            \"query\" : {\n" +
+                "            \"match\" : {\n" +
+                "                \"field\" : {\n" +
+                "                    \"query\" : \"value\"" +
+                "                }\n" +
+                "            }\n" +
+                "        }\n" +
+                "        }";
+        CountRequestBuilder countRequestBuilder = client.prepareCount().setSource(new BytesArray(source));
+        String preToString = countRequestBuilder.request().source().toUtf8();
+        assertThat(countRequestBuilder.toString(), equalTo(source));
+        String postToString = countRequestBuilder.request().source().toUtf8();
         assertThat(preToString, equalTo(postToString));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java b/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
index ca7d0c8..407cfba 100644
--- a/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/count/CountRequestTests.java
@@ -21,13 +21,18 @@ package org.elasticsearch.action.count;
 
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
+import java.util.Map;
+
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.notNullValue;
+import static org.hamcrest.CoreMatchers.nullValue;
 
 public class CountRequestTests extends ESTestCase {
 
@@ -51,9 +56,8 @@ public class CountRequestTests extends ESTestCase {
         if (randomBoolean()) {
             countRequest.preference(randomAsciiOfLengthBetween(1, 10));
         }
-        final boolean querySet = randomBoolean();
-        if (querySet) {
-            countRequest.query(QueryBuilders.termQuery("field", "value"));
+        if (randomBoolean()) {
+            countRequest.source(new QuerySourceBuilder().setQuery(QueryBuilders.termQuery("field", "value")));
         }
         if (randomBoolean()) {
             countRequest.minScore(randomFloat());
@@ -68,15 +72,31 @@ public class CountRequestTests extends ESTestCase {
         assertThat(searchRequest.types(), equalTo(countRequest.types()));
         assertThat(searchRequest.routing(), equalTo(countRequest.routing()));
         assertThat(searchRequest.preference(), equalTo(countRequest.preference()));
-        SearchSourceBuilder source = searchRequest.source();
-        assertThat(source.size(), equalTo(0));
-        if (querySet) {
-            assertThat(source.query(), notNullValue());
+
+        if (countRequest.source() == null) {
+            assertThat(searchRequest.source(), nullValue());
+        } else {
+            Map<String, Object> sourceMap = XContentHelper.convertToMap(searchRequest.source(), false).v2();
+            assertThat(sourceMap.size(), equalTo(1));
+            assertThat(sourceMap.get("query"), notNullValue());
+        }
+
+        Map<String, Object> extraSourceMap = XContentHelper.convertToMap(searchRequest.extraSource(), false).v2();
+        int count = 1;
+        assertThat((Integer)extraSourceMap.get("size"), equalTo(0));
+        if (countRequest.minScore() == CountRequest.DEFAULT_MIN_SCORE) {
+            assertThat(extraSourceMap.get("min_score"), nullValue());
+        } else {
+            assertThat(((Number)extraSourceMap.get("min_score")).floatValue(), equalTo(countRequest.minScore()));
+            count++;
+        }
+        if (countRequest.terminateAfter() == SearchContext.DEFAULT_TERMINATE_AFTER) {
+            assertThat(extraSourceMap.get("terminate_after"), nullValue());
         } else {
-            assertNull(source.query());
+            assertThat((Integer)extraSourceMap.get("terminate_after"), equalTo(countRequest.terminateAfter()));
+            count++;
         }
-        assertThat(source.minScore(), equalTo(countRequest.minScore()));
-        assertThat(source.terminateAfter(), equalTo(countRequest.terminateAfter()));
+        assertThat(extraSourceMap.size(), equalTo(count));
     }
 
     private static String[] randomStringArray() {
diff --git a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
index b07ba2f..5fd9bae 100644
--- a/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java
@@ -20,13 +20,6 @@
 package org.elasticsearch.action.search;
 
 import org.elasticsearch.action.support.IndicesOptions;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.MatchAllQueryParser;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.rest.action.search.RestMultiSearchAction;
-import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.test.StreamsUtils;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -35,7 +28,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.Collections;
 
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.nullValue;
@@ -44,9 +36,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch1.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(),true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(8));
         assertThat(request.requests().get(0).indices()[0], equalTo("test"));
         assertThat(request.requests().get(0).indicesOptions(), equalTo(IndicesOptions.fromOptions(true, true, true, true, IndicesOptions.strictExpandOpenAndForbidClosed())));
@@ -71,9 +62,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd2() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch2.json");
-        MultiSearchRequest request =RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(5));
         assertThat(request.requests().get(0).indices()[0], equalTo("test"));
         assertThat(request.requests().get(0).types().length, equalTo(0));
@@ -90,9 +80,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd3() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch3.json");
-        MultiSearchRequest request =RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(4));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -110,9 +99,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd4() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch4.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), false, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, false, null, null, null);
         assertThat(request.requests().size(), equalTo(3));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -132,9 +120,8 @@ public class MultiSearchRequestTests extends ESTestCase {
 
     @Test
     public void simpleAdd5() throws Exception {
-        IndicesQueriesRegistry registry = new IndicesQueriesRegistry(Settings.EMPTY, Collections.singleton(new MatchAllQueryParser()), new NamedWriteableRegistry());
         byte[] data = StreamsUtils.copyToBytesFromClasspath("/org/elasticsearch/action/search/simple-msearch5.json");
-        MultiSearchRequest request = RestMultiSearchAction.parseRequest(new MultiSearchRequest(), new BytesArray(data), true, null, null, null, null, IndicesOptions.strictExpandOpenAndForbidClosed(), true, registry);
+        MultiSearchRequest request = new MultiSearchRequest().add(data, 0, data.length, true, null, null, null);
         assertThat(request.requests().size(), equalTo(3));
         assertThat(request.requests().get(0).indices()[0], equalTo("test0"));
         assertThat(request.requests().get(0).indices()[1], equalTo("test1"));
@@ -150,18 +137,6 @@ public class MultiSearchRequestTests extends ESTestCase {
         assertThat(request.requests().get(2).types()[0], equalTo("type2"));
         assertThat(request.requests().get(2).types()[1], equalTo("type1"));
         assertThat(request.requests().get(2).routing(), equalTo("123"));
-        assertNotNull(request.requests().get(0).template());
-        assertNotNull(request.requests().get(1).template());
-        assertNotNull(request.requests().get(2).template());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(0).template().getType());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(1).template().getType());
-        assertEquals(ScriptService.ScriptType.INLINE, request.requests().get(2).template().getType());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(0).template().getScript());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(1).template().getScript());
-        assertEquals("{\"query\":{\"match_{{template}}\":{}}}", request.requests().get(2).template().getScript());
-        assertEquals(1, request.requests().get(0).template().getParams().size());
-        assertEquals(1, request.requests().get(1).template().getParams().size());
-        assertEquals(1, request.requests().get(2).template().getParams().size());
     }
 
     public void testResponseErrorToXContent() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
index 34bea06..1a05794 100644
--- a/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/action/search/SearchRequestBuilderTests.java
@@ -21,7 +21,12 @@ package org.elasticsearch.action.search;
 
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.transport.TransportClient;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESTestCase;
@@ -29,6 +34,9 @@ import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import java.io.IOException;
+
+import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.equalTo;
 
 public class SearchRequestBuilderTests extends ESTestCase {
@@ -65,18 +73,62 @@ public class SearchRequestBuilderTests extends ESTestCase {
     }
 
     @Test
-    public void testSearchSourceBuilderToString() {
+    public void testXContentBuilderQueryToString() throws IOException {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        searchRequestBuilder.setQuery(xContentBuilder);
+        assertThat(searchRequestBuilder.toString(), equalTo(new SearchSourceBuilder().query(xContentBuilder).toString()));
+    }
+
+    @Test
+    public void testStringQueryToString() {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        String query = "{ \"match_all\" : {} }";
+        searchRequestBuilder.setQuery(query);
+        assertThat(searchRequestBuilder.toString(), containsString("\"query\":{ \"match_all\" : {} }"));
+    }
+
+    @Test
+    public void testStringSourceToString() {
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
+        String source = "{ \"query\" : { \"match_all\" : {} } }";
+        searchRequestBuilder.setSource(new BytesArray(source));
+        assertThat(searchRequestBuilder.toString(), equalTo(source));
+    }
+
+    @Test
+    public void testXContentBuilderSourceToString() throws IOException {
         SearchRequestBuilder searchRequestBuilder = client.prepareSearch();
-        searchRequestBuilder.setSource(new SearchSourceBuilder().query(QueryBuilders.termQuery("field", "value")));
-        assertThat(searchRequestBuilder.toString(), equalTo(new SearchSourceBuilder().query(QueryBuilders.termQuery("field", "value")).toString()));
+        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));
+        xContentBuilder.startObject();
+        xContentBuilder.startObject("query");
+        xContentBuilder.startObject("match_all");
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        xContentBuilder.endObject();
+        searchRequestBuilder.setSource(xContentBuilder.bytes());
+        assertThat(searchRequestBuilder.toString(), equalTo(XContentHelper.convertToJson(xContentBuilder.bytes(), false, true)));
     }
 
     @Test
     public void testThatToStringDoesntWipeRequestSource() {
-        SearchRequestBuilder searchRequestBuilder = client.prepareSearch().setSource(new SearchSourceBuilder().query(QueryBuilders.termQuery("field", "value")));
-        String preToString = searchRequestBuilder.request().toString();
-        assertThat(searchRequestBuilder.toString(), equalTo(new SearchSourceBuilder().query(QueryBuilders.termQuery("field", "value")).toString()));
-        String postToString = searchRequestBuilder.request().toString();
+        String source = "{\n" +
+                "            \"query\" : {\n" +
+                "            \"match\" : {\n" +
+                "                \"field\" : {\n" +
+                "                    \"query\" : \"value\"" +
+                "                }\n" +
+                "            }\n" +
+                "        }\n" +
+                "        }";
+        SearchRequestBuilder searchRequestBuilder = client.prepareSearch().setSource(new BytesArray(source));
+        String preToString = searchRequestBuilder.request().source().toUtf8();
+        assertThat(searchRequestBuilder.toString(), equalTo(source));
+        String postToString = searchRequestBuilder.request().source().toUtf8();
         assertThat(preToString, equalTo(postToString));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java b/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
index e5143a3..100f68f 100644
--- a/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
+++ b/core/src/test/java/org/elasticsearch/action/support/replication/ClusterStateCreationUtils.java
@@ -27,7 +27,12 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.cluster.routing.IndexRoutingTable;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.index.shard.ShardId;
@@ -35,8 +40,10 @@ import org.elasticsearch.index.shard.ShardId;
 import java.util.HashSet;
 import java.util.Set;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
-import static org.elasticsearch.test.ESTestCase.randomBoolean;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_CREATION_DATE;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
 import static org.elasticsearch.test.ESTestCase.randomFrom;
 import static org.elasticsearch.test.ESTestCase.randomIntBetween;
 
@@ -124,7 +131,7 @@ public class ClusterStateCreationUtils {
         ClusterState.Builder state = ClusterState.builder(new ClusterName("test"));
         state.nodes(discoBuilder);
         state.metaData(MetaData.builder().put(indexMetaData, false).generateClusterUuidIfNeeded());
-        state.routingTable(RoutingTable.builder().add(IndexRoutingTable.builder(index).addIndexShard(indexShardRoutingBuilder.build())));
+        state.routingTable(RoutingTable.builder().add(IndexRoutingTable.builder(index).addIndexShard(indexShardRoutingBuilder.build())).build());
         return state.build();
     }
 
@@ -158,7 +165,7 @@ public class ClusterStateCreationUtils {
             indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(index, i, newNode(1).id(), null, null, false, ShardRoutingState.STARTED, 0, null));
             indexRoutingTableBuilder.addIndexShard(indexShardRoutingBuilder.build());
         }
-        state.routingTable(RoutingTable.builder().add(indexRoutingTableBuilder));
+        state.routingTable(RoutingTable.builder().add(indexRoutingTableBuilder.build()).build());
         return state.build();
     }
 
@@ -214,7 +221,7 @@ public class ClusterStateCreationUtils {
         ClusterState.Builder state = ClusterState.builder(new ClusterName("test"));
         state.nodes(discoBuilder);
         state.metaData(MetaData.builder().generateClusterUuidIfNeeded());
-        state.routingTable(RoutingTable.builder());
+        state.routingTable(RoutingTable.builder().build());
         return state.build();
     }
 
diff --git a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java b/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
index 9ec74c9..6b2608c 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/cluster/ClusterAllocationRerouteBenchmark.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.benchmark.cluster;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -35,6 +34,7 @@ import org.elasticsearch.test.ESAllocationTestCase;
 
 import java.util.Random;
 
+import static java.util.Collections.singletonMap;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 
 public class ClusterAllocationRerouteBenchmark {
@@ -64,7 +64,7 @@ public class ClusterAllocationRerouteBenchmark {
         RoutingTable routingTable = rb.build();
         DiscoveryNodes.Builder nb = DiscoveryNodes.builder();
         for (int i = 1; i <= numberOfNodes; i++) {
-            nb.put(ESAllocationTestCase.newNode("node" + i, numberOfTags == 0 ? ImmutableMap.<String, String>of() : ImmutableMap.of("tag", "tag_" + (i % numberOfTags))));
+            nb.put(ESAllocationTestCase.newNode("node" + i, singletonMap("tag", "tag_" + (i % numberOfTags))));
         }
         ClusterState initialClusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).nodes(nb).build();
 
diff --git a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java b/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
index 89e176f..213a522 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/search/SuggestSearchBenchMark.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.node.Node;
 import org.elasticsearch.search.suggest.Suggest.Suggestion.Entry.Option;
-import org.elasticsearch.search.suggest.SuggestBuilder;
 import org.elasticsearch.search.suggest.SuggestBuilders;
 
 import java.io.IOException;
@@ -42,9 +41,7 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
 
 /**
@@ -121,9 +118,7 @@ public class SuggestSearchBenchMark {
             String term = "prefix" + startChar;
             SearchResponse response = client.prepareSearch()
                     .setQuery(prefixQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").field("field").text(term)
-                                    .suggestMode("always")))
+                    .addSuggestion(SuggestBuilders.termSuggestion("field").field("field").text(term).suggestMode("always"))
                     .execute().actionGet();
             if (response.getHits().totalHits() == 0) {
                 System.err.println("No hits");
@@ -140,9 +135,7 @@ public class SuggestSearchBenchMark {
             String term = "prefix" + startChar;
             SearchResponse response = client.prepareSearch()
                     .setQuery(matchQuery("field", term))
-                    .suggest(
-                            new SuggestBuilder().addSuggestion(SuggestBuilders.termSuggestion("field").text(term).field("field")
-                                    .suggestMode("always")))
+                    .addSuggestion(SuggestBuilders.termSuggestion("field").text(term).field("field").suggestMode("always"))
                     .execute().actionGet();
             timeTaken += response.getTookInMillis();
             if (response.getSuggest() == null) {
diff --git a/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java b/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
index 78ca44b..e2da702 100644
--- a/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
+++ b/core/src/test/java/org/elasticsearch/broadcast/BroadcastActionsIT.java
@@ -68,6 +68,15 @@ public class BroadcastActionsIT extends ESIntegTestCase {
             assertThat(countResponse.getSuccessfulShards(), equalTo(numShards.numPrimaries));
             assertThat(countResponse.getFailedShards(), equalTo(0));
         }
+
+        for (int i = 0; i < 5; i++) {
+            // test failed (simply query that can't be parsed)
+            try {
+                client().count(countRequest("test").source("{ term : { _type : \"type1 } }".getBytes(StandardCharsets.UTF_8))).actionGet();
+            } catch(SearchPhaseExecutionException e) {
+                assertThat(e.shardFailures().length, equalTo(numShards.numPrimaries));
+            }
+        }
     }
 
     private XContentBuilder source(String id, String nameValue) throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
index 8e71f3d..990c399 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityIT.java
@@ -246,7 +246,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
         SortedSet<String> expectedVersions = new TreeSet<>();
         for (Version v : VersionUtils.allVersions()) {
             if (v.snapshot()) continue;  // snapshots are unreleased, so there is no backcompat yet
-            if (v.onOrBefore(Version.V_0_20_6)) continue; // we can only test back one major lucene version
+            if (v.onOrBefore(Version.V_2_0_0_beta1)) continue; // we can only test back one major lucene version
             if (v.equals(Version.CURRENT)) continue; // the current version is always compatible with itself
             expectedVersions.add("index-" + v.toString() + ".zip");
         }
@@ -312,7 +312,7 @@ public class OldIndexBackwardsCompatibilityIT extends ESIntegTestCase {
             client().admin().indices().prepareOpen(indexName).get();
             fail("Shouldn't be able to open an old index");
         } catch (IllegalStateException ex) {
-            assertThat(ex.getMessage(), containsString("was created before v0.90.0 and wasn't upgraded"));
+            assertThat(ex.getMessage(), containsString("was created before v2.0.0.beta1 and wasn't upgraded"));
         }
         unloadIndex(indexName);
         logger.info("--> Done testing " + index + ", took " + ((System.currentTimeMillis() - startTime) / 1000.0) + " seconds");
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java b/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
index 486267b..8957485 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/RecoveryWithUnsupportedIndicesIT.java
@@ -36,7 +36,7 @@ public class RecoveryWithUnsupportedIndicesIT extends StaticIndexBackwardCompati
             internalCluster().startNode(nodeSettings);
             fail();
         } catch (Exception ex) {
-            assertThat(ex.getMessage(), containsString(" was created before v0.90.0 and wasn't upgraded"));
+            assertThat(ex.getMessage(), containsString(" was created before v2.0.0.beta1 and wasn't upgraded"));
         }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
index 9ef4238..740b185 100644
--- a/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
+++ b/core/src/test/java/org/elasticsearch/bwcompat/RestoreBackwardsCompatIT.java
@@ -95,9 +95,8 @@ public class RestoreBackwardsCompatIT extends AbstractSnapshotIntegTestCase {
             if (Modifier.isStatic(field.getModifiers()) && field.getType() == Version.class) {
                 Version v = (Version) field.get(Version.class);
                 if (v.snapshot()) continue;
-                if (v.onOrBefore(Version.V_1_0_0_Beta1)) continue;
+                if (v.onOrBefore(Version.V_2_0_0_beta1)) continue;
                 if (v.equals(Version.CURRENT)) continue;
-
                 expectedVersions.add(v.toString());
             }
         }
diff --git a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
index 0cf16b4..b00b677 100644
--- a/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
+++ b/core/src/test/java/org/elasticsearch/client/AbstractClientHeadersTestCase.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.client;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.GenericAction;
@@ -138,10 +137,9 @@ public abstract class AbstractClientHeadersTestCase extends ESTestCase {
     @Test
     public void testOverideHeader() throws Exception {
         String key1Val = randomAsciiOfLength(5);
-        Map<String, Object> expected = ImmutableMap.<String, Object>builder()
-                .put("key1", key1Val)
-                .put("key2", "val 2")
-                .build();
+        Map<String, Object> expected = new HashMap<>();
+        expected.put("key1", key1Val);
+        expected.put("key2", "val 2");
 
         client.prepareGet("idx", "type", "id")
                 .putHeader("key1", key1Val)
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
index f672b26..f938a78 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.cluster;
 
+import com.carrotsearch.hppc.cursors.ObjectCursor;
+
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.ActionModule;
@@ -34,6 +36,7 @@ import org.elasticsearch.cluster.routing.RoutingNodes;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexService;
@@ -53,7 +56,6 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.util.Collection;
-import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -149,24 +151,24 @@ public class ClusterInfoServiceIT extends ESIntegTestCase {
         final InternalClusterInfoService infoService = (InternalClusterInfoService) internalTestCluster.getInstance(ClusterInfoService.class, internalTestCluster.getMasterName());
         ClusterInfo info = infoService.refresh();
         assertNotNull("info should not be null", info);
-        final Map<String, DiskUsage> leastUsages = info.getNodeLeastAvailableDiskUsages();
-        final Map<String, DiskUsage> mostUsages = info.getNodeMostAvailableDiskUsages();
-        final Map<String, Long> shardSizes = info.shardSizes;
+        ImmutableOpenMap<String, DiskUsage> leastUsages = info.getNodeLeastAvailableDiskUsages();
+        ImmutableOpenMap<String, DiskUsage> mostUsages = info.getNodeMostAvailableDiskUsages();
+        ImmutableOpenMap<String, Long> shardSizes = info.shardSizes;
         assertNotNull(leastUsages);
         assertNotNull(shardSizes);
         assertThat("some usages are populated", leastUsages.values().size(), Matchers.equalTo(2));
         assertThat("some shard sizes are populated", shardSizes.values().size(), greaterThan(0));
-        for (DiskUsage usage : leastUsages.values()) {
-            logger.info("--> usage: {}", usage);
-            assertThat("usage has be retrieved", usage.getFreeBytes(), greaterThan(0L));
+        for (ObjectCursor<DiskUsage> usage : leastUsages.values()) {
+            logger.info("--> usage: {}", usage.value);
+            assertThat("usage has be retrieved", usage.value.getFreeBytes(), greaterThan(0L));
         }
-        for (DiskUsage usage : mostUsages.values()) {
-            logger.info("--> usage: {}", usage);
-            assertThat("usage has be retrieved", usage.getFreeBytes(), greaterThan(0L));
+        for (ObjectCursor<DiskUsage> usage : mostUsages.values()) {
+            logger.info("--> usage: {}", usage.value);
+            assertThat("usage has be retrieved", usage.value.getFreeBytes(), greaterThan(0L));
         }
-        for (Long size : shardSizes.values()) {
-            logger.info("--> shard size: {}", size);
-            assertThat("shard size is greater than 0", size, greaterThanOrEqualTo(0L));
+        for (ObjectCursor<Long> size : shardSizes.values()) {
+            logger.info("--> shard size: {}", size.value);
+            assertThat("shard size is greater than 0", size.value, greaterThanOrEqualTo(0L));
         }
         ClusterService clusterService = internalTestCluster.getInstance(ClusterService.class, internalTestCluster.getMasterName());
         ClusterState state = clusterService.state();
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
index fb60e50..ac2182c 100644
--- a/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffIT.java
@@ -20,15 +20,25 @@
 package org.elasticsearch.cluster;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
-import com.google.common.collect.ImmutableMap;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.block.ClusterBlock;
 import org.elasticsearch.cluster.block.ClusterBlocks;
-import org.elasticsearch.cluster.metadata.*;
+import org.elasticsearch.cluster.metadata.AliasMetaData;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.metadata.RepositoriesMetaData;
+import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.cluster.routing.IndexRoutingTable;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
@@ -40,7 +50,6 @@ import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.gateway.GatewayService;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -48,12 +57,13 @@ import org.junit.Test;
 import java.util.Collections;
 import java.util.List;
 
+import static java.util.Collections.emptyList;
 import static org.elasticsearch.cluster.metadata.AliasMetaData.newAliasMetaDataBuilder;
 import static org.elasticsearch.cluster.routing.RandomShardRoutingMutator.randomChange;
 import static org.elasticsearch.cluster.routing.RandomShardRoutingMutator.randomReason;
+import static org.elasticsearch.test.VersionUtils.randomVersion;
 import static org.elasticsearch.test.XContentTestUtils.convertToMap;
 import static org.elasticsearch.test.XContentTestUtils.differenceBetweenMapsIgnoringArrayOrder;
-import static org.elasticsearch.test.VersionUtils.randomVersion;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.is;
 
@@ -199,7 +209,7 @@ public class ClusterStateDiffIT extends ESIntegTestCase {
         RoutingTable.Builder builder = RoutingTable.builder(clusterState.routingTable());
         int numberOfIndices = clusterState.routingTable().indicesRouting().size();
         if (numberOfIndices > 0) {
-            List<String> randomIndices = randomSubsetOf(randomInt(numberOfIndices - 1), clusterState.routingTable().indicesRouting().keySet().toArray(new String[numberOfIndices]));
+            List<String> randomIndices = randomSubsetOf(randomInt(numberOfIndices - 1), clusterState.routingTable().indicesRouting().keys().toArray(String.class));
             for (String index : randomIndices) {
                 if (randomBoolean()) {
                     builder.remove(index);
@@ -533,7 +543,7 @@ public class ClusterStateDiffIT extends ESIntegTestCase {
                             randomName("warm"),
                             new String[]{randomName("type")},
                             randomBoolean(),
-                            new IndexWarmersMetaData.SearchSource(new BytesArray(randomAsciiOfLength(1000))))
+                            new BytesArray(randomAsciiOfLength(1000)))
             );
         } else {
             return new IndexWarmersMetaData();
@@ -663,13 +673,13 @@ public class ClusterStateDiffIT extends ESIntegTestCase {
                                 SnapshotsInProgress.State.fromValue((byte) randomIntBetween(0, 6)),
                                 Collections.<String>emptyList(),
                                 Math.abs(randomLong()),
-                                ImmutableMap.<ShardId, SnapshotsInProgress.ShardSnapshotStatus>of()));
+                                ImmutableOpenMap.of()));
                     case 1:
                         return new RestoreInProgress(new RestoreInProgress.Entry(
                                 new SnapshotId(randomName("repo"), randomName("snap")),
                                 RestoreInProgress.State.fromValue((byte) randomIntBetween(0, 3)),
-                                Collections.<String>emptyList(),
-                                ImmutableMap.<ShardId, RestoreInProgress.ShardRestoreStatus>of()));
+                                emptyList(),
+                                ImmutableOpenMap.of()));
                     default:
                         throw new IllegalArgumentException("Shouldn't be here");
                 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java b/core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java
index a427829..595dbc9 100644
--- a/core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/DiskUsageTests.java
@@ -21,13 +21,13 @@ package org.elasticsearch.cluster;
 
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.node.stats.NodeStats;
-import org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse;
 import org.elasticsearch.action.admin.indices.stats.CommonStats;
 import org.elasticsearch.action.admin.indices.stats.ShardStats;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingHelper;
 import org.elasticsearch.cluster.routing.UnassignedInfo;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.index.shard.ShardPath;
 import org.elasticsearch.index.store.StoreStats;
@@ -36,8 +36,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.nio.file.Path;
-import java.util.HashMap;
-import java.util.Map;
 
 import static org.hamcrest.Matchers.equalTo;
 
@@ -95,7 +93,7 @@ public class DiskUsageTests extends ESTestCase {
             }
         }
     }
-    
+
     public void testFillShardLevelInfo() {
         ShardRouting test_0 = ShardRouting.newUnassigned("test", 0, null, false, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
         ShardRoutingHelper.initialize(test_0, "node1");
@@ -113,8 +111,8 @@ public class DiskUsageTests extends ESTestCase {
                 new ShardStats(test_0, new ShardPath(false, test0Path, test0Path, "0xdeadbeef", test_0.shardId()), commonStats0 , null),
                 new ShardStats(test_1, new ShardPath(false, test1Path, test1Path, "0xdeadbeef", test_1.shardId()), commonStats1 , null)
         };
-        HashMap<String, Long> shardSizes = new HashMap<>();
-        HashMap<ShardRouting, String> routingToPath = new HashMap<>();
+        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
+        ImmutableOpenMap.Builder<ShardRouting, String> routingToPath = ImmutableOpenMap.builder();
         InternalClusterInfoService.buildShardLevelInfo(logger, stats, shardSizes, routingToPath);
         assertEquals(2, shardSizes.size());
         assertTrue(shardSizes.containsKey(ClusterInfo.shardIdentifierFromRouting(test_0)));
@@ -130,8 +128,8 @@ public class DiskUsageTests extends ESTestCase {
     }
 
     public void testFillDiskUsage() {
-        Map<String, DiskUsage> newLeastAvaiableUsages = new HashMap<>();
-        Map<String, DiskUsage> newMostAvaiableUsages = new HashMap<>();
+        ImmutableOpenMap.Builder<String, DiskUsage> newLeastAvaiableUsages = ImmutableOpenMap.builder();
+        ImmutableOpenMap.Builder<String, DiskUsage> newMostAvaiableUsages = ImmutableOpenMap.builder();
         FsInfo.Path[] node1FSInfo =  new FsInfo.Path[] {
                 new FsInfo.Path("/middle", "/dev/sda", 100, 90, 80),
                 new FsInfo.Path("/least", "/dev/sdb", 200, 190, 70),
diff --git a/core/src/test/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java b/core/src/test/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
index eff22c8..dd1cb0b 100644
--- a/core/src/test/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
+++ b/core/src/test/java/org/elasticsearch/cluster/MockInternalClusterInfoService.java
@@ -27,6 +27,7 @@ import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;
 import org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
@@ -35,10 +36,6 @@ import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.threadpool.ThreadPool;
 
-import java.util.AbstractMap;
-import java.util.Collections;
-import java.util.Map;
-import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 
 /**
@@ -116,30 +113,24 @@ public class MockInternalClusterInfoService extends InternalClusterInfoService {
         return new CountDownLatch(0);
     }
 
+    @Override
     public ClusterInfo getClusterInfo() {
         ClusterInfo clusterInfo = super.getClusterInfo();
-        return new ClusterInfo(clusterInfo.getNodeLeastAvailableDiskUsages(), clusterInfo.getNodeMostAvailableDiskUsages(), clusterInfo.shardSizes, DEV_NULL_MAP);
+        return new DevNullClusterInfo(clusterInfo.getNodeLeastAvailableDiskUsages(), clusterInfo.getNodeMostAvailableDiskUsages(), clusterInfo.shardSizes);
     }
 
-    public static final Map<ShardRouting, String> DEV_NULL_MAP = Collections.unmodifiableMap(new StaticValueMap("/dev/null"));
-
-    // a test only map that always returns the same value no matter what key is passed
-    private static final class StaticValueMap extends AbstractMap<ShardRouting, String> {
-
-        private final String value;
-
-        private StaticValueMap(String value) {
-            this.value = value;
-        }
-
-        @Override
-        public String get(Object key) {
-            return value;
+    /**
+     * ClusterInfo that always points to DevNull.
+     */
+    public static class DevNullClusterInfo extends ClusterInfo {
+        public DevNullClusterInfo(ImmutableOpenMap<String, DiskUsage> leastAvailableSpaceUsage,
+            ImmutableOpenMap<String, DiskUsage> mostAvailableSpaceUsage, ImmutableOpenMap<String, Long> shardSizes) {
+            super(leastAvailableSpaceUsage, mostAvailableSpaceUsage, shardSizes, null);
         }
 
         @Override
-        public Set<Entry<ShardRouting, String>> entrySet() {
-            throw new UnsupportedOperationException("this is a test-only map that only supports #get(Object key)");
+        public String getDataPath(ShardRouting shardRouting) {
+            return "/dev/null";
         }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java b/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java
deleted file mode 100644
index 88f27bc..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/metadata/MetaDataIndexUpgradeServiceTests.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.cluster.metadata;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import org.elasticsearch.Version;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.store.IndexStoreModule;
-import org.elasticsearch.test.ESTestCase;
-
-import java.util.Arrays;
-import java.util.Locale;
-
-public class MetaDataIndexUpgradeServiceTests extends ESTestCase {
-
-    public void testUpgradeStoreSettings() {
-        final String type = RandomPicks.randomFrom(random(), Arrays.asList("nio_fs", "mmap_fs", "simple_fs", "default", "fs"));
-        MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(Settings.EMPTY, null);
-        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .put(IndexStoreModule.STORE_TYPE, randomBoolean() ? type : type.toUpperCase(Locale.ROOT))
-                .build();
-        IndexMetaData test = IndexMetaData.builder("test")
-                .settings(indexSettings)
-                .numberOfShards(1)
-                .numberOfReplicas(1)
-                .build();
-        IndexMetaData indexMetaData = metaDataIndexUpgradeService.upgradeSettings(test);
-        assertEquals(type.replace("_", ""), indexMetaData.getSettings().get(IndexStoreModule.STORE_TYPE));
-    }
-
-    public void testNoStoreSetting() {
-        Settings indexSettings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
-                .build();
-        IndexMetaData test = IndexMetaData.builder("test")
-                .settings(indexSettings)
-                .numberOfShards(1)
-                .numberOfReplicas(1)
-                .build();
-        MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(Settings.EMPTY, null);
-        IndexMetaData indexMetaData = metaDataIndexUpgradeService.upgradeSettings(test);
-        assertSame(indexMetaData, test);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java b/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
index b37495e..2136d1e 100644
--- a/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeFiltersTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.node;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
@@ -33,8 +32,12 @@ import java.net.InetAddress;
 import java.net.UnknownHostException;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonMap;
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.AND;
 import static org.elasticsearch.cluster.node.DiscoveryNodeFilters.OpType.OR;
 import static org.hamcrest.Matchers.equalTo;
@@ -62,10 +65,10 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build();
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -76,10 +79,10 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build();
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -91,13 +94,13 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
-        node = new DiscoveryNode("name3", "id3", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        node = new DiscoveryNode("name3", "id3", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -109,19 +112,30 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
+        Map<String, String> attributes = new HashMap<>();
+        attributes.put("tag", "A");
+        attributes.put("group", "B");
         DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE,
-                ImmutableMap.of("tag", "A", "group", "B"), Version.CURRENT);
+                attributes, Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
+        attributes = new HashMap<>();
+        attributes.put("tag", "A");
+        attributes.put("group", "B");
+        attributes.put("name", "X");
         node = new DiscoveryNode("name2", "id2", DummyTransportAddress.INSTANCE,
-                ImmutableMap.of("tag", "A", "group", "B", "name", "X"), Version.CURRENT);
+                attributes, Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
 
+        attributes = new HashMap<>();
+        attributes.put("tag", "A");
+        attributes.put("group", "F");
+        attributes.put("name", "X");
         node = new DiscoveryNode("name3", "id3", DummyTransportAddress.INSTANCE,
-                ImmutableMap.of("tag", "A", "group", "F", "name", "X"), Version.CURRENT);
+                attributes, Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
 
-        node = new DiscoveryNode("name4", "id4", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        node = new DiscoveryNode("name4", "id4", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -132,7 +146,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build();
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT);
+        DiscoveryNode node = new DiscoveryNode("name1", "id1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -144,7 +158,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -156,7 +170,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -168,7 +182,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -180,7 +194,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -192,7 +206,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -204,7 +218,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -216,7 +230,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(AND, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(false));
     }
 
@@ -228,7 +242,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
@@ -240,7 +254,7 @@ public class DiscoveryNodeFiltersTests extends ESTestCase {
                 .build());
         DiscoveryNodeFilters filters = DiscoveryNodeFilters.buildFromSettings(OR, "xxx.", settings);
 
-        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, ImmutableMap.of("tag", "A"), null);
+        DiscoveryNode node = new DiscoveryNode("", "", "", "192.1.1.54", localAddress, singletonMap("tag", "A"), null);
         assertThat(filters.match(node), equalTo(true));
     }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
index a2dbf78..29281e2 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java
@@ -38,43 +38,32 @@ import java.util.Arrays;
 public class RoutingBackwardCompatibilityTests extends ESTestCase {
 
     public void testBackwardCompatibility() throws Exception {
-        Path baseDir = createTempDir();
-        Node node = new Node(Settings.builder().put("path.home", baseDir.toString()).build());
-        try {
-            try (BufferedReader reader = new BufferedReader(new InputStreamReader(RoutingBackwardCompatibilityTests.class.getResourceAsStream("/org/elasticsearch/cluster/routing/shard_routes.txt"), "UTF-8"))) {
-                for (String line = reader.readLine(); line != null; line = reader.readLine()) {
-                    if (line.startsWith("#")) { // comment
-                        continue;
-                    }
-                    String[] parts = line.split("\t");
-                    assertEquals(Arrays.toString(parts), 7, parts.length);
-                    final String index = parts[0];
-                    final int numberOfShards = Integer.parseInt(parts[1]);
-                    final String type = parts[2];
-                    final String id = parts[3];
-                    final String routing = "null".equals(parts[4]) ? null : parts[4];
-                    final int pre20ExpectedShardId = Integer.parseInt(parts[5]);
-                    final int currentExpectedShard = Integer.parseInt(parts[6]);
+        try (BufferedReader reader = new BufferedReader(new InputStreamReader(RoutingBackwardCompatibilityTests.class.getResourceAsStream("/org/elasticsearch/cluster/routing/shard_routes.txt"), "UTF-8"))) {
+            for (String line = reader.readLine(); line != null; line = reader.readLine()) {
+                if (line.startsWith("#")) { // comment
+                    continue;
+                }
+                String[] parts = line.split("\t");
+                assertEquals(Arrays.toString(parts), 7, parts.length);
+                final String index = parts[0];
+                final int numberOfShards = Integer.parseInt(parts[1]);
+                final String type = parts[2];
+                final String id = parts[3];
+                final String routing = "null".equals(parts[4]) ? null : parts[4];
+                final int pre20ExpectedShardId = Integer.parseInt(parts[5]); // not needed anymore - old hashing is gone
+                final int currentExpectedShard = Integer.parseInt(parts[6]);
 
-                    OperationRouting operationRouting = node.injector().getInstance(OperationRouting.class);
-                    for (Version version : VersionUtils.allVersions()) {
-                        final Settings settings = settings(version).build();
-                        IndexMetaData indexMetaData = IndexMetaData.builder(index).settings(settings).numberOfShards(numberOfShards).numberOfReplicas(randomInt(3)).build();
-                        MetaData.Builder metaData = MetaData.builder().put(indexMetaData, false);
-                        RoutingTable routingTable = RoutingTable.builder().addAsNew(indexMetaData).build();
-                        ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
-                        final int shardId = operationRouting.indexShards(clusterState, index, type, id, routing).shardId().getId();
-                        if (version.before(Version.V_2_0_0_beta1)) {
-                            assertEquals(pre20ExpectedShardId, shardId);
-                        } else {
-                            assertEquals(currentExpectedShard, shardId);
-                        }
-                    }
+                OperationRouting operationRouting = new OperationRouting(Settings.EMPTY, null);
+                for (Version version : VersionUtils.allVersions()) {
+                    final Settings settings = settings(version).build();
+                    IndexMetaData indexMetaData = IndexMetaData.builder(index).settings(settings).numberOfShards(numberOfShards).numberOfReplicas(randomInt(3)).build();
+                    MetaData.Builder metaData = MetaData.builder().put(indexMetaData, false);
+                    RoutingTable routingTable = RoutingTable.builder().addAsNew(indexMetaData).build();
+                    ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
+                    final int shardId = operationRouting.indexShards(clusterState, index, type, id, routing).shardId().getId();
+                    assertEquals(currentExpectedShard, shardId);
                 }
             }
-        } finally {
-            node.close();
         }
     }
-
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeIT.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeIT.java
deleted file mode 100644
index bff1977..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeIT.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing;
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.elasticsearch.action.admin.indices.get.GetIndexResponse;
-import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.node.Node;
-import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.test.ESIntegTestCase;
-
-import java.nio.file.Path;
-
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-
-@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0, minNumDataNodes = 0, maxNumDataNodes = 0)
-@LuceneTestCase.SuppressFileSystems("*") // extra files break the single data cluster expectation when unzipping the static index
-public class RoutingBackwardCompatibilityUponUpgradeIT extends ESIntegTestCase {
-
-    public void testDefaultRouting() throws Exception {
-        test("default_routing_1_x", DjbHashFunction.class, false);
-    }
-
-    public void testCustomRouting() throws Exception {
-        test("custom_routing_1_x", SimpleHashFunction.class, true);
-    }
-
-    private void test(String name, Class<? extends HashFunction> expectedHashFunction, boolean expectedUseType) throws Exception {
-        Path zippedIndexDir = getDataPath("/org/elasticsearch/cluster/routing/" + name + ".zip");
-        Settings baseSettings = prepareBackwardsDataDir(zippedIndexDir);
-        internalCluster().startNode(Settings.builder()
-                .put(baseSettings)
-                .put(Node.HTTP_ENABLED, true)
-                .build());
-        ensureYellow("test");
-        GetIndexResponse getIndexResponse = client().admin().indices().prepareGetIndex().get();
-        assertArrayEquals(new String[] {"test"}, getIndexResponse.indices());
-        GetSettingsResponse getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
-        assertEquals(expectedHashFunction.getName(), getSettingsResponse.getSetting("test", IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION));
-        assertEquals(Boolean.valueOf(expectedUseType).toString(), getSettingsResponse.getSetting("test", IndexMetaData.SETTING_LEGACY_ROUTING_USE_TYPE));
-        SearchResponse allDocs = client().prepareSearch("test").get();
-        assertSearchResponse(allDocs);
-        assertHitCount(allDocs, 4);
-        // Make sure routing works
-        for (SearchHit hit : allDocs.getHits().hits()) {
-            GetResponse get = client().prepareGet(hit.index(), hit.type(), hit.id()).get();
-            assertTrue(get.isExists());
-        }
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
index c2ba1cb..9309fa7 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingServiceTests.java
@@ -36,8 +36,6 @@ import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 
-import java.util.Collections;
-import java.util.Comparator;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
 
@@ -76,7 +74,7 @@ public class RoutingServiceTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -106,7 +104,7 @@ public class RoutingServiceTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -153,7 +151,7 @@ public class RoutingServiceTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2")).localNodeId("node1").masterNodeId("node1")).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java
index ea5dda0..2a7ed6a 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTests.java
@@ -35,6 +35,7 @@ import org.junit.Test;
 
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
+import static org.hamcrest.Matchers.containsString;
 import static org.hamcrest.Matchers.is;
 import static org.hamcrest.Matchers.nullValue;
 
@@ -55,6 +56,7 @@ public class RoutingTableTests extends ESAllocationTestCase {
             .build());
     private ClusterState clusterState;
 
+    @Override
     @Before
     public void setUp() throws Exception {
         super.setUp();
@@ -264,4 +266,34 @@ public class RoutingTableTests extends ESAllocationTestCase {
             fail("Calling with non-existing index should be ignored at the moment");
         }
     }
+
+    public void testRoutingTableBuiltMoreThanOnce() {
+        RoutingTable.Builder b = RoutingTable.builder();
+        b.build(); // Ok the first time
+        try {
+            b.build();
+            fail("expected exception");
+        } catch (IllegalStateException e) {
+            assertThat(e.getMessage(), containsString("cannot be reused"));
+        }
+        try {
+            b.add((IndexRoutingTable) null);
+            fail("expected exception");
+        } catch (IllegalStateException e) {
+            assertThat(e.getMessage(), containsString("cannot be reused"));
+        }
+        try {
+            b.updateNumberOfReplicas(1, "foo");
+            fail("expected exception");
+        } catch (IllegalStateException e) {
+            assertThat(e.getMessage(), containsString("cannot be reused"));
+        }
+        try {
+            b.remove("foo");
+            fail("expected exception");
+        } catch (IllegalStateException e) {
+            assertThat(e.getMessage(), containsString("cannot be reused"));
+        }
+
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
index fdc1c52..fc7ce64 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/UnassignedInfoTests.java
@@ -21,6 +21,7 @@ package org.elasticsearch.cluster.routing;
 
 import com.carrotsearch.hppc.IntHashSet;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
@@ -40,8 +41,15 @@ import org.junit.Test;
 import java.util.Collections;
 import java.util.EnumSet;
 
-import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.UNASSIGNED;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.lessThan;
+import static org.hamcrest.Matchers.lessThanOrEqualTo;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
 
 /**
  */
@@ -89,7 +97,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.INDEX_CREATED));
         }
@@ -102,7 +110,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsRecovery(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsRecovery(metaData.index("test")).build()).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.CLUSTER_RECOVERED));
         }
@@ -115,7 +123,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsFromCloseToOpen(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsFromCloseToOpen(metaData.index("test")).build()).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.INDEX_REOPENED));
         }
@@ -128,7 +136,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNewRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test"), new IntHashSet())).build();
+                .routingTable(RoutingTable.builder().addAsNewRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test"), new IntHashSet()).build()).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.NEW_INDEX_RESTORED));
         }
@@ -141,7 +149,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test"))).build();
+                .routingTable(RoutingTable.builder().addAsRestore(metaData.index("test"), new RestoreSource(new SnapshotId("rep1", "snp1"), Version.CURRENT, "test")).build()).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.EXISTING_INDEX_RESTORED));
         }
@@ -154,7 +162,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsFromDangling(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsFromDangling(metaData.index("test")).build()).build();
         for (ShardRouting shard : clusterState.getRoutingNodes().shardsWithState(UNASSIGNED)) {
             assertThat(shard.unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.DANGLING_INDEX_IMPORTED));
         }
@@ -168,7 +176,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -178,7 +186,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
             builder.addIndexShard(indexShardRoutingTable);
         }
         builder.addReplica();
-        clusterState = ClusterState.builder(clusterState).routingTable(RoutingTable.builder(clusterState.routingTable()).add(builder)).build();
+        clusterState = ClusterState.builder(clusterState).routingTable(RoutingTable.builder(clusterState.routingTable()).add(builder).build()).build();
         assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));
         assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());
         assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.REPLICA_ADDED));
@@ -211,7 +219,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -241,7 +249,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test")).build()).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         // starting primaries
@@ -305,7 +313,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2")).build()).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         assertThat(UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(),
@@ -331,7 +339,7 @@ public class UnassignedInfoTests extends ESAllocationTestCase {
                 .build();
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT)
                 .metaData(metaData)
-                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2"))).build();
+                .routingTable(RoutingTable.builder().addAsNew(metaData.index("test1")).addAsNew(metaData.index("test2")).build()).build();
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().put(newNode("node1")).put(newNode("node2"))).build();
         clusterState = ClusterState.builder(clusterState).routingResult(allocation.reroute(clusterState)).build();
         assertThat(UnassignedInfo.getNumberOfDelayedUnassigned(System.currentTimeMillis(),
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
index a35e9f4..836422f 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.cluster.routing.allocation;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -235,8 +236,8 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
 
     private void assertNumIndexShardsPerNode(ClusterState state, Matcher<Integer> matcher) {
-        for (String index : state.routingTable().indicesRouting().keySet()) {
-            assertNumIndexShardsPerNode(state, index, matcher);
+        for (ObjectCursor<String> index : state.routingTable().indicesRouting().keys()) {
+            assertNumIndexShardsPerNode(state, index.value, matcher);
         }
     }
 
@@ -248,10 +249,10 @@ public class AddIncrementallyTests extends ESAllocationTestCase {
 
 
     private void assertAtLeastOneIndexShardPerNode(ClusterState state) {
-        for (String index : state.routingTable().indicesRouting().keySet()) {
+        for (ObjectCursor<String> index : state.routingTable().indicesRouting().keys()) {
 
             for (RoutingNode node : state.getRoutingNodes()) {
-                assertThat(node.shardsWithState(index, STARTED).size(), Matchers.greaterThanOrEqualTo(1));
+                assertThat(node.shardsWithState(index.value, STARTED).size(), Matchers.greaterThanOrEqualTo(1));
             }
         }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
index a983d88..96cba27 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -43,7 +42,10 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
+import static java.util.Collections.singletonMap;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -118,7 +120,7 @@ public class AllocationCommandsTests extends ESAllocationTestCase {
                 .put(newNode("node1"))
                 .put(newNode("node2"))
                 .put(newNode("node3"))
-                .put(newNode("node4", ImmutableMap.of("data", Boolean.FALSE.toString())))
+                .put(newNode("node4", singletonMap("data", Boolean.FALSE.toString())))
         ).build();
         RoutingAllocation.Result rerouteResult = allocation.reroute(clusterState);
         clusterState = ClusterState.builder(clusterState).routingTable(rerouteResult.routingTable()).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
index 7f050f3..e17ac30 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
@@ -38,9 +37,15 @@ import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
+import static java.util.Collections.singletonMap;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.UNASSIGNED;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.sameInstance;
 
 /**
  */
@@ -70,8 +75,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -89,7 +94,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node3", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -109,7 +114,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, make sure nothing moves");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node4", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
@@ -139,9 +144,9 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node3", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node3", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -159,7 +164,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node4", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -179,7 +184,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, make sure nothing moves");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node5", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node5", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
@@ -214,8 +219,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -247,7 +252,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node3", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -272,7 +277,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, some more relocation should happen");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node4", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -314,8 +319,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -333,7 +338,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node3", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -362,7 +367,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, some more relocation should happen");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node4", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -406,8 +411,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -425,7 +430,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node3", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -445,7 +450,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, we will have another relocation");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node4", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -485,10 +490,10 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node3", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node4", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node3", singletonMap("rack_id", "1")))
+                .put(newNode("node4", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -506,7 +511,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node5", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node5", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -526,7 +531,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, we will have another relocation");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node6", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node6", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -567,8 +572,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -584,7 +589,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node3", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -604,7 +609,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, make sure nothing moves");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node4", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
@@ -635,9 +640,9 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node3", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
+                .put(newNode("node3", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -653,7 +658,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node4", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -673,7 +678,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, make sure nothing moves");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node5", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node5", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         assertThat(routingTable, sameInstance(clusterState.routingTable()));
@@ -711,8 +716,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes on same rack and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "1")))
+                .put(newNode("node1", singletonMap("rack_id", "1")))
+                .put(newNode("node2", singletonMap("rack_id", "1")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -726,7 +731,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node with a new rack and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node3", ImmutableMap.of("rack_id", "2")))
+                .put(newNode("node3", singletonMap("rack_id", "2")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -750,7 +755,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add another node with a new rack, some more relocation should happen");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node4", ImmutableMap.of("rack_id", "3")))
+                .put(newNode("node4", singletonMap("rack_id", "3")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -791,8 +796,8 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes in different zones and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("A-0", ImmutableMap.of("zone", "a")))
-                .put(newNode("B-0", ImmutableMap.of("zone", "b")))
+                .put(newNode("A-0", singletonMap("zone", "a")))
+                .put(newNode("B-0", singletonMap("zone", "b")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -813,7 +818,7 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> add a new node in zone 'a' and reroute");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("A-1", ImmutableMap.of("zone", "a")))
+                .put(newNode("A-1", singletonMap("zone", "a")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -853,12 +858,12 @@ public class AwarenessAllocationTests extends ESAllocationTestCase {
 
         logger.info("--> adding 5 nodes in different zones and do rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                        .put(newNode("A-0", ImmutableMap.of("zone", "a")))
-                        .put(newNode("A-1", ImmutableMap.of("zone", "a")))
-                        .put(newNode("A-2", ImmutableMap.of("zone", "a")))
-                        .put(newNode("A-3", ImmutableMap.of("zone", "a")))
-                        .put(newNode("A-4", ImmutableMap.of("zone", "a")))
-                        .put(newNode("B-0", ImmutableMap.of("zone", "b")))
+                        .put(newNode("A-0", singletonMap("zone", "a")))
+                        .put(newNode("A-1", singletonMap("zone", "a")))
+                        .put(newNode("A-2", singletonMap("zone", "a")))
+                        .put(newNode("A-3", singletonMap("zone", "a")))
+                        .put(newNode("A-4", singletonMap("zone", "a")))
+                        .put(newNode("B-0", singletonMap("zone", "b")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
index e17fe47..2bd18f8 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.cluster.routing.allocation;
 
 import com.carrotsearch.hppc.cursors.ObjectCursor;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
@@ -27,7 +28,11 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.cluster.routing.RoutingNode;
+import org.elasticsearch.cluster.routing.RoutingNodes;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
@@ -35,9 +40,9 @@ import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllo
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.node.settings.NodeSettingsService;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.elasticsearch.test.gateway.NoopGatewayAllocator;
-import org.elasticsearch.node.settings.NodeSettingsService;
 import org.hamcrest.Matchers;
 import org.junit.Test;
 
@@ -246,11 +251,11 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         final int minAvgNumberOfShards = Math.round(Math.round(Math.floor(avgNumShards - treshold)));
         final int maxAvgNumberOfShards = Math.round(Math.round(Math.ceil(avgNumShards + treshold)));
 
-        for (String index : nodes.getRoutingTable().indicesRouting().keySet()) {
+        for (ObjectCursor<String> index : nodes.getRoutingTable().indicesRouting().keys()) {
             for (RoutingNode node : nodes) {
 //              logger.info(node.nodeId() +":"+index+ ": " + node.shardsWithState(index, INITIALIZING, STARTED).size() + " shards ("+minAvgNumberOfShards+" to "+maxAvgNumberOfShards+")");
-                assertThat(node.shardsWithState(index, STARTED).size(), Matchers.greaterThanOrEqualTo(minAvgNumberOfShards));
-                assertThat(node.shardsWithState(index, STARTED).size(), Matchers.lessThanOrEqualTo(maxAvgNumberOfShards));
+                assertThat(node.shardsWithState(index.value, STARTED).size(), Matchers.greaterThanOrEqualTo(minAvgNumberOfShards));
+                assertThat(node.shardsWithState(index.value, STARTED).size(), Matchers.lessThanOrEqualTo(maxAvgNumberOfShards));
             }
         }
     }
@@ -262,10 +267,10 @@ public class BalanceConfigurationTests extends ESAllocationTestCase {
         final int minAvgNumberOfShards = Math.round(Math.round(Math.floor(avgNumShards - treshold)));
         final int maxAvgNumberOfShards = Math.round(Math.round(Math.ceil(avgNumShards + treshold)));
 
-        for (String index : nodes.getRoutingTable().indicesRouting().keySet()) {
+        for (ObjectCursor<String> index : nodes.getRoutingTable().indicesRouting().keys()) {
             for (RoutingNode node : nodes) {
                 int primaries = 0;
-                for (ShardRouting shard : node.shardsWithState(index, STARTED)) {
+                for (ShardRouting shard : node.shardsWithState(index.value, STARTED)) {
                     primaries += shard.primary() ? 1 : 0;
                 }
 //                logger.info(node.nodeId() + ": " + primaries + " primaries ("+minAvgNumberOfShards+" to "+maxAvgNumberOfShards+")");
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
index e9a905d..eb4d62a 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java
@@ -19,14 +19,13 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.common.logging.ESLogger;
 import org.elasticsearch.common.logging.Loggers;
@@ -36,6 +35,7 @@ import org.junit.Test;
 
 import java.util.List;
 
+import static java.util.Collections.singletonMap;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
@@ -67,10 +67,10 @@ public class FilterRoutingTests extends ESAllocationTestCase {
 
         logger.info("--> adding four nodes and performing rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("tag1", "value1")))
-                .put(newNode("node2", ImmutableMap.of("tag1", "value2")))
-                .put(newNode("node3", ImmutableMap.of("tag1", "value3")))
-                .put(newNode("node4", ImmutableMap.of("tag1", "value4")))
+                .put(newNode("node1", singletonMap("tag1", "value1")))
+                .put(newNode("node2", singletonMap("tag1", "value2")))
+                .put(newNode("node3", singletonMap("tag1", "value3")))
+                .put(newNode("node4", singletonMap("tag1", "value4")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -116,10 +116,10 @@ public class FilterRoutingTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes and performing rerouting");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("tag1", "value1")))
-                .put(newNode("node2", ImmutableMap.of("tag1", "value2")))
-                .put(newNode("node3", ImmutableMap.of("tag1", "value3")))
-                .put(newNode("node4", ImmutableMap.of("tag1", "value4")))
+                .put(newNode("node1", singletonMap("tag1", "value1")))
+                .put(newNode("node2", singletonMap("tag1", "value2")))
+                .put(newNode("node3", singletonMap("tag1", "value3")))
+                .put(newNode("node4", singletonMap("tag1", "value4")))
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
index f307b63..616949e 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java
@@ -18,18 +18,21 @@
  */
 package org.elasticsearch.cluster.routing.allocation;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
-import static org.elasticsearch.cluster.routing.ShardRoutingState.*;
+import static java.util.Collections.singletonMap;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.RELOCATING;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.STARTED;
+import static org.elasticsearch.cluster.routing.ShardRoutingState.UNASSIGNED;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -65,8 +68,8 @@ public class PreferLocalPrimariesToRelocatingPrimariesTests extends ESAllocation
 
         logger.info("adding two nodes and performing rerouting till all are allocated");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("tag1", "value1")))
-                .put(newNode("node2", ImmutableMap.of("tag1", "value2")))).build();
+                .put(newNode("node1", singletonMap("tag1", "value1")))
+                .put(newNode("node2", singletonMap("tag1", "value2")))).build();
 
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
@@ -101,7 +104,7 @@ public class PreferLocalPrimariesToRelocatingPrimariesTests extends ESAllocation
 
         logger.info("start node back up");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(newNode("node1", ImmutableMap.of("tag1", "value1")))).build();
+                .put(newNode("node1", singletonMap("tag1", "value1")))).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
index 5213797..86369b9 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java
@@ -19,15 +19,14 @@
 
 package org.elasticsearch.cluster.routing.allocation;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingState;
 import org.elasticsearch.cluster.routing.allocation.decider.SameShardAllocationDecider;
 import org.elasticsearch.common.logging.ESLogger;
@@ -36,6 +35,7 @@ import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
+import static java.util.Collections.emptyMap;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.cluster.routing.allocation.RoutingNodesUtils.numberOfShardsOfType;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -62,8 +62,8 @@ public class SameShardRoutingTests extends ESAllocationTestCase {
 
         logger.info("--> adding two nodes with the same host");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(new DiscoveryNode("node1", "node1", "test1", "test1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT))
-                .put(new DiscoveryNode("node2", "node2", "test1", "test1", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT))).build();
+                .put(new DiscoveryNode("node1", "node1", "test1", "test1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT))
+                .put(new DiscoveryNode("node2", "node2", "test1", "test1", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT))).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
@@ -78,7 +78,7 @@ public class SameShardRoutingTests extends ESAllocationTestCase {
 
         logger.info("--> add another node, with a different host, replicas will be allocating");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes())
-                .put(new DiscoveryNode("node3", "node3", "test2", "test2", DummyTransportAddress.INSTANCE, ImmutableMap.<String, String>of(), Version.CURRENT))).build();
+                .put(new DiscoveryNode("node3", "node3", "test2", "test2", DummyTransportAddress.INSTANCE, emptyMap(), Version.CURRENT))).build();
         routingTable = strategy.reroute(clusterState).routingTable();
         clusterState = ClusterState.builder(clusterState).routingTable(routingTable).build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java
index dc23085..1e8a5fb 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/StartedShardsRoutingTests.java
@@ -24,7 +24,13 @@ import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.cluster.routing.AllocationId;
+import org.elasticsearch.cluster.routing.IndexRoutingTable;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.TestShardRouting;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
@@ -54,7 +60,7 @@ public class StartedShardsRoutingTests extends ESAllocationTestCase {
         stateBuilder.routingTable(RoutingTable.builder().add(IndexRoutingTable.builder("test")
                 .addIndexShard(new IndexShardRoutingTable.Builder(initShard.shardId()).addShard(initShard).build())
                 .addIndexShard(new IndexShardRoutingTable.Builder(startedShard.shardId()).addShard(startedShard).build())
-                .addIndexShard(new IndexShardRoutingTable.Builder(relocatingShard.shardId()).addShard(relocatingShard).build())));
+                .addIndexShard(new IndexShardRoutingTable.Builder(relocatingShard.shardId()).addShard(relocatingShard).build())).build());
 
         ClusterState state = stateBuilder.build();
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
index 5852faf..525c446 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java
@@ -24,7 +24,7 @@ import org.elasticsearch.cluster.ClusterInfo;
 import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.DiskUsage;
-import org.elasticsearch.cluster.MockInternalClusterInfoService;
+import org.elasticsearch.cluster.MockInternalClusterInfoService.DevNullClusterInfo;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -43,6 +43,7 @@ import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
 import org.elasticsearch.cluster.routing.allocation.command.AllocationCommand;
 import org.elasticsearch.cluster.routing.allocation.command.AllocationCommands;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.index.shard.ShardId;
@@ -51,7 +52,6 @@ import org.elasticsearch.test.gateway.NoopGatewayAllocator;
 import org.junit.Test;
 
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
@@ -78,16 +78,18 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.8).build();
 
-        Map<String, DiskUsage> usages = new HashMap<>();
-        usages.put("node1", new DiskUsage("node1", "node1", "/dev/null", 100, 10)); // 90% used
-        usages.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 35)); // 65% used
-        usages.put("node3", new DiskUsage("node3", "node3", "/dev/null", 100, 60)); // 40% used
-        usages.put("node4", new DiskUsage("node4", "node4", "/dev/null", 100, 80)); // 20% used
+        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
+        usagesBuilder.put("node1", new DiskUsage("node1", "node1", "/dev/null", 100, 10)); // 90% used
+        usagesBuilder.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 35)); // 65% used
+        usagesBuilder.put("node3", new DiskUsage("node3", "node3", "/dev/null", 100, 60)); // 40% used
+        usagesBuilder.put("node4", new DiskUsage("node4", "node4", "/dev/null", 100, 80)); // 20% used
+        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
 
-        Map<String, Long> shardSizes = new HashMap<>();
-        shardSizes.put("[test][0][p]", 10L); // 10 bytes
-        shardSizes.put("[test][0][r]", 10L);
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
+        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
+        shardSizesBuilder.put("[test][0][p]", 10L); // 10 bytes
+        shardSizesBuilder.put("[test][0][r]", 10L);
+        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
+        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -271,17 +273,19 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "30b")
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "9b").build();
 
-        Map<String, DiskUsage> usages = new HashMap<>();
-        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 10)); // 90% used
-        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 10)); // 90% used
-        usages.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 60)); // 40% used
-        usages.put("node4", new DiskUsage("node4", "n4", "/dev/null", 100, 80)); // 20% used
-        usages.put("node5", new DiskUsage("node5", "n5", "/dev/null", 100, 85)); // 15% used
+        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
+        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 10)); // 90% used
+        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 10)); // 90% used
+        usagesBuilder.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 60)); // 40% used
+        usagesBuilder.put("node4", new DiskUsage("node4", "n4", "/dev/null", 100, 80)); // 20% used
+        usagesBuilder.put("node5", new DiskUsage("node5", "n5", "/dev/null", 100, 85)); // 15% used
+        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
 
-        Map<String, Long> shardSizes = new HashMap<>();
-        shardSizes.put("[test][0][p]", 10L); // 10 bytes
-        shardSizes.put("[test][0][r]", 10L);
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
+        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
+        shardSizesBuilder.put("[test][0][p]", 10L); // 10 bytes
+        shardSizesBuilder.put("[test][0][r]", 10L);
+        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
+        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -342,8 +346,10 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         logger.info("--> nodeWithoutPrimary: {}", nodeWithoutPrimary);
 
         // Make node without the primary now habitable to replicas
-        usages.put(nodeWithoutPrimary, new DiskUsage(nodeWithoutPrimary, "", "/dev/null", 100, 35)); // 65% used
-        final ClusterInfo clusterInfo2 = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
+        usagesBuilder = ImmutableOpenMap.builder(usages);
+        usagesBuilder.put(nodeWithoutPrimary, new DiskUsage(nodeWithoutPrimary, "", "/dev/null", 100, 35)); // 65% used
+        usages = usagesBuilder.build();
+        final ClusterInfo clusterInfo2 = new DevNullClusterInfo(usages, usages, shardSizes);
         cis = new ClusterInfoService() {
             @Override
             public ClusterInfo getClusterInfo() {
@@ -536,13 +542,15 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "71%").build();
 
-        Map<String, DiskUsage> usages = new HashMap<>();
-        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 31)); // 69% used
-        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 1));  // 99% used
+        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
+        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 31)); // 69% used
+        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 1));  // 99% used
+        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
 
-        Map<String, Long> shardSizes = new HashMap<>();
-        shardSizes.put("[test][0][p]", 10L); // 10 bytes
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
+        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
+        shardSizesBuilder.put("[test][0][p]", 10L); // 10 bytes
+        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
+        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -602,14 +610,16 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.85).build();
 
-        Map<String, DiskUsage> usages = new HashMap<>();
-        usages.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 50)); // 50% used
-        usages.put("node3", new DiskUsage("node3", "node3", "/dev/null", 100, 0));  // 100% used
+        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
+        usagesBuilder.put("node2", new DiskUsage("node2", "node2", "/dev/null", 100, 50)); // 50% used
+        usagesBuilder.put("node3", new DiskUsage("node3", "node3", "/dev/null", 100, 0));  // 100% used
+        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
 
-        Map<String, Long> shardSizes = new HashMap<>();
-        shardSizes.put("[test][0][p]", 10L); // 10 bytes
-        shardSizes.put("[test][0][r]", 10L); // 10 bytes
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
+        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
+        shardSizesBuilder.put("[test][0][p]", 10L); // 10 bytes
+        shardSizesBuilder.put("[test][0][r]", 10L); // 10 bytes
+        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
+        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -673,11 +683,11 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
         RoutingNode rn = new RoutingNode("node1", newNode("node1"));
         DiskThresholdDecider decider = new DiskThresholdDecider(Settings.EMPTY);
 
-        Map<String, DiskUsage> usages = new HashMap<>();
+        ImmutableOpenMap.Builder<String, DiskUsage> usages = ImmutableOpenMap.builder();
         usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 50)); // 50% used
         usages.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 0));  // 100% used
 
-        DiskUsage node1Usage = decider.averageUsage(rn, usages);
+        DiskUsage node1Usage = decider.averageUsage(rn, usages.build());
         assertThat(node1Usage.getTotalBytes(), equalTo(100L));
         assertThat(node1Usage.getFreeBytes(), equalTo(25L));
     }
@@ -703,17 +713,19 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, 0.7)
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, 0.8).build();
 
-        Map<String, DiskUsage> usages = new HashMap<>();
-        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 40)); // 60% used
-        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 40)); // 60% used
-        usages.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 40)); // 60% used
+        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
+        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 40)); // 60% used
+        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 40)); // 60% used
+        usagesBuilder.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 40)); // 60% used
+        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
 
-        Map<String, Long> shardSizes = new HashMap<>();
-        shardSizes.put("[test][0][p]", 14L); // 14 bytes
-        shardSizes.put("[test][0][r]", 14L);
-        shardSizes.put("[test2][0][p]", 1L); // 1 bytes
-        shardSizes.put("[test2][0][r]", 1L);
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
+        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
+        shardSizesBuilder.put("[test][0][p]", 14L); // 14 bytes
+        shardSizesBuilder.put("[test][0][r]", 14L);
+        shardSizesBuilder.put("[test2][0][p]", 1L); // 1 bytes
+        shardSizesBuilder.put("[test2][0][r]", 1L);
+        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
+        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
 
         AllocationDeciders deciders = new AllocationDeciders(Settings.EMPTY,
                 new HashSet<>(Arrays.asList(
@@ -809,14 +821,17 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "70%").build();
 
         // We have an index with 2 primary shards each taking 40 bytes. Each node has 100 bytes available
-        Map<String, DiskUsage> usages = new HashMap<>();
-        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 20)); // 80% used
-        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 100)); // 0% used
+        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
+        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 20)); // 80% used
+        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 100)); // 0% used
+        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
 
-        Map<String, Long> shardSizes = new HashMap<>();
-        shardSizes.put("[test][0][p]", 40L);
-        shardSizes.put("[test][1][p]", 40L);
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
+        ImmutableOpenMap.Builder<String, Long> shardSizesBuilder = ImmutableOpenMap.builder();
+        shardSizesBuilder.put("[test][0][p]", 40L);
+        shardSizesBuilder.put("[test][1][p]", 40L);
+        ImmutableOpenMap<String, Long> shardSizes = shardSizesBuilder.build();
+
+        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes);
 
         DiskThresholdDecider diskThresholdDecider = new DiskThresholdDecider(diskSettings);
         MetaData metaData = MetaData.builder()
@@ -852,7 +867,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                                         .build()
                         )
         );
-        ClusterState clusterState = ClusterState.builder(baseClusterState).routingTable(builder).build();
+        ClusterState clusterState = ClusterState.builder(baseClusterState).routingTable(builder.build()).build();
         RoutingAllocation routingAllocation = new RoutingAllocation(null, new RoutingNodes(clusterState), discoveryNodes, clusterInfo);
         Decision decision = diskThresholdDecider.canRemain(firstRouting, firstRoutingNode, routingAllocation);
         assertThat(decision.type(), equalTo(Decision.Type.NO));
@@ -872,7 +887,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                                         .build()
                         )
         );
-        clusterState = ClusterState.builder(baseClusterState).routingTable(builder).build();
+        clusterState = ClusterState.builder(baseClusterState).routingTable(builder.build()).build();
         routingAllocation = new RoutingAllocation(null, new RoutingNodes(clusterState), discoveryNodes, clusterInfo);
         decision = diskThresholdDecider.canRemain(firstRouting, firstRoutingNode, routingAllocation);
         assertThat(decision.type(), equalTo(Decision.Type.YES));
@@ -917,16 +932,17 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, "60%")
                 .put(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, "70%").build();
 
-        Map<String, DiskUsage> usages = new HashMap<>();
-        usages.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 100)); // 0% used
-        usages.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 20));  // 80% used
-        usages.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 100)); // 0% used
+        ImmutableOpenMap.Builder<String, DiskUsage> usagesBuilder = ImmutableOpenMap.builder();
+        usagesBuilder.put("node1", new DiskUsage("node1", "n1", "/dev/null", 100, 100)); // 0% used
+        usagesBuilder.put("node2", new DiskUsage("node2", "n2", "/dev/null", 100, 20));  // 80% used
+        usagesBuilder.put("node3", new DiskUsage("node3", "n3", "/dev/null", 100, 100)); // 0% used
+        ImmutableOpenMap<String, DiskUsage> usages = usagesBuilder.build();
 
         // We have an index with 1 primary shards each taking 40 bytes. Each node has 100 bytes available
-        Map<String, Long> shardSizes = new HashMap<>();
+        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
         shardSizes.put("[test][0][p]", 40L);
         shardSizes.put("[test][1][p]", 40L);
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(usages), Collections.unmodifiableMap(usages), Collections.unmodifiableMap(shardSizes), MockInternalClusterInfoService.DEV_NULL_MAP);
+        final ClusterInfo clusterInfo = new DevNullClusterInfo(usages, usages, shardSizes.build());
 
         DiskThresholdDecider diskThresholdDecider = new DiskThresholdDecider(diskSettings);
         MetaData metaData = MetaData.builder()
@@ -970,7 +986,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                                         .build()
                         )
         );
-        ClusterState clusterState = ClusterState.builder(baseClusterState).routingTable(builder).build();
+        ClusterState clusterState = ClusterState.builder(baseClusterState).routingTable(builder.build()).build();
         RoutingAllocation routingAllocation = new RoutingAllocation(null, new RoutingNodes(clusterState), discoveryNodes, clusterInfo);
         Decision decision = diskThresholdDecider.canRemain(firstRouting, firstRoutingNode, routingAllocation);
 
@@ -1027,7 +1043,7 @@ public class DiskThresholdDeciderTests extends ESAllocationTestCase {
                         )
         );
 
-        clusterState = ClusterState.builder(updateClusterState).routingTable(builder).build();
+        clusterState = ClusterState.builder(updateClusterState).routingTable(builder.build()).build();
         routingAllocation = new RoutingAllocation(null, new RoutingNodes(clusterState), discoveryNodes, clusterInfo);
         decision = diskThresholdDecider.canRemain(firstRouting, firstRoutingNode, routingAllocation);
         assertThat(decision.type(), equalTo(Decision.Type.YES));
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
index 6460664..5417a9b 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderUnitTests.java
@@ -25,7 +25,7 @@ import org.elasticsearch.cluster.ClusterInfoService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.DiskUsage;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
-import org.elasticsearch.cluster.MockInternalClusterInfoService;
+import org.elasticsearch.cluster.MockInternalClusterInfoService.DevNullClusterInfo;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
@@ -36,6 +36,7 @@ import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.ShardRoutingHelper;
 import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
+import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.transport.DummyTransportAddress;
 import org.elasticsearch.common.transport.LocalTransportAddress;
@@ -45,9 +46,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 
@@ -123,17 +121,17 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         ).build();
 
         // actual test -- after all that bloat :)
-        Map<String, DiskUsage> leastAvailableUsages = new HashMap<>();
+        ImmutableOpenMap.Builder<String, DiskUsage> leastAvailableUsages = ImmutableOpenMap.builder();
         leastAvailableUsages.put("node_0", new DiskUsage("node_0", "node_0", "_na_", 100, 0)); // all full
         leastAvailableUsages.put("node_1", new DiskUsage("node_1", "node_1", "_na_", 100, 0)); // all full
 
-        Map<String, DiskUsage> mostAvailableUsage = new HashMap<>();
+        ImmutableOpenMap.Builder<String, DiskUsage> mostAvailableUsage = ImmutableOpenMap.builder();
         mostAvailableUsage.put("node_0", new DiskUsage("node_0", "node_0", "_na_", 100, randomIntBetween(20, 100))); // 20 - 99 percent since after allocation there must be at least 10% left and shard is 10byte
         mostAvailableUsage.put("node_1", new DiskUsage("node_1", "node_1", "_na_", 100, randomIntBetween(0, 10))); // this is weird and smells like a bug! it should be up to 20%?
 
-        Map<String, Long> shardSizes = new HashMap<>();
+        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
         shardSizes.put("[test][0][p]", 10L); // 10 bytes
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(leastAvailableUsages), Collections.unmodifiableMap(mostAvailableUsage), Collections.unmodifiableMap(shardSizes), Collections.EMPTY_MAP);
+        final ClusterInfo clusterInfo = new ClusterInfo(leastAvailableUsages.build(), mostAvailableUsage.build(), shardSizes.build(), ImmutableOpenMap.of());
         RoutingAllocation allocation = new RoutingAllocation(new AllocationDeciders(Settings.EMPTY, new AllocationDecider[]{decider}), clusterState.getRoutingNodes(), clusterState.nodes(), clusterInfo);
         assertEquals(mostAvailableUsage.toString(), Decision.YES, decider.canAllocate(test_0, new RoutingNode("node_0", node_0), allocation));
         assertEquals(mostAvailableUsage.toString(), Decision.NO, decider.canAllocate(test_0, new RoutingNode("node_1", node_1), allocation));
@@ -143,7 +141,7 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         NodeSettingsService nss = new NodeSettingsService(Settings.EMPTY);
         ClusterInfoService cis = EmptyClusterInfoService.INSTANCE;
         DiskThresholdDecider decider = new DiskThresholdDecider(Settings.EMPTY, nss, cis, null);
-        Map<ShardRouting, String> shardRoutingMap = new HashMap<>();
+        ImmutableOpenMap.Builder<ShardRouting, String> shardRoutingMap = ImmutableOpenMap.builder();
 
         DiscoveryNode node_0 = new DiscoveryNode("node_0", DummyTransportAddress.INSTANCE, Version.CURRENT);
         DiscoveryNode node_1 = new DiscoveryNode("node_1", DummyTransportAddress.INSTANCE, Version.CURRENT);
@@ -158,6 +156,16 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         ShardRoutingHelper.moveToStarted(test_1);
         shardRoutingMap.put(test_1, "/node1/least");
 
+        ShardRouting test_2 = ShardRouting.newUnassigned("test", 2, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
+        ShardRoutingHelper.initialize(test_2, node_1.getId());
+        ShardRoutingHelper.moveToStarted(test_2);
+        shardRoutingMap.put(test_2, "/node1/most");
+
+        ShardRouting test_3 = ShardRouting.newUnassigned("test", 3, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
+        ShardRoutingHelper.initialize(test_3, node_1.getId());
+        ShardRoutingHelper.moveToStarted(test_3);
+        // Intentionally not in the shardRoutingMap. We want to test what happens when we don't know where it is.
+
         MetaData metaData = MetaData.builder()
                 .put(IndexMetaData.builder("test").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1))
                 .build();
@@ -175,20 +183,20 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
         ).build();
 
         // actual test -- after all that bloat :)
-        Map<String, DiskUsage> leastAvailableUsages = new HashMap<>();
+        ImmutableOpenMap.Builder<String, DiskUsage> leastAvailableUsages = ImmutableOpenMap.builder();
         leastAvailableUsages.put("node_0", new DiskUsage("node_0", "node_0", "/node0/least", 100, 10)); // 90% used
         leastAvailableUsages.put("node_1", new DiskUsage("node_1", "node_1", "/node1/least", 100, 9)); // 91% used
 
-        Map<String, DiskUsage> mostAvailableUsage = new HashMap<>();
+        ImmutableOpenMap.Builder<String, DiskUsage> mostAvailableUsage = ImmutableOpenMap.builder();
         mostAvailableUsage.put("node_0", new DiskUsage("node_0", "node_0", "/node0/most", 100, 90)); // 10% used
         mostAvailableUsage.put("node_1", new DiskUsage("node_1", "node_1", "/node1/most", 100, 90)); // 10% used
 
-        Map<String, Long> shardSizes = new HashMap<>();
+        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
         shardSizes.put("[test][0][p]", 10L); // 10 bytes
         shardSizes.put("[test][1][p]", 10L);
         shardSizes.put("[test][2][p]", 10L);
 
-        final ClusterInfo clusterInfo = new ClusterInfo(Collections.unmodifiableMap(leastAvailableUsages), Collections.unmodifiableMap(mostAvailableUsage), Collections.unmodifiableMap(shardSizes), shardRoutingMap);
+        final ClusterInfo clusterInfo = new ClusterInfo(leastAvailableUsages.build(), mostAvailableUsage.build(), shardSizes.build(), shardRoutingMap.build());
         RoutingAllocation allocation = new RoutingAllocation(new AllocationDeciders(Settings.EMPTY, new AllocationDecider[]{decider}), clusterState.getRoutingNodes(), clusterState.nodes(), clusterInfo);
         assertEquals(Decision.YES, decider.canRemain(test_0, new RoutingNode("node_0", node_0), allocation));
         assertEquals(Decision.NO, decider.canRemain(test_1, new RoutingNode("node_1", node_1), allocation));
@@ -205,26 +213,19 @@ public class DiskThresholdDeciderUnitTests extends ESTestCase {
             // not allocated on that node
         }
 
-        ShardRouting test_2 = ShardRouting.newUnassigned("test", 2, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
-        ShardRoutingHelper.initialize(test_2, node_1.getId());
-        ShardRoutingHelper.moveToStarted(test_2);
-        shardRoutingMap.put(test_2, "/node1/most");
         assertEquals("can stay since allocated on a different path with enough space", Decision.YES, decider.canRemain(test_2, new RoutingNode("node_1", node_1), allocation));
 
-        ShardRouting test_3 = ShardRouting.newUnassigned("test", 3, null, true, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
-        ShardRoutingHelper.initialize(test_3, node_1.getId());
-        ShardRoutingHelper.moveToStarted(test_3);
         assertEquals("can stay since we don't have information about this shard", Decision.YES, decider.canRemain(test_2, new RoutingNode("node_1", node_1), allocation));
     }
 
 
     public void testShardSizeAndRelocatingSize() {
-        Map<String, Long> shardSizes = new HashMap<>();
+        ImmutableOpenMap.Builder<String, Long> shardSizes = ImmutableOpenMap.builder();
         shardSizes.put("[test][0][r]", 10L);
         shardSizes.put("[test][1][r]", 100L);
         shardSizes.put("[test][2][r]", 1000L);
         shardSizes.put("[other][0][p]", 10000L);
-        ClusterInfo info = new ClusterInfo(Collections.EMPTY_MAP, Collections.EMPTY_MAP, shardSizes, MockInternalClusterInfoService.DEV_NULL_MAP);
+        ClusterInfo info = new DevNullClusterInfo(ImmutableOpenMap.of(), ImmutableOpenMap.of(), shardSizes.build());
         ShardRouting test_0 = ShardRouting.newUnassigned("test", 0, null, false, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, "foo"));
         ShardRoutingHelper.initialize(test_0, "node1");
         ShardRoutingHelper.moveToStarted(test_0);
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java
index ed454ae..4dcc5ac 100644
--- a/core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java
@@ -24,8 +24,6 @@ import org.elasticsearch.test.ESTestCase;
 
 public class Murmur3HashFunctionTests extends ESTestCase {
 
-    private static Murmur3HashFunction HASH = new Murmur3HashFunction();
-
     public void testKnownValues() {
         assertHash(0x5a0cb7c3, "hell");
         assertHash(0xd7c31989, "hello");
@@ -37,6 +35,6 @@ public class Murmur3HashFunctionTests extends ESTestCase {
     }
 
     private static void assertHash(int expected, String stringInput) {
-        assertEquals(expected, HASH.hash(stringInput));
+        assertEquals(expected, Murmur3HashFunction.hash(stringInput));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java b/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
index fe782f1..87280f6 100644
--- a/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/serialization/DiffableTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.cluster.serialization;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.cluster.AbstractDiffable;
 import org.elasticsearch.cluster.Diff;
 import org.elasticsearch.cluster.DiffableUtils;
@@ -36,23 +35,24 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.hamcrest.CoreMatchers.equalTo;
 
 public class DiffableTests extends ESTestCase {
 
     @Test
     public void testJdkMapDiff() throws IOException {
-        ImmutableMap.Builder<String, TestDiffable> builder = ImmutableMap.builder();
-        builder.put("foo", new TestDiffable("1"));
-        builder.put("bar", new TestDiffable("2"));
-        builder.put("baz", new TestDiffable("3"));
-        ImmutableMap<String, TestDiffable> before = builder.build();
+        Map<String, TestDiffable> before = new HashMap<>();
+        before.put("foo", new TestDiffable("1"));
+        before.put("bar", new TestDiffable("2"));
+        before.put("baz", new TestDiffable("3"));
+        before = unmodifiableMap(before);
         Map<String, TestDiffable> map = new HashMap<>();
         map.putAll(before);
         map.remove("bar");
         map.put("baz", new TestDiffable("4"));
         map.put("new", new TestDiffable("5"));
-        ImmutableMap<String, TestDiffable> after = ImmutableMap.copyOf(map);
+        Map<String, TestDiffable> after = unmodifiableMap(new HashMap<>(map));
         Diff diff = DiffableUtils.diff(before, after);
         BytesStreamOutput out = new BytesStreamOutput();
         diff.writeTo(out);
diff --git a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
index d405fb1..236378e 100644
--- a/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java
@@ -19,28 +19,43 @@
 
 package org.elasticsearch.cluster.structure;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.cluster.routing.GroupShardsIterator;
+import org.elasticsearch.cluster.routing.OperationRouting;
+import org.elasticsearch.cluster.routing.PlainShardIterator;
+import org.elasticsearch.cluster.routing.RotationShardShuffler;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardIterator;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardShuffler;
+import org.elasticsearch.cluster.routing.ShardsIterator;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
-import org.elasticsearch.cluster.routing.OperationRouting;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESAllocationTestCase;
 import org.junit.Test;
 
 import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
 
+import static java.util.Collections.singletonMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.cluster.routing.ShardRoutingState.INITIALIZING;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.anyOf;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.sameInstance;
 
 public class RoutingIteratorTests extends ESAllocationTestCase {
 
@@ -231,9 +246,15 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
 
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
 
+        Map<String, String> node1Attributes = new HashMap<>();
+        node1Attributes.put("rack_id", "rack_1");
+        node1Attributes.put("zone", "zone1");
+        Map<String, String> node2Attributes = new HashMap<>();
+        node2Attributes.put("rack_id", "rack_2");
+        node2Attributes.put("zone", "zone2");
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                .put(newNode("node1", ImmutableMap.of("rack_id", "rack_1", "zone", "zone1")))
-                .put(newNode("node2", ImmutableMap.of("rack_id", "rack_2", "zone", "zone2")))
+                .put(newNode("node1", unmodifiableMap(node1Attributes)))
+                .put(newNode("node2", unmodifiableMap(node2Attributes)))
                 .localNodeId("node1")
         ).build();
         routingTable = strategy.reroute(clusterState).routingTable();
@@ -281,8 +302,8 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         ClusterState clusterState = ClusterState.builder(ClusterName.DEFAULT).metaData(metaData).routingTable(routingTable).build();
 
         clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder()
-                        .put(newNode("fred","node1", ImmutableMap.of("disk", "ebs")))
-                        .put(newNode("barney","node2", ImmutableMap.of("disk", "ephemeral")))
+                        .put(newNode("fred","node1", singletonMap("disk", "ebs")))
+                        .put(newNode("barney","node2", singletonMap("disk", "ephemeral")))
                         .localNodeId("node1")
         ).build();
 
@@ -314,7 +335,7 @@ public class RoutingIteratorTests extends ESAllocationTestCase {
         } catch (IllegalArgumentException illegal) {
             //expected exception
         }
-        
+
         shardsIterator = clusterState.routingTable().index("test").shard(0).onlyNodeSelectorActiveInitializingShardsIt("fred",clusterState.nodes());
         assertThat(shardsIterator.size(), equalTo(1));
         assertThat(shardsIterator.nextOrNull().currentNodeId(),equalTo("node1"));
diff --git a/core/src/test/java/org/elasticsearch/common/cache/CacheTests.java b/core/src/test/java/org/elasticsearch/common/cache/CacheTests.java
new file mode 100644
index 0000000..d1481a5
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/common/cache/CacheTests.java
@@ -0,0 +1,536 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.common.cache;
+
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Before;
+
+import java.util.*;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.AtomicReferenceArray;
+
+import static org.hamcrest.CoreMatchers.instanceOf;
+
+public class CacheTests extends ESTestCase {
+    private int numberOfEntries;
+
+    @Before
+    public void setUp() throws Exception {
+        super.setUp();
+        numberOfEntries = randomIntBetween(1000, 10000);
+        logger.debug("numberOfEntries: " + numberOfEntries);
+    }
+
+    // cache some entries, then randomly lookup keys that do not exist, then check the stats
+    public void testCacheStats() {
+        AtomicLong evictions = new AtomicLong();
+        Set<Integer> keys = new HashSet<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .setMaximumWeight(numberOfEntries / 2)
+                        .removalListener(notification -> {
+                            keys.remove(notification.getKey());
+                            evictions.incrementAndGet();
+                        })
+                        .build();
+
+        for (int i = 0; i < numberOfEntries; i++) {
+            // track the keys, which will be removed upon eviction (see the RemovalListener)
+            keys.add(i);
+            cache.put(i, Integer.toString(i));
+        }
+        long hits = 0;
+        long misses = 0;
+        Integer missingKey = 0;
+        for (Integer key : keys) {
+            --missingKey;
+            if (rarely()) {
+                misses++;
+                cache.get(missingKey);
+            } else {
+                hits++;
+                cache.get(key);
+            }
+        }
+        assertEquals(hits, cache.stats().getHits());
+        assertEquals(misses, cache.stats().getMisses());
+        assertEquals((long) Math.ceil(numberOfEntries / 2.0), evictions.get());
+        assertEquals(evictions.get(), cache.stats().getEvictions());
+    }
+
+    // cache some entries in batches of size maximumWeight; for each batch, touch the even entries to affect the
+    // ordering; upon the next caching of entries, the entries from the previous batch will be evicted; we can then
+    // check that the evicted entries were evicted in LRU order (first the odds in a batch, then the evens in a batch)
+    // for each batch
+    public void testCacheEvictions() {
+        int maximumWeight = randomIntBetween(1, numberOfEntries);
+        AtomicLong evictions = new AtomicLong();
+        List<Integer> evictedKeys = new ArrayList<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .setMaximumWeight(maximumWeight)
+                        .removalListener(notification -> {
+                            evictions.incrementAndGet();
+                            evictedKeys.add(notification.getKey());
+                        })
+                        .build();
+        // cache entries up to numberOfEntries - maximumWeight; all of these entries will ultimately be evicted in
+        // batches of size maximumWeight, first the odds in the batch, then the evens in the batch
+        List<Integer> expectedEvictions = new ArrayList<>();
+        int iterations = (int)Math.ceil((numberOfEntries - maximumWeight) / (1.0 * maximumWeight));
+        for (int i = 0; i < iterations; i++) {
+            for (int j = i * maximumWeight; j < (i + 1) * maximumWeight && j < numberOfEntries - maximumWeight; j++) {
+                cache.put(j, Integer.toString(j));
+                if (j % 2 == 1) {
+                    expectedEvictions.add(j);
+                }
+            }
+            for (int j = i * maximumWeight; j < (i + 1) * maximumWeight && j < numberOfEntries - maximumWeight; j++) {
+                if (j % 2 == 0) {
+                    cache.get(j);
+                    expectedEvictions.add(j);
+                }
+            }
+        }
+        // finish filling the cache
+        for (int i = numberOfEntries - maximumWeight; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        assertEquals(numberOfEntries - maximumWeight, evictions.get());
+        assertEquals(evictions.get(), cache.stats().getEvictions());
+
+        // assert that the keys were evicted in LRU order
+        Set<Integer> keys = new HashSet<>();
+        List<Integer> remainingKeys = new ArrayList<>();
+        for (Integer key : cache.keys()) {
+            keys.add(key);
+            remainingKeys.add(key);
+        }
+        assertEquals(expectedEvictions.size(), evictedKeys.size());
+        for (int i = 0; i < expectedEvictions.size(); i++) {
+            assertFalse(keys.contains(expectedEvictions.get(i)));
+            assertEquals(expectedEvictions.get(i), evictedKeys.get(i));
+        }
+        for (int i = numberOfEntries - maximumWeight; i < numberOfEntries; i++) {
+            assertTrue(keys.contains(i));
+            assertEquals(
+                    numberOfEntries - i + (numberOfEntries - maximumWeight) - 1,
+                    (int) remainingKeys.get(i - (numberOfEntries - maximumWeight))
+            );
+        }
+    }
+
+    // cache some entries and exceed the maximum weight, then check that the cache has the expected weight and the
+    // expected evictions occurred
+    public void testWeigher() {
+        int maximumWeight = 2 * numberOfEntries;
+        int weight = randomIntBetween(2, 10);
+        AtomicLong evictions = new AtomicLong();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .setMaximumWeight(maximumWeight)
+                        .weigher((k, v) -> weight)
+                        .removalListener(notification -> evictions.incrementAndGet())
+                        .build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        // cache weight should be the largest multiple of weight less than maximumWeight
+        assertEquals(weight * (maximumWeight / weight), cache.weight());
+
+        // the number of evicted entries should be the number of entries that fit in the excess weight
+        assertEquals((int) Math.ceil((weight - 2) * numberOfEntries / (1.0 * weight)), evictions.get());
+
+        assertEquals(evictions.get(), cache.stats().getEvictions());
+    }
+
+    // cache some entries, randomly invalidate some of them, then check that the weight of the cache is correct
+    public void testWeight() {
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .weigher((k, v) -> k)
+                        .build();
+        int weight = 0;
+        for (int i = 0; i < numberOfEntries; i++) {
+            weight += i;
+            cache.put(i, Integer.toString(i));
+        }
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                weight -= i;
+                cache.invalidate(i);
+            }
+        }
+        assertEquals(weight, cache.weight());
+    }
+
+    // cache some entries, randomly invalidate some of them, then check that the number of cached entries is correct
+    public void testCount() {
+        Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        int count = 0;
+        for (int i = 0; i < numberOfEntries; i++) {
+            count++;
+            cache.put(i, Integer.toString(i));
+        }
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                count--;
+                cache.invalidate(i);
+            }
+        }
+        assertEquals(count, cache.count());
+    }
+
+    // cache some entries, step the clock forward, cache some more entries, step the clock forward and then check that
+    // the first batch of cached entries expired and were removed
+    public void testExpirationAfterAccess() {
+        AtomicLong now = new AtomicLong();
+        Cache<Integer, String> cache = new Cache<Integer, String>() {
+            @Override
+            protected long now() {
+                return now.get();
+            }
+        };
+        cache.setExpireAfterAccess(1);
+        List<Integer> evictedKeys = new ArrayList<>();
+        cache.setRemovalListener(notification -> {
+            assertEquals(RemovalNotification.RemovalReason.EVICTED, notification.getRemovalReason());
+            evictedKeys.add(notification.getKey());
+        });
+        now.set(0);
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(1);
+        for (int i = numberOfEntries; i < 2 * numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(2);
+        cache.refresh();
+        assertEquals(numberOfEntries, cache.count());
+        for (int i = 0; i < evictedKeys.size(); i++) {
+            assertEquals(i, (int) evictedKeys.get(i));
+        }
+        Set<Integer> remainingKeys = new HashSet<>();
+        for (Integer key : cache.keys()) {
+            remainingKeys.add(key);
+        }
+        for (int i = numberOfEntries; i < 2 * numberOfEntries; i++) {
+            assertTrue(remainingKeys.contains(i));
+        }
+    }
+
+    public void testExpirationAfterWrite() {
+        AtomicLong now = new AtomicLong();
+        Cache<Integer, String> cache = new Cache<Integer, String>() {
+            @Override
+            protected long now() {
+                return now.get();
+            }
+        };
+        cache.setExpireAfterWrite(1);
+        List<Integer> evictedKeys = new ArrayList<>();
+        cache.setRemovalListener(notification -> {
+            assertEquals(RemovalNotification.RemovalReason.EVICTED, notification.getRemovalReason());
+            evictedKeys.add(notification.getKey());
+        });
+        now.set(0);
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(1);
+        for (int i = numberOfEntries; i < 2 * numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(2);
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.get(i);
+        }
+        cache.refresh();
+        assertEquals(numberOfEntries, cache.count());
+        for (int i = 0; i < evictedKeys.size(); i++) {
+            assertEquals(i, (int) evictedKeys.get(i));
+        }
+        Set<Integer> remainingKeys = new HashSet<>();
+        for (Integer key : cache.keys()) {
+            remainingKeys.add(key);
+        }
+        for (int i = numberOfEntries; i < 2 * numberOfEntries; i++) {
+            assertTrue(remainingKeys.contains(i));
+        }
+    }
+
+    // randomly promote some entries, step the clock forward, then check that the promoted entries remain and the
+    // non-promoted entries were removed
+    public void testPromotion() {
+        AtomicLong now = new AtomicLong();
+        Cache<Integer, String> cache = new Cache<Integer, String>() {
+            @Override
+            protected long now() {
+                return now.get();
+            }
+        };
+        cache.setExpireAfterAccess(1);
+        now.set(0);
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        now.set(1);
+        Set<Integer> promotedKeys = new HashSet<>();
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                cache.get(i);
+                promotedKeys.add(i);
+            }
+        }
+        now.set(2);
+        cache.refresh();
+        assertEquals(promotedKeys.size(), cache.count());
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (promotedKeys.contains(i)) {
+                assertNotNull(cache.get(i));
+            } else {
+                assertNull(cache.get(i));
+            }
+        }
+    }
+
+
+    // randomly invalidate some cached entries, then check that a lookup for each of those and only those keys is null
+    public void testInvalidate() {
+        Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        Set<Integer> keys = new HashSet<>();
+        for (Integer key : cache.keys()) {
+            if (rarely()) {
+                cache.invalidate(key);
+                keys.add(key);
+            }
+        }
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (keys.contains(i)) {
+                assertNull(cache.get(i));
+            } else {
+                assertNotNull(cache.get(i));
+            }
+        }
+    }
+
+    // randomly invalidate some cached entries, then check that we receive invalidate notifications for those and only
+    // those entries
+    public void testNotificationOnInvalidate() {
+        Set<Integer> notifications = new HashSet<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .removalListener(notification -> {
+                            assertEquals(RemovalNotification.RemovalReason.INVALIDATED, notification.getRemovalReason());
+                            notifications.add(notification.getKey());
+                        })
+                        .build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        Set<Integer> invalidated = new HashSet<>();
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                cache.invalidate(i);
+                invalidated.add(i);
+            }
+        }
+        assertEquals(notifications, invalidated);
+    }
+
+    // invalidate all cached entries, then check that the cache is empty
+    public void testInvalidateAll() {
+        Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        cache.invalidateAll();
+        assertEquals(0, cache.count());
+        assertEquals(0, cache.weight());
+    }
+
+    // invalidate all cached entries, then check that we receive invalidate notifications for all entries
+    public void testNotificationOnInvalidateAll() {
+        Set<Integer> notifications = new HashSet<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .removalListener(notification -> {
+                            assertEquals(RemovalNotification.RemovalReason.INVALIDATED, notification.getRemovalReason());
+                            notifications.add(notification.getKey());
+                        })
+                        .build();
+        Set<Integer> invalidated = new HashSet<>();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+            invalidated.add(i);
+        }
+        cache.invalidateAll();
+        assertEquals(invalidated, notifications);
+    }
+
+    // randomly replace some entries, increasing the weight by 1 for each replacement, then count that the cache size
+    // is correct
+    public void testReplaceRecomputesSize() {
+        class Key {
+            private int key;
+            private long weight;
+
+            public Key(int key, long weight) {
+                this.key = key;
+                this.weight = weight;
+            }
+
+            @Override
+            public boolean equals(Object o) {
+                if (this == o) return true;
+                if (o == null || getClass() != o.getClass()) return false;
+
+                Key key1 = (Key) o;
+
+                return key == key1.key;
+
+            }
+
+            @Override
+            public int hashCode() {
+                return key;
+            }
+        }
+        Cache<Key, String> cache = CacheBuilder.<Key, String>builder().weigher((k, s) -> k.weight).build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(new Key(i, 1), Integer.toString(i));
+        }
+        assertEquals(numberOfEntries, cache.count());
+        assertEquals(numberOfEntries, cache.weight());
+        int replaced = 0;
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                replaced++;
+                cache.put(new Key(i, 2), Integer.toString(i));
+            }
+        }
+        assertEquals(numberOfEntries, cache.count());
+        assertEquals(numberOfEntries + replaced, cache.weight());
+    }
+
+    // randomly replace some entries, then check that we received replacement notifications for those and only those
+    // entries
+    public void testNotificationOnReplace() {
+        Set<Integer> notifications = new HashSet<>();
+        Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .removalListener(notification -> {
+                            assertEquals(RemovalNotification.RemovalReason.REPLACED, notification.getRemovalReason());
+                            notifications.add(notification.getKey());
+                        })
+                        .build();
+        for (int i = 0; i < numberOfEntries; i++) {
+            cache.put(i, Integer.toString(i));
+        }
+        Set<Integer> replacements = new HashSet<>();
+        for (int i = 0; i < numberOfEntries; i++) {
+            if (rarely()) {
+                cache.put(i, Integer.toString(i) + Integer.toString(i));
+                replacements.add(i);
+            }
+        }
+        assertEquals(replacements, notifications);
+    }
+
+    public void testComputeIfAbsentCallsOnce() throws InterruptedException {
+        int numberOfThreads = randomIntBetween(2, 200);
+        final Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        List<Thread> threads = new ArrayList<>();
+        AtomicReferenceArray flags = new AtomicReferenceArray(numberOfEntries);
+        for (int j = 0; j < numberOfEntries; j++) {
+            flags.set(j, false);
+        }
+        CountDownLatch latch = new CountDownLatch(1 + numberOfThreads);
+        for (int i = 0; i < numberOfThreads; i++) {
+            Thread thread = new Thread(() -> {
+                latch.countDown();
+                for (int j = 0; j < numberOfEntries; j++) {
+                    try {
+                        cache.computeIfAbsent(j, key -> {
+                            assertTrue(flags.compareAndSet(key, false, true));
+                            return Integer.toString(key);
+                        });
+                    } catch (ExecutionException e) {
+                        throw new RuntimeException(e);
+                    }
+                }
+            });
+            threads.add(thread);
+            thread.start();
+        }
+        latch.countDown();
+        for (Thread thread : threads) {
+            thread.join();
+        }
+    }
+
+    public void testComputeIfAbsentThrowsExceptionIfLoaderReturnsANullValue() {
+        final Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();
+        try {
+            cache.computeIfAbsent(1, k -> null);
+            fail("expected ExecutionException");
+        } catch (ExecutionException e) {
+            assertThat(e.getCause(), instanceOf(NullPointerException.class));
+        }
+    }
+
+    // test that the cache is not corrupted under lots of concurrent modifications, even hitting the same key
+    // here be dragons: this test did catch one subtle bug during development; do not remove lightly
+    public void testTorture() throws InterruptedException {
+        int numberOfThreads = randomIntBetween(2, 200);
+        final Cache<Integer, String> cache =
+                CacheBuilder.<Integer, String>builder()
+                        .setMaximumWeight(1000)
+                        .weigher((k, v) -> 2)
+                        .build();
+
+        CountDownLatch latch = new CountDownLatch(1 + numberOfThreads);
+        List<Thread> threads = new ArrayList<>();
+        for (int i = 0; i < numberOfThreads; i++) {
+            Thread thread = new Thread(() -> {
+                Random random = new Random(random().nextLong());
+                latch.countDown();
+                for (int j = 0; j < numberOfEntries; j++) {
+                    Integer key = random.nextInt(numberOfEntries);
+                    cache.put(key, Integer.toString(j));
+                }
+            });
+            threads.add(thread);
+            thread.start();
+        }
+        latch.countDown();
+        for (Thread thread : threads) {
+            thread.join();
+        }
+        cache.refresh();
+        assertEquals(500, cache.count());
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/common/cli/CliToolTests.java b/core/src/test/java/org/elasticsearch/common/cli/CliToolTests.java
index 748e417..f275d1d 100644
--- a/core/src/test/java/org/elasticsearch/common/cli/CliToolTests.java
+++ b/core/src/test/java/org/elasticsearch/common/cli/CliToolTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.common.cli;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.commons.cli.CommandLine;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.Strings;
@@ -29,14 +28,20 @@ import org.elasticsearch.node.internal.InternalSettingsPreparer;
 import org.junit.Test;
 
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.common.cli.CliTool.ExitStatus.OK;
 import static org.elasticsearch.common.cli.CliTool.ExitStatus.USAGE;
 import static org.elasticsearch.common.cli.CliToolConfig.Builder.cmd;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.arrayContaining;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.hasItem;
+import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.is;
 
 /**
  *
@@ -387,11 +392,11 @@ public class CliToolTests extends CliToolTestCase {
             super(CliToolConfig.config(name, MultiCmdTool.class)
                     .cmds(cmds(commands))
                     .build(), terminal);
-            ImmutableMap.Builder<String, Command> commandByName = ImmutableMap.builder();
+            Map<String, Command> commandByName = new HashMap<>();
             for (int i = 0; i < commands.length; i++) {
                 commandByName.put(commands[i].name, commands[i]);
             }
-            this.commands = commandByName.build();
+            this.commands = unmodifiableMap(commandByName);
         }
 
         @Override
diff --git a/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java b/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java
index f6372e5..d35b540 100644
--- a/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java
+++ b/core/src/test/java/org/elasticsearch/common/collect/CopyOnWriteHashMapTests.java
@@ -19,12 +19,13 @@
 
 package org.elasticsearch.common.collect;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.test.ESTestCase;
 
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+
 public class CopyOnWriteHashMapTests extends ESTestCase {
 
     private static class O {
@@ -94,7 +95,7 @@ public class CopyOnWriteHashMapTests extends ESTestCase {
                 map = newMap;
             }
             assertEquals(ref, CopyOnWriteHashMap.copyOf(ref));
-            assertEquals(ImmutableMap.of(), CopyOnWriteHashMap.copyOf(ref).copyAndRemoveAll(ref.keySet()));
+            assertEquals(emptyMap(), CopyOnWriteHashMap.copyOf(ref).copyAndRemoveAll(ref.keySet()));
         }
     }
 
@@ -140,7 +141,7 @@ public class CopyOnWriteHashMapTests extends ESTestCase {
         } catch (IllegalArgumentException e) {
             // expected
         }
-        
+
         try {
             new CopyOnWriteHashMap<>().copyAndPut(null, "b");
             fail();
diff --git a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
index 9b327fb..7901694 100644
--- a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
+++ b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
@@ -166,8 +166,10 @@ public abstract class ModuleTestCase extends ESTestCase {
                 }
             } else  if (element instanceof ProviderInstanceBinding) {
                 ProviderInstanceBinding binding = (ProviderInstanceBinding) element;
-                assertTrue(tester.test(to.cast(binding.getProviderInstance().get())));
-                return;
+                if (to.equals(binding.getKey().getTypeLiteral().getType())) {
+                    assertTrue(tester.test(to.cast(binding.getProviderInstance().get())));
+                    return;
+                }
             }
         }
         StringBuilder s = new StringBuilder();
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
index 2fb90c7..13ac6fd 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/LuceneTests.java
@@ -20,10 +20,14 @@ package org.elasticsearch.common.lucene;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.*;
 import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.util.Version;
 import org.elasticsearch.test.ESTestCase;
@@ -322,4 +326,37 @@ public class LuceneTests extends ESTestCase {
         writer.close();
         dir.close();
     }
+
+    public void testCount() throws Exception {
+        Directory dir = newDirectory();
+        RandomIndexWriter w = new RandomIndexWriter(getRandom(), dir);
+
+        try (DirectoryReader reader = w.getReader()) {
+            // match_all does not match anything on an empty index
+            IndexSearcher searcher = newSearcher(reader);
+            assertFalse(Lucene.exists(searcher, new MatchAllDocsQuery()));
+        }
+
+        Document doc = new Document();
+        w.addDocument(doc);
+
+        doc.add(new StringField("foo", "bar", Store.NO));
+        w.addDocument(doc);
+
+        try (DirectoryReader reader = w.getReader()) {
+            IndexSearcher searcher = newSearcher(reader);
+            assertTrue(Lucene.exists(searcher, new MatchAllDocsQuery()));
+            assertFalse(Lucene.exists(searcher, new TermQuery(new Term("baz", "bar"))));
+            assertTrue(Lucene.exists(searcher, new TermQuery(new Term("foo", "bar"))));
+        }
+
+        w.deleteDocuments(new Term("foo", "bar"));
+        try (DirectoryReader reader = w.getReader()) {
+            IndexSearcher searcher = newSearcher(reader);
+            assertFalse(Lucene.exists(searcher, new TermQuery(new Term("foo", "bar"))));
+        }
+
+        w.close();
+        dir.close();
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java b/core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java
index cdf7db9..43e151e 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java
@@ -44,23 +44,23 @@ public class MultiPhrasePrefixQueryTests extends ESTestCase {
 
         MultiPhrasePrefixQuery query = new MultiPhrasePrefixQuery();
         query.add(new Term("field", "aa"));
-        assertThat(Lucene.count(searcher, query), equalTo(1l));
+        assertThat(searcher.count(query), equalTo(1));
 
         query = new MultiPhrasePrefixQuery();
         query.add(new Term("field", "aaa"));
         query.add(new Term("field", "bb"));
-        assertThat(Lucene.count(searcher, query), equalTo(1l));
+        assertThat(searcher.count(query), equalTo(1));
 
         query = new MultiPhrasePrefixQuery();
         query.setSlop(1);
         query.add(new Term("field", "aaa"));
         query.add(new Term("field", "cc"));
-        assertThat(Lucene.count(searcher, query), equalTo(1l));
+        assertThat(searcher.count(query), equalTo(1));
 
         query = new MultiPhrasePrefixQuery();
         query.setSlop(1);
         query.add(new Term("field", "xxx"));
-        assertThat(Lucene.count(searcher, query), equalTo(0l));
+        assertThat(searcher.count(query), equalTo(0));
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java b/core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java
index 5db7e7e..119c595 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/search/morelikethis/MoreLikeThisQueryTests.java
@@ -65,7 +65,7 @@ public class MoreLikeThisQueryTests extends ESTestCase {
         mltQuery.setLikeText("lucene");
         mltQuery.setMinTermFrequency(1);
         mltQuery.setMinDocFreq(1);
-        long count = Lucene.count(searcher, mltQuery);
+        long count = searcher.count(mltQuery);
         assertThat(count, equalTo(2l));
 
         reader.close();
diff --git a/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java b/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
index 290af55..98a4364 100644
--- a/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java
@@ -18,16 +18,25 @@
  */
 package org.elasticsearch.common.lucene.uid;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.core.KeywordAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.document.*;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.index.*;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexOptions;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.Numbers;
@@ -41,13 +50,17 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
 
 public class VersionsTests extends ESTestCase {
-    
+
     public static DirectoryReader reopen(DirectoryReader reader) throws IOException {
         return reopen(reader, true);
     }
@@ -99,7 +112,7 @@ public class VersionsTests extends ESTestCase {
         doc.add(uid);
         doc.add(version);
         writer.updateDocument(new Term(UidFieldMapper.NAME, "1"), doc);
-        
+
         directoryReader = reopen(directoryReader);
         assertThat(Versions.loadVersion(directoryReader, new Term(UidFieldMapper.NAME, "1")), equalTo(3l));
         assertThat(Versions.loadDocIdAndVersion(directoryReader, new Term(UidFieldMapper.NAME, "1")).version, equalTo(3l));
@@ -261,8 +274,13 @@ public class VersionsTests extends ESTestCase {
         iw.addDocument(document);
         iw.commit();
 
-        final Map<String, Long> expectedVersions = ImmutableMap.<String, Long>builder()
-                .put("1", 0L).put("2", 0L).put("3", 0L).put("4", 4L).put("5", 5L).put("6", 6L).build();
+        Map<String, Long> expectedVersions = new HashMap<>();
+        expectedVersions.put("1", 0L);
+        expectedVersions.put("2", 0L);
+        expectedVersions.put("3", 0L);
+        expectedVersions.put("4", 4L);
+        expectedVersions.put("5", 5L);
+        expectedVersions.put("6", 6L);
 
         // Force merge and check versions
         iw.forceMerge(1, true);
diff --git a/core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java b/core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java
index aec4fb2..1309b58 100644
--- a/core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java
+++ b/core/src/test/java/org/elasticsearch/common/path/PathTrieTests.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.common.path;
 
 import org.elasticsearch.test.ESTestCase;
-import org.junit.Test;
 
 import java.util.HashMap;
 import java.util.Map;
diff --git a/core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java b/core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java
index ba34812..abce42b 100644
--- a/core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java
+++ b/core/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.common.xcontent.support;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.xcontent.XContentBuilder;
@@ -39,7 +37,12 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static org.hamcrest.Matchers.*;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonMap;
+import static org.hamcrest.Matchers.hasEntry;
+import static org.hamcrest.Matchers.hasKey;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.nullValue;
 import static org.hamcrest.core.IsEqual.equalTo;
 
 /**
@@ -561,7 +564,7 @@ public class XContentMapValuesTests extends ESTestCase {
                 assertEquals(XContentParser.Token.START_ARRAY, parser.nextToken());
             }
             assertEquals(
-                    Arrays.asList(ImmutableMap.of("foo", "bar"), Collections.<String, Object>emptyMap()),
+                    Arrays.asList(singletonMap("foo", "bar"), emptyMap()),
                     parser.list());
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
index 2ac6900..ca95e50 100644
--- a/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
+++ b/core/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptionsIT.java
@@ -31,7 +31,7 @@ import org.elasticsearch.cluster.block.ClusterBlockLevel;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.DjbHashFunction;
+import org.elasticsearch.cluster.routing.Murmur3HashFunction;
 import org.elasticsearch.cluster.routing.allocation.command.MoveAllocationCommand;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.Priority;
@@ -441,7 +441,7 @@ public class DiscoveryWithServiceDisruptionsIT extends ESIntegTestCase {
                                 logger.info("[{}] Acquired semaphore and it has {} permits left", name, semaphore.availablePermits());
                                 try {
                                     id = Integer.toString(idGenerator.incrementAndGet());
-                                    int shard = ((InternalTestCluster) cluster()).getInstance(DjbHashFunction.class).hash(id) % numPrimaries;
+                                    int shard = Murmur3HashFunction.hash(id) % numPrimaries;
                                     logger.trace("[{}] indexing id [{}] through node [{}] targeting shard [{}]", name, id, node, shard);
                                     IndexResponse response = client.prepareIndex("test", "type", id).setSource("{}").setTimeout("1s").get();
                                     assertThat(response.getVersion(), equalTo(1l));
diff --git a/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java b/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
index a39b154..9d1ce5c 100644
--- a/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
+++ b/core/src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.discovery;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.cluster.ClusterState;
@@ -45,6 +44,7 @@ import org.junit.Test;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
 
+import static java.util.Collections.emptyMap;
 import static org.hamcrest.Matchers.equalTo;
 
 public class ZenFaultDetectionTests extends ESTestCase {
@@ -65,9 +65,9 @@ public class ZenFaultDetectionTests extends ESTestCase {
         super.setUp();
         threadPool = new ThreadPool(getClass().getName());
         serviceA = build(Settings.builder().put("name", "TS_A").build(), version0);
-        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version0);
+        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), emptyMap(), version0);
         serviceB = build(Settings.builder().put("name", "TS_B").build(), version1);
-        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version1);
+        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), emptyMap(), version1);
 
         // wait till all nodes are properly connected and the event has been sent, so tests in this class
         // will not get this callback called on the connections done in this setup
diff --git a/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java b/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
index da7f440..6cf8ba7 100644
--- a/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
+++ b/core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java
@@ -32,8 +32,6 @@ import org.elasticsearch.action.index.IndexResponse;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.search.MultiMatchQuery;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -165,6 +163,13 @@ public class DocumentActionsIT extends ESIntegTestCase {
             assertThat(countResponse.getSuccessfulShards(), equalTo(numShards.numPrimaries));
             assertThat(countResponse.getFailedShards(), equalTo(0));
 
+            // test failed (simply query that can't be parsed)
+            try {
+                client().count(countRequest("test").source("{ term : { _type : \"type1 } }")).actionGet();
+            } catch(SearchPhaseExecutionException e) {
+                assertThat(e.shardFailures().length, equalTo(numShards.numPrimaries));
+            }
+
             // count with no query is a match all one
             countResponse = client().prepareCount("test").execute().actionGet();
             assertThat("Failures " + countResponse.getShardFailures(), countResponse.getShardFailures() == null ? 0 : countResponse.getShardFailures().length, equalTo(0));
diff --git a/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java b/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
index 2b4ccf7..e692b62 100644
--- a/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
+++ b/core/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorTests.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.gateway;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
+
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.ClusterInfo;
 import org.elasticsearch.cluster.ClusterState;
@@ -27,7 +28,15 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.node.DiscoveryNodes;
-import org.elasticsearch.cluster.routing.*;
+import org.elasticsearch.cluster.routing.IndexRoutingTable;
+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;
+import org.elasticsearch.cluster.routing.RoutingNode;
+import org.elasticsearch.cluster.routing.RoutingNodes;
+import org.elasticsearch.cluster.routing.RoutingTable;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.ShardRoutingState;
+import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.UnassignedInfo;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
 import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;
@@ -49,6 +58,7 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 
+import static java.util.Collections.unmodifiableMap;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -358,7 +368,8 @@ public class ReplicaShardAllocatorTests extends ESAllocationTestCase {
             if (syncId != null) {
                 commitData.put(Engine.SYNC_COMMIT_ID, syncId);
             }
-            data.put(node, new TransportNodesListShardStoreMetaData.StoreFilesMetaData(allocated, shardId, new Store.MetadataSnapshot(filesAsMap, commitData, randomInt())));
+            data.put(node, new TransportNodesListShardStoreMetaData.StoreFilesMetaData(allocated, shardId,
+                    new Store.MetadataSnapshot(unmodifiableMap(filesAsMap), unmodifiableMap(commitData), randomInt())));
             return this;
         }
 
diff --git a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
index 13957b7..ff1e885 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexModuleTests.java
@@ -20,7 +20,9 @@ package org.elasticsearch.index;
 
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.search.IndexSearcher;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.inject.ModuleTestCase;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.engine.EngineConfig;
 import org.elasticsearch.index.engine.EngineException;
 import org.elasticsearch.index.engine.EngineFactory;
@@ -31,23 +33,25 @@ import org.elasticsearch.test.engine.MockEngineFactory;
 public class IndexModuleTests extends ModuleTestCase {
 
     public void testWrapperIsBound() {
-        IndexModule module = new IndexModule();
+        IndexModule module = new IndexModule(IndexMetaData.PROTO);
         assertInstanceBinding(module, IndexSearcherWrapper.class,(x) -> x == null);
         module.indexSearcherWrapper = Wrapper.class;
         assertBinding(module, IndexSearcherWrapper.class, Wrapper.class);
     }
 
     public void testEngineFactoryBound() {
-        IndexModule module = new IndexModule();
+        IndexModule module = new IndexModule(IndexMetaData.PROTO);
         assertBinding(module, EngineFactory.class, InternalEngineFactory.class);
         module.engineFactoryImpl = MockEngineFactory.class;
         assertBinding(module, EngineFactory.class, MockEngineFactory.class);
     }
 
     public void testOtherServiceBound() {
-        IndexModule module = new IndexModule();
+        final IndexMetaData meta = IndexMetaData.builder(IndexMetaData.PROTO).index("foo").build();
+        IndexModule module = new IndexModule(meta);
         assertBinding(module, IndexService.class, IndexService.class);
         assertBinding(module, IndexServicesProvider.class, IndexServicesProvider.class);
+        assertInstanceBinding(module, IndexMetaData.class, (x) -> x == meta);
     }
 
     public static final class Wrapper implements IndexSearcherWrapper {
diff --git a/core/src/test/java/org/elasticsearch/index/IndexServiceTests.java b/core/src/test/java/org/elasticsearch/index/IndexServiceTests.java
index 7d66382..412141c 100644
--- a/core/src/test/java/org/elasticsearch/index/IndexServiceTests.java
+++ b/core/src/test/java/org/elasticsearch/index/IndexServiceTests.java
@@ -19,14 +19,27 @@
 
 package org.elasticsearch.index;
 
+import org.elasticsearch.cluster.metadata.AliasMetaData;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.Nullable;
+import org.elasticsearch.common.compress.CompressedXContent;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.test.ESTestCase;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.indices.InvalidAliasNameException;
+import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.junit.Test;
 
+import java.io.IOException;
+
+import static org.elasticsearch.index.query.QueryBuilders.termQuery;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.nullValue;
+
 /** Unit test(s) for IndexService */
-public class IndexServiceTests extends ESTestCase {
+public class IndexServiceTests extends ESSingleNodeTestCase {
 
     @Test
     public void testDetermineShadowEngineShouldBeUsed() {
@@ -46,4 +59,92 @@ public class IndexServiceTests extends ESTestCase {
         assertFalse("no shadow replicas for primary shard with shadow settings", IndexService.useShadowEngine(true, shadowSettings));
         assertTrue("shadow replicas for replica shards with shadow settings",IndexService.useShadowEngine(false, shadowSettings));
     }
+
+    public IndexService newIndexService() {
+        Settings settings = Settings.builder().put("name", "indexServiceTests").build();
+        return createIndex("test", settings);
+    }
+
+
+    public static CompressedXContent filter(QueryBuilder filterBuilder) throws IOException {
+        XContentBuilder builder = XContentFactory.jsonBuilder();
+        filterBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);
+        builder.close();
+        return new CompressedXContent(builder.string());
+    }
+
+    @Test
+    public void testFilteringAliases() throws Exception {
+
+        IndexService indexService = newIndexService();
+        add(indexService, "cats", filter(termQuery("animal", "cat")));
+        add(indexService, "dogs", filter(termQuery("animal", "dog")));
+        add(indexService, "all", null);
+
+        assertThat(indexService.getMetaData().getAliases().containsKey("cats"), equalTo(true));
+        assertThat(indexService.getMetaData().getAliases().containsKey("dogs"), equalTo(true));
+        assertThat(indexService.getMetaData().getAliases().containsKey("turtles"), equalTo(false));
+
+        assertThat(indexService.aliasFilter("cats").toString(), equalTo("animal:cat"));
+        assertThat(indexService.aliasFilter("cats", "dogs").toString(), equalTo("animal:cat animal:dog"));
+
+        // Non-filtering alias should turn off all filters because filters are ORed
+        assertThat(indexService.aliasFilter("all"), nullValue());
+        assertThat(indexService.aliasFilter("cats", "all"), nullValue());
+        assertThat(indexService.aliasFilter("all", "cats"), nullValue());
+
+        add(indexService, "cats", filter(termQuery("animal", "feline")));
+        add(indexService, "dogs", filter(termQuery("animal", "canine")));
+        assertThat(indexService.aliasFilter("dogs", "cats").toString(), equalTo("animal:canine animal:feline"));
+    }
+
+    @Test
+    public void testAliasFilters() throws Exception {
+        IndexService indexService = newIndexService();
+        add(indexService, "cats", filter(termQuery("animal", "cat")));
+        add(indexService, "dogs", filter(termQuery("animal", "dog")));
+
+        assertThat(indexService.aliasFilter(), nullValue());
+        assertThat(indexService.aliasFilter("dogs").toString(), equalTo("animal:dog"));
+        assertThat(indexService.aliasFilter("dogs", "cats").toString(), equalTo("animal:dog animal:cat"));
+
+        add(indexService, "cats", filter(termQuery("animal", "feline")));
+        add(indexService, "dogs", filter(termQuery("animal", "canine")));
+
+        assertThat(indexService.aliasFilter("dogs", "cats").toString(), equalTo("animal:canine animal:feline"));
+    }
+
+    @Test(expected = InvalidAliasNameException.class)
+    public void testRemovedAliasFilter() throws Exception {
+        IndexService indexService = newIndexService();
+
+        add(indexService, "cats", filter(termQuery("animal", "cat")));
+        remove(indexService, "cats");
+        indexService.aliasFilter("cats");
+    }
+
+
+    @Test
+    public void testUnknownAliasFilter() throws Exception {
+        IndexService indexService = newIndexService();
+        add(indexService, "cats", filter(termQuery("animal", "cat")));
+        add(indexService, "dogs", filter(termQuery("animal", "dog")));
+
+        try {
+            indexService.aliasFilter("unknown");
+            fail();
+        } catch (InvalidAliasNameException e) {
+            // all is well
+        }
+    }
+
+    private void remove(IndexService service, String alias) {
+        IndexMetaData build = IndexMetaData.builder(service.getMetaData()).removeAlias(alias).build();
+        service.updateMetaData(build);
+    }
+
+    private void add(IndexService service, String alias, @Nullable CompressedXContent filter) {
+        IndexMetaData build = IndexMetaData.builder(service.getMetaData()).putAlias(AliasMetaData.builder(alias).filter(filter).build()).build();
+        service.updateMetaData(build);
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/aliases/IndexAliasesServiceTests.java b/core/src/test/java/org/elasticsearch/index/aliases/IndexAliasesServiceTests.java
deleted file mode 100644
index cb6a242..0000000
--- a/core/src/test/java/org/elasticsearch/index/aliases/IndexAliasesServiceTests.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.aliases;
-
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.indices.InvalidAliasNameException;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.nullValue;
-
-/**
- *
- */
-public class IndexAliasesServiceTests extends ESSingleNodeTestCase {
-
-    public IndexAliasesService newIndexAliasesService() {
-        Settings settings = Settings.builder().put("name", "IndexAliasesServiceTests").build();
-        IndexService indexService = createIndex("test", settings);
-        return indexService.aliasesService();
-    }
-
-    public static CompressedXContent filter(QueryBuilder filterBuilder) throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        filterBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);
-        builder.close();
-        return new CompressedXContent(builder.string());
-    }
-
-    @Test
-    public void testFilteringAliases() throws Exception {
-        IndexAliasesService indexAliasesService = newIndexAliasesService();
-        indexAliasesService.add("cats", filter(termQuery("animal", "cat")));
-        indexAliasesService.add("dogs", filter(termQuery("animal", "dog")));
-        indexAliasesService.add("all", null);
-
-        assertThat(indexAliasesService.hasAlias("cats"), equalTo(true));
-        assertThat(indexAliasesService.hasAlias("dogs"), equalTo(true));
-        assertThat(indexAliasesService.hasAlias("turtles"), equalTo(false));
-
-        assertThat(indexAliasesService.aliasFilter("cats").toString(), equalTo("animal:cat"));
-        assertThat(indexAliasesService.aliasFilter("cats", "dogs").toString(), equalTo("animal:cat animal:dog"));
-
-        // Non-filtering alias should turn off all filters because filters are ORed
-        assertThat(indexAliasesService.aliasFilter("all"), nullValue());
-        assertThat(indexAliasesService.aliasFilter("cats", "all"), nullValue());
-        assertThat(indexAliasesService.aliasFilter("all", "cats"), nullValue());
-
-        indexAliasesService.add("cats", filter(termQuery("animal", "feline")));
-        indexAliasesService.add("dogs", filter(termQuery("animal", "canine")));
-        assertThat(indexAliasesService.aliasFilter("dogs", "cats").toString(), equalTo("animal:canine animal:feline"));
-    }
-
-    @Test
-    public void testAliasFilters() throws Exception {
-        IndexAliasesService indexAliasesService = newIndexAliasesService();
-        indexAliasesService.add("cats", filter(termQuery("animal", "cat")));
-        indexAliasesService.add("dogs", filter(termQuery("animal", "dog")));
-
-        assertThat(indexAliasesService.aliasFilter(), nullValue());
-        assertThat(indexAliasesService.aliasFilter("dogs").toString(), equalTo("animal:dog"));
-        assertThat(indexAliasesService.aliasFilter("dogs", "cats").toString(), equalTo("animal:dog animal:cat"));
-
-        indexAliasesService.add("cats", filter(termQuery("animal", "feline")));
-        indexAliasesService.add("dogs", filter(termQuery("animal", "canine")));
-
-        assertThat(indexAliasesService.aliasFilter("dogs", "cats").toString(), equalTo("animal:canine animal:feline"));
-    }
-
-    @Test(expected = InvalidAliasNameException.class)
-    public void testRemovedAliasFilter() throws Exception {
-        IndexAliasesService indexAliasesService = newIndexAliasesService();
-        indexAliasesService.add("cats", filter(termQuery("animal", "cat")));
-        indexAliasesService.remove("cats");
-        indexAliasesService.aliasFilter("cats");
-    }
-
-
-    @Test
-    public void testUnknownAliasFilter() throws Exception {
-        IndexAliasesService indexAliasesService = newIndexAliasesService();
-        indexAliasesService.add("cats", filter(termQuery("animal", "cat")));
-        indexAliasesService.add("dogs", filter(termQuery("animal", "dog")));
-
-        try {
-            indexAliasesService.aliasFilter("unknown");
-            fail();
-        } catch (InvalidAliasNameException e) {
-            // all is well
-        }
-    }
-
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
index 6a96086..c781a58 100644
--- a/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
+++ b/core/src/test/java/org/elasticsearch/index/cache/bitset/BitSetFilterCacheTests.java
@@ -96,7 +96,7 @@ public class BitSetFilterCacheTests extends ESTestCase {
         // now cached
         assertThat(matchCount(filter, reader), equalTo(3));
         // There are 3 segments
-        assertThat(cache.getLoadedFilters().size(), equalTo(3l));
+        assertThat(cache.getLoadedFilters().weight(), equalTo(3L));
 
         writer.forceMerge(1);
         reader.close();
@@ -108,12 +108,12 @@ public class BitSetFilterCacheTests extends ESTestCase {
         // now cached
         assertThat(matchCount(filter, reader), equalTo(3));
         // Only one segment now, so the size must be 1
-        assertThat(cache.getLoadedFilters().size(), equalTo(1l));
+        assertThat(cache.getLoadedFilters().weight(), equalTo(1L));
 
         reader.close();
         writer.close();
         // There is no reference from readers and writer to any segment in the test index, so the size in the fbs cache must be 0
-        assertThat(cache.getLoadedFilters().size(), equalTo(0l));
+        assertThat(cache.getLoadedFilters().weight(), equalTo(0L));
     }
 
     public void testListener() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java b/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java
deleted file mode 100644
index 984e151..0000000
--- a/core/src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.codec.postingformat;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterLeafReader;
-import org.apache.lucene.index.SegmentWriteState;
-import org.elasticsearch.common.util.BloomFilter;
-import org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat;
-import org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.BloomFilteredFieldsConsumer;
-import org.elasticsearch.index.codec.postingsformat.Elasticsearch090PostingsFormat;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.stream.StreamSupport;
-
-/** read-write version with blooms for testing */
-public class Elasticsearch090RWPostingsFormat extends Elasticsearch090PostingsFormat {
-    @Override
-    public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-        final PostingsFormat delegate = getDefaultWrapped();
-        final BloomFilteredFieldsConsumer fieldsConsumer = new BloomFilterPostingsFormat(delegate, BloomFilter.Factory.DEFAULT) {
-            @Override
-            public BloomFilteredFieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-                return new BloomFilteredFieldsConsumer(delegate.fieldsConsumer(state), state,delegate);
-            } 
-        }.fieldsConsumer(state);
-        return new FieldsConsumer() {
-
-            @Override
-            public void write(Fields fields) throws IOException {
-
-                Fields maskedFields = new FilterLeafReader.FilterFields(fields) {
-                    @Override
-                    public Iterator<String> iterator() {
-                        return StreamSupport.stream(this.in.spliterator(), false).filter(UID_FIELD_FILTER.negate()).iterator();
-                    }
-                };
-                fieldsConsumer.getDelegate().write(maskedFields);
-                maskedFields = new FilterLeafReader.FilterFields(fields) {
-                    @Override
-                    public Iterator<String> iterator() {
-                        return Collections.singleton(UidFieldMapper.NAME).iterator();
-                    }
-                };
-                // only go through bloom for the UID field
-                fieldsConsumer.write(maskedFields);
-            }
-
-            @Override
-            public void close() throws IOException {
-                fieldsConsumer.close();
-            }
-        };
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java b/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java
deleted file mode 100644
index f988445..0000000
--- a/core/src/test/java/org/elasticsearch/index/codec/postingformat/PostingsFormatTests.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.codec.postingformat;
-
-import com.carrotsearch.randomizedtesting.annotations.Listeners;
-import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BasePostingsFormatTestCase;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TimeUnits;
-import org.elasticsearch.test.junit.listeners.ReproduceInfoPrinter;
-
-/** Runs elasticsearch postings format against lucene's standard postings format tests */
-@Listeners({
-        ReproduceInfoPrinter.class
-})
-@TimeoutSuite(millis = TimeUnits.HOUR)
-@LuceneTestCase.SuppressSysoutChecks(bugUrl = "we log a lot on purpose")
-public class PostingsFormatTests extends BasePostingsFormatTestCase {
-
-    @Override
-    protected Codec getCodec() {
-        return TestUtil.alwaysPostingsFormat(new Elasticsearch090RWPostingsFormat());
-    }
-    
-}
diff --git a/core/src/test/java/org/elasticsearch/index/engine/EngineSearcherTotalHitsMatcher.java b/core/src/test/java/org/elasticsearch/index/engine/EngineSearcherTotalHitsMatcher.java
index 583b0f5..362ee9c 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/EngineSearcherTotalHitsMatcher.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/EngineSearcherTotalHitsMatcher.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.engine;
 
 import org.apache.lucene.search.Query;
-import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.hamcrest.Description;
 import org.hamcrest.Matcher;
@@ -46,7 +45,7 @@ public final class EngineSearcherTotalHitsMatcher extends TypeSafeMatcher<Engine
     @Override
     public boolean matchesSafely(Engine.Searcher searcher) {
         try {
-            this.count = (int) Lucene.count(searcher.searcher(), query);
+            this.count = (int) searcher.searcher().count(query);
             return count == totalHits;
         } catch (IOException e) {
             return false;
diff --git a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
index 3dcb548..5f6e1db 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.engine;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.log4j.AppenderSkeleton;
 import org.apache.log4j.Level;
 import org.apache.log4j.LogManager;
@@ -29,7 +28,16 @@ import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.*;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;
+import org.apache.lucene.index.LiveIndexWriterConfig;
+import org.apache.lucene.index.LogByteSizeMergePolicy;
+import org.apache.lucene.index.MergePolicy;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.SnapshotDeletionPolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TieredMergePolicy;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
@@ -61,9 +69,17 @@ import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.codec.CodecService;
 import org.elasticsearch.index.engine.Engine.Searcher;
 import org.elasticsearch.index.indexing.ShardIndexingService;
-import org.elasticsearch.index.mapper.*;
+import org.elasticsearch.index.mapper.ContentPath;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.DocumentMapperForType;
+import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.Mapper.BuilderContext;
+import org.elasticsearch.index.mapper.MapperBuilders;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.mapper.Mapping;
+import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext.Document;
+import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.mapper.object.RootObjectMapper;
@@ -89,7 +105,13 @@ import java.nio.charset.Charset;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
 import java.util.concurrent.BrokenBarrierException;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.CyclicBarrier;
@@ -97,10 +119,16 @@ import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
 
+import static java.util.Collections.emptyMap;
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 import static org.elasticsearch.index.engine.Engine.Operation.Origin.PRIMARY;
 import static org.elasticsearch.index.engine.Engine.Operation.Origin.REPLICA;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.hasKey;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
 
 public class InternalEngineTests extends ESTestCase {
 
@@ -1005,6 +1033,7 @@ public class InternalEngineTests extends ESTestCase {
                 final CountDownLatch indexed = new CountDownLatch(1);
 
                 Thread thread = new Thread() {
+                    @Override
                     public void run() {
                         try {
                             try {
@@ -1673,7 +1702,7 @@ public class InternalEngineTests extends ESTestCase {
     private Mapping dynamicUpdate() {
         BuilderContext context = new BuilderContext(Settings.EMPTY, new ContentPath());
         final RootObjectMapper root = MapperBuilders.rootObject("some_type").build(context);
-        return new Mapping(Version.CURRENT, root, new MetadataFieldMapper[0], new Mapping.SourceTransform[0], ImmutableMap.<String, Object>of());
+        return new Mapping(Version.CURRENT, root, new MetadataFieldMapper[0], new Mapping.SourceTransform[0], emptyMap());
     }
 
     public void testUpgradeOldIndex() throws IOException {
@@ -1686,10 +1715,6 @@ public class InternalEngineTests extends ESTestCase {
         Collections.shuffle(indexes, random());
         for (Path indexFile : indexes.subList(0, scaledRandomIntBetween(1, indexes.size() / 2))) {
             final String indexName = indexFile.getFileName().toString().replace(".zip", "").toLowerCase(Locale.ROOT);
-            Version version = Version.fromString(indexName.replace("index-", ""));
-            if (version.onOrAfter(Version.V_2_0_0_beta1)) {
-                continue;
-            }
             Path unzipDir = createTempDir();
             Path unzipDataDir = unzipDir.resolve("data");
             // decompress the index
@@ -1709,11 +1734,9 @@ public class InternalEngineTests extends ESTestCase {
             assertTrue("[" + indexFile + "] missing index dir: " + src.toString(), Files.exists(src));
             assertTrue("[" + indexFile + "] missing translog dir: " + translog.toString(), Files.exists(translog));
             Path[] tlogFiles = filterExtraFSFiles(FileSystemUtils.files(translog));
-            assertEquals(Arrays.toString(tlogFiles), tlogFiles.length, 1);
+            assertEquals(Arrays.toString(tlogFiles), tlogFiles.length, 2); // ckp & tlog
+            Path tlogFile = tlogFiles[0].getFileName().toString().endsWith("tlog") ? tlogFiles[0] : tlogFiles[1];
             final long size = Files.size(tlogFiles[0]);
-
-            final long generation = TranslogTests.parseLegacyTranslogFile(tlogFiles[0]);
-            assertTrue(generation >= 1);
             logger.debug("upgrading index {} file: {} size: {}", indexName, tlogFiles[0].getFileName(), size);
             Directory directory = newFSDirectory(src.resolve("0").resolve("index"));
             Store store = createStore(directory);
@@ -1870,7 +1893,7 @@ public class InternalEngineTests extends ESTestCase {
         public final AtomicInteger recoveredOps = new AtomicInteger(0);
 
         public TranslogHandler(String indexName, ESLogger logger) {
-            super(new ShardId("test", 0), null, null, null, null, logger);
+            super(new ShardId("test", 0), null, logger);
             Settings settings = Settings.settingsBuilder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
             RootObjectMapper.Builder rootBuilder = new RootObjectMapper.Builder("test");
             Index index = new Index(indexName);
@@ -1892,13 +1915,6 @@ public class InternalEngineTests extends ESTestCase {
         protected void operationProcessed() {
             recoveredOps.incrementAndGet();
         }
-
-        @Override
-        public void performRecoveryOperation(Engine engine, Translog.Operation operation, boolean allowMappingUpdates) {
-            if (operation.opType() != Translog.Operation.Type.DELETE_BY_QUERY) { // we don't support del by query in this test
-                super.performRecoveryOperation(engine, operation, allowMappingUpdates);
-            }
-        }
     }
 
     public void testRecoverFromForeignTranslog() throws IOException {
diff --git a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
index 6c3054f..117ef2f 100644
--- a/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTestCase.java
@@ -601,10 +601,10 @@ public abstract class AbstractStringFieldDataTestCase extends AbstractFieldDataI
         assertThat(ifd.loadGlobal(topLevelReader), sameInstance(globalOrdinals));
         // 3 b/c 1 segment level caches and 1 top level cache
         // in case of doc values, we don't cache atomic FD, so only the top-level cache is there
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(hasDocValues() ? 1L : 4L));
+        assertThat(indicesFieldDataCache.getCache().weight(), equalTo(hasDocValues() ? 1L : 4L));
 
         IndexOrdinalsFieldData cachedInstance = null;
-        for (Accountable ramUsage : indicesFieldDataCache.getCache().asMap().values()) {
+        for (Accountable ramUsage : indicesFieldDataCache.getCache().values()) {
             if (ramUsage instanceof IndexOrdinalsFieldData) {
                 cachedInstance = (IndexOrdinalsFieldData) ramUsage;
                 break;
@@ -613,12 +613,12 @@ public abstract class AbstractStringFieldDataTestCase extends AbstractFieldDataI
         assertThat(cachedInstance, sameInstance(globalOrdinals));
         topLevelReader.close();
         // Now only 3 segment level entries, only the toplevel reader has been closed, but the segment readers are still used by IW
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(hasDocValues() ? 0L : 3L));
+        assertThat(indicesFieldDataCache.getCache().weight(), equalTo(hasDocValues() ? 0L : 3L));
 
         refreshReader();
         assertThat(ifd.loadGlobal(topLevelReader), not(sameInstance(globalOrdinals)));
 
         ifdService.clear();
-        assertThat(indicesFieldDataCache.getCache().size(), equalTo(0l));
+        assertThat(indicesFieldDataCache.getCache().weight(), equalTo(0l));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
index ce71635..f01df63 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/DynamicMappingTests.java
@@ -18,12 +18,15 @@
  */
 package org.elasticsearch.index.mapper;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.mapping.get.GetMappingsResponse;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.*;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
+import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.core.IntegerFieldMapper;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
@@ -31,6 +34,7 @@ import org.elasticsearch.test.ESSingleNodeTestCase;
 
 import java.io.IOException;
 
+import static java.util.Collections.emptyMap;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.nullValue;
@@ -186,7 +190,7 @@ public class DynamicMappingTests extends ESSingleNodeTestCase {
 
     private String serialize(ToXContent mapper) throws Exception {
         XContentBuilder builder = XContentFactory.jsonBuilder().startObject();
-        mapper.toXContent(builder, new ToXContent.MapParams(ImmutableMap.<String, String>of()));
+        mapper.toXContent(builder, new ToXContent.MapParams(emptyMap()));
         return builder.endObject().string();
     }
 
@@ -321,7 +325,7 @@ public class DynamicMappingTests extends ESSingleNodeTestCase {
         String mapping = XContentFactory.jsonBuilder().startObject().startObject("type") .startObject("properties")
                 .startObject("foo").field("type", "object").endObject()
                 .endObject().endObject().endObject().string();
-        
+
         DocumentMapper mapper = parser.parse(mapping);
         assertEquals(mapping, serialize(mapper));
 
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java
index b5a002d..0bc56b0 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/binary/BinaryMappingTests.java
@@ -93,35 +93,4 @@ public class BinaryMappingTests extends ESSingleNodeTestCase {
             assertEquals(new BytesArray(value), originalValue);
         }
     }
-
-    public void testCompressedBackCompat() throws Exception {
-        String mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
-                .startObject("properties")
-                .startObject("field")
-                .field("type", "binary")
-                .field("store", "yes")
-                .endObject()
-                .endObject()
-                .endObject().endObject().string();
-
-        Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_5_0).build();
-        DocumentMapper mapper = createIndex("test", settings).mapperService().documentMapperParser().parse(mapping);
-
-        final byte[] original = new byte[100];
-        original[56] = 1;
-        BytesStreamOutput out = new BytesStreamOutput();
-        try (StreamOutput compressed = CompressorFactory.defaultCompressor().streamOutput(out)) {
-            new BytesArray(original).writeTo(compressed);
-        }
-        final byte[] binaryValue = out.bytes().toBytes();
-        assertTrue(CompressorFactory.isCompressed(new BytesArray(binaryValue)));
-        
-        ParsedDocument doc = mapper.parse("test", "type", "id", XContentFactory.jsonBuilder().startObject().field("field", binaryValue).endObject().bytes());
-        BytesRef indexedValue = doc.rootDoc().getBinaryValue("field");
-        assertEquals(new BytesRef(binaryValue), indexedValue);
-        FieldMapper fieldMapper = mapper.mappers().smartNameFieldMapper("field");
-        Object originalValue = fieldMapper.fieldType().valueForSearch(indexedValue);
-        assertEquals(new BytesArray(original), originalValue);
-    }
-
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java b/core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java
index f241d55..7ab7886 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/core/BinaryFieldTypeTests.java
@@ -28,15 +28,4 @@ public class BinaryFieldTypeTests extends FieldTypeTestCase {
     protected MappedFieldType createDefaultFieldType() {
         return new BinaryFieldMapper.BinaryFieldType();
     }
-
-    @Before
-    public void setupProperties() {
-        addModifier(new Modifier("try_uncompressing", false, true) {
-            @Override
-            public void modify(MappedFieldType ft) {
-                BinaryFieldMapper.BinaryFieldType bft = (BinaryFieldMapper.BinaryFieldType)ft;
-                bft.setTryUncompressing(!bft.tryUncompressing());
-            }
-        });
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java b/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
index e6b08fb..a54b63d 100644
--- a/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
+++ b/core/src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.index.mapper.string;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.index.DocValuesType;
 import org.apache.lucene.index.IndexOptions;
 import org.apache.lucene.index.IndexableField;
@@ -44,6 +43,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.ParseContext.Document;
 import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.core.StringFieldMapper;
+import org.elasticsearch.index.mapper.core.StringFieldMapper.Builder;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.elasticsearch.test.VersionUtils;
 import org.junit.Before;
@@ -52,8 +52,11 @@ import org.junit.Test;
 import java.util.Arrays;
 import java.util.Map;
 
-import static org.elasticsearch.index.mapper.core.StringFieldMapper.Builder;
-import static org.hamcrest.Matchers.*;
+import static java.util.Collections.emptyMap;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
 
 /**
  */
@@ -121,7 +124,7 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
     }
 
     private void assertParseIdemPotent(IndexableFieldType expected, DocumentMapper mapper) throws Exception {
-        String mapping = mapper.toXContent(XContentFactory.jsonBuilder().startObject(), new ToXContent.MapParams(ImmutableMap.<String, String>of())).endObject().string();
+        String mapping = mapper.toXContent(XContentFactory.jsonBuilder().startObject(), new ToXContent.MapParams(emptyMap())).endObject().string();
         mapper = parser.parse(mapping);
         ParsedDocument doc = mapper.parse("test", "type", "1", XContentFactory.jsonBuilder()
                 .startObject()
@@ -214,7 +217,7 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
         assertThat(fieldType.omitNorms(), equalTo(false));
         assertParseIdemPotent(fieldType, defaultMapper);
     }
-    
+
     @Test
     public void testSearchQuoteAnalyzerSerialization() throws Exception {
         // Cases where search_quote_analyzer should not be added to the mapping.
@@ -250,7 +253,7 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
             Map<String, Object> serializedMap = getSerializedMap(fieldName, mapper);
             assertFalse(fieldName, serializedMap.containsKey("search_quote_analyzer"));
         }
-        
+
         // Cases where search_quote_analyzer should be present.
         mapping = XContentFactory.jsonBuilder().startObject().startObject("type")
                 .startObject("properties")
@@ -268,20 +271,20 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
                 .endObject()
                 .endObject()
                 .endObject().endObject().string();
-        
+
         mapper = parser.parse(mapping);
         for (String fieldName : Arrays.asList("field1", "field2")) {
             Map<String, Object> serializedMap = getSerializedMap(fieldName, mapper);
             assertEquals(serializedMap.get("search_quote_analyzer"), "simple");
         }
     }
-    
+
     private Map<String, Object> getSerializedMap(String fieldName, DocumentMapper mapper) throws Exception {
         FieldMapper fieldMapper = mapper.mappers().smartNameFieldMapper(fieldName);
         XContentBuilder builder = JsonXContent.contentBuilder().startObject();
         fieldMapper.toXContent(builder, ToXContent.EMPTY_PARAMS).endObject();
         builder.close();
-        
+
         Map<String, Object> fieldMap;
         try (XContentParser parser = JsonXContent.jsonXContent.createParser(builder.bytes())) {
             fieldMap = parser.map();
@@ -464,7 +467,7 @@ public class SimpleStringMappingTests extends ESSingleNodeTestCase {
         assertEquals(DocValuesType.NONE, docValuesType(doc, "str3"));
         assertEquals(DocValuesType.NONE, docValuesType(doc, "str4"));
         assertEquals(DocValuesType.SORTED_SET, docValuesType(doc, "str5"));
-        
+
     }
 
     // TODO: this function shouldn't be necessary.  parsing should just add a single field that is indexed and dv
diff --git a/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java
index 022c2e1..0db757f 100644
--- a/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/GeohashCellQueryBuilderTests.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import com.spatial4j.core.shape.Point;
-
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.TermsQuery;
 import org.apache.lucene.search.Query;
@@ -29,7 +27,6 @@ import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.query.GeohashCellQuery.Builder;
-import org.elasticsearch.test.geo.RandomShapeGenerator;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -108,15 +105,4 @@ public class GeohashCellQueryBuilderTests extends AbstractQueryTestCase<Builder>
         builder.precision(-1);
     }
 
-    @Test
-    public void testLocationParsing() throws IOException {
-        Point point = RandomShapeGenerator.xRandomPoint(getRandom());
-        Builder pointTestBuilder = new GeohashCellQuery.Builder("pin", new GeoPoint(point.getY(), point.getX()));
-        String pointTest1 = "{\"geohash_cell\": {\"pin\": {\"lat\": " + point.getY() + ",\"lon\": " + point.getX() + "}}}";
-        assertParsedQuery(pointTest1, pointTestBuilder);
-        String pointTest2 = "{\"geohash_cell\": {\"pin\": \"" + point.getY() + "," + point.getX() + "\"}}";
-        assertParsedQuery(pointTest2, pointTestBuilder);
-        String pointTest3 = "{\"geohash_cell\": {\"pin\": [" + point.getX() + "," + point.getY() + "]}}";
-        assertParsedQuery(pointTest3, pointTestBuilder);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java
index 665b4b0..177caf0 100644
--- a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java
@@ -137,10 +137,4 @@ public class IdsQueryBuilderTests extends AbstractQueryTestCase<IdsQueryBuilder>
             //all good
         }
     }
-
-    @Test(expected= ParsingException.class) // see #7686.
-    public void testIdsQueryWithInvalidValues() throws Exception {
-        String query = "{ \"ids\": { \"values\": [[1]] } }";
-        parseQuery(query);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java
index 2ca9441..5ae54d4 100644
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTests.java
@@ -20,11 +20,7 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchNoDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.*;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -32,18 +28,9 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
+import java.util.*;
+
+import static org.hamcrest.Matchers.*;
 
 public class SimpleQueryStringBuilderTests extends AbstractQueryTestCase<SimpleQueryStringBuilder> {
 
@@ -340,14 +327,4 @@ public class SimpleQueryStringBuilderTests extends AbstractQueryTestCase<SimpleQ
         assertThat(query, instanceOf(TermQuery.class));
         assertThat(query.getBoost(), equalTo(10f));
     }
-
-    public void testNegativeFlags() throws IOException {
-        String query = "{\"simple_query_string\": {\"query\": \"foo bar\", \"flags\": -1}}";
-        SimpleQueryStringBuilder builder = new SimpleQueryStringBuilder("foo bar");
-        builder.flags(SimpleQueryStringFlag.ALL);
-        assertParsedQuery(query, builder);
-        SimpleQueryStringBuilder otherBuilder = new SimpleQueryStringBuilder("foo bar");
-        otherBuilder.flags(-1);
-        assertThat(builder, equalTo(otherBuilder));
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
index 62a04c4..3d89633 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTests.java
@@ -22,7 +22,6 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
 import org.junit.BeforeClass;
@@ -85,35 +84,7 @@ public class TemplateQueryBuilderTests extends AbstractQueryTestCase<TemplateQue
         builder.doXContent(content, null);
         content.endObject();
         content.close();
-        assertEquals("{\"template\":{\"inline\":\"I am a $template string\",\"lang\":\"mustache\",\"params\":{\"template\":\"filled\"}}}",
-                content.string());
-    }
-
-    @Test
-    public void testRawEscapedTemplate() throws IOException {
-        String expectedTemplateString = "{\"match_{{template}}\": {}}\"";
-        String query = "{\"template\": {\"query\": \"{\\\"match_{{template}}\\\": {}}\\\"\",\"params\" : {\"template\" : \"all\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        QueryBuilder<?> expectedBuilder = new TemplateQueryBuilder(new Template(expectedTemplateString, ScriptType.INLINE, null, null,
-                params));
-        assertParsedQuery(query, expectedBuilder);
-    }
-
-    @Test
-    public void testRawTemplate() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject("match_{{template}}");
-        builder.endObject();
-        builder.endObject();
-        String expectedTemplateString = "{\"match_{{template}}\":{}}";
-        String query = "{\"template\": {\"query\": {\"match_{{template}}\": {}},\"params\" : {\"template\" : \"all\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        QueryBuilder<?> expectedBuilder = new TemplateQueryBuilder(new Template(expectedTemplateString, ScriptType.INLINE, null,
-                XContentType.JSON, params));
-        assertParsedQuery(query, expectedBuilder);
+        assertEquals("{\"template\":{\"inline\":\"I am a $template string\",\"params\":{\"template\":\"filled\"}}}", content.string());
     }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
index 71dd323..0c9fc74 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryIT.java
@@ -27,15 +27,13 @@ import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchRequest;
 import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.Template;
 import org.elasticsearch.script.mustache.MustacheScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Before;
 import org.junit.Test;
@@ -101,14 +99,24 @@ public class TemplateQueryIT extends ESIntegTestCase {
                 "        }\n" +
                 "    }\n" +
                 "}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        SearchResponse sr = client().prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().size(0).query(
-                                QueryBuilders.templateQuery(new Template("{ \"query\": { \"match_{{template}}\": {} } }",
-                                        ScriptType.INLINE, null, null, params)))).execute()
-                .actionGet();
+        SearchResponse sr = client().prepareSearch().setSource(new BytesArray(request))
+                .execute().actionGet();
+        assertNoFailures(sr);
+        assertThat(sr.getHits().hits().length, equalTo(0));
+        request = "{\n" +
+                "    \"query\": {\n" +
+                "        \"template\": {\n" +
+                "            \"query\": {\"match_{{template}}\": {}},\n" +
+                "            \"params\" : {\n" +
+                "                \"template\" : \"all\"\n" +
+                "            }\n" +
+                "        }\n" +
+                "    },\n" +
+                "    \"size\":0" +
+                "}";
+
+        sr = client().prepareSearch().setSource(new BytesArray(request))
+                .execute().actionGet();
         assertNoFailures(sr);
         assertThat(sr.getHits().hits().length, equalTo(0));
     }
@@ -137,11 +145,25 @@ public class TemplateQueryIT extends ESIntegTestCase {
     }
 
     @Test
+    public void testRawEscapedTemplate() throws IOException {
+        String query = "{\"template\": {\"query\": \"{\\\"match_{{template}}\\\": {}}\\\"\",\"params\" : {\"template\" : \"all\"}}}";
+
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
+        assertHitCount(sr, 2);
+    }
+
+    @Test
+    public void testRawTemplate() throws IOException {
+        String query = "{\"template\": {\"query\": {\"match_{{template}}\": {}},\"params\" : {\"template\" : \"all\"}}}";
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
+        assertHitCount(sr, 2);
+    }
+
+    @Test
     public void testRawFSTemplate() throws IOException {
-        Map<String, Object> params = new HashMap<>();
-        params.put("template", "all");
-        TemplateQueryBuilder builder = new TemplateQueryBuilder(new Template("storedTemplate", ScriptType.FILE, null, null, params));
-        SearchResponse sr = client().prepareSearch().setQuery(builder).get();
+        String query = "{\"template\": {\"file\": \"storedTemplate\",\"params\" : {\"template\" : \"all\"}}}";
+
+        SearchResponse sr = client().prepareSearch().setQuery(query).get();
         assertHitCount(sr, 2);
     }
 
@@ -151,18 +173,13 @@ public class TemplateQueryIT extends ESIntegTestCase {
         searchRequest.indices("_all");
 
         String query = "{ \"template\" : { \"query\": {\"match_{{template}}\": {} } }, \"params\" : { \"template\":\"all\" } }";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(query);
+        searchRequest.templateSource(bytesRef);
 
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertHitCount(searchResponse, 2);
     }
 
-    private Template parseTemplate(String template) throws IOException {
-        try (XContentParser parser = XContentFactory.xContent(template).createParser(template)) {
-            return TemplateQueryParser.parse(parser, ParseFieldMatcher.EMPTY, "params", "template");
-        }
-    }
-
     @Test
     // Releates to #6318
     public void testSearchRequestFail() throws Exception {
@@ -170,14 +187,16 @@ public class TemplateQueryIT extends ESIntegTestCase {
         searchRequest.indices("_all");
         try {
             String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  } }";
-            searchRequest.template(parseTemplate(query));
+            BytesReference bytesRef = new BytesArray(query);
+            searchRequest.templateSource(bytesRef);
             client().search(searchRequest).get();
             fail("expected exception");
         } catch (Exception ex) {
             // expected - no params
         }
         String query = "{ \"template\" : { \"query\": {\"match_all\": {}}, \"size\" : \"{{my_size}}\"  }, \"params\" : { \"my_size\": 1 } }";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(query);
+        searchRequest.templateSource(bytesRef);
 
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
@@ -215,9 +234,10 @@ public class TemplateQueryIT extends ESIntegTestCase {
     public void testSearchTemplateQueryFromFile() throws Exception {
         SearchRequest searchRequest = new SearchRequest();
         searchRequest.indices("_all");
-        String query = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
+        String templateString = "{" + "  \"file\": \"full-query-template\"," + "  \"params\":{" + "    \"mySize\": 2,"
                 + "    \"myField\": \"text\"," + "    \"myValue\": \"value1\"" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -229,9 +249,10 @@ public class TemplateQueryIT extends ESIntegTestCase {
     public void testTemplateQueryAsEscapedString() throws Exception {
         SearchRequest searchRequest = new SearchRequest();
         searchRequest.indices("_all");
-        String query = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
+        String templateString = "{" + "  \"template\" : \"{ \\\"size\\\": \\\"{{size}}\\\", \\\"query\\\":{\\\"match_all\\\":{}}}\","
                 + "  \"params\":{" + "    \"size\": 1" + "  }" + "}";
-        searchRequest.template(parseTemplate(query));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -247,7 +268,8 @@ public class TemplateQueryIT extends ESIntegTestCase {
         String templateString = "{"
                 + "  \"template\" : \"{ {{#use_size}} \\\"size\\\": \\\"{{size}}\\\", {{/use_size}} \\\"query\\\":{\\\"match_all\\\":{}}}\","
                 + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -263,7 +285,8 @@ public class TemplateQueryIT extends ESIntegTestCase {
         String templateString = "{"
                 + "  \"inline\" : \"{ \\\"query\\\":{\\\"match_all\\\":{}} {{#use_size}}, \\\"size\\\": \\\"{{size}}\\\" {{/use_size}} }\","
                 + "  \"params\":{" + "    \"size\": 1," + "    \"use_size\": true" + "  }" + "}";
-        searchRequest.template(parseTemplate(templateString));
+        BytesReference bytesRef = new BytesArray(templateString);
+        searchRequest.templateSource(bytesRef);
         SearchResponse searchResponse = client().search(searchRequest).get();
         assertThat(searchResponse.getHits().hits().length, equalTo(1));
     }
@@ -428,15 +451,12 @@ public class TemplateQueryIT extends ESIntegTestCase {
                 .execute().actionGet();
         assertHitCount(sr, 1);
 
-        // "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
-        Map<String, Object> params = new HashMap<>();
-        params.put("fieldParam", "foo");
-        TemplateQueryBuilder templateQuery = new TemplateQueryBuilder(new Template("3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
+        String query = "{\"template\": {\"id\": \"3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
+        sr = client().prepareSearch().setQuery(query).get();
         assertHitCount(sr, 4);
 
-        templateQuery = new TemplateQueryBuilder(new Template("/mustache/3", ScriptType.INDEXED, null, null, params));
-        sr = client().prepareSearch().setQuery(templateQuery).get();
+        query = "{\"template\": {\"id\": \"/mustache/3\",\"params\" : {\"fieldParam\" : \"foo\"}}}";
+        sr = client().prepareSearch().setQuery(query).get();
         assertHitCount(sr, 4);
     }
 
@@ -451,7 +471,7 @@ public class TemplateQueryIT extends ESIntegTestCase {
 
         int iterations = randomIntBetween(2, 11);
         for (int i = 1; i < iterations; i++) {
-            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01",
+            PutIndexedScriptResponse scriptResponse = client().preparePutIndexedScript(MustacheScriptEngineService.NAME, "git01", 
                     "{\"query\": {\"match\": {\"searchtext\": {\"query\": \"{{P_Keyword1}}\",\"type\": \"ooophrase_prefix\"}}}}").get();
             assertEquals(i * 2 - 1, scriptResponse.getVersion());
 
@@ -487,7 +507,7 @@ public class TemplateQueryIT extends ESIntegTestCase {
         }
     }
 
-
+    
     @Test
     public void testIndexedTemplateWithArray() throws Exception {
       createIndex(ScriptService.SCRIPT_INDEX);
diff --git a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
index c72470c..432c833 100644
--- a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
+++ b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
@@ -27,10 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.QueryParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.*;
 import org.elasticsearch.indices.IndicesModule;
 import org.elasticsearch.plugins.Plugin;
 
@@ -67,22 +64,22 @@ public class DummyQueryParserPlugin extends Plugin {
 
         @Override
         protected DummyQueryBuilder doReadFrom(StreamInput in) throws IOException {
-            return new DummyQueryBuilder();
+            return null;
         }
 
         @Override
         protected void doWriteTo(StreamOutput out) throws IOException {
-            // Do Nothing
+
         }
 
         @Override
-        protected int doHashCode() {
-            return 0;
+        protected boolean doEquals(DummyQueryBuilder other) {
+            return false;
         }
 
         @Override
-        protected boolean doEquals(DummyQueryBuilder other) {
-            return true;
+        protected int doHashCode() {
+            return 0;
         }
 
         @Override
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index 89d2701..b891219 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -399,35 +399,6 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         assertEquals(durabilty, shard.getTranslogDurability());
     }
 
-    public void testDeleteByQueryBWC() {
-        Version version = VersionUtils.randomVersion(random());
-        assertAcked(client().admin().indices().prepareCreate("test")
-                .setSettings(SETTING_NUMBER_OF_SHARDS, 1, SETTING_NUMBER_OF_REPLICAS, 0, IndexMetaData.SETTING_VERSION_CREATED, version.id));
-        ensureGreen("test");
-        client().prepareIndex("test", "person").setSource("{ \"user\" : \"kimchy\" }").get();
-
-        IndicesService indicesService = getInstanceFromNode(IndicesService.class);
-        IndexService test = indicesService.indexService("test");
-        IndexShard shard = test.getShardOrNull(0);
-        int numDocs = 1;
-        shard.state = IndexShardState.RECOVERING;
-        try {
-            shard.recoveryState().getTranslog().totalOperations(1);
-            shard.getEngine().config().getTranslogRecoveryPerformer().performRecoveryOperation(shard.getEngine(), new Translog.DeleteByQuery(new Engine.DeleteByQuery(null, new BytesArray("{\"term\" : { \"user\" : \"kimchy\" }}"), null, null, null, Engine.Operation.Origin.RECOVERY, 0, "person")), false);
-            assertTrue(version.onOrBefore(Version.V_1_0_0_Beta2));
-            numDocs = 0;
-        } catch (ParsingException ex) {
-            assertTrue(version.after(Version.V_1_0_0_Beta2));
-        } finally {
-            shard.state = IndexShardState.STARTED;
-        }
-        shard.getEngine().refresh("foo");
-
-        try (Engine.Searcher searcher = shard.getEngine().acquireSearcher("foo")) {
-            assertEquals(numDocs, searcher.reader().numDocs());
-        }
-    }
-
     public void testMinimumCompatVersion() {
         Version versionCreated = VersionUtils.randomVersion(random());
         assertAcked(client().admin().indices().prepareCreate("test")
@@ -955,7 +926,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
             }
         };
 
-        IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndexAliasesService(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper, indexServices.getIndexingMemoryController());
+        IndexServicesProvider newProvider = new IndexServicesProvider(indexServices.getIndicesLifecycle(), indexServices.getThreadPool(), indexServices.getMapperService(), indexServices.getQueryParserService(), indexServices.getIndexCache(), indexServices.getIndicesQueryCache(), indexServices.getCodecService(), indexServices.getTermVectorsService(), indexServices.getIndexFieldDataService(), indexServices.getWarmer(), indexServices.getSimilarityService(), indexServices.getFactory(), indexServices.getBigArrays(), wrapper, indexServices.getIndexingMemoryController());
         IndexShard newShard = new IndexShard(shard.shardId(), shard.indexSettings, shard.shardPath(), shard.store(), newProvider);
 
         ShardRoutingHelper.reinit(routing);
diff --git a/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java b/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java
deleted file mode 100644
index e53358c..0000000
--- a/core/src/test/java/org/elasticsearch/index/store/IndexStoreBWCTests.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.store;
-
-import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.Constants;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.ShardPath;
-import org.elasticsearch.test.ESSingleNodeTestCase;
-import org.elasticsearch.test.ESTestCase;
-
-import java.io.IOException;
-import java.nio.file.Path;
-import java.util.Arrays;
-import java.util.Locale;
-
-/**
- */
-public class IndexStoreBWCTests extends ESSingleNodeTestCase {
-
-
-    public void testOldCoreTypesFail() {
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "nio_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "mmap_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-        try {
-            createIndex("test", Settings.builder().put(IndexStoreModule.STORE_TYPE, "simple_fs").build());
-            fail();
-        } catch (Exception ex) {
-        }
-    }
-
-    public void testUpgradeCoreTypes() throws IOException {
-        String type = RandomPicks.randomFrom(random(), Arrays.asList("nio", "mmap", "simple"));
-        createIndex("test", Settings.builder()
-                .put(IndexStoreModule.STORE_TYPE, type+"fs")
-                .put(IndexMetaData.SETTING_VERSION_CREATED, Version.V_1_7_0)
-                .build());
-
-        client().admin().indices().prepareClose("test").get();
-        client().admin().indices().prepareUpdateSettings("test").setSettings(Settings.builder()
-                .put(IndexStoreModule.STORE_TYPE, type + "_fs").build()).get();
-        GetSettingsResponse getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
-        String actualType = getSettingsResponse.getSetting("test", IndexStoreModule.STORE_TYPE);
-        assertEquals(type + "_fs", actualType);
-
-        // now reopen and upgrade
-        client().admin().indices().prepareOpen("test").get();
-
-        getSettingsResponse = client().admin().indices().prepareGetSettings("test").get();
-        actualType = getSettingsResponse.getSetting("test", IndexStoreModule.STORE_TYPE);
-        assertEquals(type+"fs", actualType);
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/store/StoreTests.java b/core/src/test/java/org/elasticsearch/index/store/StoreTests.java
index 11d01c9..123e4e0 100644
--- a/core/src/test/java/org/elasticsearch/index/store/StoreTests.java
+++ b/core/src/test/java/org/elasticsearch/index/store/StoreTests.java
@@ -24,9 +24,35 @@ import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene53.Lucene53Codec;
-import org.apache.lucene.document.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.store.*;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.IndexNotFoundException;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy;
+import org.apache.lucene.index.NoDeletionPolicy;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.index.SnapshotDeletionPolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.TestUtil;
@@ -55,14 +81,30 @@ import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.file.NoSuchFileException;
 import java.nio.file.Path;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.zip.Adler32;
 
-import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.unmodifiableMap;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.empty;
+import static org.hamcrest.Matchers.endsWith;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.is;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
 
 public class StoreTests extends ESTestCase {
 
@@ -806,9 +848,9 @@ public class StoreTests extends ESTestCase {
         Map<String, StoreFileMetaData> metaDataMap = new HashMap<>();
         metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[]{1})));
         metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef()));
-        Store.MetadataSnapshot first = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
+        Store.MetadataSnapshot first = new Store.MetadataSnapshot(unmodifiableMap(new HashMap<>(metaDataMap)), emptyMap(), 0);
 
-        Store.MetadataSnapshot second = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
+        Store.MetadataSnapshot second = new Store.MetadataSnapshot(unmodifiableMap(new HashMap<>(metaDataMap)), emptyMap(), 0);
         Store.RecoveryDiff recoveryDiff = first.recoveryDiff(second);
         assertEquals(recoveryDiff.toString(), recoveryDiff.different.size(), 2);
     }
@@ -1074,7 +1116,7 @@ public class StoreTests extends ESTestCase {
         Map<String, StoreFileMetaData> metaDataMap = new HashMap<>();
         metaDataMap.put("segments_1", new StoreFileMetaData("segments_1", 50, null, null, new BytesRef(new byte[]{1})));
         metaDataMap.put("_0_1.del", new StoreFileMetaData("_0_1.del", 42, "foobarbaz", null, new BytesRef()));
-        Store.MetadataSnapshot snapshot = new Store.MetadataSnapshot(metaDataMap, Collections.EMPTY_MAP, 0);
+        Store.MetadataSnapshot snapshot = new Store.MetadataSnapshot(unmodifiableMap(metaDataMap), emptyMap(), 0);
 
         final ShardId shardId = new ShardId(new Index("index"), 1);
         DirectoryService directoryService = new LuceneManagedDirectoryService(random());
@@ -1208,7 +1250,7 @@ public class StoreTests extends ESTestCase {
         Map<String, String> commitUserData = new HashMap<>();
         commitUserData.put("userdata_1", "test");
         commitUserData.put("userdata_2", "test");
-        return new Store.MetadataSnapshot(storeFileMetaDataMap, commitUserData, 0);
+        return new Store.MetadataSnapshot(unmodifiableMap(storeFileMetaDataMap), unmodifiableMap(commitUserData), 0);
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
index 5a25f65..0b3e12d 100644
--- a/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
+++ b/core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java
@@ -488,9 +488,6 @@ public class TranslogTests extends ESTestCase {
                                             1 + randomInt(100000),
                                             randomFrom(VersionType.values()));
                                     break;
-                                case DELETE_BY_QUERY:
-                                    // deprecated
-                                    continue;
                                 default:
                                     throw new ElasticsearchException("not supported op type");
                             }
@@ -695,9 +692,6 @@ public class TranslogTests extends ESTestCase {
                             case DELETE:
                                 op = new Translog.Delete(newUid("" + id));
                                 break;
-                            case DELETE_BY_QUERY:
-                                // deprecated
-                                continue;
                             default:
                                 throw new ElasticsearchException("unknown type");
                         }
@@ -1139,147 +1133,4 @@ public class TranslogTests extends ESTestCase {
             assertNull(snapshot.next());
         }
     }
-
-    public void testUpgradeOldTranslogFiles() throws IOException {
-        List<Path> indexes = new ArrayList<>();
-        try (DirectoryStream<Path> stream = Files.newDirectoryStream(getBwcIndicesPath(), "index-*.zip")) {
-            for (Path path : stream) {
-                indexes.add(path);
-            }
-        }
-        TranslogConfig config = this.translog.getConfig();
-        Translog.TranslogGeneration gen = translog.getGeneration();
-        this.translog.close();
-        try {
-            Translog.upgradeLegacyTranslog(logger, translog.getConfig());
-            fail("no generation set");
-        } catch (IllegalArgumentException ex) {
-
-        }
-        translog.getConfig().setTranslogGeneration(gen);
-        try {
-            Translog.upgradeLegacyTranslog(logger, translog.getConfig());
-            fail("already upgraded generation set");
-        } catch (IllegalArgumentException ex) {
-
-        }
-
-        for (Path indexFile : indexes) {
-            final String indexName = indexFile.getFileName().toString().replace(".zip", "").toLowerCase(Locale.ROOT);
-            Version version = Version.fromString(indexName.replace("index-", ""));
-            if (version.onOrAfter(Version.V_2_0_0_beta1)) {
-                continue;
-            }
-            Path unzipDir = createTempDir();
-            Path unzipDataDir = unzipDir.resolve("data");
-            // decompress the index
-            try (InputStream stream = Files.newInputStream(indexFile)) {
-                TestUtil.unzip(stream, unzipDir);
-            }
-            // check it is unique
-            assertTrue(Files.exists(unzipDataDir));
-            Path[] list = FileSystemUtils.files(unzipDataDir);
-            if (list.length != 1) {
-                throw new IllegalStateException("Backwards index must contain exactly one cluster but was " + list.length);
-            }
-            // the bwc scripts packs the indices under this path
-            Path src = list[0].resolve("nodes/0/indices/" + indexName);
-            Path translog = list[0].resolve("nodes/0/indices/" + indexName).resolve("0").resolve("translog");
-
-            assertTrue("[" + indexFile + "] missing index dir: " + src.toString(), Files.exists(src));
-            assertTrue("[" + indexFile + "] missing translog dir: " + translog.toString(), Files.exists(translog));
-            Path[] tlogFiles =  FileSystemUtils.files(translog);
-            assertEquals(tlogFiles.length, 1);
-            final long size = Files.size(tlogFiles[0]);
-
-            final long generation = parseLegacyTranslogFile(tlogFiles[0]);
-            assertTrue(generation >= 1);
-            logger.info("upgrading index {} file: {} size: {}", indexName, tlogFiles[0].getFileName(), size);
-            TranslogConfig upgradeConfig = new TranslogConfig(config.getShardId(), translog, config.getIndexSettings(), config.getDurabilty(), config.getBigArrays(), config.getThreadPool());
-            upgradeConfig.setTranslogGeneration(new Translog.TranslogGeneration(null, generation));
-            Translog.upgradeLegacyTranslog(logger, upgradeConfig);
-            try (Translog upgraded = new Translog(upgradeConfig)) {
-                assertEquals(generation + 1, upgraded.getGeneration().translogFileGeneration);
-                assertEquals(upgraded.getRecoveredReaders().size(), 1);
-                final long headerSize;
-                if (version.before(Version.V_1_4_0_Beta1)) {
-                    assertTrue(upgraded.getRecoveredReaders().get(0).getClass().toString(), upgraded.getRecoveredReaders().get(0).getClass() == LegacyTranslogReader.class);
-                   headerSize = 0;
-                } else {
-                    assertTrue(upgraded.getRecoveredReaders().get(0).getClass().toString(), upgraded.getRecoveredReaders().get(0).getClass() == LegacyTranslogReaderBase.class);
-                    headerSize = CodecUtil.headerLength(TranslogWriter.TRANSLOG_CODEC);
-                }
-                List<Translog.Operation> operations = new ArrayList<>();
-                try (Translog.Snapshot snapshot = upgraded.newSnapshot()) {
-                    Translog.Operation op = null;
-                    while ((op = snapshot.next()) != null) {
-                        operations.add(op);
-                    }
-                }
-                if (size > headerSize) {
-                    assertFalse(operations.toString(), operations.isEmpty());
-                } else {
-                    assertTrue(operations.toString(), operations.isEmpty());
-                }
-            }
-        }
-    }
-
-    /**
-     * this tests a set of files that has some of the operations flushed with a buffered translog such that tlogs are truncated.
-     * 3 of the 6 files are created with ES 1.3 and the rest is created wiht ES 1.4 such that both the checksummed as well as the
-     * super old version of the translog without a header is tested.
-     */
-    public void testOpenAndReadTruncatedLegacyTranslogs() throws IOException {
-        Path zip = getDataPath("/org/elasticsearch/index/translog/legacy_translogs.zip");
-        Path unzipDir = createTempDir();
-        try (InputStream stream = Files.newInputStream(zip)) {
-            TestUtil.unzip(stream, unzipDir);
-        }
-        TranslogConfig config = this.translog.getConfig();
-        int count = 0;
-        try (DirectoryStream<Path> stream = Files.newDirectoryStream(unzipDir)) {
-
-            for (Path legacyTranslog : stream) {
-                logger.debug("upgrading {} ", legacyTranslog.getFileName());
-                Path directory = legacyTranslog.resolveSibling("translog_" + count++);
-                Files.createDirectories(directory);
-                Files.copy(legacyTranslog, directory.resolve(legacyTranslog.getFileName()));
-                TranslogConfig upgradeConfig = new TranslogConfig(config.getShardId(), directory, config.getIndexSettings(), config.getDurabilty(), config.getBigArrays(), config.getThreadPool());
-                try {
-                    Translog.upgradeLegacyTranslog(logger, upgradeConfig);
-                    fail("no generation set");
-                } catch (IllegalArgumentException ex) {
-                    // expected
-                }
-                long generation = parseLegacyTranslogFile(legacyTranslog);
-                upgradeConfig.setTranslogGeneration(new Translog.TranslogGeneration(null, generation));
-                Translog.upgradeLegacyTranslog(logger, upgradeConfig);
-                try (Translog tlog = new Translog(upgradeConfig)) {
-                    List<Translog.Operation> operations = new ArrayList<>();
-                    try (Translog.Snapshot snapshot = tlog.newSnapshot()) {
-                        Translog.Operation op = null;
-                        while ((op = snapshot.next()) != null) {
-                            operations.add(op);
-                        }
-                    }
-                    logger.debug("num ops recovered: {} for file {} ", operations.size(), legacyTranslog.getFileName());
-                    assertFalse(operations.isEmpty());
-                }
-            }
-        }
-    }
-
-    public static long parseLegacyTranslogFile(Path translogFile) {
-        final String fileName = translogFile.getFileName().toString();
-        final Matcher matcher = PARSE_LEGACY_ID_PATTERN.matcher(fileName);
-        if (matcher.matches()) {
-            try {
-                return Long.parseLong(matcher.group(1));
-            } catch (NumberFormatException e) {
-                throw new IllegalStateException("number formatting issue in a file that passed PARSE_STRICT_ID_PATTERN: " + fileName + "]", e);
-            }
-        }
-        throw new IllegalArgumentException("can't parse id from file: " + fileName);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java b/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
index 3e95685..bf3b84e 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerIT.java
@@ -35,6 +35,8 @@ import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.shard.IndexShardState;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.test.ESIntegTestCase;
+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
+import org.elasticsearch.test.ESIntegTestCase.Scope;
 import org.hamcrest.Matchers;
 import org.junit.Test;
 
@@ -55,8 +57,6 @@ import static org.elasticsearch.index.shard.IndexShardState.CREATED;
 import static org.elasticsearch.index.shard.IndexShardState.POST_RECOVERY;
 import static org.elasticsearch.index.shard.IndexShardState.RECOVERING;
 import static org.elasticsearch.index.shard.IndexShardState.STARTED;
-import static org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import static org.elasticsearch.test.ESIntegTestCase.Scope;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.Matchers.greaterThanOrEqualTo;
@@ -105,7 +105,7 @@ public class IndicesLifecycleListenerIT extends ESIntegTestCase {
         } catch (Exception e) {
             assertTrue(e.getMessage().contains("failing on purpose"));
             ClusterStateResponse resp = client().admin().cluster().prepareState().get();
-            assertFalse(resp.getState().routingTable().indicesRouting().keySet().contains("failed"));
+            assertFalse(resp.getState().routingTable().indicesRouting().keys().contains("failed"));
         }
     }
 
@@ -149,7 +149,7 @@ public class IndicesLifecycleListenerIT extends ESIntegTestCase {
         } catch (ElasticsearchException e) {
             assertTrue(e.getMessage().contains("failing on purpose"));
             ClusterStateResponse resp = client().admin().cluster().prepareState().get();
-            assertFalse(resp.getState().routingTable().indicesRouting().keySet().contains("failed"));
+            assertFalse(resp.getState().routingTable().indicesRouting().keys().contains("failed"));
         }
 
 
@@ -198,7 +198,7 @@ public class IndicesLifecycleListenerIT extends ESIntegTestCase {
         assertAcked(client().admin().indices().prepareClose("test"));
 
         assertThat(stateChangeListenerNode1.afterCloseSettings.getAsInt(SETTING_NUMBER_OF_SHARDS, -1), equalTo(6));
-        assertThat(stateChangeListenerNode1.afterCloseSettings.getAsInt(SETTING_NUMBER_OF_REPLICAS, -1), equalTo(0));
+        assertThat(stateChangeListenerNode1.afterCloseSettings.getAsInt(SETTING_NUMBER_OF_REPLICAS, -1), equalTo(1));
 
         assertShardStatesMatch(stateChangeListenerNode1, 6, CLOSED);
         assertShardStatesMatch(stateChangeListenerNode2, 6, CLOSED);
diff --git a/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java b/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
index 348d5bc..9cf2034 100644
--- a/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java
@@ -48,10 +48,10 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.suggest.SuggestRequestBuilder;
 import org.elasticsearch.action.support.IndicesOptions;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.IndexNotFoundException;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.suggest.SuggestBuilders;
 import org.elasticsearch.search.warmer.IndexWarmersMetaData;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -61,9 +61,7 @@ import static org.elasticsearch.action.percolate.PercolateSourceBuilder.docBuild
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.*;
 
 public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
 
@@ -510,7 +508,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
                 .setIndicesOptions(IndicesOptions.lenientExpandOpen())
                 .execute().actionGet();
         assertHitCount(response, 0l);
-
+        
         //you should still be able to run empty searches without things blowing up
         response  = client().prepareSearch()
                 .setIndicesOptions(IndicesOptions.lenientExpandOpen())
@@ -615,7 +613,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareExists("bar").get().isExists(), equalTo(false));
         assertThat(client().admin().indices().prepareExists("barbaz").get().isExists(), equalTo(false));
     }
-
+    
     @Test
     public void testPutWarmer() throws Exception {
         createIndex("foobar");
@@ -624,26 +622,26 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
 
     }
-
+    
     @Test
     public void testPutWarmer_wildcard() throws Exception {
         createIndex("foo", "foobar", "bar", "barbaz");
         ensureYellow();
 
         verify(client().admin().indices().preparePutWarmer("warmer1").setSearchRequest(client().prepareSearch().setIndices("foo*").setQuery(QueryBuilders.matchAllQuery())), false);
-
+        
         assertThat(client().admin().indices().prepareGetWarmers("foo").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer1").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("bar").setWarmers("warmer1").get().getWarmers().size(), equalTo(0));
         assertThat(client().admin().indices().prepareGetWarmers("barbaz").setWarmers("warmer1").get().getWarmers().size(), equalTo(0));
 
         verify(client().admin().indices().preparePutWarmer("warmer2").setSearchRequest(client().prepareSearch().setIndices().setQuery(QueryBuilders.matchAllQuery())), false);
-
+        
         assertThat(client().admin().indices().prepareGetWarmers("foo").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("foobar").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("bar").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
         assertThat(client().admin().indices().prepareGetWarmers("barbaz").setWarmers("warmer2").get().getWarmers().size(), equalTo(1));
-
+        
     }
 
     @Test
@@ -654,7 +652,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("foobar").get().exists(), equalTo(true));
 
     }
-
+    
     @Test
     public void testPutAlias_wildcard() throws Exception {
         createIndex("foo", "foobar", "bar", "barbaz");
@@ -671,14 +669,14 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("foobar").get().exists(), equalTo(true));
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("bar").get().exists(), equalTo(true));
         assertThat(client().admin().indices().prepareAliasesExist("foobar_alias").setIndices("barbaz").get().exists(), equalTo(true));
-
+        
     }
-
+    
     @Test
     public void testDeleteWarmer() throws Exception {
-        SearchSourceBuilder source = new SearchSourceBuilder();
-        source.query(QueryBuilders.matchAllQuery());
-        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry("test1", new String[] { "typ1" }, false, new IndexWarmersMetaData.SearchSource(source));
+        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry(
+                "test1", new String[]{"typ1"}, false, new BytesArray("{\"query\" : { \"match_all\" : {}}}")
+        );
         assertAcked(prepareCreate("foobar").addCustom(new IndexWarmersMetaData(entry)));
         ensureYellow();
 
@@ -692,9 +690,9 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
     public void testDeleteWarmer_wildcard() throws Exception {
         verify(client().admin().indices().prepareDeleteWarmer().setIndices("_all").setNames("test1"), true);
 
-        SearchSourceBuilder source = new SearchSourceBuilder();
-        source.query(QueryBuilders.matchAllQuery());
-        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry("test1", new String[] { "type1" }, false, new IndexWarmersMetaData.SearchSource(source));
+        IndexWarmersMetaData.Entry entry = new IndexWarmersMetaData.Entry(
+                "test1", new String[]{"type1"}, false, new BytesArray("{\"query\" : { \"match_all\" : {}}}")
+        );
         assertAcked(prepareCreate("foo").addCustom(new IndexWarmersMetaData(entry)));
         assertAcked(prepareCreate("foobar").addCustom(new IndexWarmersMetaData(entry)));
         assertAcked(prepareCreate("bar").addCustom(new IndexWarmersMetaData(entry)));
@@ -739,7 +737,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareGetMappings("foobar").get().mappings().get("foobar").get("type3"), notNullValue());
         assertThat(client().admin().indices().prepareGetMappings("bar").get().mappings().get("bar").get("type3"), notNullValue());
         assertThat(client().admin().indices().prepareGetMappings("barbaz").get().mappings().get("barbaz").get("type3"), notNullValue());
-
+        
 
         verify(client().admin().indices().preparePutMapping("c*").setType("type1").setSource("field", "type=string"), true);
 
@@ -885,7 +883,7 @@ public class IndicesOptionsIntegrationIT extends ESIntegTestCase {
     private static void verify(ActionRequestBuilder requestBuilder, boolean fail) {
         verify(requestBuilder, fail, 0);
     }
-
+    
     private static void verify(ActionRequestBuilder requestBuilder, boolean fail, long expectedCount) {
         if (fail) {
             if (requestBuilder instanceof MultiSearchRequestBuilder) {
diff --git a/core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java b/core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java
index 234f3ea..4f0bd60 100644
--- a/core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerIT.java
@@ -188,7 +188,7 @@ public class RandomExceptionCircuitBreakerIT extends ESIntegTestCase {
                 for (String node : internalCluster().getNodeNames()) {
                     final IndicesFieldDataCache fdCache = internalCluster().getInstance(IndicesFieldDataCache.class, node);
                     // Clean up the cache, ensuring that entries' listeners have been called
-                    fdCache.getCache().cleanUp();
+                    fdCache.getCache().refresh();
                 }
                 NodesStatsResponse nodeStats = client().admin().cluster().prepareNodesStats()
                         .clear().setBreaker(true).execute().actionGet();
diff --git a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
index a1b2508..dbdfc2b 100644
--- a/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
+++ b/core/src/test/java/org/elasticsearch/indices/state/RareClusterStateIT.java
@@ -23,7 +23,10 @@ import org.elasticsearch.Version;
 import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;
 import org.elasticsearch.action.index.IndexResponse;
-import org.elasticsearch.cluster.*;
+import org.elasticsearch.cluster.ClusterInfo;
+import org.elasticsearch.cluster.ClusterService;
+import org.elasticsearch.cluster.ClusterState;
+import org.elasticsearch.cluster.ClusterStateUpdateTask;
 import org.elasticsearch.cluster.block.ClusterBlocks;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.MappingMetaData;
@@ -54,12 +57,19 @@ import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.atomic.AtomicReference;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.hasItem;
+import static org.hamcrest.Matchers.hasSize;
+import static org.hamcrest.Matchers.instanceOf;
 
 /**
  */
@@ -88,7 +98,7 @@ public class RareClusterStateIT extends ESIntegTestCase {
         AllocationDeciders allocationDeciders = new AllocationDeciders(Settings.EMPTY, new AllocationDecider[0]);
         RoutingNodes routingNodes = new RoutingNodes(
                 ClusterState.builder(current)
-                        .routingTable(RoutingTable.builder(current.routingTable()).remove("a").addAsRecovery(current.metaData().index("a")))
+                        .routingTable(RoutingTable.builder(current.routingTable()).remove("a").addAsRecovery(current.metaData().index("a")).build())
                         .nodes(DiscoveryNodes.EMPTY_NODES)
                         .build(), false
         );
@@ -127,7 +137,7 @@ public class RareClusterStateIT extends ESIntegTestCase {
 
                 RoutingTable.Builder routingTable = RoutingTable.builder(updatedState.routingTable());
                 routingTable.addAsRecovery(updatedState.metaData().index(index));
-                updatedState = ClusterState.builder(updatedState).routingTable(routingTable).build();
+                updatedState = ClusterState.builder(updatedState).routingTable(routingTable.build()).build();
 
                 RoutingAllocation.Result result = allocationService.reroute(updatedState);
                 return ClusterState.builder(updatedState).routingResult(result).build();
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
index f624700..e98794c 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.plugins;
 
-import com.google.common.hash.Hashing;
 import org.apache.http.impl.client.HttpClients;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.Version;
diff --git a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
index efda8d2..e38c993 100644
--- a/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
+++ b/core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.script;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.script.ScriptService.ScriptType;
@@ -38,6 +36,7 @@ import java.util.Map;
 import java.util.Set;
 
 import static java.util.Collections.singleton;
+import static java.util.Collections.unmodifiableMap;
 import static java.util.Collections.unmodifiableSet;
 import static org.elasticsearch.common.util.set.Sets.newHashSet;
 import static org.hamcrest.CoreMatchers.equalTo;
@@ -244,14 +243,14 @@ public class ScriptModesTests extends ESTestCase {
         return ScriptModes.ENGINE_SETTINGS_PREFIX + "." + lang + "." + scriptType + "." + scriptContext.getKey();
     }
 
-    static ImmutableMap<String, ScriptEngineService> buildScriptEnginesByLangMap(Set<ScriptEngineService> scriptEngines) {
-        ImmutableMap.Builder<String, ScriptEngineService> builder = ImmutableMap.builder();
+    static Map<String, ScriptEngineService> buildScriptEnginesByLangMap(Set<ScriptEngineService> scriptEngines) {
+        Map<String, ScriptEngineService> builder = new HashMap<>();
         for (ScriptEngineService scriptEngine : scriptEngines) {
             for (String type : scriptEngine.types()) {
                 builder.put(type, scriptEngine);
             }
         }
-        return builder.build();
+        return unmodifiableMap(builder);
     }
 
     private static class CustomScriptEngineService implements ScriptEngineService {
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
index e5634fe..631f705 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/AggregationsBinaryIT.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.search.aggregations;
 
-import org.apache.lucene.util.LuceneTestCase.AwaitsFix;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Requests;
@@ -42,8 +41,6 @@ import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.core.IsNull.notNullValue;
 
 @ESIntegTestCase.SuiteScopeTestCase
-@AwaitsFix(bugUrl = "needs fixing after the search request refactor. Do we need agg binary?")
-// NO RELEASE
 public class AggregationsBinaryIT extends ESIntegTestCase {
 
     private static final String STRING_FIELD_NAME = "s_value";
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
index 87307c0..64f80d6 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/ParsingIT.java
@@ -31,151 +31,150 @@ import java.util.regex.Pattern;
 
 public class ParsingIT extends ESIntegTestCase {
 
-    // NORELEASE move these tests to unit tests when aggs refactoring is done
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testTwoTypes() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("in_stock")
-//                    .startObject("filter")
-//                        .startObject("range")
-//                            .startObject("stock")
-//                                .field("gt", 0)
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//                    .startObject("terms")
-//                        .field("field", "stock")
-//                    .endObject()
-//                .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testTwoAggs() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        .startObject("tag_count")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//                    .startObject("aggs") // 2nd "aggs": illegal
-//                        .startObject("tag_count2")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testInvalidAggregationName() throws Exception {
-//
-//        Matcher matcher = Pattern.compile("[^\\[\\]>]+").matcher("");
-//        String name;
-//        SecureRandom rand = new SecureRandom();
-//        int len = randomIntBetween(1, 5);
-//        char[] word = new char[len];
-//        while(true) {
-//            for (int i = 0; i < word.length; i++) {
-//                word[i] = (char) rand.nextInt(127);
-//            }
-//            name = String.valueOf(word);
-//            if (!matcher.reset(name).matches()) {
-//                break;
-//            }
-//        }
-//
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject(name)
-//                    .startObject("filter")
-//                        .startObject("range")
-//                            .startObject("stock")
-//                                .field("gt", 0)
-//                            .endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testSameAggregationName() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        final String name = RandomStrings.randomAsciiOfLength(getRandom(), 10);
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject(name)
-//                    .startObject("terms")
-//                        .field("field", "a")
-//                    .endObject()
-//                .endObject()
-//                .startObject(name)
-//                    .startObject("terms")
-//                        .field("field", "b")
-//                    .endObject()
-//                .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testMissingName() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        // the aggregation name is missing
-//                        //.startObject("tag_count")
-//                            .startObject("cardinality")
-//                                .field("field", "tag")
-//                            .endObject()
-//                        //.endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
-//
-//    @Test(expected=SearchPhaseExecutionException.class)
-//    public void testMissingType() throws Exception {
-//        createIndex("idx");
-//        ensureGreen();
-//        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
-//            .startObject()
-//                .startObject("by_date")
-//                    .startObject("date_histogram")
-//                        .field("field", "timestamp")
-//                        .field("interval", "month")
-//                    .endObject()
-//                    .startObject("aggs")
-//                        .startObject("tag_count")
-//                            // the aggregation type is missing
-//                            //.startObject("cardinality")
-//                                .field("field", "tag")
-//                            //.endObject()
-//                        .endObject()
-//                    .endObject()
-//            .endObject()).execute().actionGet();
-//    }
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testTwoTypes() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("in_stock")
+                    .startObject("filter")
+                        .startObject("range")
+                            .startObject("stock")
+                                .field("gt", 0)
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                    .startObject("terms")
+                        .field("field", "stock")
+                    .endObject()
+                .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testTwoAggs() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        .startObject("tag_count")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+                    .startObject("aggs") // 2nd "aggs": illegal
+                        .startObject("tag_count2")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testInvalidAggregationName() throws Exception {
+
+        Matcher matcher = Pattern.compile("[^\\[\\]>]+").matcher("");
+        String name;
+        SecureRandom rand = new SecureRandom();
+        int len = randomIntBetween(1, 5);
+        char[] word = new char[len];
+        while(true) {
+            for (int i = 0; i < word.length; i++) {
+                word[i] = (char) rand.nextInt(127);
+            }
+            name = String.valueOf(word);
+            if (!matcher.reset(name).matches()) {
+                break;
+            }
+        }
+
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject(name)
+                    .startObject("filter")
+                        .startObject("range")
+                            .startObject("stock")
+                                .field("gt", 0)
+                            .endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testSameAggregationName() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        final String name = RandomStrings.randomAsciiOfLength(getRandom(), 10);
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject(name)
+                    .startObject("terms")
+                        .field("field", "a")
+                    .endObject()
+                .endObject()
+                .startObject(name)
+                    .startObject("terms")
+                        .field("field", "b")
+                    .endObject()
+                .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testMissingName() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        // the aggregation name is missing
+                        //.startObject("tag_count")
+                            .startObject("cardinality")
+                                .field("field", "tag")
+                            .endObject()
+                        //.endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
+
+    @Test(expected=SearchPhaseExecutionException.class)
+    public void testMissingType() throws Exception {
+        createIndex("idx");
+        ensureGreen();
+        client().prepareSearch("idx").setAggregations(JsonXContent.contentBuilder()
+            .startObject()
+                .startObject("by_date")
+                    .startObject("date_histogram")
+                        .field("field", "timestamp")
+                        .field("interval", "month")
+                    .endObject()
+                    .startObject("aggs")
+                        .startObject("tag_count")
+                            // the aggregation type is missing
+                            //.startObject("cardinality")
+                                .field("field", "tag")
+                            //.endObject()
+                        .endObject()
+                    .endObject()
+            .endObject()).execute().actionGet();
+    }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java
index d045705..e76f48a 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsIT.java
@@ -18,14 +18,13 @@
  */
 package org.elasticsearch.search.aggregations.bucket;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.search.aggregations.Aggregator.SubAggCollectionMode;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
 import org.junit.Test;
 
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.Map;
 
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
@@ -49,16 +48,15 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms  terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
-                .put("1", 8l)
-                .put("3", 8l)
-                .put("2", 5l)
-                .build();
+        Map<String, Long> expected = new HashMap<>();
+        expected.put("1", 8l);
+        expected.put("3", 8l);
+        expected.put("2", 5l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsString())));
         }
     }
-    
+
     @Test
     public void shardSizeEqualsSize_string() throws Exception {
         createIdx("type=string,index=not_analyzed");
@@ -74,11 +72,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms  terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
-                .put("1", 8l)
-                .put("3", 8l)
-                .put("2", 4l)
-                .build();
+        Map<String, Long> expected = new HashMap<>();
+        expected.put("1", 8l);
+        expected.put("3", 8l);
+        expected.put("2", 4l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsString())));
         }
@@ -100,11 +97,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3)); // we still only return 3 entries (based on the 'size' param)
-        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
-                .put("1", 8l)
-                .put("3", 8l)
-                .put("2", 5l) // <-- count is now fixed
-                .build();
+        Map<String, Long> expected = new HashMap<>();
+        expected.put("1", 8l);
+        expected.put("3", 8l);
+        expected.put("2", 5l); // <-- count is now fixed
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsString())));
         }
@@ -126,16 +122,15 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3)); // we still only return 3 entries (based on the 'size' param)
-        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
-                .put("1", 5l)
-                .put("2", 4l)
-                .put("3", 3l) // <-- count is now fixed
-                .build();
+        Map<String, Long> expected = new HashMap<>();
+        expected.put("1", 5l);
+        expected.put("2", 4l);
+        expected.put("3", 3l); // <-- count is now fixed
         for (Terms.Bucket bucket: buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKey())));
         }
     }
-    
+
     @Test
     public void noShardSizeTermOrder_string() throws Exception {
         createIdx("type=string,index=not_analyzed");
@@ -151,11 +146,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms  terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<String, Long> expected = ImmutableMap.<String, Long>builder()
-                .put("1", 8l)
-                .put("2", 5l)
-                .put("3", 8l)
-                .build();
+        Map<String, Long> expected = new HashMap<>();
+        expected.put("1", 8l);
+        expected.put("2", 5l);
+        expected.put("3", 8l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsString())));
         }
@@ -177,11 +171,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 8l)
-                .put(3, 8l)
-                .put(2, 5l)
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 8l);
+        expected.put(3, 8l);
+        expected.put(2, 5l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -203,11 +196,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 8l)
-                .put(3, 8l)
-                .put(2, 4l)
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 8l);
+        expected.put(3, 8l);
+        expected.put(2, 4l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -229,11 +221,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3)); // we still only return 3 entries (based on the 'size' param)
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 8l)
-                .put(3, 8l)
-                .put(2, 5l) // <-- count is now fixed
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 8l);
+        expected.put(3, 8l);
+        expected.put(2, 5l); // <-- count is now fixed
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -255,11 +246,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3)); // we still only return 3 entries (based on the 'size' param)
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 5l)
-                .put(2, 4l)
-                .put(3, 3l)
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 5l);
+        expected.put(2, 4l);
+        expected.put(3, 3l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -281,11 +271,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 8l)
-                .put(2, 5l)
-                .put(3, 8l)
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 8l);
+        expected.put(2, 5l);
+        expected.put(3, 8l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -307,11 +296,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 8l)
-                .put(3, 8l)
-                .put(2, 5l)
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 8l);
+        expected.put(3, 8l);
+        expected.put(2, 5l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -333,11 +321,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 8l)
-                .put(3, 8l)
-                .put(2, 4l)
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 8l);
+        expected.put(3, 8l);
+        expected.put(2, 4l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -359,11 +346,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 8l)
-                .put(3, 8l)
-                .put(2, 5l) // <-- count is now fixed
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 8l);
+        expected.put(3, 8l);
+        expected.put(2, 5l); // <-- count is now fixed
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -385,11 +371,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 5l)
-                .put(2, 4l)
-                .put(3, 3l)
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 5l);
+        expected.put(2, 4l);
+        expected.put(3, 3l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
@@ -411,11 +396,10 @@ public class ShardSizeTermsIT extends ShardSizeTestCase {
         Terms terms = response.getAggregations().get("keys");
         Collection<Terms.Bucket> buckets = terms.getBuckets();
         assertThat(buckets.size(), equalTo(3));
-        Map<Integer, Long> expected = ImmutableMap.<Integer, Long>builder()
-                .put(1, 8l)
-                .put(2, 5l)
-                .put(3, 8l)
-                .build();
+        Map<Integer, Long> expected = new HashMap<>();
+        expected.put(1, 8l);
+        expected.put(2, 5l);
+        expected.put(3, 8l);
         for (Terms.Bucket bucket : buckets) {
             assertThat(bucket.getDocCount(), equalTo(expected.get(bucket.getKeyAsNumber().intValue())));
         }
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
index 71c7ccd..ae01ea1 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/metrics/TopHitsIT.java
@@ -24,6 +24,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.plugins.Plugin;
@@ -539,7 +540,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                 .field(TERMS_AGGS_FIELD)
                                 .subAggregation(
                                         topHits("hits").setSize(1)
-                                            .highlighter(new HighlightBuilder().field("text"))
+                                            .addHighlightedField("text")
                                             .setExplain(true)
                                             .addFieldDataField("field1")
                                             .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
@@ -602,39 +603,38 @@ public class TopHitsIT extends ESIntegTestCase {
         }
     }
 
-    // @Test
-    // public void testFailWithSubAgg() throws Exception {
-    // String source = "{\n" +
-    // "  \"aggs\": {\n" +
-    // "    \"top-tags\": {\n" +
-    // "      \"terms\": {\n" +
-    // "        \"field\": \"tags\"\n" +
-    // "      },\n" +
-    // "      \"aggs\": {\n" +
-    // "        \"top_tags_hits\": {\n" +
-    // "          \"top_hits\": {},\n" +
-    // "          \"aggs\": {\n" +
-    // "            \"max\": {\n" +
-    // "              \"max\": {\n" +
-    // "                \"field\": \"age\"\n" +
-    // "              }\n" +
-    // "            }\n" +
-    // "          }\n" +
-    // "        }\n" +
-    // "      }\n" +
-    // "    }\n" +
-    // "  }\n" +
-    // "}";
-    // try {
-    // client().prepareSearch("idx").setTypes("type")
-    // .setSource(new BytesArray(source))
-    // .get();
-    // fail();
-    // } catch (SearchPhaseExecutionException e) {
-    // assertThat(e.toString(),
-    // containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
-    // }
-    // } NORELEASE this needs to be tested in a top_hits aggregations unit test
+    @Test
+    public void testFailWithSubAgg() throws Exception {
+        String source = "{\n" +
+                "  \"aggs\": {\n" +
+                "    \"top-tags\": {\n" +
+                "      \"terms\": {\n" +
+                "        \"field\": \"tags\"\n" +
+                "      },\n" +
+                "      \"aggs\": {\n" +
+                "        \"top_tags_hits\": {\n" +
+                "          \"top_hits\": {},\n" +
+                "          \"aggs\": {\n" +
+                "            \"max\": {\n" +
+                "              \"max\": {\n" +
+                "                \"field\": \"age\"\n" +
+                "              }\n" +
+                "            }\n" +
+                "          }\n" +
+                "        }\n" +
+                "      }\n" +
+                "    }\n" +
+                "  }\n" +
+                "}";
+        try {
+            client().prepareSearch("idx").setTypes("type")
+                    .setSource(new BytesArray(source))
+                            .get();
+            fail();
+        } catch (SearchPhaseExecutionException e) {
+            assertThat(e.toString(), containsString("Aggregator [top_tags_hits] of type [top_hits] cannot accept sub-aggregations"));
+        }
+    }
 
     @Test
     public void testEmptyIndex() throws Exception {
@@ -862,7 +862,7 @@ public class TopHitsIT extends ESIntegTestCase {
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "comment").queryName("test")))
                 .addAggregation(
                         nested("to-comments").path("comments").subAggregation(
-                                topHits("top-comments").setSize(1).highlighter(new HighlightBuilder().field(hlField)).setExplain(true)
+                                topHits("top-comments").setSize(1).addHighlightedField(hlField).setExplain(true)
                                                 .addFieldDataField("comments.user")
                                         .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap())).setFetchSource("message", null)
                                         .setVersion(true).addSort("comments.date", SortOrder.ASC))).get();
@@ -914,7 +914,7 @@ public class TopHitsIT extends ESIntegTestCase {
                                         nested("to-comments")
                                                 .path("comments")
                                                 .subAggregation(topHits("comments")
-                                                        .highlighter(new HighlightBuilder().field(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text"))))
+                                                        .addHighlightedField(new HighlightBuilder.Field("comments.message").highlightQuery(matchQuery("comments.message", "text")))
                                                         .addSort("comments.id", SortOrder.ASC))
                                 )
                 )
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
index eba86f3..e5cc0ee 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java
@@ -29,23 +29,18 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Priority;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.GeohashCellQuery;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
 
-import static org.elasticsearch.client.Requests.clusterHealthRequest;
-import static org.elasticsearch.client.Requests.refreshRequest;
-import static org.elasticsearch.client.Requests.searchRequest;
+import static org.elasticsearch.client.Requests.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.*;
 
 public class TransportSearchFailuresIT extends ESIntegTestCase {
 
@@ -71,9 +66,7 @@ public class TransportSearchFailuresIT extends ESIntegTestCase {
         assertThat(refreshResponse.getFailedShards(), equalTo(0));
         for (int i = 0; i < 5; i++) {
             try {
-                SearchResponse searchResponse = client().search(
-                        searchRequest("test").source(new SearchSourceBuilder().query(new GeohashCellQuery.Builder("foo", "biz"))))
-                        .actionGet();
+                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
                 assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
                 assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
                 assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
@@ -85,15 +78,11 @@ public class TransportSearchFailuresIT extends ESIntegTestCase {
         }
 
         allowNodes("test", 2);
-        assertThat(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(">=2").execute()
-                .actionGet().isTimedOut(), equalTo(false));
+        assertThat(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForNodes(">=2").execute().actionGet().isTimedOut(), equalTo(false));
 
         logger.info("Running Cluster Health");
-        ClusterHealthResponse clusterHealth = client()
-                .admin()
-                .cluster()
-                .health(clusterHealthRequest("test").waitForYellowStatus().waitForRelocatingShards(0)
-                        .waitForActiveShards(test.totalNumShards)).actionGet();
+        ClusterHealthResponse clusterHealth = client().admin().cluster().health(clusterHealthRequest("test")
+                .waitForYellowStatus().waitForRelocatingShards(0).waitForActiveShards(test.totalNumShards)).actionGet();
         logger.info("Done Cluster Health, status " + clusterHealth.getStatus());
         assertThat(clusterHealth.isTimedOut(), equalTo(false));
         assertThat(clusterHealth.getStatus(), anyOf(equalTo(ClusterHealthStatus.YELLOW), equalTo(ClusterHealthStatus.GREEN)));
@@ -106,9 +95,7 @@ public class TransportSearchFailuresIT extends ESIntegTestCase {
 
         for (int i = 0; i < 5; i++) {
             try {
-                SearchResponse searchResponse = client().search(
-                        searchRequest("test").source(new SearchSourceBuilder().query(new GeohashCellQuery.Builder("foo", "biz"))))
-                        .actionGet();
+                SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
                 assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
                 assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
                 assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
diff --git a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
index 6781c7c..a9b41ce 100644
--- a/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchIT.java
@@ -25,10 +25,10 @@ import org.elasticsearch.action.search.MultiSearchResponse;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.client.Requests;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.GeohashCellQuery;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionBuilder;
 import org.elasticsearch.script.Script;
@@ -52,6 +52,7 @@ import static org.elasticsearch.action.search.SearchType.DFS_QUERY_AND_FETCH;
 import static org.elasticsearch.action.search.SearchType.DFS_QUERY_THEN_FETCH;
 import static org.elasticsearch.action.search.SearchType.QUERY_AND_FETCH;
 import static org.elasticsearch.action.search.SearchType.QUERY_THEN_FETCH;
+
 import static org.elasticsearch.client.Requests.createIndexRequest;
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
@@ -130,7 +131,7 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
             .settings(settingsBuilder))
             .actionGet();
         ensureGreen();
-
+        
         // we need to have age (ie number of repeats of "test" term) high enough
         // to produce the same 8-bit norm for all docs here, so that
         // the tf is basically the entire score (assuming idf is fixed, which
@@ -334,7 +335,7 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
 
         do {
             searchResponse = client().prepareSearchScroll(searchResponse.getScrollId()).setScroll("10m").get();
-
+    
             assertThat(searchResponse.getHits().totalHits(), equalTo(100l));
             assertThat(searchResponse.getHits().hits().length, lessThanOrEqualTo(40));
             for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
@@ -377,8 +378,7 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
 
         logger.info("Start Testing failed search with wrong query");
         try {
-            SearchResponse searchResponse = client().search(
-                    searchRequest("test").source(new SearchSourceBuilder().query(new GeohashCellQuery.Builder("foo", "biz")))).actionGet();
+            SearchResponse searchResponse = client().search(searchRequest("test").source(new BytesArray("{ xxx }"))).actionGet();
             assertThat(searchResponse.getTotalShards(), equalTo(test.numPrimaries));
             assertThat(searchResponse.getSuccessfulShards(), equalTo(0));
             assertThat(searchResponse.getFailedShards(), equalTo(test.numPrimaries));
@@ -388,7 +388,7 @@ public class TransportTwoNodesSearchIT extends ESIntegTestCase {
             // all is well
         }
         logger.info("Done Testing failed search");
-     }
+    }
 
     @Test
     public void testFailedSearchWithWrongFrom() throws Exception {
diff --git a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
index 89a9af4..80a683c 100644
--- a/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java
@@ -16,410 +16,75 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-
 package org.elasticsearch.search.builder;
 
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.AbstractQueryTestCase;
-import org.elasticsearch.index.query.EmptyQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.indices.IndicesModule;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder.InnerHit;
-import org.elasticsearch.search.fetch.source.FetchSourceContext;
-import org.elasticsearch.search.highlight.HighlightBuilder;
-import org.elasticsearch.search.rescore.RescoreBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.suggest.SuggestBuilder;
-import org.elasticsearch.search.suggest.SuggestBuilders;
+import org.elasticsearch.common.xcontent.json.JsonXContent;
 import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.List;
-import java.util.concurrent.TimeUnit;
+import java.util.Map;
 
-import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.*;
 
 public class SearchSourceBuilderTests extends ESTestCase {
 
-    private static Injector injector;
+    SearchSourceBuilder builder = new SearchSourceBuilder();
 
-    private static NamedWriteableRegistry namedWriteableRegistry;
+    @Test // issue #6632
+    public void testThatSearchSourceBuilderIncludesExcludesAreAppliedCorrectly() throws Exception {
+        builder.fetchSource("foo", null);
+        assertIncludes(builder, "foo");
+        assertExcludes(builder);
 
-    private static IndicesQueriesRegistry indicesQueriesRegistry;
+        builder.fetchSource(null, "foo");
+        assertIncludes(builder);
+        assertExcludes(builder, "foo");
 
-    @BeforeClass
-    public static void init() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("name", SearchSourceBuilderTests.class.toString())
-                .put("path.home", createTempDir())
-                .build();
-        injector = new ModulesBuilder().add(
-                new SettingsModule(settings),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesModule(settings) {
-                    @Override
-                    public void configure() {
-                        // skip services
-                        bindQueryParsersExtension();
-                    }
-                },
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(NamedWriteableRegistry.class).asEagerSingleton();
-                    }
-                }
-        ).createInjector();
-        indicesQueriesRegistry = injector.getInstance(IndicesQueriesRegistry.class);
-        namedWriteableRegistry = injector.getInstance(NamedWriteableRegistry.class);
-    }
+        builder.fetchSource(null, new String[]{"foo"});
+        assertIncludes(builder);
+        assertExcludes(builder, "foo");
 
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        namedWriteableRegistry = null;
-        indicesQueriesRegistry = null;
-    }
+        builder.fetchSource(new String[]{"foo"}, null);
+        assertIncludes(builder, "foo");
+        assertExcludes(builder);
 
-    protected final SearchSourceBuilder createSearchSourceBuilder() throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        if (randomBoolean()) {
-            builder.from(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            builder.size(randomIntBetween(0, 10000));
-        }
-        if (randomBoolean()) {
-            builder.explain(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.version(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.trackScores(randomBoolean());
-        }
-        if (randomBoolean()) {
-            builder.minScore(randomFloat() * 1000);
-        }
-        if (randomBoolean()) {
-            builder.timeout(new TimeValue(randomIntBetween(1, 100), randomFrom(TimeUnit.values())));
-        }
-        if (randomBoolean()) {
-            builder.terminateAfter(randomIntBetween(1, 100000));
-        }
-        // if (randomBoolean()) {
-        // builder.defaultRescoreWindowSize(randomIntBetween(1, 100));
-        // }
-        if (randomBoolean()) {
-            int fieldsSize = randomInt(25);
-            List<String> fields = new ArrayList<>(fieldsSize);
-            for (int i = 0; i < fieldsSize; i++) {
-                fields.add(randomAsciiOfLengthBetween(5, 50));
-            }
-            builder.fields(fields);
-        }
-        if (randomBoolean()) {
-            int fieldDataFieldsSize = randomInt(25);
-            for (int i = 0; i < fieldDataFieldsSize; i++) {
-                builder.fieldDataField(randomAsciiOfLengthBetween(5, 50));
-            }
-        }
-        if (randomBoolean()) {
-            int scriptFieldsSize = randomInt(25);
-            for (int i = 0; i < scriptFieldsSize; i++) {
-                if (randomBoolean()) {
-                    builder.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"), randomBoolean());
-                } else {
-                    builder.scriptField(randomAsciiOfLengthBetween(5, 50), new Script("foo"));
-                }
-            }
-        }
-        if (randomBoolean()) {
-            FetchSourceContext fetchSourceContext;
-            int branch = randomInt(5);
-            String[] includes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < includes.length; i++) {
-                includes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            String[] excludes = new String[randomIntBetween(0, 20)];
-            for (int i = 0; i < excludes.length; i++) {
-                excludes[i] = randomAsciiOfLengthBetween(5, 20);
-            }
-            switch (branch) {
-            case 0:
-                fetchSourceContext = new FetchSourceContext(randomBoolean());
-                break;
-            case 1:
-                fetchSourceContext = new FetchSourceContext(includes, excludes);
-                break;
-            case 2:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20));
-                break;
-            case 3:
-                fetchSourceContext = new FetchSourceContext(true, includes, excludes, randomBoolean());
-                break;
-            case 4:
-                fetchSourceContext = new FetchSourceContext(includes);
-                break;
-            case 5:
-                fetchSourceContext = new FetchSourceContext(randomAsciiOfLengthBetween(5, 20));
-                break;
-            default:
-                throw new IllegalStateException();
-            }
-            builder.fetchSource(fetchSourceContext);
-        }
-        if (randomBoolean()) {
-            int size = randomIntBetween(0, 20);
-            List<String> statsGroups = new ArrayList<>(size);
-            for (int i = 0; i < size; i++) {
-                statsGroups.add(randomAsciiOfLengthBetween(5, 20));
-            }
-            builder.stats(statsGroups);
-        }
-        if (randomBoolean()) {
-            int indexBoostSize = randomIntBetween(1, 10);
-            for (int i = 0; i < indexBoostSize; i++) {
-                builder.indexBoost(randomAsciiOfLengthBetween(5, 20), randomFloat() * 10);
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE make RandomQueryBuilder work outside of the
-            // AbstractQueryTestCase
-            // builder.query(RandomQueryBuilder.createQuery(getRandom()));
-            builder.query(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            // NORELEASE make RandomQueryBuilder work outside of the
-            // AbstractQueryTestCase
-            // builder.postFilter(RandomQueryBuilder.createQuery(getRandom()));
-            builder.postFilter(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            int numSorts = randomIntBetween(1, 5);
-            for (int i = 0; i < numSorts; i++) {
-                int branch = randomInt(5);
-                switch (branch) {
-                case 0:
-                    builder.sort(SortBuilders.fieldSort(randomAsciiOfLengthBetween(5, 20)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 1:
-                    builder.sort(SortBuilders.geoDistanceSort(randomAsciiOfLengthBetween(5, 20))
-                            .geohashes(AbstractQueryTestCase.randomGeohash(1, 12)).order(randomFrom(SortOrder.values())));
-                    break;
-                case 2:
-                    builder.sort(SortBuilders.scoreSort().order(randomFrom(SortOrder.values())));
-                    break;
-                case 3:
-                    builder.sort(SortBuilders.scriptSort(new Script("foo"), "number").order(randomFrom(SortOrder.values())));
-                    break;
-                case 4:
-                    builder.sort(randomAsciiOfLengthBetween(5, 20));
-                    break;
-                case 5:
-                    builder.sort(randomAsciiOfLengthBetween(5, 20), randomFrom(SortOrder.values()));
-                    break;
-                }
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random highlight builder method
-            builder.highlighter(new HighlightBuilder().field(randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random suggest builder method
-            builder.suggest(new SuggestBuilder().setText(randomAsciiOfLengthBetween(1, 5)).addSuggestion(
-                    SuggestBuilders.termSuggestion(randomAsciiOfLengthBetween(1, 5))));
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random inner hits builder method
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            InnerHit innerHit = new InnerHit();
-            innerHit.field(randomAsciiOfLengthBetween(5, 20));
-            innerHitsBuilder.addNestedInnerHits(randomAsciiOfLengthBetween(5, 20), randomAsciiOfLengthBetween(5, 20), innerHit);
-            builder.innerHits(innerHitsBuilder);
-        }
-        if (randomBoolean()) {
-            int numRescores = randomIntBetween(1, 5);
-            for (int i = 0; i < numRescores; i++) {
-                // NORELEASE need a random rescore builder method
-                RescoreBuilder rescoreBuilder = new RescoreBuilder();
-                rescoreBuilder.rescorer(RescoreBuilder.queryRescorer(QueryBuilders.termQuery(randomAsciiOfLengthBetween(5, 20),
-                        randomAsciiOfLengthBetween(5, 20))));
-                builder.addRescorer(rescoreBuilder);
-            }
-        }
-        if (randomBoolean()) {
-            // NORELEASE need a random aggregation builder method
-            builder.aggregation(AggregationBuilders.avg(randomAsciiOfLengthBetween(5, 20)));
-        }
-        if (true) {
-            // NORELEASE need a method to randomly build content for ext
-            XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
-            xContentBuilder.startObject();
-            xContentBuilder.field("term_vectors_fetch", randomAsciiOfLengthBetween(5, 20));
-            xContentBuilder.endObject();
-            builder.ext(xContentBuilder);
-        }
-        return builder;
-    }
-
-    @Test
-    public void testFromXContent() throws IOException {
-        SearchSourceBuilder testBuilder = createSearchSourceBuilder();
-        String builderAsString = testBuilder.toString();
-        assertParseSearchSource(testBuilder, builderAsString);
-    }
-
-    private void assertParseSearchSource(SearchSourceBuilder testBuilder, String builderAsString) throws IOException {
-        XContentParser parser = XContentFactory.xContent(builderAsString).createParser(builderAsString);
-        SearchSourceBuilder newBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-        assertNotSame(testBuilder, newBuilder);
-        assertEquals(testBuilder, newBuilder);
-        assertEquals(testBuilder.hashCode(), newBuilder.hashCode());
-    }
+        builder.fetchSource("foo", "bar");
+        assertIncludes(builder, "foo");
+        assertExcludes(builder, "bar");
 
-    private static QueryParseContext createParseContext(XContentParser parser) {
-        QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-        context.reset(parser);
-        context.parseFieldMatcher(ParseFieldMatcher.STRICT);
-        return context;
+        builder.fetchSource(new String[]{"foo"}, new String[]{"bar", "baz"});
+        assertIncludes(builder, "foo");
+        assertExcludes(builder, "bar", "baz");
     }
 
-    @Test
-    public void testSerialization() throws IOException {
-        SearchSourceBuilder testBuilder = createSearchSourceBuilder();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testBuilder.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                SearchSourceBuilder deserializedBuilder = SearchSourceBuilder.readSearchSourceFrom(in);
-                assertEquals(deserializedBuilder, testBuilder);
-                assertEquals(deserializedBuilder.hashCode(), testBuilder.hashCode());
-                assertNotSame(deserializedBuilder, testBuilder);
-            }
-        }
+    private void assertIncludes(SearchSourceBuilder builder, String... elems) throws IOException {
+        assertFieldValues(builder, "includes", elems);
     }
 
-    @Test
-    public void testEqualsAndHashcode() throws IOException {
-        SearchSourceBuilder firstBuilder = createSearchSourceBuilder();
-        assertFalse("source builder is equal to null", firstBuilder.equals(null));
-        assertFalse("source builder is equal to incompatible type", firstBuilder.equals(""));
-        assertTrue("source builder is not equal to self", firstBuilder.equals(firstBuilder));
-        assertThat("same source builder's hashcode returns different values if called multiple times", firstBuilder.hashCode(),
-                equalTo(firstBuilder.hashCode()));
-
-        SearchSourceBuilder secondBuilder = copyBuilder(firstBuilder);
-        assertTrue("source builder is not equal to self", secondBuilder.equals(secondBuilder));
-        assertTrue("source builder is not equal to its copy", firstBuilder.equals(secondBuilder));
-        assertTrue("source builder is not symmetric", secondBuilder.equals(firstBuilder));
-        assertThat("source builder copy's hashcode is different from original hashcode", secondBuilder.hashCode(), equalTo(firstBuilder.hashCode()));
-
-        SearchSourceBuilder thirdBuilder = copyBuilder(secondBuilder);
-        assertTrue("source builder is not equal to self", thirdBuilder.equals(thirdBuilder));
-        assertTrue("source builder is not equal to its copy", secondBuilder.equals(thirdBuilder));
-        assertThat("source builder copy's hashcode is different from original hashcode", secondBuilder.hashCode(), equalTo(thirdBuilder.hashCode()));
-        assertTrue("equals is not transitive", firstBuilder.equals(thirdBuilder));
-        assertThat("source builder copy's hashcode is different from original hashcode", firstBuilder.hashCode(), equalTo(thirdBuilder.hashCode()));
-        assertTrue("equals is not symmetric", thirdBuilder.equals(secondBuilder));
-        assertTrue("equals is not symmetric", thirdBuilder.equals(firstBuilder));
+    private void assertExcludes(SearchSourceBuilder builder, String... elems) throws IOException {
+        assertFieldValues(builder, "excludes", elems);
     }
 
-    //we use the streaming infra to create a copy of the query provided as argument
-    protected SearchSourceBuilder copyBuilder(SearchSourceBuilder builder) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            builder.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                return SearchSourceBuilder.readSearchSourceFrom(in);
-            }
-        }
-    }
+    private void assertFieldValues(SearchSourceBuilder builder, String fieldName, String... elems) throws IOException {
+        Map<String, Object> map = getSourceMap(builder);
 
-    public void testParseIncludeExclude() throws IOException {
-        {
-            String restContent = " { \"_source\": { \"includes\": \"include\", \"excludes\": \"*.field2\"}}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-                assertArrayEquals(new String[]{"*.field2" }, searchSourceBuilder.fetchSource().excludes());
-                assertArrayEquals(new String[]{"include" }, searchSourceBuilder.fetchSource().includes());
-            }
-        }
-        {
-            String restContent = " { \"_source\": false}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-                assertArrayEquals(new String[]{}, searchSourceBuilder.fetchSource().excludes());
-                assertArrayEquals(new String[]{}, searchSourceBuilder.fetchSource().includes());
-                assertFalse(searchSourceBuilder.fetchSource().fetchSource());
-            }
-        }
+        assertThat(map, hasKey(fieldName));
+        assertThat(map.get(fieldName), is(instanceOf(List.class)));
+        List<String> castedList = (List<String>) map.get(fieldName);
+        assertThat(castedList, hasSize(elems.length));
+        assertThat(castedList, hasItems(elems));
     }
 
-    @Test
-    public void testParseSort() throws IOException {
-        {
-            String restContent = " { \"sort\": \"foo\"}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-                assertEquals(1, searchSourceBuilder.sorts().size());
-                assertEquals("{\"foo\":{}}", searchSourceBuilder.sorts().get(0).toUtf8());
-            }
-        }
-
-        {
-            String restContent = "{\"sort\" : [\n" +
-                    "        { \"post_date\" : {\"order\" : \"asc\"}},\n" +
-                    "        \"user\",\n" +
-                    "        { \"name\" : \"desc\" },\n" +
-                    "        { \"age\" : \"desc\" },\n" +
-                    "        \"_score\"\n" +
-                    "    ]}";
-            try (XContentParser parser = XContentFactory.xContent(restContent).createParser(restContent)) {
-                SearchSourceBuilder searchSourceBuilder = SearchSourceBuilder.parseSearchSource(parser, createParseContext(parser));
-                assertEquals(5, searchSourceBuilder.sorts().size());
-                assertEquals("{\"post_date\":{\"order\":\"asc\"}}", searchSourceBuilder.sorts().get(0).toUtf8());
-                assertEquals("\"user\"", searchSourceBuilder.sorts().get(1).toUtf8());
-                assertEquals("{\"name\":\"desc\"}", searchSourceBuilder.sorts().get(2).toUtf8());
-                assertEquals("{\"age\":\"desc\"}", searchSourceBuilder.sorts().get(3).toUtf8());
-                assertEquals("\"_score\"", searchSourceBuilder.sorts().get(4).toUtf8());
-            }
+    private Map<String, Object> getSourceMap(SearchSourceBuilder builder) throws IOException {
+        Map<String, Object> data;
+        try (XContentParser parser = JsonXContent.jsonXContent.createParser(builder.toString())) {
+            data = parser.map();
         }
+        assertThat(data, hasKey("_source"));
+        return (Map<String, Object>) data.get("_source");
     }
 
-    @Test
-    public void testEmptyPostFilter() throws IOException {
-        SearchSourceBuilder builder = new SearchSourceBuilder();
-        builder.postFilter(EmptyQueryBuilder.PROTOTYPE);
-        String query = "{ \"post_filter\": {} }";
-        assertParseSearchSource(builder, query);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
index dadb0f6..09f33f6 100644
--- a/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java
@@ -27,6 +27,7 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.SearchType;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
 import org.elasticsearch.common.settings.Settings;
@@ -43,7 +44,6 @@ import org.elasticsearch.search.aggregations.AggregationBuilders;
 import org.elasticsearch.search.aggregations.bucket.filter.Filter;
 import org.elasticsearch.search.aggregations.bucket.global.Global;
 import org.elasticsearch.search.aggregations.bucket.terms.Terms;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -53,43 +53,15 @@ import org.hamcrest.Matchers;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
+import java.util.*;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasParentQuery;
-import static org.elasticsearch.index.query.QueryBuilders.idsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.notQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termsQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.fieldValueFactorFunction;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.weightFactorFunction;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThanOrEqualTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.notNullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -1481,9 +1453,14 @@ public class ChildQuerySearchIT extends ESIntegTestCase {
 
         SearchResponse resp;
         resp = client().prepareSearch("test")
-                .setSource(new SearchSourceBuilder().query(QueryBuilders.hasChildQuery("posts", QueryBuilders.matchQuery("field", "bar"))))
-                .get();
+                .setSource(new BytesArray("{\"query\": {\"has_child\": {\"type\": \"posts\", \"query\": {\"match\": {\"field\": \"bar\"}}}}}")).get();
         assertHitCount(resp, 1L);
+
+        // Now reverse the order for the type after the query
+        resp = client().prepareSearch("test")
+                .setSource(new BytesArray("{\"query\": {\"has_child\": {\"query\": {\"match\": {\"field\": \"bar\"}}, \"type\": \"posts\"}}}")).get();
+        assertHitCount(resp, 1L);
+
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
index 66ba1fe..4804842 100644
--- a/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
+++ b/core/src/test/java/org/elasticsearch/search/fetch/FetchSubPhasePluginIT.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.search.fetch;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.index.PostingsEnum;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.BytesRef;
@@ -28,14 +26,12 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.termvectors.TermVectorsRequest;
 import org.elasticsearch.action.termvectors.TermVectorsResponse;
 import org.elasticsearch.common.Priority;
-import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.termvectors.TermVectorsService;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchModule;
 import org.elasticsearch.search.SearchParseElement;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.internal.InternalSearchHit;
 import org.elasticsearch.search.internal.InternalSearchHitField;
 import org.elasticsearch.search.internal.SearchContext;
@@ -50,10 +46,11 @@ import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
 
+import static java.util.Collections.singletonMap;
 import static org.elasticsearch.client.Requests.indexRequest;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.Matchers.equalTo;
 
 /**
  *
@@ -89,16 +86,14 @@ public class FetchSubPhasePluginIT extends ESIntegTestCase {
 
         client().admin().indices().prepareRefresh().execute().actionGet();
 
-        XContentBuilder extSource = jsonBuilder().startObject()
+        String searchSource = jsonBuilder().startObject()
                 .field("term_vectors_fetch", "test")
-                .endObject();
-         SearchResponse response = client().prepareSearch().setSource(new SearchSourceBuilder().ext(extSource)).get();
+                .endObject().string();
+        SearchResponse response = client().prepareSearch().setSource(new BytesArray(searchSource)).get();
         assertSearchResponse(response);
         assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("i"), equalTo(2));
-        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("am"),
-                equalTo(2));
-        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("sam"),
-                equalTo(1));
+        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("am"), equalTo(2));
+        assertThat(((Map<String, Integer>) response.getHits().getAt(0).field("term_vectors_fetch").getValues().get(0)).get("sam"), equalTo(1));
     }
 
     public static class FetchTermVectorsPlugin extends Plugin {
@@ -140,7 +135,7 @@ public class FetchSubPhasePluginIT extends ESIntegTestCase {
 
         @Override
         public Map<String, ? extends SearchParseElement> parseElements() {
-            return ImmutableMap.of("term_vectors_fetch", new TermVectorsFetchParseElement());
+            return singletonMap("term_vectors_fetch", new TermVectorsFetchParseElement());
         }
 
         @Override
diff --git a/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java b/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java
deleted file mode 100644
index 7fce514..0000000
--- a/core/src/test/java/org/elasticsearch/search/fetch/FieldDataFieldsTests.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.search.fetch;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.bytes.BytesArray;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.TestSearchContext;
-import org.junit.Test;
-
-public class FieldDataFieldsTests extends ESTestCase {
-
-    public void testValidFieldDataFieldString() throws Exception {
-        FieldDataFieldsParseElement parseElement = new FieldDataFieldsParseElement();
-
-        BytesArray data = new BytesArray(new BytesRef("{\"fielddata_fields\": \"foobar\"}"));
-        XContentParser parser = XContentFactory.xContent(data).createParser(data);
-        parser.nextToken();
-        parser.nextToken();
-        parser.nextToken();
-        SearchContext context = new TestSearchContext();
-        parseElement.parse(parser, context);
-    }
-
-    public void testValidFieldDataFieldArray() throws Exception {
-        FieldDataFieldsParseElement parseElement = new FieldDataFieldsParseElement();
-
-        BytesArray data = new BytesArray(new BytesRef("{\"fielddata_fields\": [ \"foo\", \"bar\", \"baz\"]}}"));
-        XContentParser parser = XContentFactory.xContent(data).createParser(data);
-        parser.nextToken();
-        parser.nextToken();
-        parser.nextToken();
-        SearchContext context = new TestSearchContext();
-        parseElement.parse(parser, context);
-    }
-
-    @Test(expected = IllegalStateException.class)
-    public void testInvalidFieldDataField() throws Exception {
-        FieldDataFieldsParseElement parseElement = new FieldDataFieldsParseElement();
-
-        BytesArray data;
-        if (randomBoolean()) {
-            data = new BytesArray(new BytesRef("{\"fielddata_fields\": {}}}"));
-        } else {
-            data = new BytesArray(new BytesRef("{\"fielddata_fields\": 1.0}}"));
-        }
-        XContentParser parser = XContentFactory.xContent(data).createParser(data);
-        parser.nextToken();
-        parser.nextToken();
-        parser.nextToken();
-        SearchContext context = new TestSearchContext();
-        parseElement.parse(parser, context);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
index d362b36..983fb52 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java
@@ -27,8 +27,11 @@ import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;
+import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder;
 import org.elasticsearch.search.MultiValueMode;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -45,23 +48,11 @@ import java.util.concurrent.ExecutionException;
 import static org.elasticsearch.client.Requests.indexRequest;
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.exponentialDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.gaussDecayFunction;
-import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.linearDecayFunction;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.*;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertOrderedSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.isOneOf;
-import static org.hamcrest.Matchers.lessThan;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 
 public class DecayFunctionScoreIT extends ESIntegTestCase {
@@ -442,10 +433,10 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         SearchResponse sr = response.actionGet();
         assertOrderedSearchHits(sr, "2", "1");
     }
-
+    
     @Test
     public void testParseDateMath() throws Exception {
-
+        
         assertAcked(prepareCreate("test").addMapping(
                 "type1",
                 jsonBuilder().startObject().startObject("type1").startObject("properties").startObject("test").field("type", "string")
@@ -466,7 +457,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
 
         assertNoFailures(sr);
         assertOrderedSearchHits(sr, "1", "2");
-
+        
         sr = client().search(
                 searchRequest().source(
                         searchSource().query(
@@ -591,9 +582,9 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         List<IndexRequestBuilder> indexBuilders = new ArrayList<>();
 
         for (int i = 0; i < numDocs; i++) {
-            double lat = 100 + (int) (10.0 * (i) / (numDocs));
+            double lat = 100 + (int) (10.0 * (float) (i) / (float) (numDocs));
             double lon = 100;
-            int day = (int) (29.0 * (i) / (numDocs)) + 1;
+            int day = (int) (29.0 * (float) (i) / (float) (numDocs)) + 1;
             String dayString = day < 10 ? "0" + Integer.toString(day) : Integer.toString(day);
             String date = "2013-05-" + dayString;
 
@@ -781,7 +772,7 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
 
         assertThat(sh.getAt(0).getId(), equalTo("2"));
         assertThat(sh.getAt(1).getId(), equalTo("1"));
-        assertThat(1.0 - sh.getAt(0).getScore(), closeTo((1.0 - sh.getAt(1).getScore())/3.0, 1.e-6d));
+        assertThat((double)(1.0 - sh.getAt(0).getScore()), closeTo((double)((1.0 - sh.getAt(1).getScore())/3.0), 1.e-6d));
         response = client().search(
                 searchRequest().source(
                         searchSource().query(
@@ -789,6 +780,47 @@ public class DecayFunctionScoreIT extends ESIntegTestCase {
         sr = response.actionGet();
         assertSearchHits(sr, "1", "2");
         sh = sr.getHits();
-        assertThat((double) (sh.getAt(0).getScore()), closeTo((sh.getAt(1).getScore()), 1.e-6d));
+        assertThat((double) (sh.getAt(0).getScore()), closeTo((double) (sh.getAt(1).getScore()), 1.e-6d));
+    }
+
+    @Test
+    public void errorMessageForFaultyFunctionScoreBody() throws Exception {
+        assertAcked(prepareCreate("test").addMapping(
+                "type",
+                jsonBuilder().startObject().startObject("type").startObject("properties").startObject("test").field("type", "string")
+                        .endObject().startObject("num").field("type", "double").endObject().endObject().endObject().endObject()));
+        ensureYellow();
+        client().index(
+                indexRequest("test").type("type").source(jsonBuilder().startObject().field("test", "value").field("num", 1.0).endObject()))
+                .actionGet();
+        refresh();
+
+        XContentBuilder query = XContentFactory.jsonBuilder();
+        // query that contains a single function and a functions[] array
+        query.startObject().startObject("function_score").field("weight", "1").startArray("functions").startObject().startObject("script_score").field("script", "3").endObject().endObject().endArray().endObject().endObject();
+        try {
+            client().search(
+                    searchRequest().source(
+                            searchSource().query(query))).actionGet();
+            fail("Search should result in SearchPhaseExecutionException");
+        } catch (SearchPhaseExecutionException e) {
+            logger.info(e.shardFailures()[0].reason());
+            assertThat(e.shardFailures()[0].reason(), containsString("already found [weight], now encountering [functions]."));
+        }
+
+        query = XContentFactory.jsonBuilder();
+        // query that contains a single function (but not boost factor) and a functions[] array
+        query.startObject().startObject("function_score").startObject("random_score").field("seed", 3).endObject().startArray("functions").startObject().startObject("random_score").field("seed", 3).endObject().endObject().endArray().endObject().endObject();
+        try {
+            client().search(
+                    searchRequest().source(
+                            searchSource().query(query))).actionGet();
+            fail("Search should result in SearchPhaseExecutionException");
+        } catch (SearchPhaseExecutionException e) {
+            logger.info(e.shardFailures()[0].reason());
+            assertThat(e.shardFailures()[0].reason(), containsString("already found [random_score], now encountering [functions]"));
+            assertThat(e.shardFailures()[0].reason(), not(containsString("did you mean [boost] instead?")));
+
+        }
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java b/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
index dce6ef3..e906ac6 100644
--- a/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/functionscore/QueryRescorerIT.java
@@ -48,19 +48,8 @@ import java.util.Comparator;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFourthHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThirdHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasScore;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.lessThanOrEqualTo;
-import static org.hamcrest.Matchers.notNullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  *
@@ -84,9 +73,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .setQuery(QueryBuilders.matchAllQuery())
                     .setRescorer(RescoreBuilder.queryRescorer(
                             QueryBuilders.functionScoreQuery(QueryBuilders.matchAllQuery(),
-                                                    ScoreFunctionBuilders.weightFactorFunction(100)).boostMode(CombineFunction.REPLACE))
-                                    .setQueryWeight(0.0f).setRescoreQueryWeight(1.0f), 1).setSize(randomIntBetween(2, 10)).execute()
-                    .actionGet();
+                                    ScoreFunctionBuilders.weightFactorFunction(100)).boostMode(CombineFunction.REPLACE)).setQueryWeight(0.0f).setRescoreQueryWeight(1.0f))
+                    .setRescoreWindow(1).setSize(randomIntBetween(2, 10)).execute().actionGet();
             assertSearchResponse(searchResponse);
             assertFirstHit(searchResponse, hasScore(100.f));
             int numDocsWith100AsAScore = 0;
@@ -118,9 +106,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
         refresh();
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                .setRescorer(
-                        RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "quick brown").slop(2).boost(4.0f))
-                                .setRescoreQueryWeight(2), 5).execute().actionGet();
+                .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "quick brown").slop(2).boost(4.0f)).setRescoreQueryWeight(2))
+                .setRescoreWindow(5).execute().actionGet();
 
         assertThat(searchResponse.getHits().totalHits(), equalTo(3l));
         assertThat(searchResponse.getHits().getHits()[0].getId(), equalTo("1"));
@@ -129,8 +116,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch()
                 .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(3)), 5)
-                .execute().actionGet();
+                .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(3)))
+                .setRescoreWindow(5).execute().actionGet();
 
         assertHitCount(searchResponse, 3);
         assertFirstHit(searchResponse, hasId("1"));
@@ -139,8 +126,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch()
                 .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                .setRescorer(RescoreBuilder.queryRescorer((QueryBuilders.matchPhraseQuery("field1", "the quick brown"))), 5).execute()
-                .actionGet();
+                .setRescorer(RescoreBuilder.queryRescorer((QueryBuilders.matchPhraseQuery("field1", "the quick brown"))))
+                .setRescoreWindow(5).execute().actionGet();
 
         assertHitCount(searchResponse, 3);
         assertFirstHit(searchResponse, hasId("1"));
@@ -186,7 +173,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSize(5)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 20).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(20).execute().actionGet();
 
         assertThat(searchResponse.getHits().hits().length, equalTo(5));
         assertHitCount(searchResponse, 9);
@@ -202,7 +189,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 20).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(20).execute().actionGet();
 
         assertThat(searchResponse.getHits().hits().length, equalTo(5));
         assertHitCount(searchResponse, 9);
@@ -219,7 +206,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 20).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(20).execute().actionGet();
 
         assertThat(searchResponse.getHits().hits().length, equalTo(5));
         assertHitCount(searchResponse, 9);
@@ -269,7 +256,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSize(5)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 2).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(2).execute().actionGet();
         // Only top 2 hits were re-ordered:
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
         assertHitCount(searchResponse, 4);
@@ -286,7 +273,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSize(5)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f), 3).execute().actionGet();
+                                .setQueryWeight(0.6f).setRescoreQueryWeight(2.0f)).setRescoreWindow(3).execute().actionGet();
 
         // Only top 3 hits were re-ordered:
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
@@ -340,7 +327,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setSize(5)
                 .setRescorer(
                         RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "lexington avenue massachusetts").slop(3))
-                                .setQueryWeight(1.0f).setRescoreQueryWeight(-1f), 3).execute().actionGet();
+                                .setQueryWeight(1.0f).setRescoreQueryWeight(-1f)).setRescoreWindow(3).execute().actionGet();
 
         // 6 and 1 got worse, and then the hit (2) outside the rescore window were sorted ahead:
         assertFirstHit(searchResponse, hasId("3"));
@@ -436,28 +423,15 @@ public class QueryRescorerIT extends ESIntegTestCase {
                                             QueryBuilders
                                                     .constantScoreQuery(QueryBuilders.matchPhraseQuery("field1", intToEnglish).slop(3)))
                                     .setQueryWeight(1.0f)
-.setRescoreQueryWeight(0.0f), rescoreWindow) // no
-                                                                                                      // weight
-                                                                                                      // -
-                                                                                                      // so
-                                                                                                      // we
-                                                                                                      // basically
-                                                                                                      // use
-                                                                                                      // the
-                                                                                                      // same
-                                                                                                      // score
-                                                                                                      // as
-                                                                                                      // the
-                                                                                                      // actual
-                                                                                                      // query
-                    .execute().actionGet();
+                                    .setRescoreQueryWeight(0.0f)) // no weight - so we basically use the same score as the actual query
+                    .setRescoreWindow(rescoreWindow).execute().actionGet();
 
             SearchResponse plain = client().prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
                     .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR)).setFrom(0).setSize(resultSize)
                     .execute().actionGet();
-
+            
             // check equivalence
             assertEquivalent(query, plain, rescored);
 
@@ -474,8 +448,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
                                             QueryBuilders
                                                     .constantScoreQuery(QueryBuilders.matchPhraseQuery("field1", "not in the index").slop(3)))
                                     .setQueryWeight(1.0f)
-.setRescoreQueryWeight(1.0f), rescoreWindow).execute()
-                    .actionGet();
+                                    .setRescoreQueryWeight(1.0f))
+                    .setRescoreWindow(rescoreWindow).execute().actionGet();
             // check equivalence
             assertEquivalent(query, plain, rescored);
 
@@ -490,7 +464,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                             RescoreBuilder
                                     .queryRescorer(
                                             QueryBuilders.matchPhraseQuery("field1", intToEnglish).slop(0))
-                                    .setQueryWeight(1.0f).setRescoreQueryWeight(1.0f), 2 * rescoreWindow).execute().actionGet();
+                                    .setQueryWeight(1.0f).setRescoreQueryWeight(1.0f)).setRescoreWindow(2 * rescoreWindow).execute().actionGet();
             // check equivalence or if the first match differs we check if the phrase is a substring of the top doc
             assertEquivalentOrSubstringMatch(intToEnglish, plain, rescored);
         }
@@ -521,7 +495,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
                     .setRescorer(
                             RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(2).boost(4.0f))
-                                    .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f), 5).setExplain(true).execute()
+                                    .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f)).setRescoreWindow(5).setExplain(true).execute()
                     .actionGet();
             assertHitCount(searchResponse, 3);
             assertFirstHit(searchResponse, hasId("1"));
@@ -557,7 +531,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                     .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                    .setRescorer(innerRescoreQuery, 5).setExplain(true).execute()
+                    .setRescorer(innerRescoreQuery).setRescoreWindow(5).setExplain(true).execute()
                     .actionGet();
             assertHitCount(searchResponse, 3);
             assertFirstHit(searchResponse, hasId("1"));
@@ -580,7 +554,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
                         .prepareSearch()
                         .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                         .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
-                        .addRescorer(innerRescoreQuery, 5).addRescorer(outerRescoreQuery, 10)
+                        .addRescorer(innerRescoreQuery).setRescoreWindow(5)
+                        .addRescorer(outerRescoreQuery).setRescoreWindow(10)
                         .setExplain(true).get();
                 assertHitCount(searchResponse, 3);
                 assertFirstHit(searchResponse, hasId("1"));
@@ -640,7 +615,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
                                         ScoreFunctionBuilders.weightFactorFunction(0.2f)).boostMode(CombineFunction.REPLACE)))
                                 .setFrom(0)
                                 .setSize(10)
-.setRescorer(rescoreQuery, 50).execute().actionGet();
+                                .setRescorer(rescoreQuery)
+                                .setRescoreWindow(50).execute().actionGet();
 
                 assertHitCount(rescored, 4);
 
@@ -701,14 +677,14 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 .setScoreMode("total");
 
         // First set the rescore window large enough that both rescores take effect
-        SearchRequestBuilder request = client().prepareSearch();
-        request.addRescorer(eightIsGreat, numDocs).addRescorer(sevenIsBetter, numDocs);
+        SearchRequestBuilder request = client().prepareSearch().setRescoreWindow(numDocs);
+        request.addRescorer(eightIsGreat).addRescorer(sevenIsBetter);
         SearchResponse response = request.get();
         assertFirstHit(response, hasId("7"));
         assertSecondHit(response, hasId("8"));
 
         // Now squash the second rescore window so it never gets to see a seven
-        response = request.setSize(1).clearRescorers().addRescorer(eightIsGreat, numDocs).addRescorer(sevenIsBetter, 1).get();
+        response = request.setSize(1).clearRescorers().addRescorer(eightIsGreat).addRescorer(sevenIsBetter, 1).get();
         assertFirstHit(response, hasId("8"));
         // We have no idea what the second hit will be because we didn't get a chance to look for seven
 
@@ -719,7 +695,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         QueryRescorer oneToo = RescoreBuilder.queryRescorer(
                 QueryBuilders.functionScoreQuery(QueryBuilders.queryStringQuery("*one*"), ScoreFunctionBuilders.weightFactorFunction(1000.0f))
                         .boostMode(CombineFunction.REPLACE)).setScoreMode("total");
-        request.clearRescorers().addRescorer(ninetyIsGood, numDocs).addRescorer(oneToo, 10);
+        request.clearRescorers().addRescorer(ninetyIsGood).addRescorer(oneToo, 10);
         response = request.setSize(2).get();
         assertFirstHit(response, hasId("91"));
         assertFirstHit(response, hasScore(2001.0f));
@@ -769,7 +745,8 @@ public class QueryRescorerIT extends ESIntegTestCase {
         request.setQuery(QueryBuilders.termQuery("text", "hello"));
         request.setFrom(1);
         request.setSize(4);
-        request.addRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchAllQuery()), 50);
+        request.addRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchAllQuery()));
+        request.setRescoreWindow(50);
 
         assertEquals(4, request.get().getHits().hits().length);
     }
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
index b2b1b6f..3cb7025 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoFilterIT.java
@@ -57,6 +57,7 @@ import java.io.IOException;
 import java.io.InputStream;
 import java.util.ArrayList;
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -516,30 +517,52 @@ public class GeoFilterIT extends ESIntegTestCase {
 
         expectedCounts.put(geoHashCellQuery("pin", point).neighbors(true).precision(precision), 1L + neighbors.size());
 
+        logger.info("random testing of setting");
 
         List<GeohashCellQuery.Builder> filterBuilders = new ArrayList<>(expectedCounts.keySet());
-        for (GeohashCellQuery.Builder builder : filterBuilders) {
-            try {
-                long expectedCount = expectedCounts.get(builder);
-                SearchResponse response = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery())
-                        .setPostFilter(builder).setSize((int) expectedCount).get();
-                assertHitCount(response, expectedCount);
-                String[] expectedIds = expectedResults.get(builder);
-                if (expectedIds == null) {
-                    ArrayList<String> ids = new ArrayList<>();
-                    for (SearchHit hit : response.getHits()) {
-                        ids.add(hit.id());
+        for (int j = filterBuilders.size() * 2 * randomIntBetween(1, 5); j > 0; j--) {
+            Collections.shuffle(filterBuilders, getRandom());
+            for (GeohashCellQuery.Builder builder : filterBuilders) {
+                try {
+                    long expectedCount = expectedCounts.get(builder);
+                    SearchResponse response = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery())
+                            .setPostFilter(builder).setSize((int) expectedCount).get();
+                    assertHitCount(response, expectedCount);
+                    String[] expectedIds = expectedResults.get(builder);
+                    if (expectedIds == null) {
+                        ArrayList<String> ids = new ArrayList<>();
+                        for (SearchHit hit : response.getHits()) {
+                            ids.add(hit.id());
+                        }
+                        expectedResults.put(builder, ids.toArray(Strings.EMPTY_ARRAY));
+                        continue;
                     }
-                    expectedResults.put(builder, ids.toArray(Strings.EMPTY_ARRAY));
-                    continue;
+
+                    assertSearchHits(response, expectedIds);
+
+                } catch (AssertionError error) {
+                    throw new AssertionError(error.getMessage() + "\n geohash_cell filter:" + builder, error);
                 }
 
-                assertSearchHits(response, expectedIds);
 
-            } catch (AssertionError error) {
-                throw new AssertionError(error.getMessage() + "\n geohash_cell filter:" + builder, error);
             }
         }
+
+        logger.info("Testing lat/lon format");
+        String pointTest1 = "{\"geohash_cell\": {\"pin\": {\"lat\": " + point.lat() + ",\"lon\": " + point.lon() + "},\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results3 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest1).execute().actionGet();
+        assertHitCount(results3, neighbors.size() + 1);
+
+
+        logger.info("Testing String format");
+        String pointTest2 = "{\"geohash_cell\": {\"pin\": \"" + point.lat() + "," + point.lon() + "\",\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results4 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest2).execute().actionGet();
+        assertHitCount(results4, neighbors.size() + 1);
+
+        logger.info("Testing Array format");
+        String pointTest3 = "{\"geohash_cell\": {\"pin\": [" + point.lon() + "," + point.lat() + "],\"precision\": " + precision + ",\"neighbors\": true}}";
+        SearchResponse results5 = client().prepareSearch("locations").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(pointTest3).execute().actionGet();
+        assertHitCount(results5, neighbors.size() + 1);
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java b/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
index 1f057af..670d317 100644
--- a/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
+++ b/core/src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationIT.java
@@ -226,6 +226,44 @@ public class GeoShapeIntegrationIT extends ESIntegTestCase {
     }
 
     @Test
+    public void testParsingMultipleShapes() throws Exception {
+        String mapping = XContentFactory.jsonBuilder()
+                .startObject()
+                .startObject("type1")
+                .startObject("properties")
+                .startObject("location1")
+                .field("type", "geo_shape")
+                .endObject()
+                .startObject("location2")
+                .field("type", "geo_shape")
+                .endObject()
+                .endObject()
+                .endObject()
+                .endObject()
+                .string();
+
+        assertAcked(prepareCreate("test").addMapping("type1", mapping));
+        ensureYellow();
+
+        String p1 = "\"location1\" : {\"type\":\"polygon\", \"coordinates\":[[[-10,-10],[10,-10],[10,10],[-10,10],[-10,-10]]]}";
+        String p2 = "\"location2\" : {\"type\":\"polygon\", \"coordinates\":[[[-20,-20],[20,-20],[20,20],[-20,20],[-20,-20]]]}";
+        String o1 = "{" + p1 + ", " + p2 + "}";
+
+        indexRandom(true, client().prepareIndex("test", "type1", "1").setSource(o1));
+
+        String filter = "{\"geo_shape\": {\"location2\": {\"indexed_shape\": {"
+                + "\"id\": \"1\","
+                + "\"type\": \"type1\","
+                + "\"index\": \"test\","
+                + "\"path\": \"location2\""
+                + "}}}}";
+
+        SearchResponse result = client().prepareSearch("test").setQuery(QueryBuilders.matchAllQuery()).setPostFilter(filter).execute().actionGet();
+        assertSearchResponse(result);
+        assertHitCount(result, 1);
+    }
+
+    @Test
     public void testShapeFetchingPath() throws Exception {
         createIndex("shapes");
         assertAcked(prepareCreate("test").addMapping("type", "location", "type=geo_shape"));
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
index 5f5ecfc..7c1f163 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchIT.java
@@ -60,7 +60,7 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
     public void testThatCustomHighlightersAreSupported() throws IOException {
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field("name").highlighterType("test-custom"))
+                .addHighlightedField("name").setHighlighterType("test-custom")
                 .execute().actionGet();
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
     }
@@ -75,7 +75,7 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field(highlightConfig))
+                .addHighlightedField(highlightConfig)
                 .execute().actionGet();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
@@ -87,8 +87,11 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
         Map<String, Object> options = new HashMap<>();
         options.put("myGlobalOption", "someValue");
 
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("test").setQuery(QueryBuilders.matchAllQuery())
-                .highlighter(new HighlightBuilder().field("name").highlighterType("test-custom").options(options))
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
+                .setQuery(QueryBuilders.matchAllQuery())
+                .setHighlighterOptions(options)
+                .setHighlighterType("test-custom")
+                .addHighlightedField("name")
                 .execute().actionGet();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
@@ -100,9 +103,11 @@ public class CustomHighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("test").setTypes("test")
                 .setQuery(QueryBuilders.boolQuery().must(QueryBuilders.matchAllQuery()).should(QueryBuilders
                         .termQuery("name", "arbitrary")))
-                .highlighter(
-                        new HighlightBuilder().highlighterType("test-custom").field("name").field("other_name").field("other_other_name")
-                                .useExplicitFieldOrder(true))
+                .setHighlighterType("test-custom")
+                .addHighlightedField("name")
+                .addHighlightedField("other_name")
+                .addHighlightedField("other_other_name")
+                .setHighlighterExplicitFieldOrder(true)
                 .get();
 
         assertHighlight(searchResponse, 0, "name", 0, equalTo("standard response for name at position 1"));
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 93fd7eb..4134c4f 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -19,22 +19,20 @@
 package org.elasticsearch.search.highlight;
 
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
-
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings.Builder;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
+import org.elasticsearch.index.query.*;
 import org.elasticsearch.index.query.IdsQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder;
+import org.elasticsearch.index.search.MatchQuery.Type;
+import org.elasticsearch.index.search.MatchQuery;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.search.MatchQuery;
-import org.elasticsearch.index.search.MatchQuery.Type;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.builder.SearchSourceBuilder;
@@ -51,38 +49,12 @@ import java.util.Map;
 import static org.elasticsearch.client.Requests.searchRequest;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.boostingQuery;
-import static org.elasticsearch.index.query.QueryBuilders.commonTermsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.fuzzyQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchPhrasePrefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchPhraseQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.regexpQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.highlight;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHighlight;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNotHighlighted;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.elasticsearch.test.hamcrest.RegexMatcher.matches;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.hasKey;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.startsWith;
+import static org.hamcrest.Matchers.*;
 
 public class HighlighterSearchIT extends ESIntegTestCase {
 
@@ -110,8 +82,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .get();
         refresh();
         String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
     }
 
@@ -150,17 +121,14 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("long_text", builder.toString()).field("text", "text").endObject())
                 .get();
         refresh();
-        String highlighter = randomFrom(new String[] { "plain", "postings", "fvh" });
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("long_text").highlighterType(highlighter))).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("long_text").highlighterType(highlighter)).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
 
-        search = client().prepareSearch().setQuery(prefixQuery("text", "te"))
-                .highlighter(new HighlightBuilder().field(new Field("long_text").highlighterType(highlighter))).get();
+        search = client().prepareSearch().setQuery(prefixQuery("text", "te")).addHighlightedField(new Field("long_text").highlighterType(highlighter)).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
     }
@@ -196,12 +164,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("unstored_text", "text").field("text", "text").endObject())
                 .get();
         refresh();
-        String highlighter = randomFrom(new String[] { "plain", "postings", "fvh" });
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("*").highlighterType(highlighter))).get();
+        String highlighter = randomFrom(new String[]{"plain", "postings", "fvh"});
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("*").highlighterType(highlighter)).get();
         assertHighlight(search, 0, "text", 0, equalTo("<em>text</em>"));
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text")))
-                .highlighter(new HighlightBuilder().field(new Field("unstored_text"))).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("text", "text"))).addHighlightedField(new Field("unstored_text")).get();
         assertNoFailures(search);
         assertThat(search.getHits().getAt(0).getHighlightFields().size(), equalTo(0));
     }
@@ -221,8 +187,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             .setSource("name", builder.toString())
             .get();
         refresh();
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "abc")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "abc"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, startsWith("<em>abc</em> <em>abc</em> <em>abc</em> <em>abc</em>"));
     }
 
@@ -278,9 +243,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         client().prepareIndex("test", "test", "1")
             .setSource("name", "ARCOTEL Hotels Deutschland").get();
         refresh();
-        SearchResponse search = client().prepareSearch("test").setTypes("test")
-                .setQuery(matchQuery("name.autocomplete", "deut tel").operator(Operator.OR))
-                .highlighter(new HighlightBuilder().field("name.autocomplete")).execute().actionGet();
+        SearchResponse search = client().prepareSearch("test").setTypes("test").setQuery(matchQuery("name.autocomplete", "deut tel").operator(Operator.OR)).addHighlightedField("name.autocomplete").execute().actionGet();
         assertHighlight(search, 0, "name.autocomplete", 0, equalTo("ARCO<em>TEL</em> Ho<em>tel</em>s <em>Deut</em>schland"));
     }
 
@@ -310,22 +273,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             .setSource("body", "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature")
             .get();
         refresh();
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com ").type(Type.PHRASE))
-                .highlighter(new HighlightBuilder().field("body")).execute().actionGet();
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com ").type(Type.PHRASE)).addHighlightedField("body").execute().actionGet();
         assertHighlight(search, 0, "body", 0, startsWith("<em>Test: http://www.facebook.com</em>"));
-        search = client()
-                .prepareSearch()
-                .setQuery(
-                        matchQuery(
-                                "body",
-                                "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature")
-                                .type(Type.PHRASE)).highlighter(new HighlightBuilder().field("body")).execute().actionGet();
-        assertHighlight(
-                search,
-                0,
-                "body",
-                0,
-                equalTo("<em>Test</em>: <em>http://www.facebook.com</em> <em>http://elasticsearch.org</em> <em>http://xing.com</em> <em>http://cnn.com</em> http://quora.com"));
+        search = client().prepareSearch().setQuery(matchQuery("body", "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature").type(Type.PHRASE)).addHighlightedField("body").execute().actionGet();
+        assertHighlight(search, 0, "body", 0, equalTo("<em>Test</em>: <em>http://www.facebook.com</em> <em>http://elasticsearch.org</em> <em>http://xing.com</em> <em>http://cnn.com</em> http://quora.com"));
     }
 
     @Test
@@ -359,43 +310,37 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                     "name2", "avinci, unilog avinci, logicacmg, logica").get();
         refresh();
 
-        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica m")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica m"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
         assertHighlight(search, 1, "name", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica ma")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica ma"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica")))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name", "logica"))).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 0, "name", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica m")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica m"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"),
                 equalTo("avinci, unilog avinci, <em>logica</em>c<em>m</em>g, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica ma")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica ma"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
 
-        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica")))
-                .highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(constantScoreQuery(matchQuery("name2", "logica"))).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
                 equalTo("avinci, unilog avinci, <em>logica</em>cmg, <em>logica</em>")));
         assertHighlight(search, 1, "name2", 0, anyOf(equalTo("<em>logica</em>cmg ehemals avinci - the know how company"),
@@ -426,25 +371,22 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                        "name2", "logicacmg ehemals avinci - the know how company").get();
         refresh();
         ensureGreen();
-        SearchResponse search = client().prepareSearch().setQuery(matchQuery("name", "logica m"))
-                .highlighter(new HighlightBuilder().field("name")).get();
+        SearchResponse search = client().prepareSearch().setQuery(matchQuery("name", "logica m")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>c<em>m</em>g ehe<em>m</em>als avinci - the know how co<em>m</em>pany"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name", "logica ma")).highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name", "logica ma")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>cmg ehe<em>ma</em>ls avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name", "logica")).highlighter(new HighlightBuilder().field("name")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name", "logica")).addHighlightedField("name").get();
         assertHighlight(search, 0, "name", 0, equalTo("<em>logica</em>cmg ehemals avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica m")).highlighter(new HighlightBuilder().field("name2"))
-                .get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica m")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> <em>ehemals</em> avinci - the know how <em>company</em>"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica ma")).highlighter(new HighlightBuilder().field("name2"))
-                .get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica ma")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> <em>ehemals</em> avinci - the know how company"));
 
-        search = client().prepareSearch().setQuery(matchQuery("name2", "logica")).highlighter(new HighlightBuilder().field("name2")).get();
+        search = client().prepareSearch().setQuery(matchQuery("name2", "logica")).addHighlightedField("name2").get();
         assertHighlight(search, 0, "name2", 0, equalTo("<em>logicacmg</em> ehemals avinci - the know how company"));
     }
 
@@ -464,19 +406,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("long_term", "thisisaverylongwordandmakessurethisfails foo highlighed"))
-                .highlighter(new HighlightBuilder().field("long_term", 18, 1))
+                .addHighlightedField("long_term", 18, 1)
                 .get();
         assertHighlight(search, 0, "long_term", 0, 1, equalTo("<em>thisisaverylongwordandmakessurethisfails</em>"));
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("no_long_term", "test foo highlighed").type(Type.PHRASE).slop(3))
-                .highlighter(new HighlightBuilder().field("no_long_term", 18, 1).postTags("</b>").preTags("<b>"))
+                .addHighlightedField("no_long_term", 18, 1).setHighlighterPostTags("</b>").setHighlighterPreTags("<b>")
                 .get();
         assertNotHighlighted(search, 0, "no_long_term");
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("no_long_term", "test foo highlighed").type(Type.PHRASE).slop(3))
-                .highlighter(new HighlightBuilder().field("no_long_term", 30, 1).postTags("</b>").preTags("<b>"))
+                .addHighlightedField("no_long_term", 30, 1).setHighlighterPostTags("</b>").setHighlighterPreTags("<b>")
                 .get();
 
         assertHighlight(search, 0, "no_long_term", 0, 1, equalTo("a <b>test</b> where <b>foo</b> is <b>highlighed</b> and"));
@@ -504,7 +446,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -513,7 +455,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 0))
+                .addHighlightedField("attachments.body", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -544,7 +486,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -553,7 +495,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 2))
+                .addHighlightedField("attachments.body", -1, 2)
                 .execute().get();
 
         for (int i = 0; i < 5; i++) {
@@ -586,7 +528,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
                 //asking for the whole field to be highlighted
-                .highlighter(new HighlightBuilder().field("title", -1, 0)).get();
+                .addHighlightedField("title", -1, 0).get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(search, i, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch. Hopefully it works."));
@@ -596,7 +538,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
                 //sentences will be generated out of each value
-                .highlighter(new HighlightBuilder().field("title")).get();
+                .addHighlightedField("title").get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(search, i, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch."));
@@ -605,7 +547,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("attachments.body", "attachment"))
-                .highlighter(new HighlightBuilder().field("attachments.body", -1, 2))
+                .addHighlightedField("attachments.body", -1, 2)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -629,7 +571,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 2).field("titleTV", -1, 2).requireFieldMatch(false))
+                .addHighlightedField("title", -1, 2)
+                .addHighlightedField("titleTV", -1, 2).setHighlighterRequireFieldMatch(false)
                 .get();
 
         assertHighlight(search, 0, "title", 0, equalTo("This is a test on the highlighting <em>bug</em> present in elasticsearch"));
@@ -639,7 +582,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         search = client().prepareSearch()
                 .setQuery(matchQuery("titleTV", "highlight"))
-                .highlighter(new HighlightBuilder().field("titleTV", -1, 2))
+                .addHighlightedField("titleTV", -1, 2)
                 .get();
 
         assertHighlight(search, 0, "titleTV", 0, equalTo("some text to <em>highlight</em>"));
@@ -659,7 +602,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1 and field2 produces different tags");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().order("score").preTags("<global>").postTags("</global>").fragmentSize(1).numOfFragments(1)
+                .highlight(highlight().order("score").preTags("<global>").postTags("</global>").fragmentSize(1).numOfFragments(1)
                         .field(new HighlightBuilder.Field("field1").numOfFragments(2))
                         .field(new HighlightBuilder.Field("field2").preTags("<field2>").postTags("</field2>").fragmentSize(50).requireFieldMatch(false)));
 
@@ -689,7 +632,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchSourceBuilder source = searchSource()
                 //postings hl doesn't support require_field_match, its field needs to be queried directly
                 .query(termQuery("field-postings", "test"))
-                .highlighter(highlight().field("field*").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field*").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -718,42 +661,36 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         //works using stored field
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(termQuery("field1", "quick"))
-                .highlighter(new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")))
+                .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>"))
                 .get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")
-                                        .highlighterType("plain").forceSource(true))),
+                        .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("plain").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("fvh")
-                                        .forceSource(true))),
+                        .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("fvh").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         assertFailures(client().prepareSearch("test")
                 .setQuery(termQuery("field1", "quick"))
-                        .highlighter(
-                                new HighlightBuilder().field(new Field("field1").preTags("<xxx>").postTags("</xxx>")
-                                        .highlighterType("postings").forceSource(true))),
+                .addHighlightedField(new Field("field1").preTags("<xxx>").postTags("</xxx>").highlighterType("postings").forceSource(true)),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         SearchSourceBuilder searchSource = SearchSourceBuilder.searchSource().query(termQuery("field1", "quick"))
-                .highlighter(highlight().forceSource(true).field("field1"));
+                .highlight(highlight().forceSource(true).field("field1"));
         assertFailures(client().prepareSearch("test").setSource(searchSource),
                 RestStatus.BAD_REQUEST,
                 containsString("source is forced for fields [field1] but type [type1] has disabled _source"));
 
         searchSource = SearchSourceBuilder.searchSource().query(termQuery("field1", "quick"))
-                .highlighter(highlight().forceSource(true).field("field*"));
+                .highlight(highlight().forceSource(true).field("field*"));
         assertFailures(client().prepareSearch("test").setSource(searchSource),
                 RestStatus.BAD_REQUEST,
                 matches("source is forced for fields \\[field\\d, field\\d\\] but type \\[type1\\] has disabled _source"));
@@ -771,7 +708,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -780,7 +717,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field1");
         source = searchSource()
                 .query(termQuery("_all", "test"))
-                .highlighter(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field1").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -789,7 +726,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(termQuery("_all", "quick"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -798,7 +735,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(prefixQuery("_all", "qui"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -807,7 +744,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all with constant score, highlighting on field2");
         source = searchSource()
                 .query(constantScoreQuery(prefixQuery("_all", "qui")))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -816,7 +753,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all with constant score, highlighting on field2");
         source = searchSource()
                 .query(boolQuery().should(constantScoreQuery(prefixQuery("_all", "qui"))))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <xxx>quick</xxx> brown fox jumps over the lazy dog"));
@@ -834,7 +771,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -843,7 +780,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field1");
         source = searchSource()
                 .query(termQuery("_all", "test"))
-                .highlighter(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field1", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -853,7 +790,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(termQuery("_all", "quick"))
-                .highlighter(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -863,7 +800,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on _all, highlighting on field2");
         source = searchSource()
                 .query(prefixQuery("_all", "qui"))
-                .highlighter(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
+                .highlight(highlight().field("field2", 100, 0).order("score").preTags("<xxx>").postTags("</xxx>").requireFieldMatch(false));
 
         searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -889,7 +826,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "t"))
-                .highlighter(highlight().highlighterType("fvh").field("field1", 20, 1).order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().highlighterType("fvh").field("field1", 20, 1).order("score").preTags("<xxx>").postTags("</xxx>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
         assertHighlight(searchResponse, 0, "field1", 0, 1, containsString("<xxx>t</xxx>"));
         logger.info("--> done");
@@ -957,7 +894,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         Field fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25)
                 .highlighterType("fvh").requireFieldMatch(requireFieldMatch);
-        SearchRequestBuilder req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
+        Field barField = new Field("bar").numOfFragments(1).order("score").fragmentSize(25)
+                .highlighterType("fvh").requireFieldMatch(requireFieldMatch);
+        SearchRequestBuilder req = client().prepareSearch("test").addHighlightedField(fooField);
 
         // First check highlighting without any matched fields set
         SearchResponse resp = req.setQuery(queryStringQuery("running scissors").field("foo")).get();
@@ -969,31 +908,21 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         // Add the subfield to the list of matched fields but don't match it.  Everything should still work
         // like before we added it.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo", "foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
-
         // Now make half the matches come from the stored field and half from just a matched field.
         resp = req.setQuery(queryStringQuery("foo.plain:running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
         // Now remove the stored field from the matched field list.  That should work too.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("foo.plain:running scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with scissors"));
 
         // Now make sure boosted fields don't blow up when matched fields is both the subfield and stored field.
-        fooField = new Field("foo").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
         fooField.matchedFields("foo", "foo.plain");
-        req = client().prepareSearch("test").highlighter(new HighlightBuilder().field(fooField));
         resp = req.setQuery(queryStringQuery("foo.plain:running^5 scissors").field("foo")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
 
@@ -1020,46 +949,41 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Speaking of two fields, you can have two fields, only one of which has matchedFields enabled
         QueryBuilder twoFieldsQuery = queryStringQuery("cats").field("foo").field("foo.plain", 5)
                 .field("bar").field("bar.plain", 5);
-        Field barField = new Field("bar").numOfFragments(1).order("score").fragmentSize(25).highlighterType("fvh")
-                .requireFieldMatch(requireFieldMatch);
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(twoFieldsQuery).addHighlightedField(barField).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>cat</em> <em>cat</em> junk junk junk junk"));
+
         // And you can enable matchedField highlighting on both
         barField.matchedFields("bar", "bar.plain");
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("junk junk <em>cats</em> junk junk"));
 
         // Setting a matchedField that isn't searched/doesn't exist is simply ignored.
         barField.matchedFields("bar", "candy");
-        resp = req.setQuery(twoFieldsQuery).highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.get();
         assertHighlight(resp, 0, "foo", 0, equalTo("junk junk <em>cats</em> junk junk"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>cat</em> <em>cat</em> junk junk junk junk"));
 
         // If the stored field doesn't have a value it doesn't matter what you match, you get nothing.
         barField.matchedFields("bar", "foo.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo.plain").field("bar"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo.plain").field("bar")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("bar")));
 
         // If the stored field is found but the matched field isn't then you don't get a result either.
         fooField.matchedFields("bar.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("foo")));
 
         // But if you add the stored field to the list of matched fields then you'll get a result again
         fooField.matchedFields("foo", "bar.plain");
-        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("running scissors").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>running</em> with <em>scissors</em>"));
         assertThat(resp.getHits().getAt(0).getHighlightFields(), not(hasKey("bar")));
 
         // You _can_ highlight fields that aren't subfields of one another.
-        resp = req.setQuery(queryStringQuery("weird").field("foo").field("foo.plain").field("bar").field("bar.plain"))
-                .highlighter(new HighlightBuilder().field(fooField).field(barField)).get();
+        resp = req.setQuery(queryStringQuery("weird").field("foo").field("foo.plain").field("bar").field("bar.plain")).get();
         assertHighlight(resp, 0, "foo", 0, equalTo("<em>weird</em>"));
         assertHighlight(resp, 0, "bar", 0, equalTo("<em>resul</em>t"));
 
@@ -1084,7 +1008,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("field1", "test"))
-                .highlighter(new HighlightBuilder().field("field1", 100, 0))
+                .addHighlightedField("field1", 100, 0)
                 .get();
         for (int i = 0; i < COUNT; i++) {
             SearchHit hit = searchResponse.getHits().getHits()[i];
@@ -1096,7 +1020,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("_all", "test"))
-                .highlighter(new HighlightBuilder().field("_all", 100, 0))
+                .addHighlightedField("_all", 100, 0)
                 .get();
         for (int i = 0; i < COUNT; i++) {
             SearchHit hit = searchResponse.getHits().getHits()[i];
@@ -1129,7 +1053,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", -1, 0))
+                .addHighlightedField("title", -1, 0)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1152,7 +1076,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "bug"))
-                .highlighter(new HighlightBuilder().field("title", 30, 1, 10))
+                .addHighlightedField("title", 30, 1, 10)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1176,7 +1100,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1, 10))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1199,7 +1124,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 30, 1, 10))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 30, 1, 10)
                 .get();
 
         for (int i = 0; i < 5; i++) {
@@ -1222,7 +1148,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1230,7 +1157,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1253,7 +1181,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1261,7 +1190,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1284,7 +1214,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1292,7 +1223,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1314,7 +1246,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -1322,7 +1255,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         search = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().encoder("html").field("title.key", 50, 1))
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title.key", 50, 1)
                 .get();
 
         assertHighlight(search, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
@@ -1343,20 +1277,22 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10))
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
         assertNoFailures(search);
 
         assertFailures(client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title", 50, 1, 10).highlighterType("fast-vector-highlighter")),
+                .addHighlightedField("title", 50, 1, 10)
+                .setHighlighterType("fast-vector-highlighter"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with term vector with position offsets to be used with fast vector highlighter"));
 
         //should not fail if there is a wildcard
         assertNoFailures(client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("tit*", 50, 1, 10).highlighterType("fast-vector-highlighter")).get());
+                .addHighlightedField("tit*", 50, 1, 10)
+                .setHighlighterType("fast-vector-highlighter").get());
     }
 
     @Test
@@ -1374,7 +1310,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10))
+                .addHighlightedField("title", 50, 1, 10)
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1385,7 +1321,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Using plain highlighter instead of FVH
         search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(new HighlightBuilder().field("title", 50, 1, 10).highlighterType("highlighter"))
+                .addHighlightedField("title", 50, 1, 10)
+                .setHighlighterType("highlighter")
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1395,9 +1332,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // Using plain highlighter instead of FVH on the field level
         search = client().prepareSearch()
                 .setQuery(matchPhraseQuery("title", "test for the workaround"))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("title").highlighterType("highlighter")).highlighterType(
-                                "highlighter"))
+                .addHighlightedField(new HighlightBuilder.Field("title").highlighterType("highlighter"))
+                .setHighlighterType("highlighter")
                 .get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
@@ -1418,7 +1354,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "tag"))
-                .highlighter(new HighlightBuilder().field("tags", -1, 0)).get();
+                .addHighlightedField("tags", -1, 0).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really long <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very long and has the <em>tag</em> token near the end"));
@@ -1435,7 +1371,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1454,7 +1390,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1474,7 +1410,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <x>quick</x> <x>brown</x> fox jumps over the lazy dog"));
@@ -1489,7 +1425,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
@@ -1519,7 +1455,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field0");
         SearchSourceBuilder source = searchSource()
                 .query(matchPhrasePrefixQuery("field0", "quick bro"))
-                .highlighter(highlight().field("field0").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field0").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1528,7 +1464,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         source = searchSource()
                 .query(matchPhrasePrefixQuery("field1", "quick bro"))
-                .highlighter(highlight().field("field1").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field1").order("score").preTags("<x>").postTags("</x>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1545,7 +1481,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
 
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field3", "fast bro"))
-                .highlighter(highlight().field("field3").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field3").order("score").preTags("<x>").postTags("</x>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -1553,7 +1489,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field4");
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field4", "the fast bro"))
-                .highlighter(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field4", 0, 1, anyOf(equalTo("<x>The quick browse</x> button is a fancy thing, right bro?"), equalTo("<x>The quick brown</x> fox jumps over the lazy dog")));
@@ -1561,7 +1497,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field4");
         source = searchSource().postFilter(typeQuery("type2")).query(matchPhrasePrefixQuery("field4", "a fast quick blue ca"))
-                .highlighter(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field4").order("score").preTags("<x>").postTags("</x>"));
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field4", 0, 1, equalTo("<x>a quick fast blue car</x>"));
@@ -1580,27 +1516,24 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                .fragmenter("simple"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("tags")
+                        .fragmentSize(-1).numOfFragments(2).fragmenter("simple")).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really <em>long</em> <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very <em>long</em> <em>tag</em> and has the <em>tag</em> token near the end"));
 
         response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                .fragmenter("span"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("tags")
+                        .fragmentSize(-1).numOfFragments(2).fragmenter("span")).get();
 
         assertHighlight(response, 0, "tags", 0, equalTo("this is a really <em>long</em> <em>tag</em> i would like to highlight"));
         assertHighlight(response, 0, "tags", 1, 2, equalTo("here is another one that is very <em>long</em> <em>tag</em> and has the <em>tag</em> token near the end"));
 
         assertFailures(client().prepareSearch("test")
                         .setQuery(QueryBuilders.matchQuery("tags", "long tag").type(MatchQuery.Type.PHRASE))
-                        .highlighter(
-                                new HighlightBuilder().field(new HighlightBuilder.Field("tags").fragmentSize(-1).numOfFragments(2)
-                                        .fragmenter("invalid"))),
+                        .addHighlightedField(new HighlightBuilder.Field("tags")
+                                .fragmentSize(-1).numOfFragments(2).fragmenter("invalid")),
                 RestStatus.BAD_REQUEST,
                 containsString("unknown fragmenter option [invalid] for the field [tags]"));
     }
@@ -1615,10 +1548,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(
-                                new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true)).field(
-                                new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
+                .addHighlightedField(new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>"));
         assertHighlight(response, 0, "field2", 0, 1, equalTo("The <b>slow<b> brown <2>fox</2>"));
@@ -1635,10 +1566,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(
-                                new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true)).field(
-                                new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
+                .addHighlightedField(new HighlightBuilder.Field("field2").preTags("<2>").postTags("</2>").requireFieldMatch(false))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>"));
         assertHighlight(response, 0, "field2", 0, 1, equalTo("The <b>slow<b> brown <2>fox</2>"));
@@ -1658,9 +1587,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // This query used to fail when the field to highlight was absent
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field", "highlight").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("highlight_field").fragmentSize(-1).numOfFragments(1)
-                                .fragmenter("simple"))).get();
+                .addHighlightedField(new HighlightBuilder.Field("highlight_field")
+                        .fragmentSize(-1).numOfFragments(1).fragmenter("simple")).get();
         assertThat(response.getHits().hits()[0].highlightFields().isEmpty(), equalTo(true));
     }
 
@@ -1679,9 +1607,13 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(
-                        new HighlightBuilder().field("text").field("byte").field("short").field("int").field("long").field("float")
-                                .field("double"))
+                .addHighlightedField("text")
+                .addHighlightedField("byte")
+                .addHighlightedField("short")
+                .addHighlightedField("int")
+                .addHighlightedField("long")
+                .addHighlightedField("float")
+                .addHighlightedField("double")
                 .get();
         // Highlighting of numeric fields is not supported, but it should not raise errors
         // (this behavior is consistent with version 0.20)
@@ -1705,7 +1637,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("text", "test").type(MatchQuery.Type.BOOLEAN))
-                .highlighter(new HighlightBuilder().field("text")).execute().actionGet();
+                .addHighlightedField("text").execute().actionGet();
         // PatternAnalyzer will throw an exception if it is resetted twice
         assertHitCount(response, 1l);
     }
@@ -1721,9 +1653,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         HighlightBuilder.Field field = new HighlightBuilder.Field("text");
 
-        HighlightBuilder highlightBuilder = new HighlightBuilder().field(field);
         SearchRequestBuilder search = client().prepareSearch("test").setQuery(QueryBuilders.matchQuery("text", "testing"))
-                .highlighter(highlightBuilder);
+                .addHighlightedField(field);
         Matcher<String> searchQueryMatcher = equalTo("<em>Testing</em> the highlight query feature");
 
         field.highlighterType("plain");
@@ -1736,12 +1667,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         response = search.get();
         assertHighlight(response, 0, "text", 0, searchQueryMatcher);
 
-        field = new HighlightBuilder.Field("text");
 
         Matcher<String> hlQueryMatcher = equalTo("Testing the highlight <em>query</em> feature");
         field.highlightQuery(matchQuery("text", "query"));
-        highlightBuilder = new HighlightBuilder().field(field);
-        search = client().prepareSearch("test").setQuery(QueryBuilders.matchQuery("text", "testing")).highlighter(highlightBuilder);
 
         field.highlighterType("fvh");
         response = search.get();
@@ -1756,7 +1684,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "text", 0, hlQueryMatcher);
 
         // Make sure the the highlightQuery is taken into account when it is set on the highlight context instead of the field
-        highlightBuilder.highlightQuery(matchQuery("text", "query"));
+        search.setHighlighterQuery(matchQuery("text", "query"));
         field.highlighterType("fvh").highlightQuery(null);
         response = search.get();
         assertHighlight(response, 0, "text", 0, hlQueryMatcher);
@@ -1792,97 +1720,97 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .fragmentSize(21)
                 .numOfFragments(1)
                 .highlighterType("plain");
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // When noMatchSize is set to 0 you also shouldn't get any
         field.highlighterType("plain").noMatchSize(0);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // When noMatchSize is between 0 and the size of the string
         field.highlighterType("plain").noMatchSize(21);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         // The FVH also works but the fragment is longer than the plain highlighter because of boundary_max_scan
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We can also ask for a fragment longer than the input string and get the whole string
         field.highlighterType("plain").noMatchSize(text.length() * 2);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         //no difference using postings hl as the noMatchSize is ignored (just needs to be greater than 0)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We can also ask for a fragment exactly the size of the input field and get the whole field
         field.highlighterType("plain").noMatchSize(text.length());
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo(text));
 
         //no difference using postings hl as the noMatchSize is ignored (just needs to be greater than 0)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // You can set noMatchSize globally in the highlighter as well
         field.highlighterType("plain").noMatchSize(null);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field).noMatchSize(21)).get();
+        response = client().prepareSearch("test").setHighlighterNoMatchSize(21).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // We don't break if noMatchSize is less than zero though
         field.highlighterType("plain").noMatchSize(randomIntBetween(Integer.MIN_VALUE, -1));
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
     }
 
@@ -1903,16 +1831,16 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .numOfFragments(1)
                 .highlighterType("plain")
                 .noMatchSize(21);
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("I am pretty long so some of me should get cut off."));
 
         // And noMatchSize returns nothing when the first entry is empty string!
@@ -1923,19 +1851,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // But if the field was actually empty then you should get no highlighting field
@@ -1945,19 +1873,19 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         // Same for if the field doesn't even exist on the document
@@ -1968,34 +1896,34 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         field.highlighterType("plain");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
         response = client().prepareSearch("test")
                 .setQuery(idsQueryBuilder)
-.highlighter(new HighlightBuilder().field(field)).get();
+                .addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "postings");
 
         // Again same if the field isn't mapped
         field = new HighlightBuilder.Field("unmapped")
                 .highlighterType("plain")
                 .noMatchSize(21);
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertNotHighlighted(response, 0, "text");
     }
 
@@ -2017,32 +1945,32 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .numOfFragments(0)
                 .highlighterType("plain")
                 .noMatchSize(20);
-        SearchResponse response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        SearchResponse response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first sentence"));
 
         // Postings hl also works but the fragment is the whole first sentence (size ignored)
         field.highlighterType("postings");
-        response = client().prepareSearch("test").highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 1, equalTo("This is the first sentence."));
 
         //if there's a match we only return the values with matches (whole value as number_of_fragments == 0)
         MatchQueryBuilder queryBuilder = QueryBuilders.matchQuery("text", "third fifth");
         field.highlighterType("plain");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
 
         field.highlighterType("fvh");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
 
         field.highlighterType("postings");
-        response = client().prepareSearch("test").setQuery(queryBuilder).highlighter(new HighlightBuilder().field(field)).get();
+        response = client().prepareSearch("test").setQuery(queryBuilder).addHighlightedField(field).get();
         assertHighlight(response, 0, "text", 0, 2, equalTo("This is the <em>third</em> sentence. This is the fourth sentence."));
         assertHighlight(response, 0, "text", 1, 2, equalTo("This is the <em>fifth</em> sentence"));
     }
@@ -2059,7 +1987,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("this is a <xxx>test</xxx>"));
@@ -2067,7 +1995,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field1, highlighting on field1");
         source = searchSource()
                 .query(termQuery("field1", "test"))
-                .highlighter(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field1").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2076,7 +2004,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2");
         source = searchSource()
                 .query(termQuery("field2", "quick"))
-                .highlighter(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field2").order("score").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2085,7 +2013,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2");
         source = searchSource()
                 .query(matchPhraseQuery("field2", "quick brown"))
-                .highlighter(highlight().field("field2").preTags("<xxx>").postTags("</xxx>"));
+                .highlight(highlight().field("field2").preTags("<xxx>").postTags("</xxx>"));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2096,7 +2024,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> searching on field2, highlighting on field2, falling back to the plain highlighter");
         source = searchSource()
                 .query(matchPhraseQuery("_all", "quick brown"))
-                .highlighter(highlight().field("field2").preTags("<xxx>").postTags("</xxx>").highlighterType("highlighter").requireFieldMatch(false));
+                .highlight(highlight().field("field2").preTags("<xxx>").postTags("</xxx>").highlighterType("highlighter").requireFieldMatch(false));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2113,9 +2041,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse response = client().prepareSearch("test")
                 .setQuery(QueryBuilders.matchQuery("field1", "fox"))
-                .highlighter(
-                        new HighlightBuilder().field(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>")
-                                .requireFieldMatch(true)))
+                .addHighlightedField(new HighlightBuilder.Field("field1").preTags("<1>").postTags("</1>").requireFieldMatch(true))
                 .get();
         assertHighlight(response, 0, "field1", 0, 1, equalTo("The <b>quick<b> brown <1>fox</1>."));
     }
@@ -2133,7 +2059,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "fox"))
-                .highlighter(highlight()
+                .highlight(highlight()
                         .field(new HighlightBuilder.Field("field1").numOfFragments(5).preTags("<field1>").postTags("</field1>")));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2148,7 +2074,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         source = searchSource()
                 .query(termQuery("field1", "fox"))
-                .highlighter(highlight()
+                .highlight(highlight()
                         .field(new HighlightBuilder.Field("field1").numOfFragments(0).preTags("<field1>").postTags("</field1>")));
 
         searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2198,7 +2124,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
             SearchSourceBuilder source = searchSource()
                     .query(multiMatchQueryBuilder)
-                    .highlighter(highlight().highlightQuery(randomBoolean() ? multiMatchQueryBuilder : null).highlighterType(highlighterType)
+                    .highlight(highlight().highlightQuery(randomBoolean() ? multiMatchQueryBuilder : null).highlighterType(highlighterType)
                             .field(new Field("field1").requireFieldMatch(true).preTags("<field1>").postTags("</field1>")));
             logger.info("Running multi-match type: [" + matchQueryType + "] highlight with type: [" + highlighterType + "]");
             SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
@@ -2222,7 +2148,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(termQuery("field1", "sentence"))
-                .highlighter(highlight().field("field1").order("score"));
+                .highlight(highlight().field("field1").order("score"));
 
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2252,7 +2178,8 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title", "test"))
-                .highlighter(new HighlightBuilder().field("title").encoder("html")).get();
+                .setHighlighterEncoder("html")
+                .addHighlightedField("title").get();
 
         for (int i = 0; i < indexRequestBuilders.length; i++) {
             assertHighlight(searchResponse, i, "title", 0, 1, equalTo("This is a html escaping highlighting <em>test</em> for *&amp;?"));
@@ -2276,7 +2203,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch()
                 //lets make sure we analyze the query and we highlight the resulting terms
                 .setQuery(matchQuery("title", "This is a Test"))
-.highlighter(new HighlightBuilder().field("title")).get();
+                .addHighlightedField("title").get();
 
         assertHitCount(searchResponse, 1l);
         SearchHit hit = searchResponse.getHits().getAt(0);
@@ -2286,7 +2213,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title
         searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title.key")).get();
+                .addHighlightedField("title.key").get();
         assertHitCount(searchResponse, 1l);
 
         //stopwords are now highlighted since we used only whitespace analyzer here
@@ -2310,7 +2237,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // simple search on body with standard analyzer with a simple field query
         SearchResponse searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title"))
+                .addHighlightedField("title")
                 .get();
 
         assertHighlight(searchResponse, 0, "title", 0, 1, equalTo("this is a <em>test</em>"));
@@ -2318,7 +2245,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         // search on title.key and highlight on title.key
         searchResponse = client().prepareSearch()
                 .setQuery(matchQuery("title.key", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title.key")).get();
+                .addHighlightedField("title.key").get();
 
         assertHighlight(searchResponse, 0, "title.key", 0, 1, equalTo("<em>this</em> <em>is</em> <em>a</em> <em>test</em>"));
     }
@@ -2340,27 +2267,30 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         SearchResponse search = client().prepareSearch()
                 .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("title"))
+                .addHighlightedField("title")
                 .get();
         assertNoFailures(search);
 
         assertFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title").highlighterType("postings-highlighter")),
+                        .addHighlightedField("title")
+                        .setHighlighterType("postings-highlighter"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with positions and offsets in the postings list to be used with postings highlighter"));
 
 
         assertFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                        .highlighter(new HighlightBuilder().field("title").highlighterType("postings")),
+                        .addHighlightedField("title")
+                        .setHighlighterType("postings"),
                 RestStatus.BAD_REQUEST,
                 containsString("the field [title] should be indexed with positions and offsets in the postings list to be used with postings highlighter"));
 
         //should not fail if there is a wildcard
         assertNoFailures(client().prepareSearch()
                         .setQuery(matchQuery("title", "this is a test"))
-                .highlighter(new HighlightBuilder().field("tit*").highlighterType("postings")).get());
+                        .addHighlightedField("tit*")
+                        .setHighlighterType("postings").get());
     }
 
     @Test
@@ -2374,7 +2304,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
                 .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
-                .highlighter(highlight().field("field2").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The quick <x>brown</x> fox jumps over the lazy dog!"));
@@ -2389,7 +2319,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(commonTermsQuery("field2", "quick brown").cutoffFrequency(100))
-                .highlighter(highlight().field("field2").preTags("<x>").postTags("</x>"));
+                .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
         assertHitCount(searchResponse, 1l);
 
@@ -2415,7 +2345,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         logger.info("--> highlighting and searching on field2");
 
         SearchSourceBuilder source = searchSource().query(prefixQuery("field2", "qui"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
 
@@ -2430,7 +2360,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(fuzzyQuery("field2", "quck"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
@@ -2445,7 +2375,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(regexpQuery("field2", "qu[a-l]+k"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
@@ -2460,13 +2390,13 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(wildcardQuery("field2", "qui*"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
 
         source = searchSource().query(wildcardQuery("field2", "qu*k"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHitCount(searchResponse, 1l);
 
@@ -2482,7 +2412,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(rangeQuery("field2").gte("aaaa").lt("zzzz"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
 
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("<em>aaab</em>"));
@@ -2497,7 +2427,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         refresh();
         logger.info("--> highlighting and searching on field2");
         SearchSourceBuilder source = searchSource().query(queryStringQuery("qui*").defaultField("field2"))
-                .highlighter(highlight().field("field2"));
+                .highlight(highlight().field("field2"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field2", 0, 1, equalTo("The <em>quick</em> brown fox jumps over the lazy dog!"));
     }
@@ -2513,7 +2443,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(constantScoreQuery(regexpQuery("field1", "pho[a-z]+")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2532,7 +2462,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 .should(constantScoreQuery(QueryBuilders.missingQuery("field1")))
                 .should(matchQuery("field1", "test"))
                 .should(constantScoreQuery(queryStringQuery("field1:photo*"))))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2548,7 +2478,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery().must(prefixQuery("field1", "photo")).should(matchQuery("field1", "test").minimumShouldMatch("0")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2564,7 +2494,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource().query(boolQuery().must(queryStringQuery("field1:photo*")).filter(missingQuery("field_null")))
-                .highlighter(highlight().field("field1"));
+                .highlight(highlight().field("field1"));
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source).get();
         assertHighlight(searchResponse, 0, "field1", 0, 1, equalTo("The <em>photography</em> word will get highlighted"));
     }
@@ -2593,10 +2523,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         SearchRequestBuilder searchRequestBuilder = client().prepareSearch()
                 .setSize(COUNT)
                 .setQuery(termQuery("field1", "test"))
-                .highlighter(new HighlightBuilder().field("field1"));
+                .addHighlightedField("field1");
         SearchResponse searchResponse =
                 searchRequestBuilder.get();
-        assertHitCount(searchResponse, COUNT);
+        assertHitCount(searchResponse, (long)COUNT);
         assertThat(searchResponse.getHits().hits().length, equalTo(COUNT));
         for (SearchHit hit : searchResponse.getHits()) {
             String prefix = prefixes.get(hit.id());
@@ -2666,8 +2596,9 @@ public class HighlighterSearchIT extends ESIntegTestCase {
             phraseBoostTestCaseForClauses(String highlighterType, float boost, QueryBuilder terms, P phrase) {
         Matcher<String> highlightedMatcher = Matchers.either(containsString("<em>highlight words together</em>")).or(
                 containsString("<em>highlight</em> <em>words</em> <em>together</em>"));
-        SearchRequestBuilder search = client().prepareSearch("test").highlighter(
-                new HighlightBuilder().field("field1", 100, 1).order("score").highlighterType(highlighterType).requireFieldMatch(true));
+        SearchRequestBuilder search = client().prepareSearch("test").setHighlighterRequireFieldMatch(true)
+                .setHighlighterOrder("score").setHighlighterType(highlighterType)
+                .addHighlightedField("field1", 100, 1);
 
         // Try with a bool query
         phrase.boost(boost);
diff --git a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
index e87b1eb..84a315c 100644
--- a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
@@ -36,7 +36,6 @@ import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -49,24 +48,9 @@ import java.util.List;
 import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasParentQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.nestedQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllSuccessful;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  */
@@ -112,14 +96,11 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments",
-                new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "fox")));
         // Inner hits can be defined in two ways: 1) with the query 2) as seperate inner_hit definition
         SearchRequest[] searchRequests = new SearchRequest[]{
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits("comment", null))).request(),
                 client().prepareSearch("articles").setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
-                        .innerHits(innerHitsBuilder).request()
+                        .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "fox"))).request()
         };
         for (SearchRequest searchRequest : searchRequests) {
             SearchResponse response = client().search(searchRequest).actionGet();
@@ -138,15 +119,10 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(1).getNestedIdentity().getOffset(), equalTo(1));
         }
 
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments",
-                new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "elephant")));
-        // Inner hits can be defined in two ways: 1) with the query 2) as
-        // seperate inner_hit definition
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")))
-                        .innerHits(innerHitsBuilder).request(),
+                        .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.message", "elephant"))).request(),
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "elephant")).innerHit(new QueryInnerHits("comment", null))).request(),
                 client().prepareSearch("articles")
@@ -173,23 +149,21 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(2).getNestedIdentity().getOffset(), equalTo(2));
         }
         InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit();
-        innerHit.highlighter(new HighlightBuilder().field("comments.message"));
+        innerHit.highlightBuilder().field("comments.message");
         innerHit.setExplain(true);
         innerHit.addFieldDataField("comments.message");
         innerHit.addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()));
         innerHit.setSize(1);
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comments", "comments", new InnerHitsBuilder.InnerHit()
+        searchRequests = new SearchRequest[] {
+                client().prepareSearch("articles")
+                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
+                        .addNestedInnerHits("comments", "comments", new InnerHitsBuilder.InnerHit()
                                 .setQuery(matchQuery("comments.message", "fox"))
-                            .highlighter(new HighlightBuilder().field("comments.message"))
+                                .addHighlightedField("comments.message")
                                 .setExplain(true)
                                 .addFieldDataField("comments.message")
                                 .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
-                            .setSize(1));
-        searchRequests = new SearchRequest[] {
-                client().prepareSearch("articles")
-                        .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")))
-                        .innerHits(innerHitsBuilder).request(),
+                                .setSize(1)).request(),
                 client().prepareSearch("articles")
                         .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, innerHit))).request()
         };
@@ -234,13 +208,11 @@ public class InnerHitsIT extends ESIntegTestCase {
         int size = randomIntBetween(0, numDocs);
         SearchResponse searchResponse;
         if (randomBoolean()) {
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            innerHitsBuilder.addNestedInnerHits("a", "field1", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)); // Sort order is DESC, because we reverse the inner objects during indexing!
-            innerHitsBuilder.addNestedInnerHits("b", "field2", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size));
             searchResponse = client().prepareSearch("idx")
                     .setSize(numDocs)
                     .addSort("_uid", SortOrder.ASC)
-                    .innerHits(innerHitsBuilder)
+                    .addNestedInnerHits("a", "field1", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size)) // Sort order is DESC, because we reverse the inner objects during indexing!
+                    .addNestedInnerHits("b", "field2", new InnerHitsBuilder.InnerHit().addSort("_doc", SortOrder.DESC).setSize(size))
                     .get();
         } else {
             BoolQueryBuilder boolQuery = new BoolQueryBuilder();
@@ -302,12 +274,10 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("articles", "comment", "6").setParent("2").setSource("message", "elephant scared by mice x y"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "fox")));
         SearchRequest[] searchRequests = new SearchRequest[]{
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
-                        .innerHits(innerHitsBuilder)
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "fox")))
                         .request(),
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "fox")).innerHit(new QueryInnerHits("comment", null)))
@@ -330,12 +300,10 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(1).type(), equalTo("comment"));
         }
 
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "elephant")));
         searchRequests = new SearchRequest[] {
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")))
-                        .innerHits(innerHitsBuilder)
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "elephant")))
                         .request(),
                 client().prepareSearch("articles")
                         .setQuery(hasChildQuery("comment", matchQuery("message", "elephant")).innerHit(new QueryInnerHits()))
@@ -359,24 +327,22 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertThat(innerHits.getAt(2).type(), equalTo("comment"));
         }
         InnerHitsBuilder.InnerHit innerHit = new InnerHitsBuilder.InnerHit();
-        innerHit.highlighter(new HighlightBuilder().field("message"));
+        innerHit.highlightBuilder().field("message");
         innerHit.setExplain(true);
         innerHit.addFieldDataField("message");
         innerHit.addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()));
         innerHit.setSize(1);
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
+        searchRequests = new SearchRequest[] {
+                client().prepareSearch("articles")
+                        .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
+                        .addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
                                         .setQuery(matchQuery("message", "fox"))
-                            .highlighter(new HighlightBuilder().field("message"))
+                                        .addHighlightedField("message")
                                         .setExplain(true)
                                         .addFieldDataField("message")
                                         .addScriptField("script", new Script("5", ScriptService.ScriptType.INLINE, MockScriptEngine.NAME, Collections.emptyMap()))
-                            .setSize(1));
-        searchRequests = new SearchRequest[] {
-                client().prepareSearch("articles")
-                        .setQuery(hasChildQuery("comment", matchQuery("message", "fox")))
-                        .innerHits(innerHitsBuilder)
-                        .request(),
+                                        .setSize(1)
+                        ).request(),
 
                 client().prepareSearch("articles")
                         .setQuery(
@@ -427,16 +393,14 @@ public class InnerHitsIT extends ESIntegTestCase {
         indexRandom(true, requestBuilders);
 
         int size = randomIntBetween(0, numDocs);
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("a", "child1", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size));
-        innerHitsBuilder.addParentChildInnerHits("b", "child2", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size));
         SearchResponse searchResponse;
         if (randomBoolean()) {
             searchResponse = client().prepareSearch("idx")
                     .setSize(numDocs)
                     .setTypes("parent")
                     .addSort("_uid", SortOrder.ASC)
-                    .innerHits(innerHitsBuilder)
+                    .addParentChildInnerHits("a", "child1", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))
+                    .addParentChildInnerHits("b", "child2", new InnerHitsBuilder.InnerHit().addSort("_uid", SortOrder.ASC).setSize(size))
                     .get();
         } else {
             BoolQueryBuilder boolQuery = new BoolQueryBuilder();
@@ -492,15 +456,12 @@ public class InnerHitsIT extends ESIntegTestCase {
     }
 
     @Test
-    @AwaitsFix(bugUrl = "need validation of type or path defined in InnerHitsBuilder")
     public void testPathOrTypeMustBeDefined() {
         createIndex("articles");
         ensureGreen("articles");
         try {
-            InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-            innerHitsBuilder.addParentChildInnerHits("comment", null, new InnerHitsBuilder.InnerHit());
             client().prepareSearch("articles")
-                    .innerHits(innerHitsBuilder)
+                    .addParentChildInnerHits("comment", null, new InnerHitsBuilder.InnerHit())
                     .get();
         } catch (Exception e) {
             assertThat(e.getMessage(), containsString("Failed to build"));
@@ -564,15 +525,13 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("articles", "remark", "2").setParent("2").setRouting("2").setSource("message", "bad"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "good")));
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
-                            .setQuery(hasChildQuery("remark", matchQuery("message", "good")))
-                            .innerHits(innerInnerHitsBuilder));
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(hasChildQuery("comment", hasChildQuery("remark", matchQuery("message", "good"))))
-                .innerHits(innerHitsBuilder)
+                .addParentChildInnerHits("comment", "comment",
+                        new InnerHitsBuilder.InnerHit()
+                                .setQuery(hasChildQuery("remark", matchQuery("message", "good")))
+                                .addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "good")))
+                )
                 .get();
 
         assertNoFailures(response);
@@ -590,15 +549,13 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(innerHits.getAt(0).getId(), equalTo("1"));
         assertThat(innerHits.getAt(0).type(), equalTo("remark"));
 
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "bad")));
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("comment", "comment", new InnerHitsBuilder.InnerHit()
-                .setQuery(hasChildQuery("remark", matchQuery("message", "bad")))
-                .innerHits(innerInnerHitsBuilder));
         response = client().prepareSearch("articles")
                 .setQuery(hasChildQuery("comment", hasChildQuery("remark", matchQuery("message", "bad"))))
-                .innerHits(innerHitsBuilder)
+                .addParentChildInnerHits("comment", "comment",
+                        new InnerHitsBuilder.InnerHit()
+                                .setQuery(hasChildQuery("remark", matchQuery("message", "bad")))
+                                .addParentChildInnerHits("remark", "remark", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("message", "bad")))
+                )
                 .get();
 
         assertNoFailures(response);
@@ -660,16 +617,12 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "good")));
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
-                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good")))
-                .innerHits(innerInnerHitsBuilder)
-        );
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good"))))
-                .innerHits(innerHitsBuilder).get();
+                .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
+                                .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "good")))
+                                .addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "good")))
+                ).get();
         assertNoFailures(response);
         assertHitCount(response, 1);
         assertSearchHit(response, 1, hasId("1"));
@@ -706,15 +659,11 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(innerHits.getAt(0).getNestedIdentity().getChild().getField().string(), equalTo("remarks"));
         assertThat(innerHits.getAt(0).getNestedIdentity().getChild().getOffset(), equalTo(0));
 
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "bad")));
-        innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
-                            .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")))
-                            .innerHits(innerInnerHitsBuilder));
         response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad"))))
-                .innerHits(innerHitsBuilder)
+                .addNestedInnerHits("comment", "comments", new InnerHitsBuilder.InnerHit()
+                        .setQuery(nestedQuery("comments.remarks", matchQuery("comments.remarks.message", "bad")))
+                        .addNestedInnerHits("remark", "comments.remarks", new InnerHitsBuilder.InnerHit().setQuery(matchQuery("comments.remarks.message", "bad"))))
                 .get();
         assertNoFailures(response);
         assertHitCount(response, 1);
@@ -797,7 +746,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getField().string(), equalTo("comments"));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getOffset(), equalTo(0));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getChild(), nullValue());
-        assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue(), equalTo("fox eat quick"));
+        assertThat(String.valueOf((Object)response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue()), equalTo("fox eat quick"));
     }
 
     @Test
@@ -825,7 +774,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
         InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.highlighter(new HighlightBuilder().field("comments.message"));
+        builder.highlightBuilder().field("comments.message");
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
                 .get();
@@ -877,7 +826,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getField().string(), equalTo("comments"));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getOffset(), equalTo(0));
         assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).getNestedIdentity().getChild(), nullValue());
-        assertThat(response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue(), equalTo("fox eat quick"));
+        assertThat(String.valueOf((Object)response.getHits().getAt(0).getInnerHits().get("comments").getAt(0).fields().get("comments.message").getValue()), equalTo("fox eat quick"));
     }
 
     @Test
@@ -904,7 +853,7 @@ public class InnerHitsIT extends ESIntegTestCase {
                 .endObject()));
         indexRandom(true, requests);
         InnerHitsBuilder.InnerHit builder = new InnerHitsBuilder.InnerHit();
-        builder.highlighter(new HighlightBuilder().field("comments.message"));
+        builder.highlightBuilder().field("comments.message");
         SearchResponse response = client().prepareSearch("articles")
                 .setQuery(nestedQuery("comments", matchQuery("comments.message", "fox")).innerHit(new QueryInnerHits(null, builder)))
                         .get();
@@ -1016,23 +965,17 @@ public class InnerHitsIT extends ESIntegTestCase {
         requests.add(client().prepareIndex("royals", "baron", "baron4").setParent("earl4").setRouting("king").setSource("{}"));
         indexRandom(true, requests);
 
-        InnerHitsBuilder innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("barons", "baron", new InnerHitsBuilder.InnerHit());
-        InnerHitsBuilder innerHitsBuilder = new InnerHitsBuilder();
-        innerHitsBuilder.addParentChildInnerHits("earls", "earl", new InnerHitsBuilder.InnerHit()
+        SearchResponse response = client().prepareSearch("royals")
+                .setTypes("duke")
+                .addParentChildInnerHits("earls", "earl", new InnerHitsBuilder.InnerHit()
                                 .addSort(SortBuilders.fieldSort("_uid").order(SortOrder.ASC))
                                 .setSize(4)
-                .innerHits(innerInnerHitsBuilder)
-        );
-        innerInnerHitsBuilder = new InnerHitsBuilder();
-        innerInnerHitsBuilder.addParentChildInnerHits("kings", "king", new InnerHitsBuilder.InnerHit());
-        innerHitsBuilder.addParentChildInnerHits("princes", "prince",
+                                .addParentChildInnerHits("barons", "baron", new InnerHitsBuilder.InnerHit())
+                )
+                .addParentChildInnerHits("princes", "prince",
                         new InnerHitsBuilder.InnerHit()
-            .innerHits(innerInnerHitsBuilder)
-        );
-        SearchResponse response = client().prepareSearch("royals")
-                .setTypes("duke")
-                .innerHits(innerHitsBuilder)
+                        .addParentChildInnerHits("kings", "king", new InnerHitsBuilder.InnerHit())
+                )
                 .get();
         assertHitCount(response, 1);
         assertThat(response.getHits().getAt(0).getId(), equalTo("duke"));
diff --git a/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java b/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
index 4d85f8b..349197d 100644
--- a/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/ExistsMissingIT.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.search.query;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.elasticsearch.action.explain.ExplainResponse;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
@@ -32,11 +30,14 @@ import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.test.ESIntegTestCase;
 
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
 
+import static java.util.Collections.emptyMap;
+import static java.util.Collections.singletonMap;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
@@ -90,14 +91,17 @@ public class ExistsMissingIT extends ESIntegTestCase {
 
         assertAcked(client().admin().indices().prepareCreate("idx").addMapping("type", mapping));
         @SuppressWarnings("unchecked")
+        Map<String, Object> barObject = new HashMap<>();
+        barObject.put("foo", "bar");
+        barObject.put("bar", singletonMap("bar", "foo"));
         final Map<String, Object>[] sources = new Map[] {
                 // simple property
-                ImmutableMap.of("foo", "bar"),
+                singletonMap("foo", "bar"),
                 // object fields
-                ImmutableMap.of("bar", ImmutableMap.of("foo", "bar", "bar", ImmutableMap.of("bar", "foo"))),
-                ImmutableMap.of("bar", ImmutableMap.of("baz", 42)),
+                singletonMap("bar", barObject),
+                singletonMap("bar", singletonMap("baz", 42)),
                 // empty doc
-                ImmutableMap.of()
+                emptyMap()
         };
         List<IndexRequestBuilder> reqs = new ArrayList<IndexRequestBuilder>();
         for (Map<String, Object> source : sources) {
diff --git a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
index b394d10..bf3e458 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
@@ -21,12 +21,11 @@ package org.elasticsearch.search.query;
 
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.BoolQueryBuilder;
 import org.elasticsearch.index.query.Operator;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.SimpleQueryStringFlag;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -35,17 +34,8 @@ import java.util.Locale;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.simpleQueryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
+import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -252,6 +242,11 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
+        // Sending a negative 'flags' value is the same as SimpleQueryStringFlag.ALL
+        searchResponse = client().prepareSearch().setQuery("{\"simple_query_string\": {\"query\": \"foo bar\", \"flags\": -1}}").get();
+        assertHitCount(searchResponse, 3l);
+        assertSearchHits(searchResponse, "1", "2", "3");
+
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
                         .defaultOperator(Operator.AND)
@@ -272,18 +267,21 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 0l);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.simpleQueryStringQuery("foo|bar").defaultOperator(Operator.AND)
-                                .flags(SimpleQueryStringFlag.NONE))).get();
+        searchResponse = client().prepareSearch().setSource(new BytesArray("{\n" +
+                "  \"query\": {\n" +
+                "    \"simple_query_string\": {\n" +
+                "      \"query\": \"foo|bar\",\n" +
+                "      \"default_operator\": \"AND\"," +
+                "      \"flags\": \"NONE\"\n" +
+                "    }\n" +
+                "  }\n" +
+                "}")).get();
         assertHitCount(searchResponse, 1l);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setQuery(
-                        simpleQueryStringQuery("baz | egg*").defaultOperator(Operator.AND).flags(SimpleQueryStringFlag.WHITESPACE,
-                                SimpleQueryStringFlag.PREFIX)).get();
+        searchResponse = client().prepareSearch().setQuery(
+                simpleQueryStringQuery("baz | egg*")
+                        .defaultOperator(Operator.AND)
+                        .flags(SimpleQueryStringFlag.WHITESPACE, SimpleQueryStringFlag.PREFIX)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("4"));
     }
diff --git a/core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java b/core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java
index e4e7a69..4aeb416 100644
--- a/core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java
+++ b/core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java
@@ -27,7 +27,10 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;
+import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.rest.action.search.RestClearScrollAction;
@@ -294,6 +297,7 @@ public class SearchScrollIT extends ESIntegTestCase {
         assertThat(clearResponse.isSucceeded(), is(true));
         assertThat(clearResponse.getNumFreed(), greaterThan(0));
         assertThat(clearResponse.status(), equalTo(RestStatus.OK));
+        assertToXContentResponse(clearResponse, true, clearResponse.getNumFreed());
 
         assertThrows(client().prepareSearchScroll(searchResponse1.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND);
         assertThrows(client().prepareSearchScroll(searchResponse2.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND);
@@ -310,6 +314,7 @@ public class SearchScrollIT extends ESIntegTestCase {
         assertThat(response.isSucceeded(), is(true));
         assertThat(response.getNumFreed(), equalTo(0));
         assertThat(response.status(), equalTo(RestStatus.NOT_FOUND));
+        assertToXContentResponse(response, true, response.getNumFreed());
     }
 
     @Test
@@ -404,6 +409,7 @@ public class SearchScrollIT extends ESIntegTestCase {
         assertThat(clearResponse.isSucceeded(), is(true));
         assertThat(clearResponse.getNumFreed(), greaterThan(0));
         assertThat(clearResponse.status(), equalTo(RestStatus.OK));
+        assertToXContentResponse(clearResponse, true, clearResponse.getNumFreed());
 
         assertThrows(internalCluster().transportClient().prepareSearchScroll(searchResponse1.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND);
         assertThrows(internalCluster().transportClient().prepareSearchScroll(searchResponse2.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)), RestStatus.NOT_FOUND);
@@ -593,4 +599,19 @@ public class SearchScrollIT extends ESIntegTestCase {
         }
     }
 
+    private void assertToXContentResponse(ClearScrollResponse response, boolean succeed, int numFreed) throws IOException {
+        XContentBuilder builder = XContentFactory.jsonBuilder();
+        builder.startObject();
+        response.toXContent(builder, ToXContent.EMPTY_PARAMS);
+        builder.endObject();
+
+        BytesReference bytesReference = builder.bytes();
+        Map<String, Object> map;
+        try (XContentParser parser = XContentFactory.xContent(bytesReference).createParser(bytesReference)) {
+            map = parser.map();
+        }
+
+        assertThat(map.get("succeeded"), is(succeed));
+        assertThat(map.get("num_freed"), equalTo(numFreed));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
index df04d43..9b97afc 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java
@@ -20,12 +20,13 @@ package org.elasticsearch.search.suggest;
 
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.CollectionUtils;
+import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
-import org.elasticsearch.test.ESIntegTestCase.Scope;
 import org.junit.Test;
 
 import java.io.IOException;
@@ -34,6 +35,7 @@ import java.util.List;
 import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.ESIntegTestCase.Scope;
 import static org.hamcrest.Matchers.hasSize;
 import static org.hamcrest.Matchers.is;
 
@@ -57,12 +59,11 @@ public class CustomSuggesterSearchIT extends ESIntegTestCase {
                 .endObject())
                 .setRefresh(true).execute().actionGet();
         ensureYellow();
-
+        
         String randomText = randomAsciiOfLength(10);
         String randomField = randomAsciiOfLength(10);
         String randomSuffix = randomAsciiOfLength(10);
-        SuggestBuilder suggestBuilder = new SuggestBuilder();
-        suggestBuilder.addSuggestion(
+        SearchRequestBuilder searchRequestBuilder = client().prepareSearch("test").setTypes("test").setFrom(0).setSize(1).addSuggestion(
                 new SuggestBuilder.SuggestionBuilder<SuggestBuilder.SuggestionBuilder>("someName", "custom") {
                     @Override
                     protected XContentBuilder innerToXContent(XContentBuilder builder, Params params) throws IOException {
@@ -72,8 +73,6 @@ public class CustomSuggesterSearchIT extends ESIntegTestCase {
                     }
                 }.text(randomText)
         );
-        SearchRequestBuilder searchRequestBuilder = client().prepareSearch("test").setTypes("test").setFrom(0).setSize(1)
-                .suggest(suggestBuilder);
 
         SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();
 
diff --git a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
index c5e0912..85993fd 100644
--- a/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/suggest/SuggestSearchIT.java
@@ -19,15 +19,10 @@
 
 package org.elasticsearch.search.suggest;
 
-
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.index.IndexRequestBuilder;
-import org.elasticsearch.action.search.ReduceSearchPhaseException;
-import org.elasticsearch.action.search.SearchPhaseExecutionException;
-import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.action.search.*;
 import org.elasticsearch.action.suggest.SuggestRequestBuilder;
 import org.elasticsearch.action.suggest.SuggestResponse;
 import org.elasticsearch.common.io.PathUtils;
@@ -55,17 +50,8 @@ import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
 import static org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion;
 import static org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion;
 import static org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.candidateGenerator;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionPhraseCollateMatchExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSuggestionSize;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.anyOf;
-import static org.hamcrest.Matchers.endsWith;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.nullValue;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 /**
  * Integration tests for term and phrase suggestions.  Many of these tests many requests that vary only slightly from one another.  Where
@@ -284,14 +270,16 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         phraseSuggestion.field("nosuchField");
         {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+            SearchRequestBuilder suggestBuilder = client().prepareSearch().setSize(0);
+            suggestBuilder.setSuggestText("tetsting sugestion");
+            suggestBuilder.addSuggestion(phraseSuggestion);
+            assertThrows(suggestBuilder, SearchPhaseExecutionException.class);
         }
         {
-            SearchRequestBuilder searchBuilder = client().prepareSearch().setSize(0);
-            searchBuilder.suggest(new SuggestBuilder().setText("tetsting sugestion").addSuggestion(phraseSuggestion));
-            assertThrows(searchBuilder, SearchPhaseExecutionException.class);
+            SearchRequestBuilder suggestBuilder = client().prepareSearch().setSize(0);
+            suggestBuilder.setSuggestText("tetsting sugestion");
+            suggestBuilder.addSuggestion(phraseSuggestion);
+            assertThrows(suggestBuilder, SearchPhaseExecutionException.class);
         }
     }
 
@@ -611,7 +599,7 @@ public class SuggestSearchIT extends ESIntegTestCase {
         // Check the name this time because we're repeating it which is funky
         assertThat(searchSuggest.getSuggestion("simple_phrase").getEntries().get(0).getText().string(), equalTo("Xor the Got-Jewel Xor the Got-Jewel Xor the Got-Jewel"));
     }
-    
+
     private List<String> readMarvelHeroNames() throws IOException, URISyntaxException {
         return Files.readAllLines(PathUtils.get(SuggestSearchIT.class.getResource("/config/names.txt").toURI()), StandardCharsets.UTF_8);
     }
@@ -829,16 +817,14 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         // When searching on a shard with a non existing mapping, we should fail
         SearchRequestBuilder request = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f)));
+            .setSuggestText("tetsting sugestion")
+            .addSuggestion(phraseSuggestion("did_you_mean").field("fielddoesnotexist").maxErrors(5.0f));
         assertThrows(request, SearchPhaseExecutionException.class);
 
         // When searching on a shard which does not hold yet any document of an existing type, we should not fail
         SearchResponse searchResponse = client().prepareSearch().setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+            .setSuggestText("tetsting sugestion")
+            .addSuggestion(phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f))
             .get();
         ElasticsearchAssertions.assertNoFailures(searchResponse);
         ElasticsearchAssertions.assertSuggestion(searchResponse.getSuggest(), 0, 0, "did_you_mean", "testing suggestions");
@@ -880,9 +866,8 @@ public class SuggestSearchIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client().prepareSearch()
                 .setSize(0)
-                .suggest(
-                        new SuggestBuilder().setText("tetsting sugestion").addSuggestion(
-                                phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f)))
+                .setSuggestText("tetsting sugestion")
+                .addSuggestion(phraseSuggestion("did_you_mean").field("name").maxErrors(5.0f))
                 .get();
 
         assertNoFailures(searchResponse);
@@ -1273,14 +1258,12 @@ public class SuggestSearchIT extends ESIntegTestCase {
     protected Suggest searchSuggest(String suggestText, int expectShardsFailed, SuggestionBuilder<?>... suggestions) {
         if (randomBoolean()) {
             SearchRequestBuilder builder = client().prepareSearch().setSize(0);
-            SuggestBuilder suggestBuilder = new SuggestBuilder();
             if (suggestText != null) {
-                suggestBuilder.setText(suggestText);
+                builder.setSuggestText(suggestText);
             }
             for (SuggestionBuilder<?> suggestion : suggestions) {
-                suggestBuilder.addSuggestion(suggestion);
+                builder.addSuggestion(suggestion);
             }
-            builder.suggest(suggestBuilder);
             SearchResponse actionGet = builder.execute().actionGet();
             assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(expectShardsFailed));
             return actionGet.getSuggest();
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index 206dfee..7f38715 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.snapshots;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.lucene.util.IOUtils;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
@@ -29,7 +28,11 @@ import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRes
 import org.elasticsearch.action.admin.cluster.snapshots.delete.DeleteSnapshotResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsResponse;
 import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;
-import org.elasticsearch.action.admin.cluster.snapshots.status.*;
+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexShardStage;
+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexShardStatus;
+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexStatus;
+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;
+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;
 import org.elasticsearch.action.admin.cluster.state.ClusterStateResponse;
 import org.elasticsearch.action.admin.indices.flush.FlushResponse;
 import org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse;
@@ -79,8 +82,24 @@ import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
 import static org.elasticsearch.index.shard.IndexShard.INDEX_REFRESH_INTERVAL;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAliasesExist;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAliasesMissing;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllSuccessful;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertBlocked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertIndexTemplateExists;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertIndexTemplateMissing;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
+import static org.hamcrest.Matchers.allOf;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.lessThan;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
+import static org.hamcrest.Matchers.startsWith;
 
 public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCase {
 
@@ -1824,7 +1843,7 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
             @Override
             public ClusterState execute(ClusterState currentState) {
                 // Simulate orphan snapshot
-                ImmutableMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableMap.builder();
+                ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();
                 shards.put(new ShardId("test-idx", 0), new ShardSnapshotStatus("unknown-node", State.ABORTED));
                 shards.put(new ShardId("test-idx", 1), new ShardSnapshotStatus("unknown-node", State.ABORTED));
                 shards.put(new ShardId("test-idx", 2), new ShardSnapshotStatus("unknown-node", State.ABORTED));
diff --git a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
index 14b6526..0597ad9 100644
--- a/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESIntegTestCase.java
@@ -1479,18 +1479,18 @@ public abstract class ESIntegTestCase extends ESTestCase {
         if (rarely()) {
             if (rarely()) {
                 client().admin().indices().prepareRefresh(indices).setIndicesOptions(IndicesOptions.lenientExpandOpen()).execute(
-                        new LatchedActionListener<RefreshResponse>(newLatch(inFlightAsyncOperations)));
+                        new LatchedActionListener<>(newLatch(inFlightAsyncOperations)));
             } else if (maybeFlush && rarely()) {
                 if (randomBoolean()) {
                     client().admin().indices().prepareFlush(indices).setIndicesOptions(IndicesOptions.lenientExpandOpen()).execute(
-                            new LatchedActionListener<FlushResponse>(newLatch(inFlightAsyncOperations)));
-                } else {
+                            new LatchedActionListener<>(newLatch(inFlightAsyncOperations)));
+                } else if (isInternalCluster()) {
                     internalCluster().getInstance(SyncedFlushService.class).attemptSyncedFlush(indices, IndicesOptions.lenientExpandOpen(),
-                            new LatchedActionListener<IndicesSyncedFlushResult>(newLatch(inFlightAsyncOperations)));
+                            new LatchedActionListener<>(newLatch(inFlightAsyncOperations)));
                 }
             } else if (rarely()) {
                 client().admin().indices().prepareOptimize(indices).setIndicesOptions(IndicesOptions.lenientExpandOpen()).setMaxNumSegments(between(1, 10)).setFlush(maybeFlush && randomBoolean()).execute(
-                        new LatchedActionListener<OptimizeResponse>(newLatch(inFlightAsyncOperations)));
+                        new LatchedActionListener<>(newLatch(inFlightAsyncOperations)));
             }
         }
         while (inFlightAsyncOperations.size() > MAX_IN_FLIGHT_ASYNC_INDEXES) {
diff --git a/core/src/test/java/org/elasticsearch/test/ESTestCase.java b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
index 8bbd978..78d004f 100644
--- a/core/src/test/java/org/elasticsearch/test/ESTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
@@ -41,7 +41,6 @@ import org.elasticsearch.bootstrap.BootstrapForTesting;
 import org.elasticsearch.cache.recycler.MockPageCacheRecycler;
 import org.elasticsearch.client.Requests;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.routing.DjbHashFunction;
 import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.io.PathUtilsForTesting;
 import org.elasticsearch.common.logging.ESLogger;
@@ -549,9 +548,6 @@ public abstract class ESTestCase extends LuceneTestCase {
     /** Return consistent index settings for the provided index version. */
     public static Settings.Builder settings(Version version) {
         Settings.Builder builder = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, version);
-        if (version.before(Version.V_2_0_0_beta1)) {
-            builder.put(IndexMetaData.SETTING_LEGACY_ROUTING_HASH_FUNCTION, DjbHashFunction.class);
-        }
         return builder;
     }
 
diff --git a/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java b/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
index f7c44a5..22973d4 100644
--- a/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
+++ b/core/src/test/java/org/elasticsearch/test/InternalTestCluster.java
@@ -1852,7 +1852,7 @@ public final class InternalTestCluster extends TestCluster {
             for (NodeAndClient nodeAndClient : nodes.values()) {
                 final IndicesFieldDataCache fdCache = getInstanceFromNode(IndicesFieldDataCache.class, nodeAndClient.node);
                 // Clean up the cache, ensuring that entries' listeners have been called
-                fdCache.getCache().cleanUp();
+                fdCache.getCache().refresh();
 
                 final String name = nodeAndClient.name;
                 final CircuitBreakerService breakerService = getInstanceFromNode(CircuitBreakerService.class, nodeAndClient.node);
diff --git a/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java b/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
index 9c24a11..5772543 100644
--- a/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
+++ b/core/src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java
@@ -53,8 +53,6 @@ import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
@@ -84,7 +82,13 @@ import java.util.function.Function;
 import java.util.function.Predicate;
 import java.util.stream.Collectors;
 
-import static org.apache.lucene.util.LuceneTestCase.random;
+import static org.elasticsearch.test.ESTestCase.assertArrayEquals;
+import static org.elasticsearch.test.ESTestCase.assertEquals;
+import static org.elasticsearch.test.ESTestCase.assertFalse;
+import static org.elasticsearch.test.ESTestCase.assertNotNull;
+import static org.elasticsearch.test.ESTestCase.assertTrue;
+import static org.elasticsearch.test.ESTestCase.fail;
+import static org.elasticsearch.test.ESTestCase.random;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
@@ -98,12 +102,6 @@ import static org.hamcrest.Matchers.instanceOf;
 import static org.hamcrest.Matchers.not;
 import static org.hamcrest.Matchers.notNullValue;
 import static org.hamcrest.Matchers.nullValue;
-import static org.junit.Assert.assertArrayEquals;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
 
 /**
  *
@@ -665,10 +663,6 @@ public class ElasticsearchAssertions {
     }
 
     public static void assertVersionSerializable(Version version, Streamable streamable) {
-        assertVersionSerializable(version, streamable, null);
-    }
-
-    public static void assertVersionSerializable(Version version, Streamable streamable, NamedWriteableRegistry namedWriteableRegistry) {
         try {
             Streamable newInstance = tryCreateNewInstance(streamable);
             if (newInstance == null) {
@@ -680,15 +674,10 @@ public class ElasticsearchAssertions {
             }
             BytesReference orig = serialize(version, streamable);
             StreamInput input = StreamInput.wrap(orig);
-            if (namedWriteableRegistry != null) {
-                input = new NamedWriteableAwareStreamInput(input, namedWriteableRegistry);
-            }
             input.setVersion(version);
             newInstance.readFrom(input);
-            assertThat("Stream should be fully read with version [" + version + "] for streamable [" + streamable + "]", input.available(),
-                    equalTo(0));
-            assertThat("Serialization failed with version [" + version + "] bytes should be equal for streamable [" + streamable + "]",
-                    serialize(version, streamable), equalTo(orig));
+            assertThat("Stream should be fully read with version [" + version + "] for streamable [" + streamable + "]", input.available(), equalTo(0));
+            assertThat("Serialization failed with version [" + version + "] bytes should be equal for streamable [" + streamable + "]", serialize(version, streamable), equalTo(orig));
         } catch (Throwable ex) {
             throw new RuntimeException("failed to check serialization - version [" + version + "] for streamable [" + streamable + "]", ex);
         }
diff --git a/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java b/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
index 852d71c..da6c0b3 100644
--- a/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
+++ b/core/src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java
@@ -18,9 +18,13 @@
  */
 package org.elasticsearch.test.rest.section;
 
-import com.google.common.collect.ImmutableMap;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 
-import java.util.*;
+import static java.util.Collections.unmodifiableMap;
 
 /**
  * Represents a test fragment that contains the information needed to call an api
@@ -41,7 +45,7 @@ public class ApiCallSection {
 
     public Map<String, String> getParams() {
         //make sure we never modify the parameters once returned
-        return ImmutableMap.copyOf(params);
+        return unmodifiableMap(params);
     }
 
     public void addParam(String key, String value) {
diff --git a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index 64cc401..c253a75 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -77,15 +77,13 @@ public class AssertingLocalTransport extends LocalTransport {
 
     @Override
     protected void handleParsedResponse(final TransportResponse response, final TransportResponseHandler handler) {
-        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response,
-                namedWriteableRegistry);
+        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response);
         super.handleParsedResponse(response, handler);
     }
 
     @Override
     public void sendRequest(final DiscoveryNode node, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options) throws IOException, TransportException {
-        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request,
-                namedWriteableRegistry);
+        ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request);
         super.sendRequest(node, requestId, action, request, options);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
index 50cb00a..c423fb7 100644
--- a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
+++ b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTestCase.java
@@ -19,7 +19,6 @@
 
 package org.elasticsearch.transport;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -42,8 +41,12 @@ import java.util.concurrent.Semaphore;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicReference;
 
+import static java.util.Collections.emptyMap;
 import static org.elasticsearch.transport.TransportRequestOptions.options;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.endsWith;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.instanceOf;
+import static org.hamcrest.Matchers.notNullValue;
 
 /**
  *
@@ -71,12 +74,12 @@ public abstract class AbstractSimpleTransportTestCase extends ESTestCase {
                 Settings.builder().put("name", "TS_A", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
                 version0, new NamedWriteableRegistry()
         );
-        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version0);
+        nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), emptyMap(), version0);
         serviceB = build(
                 Settings.builder().put("name", "TS_B", TransportService.SETTING_TRACE_LOG_INCLUDE, "", TransportService.SETTING_TRACE_LOG_EXCLUDE, "NOTHING").build(),
                 version1, new NamedWriteableRegistry()
         );
-        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), version1);
+        nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), emptyMap(), version1);
 
         // wait till all nodes are properly connected and the event has been sent, so tests in this class
         // will not get this callback called on the connections done in this setup
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java
index d268907..4e03b8e 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyScheduledPingTests.java
@@ -18,7 +18,6 @@
  */
 package org.elasticsearch.transport.netty;
 
-import com.google.common.collect.ImmutableMap;
 import org.elasticsearch.Version;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
@@ -29,11 +28,19 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.test.ESTestCase;
 import org.elasticsearch.test.transport.MockTransportService;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
+import org.elasticsearch.transport.BaseTransportResponseHandler;
+import org.elasticsearch.transport.TransportChannel;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportRequest;
+import org.elasticsearch.transport.TransportRequestHandler;
+import org.elasticsearch.transport.TransportRequestOptions;
+import org.elasticsearch.transport.TransportResponse;
+import org.elasticsearch.transport.TransportResponseOptions;
 import org.junit.Test;
 
 import java.io.IOException;
 
+import static java.util.Collections.emptyMap;
 import static org.hamcrest.Matchers.equalTo;
 import static org.hamcrest.Matchers.greaterThan;
 
@@ -57,8 +64,8 @@ public class NettyScheduledPingTests extends ESTestCase {
         MockTransportService serviceB = new MockTransportService(settings, nettyB, threadPool);
         serviceB.start();
 
-        DiscoveryNode nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), Version.CURRENT);
-        DiscoveryNode nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), ImmutableMap.<String, String>of(), Version.CURRENT);
+        DiscoveryNode nodeA = new DiscoveryNode("TS_A", "TS_A", serviceA.boundAddress().publishAddress(), emptyMap(), Version.CURRENT);
+        DiscoveryNode nodeB = new DiscoveryNode("TS_B", "TS_B", serviceB.boundAddress().publishAddress(), emptyMap(), Version.CURRENT);
 
         serviceA.connectToNode(nodeB);
         serviceB.connectToNode(nodeA);
diff --git a/core/src/test/java/org/elasticsearch/update/UpdateIT.java b/core/src/test/java/org/elasticsearch/update/UpdateIT.java
new file mode 100644
index 0000000..8c62d97
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/update/UpdateIT.java
@@ -0,0 +1,1210 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.update;
+
+import org.elasticsearch.ElasticsearchTimeoutException;
+import org.elasticsearch.action.ActionListener;
+import org.elasticsearch.action.ActionRequestValidationException;
+import org.elasticsearch.action.admin.indices.alias.Alias;
+import org.elasticsearch.action.delete.DeleteRequest;
+import org.elasticsearch.action.delete.DeleteResponse;
+import org.elasticsearch.action.get.GetResponse;
+import org.elasticsearch.action.update.UpdateRequest;
+import org.elasticsearch.action.update.UpdateRequestBuilder;
+import org.elasticsearch.action.update.UpdateResponse;
+import org.elasticsearch.client.transport.NoNodeAvailableException;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.index.VersionType;
+import org.elasticsearch.index.engine.DocumentMissingException;
+import org.elasticsearch.index.engine.VersionConflictEngineException;
+import org.elasticsearch.index.shard.MergePolicyConfig;
+import org.elasticsearch.plugins.Plugin;
+import org.elasticsearch.script.CompiledScript;
+import org.elasticsearch.script.ExecutableScript;
+import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptEngineService;
+import org.elasticsearch.script.ScriptModule;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.script.SearchScript;
+import org.elasticsearch.search.lookup.SearchLookup;
+import org.elasticsearch.test.ESIntegTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.CopyOnWriteArrayList;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+
+import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
+import static org.hamcrest.Matchers.*;
+
+public class UpdateIT extends ESIntegTestCase {
+
+    public static class PutFieldValuesScriptPlugin extends Plugin {
+
+        public PutFieldValuesScriptPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return PutFieldValuesScriptEngine.NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for " + UpdateIT.class;
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(PutFieldValuesScriptEngine.class);
+        }
+
+    }
+
+    public static class PutFieldValuesScriptEngine implements ScriptEngineService {
+
+        public static final String NAME = "put_values";
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public String[] types() {
+            return new String[] { NAME };
+        }
+
+        @Override
+        public String[] extensions() {
+            return types();
+        }
+
+        @Override
+        public boolean sandboxed() {
+            return true;
+        }
+
+        @Override
+        public Object compile(String script) {
+            return new Object(); // unused
+        }
+
+        @Override
+        public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> originalParams) {
+            return new ExecutableScript() {
+
+                Map<String, Object> vars = new HashMap<>();
+
+                @Override
+                public void setNextVar(String name, Object value) {
+                    vars.put(name, value);
+                }
+
+                @Override
+                public Object run() {
+                    Map<String, Object> ctx = (Map<String, Object>) vars.get("ctx");
+                    assertNotNull(ctx);
+
+                    Map<String, Object> params = new HashMap<>(originalParams);
+
+                    Map<String, Object> newCtx = (Map<String, Object>) params.remove("_ctx");
+                    if (newCtx != null) {
+                        assertFalse(newCtx.containsKey("_source"));
+                        ctx.putAll(newCtx);
+                    }
+
+                    Map<String, Object> source = (Map<String, Object>) ctx.get("_source");
+                    source.putAll(params);
+
+                    return ctx;
+                }
+
+                @Override
+                public Object unwrap(Object value) {
+                    return value;
+                }
+
+            };
+        }
+
+        @Override
+        public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String, Object> vars) {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public void scriptRemoved(CompiledScript script) {
+        }
+
+    }
+
+    public static class FieldIncrementScriptPlugin extends Plugin {
+
+        public FieldIncrementScriptPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return FieldIncrementScriptEngine.NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for " + UpdateIT.class;
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(FieldIncrementScriptEngine.class);
+        }
+
+    }
+
+    public static class FieldIncrementScriptEngine implements ScriptEngineService {
+
+        public static final String NAME = "field_inc";
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public String[] types() {
+            return new String[] { NAME };
+        }
+
+        @Override
+        public String[] extensions() {
+            return types();
+        }
+
+        @Override
+        public boolean sandboxed() {
+            return true;
+        }
+
+        @Override
+        public Object compile(String script) {
+            return script;
+        }
+
+        @Override
+        public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> params) {
+            final String field = (String) compiledScript.compiled();
+            return new ExecutableScript() {
+
+                Map<String, Object> vars = new HashMap<>();
+
+                @Override
+                public void setNextVar(String name, Object value) {
+                    vars.put(name, value);
+                }
+
+                @Override
+                public Object run() {
+                    Map<String, Object> ctx = (Map<String, Object>) vars.get("ctx");
+                    assertNotNull(ctx);
+                    Map<String, Object> source = (Map<String, Object>) ctx.get("_source");
+                    Number currentValue = (Number) source.get(field);
+                    Number inc = params == null ? 1L : (Number) params.getOrDefault("inc", 1);
+                    source.put(field, currentValue.longValue() + inc.longValue());
+                    return ctx;
+                }
+
+                @Override
+                public Object unwrap(Object value) {
+                    return value;
+                }
+
+            };
+        }
+
+        @Override
+        public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String, Object> vars) {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public void scriptRemoved(CompiledScript script) {
+        }
+
+    }
+
+    public static class ScriptedUpsertScriptPlugin extends Plugin {
+
+        public ScriptedUpsertScriptPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return ScriptedUpsertScriptEngine.NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for " + UpdateIT.class + ".testScriptedUpsert";
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(ScriptedUpsertScriptEngine.class);
+        }
+
+    }
+
+    public static class ScriptedUpsertScriptEngine implements ScriptEngineService {
+
+        public static final String NAME = "scripted_upsert";
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public String[] types() {
+            return new String[] { NAME };
+        }
+
+        @Override
+        public String[] extensions() {
+            return types();
+        }
+
+        @Override
+        public boolean sandboxed() {
+            return true;
+        }
+
+        @Override
+        public Object compile(String script) {
+            return new Object(); // unused
+        }
+
+        @Override
+        public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> params) {
+            return new ExecutableScript() {
+
+                Map<String, Object> vars = new HashMap<>();
+
+                @Override
+                public void setNextVar(String name, Object value) {
+                    vars.put(name, value);
+                }
+
+                @Override
+                public Object run() {
+                    Map<String, Object> ctx = (Map<String, Object>) vars.get("ctx");
+                    assertNotNull(ctx);
+                    Map<String, Object> source = (Map<String, Object>) ctx.get("_source");
+                    Number payment = (Number) params.get("payment");
+                    Number oldBalance = (Number) source.get("balance");
+                    int deduction = "create".equals(ctx.get("op")) ? payment.intValue() / 2 : payment.intValue();
+                    source.put("balance", oldBalance.intValue() - deduction);
+                    return ctx;
+                }
+
+                @Override
+                public Object unwrap(Object value) {
+                    return value;
+                }
+
+            };
+        }
+
+        @Override
+        public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String, Object> vars) {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public void scriptRemoved(CompiledScript script) {
+        }
+
+    }
+
+    public static class ExtractContextInSourceScriptPlugin extends Plugin {
+
+        public ExtractContextInSourceScriptPlugin() {
+        }
+
+        @Override
+        public String name() {
+            return ExtractContextInSourceScriptEngine.NAME;
+        }
+
+        @Override
+        public String description() {
+            return "Mock script engine for " + UpdateIT.class;
+        }
+
+        public void onModule(ScriptModule module) {
+            module.addScriptEngine(ExtractContextInSourceScriptEngine.class);
+        }
+
+    }
+
+    public static class ExtractContextInSourceScriptEngine implements ScriptEngineService {
+
+        public static final String NAME = "extract_ctx";
+
+        @Override
+        public void close() throws IOException {
+        }
+
+        @Override
+        public String[] types() {
+            return new String[] { NAME };
+        }
+
+        @Override
+        public String[] extensions() {
+            return types();
+        }
+
+        @Override
+        public boolean sandboxed() {
+            return true;
+        }
+
+        @Override
+        public Object compile(String script) {
+            return new Object(); // unused
+        }
+
+        @Override
+        public ExecutableScript executable(CompiledScript compiledScript, Map<String, Object> params) {
+            return new ExecutableScript() {
+
+                Map<String, Object> vars = new HashMap<>();
+
+                @Override
+                public void setNextVar(String name, Object value) {
+                    vars.put(name, value);
+                }
+
+                @Override
+                public Object run() {
+                    Map<String, Object> ctx = (Map<String, Object>) vars.get("ctx");
+                    assertNotNull(ctx);
+
+                    Map<String, Object> source = (Map<String, Object>) ctx.get("_source");
+                    Map<String, Object> ctxWithoutSource = new HashMap<>(ctx);
+                    ctxWithoutSource.remove("_source");
+                    source.put("update_context", ctxWithoutSource);
+
+                    return ctx;
+                }
+
+                @Override
+                public Object unwrap(Object value) {
+                    return value;
+                }
+
+            };
+        }
+
+        @Override
+        public SearchScript search(CompiledScript compiledScript, SearchLookup lookup, Map<String, Object> vars) {
+            throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public void scriptRemoved(CompiledScript script) {
+        }
+
+    }
+
+    @Override
+    protected Collection<Class<? extends Plugin>> nodePlugins() {
+        return Arrays.asList(
+                PutFieldValuesScriptPlugin.class,
+                FieldIncrementScriptPlugin.class,
+                ScriptedUpsertScriptPlugin.class,
+                ExtractContextInSourceScriptPlugin.class);
+    }
+
+    private void createTestIndex() throws Exception {
+        logger.info("--> creating index test");
+
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
+                .addMapping("type1", XContentFactory.jsonBuilder()
+                        .startObject()
+                        .startObject("type1")
+                        .startObject("_timestamp").field("enabled", true).endObject()
+                        .startObject("_ttl").field("enabled", true).endObject()
+                        .endObject()
+                        .endObject()));
+    }
+
+    @Test
+    public void testUpsert() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                .execute().actionGet();
+        assertTrue(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("1"));
+        }
+
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                .execute().actionGet();
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("2"));
+        }
+    }
+
+    @Test
+    public void testScriptedUpsert() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        // Script logic is
+        // 1) New accounts take balance from "balance" in upsert doc and first payment is charged at 50%
+        // 2) Existing accounts subtract full payment from balance stored in elasticsearch
+
+        int openingBalance=10;
+
+        Map<String, Object> params = new HashMap<>();
+        params.put("payment", 2);
+
+        // Pay money from what will be a new account and opening balance comes from upsert doc
+        // provided by client
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("balance", openingBalance).endObject())
+                .setScriptedUpsert(true)
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "scripted_upsert", params))
+                .execute().actionGet();
+        assertTrue(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("balance").toString(), equalTo("9"));
+        }
+
+        // Now pay money for an existing account where balance is stored in es
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("balance", openingBalance).endObject())
+                .setScriptedUpsert(true)
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "scripted_upsert", params))
+                .execute().actionGet();
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("balance").toString(), equalTo("7"));
+        }
+    }
+
+    @Test
+    public void testUpsertDoc() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setDocAsUpsert(true)
+                .setFields("_source")
+                .execute().actionGet();
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
+    }
+
+    @Test
+    // See: https://github.com/elasticsearch/elasticsearch/issues/3265
+    public void testNotUpsertDoc() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        assertThrows(client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setDocAsUpsert(false)
+                .setFields("_source")
+                .execute(), DocumentMissingException.class);
+    }
+
+    @Test
+    public void testUpsertFields() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("extra", "foo")))
+                .setFields("_source")
+                .execute().actionGet();
+
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra"), nullValue());
+
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("extra", "foo")))
+                .setFields("_source")
+                .execute().actionGet();
+
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra").toString(), equalTo("foo"));
+    }
+
+    @Test
+    public void testVersionedUpdate() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias")));
+        ensureGreen();
+
+        index("test", "type", "1", "text", "value"); // version is now 1
+
+        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1")
+                        .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2"))).setVersion(2)
+                        .execute(),
+                VersionConflictEngineException.class);
+
+        client().prepareUpdate(indexOrAlias(), "type", "1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2"))).setVersion(1).get();
+        assertThat(client().prepareGet("test", "type", "1").get().getVersion(), equalTo(2l));
+
+        // and again with a higher version..
+        client().prepareUpdate(indexOrAlias(), "type", "1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v3"))).setVersion(2).get();
+
+        assertThat(client().prepareGet("test", "type", "1").get().getVersion(), equalTo(3l));
+
+        // after delete
+        client().prepareDelete("test", "type", "1").get();
+        assertThrows(client().prepareUpdate("test", "type", "1")
+                        .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2"))).setVersion(3)
+                        .execute(),
+                DocumentMissingException.class);
+
+        // external versioning
+        client().prepareIndex("test", "type", "2").setSource("text", "value").setVersion(10).setVersionType(VersionType.EXTERNAL).get();
+
+        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "2")
+                        .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2"))).setVersion(2)
+                        .setVersionType(VersionType.EXTERNAL).execute(),
+                ActionRequestValidationException.class);
+
+
+        // With force version
+        client().prepareUpdate(indexOrAlias(), "type", "2")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE,  "put_values", Collections.singletonMap("text", "v10")))
+                .setVersion(10).setVersionType(VersionType.FORCE).get();
+
+        GetResponse get = get("test", "type", "2");
+        assertThat(get.getVersion(), equalTo(10l));
+        assertThat((String) get.getSource().get("text"), equalTo("v10"));
+
+        // upserts - the combination with versions is a bit weird. Test are here to ensure we do not change our behavior unintentionally
+
+        // With internal versions, tt means "if object is there with version X, update it or explode. If it is not there, index.
+        client().prepareUpdate(indexOrAlias(), "type", "3")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("text", "v2")))
+                .setVersion(10).setUpsert("{ \"text\": \"v0\" }").get();
+        get = get("test", "type", "3");
+        assertThat(get.getVersion(), equalTo(1l));
+        assertThat((String) get.getSource().get("text"), equalTo("v0"));
+
+        // retry on conflict is rejected:
+        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1").setVersion(10).setRetryOnConflict(5), ActionRequestValidationException.class);
+    }
+
+    @Test
+    public void testIndexAutoCreation() throws Exception {
+        UpdateResponse updateResponse = client().prepareUpdate("test", "type1", "1")
+                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("extra", "foo")))
+                .setFields("_source")
+                .execute().actionGet();
+
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
+        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra"), nullValue());
+    }
+
+    @Test
+    public void testUpdate() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        try {
+            client().prepareUpdate(indexOrAlias(), "type1", "1")
+                    .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null)).execute().actionGet();
+            fail();
+        } catch (DocumentMissingException e) {
+            // all is well
+        }
+
+        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
+
+        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null)).execute().actionGet();
+        assertThat(updateResponse.getVersion(), equalTo(2L));
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("2"));
+        }
+
+        Map<String, Object> params = new HashMap<>();
+        params.put("inc", 3);
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", params)).execute().actionGet();
+        assertThat(updateResponse.getVersion(), equalTo(3L));
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("5"));
+        }
+
+        // check noop
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("_ctx", Collections.singletonMap("op", "none")))).execute().actionGet();
+        assertThat(updateResponse.getVersion(), equalTo(3L));
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("5"));
+        }
+
+        // check delete
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("_ctx", Collections.singletonMap("op", "delete")))).execute().actionGet();
+        assertThat(updateResponse.getVersion(), equalTo(4L));
+        assertFalse(updateResponse.isCreated());
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+
+        for (int i = 0; i < 5; i++) {
+            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.isExists(), equalTo(false));
+        }
+
+        // check TTL is kept after an update without TTL
+        client().prepareIndex("test", "type1", "2").setSource("field", 1).setTTL(86400000L).setRefresh(true).execute().actionGet();
+        GetResponse getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
+        long ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
+        assertThat(ttl, greaterThan(0L));
+        client().prepareUpdate(indexOrAlias(), "type1", "2")
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null)).execute().actionGet();
+        getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
+        ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
+        assertThat(ttl, greaterThan(0L));
+
+        // check TTL update
+        client().prepareUpdate(indexOrAlias(), "type1", "2")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("_ctx", Collections.singletonMap("_ttl", 3600000)))).execute().actionGet();
+        getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
+        ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
+        assertThat(ttl, greaterThan(0L));
+        assertThat(ttl, lessThanOrEqualTo(3600000L));
+
+        // check timestamp update
+        client().prepareIndex("test", "type1", "3").setSource("field", 1).setRefresh(true).execute().actionGet();
+        client().prepareUpdate(indexOrAlias(), "type1", "3")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "put_values", Collections.singletonMap("_ctx", Collections.singletonMap("_timestamp", "2009-11-15T14:12:12")))).execute()
+                .actionGet();
+        getResponse = client().prepareGet("test", "type1", "3").setFields("_timestamp").execute().actionGet();
+        long timestamp = ((Number) getResponse.getField("_timestamp").getValue()).longValue();
+        assertThat(timestamp, equalTo(1258294332000L));
+
+        // check fields parameter
+        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
+                .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null)).setFields("_source", "field")
+                .execute().actionGet();
+        assertThat(updateResponse.getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult(), notNullValue());
+        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
+        assertThat(updateResponse.getGetResult().sourceRef(), notNullValue());
+        assertThat(updateResponse.getGetResult().field("field").getValue(), notNullValue());
+
+        // check updates without script
+        // add new field
+        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("field2", 2).endObject()).execute().actionGet();
+        for (int i = 0; i < 5; i++) {
+            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("1"));
+            assertThat(getResponse.getSourceAsMap().get("field2").toString(), equalTo("2"));
+        }
+
+        // change existing field
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("field", 3).endObject()).execute().actionGet();
+        for (int i = 0; i < 5; i++) {
+            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("3"));
+            assertThat(getResponse.getSourceAsMap().get("field2").toString(), equalTo("2"));
+        }
+
+        // recursive map
+        Map<String, Object> testMap = new HashMap<>();
+        Map<String, Object> testMap2 = new HashMap<>();
+        Map<String, Object> testMap3 = new HashMap<>();
+        testMap3.put("commonkey", testMap);
+        testMap3.put("map3", 5);
+        testMap2.put("map2", 6);
+        testMap.put("commonkey", testMap2);
+        testMap.put("map1", 8);
+
+        client().prepareIndex("test", "type1", "1").setSource("map", testMap).execute().actionGet();
+        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("map", testMap3).endObject()).execute().actionGet();
+        for (int i = 0; i < 5; i++) {
+            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
+            Map map1 = (Map) getResponse.getSourceAsMap().get("map");
+            assertThat(map1.size(), equalTo(3));
+            assertThat(map1.containsKey("map1"), equalTo(true));
+            assertThat(map1.containsKey("map3"), equalTo(true));
+            assertThat(map1.containsKey("commonkey"), equalTo(true));
+            Map map2 = (Map) map1.get("commonkey");
+            assertThat(map2.size(), equalTo(3));
+            assertThat(map2.containsKey("map1"), equalTo(true));
+            assertThat(map2.containsKey("map2"), equalTo(true));
+            assertThat(map2.containsKey("commonkey"), equalTo(true));
+        }
+    }
+
+    @Test
+    public void testUpdateRequestWithBothScriptAndDoc() throws Exception {
+        createTestIndex();
+        ensureGreen();
+
+        try {
+            client().prepareUpdate(indexOrAlias(), "type1", "1")
+                    .setDoc(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
+                    .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                    .execute().actionGet();
+            fail("Should have thrown ActionRequestValidationException");
+        } catch (ActionRequestValidationException e) {
+            assertThat(e.validationErrors().size(), equalTo(1));
+            assertThat(e.validationErrors().get(0), containsString("can't provide both script and doc"));
+            assertThat(e.getMessage(), containsString("can't provide both script and doc"));
+        }
+    }
+
+    @Test
+    public void testUpdateRequestWithScriptAndShouldUpsertDoc() throws Exception {
+        createTestIndex();
+        ensureGreen();
+        try {
+            client().prepareUpdate(indexOrAlias(), "type1", "1")
+                    .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                    .setDocAsUpsert(true)
+                    .execute().actionGet();
+            fail("Should have thrown ActionRequestValidationException");
+        } catch (ActionRequestValidationException e) {
+            assertThat(e.validationErrors().size(), equalTo(1));
+            assertThat(e.validationErrors().get(0), containsString("doc must be specified if doc_as_upsert is enabled"));
+            assertThat(e.getMessage(), containsString("doc must be specified if doc_as_upsert is enabled"));
+        }
+    }
+
+    @Test
+    public void testContextVariables() throws Exception {
+        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
+                        .addMapping("type1", XContentFactory.jsonBuilder()
+                                .startObject()
+                                .startObject("type1")
+                                .startObject("_timestamp").field("enabled", true).endObject()
+                                .startObject("_ttl").field("enabled", true).endObject()
+                                .endObject()
+                                .endObject())
+                        .addMapping("subtype1", XContentFactory.jsonBuilder()
+                                .startObject()
+                                .startObject("subtype1")
+                                .startObject("_parent").field("type", "type1").endObject()
+                                .startObject("_timestamp").field("enabled", true).endObject()
+                                .startObject("_ttl").field("enabled", true).endObject()
+                                .endObject()
+                                .endObject())
+        );
+        ensureGreen();
+
+        // Index some documents
+        long timestamp = System.currentTimeMillis();
+        client().prepareIndex()
+                .setIndex("test")
+                .setType("type1")
+                .setId("parentId1")
+                .setTimestamp(String.valueOf(timestamp-1))
+                .setSource("field1", 0, "content", "bar")
+                .execute().actionGet();
+
+        long ttl = 10000;
+        client().prepareIndex()
+                .setIndex("test")
+                .setType("subtype1")
+                .setId("id1")
+                .setParent("parentId1")
+                .setRouting("routing1")
+                .setTimestamp(String.valueOf(timestamp))
+                .setTTL(ttl)
+                .setSource("field1", 1, "content", "foo")
+                .execute().actionGet();
+
+        // Update the first object and note context variables values
+        UpdateResponse updateResponse = client().prepareUpdate("test", "subtype1", "id1")
+                .setRouting("routing1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "extract_ctx", null))
+                .execute().actionGet();
+
+        assertEquals(2, updateResponse.getVersion());
+
+        GetResponse getResponse = client().prepareGet("test", "subtype1", "id1").setRouting("routing1").execute().actionGet();
+        Map<String, Object> updateContext = (Map<String, Object>) getResponse.getSourceAsMap().get("update_context");
+        assertEquals("test", updateContext.get("_index"));
+        assertEquals("subtype1", updateContext.get("_type"));
+        assertEquals("id1", updateContext.get("_id"));
+        assertEquals(1, updateContext.get("_version"));
+        assertEquals("parentId1", updateContext.get("_parent"));
+        assertEquals("routing1", updateContext.get("_routing"));
+        assertThat(((Integer) updateContext.get("_ttl")).longValue(), allOf(greaterThanOrEqualTo(ttl-3000), lessThanOrEqualTo(ttl)));
+
+        // Idem with the second object
+        updateResponse = client().prepareUpdate("test", "type1", "parentId1")
+                .setScript(new Script("", ScriptService.ScriptType.INLINE, "extract_ctx", null))
+                .execute().actionGet();
+
+        assertEquals(2, updateResponse.getVersion());
+
+        getResponse = client().prepareGet("test", "type1", "parentId1").execute().actionGet();
+        updateContext = (Map<String, Object>) getResponse.getSourceAsMap().get("update_context");
+        assertEquals("test", updateContext.get("_index"));
+        assertEquals("type1", updateContext.get("_type"));
+        assertEquals("parentId1", updateContext.get("_id"));
+        assertEquals(1, updateContext.get("_version"));
+        assertNull(updateContext.get("_parent"));
+        assertNull(updateContext.get("_routing"));
+        assertNull(updateContext.get("_ttl"));
+    }
+
+    @Test
+    public void testConcurrentUpdateWithRetryOnConflict() throws Exception {
+        final boolean useBulkApi = randomBoolean();
+        createTestIndex();
+        ensureGreen();
+
+        int numberOfThreads = scaledRandomIntBetween(2,5);
+        final CountDownLatch latch = new CountDownLatch(numberOfThreads);
+        final CountDownLatch startLatch = new CountDownLatch(1);
+        final int numberOfUpdatesPerThread = scaledRandomIntBetween(100, 10000);
+        final List<Throwable> failures = new CopyOnWriteArrayList<>();
+        for (int i = 0; i < numberOfThreads; i++) {
+            Runnable r = new Runnable() {
+
+                @Override
+                public void run() {
+                    try {
+                        startLatch.await();
+                        for (int i = 0; i < numberOfUpdatesPerThread; i++) {
+                            if (useBulkApi) {
+                                UpdateRequestBuilder updateRequestBuilder = client().prepareUpdate(indexOrAlias(), "type1", Integer.toString(i))
+                                        .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                                        .setRetryOnConflict(Integer.MAX_VALUE)
+                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject());
+                                client().prepareBulk().add(updateRequestBuilder).execute().actionGet();
+                            } else {
+                                client().prepareUpdate(indexOrAlias(), "type1", Integer.toString(i))
+                                        .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                                        .setRetryOnConflict(Integer.MAX_VALUE)
+                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
+                                        .execute().actionGet();
+                            }
+                        }
+                    } catch (Throwable e) {
+                        failures.add(e);
+                    } finally {
+                        latch.countDown();
+                    }
+                }
+
+            };
+            new Thread(r).start();
+        }
+        startLatch.countDown();
+        latch.await();
+        for (Throwable throwable : failures) {
+            logger.info("Captured failure on concurrent update:", throwable);
+        }
+        assertThat(failures.size(), equalTo(0));
+        for (int i = 0; i < numberOfUpdatesPerThread; i++) {
+            GetResponse response = client().prepareGet("test", "type1", Integer.toString(i)).execute().actionGet();
+            assertThat(response.getId(), equalTo(Integer.toString(i)));
+            assertThat(response.isExists(), equalTo(true));
+            assertThat(response.getVersion(), equalTo((long) numberOfThreads));
+            assertThat((Integer) response.getSource().get("field"), equalTo(numberOfThreads));
+        }
+    }
+
+    @Test
+    public void stressUpdateDeleteConcurrency() throws Exception {
+        //We create an index with merging disabled so that deletes don't get merged away
+        assertAcked(prepareCreate("test")
+                .addMapping("type1", XContentFactory.jsonBuilder()
+                        .startObject()
+                        .startObject("type1")
+                        .startObject("_timestamp").field("enabled", true).endObject()
+                        .startObject("_ttl").field("enabled", true).endObject()
+                        .endObject()
+                        .endObject())
+                .setSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)));
+        ensureGreen();
+
+        final int numberOfThreads = scaledRandomIntBetween(3,5);
+        final int numberOfIdsPerThread = scaledRandomIntBetween(3,10);
+        final int numberOfUpdatesPerId = scaledRandomIntBetween(10,100);
+        final int retryOnConflict = randomIntBetween(0,1);
+        final CountDownLatch latch = new CountDownLatch(numberOfThreads);
+        final CountDownLatch startLatch = new CountDownLatch(1);
+        final List<Throwable> failures = new CopyOnWriteArrayList<>();
+
+        final class UpdateThread extends Thread {
+            final Map<Integer,Integer> failedMap = new HashMap<>();
+            final int numberOfIds;
+            final int updatesPerId;
+            final int maxUpdateRequests = numberOfIdsPerThread*numberOfUpdatesPerId;
+            final int maxDeleteRequests = numberOfIdsPerThread*numberOfUpdatesPerId;
+            private final Semaphore updateRequestsOutstanding = new Semaphore(maxUpdateRequests);
+            private final Semaphore deleteRequestsOutstanding = new Semaphore(maxDeleteRequests);
+
+            public UpdateThread(int numberOfIds, int updatesPerId) {
+                this.numberOfIds = numberOfIds;
+                this.updatesPerId = updatesPerId;
+            }
+
+            final class UpdateListener implements ActionListener<UpdateResponse> {
+                int id;
+
+                public UpdateListener(int id) {
+                    this.id = id;
+                }
+
+                @Override
+                public void onResponse(UpdateResponse updateResponse) {
+                    updateRequestsOutstanding.release(1);
+                }
+
+                @Override
+                public void onFailure(Throwable e) {
+                    synchronized (failedMap) {
+                        incrementMapValue(id, failedMap);
+                    }
+                    updateRequestsOutstanding.release(1);
+                }
+
+            }
+
+            final class DeleteListener implements ActionListener<DeleteResponse> {
+                int id;
+
+                public DeleteListener(int id) {
+                    this.id = id;
+                }
+
+                @Override
+                public void onResponse(DeleteResponse deleteResponse) {
+                    deleteRequestsOutstanding.release(1);
+                }
+
+                @Override
+                public void onFailure(Throwable e) {
+                    synchronized (failedMap) {
+                        incrementMapValue(id, failedMap);
+                    }
+                    deleteRequestsOutstanding.release(1);
+                }
+            }
+
+            @Override
+            public void run(){
+                try {
+                    startLatch.await();
+                    boolean hasWaitedForNoNode = false;
+                    for (int j = 0; j < numberOfIds; j++) {
+                        for (int k = 0; k < numberOfUpdatesPerId; ++k) {
+                            updateRequestsOutstanding.acquire();
+                            try {
+                                UpdateRequest ur = client().prepareUpdate("test", "type1", Integer.toString(j))
+                                        .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                                        .setRetryOnConflict(retryOnConflict)
+                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
+                                        .request();
+                                client().update(ur, new UpdateListener(j));
+                            } catch (NoNodeAvailableException nne) {
+                                updateRequestsOutstanding.release();
+                                synchronized (failedMap) {
+                                    incrementMapValue(j, failedMap);
+                                }
+                                if (hasWaitedForNoNode) {
+                                    throw nne;
+                                }
+                                logger.warn("Got NoNodeException waiting for 1 second for things to recover.");
+                                hasWaitedForNoNode = true;
+                                Thread.sleep(1000);
+                            }
+
+                            try {
+                                deleteRequestsOutstanding.acquire();
+                                DeleteRequest dr = client().prepareDelete("test", "type1", Integer.toString(j)).request();
+                                client().delete(dr, new DeleteListener(j));
+                            } catch (NoNodeAvailableException nne) {
+                                deleteRequestsOutstanding.release();
+                                synchronized (failedMap) {
+                                    incrementMapValue(j, failedMap);
+                                }
+                                if (hasWaitedForNoNode) {
+                                    throw nne;
+                                }
+                                logger.warn("Got NoNodeException waiting for 1 second for things to recover.");
+                                hasWaitedForNoNode = true;
+                                Thread.sleep(1000); //Wait for no-node to clear
+                            }
+                        }
+                    }
+                } catch (Throwable e) {
+                    logger.error("Something went wrong", e);
+                    failures.add(e);
+                } finally {
+                    try {
+                        waitForOutstandingRequests(TimeValue.timeValueSeconds(60), updateRequestsOutstanding, maxUpdateRequests, "Update");
+                        waitForOutstandingRequests(TimeValue.timeValueSeconds(60), deleteRequestsOutstanding, maxDeleteRequests, "Delete");
+                    } catch (ElasticsearchTimeoutException ete) {
+                        failures.add(ete);
+                    }
+                    latch.countDown();
+                }
+            }
+
+            private void incrementMapValue(int j, Map<Integer,Integer> map) {
+                if (!map.containsKey(j)) {
+                    map.put(j, 0);
+                }
+                map.put(j, map.get(j) + 1);
+            }
+
+            private void waitForOutstandingRequests(TimeValue timeOut, Semaphore requestsOutstanding, int maxRequests, String name) {
+                long start = System.currentTimeMillis();
+                do {
+                    long msRemaining = timeOut.getMillis() - (System.currentTimeMillis() - start);
+                    logger.info("[{}] going to try and acquire [{}] in [{}]ms [{}] available to acquire right now",name, maxRequests,msRemaining, requestsOutstanding.availablePermits());
+                    try {
+                        requestsOutstanding.tryAcquire(maxRequests, msRemaining, TimeUnit.MILLISECONDS );
+                        return;
+                    } catch (InterruptedException ie) {
+                        //Just keep swimming
+                    }
+                } while ((System.currentTimeMillis() - start) < timeOut.getMillis());
+                throw new ElasticsearchTimeoutException("Requests were still outstanding after the timeout [" + timeOut + "] for type [" + name + "]" );
+            }
+        }
+        final List<UpdateThread> threads = new ArrayList<>();
+
+        for (int i = 0; i < numberOfThreads; i++) {
+            UpdateThread ut = new UpdateThread(numberOfIdsPerThread, numberOfUpdatesPerId);
+            ut.start();
+            threads.add(ut);
+        }
+
+        startLatch.countDown();
+        latch.await();
+
+        for (UpdateThread ut : threads){
+            ut.join(); //Threads should have finished because of the latch.await
+        }
+
+        //If are no errors every request received a response otherwise the test would have timedout
+        //aquiring the request outstanding semaphores.
+        for (Throwable throwable : failures) {
+            logger.info("Captured failure on concurrent update:", throwable);
+        }
+
+        assertThat(failures.size(), equalTo(0));
+
+        //Upsert all the ids one last time to make sure they are available at get time
+        //This means that we add 1 to the expected versions and attempts
+        //All the previous operations should be complete or failed at this point
+        for (int i = 0; i < numberOfIdsPerThread; ++i) {
+            UpdateResponse ur = client().prepareUpdate("test", "type1", Integer.toString(i))
+                    .setScript(new Script("field", ScriptService.ScriptType.INLINE, "field_inc", null))
+                .setRetryOnConflict(Integer.MAX_VALUE)
+                .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
+                .execute().actionGet();
+        }
+
+        refresh();
+
+        for (int i = 0; i < numberOfIdsPerThread; ++i) {
+            int totalFailures = 0;
+            GetResponse response = client().prepareGet("test", "type1", Integer.toString(i)).execute().actionGet();
+            if (response.isExists()) {
+                assertThat(response.getId(), equalTo(Integer.toString(i)));
+                int expectedVersion = (numberOfThreads * numberOfUpdatesPerId * 2) + 1;
+                for (UpdateThread ut : threads) {
+                    if (ut.failedMap.containsKey(i)) {
+                        totalFailures += ut.failedMap.get(i);
+                    }
+                }
+                expectedVersion -= totalFailures;
+                logger.error("Actual version [{}] Expected version [{}] Total failures [{}]", response.getVersion(), expectedVersion, totalFailures);
+                assertThat(response.getVersion(), equalTo((long) expectedVersion));
+                assertThat(response.getVersion() + totalFailures,
+                        equalTo(
+                                (long)((numberOfUpdatesPerId * numberOfThreads * 2) + 1)
+                ));
+            }
+        }
+    }
+
+    private static String indexOrAlias() {
+        return randomBoolean() ? "test" : "alias";
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java b/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
index b22dff7..10812c1 100644
--- a/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
+++ b/core/src/test/java/org/elasticsearch/validate/RenderSearchTemplateIT.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.validate;
 
-import org.elasticsearch.action.admin.indices.validate.template.RenderSearchTemplateResponse;
+import org.elasticsearch.action.admin.cluster.validate.template.RenderSearchTemplateResponse;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.settings.Settings;
@@ -61,7 +61,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "bar");
         params.put("size", 20);
         Template template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         BytesReference source = response.source();
         assertThat(source, notNullValue());
@@ -75,7 +75,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "baz");
         params.put("size", 100);
         template = new Template(TEMPLATE_CONTENTS, ScriptType.INLINE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         source = response.source();
         assertThat(source, notNullValue());
@@ -91,7 +91,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "bar");
         params.put("size", 20);
         Template template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         BytesReference source = response.source();
         assertThat(source, notNullValue());
@@ -105,7 +105,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "baz");
         params.put("size", 100);
         template = new Template("index_template_1", ScriptType.INDEXED, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         source = response.source();
         assertThat(source, notNullValue());
@@ -121,7 +121,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "bar");
         params.put("size", 20);
         Template template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        RenderSearchTemplateResponse response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        RenderSearchTemplateResponse response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         BytesReference source = response.source();
         assertThat(source, notNullValue());
@@ -135,7 +135,7 @@ public class RenderSearchTemplateIT extends ESIntegTestCase {
         params.put("value", "baz");
         params.put("size", 100);
         template = new Template("file_template_1", ScriptType.FILE, MustacheScriptEngineService.NAME, XContentType.JSON, params);
-        response = client().admin().indices().prepareRenderSearchTemplate().template(template).get();
+        response = client().admin().cluster().prepareRenderSearchTemplate().template(template).get();
         assertThat(response, notNullValue());
         source = response.source();
         assertThat(source, notNullValue());
diff --git a/core/src/test/resources/indices/bwc/index-0.90.0.Beta1.zip b/core/src/test/resources/indices/bwc/index-0.90.0.Beta1.zip
deleted file mode 100644
index 5bbdea4..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.0.Beta1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.0.RC1.zip b/core/src/test/resources/indices/bwc/index-0.90.0.RC1.zip
deleted file mode 100644
index d9072ce..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.0.RC1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.0.RC2.zip b/core/src/test/resources/indices/bwc/index-0.90.0.RC2.zip
deleted file mode 100644
index dce299b..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.0.RC2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.0.zip b/core/src/test/resources/indices/bwc/index-0.90.0.zip
deleted file mode 100644
index 3ec908d..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.1.zip b/core/src/test/resources/indices/bwc/index-0.90.1.zip
deleted file mode 100644
index 67db98f..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.10.zip b/core/src/test/resources/indices/bwc/index-0.90.10.zip
deleted file mode 100644
index 6bdb9f2..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.10.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.11.zip b/core/src/test/resources/indices/bwc/index-0.90.11.zip
deleted file mode 100644
index b5253f9..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.11.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.12.zip b/core/src/test/resources/indices/bwc/index-0.90.12.zip
deleted file mode 100644
index 0392049..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.12.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.13.zip b/core/src/test/resources/indices/bwc/index-0.90.13.zip
deleted file mode 100644
index 025b4c3..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.13.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.2.zip b/core/src/test/resources/indices/bwc/index-0.90.2.zip
deleted file mode 100644
index 413e08e..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.3.zip b/core/src/test/resources/indices/bwc/index-0.90.3.zip
deleted file mode 100644
index c31d4de..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.4.zip b/core/src/test/resources/indices/bwc/index-0.90.4.zip
deleted file mode 100644
index 8b07a92..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.5.zip b/core/src/test/resources/indices/bwc/index-0.90.5.zip
deleted file mode 100644
index dfd0fd0..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.6.zip b/core/src/test/resources/indices/bwc/index-0.90.6.zip
deleted file mode 100644
index 1f3cff2..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.6.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.7.zip b/core/src/test/resources/indices/bwc/index-0.90.7.zip
deleted file mode 100644
index 6d0e65c..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.7.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.8.zip b/core/src/test/resources/indices/bwc/index-0.90.8.zip
deleted file mode 100644
index 8ff8ac3..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.8.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-0.90.9.zip b/core/src/test/resources/indices/bwc/index-0.90.9.zip
deleted file mode 100644
index 4445b39..0000000
Binary files a/core/src/test/resources/indices/bwc/index-0.90.9.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.Beta1.zip b/core/src/test/resources/indices/bwc/index-1.0.0.Beta1.zip
deleted file mode 100644
index 167dde8..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.Beta1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.Beta2.zip b/core/src/test/resources/indices/bwc/index-1.0.0.Beta2.zip
deleted file mode 100644
index 95fbfef..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.Beta2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.RC1.zip b/core/src/test/resources/indices/bwc/index-1.0.0.RC1.zip
deleted file mode 100644
index 3ced97a..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.RC1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.RC2.zip b/core/src/test/resources/indices/bwc/index-1.0.0.RC2.zip
deleted file mode 100644
index 1298cfb..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.RC2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.0.zip b/core/src/test/resources/indices/bwc/index-1.0.0.zip
deleted file mode 100644
index 2cb9abc..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.1.zip b/core/src/test/resources/indices/bwc/index-1.0.1.zip
deleted file mode 100644
index 844271b..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.2.zip b/core/src/test/resources/indices/bwc/index-1.0.2.zip
deleted file mode 100644
index dd8e393..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.0.3.zip b/core/src/test/resources/indices/bwc/index-1.0.3.zip
deleted file mode 100644
index e4437ef..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.0.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.1.0.zip b/core/src/test/resources/indices/bwc/index-1.1.0.zip
deleted file mode 100644
index 4f05370..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.1.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.1.1.zip b/core/src/test/resources/indices/bwc/index-1.1.1.zip
deleted file mode 100644
index effeb94..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.1.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.1.2.zip b/core/src/test/resources/indices/bwc/index-1.1.2.zip
deleted file mode 100644
index bedffa4..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.1.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.0.zip b/core/src/test/resources/indices/bwc/index-1.2.0.zip
deleted file mode 100644
index 4644a38..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.1.zip b/core/src/test/resources/indices/bwc/index-1.2.1.zip
deleted file mode 100644
index 553b46b..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.2.zip b/core/src/test/resources/indices/bwc/index-1.2.2.zip
deleted file mode 100644
index 3f51a47..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.3.zip b/core/src/test/resources/indices/bwc/index-1.2.3.zip
deleted file mode 100644
index 8c8bfbd..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.2.4.zip b/core/src/test/resources/indices/bwc/index-1.2.4.zip
deleted file mode 100644
index e3a1519..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.2.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.0.zip b/core/src/test/resources/indices/bwc/index-1.3.0.zip
deleted file mode 100644
index d98958d..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.1.zip b/core/src/test/resources/indices/bwc/index-1.3.1.zip
deleted file mode 100644
index 167d0f4..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.2.zip b/core/src/test/resources/indices/bwc/index-1.3.2.zip
deleted file mode 100644
index 756eaf6..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.3.zip b/core/src/test/resources/indices/bwc/index-1.3.3.zip
deleted file mode 100644
index 8470dee..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.4.zip b/core/src/test/resources/indices/bwc/index-1.3.4.zip
deleted file mode 100644
index 2175012..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.5.zip b/core/src/test/resources/indices/bwc/index-1.3.5.zip
deleted file mode 100644
index 19d1e56..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.6.zip b/core/src/test/resources/indices/bwc/index-1.3.6.zip
deleted file mode 100644
index ad8e8bd..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.6.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.7.zip b/core/src/test/resources/indices/bwc/index-1.3.7.zip
deleted file mode 100644
index 3a645a9..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.7.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.8.zip b/core/src/test/resources/indices/bwc/index-1.3.8.zip
deleted file mode 100644
index f8ab0a2..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.8.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.3.9.zip b/core/src/test/resources/indices/bwc/index-1.3.9.zip
deleted file mode 100644
index 5ef35b2..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.3.9.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.0.Beta1.zip b/core/src/test/resources/indices/bwc/index-1.4.0.Beta1.zip
deleted file mode 100644
index 4546f5d..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.0.Beta1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.0.zip b/core/src/test/resources/indices/bwc/index-1.4.0.zip
deleted file mode 100644
index 467d19a..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.1.zip b/core/src/test/resources/indices/bwc/index-1.4.1.zip
deleted file mode 100644
index 2adbb28..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.2.zip b/core/src/test/resources/indices/bwc/index-1.4.2.zip
deleted file mode 100644
index 4fac208..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.3.zip b/core/src/test/resources/indices/bwc/index-1.4.3.zip
deleted file mode 100644
index 1a0d667..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.4.zip b/core/src/test/resources/indices/bwc/index-1.4.4.zip
deleted file mode 100644
index 0328a9e..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.4.5.zip b/core/src/test/resources/indices/bwc/index-1.4.5.zip
deleted file mode 100644
index eeb25ab..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.4.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.5.0.zip b/core/src/test/resources/indices/bwc/index-1.5.0.zip
deleted file mode 100644
index f1dab08..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.5.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.5.1.zip b/core/src/test/resources/indices/bwc/index-1.5.1.zip
deleted file mode 100644
index 342e311..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.5.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.5.2.zip b/core/src/test/resources/indices/bwc/index-1.5.2.zip
deleted file mode 100644
index fb36b19..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.5.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.6.0.zip b/core/src/test/resources/indices/bwc/index-1.6.0.zip
deleted file mode 100644
index 02a5806..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.6.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.6.1.zip b/core/src/test/resources/indices/bwc/index-1.6.1.zip
deleted file mode 100644
index 04820f9..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.6.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.6.2.zip b/core/src/test/resources/indices/bwc/index-1.6.2.zip
deleted file mode 100644
index af6ce56..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.6.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.7.0.zip b/core/src/test/resources/indices/bwc/index-1.7.0.zip
deleted file mode 100644
index 941be64..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.7.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.7.1.zip b/core/src/test/resources/indices/bwc/index-1.7.1.zip
deleted file mode 100644
index debd797..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.7.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/index-1.7.2.zip b/core/src/test/resources/indices/bwc/index-1.7.2.zip
deleted file mode 100644
index 18bb6c7..0000000
Binary files a/core/src/test/resources/indices/bwc/index-1.7.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.0.Beta2.zip b/core/src/test/resources/indices/bwc/repo-1.0.0.Beta2.zip
deleted file mode 100644
index 020f6f4..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.0.Beta2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.0.RC1.zip b/core/src/test/resources/indices/bwc/repo-1.0.0.RC1.zip
deleted file mode 100644
index a84c507..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.0.RC1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.0.RC2.zip b/core/src/test/resources/indices/bwc/repo-1.0.0.RC2.zip
deleted file mode 100644
index e5d65eb..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.0.RC2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.0.zip b/core/src/test/resources/indices/bwc/repo-1.0.0.zip
deleted file mode 100644
index 13f778d..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.1.zip b/core/src/test/resources/indices/bwc/repo-1.0.1.zip
deleted file mode 100644
index 76ed278..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.2.zip b/core/src/test/resources/indices/bwc/repo-1.0.2.zip
deleted file mode 100644
index 762eabf..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.0.3.zip b/core/src/test/resources/indices/bwc/repo-1.0.3.zip
deleted file mode 100644
index ba79dea..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.0.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.1.0.zip b/core/src/test/resources/indices/bwc/repo-1.1.0.zip
deleted file mode 100644
index cbf84c7..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.1.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.1.1.zip b/core/src/test/resources/indices/bwc/repo-1.1.1.zip
deleted file mode 100644
index 00ea044..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.1.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.1.2.zip b/core/src/test/resources/indices/bwc/repo-1.1.2.zip
deleted file mode 100644
index 18abd99..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.1.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.0.zip b/core/src/test/resources/indices/bwc/repo-1.2.0.zip
deleted file mode 100644
index f5e62a1..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.1.zip b/core/src/test/resources/indices/bwc/repo-1.2.1.zip
deleted file mode 100644
index 935e71b..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.2.zip b/core/src/test/resources/indices/bwc/repo-1.2.2.zip
deleted file mode 100644
index d69b22a..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.3.zip b/core/src/test/resources/indices/bwc/repo-1.2.3.zip
deleted file mode 100644
index 295f9f7..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.2.4.zip b/core/src/test/resources/indices/bwc/repo-1.2.4.zip
deleted file mode 100644
index e9efc00..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.2.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.0.zip b/core/src/test/resources/indices/bwc/repo-1.3.0.zip
deleted file mode 100644
index 5a59e21..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.1.zip b/core/src/test/resources/indices/bwc/repo-1.3.1.zip
deleted file mode 100644
index 2ae1d7c..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.2.zip b/core/src/test/resources/indices/bwc/repo-1.3.2.zip
deleted file mode 100644
index c67b997..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.3.zip b/core/src/test/resources/indices/bwc/repo-1.3.3.zip
deleted file mode 100644
index 64e3235..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.4.zip b/core/src/test/resources/indices/bwc/repo-1.3.4.zip
deleted file mode 100644
index 55e6744..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.5.zip b/core/src/test/resources/indices/bwc/repo-1.3.5.zip
deleted file mode 100644
index 35a5fd4..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.6.zip b/core/src/test/resources/indices/bwc/repo-1.3.6.zip
deleted file mode 100644
index f1eb21c..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.6.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.7.zip b/core/src/test/resources/indices/bwc/repo-1.3.7.zip
deleted file mode 100644
index 543c13c..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.7.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.8.zip b/core/src/test/resources/indices/bwc/repo-1.3.8.zip
deleted file mode 100644
index 93abac3..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.8.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.3.9.zip b/core/src/test/resources/indices/bwc/repo-1.3.9.zip
deleted file mode 100644
index 7dc194d..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.3.9.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.0.Beta1.zip b/core/src/test/resources/indices/bwc/repo-1.4.0.Beta1.zip
deleted file mode 100644
index 5adf788..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.0.Beta1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.0.zip b/core/src/test/resources/indices/bwc/repo-1.4.0.zip
deleted file mode 100644
index 21f867c..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.1.zip b/core/src/test/resources/indices/bwc/repo-1.4.1.zip
deleted file mode 100644
index 18166ea..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.2.zip b/core/src/test/resources/indices/bwc/repo-1.4.2.zip
deleted file mode 100644
index f03625a..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.3.zip b/core/src/test/resources/indices/bwc/repo-1.4.3.zip
deleted file mode 100644
index d78fbb1..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.3.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.4.zip b/core/src/test/resources/indices/bwc/repo-1.4.4.zip
deleted file mode 100644
index 1e89446..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.4.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.4.5.zip b/core/src/test/resources/indices/bwc/repo-1.4.5.zip
deleted file mode 100644
index fcae439..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.4.5.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.5.0.zip b/core/src/test/resources/indices/bwc/repo-1.5.0.zip
deleted file mode 100644
index a55e6c4..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.5.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.5.1.zip b/core/src/test/resources/indices/bwc/repo-1.5.1.zip
deleted file mode 100644
index 7cccbbe..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.5.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.5.2.zip b/core/src/test/resources/indices/bwc/repo-1.5.2.zip
deleted file mode 100644
index cee5783..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.5.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.6.0.zip b/core/src/test/resources/indices/bwc/repo-1.6.0.zip
deleted file mode 100644
index 1c31a02..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.6.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.6.1.zip b/core/src/test/resources/indices/bwc/repo-1.6.1.zip
deleted file mode 100644
index 746f3ce..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.6.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.6.2.zip b/core/src/test/resources/indices/bwc/repo-1.6.2.zip
deleted file mode 100644
index de4c5be..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.6.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.7.0.zip b/core/src/test/resources/indices/bwc/repo-1.7.0.zip
deleted file mode 100644
index 893689b..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.7.0.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.7.1.zip b/core/src/test/resources/indices/bwc/repo-1.7.1.zip
deleted file mode 100644
index cd5c6c0..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.7.1.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/repo-1.7.2.zip b/core/src/test/resources/indices/bwc/repo-1.7.2.zip
deleted file mode 100644
index a0daaac..0000000
Binary files a/core/src/test/resources/indices/bwc/repo-1.7.2.zip and /dev/null differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.0.Beta1.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.0.Beta1.zip
new file mode 100644
index 0000000..5bbdea4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.0.Beta1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC1.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC1.zip
new file mode 100644
index 0000000..d9072ce
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC2.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC2.zip
new file mode 100644
index 0000000..dce299b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.0.RC2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.0.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.0.zip
new file mode 100644
index 0000000..3ec908d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.1.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.1.zip
new file mode 100644
index 0000000..67db98f
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.10.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.10.zip
new file mode 100644
index 0000000..6bdb9f2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.10.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.11.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.11.zip
new file mode 100644
index 0000000..b5253f9
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.11.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.12.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.12.zip
new file mode 100644
index 0000000..0392049
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.12.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.13.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.13.zip
new file mode 100644
index 0000000..025b4c3
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.13.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.2.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.2.zip
new file mode 100644
index 0000000..413e08e
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.3.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.3.zip
new file mode 100644
index 0000000..c31d4de
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.4.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.4.zip
new file mode 100644
index 0000000..8b07a92
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.5.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.5.zip
new file mode 100644
index 0000000..dfd0fd0
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.6.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.6.zip
new file mode 100644
index 0000000..1f3cff2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.6.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.7.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.7.zip
new file mode 100644
index 0000000..6d0e65c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.7.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.8.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.8.zip
new file mode 100644
index 0000000..8ff8ac3
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.8.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-0.90.9.zip b/core/src/test/resources/indices/bwc/unsupported-0.90.9.zip
new file mode 100644
index 0000000..4445b39
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-0.90.9.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta1.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta1.zip
new file mode 100644
index 0000000..167dde8
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta2.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta2.zip
new file mode 100644
index 0000000..95fbfef
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.Beta2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC1.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC1.zip
new file mode 100644
index 0000000..3ced97a
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC2.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC2.zip
new file mode 100644
index 0000000..1298cfb
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.RC2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.0.zip
new file mode 100644
index 0000000..2cb9abc
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.1.zip
new file mode 100644
index 0000000..844271b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.2.zip
new file mode 100644
index 0000000..dd8e393
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.0.3.zip b/core/src/test/resources/indices/bwc/unsupported-1.0.3.zip
new file mode 100644
index 0000000..e4437ef
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.0.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.1.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.1.0.zip
new file mode 100644
index 0000000..4f05370
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.1.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.1.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.1.1.zip
new file mode 100644
index 0000000..effeb94
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.1.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.1.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.1.2.zip
new file mode 100644
index 0000000..bedffa4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.1.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.0.zip
new file mode 100644
index 0000000..4644a38
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.1.zip
new file mode 100644
index 0000000..553b46b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.2.zip
new file mode 100644
index 0000000..3f51a47
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.3.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.3.zip
new file mode 100644
index 0000000..8c8bfbd
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.2.4.zip b/core/src/test/resources/indices/bwc/unsupported-1.2.4.zip
new file mode 100644
index 0000000..e3a1519
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.2.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.0.zip
new file mode 100644
index 0000000..d98958d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.1.zip
new file mode 100644
index 0000000..167d0f4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.2.zip
new file mode 100644
index 0000000..756eaf6
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.3.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.3.zip
new file mode 100644
index 0000000..8470dee
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.4.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.4.zip
new file mode 100644
index 0000000..2175012
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.5.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.5.zip
new file mode 100644
index 0000000..19d1e56
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.6.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.6.zip
new file mode 100644
index 0000000..ad8e8bd
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.6.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.7.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.7.zip
new file mode 100644
index 0000000..3a645a9
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.7.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.8.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.8.zip
new file mode 100644
index 0000000..f8ab0a2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.8.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.3.9.zip b/core/src/test/resources/indices/bwc/unsupported-1.3.9.zip
new file mode 100644
index 0000000..5ef35b2
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.3.9.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.0.Beta1.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.0.Beta1.zip
new file mode 100644
index 0000000..4546f5d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.0.Beta1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.0.zip
new file mode 100644
index 0000000..467d19a
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.1.zip
new file mode 100644
index 0000000..2adbb28
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.2.zip
new file mode 100644
index 0000000..4fac208
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.3.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.3.zip
new file mode 100644
index 0000000..1a0d667
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.4.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.4.zip
new file mode 100644
index 0000000..0328a9e
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.4.5.zip b/core/src/test/resources/indices/bwc/unsupported-1.4.5.zip
new file mode 100644
index 0000000..eeb25ab
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.4.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.5.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.5.0.zip
new file mode 100644
index 0000000..f1dab08
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.5.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.5.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.5.1.zip
new file mode 100644
index 0000000..342e311
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.5.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.5.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.5.2.zip
new file mode 100644
index 0000000..fb36b19
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.5.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.6.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.6.0.zip
new file mode 100644
index 0000000..02a5806
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.6.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.6.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.6.1.zip
new file mode 100644
index 0000000..04820f9
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.6.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.6.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.6.2.zip
new file mode 100644
index 0000000..af6ce56
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.6.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.7.0.zip b/core/src/test/resources/indices/bwc/unsupported-1.7.0.zip
new file mode 100644
index 0000000..941be64
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.7.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.7.1.zip b/core/src/test/resources/indices/bwc/unsupported-1.7.1.zip
new file mode 100644
index 0000000..debd797
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.7.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupported-1.7.2.zip b/core/src/test/resources/indices/bwc/unsupported-1.7.2.zip
new file mode 100644
index 0000000..18bb6c7
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupported-1.7.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.Beta2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.Beta2.zip
new file mode 100644
index 0000000..020f6f4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.Beta2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC1.zip
new file mode 100644
index 0000000..a84c507
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC2.zip
new file mode 100644
index 0000000..e5d65eb
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.RC2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.zip
new file mode 100644
index 0000000..13f778d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.1.zip
new file mode 100644
index 0000000..76ed278
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.2.zip
new file mode 100644
index 0000000..762eabf
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.3.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.3.zip
new file mode 100644
index 0000000..ba79dea
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.0.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.0.zip
new file mode 100644
index 0000000..cbf84c7
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.1.zip
new file mode 100644
index 0000000..00ea044
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.2.zip
new file mode 100644
index 0000000..18abd99
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.1.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.0.zip
new file mode 100644
index 0000000..f5e62a1
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.1.zip
new file mode 100644
index 0000000..935e71b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.2.zip
new file mode 100644
index 0000000..d69b22a
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.3.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.3.zip
new file mode 100644
index 0000000..295f9f7
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.4.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.4.zip
new file mode 100644
index 0000000..e9efc00
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.2.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.0.zip
new file mode 100644
index 0000000..5a59e21
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.1.zip
new file mode 100644
index 0000000..2ae1d7c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.2.zip
new file mode 100644
index 0000000..c67b997
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.3.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.3.zip
new file mode 100644
index 0000000..64e3235
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.4.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.4.zip
new file mode 100644
index 0000000..55e6744
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.5.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.5.zip
new file mode 100644
index 0000000..35a5fd4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.6.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.6.zip
new file mode 100644
index 0000000..f1eb21c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.6.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.7.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.7.zip
new file mode 100644
index 0000000..543c13c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.7.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.8.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.8.zip
new file mode 100644
index 0000000..93abac3
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.8.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.9.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.9.zip
new file mode 100644
index 0000000..7dc194d
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.3.9.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.Beta1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.Beta1.zip
new file mode 100644
index 0000000..5adf788
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.Beta1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.zip
new file mode 100644
index 0000000..21f867c
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.1.zip
new file mode 100644
index 0000000..18166ea
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.2.zip
new file mode 100644
index 0000000..f03625a
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.3.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.3.zip
new file mode 100644
index 0000000..d78fbb1
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.3.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.4.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.4.zip
new file mode 100644
index 0000000..1e89446
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.4.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.5.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.5.zip
new file mode 100644
index 0000000..fcae439
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.4.5.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.0.zip
new file mode 100644
index 0000000..a55e6c4
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.1.zip
new file mode 100644
index 0000000..7cccbbe
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.2.zip
new file mode 100644
index 0000000..cee5783
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.5.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.0.zip
new file mode 100644
index 0000000..1c31a02
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.1.zip
new file mode 100644
index 0000000..746f3ce
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.2.zip
new file mode 100644
index 0000000..de4c5be
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.6.2.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.0.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.0.zip
new file mode 100644
index 0000000..893689b
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.0.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.1.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.1.zip
new file mode 100644
index 0000000..cd5c6c0
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.1.zip differ
diff --git a/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.2.zip b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.2.zip
new file mode 100644
index 0000000..a0daaac
Binary files /dev/null and b/core/src/test/resources/indices/bwc/unsupportedrepo-1.7.2.zip differ
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
index eefec53..3d98f37 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch1.json
@@ -1,16 +1,16 @@
 {"index":"test", "ignore_unavailable" : true, "expand_wildcards" : "open,closed"}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index" : "test", "type" : "type1", "expand_wildcards" : ["open", "closed"]}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : false, "expand_wildcards" : ["open"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : true, "allow_no_indices": true, "expand_wildcards" : ["open", "closed"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"index":"test", "ignore_unavailable" : true, "allow_no_indices": false, "expand_wildcards" : ["closed"]}}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
 
-{"query" : {"match_all" :{}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
index 79330d8..e2e06d9 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch2.json
@@ -1,10 +1,10 @@
 {"index":"test"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test", "type" : "type1"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
index a6b52fd..6416720 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch3.json
@@ -1,8 +1,8 @@
 {"index":["test0", "test1"]}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test2,test3", "type" : "type1"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ]}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"search_type" : "dfs_query_then_fetch"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
index 844d8be..b98e24b 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch4.json
@@ -1,6 +1,6 @@
 {"index":["test0", "test1"], "request_cache": true}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : "test2,test3", "type" : "type1", "preference": "_local"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ], "routing": "123"}
-{"query" : {"match_all" : {}}}
+{"query" : {"match_all" {}}}
diff --git a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
index b337eae..5f08919 100644
--- a/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
+++ b/core/src/test/resources/org/elasticsearch/action/search/simple-msearch5.json
@@ -1,6 +1,6 @@
 {"index":["test0", "test1"], "request_cache": true}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
 {"index" : "test2,test3", "type" : "type1", "preference": "_local"}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
 {"index" : ["test4", "test1"], "type" :  [ "type2", "type1" ], "routing": "123"}
-{"template": {"query" : {"match_{{template}}" :{}}}, "params": {"template": "all" } } }
+{"template": {"query" : {"match_{{template}}" {}}}, "params": {"template": "all" } } }
diff --git a/core/src/test/resources/org/elasticsearch/cluster/routing/custom_routing_1_x.zip b/core/src/test/resources/org/elasticsearch/cluster/routing/custom_routing_1_x.zip
deleted file mode 100644
index 5772361..0000000
Binary files a/core/src/test/resources/org/elasticsearch/cluster/routing/custom_routing_1_x.zip and /dev/null differ
diff --git a/core/src/test/resources/org/elasticsearch/cluster/routing/default_routing_1_x.zip b/core/src/test/resources/org/elasticsearch/cluster/routing/default_routing_1_x.zip
deleted file mode 100644
index 2fffc0b..0000000
Binary files a/core/src/test/resources/org/elasticsearch/cluster/routing/default_routing_1_x.zip and /dev/null differ
diff --git a/dev-tools/src/main/resources/forbidden/all-signatures.txt b/dev-tools/src/main/resources/forbidden/all-signatures.txt
index ee534b4..f9fba0a 100644
--- a/dev-tools/src/main/resources/forbidden/all-signatures.txt
+++ b/dev-tools/src/main/resources/forbidden/all-signatures.txt
@@ -87,55 +87,6 @@ java.net.InetAddress#getCanonicalHostName()
 
 java.net.InetSocketAddress#getHostName() @ Use getHostString() instead, which avoids a DNS lookup
 
-@defaultMessage avoid adding additional dependencies on Guava
-com.google.common.collect.Lists
-com.google.common.collect.ImmutableList
-com.google.common.base.Objects
-com.google.common.base.Predicate
-com.google.common.base.Predicates
-com.google.common.base.Strings
-com.google.common.base.Throwables
-com.google.common.collect.Maps
-com.google.common.collect.ForwardingSet
-com.google.common.collect.Sets
-com.google.common.base.Preconditions
-com.google.common.collect.ImmutableSortedSet
-com.google.common.collect.Queues
-com.google.common.util.concurrent.ListenableFuture
-com.google.common.util.concurrent.SettableFuture
-com.google.common.util.concurrent.Futures
-com.google.common.util.concurrent.MoreExecutors
-com.google.common.collect.ImmutableSortedMap
-com.google.common.base.Charsets
-com.google.common.base.Function
-com.google.common.collect.Collections2
-com.google.common.cache.LoadingCache
-com.google.common.cache.CacheLoader
-com.google.common.collect.Iterables
-com.google.common.util.concurrent.UncheckedExecutionException
-com.google.common.util.concurrent.AtomicLongMap
-com.google.common.primitives.Longs
-com.google.common.io.ByteStreams
-com.google.common.collect.UnmodifiableIterator
-com.google.common.collect.ObjectArrays
-com.google.common.collect.Multimap
-com.google.common.collect.MultimapBuilder
-com.google.common.math.LongMath
-com.google.common.base.Joiner
-com.google.common.collect.ArrayListMultimap
-com.google.common.collect.HashMultimap
-com.google.common.collect.FluentIterable
-com.google.common.io.Files
-com.google.common.primitives.Ints
-com.google.common.collect.ImmutableSet
-com.google.common.collect.ImmutableSet$Builder
-com.google.common.io.Resources
-com.google.common.hash.HashCode
-com.google.common.hash.HashFunction
-com.google.common.hash.Hashing
-com.google.common.collect.Iterators
-com.google.common.net.InetAddresses
-
 @defaultMessage Do not violate java's access system
 java.lang.reflect.AccessibleObject#setAccessible(boolean)
 java.lang.reflect.AccessibleObject#setAccessible(java.lang.reflect.AccessibleObject[], boolean)
diff --git a/dev-tools/src/main/resources/forbidden/third-party-signatures.txt b/dev-tools/src/main/resources/forbidden/third-party-signatures.txt
index 9979f8c..ac1ce33 100644
--- a/dev-tools/src/main/resources/forbidden/third-party-signatures.txt
+++ b/dev-tools/src/main/resources/forbidden/third-party-signatures.txt
@@ -14,9 +14,6 @@
 # either express or implied. See the License for the specific
 # language governing permissions and limitations under the License.
 
-@defaultMessage Use Long.compare instead we are on Java7
-com.google.common.primitives.Longs#compare(long,long)
-
 @defaultMessage unsafe encoders/decoders have problems in the lzf compress library.  Use variants of encode/decode functions which take Encoder/Decoder.
 com.ning.compress.lzf.impl.UnsafeChunkEncoders#createEncoder(int)
 com.ning.compress.lzf.impl.UnsafeChunkEncoders#createNonAllocatingEncoder(int)
@@ -67,5 +64,3 @@ org.joda.time.DateTime#<init>(int, int, int, int, int, int)
 org.joda.time.DateTime#<init>(int, int, int, int, int, int, int)
 org.joda.time.DateTime#now()
 org.joda.time.DateTimeZone#getDefault()
-
-com.google.common.collect.Iterators#emptyIterator() @ Use Collections.emptyIterator instead
\ No newline at end of file
diff --git a/distribution/deb/pom.xml b/distribution/deb/pom.xml
index 182398d..c43e32b 100644
--- a/distribution/deb/pom.xml
+++ b/distribution/deb/pom.xml
@@ -120,6 +120,19 @@
                                         <group>root</group>
                                     </mapper>
                                 </data>
+                                <!-- create the conf dir manually so it gets proper permissions -->
+                                <data>
+                                    <type>template</type>
+                                    <paths>
+                                        <path>${packaging.elasticsearch.conf.dir}</path>
+                                    </paths>
+                                    <mapper>
+                                        <type>perm</type>
+                                        <filemode>750</filemode>
+                                        <user>root</user>
+                                        <group>elasticsearch</group>
+                                    </mapper>
+                                </data>
                                 <!-- Add configuration files -->
                                 <data>
                                     <src>${project.basedir}/../src/main/resources/config</src>
@@ -128,8 +141,9 @@
                                     <mapper>
                                         <type>perm</type>
                                         <prefix>${packaging.elasticsearch.conf.dir}</prefix>
+                                        <filemode>750</filemode>
                                         <user>root</user>
-                                        <group>root</group>
+                                        <group>elasticsearch</group>
                                     </mapper>
                                 </data>
                                 <data>
@@ -137,6 +151,12 @@
                                     <paths>
                                         <path>${packaging.elasticsearch.conf.dir}/scripts</path>
                                     </paths>
+                                    <mapper>
+                                        <type>perm</type>
+                                        <filemode>750</filemode>
+                                        <user>root</user>
+                                        <group>elasticsearch</group>
+                                    </mapper>
                                 </data>
                                 <!-- Add environment vars file -->
                                 <data>
diff --git a/distribution/licenses/compiler-0.8.13.jar.sha1 b/distribution/licenses/compiler-0.8.13.jar.sha1
deleted file mode 100644
index dc15a10..0000000
--- a/distribution/licenses/compiler-0.8.13.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-f92c71c9bf324fcb66fa07a0fc9c2729d67b8919
diff --git a/distribution/licenses/compiler-0.9.1.jar.sha1 b/distribution/licenses/compiler-0.9.1.jar.sha1
new file mode 100644
index 0000000..96152e0
--- /dev/null
+++ b/distribution/licenses/compiler-0.9.1.jar.sha1
@@ -0,0 +1 @@
+14aec5344639782ee76441401b773946c65eb2b3
diff --git a/distribution/licenses/guava-18.0.jar.sha1 b/distribution/licenses/guava-18.0.jar.sha1
deleted file mode 100644
index 4e97237..0000000
--- a/distribution/licenses/guava-18.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-cce0823396aa693798f8882e64213b1772032b09
diff --git a/distribution/licenses/guava-LICENSE.txt b/distribution/licenses/guava-LICENSE.txt
deleted file mode 100644
index d645695..0000000
--- a/distribution/licenses/guava-LICENSE.txt
+++ /dev/null
@@ -1,202 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/distribution/licenses/guava-NOTICE.txt b/distribution/licenses/guava-NOTICE.txt
deleted file mode 100644
index 8d1c8b6..0000000
--- a/distribution/licenses/guava-NOTICE.txt
+++ /dev/null
@@ -1 +0,0 @@
- 
diff --git a/distribution/rpm/pom.xml b/distribution/rpm/pom.xml
index 1e3004c..218e19e 100644
--- a/distribution/rpm/pom.xml
+++ b/distribution/rpm/pom.xml
@@ -142,10 +142,14 @@
                                  that creates the conf.dir.-->
                             <directory>${packaging.elasticsearch.conf.dir}</directory>
                             <configuration>noreplace</configuration>
+                            <groupname>elasticsearch</groupname>
+                            <filemode>750</filemode>
                         </mapping>
                         <mapping>
                             <directory>${packaging.elasticsearch.conf.dir}/</directory>
                             <configuration>noreplace</configuration>
+                            <groupname>elasticsearch</groupname>
+                            <filemode>750</filemode>
                             <sources>
                                 <source>
                                     <location>${project.basedir}/../src/main/resources/config/</location>
@@ -158,6 +162,8 @@
                         <mapping>
                             <directory>${packaging.elasticsearch.conf.dir}/scripts</directory>
                             <configuration>noreplace</configuration>
+                            <groupname>elasticsearch</groupname>
+                            <filemode>750</filemode>
                         </mapping>
                         <!-- Add environment vars file -->
                         <mapping>
diff --git a/docs/reference/migration/migrate_2_0/crud.asciidoc b/docs/reference/migration/migrate_2_0/crud.asciidoc
index 060cfc7..a7c947e 100644
--- a/docs/reference/migration/migrate_2_0/crud.asciidoc
+++ b/docs/reference/migration/migrate_2_0/crud.asciidoc
@@ -76,7 +76,7 @@ might return:
   "_index":     "my_index",
   "_type":      "my_type",
   "_id":        "1",
-  "_timestamp": 10000000, <1>,
+  "_timestamp": 10000000, <1>
   "_source": {
     "foo" : [ "bar" ]
   }
diff --git a/docs/reference/migration/migrate_2_0/java.asciidoc b/docs/reference/migration/migrate_2_0/java.asciidoc
index 65bfaef..ef9c7ef 100644
--- a/docs/reference/migration/migrate_2_0/java.asciidoc
+++ b/docs/reference/migration/migrate_2_0/java.asciidoc
@@ -22,8 +22,8 @@ Client client = TransportClient.builder().settings(settings).build();
 --------------------------------------------------
 
 The transport client also no longer supports loading settings from config files.
-If you have have a config file, you can load into settings yourself before
-consturcting the transport client:
+If you have a config file, you can load it into settings yourself before
+constructing the transport client:
 
 [source,java]
 --------------------------------------------------
diff --git a/docs/reference/migration/migrate_2_0/mapping.asciidoc b/docs/reference/migration/migrate_2_0/mapping.asciidoc
index c028a2e..86d81b2 100644
--- a/docs/reference/migration/migrate_2_0/mapping.asciidoc
+++ b/docs/reference/migration/migrate_2_0/mapping.asciidoc
@@ -160,7 +160,7 @@ You can no longer create fields with dots in the name.
 
 In 1.x, Elasticsearch would issue a warning if a type name included a dot,
 e.g. `my.type`.  Now that type names are no longer used to distinguish between
-fields in differnt types, this warning has been relaxed: type names may now
+fields in different types, this warning has been relaxed: type names may now
 contain dots, but they may not *begin* with a dot.  The only exception to this
 is the special `.percolator` type.
 
diff --git a/docs/reference/migration/migrate_2_0/settings.asciidoc b/docs/reference/migration/migrate_2_0/settings.asciidoc
index 0be16cb..923d506 100644
--- a/docs/reference/migration/migrate_2_0/settings.asciidoc
+++ b/docs/reference/migration/migrate_2_0/settings.asciidoc
@@ -3,7 +3,7 @@
 ==== Command line flags
 
 Command line flags using single dash notation must be now specified as the first arguments.
-For example if previously using: 
+For example if previously using:
 
 [source,sh]
 ---------------
@@ -14,7 +14,7 @@ This will now need to be changed to:
 
 [source,sh]
 ---------------
-./elasticsearch -Des.path.conf=/opt/elasticsearch/conf/test_node --node.name=test_node 
+./elasticsearch -Des.path.conf=/opt/elasticsearch/conf/test_node --node.name=test_node
 ---------------
 
 for the flag to take effect.
@@ -174,3 +174,9 @@ environment variable, or the `-Des.config`, `-Des.default.config`, or
 Instead, the config file must be named `elasticsearch.yml` and must be located
 in the default `config/` directory, or in the directory specified in the
 `CONF_DIR` environment variable.
+
+==== `ES_CLASSPATH removed`
+
+The `ES_CLASSPATH` environment variable is no longer used to set the class
+path. External libraries should preferably be loaded using the plugin
+mechanism or, if you really must, be copied to the `lib/` directory.
diff --git a/docs/reference/setup.asciidoc b/docs/reference/setup.asciidoc
index 9d14e62..15f23e6 100644
--- a/docs/reference/setup.asciidoc
+++ b/docs/reference/setup.asciidoc
@@ -10,6 +10,14 @@ then check the <<setup-installation,installation>> docs.
 NOTE: Elasticsearch can also be installed from our repositories using `apt` or `yum`.
 See <<setup-repositories>>.
 
+[[supported-platforms]]
+[float]
+== Supported platforms
+
+The matrix of officially supported operating systems and JVMs is available here:
+link:/support/matrix[Support Matrix].  Elasticsearch is tested on the listed
+platforms, but it is possible that it will work on other platforms too.
+
 [[setup-installation]]
 [float]
 == Installation
diff --git a/docs/reference/setup/as-a-service.asciidoc b/docs/reference/setup/as-a-service.asciidoc
index 50454ca..1bd6d9b 100644
--- a/docs/reference/setup/as-a-service.asciidoc
+++ b/docs/reference/setup/as-a-service.asciidoc
@@ -39,6 +39,8 @@ sudo update-rc.d elasticsearch defaults 95 10
 sudo /etc/init.d/elasticsearch start
 --------------------------------------------------
 
+Users running Debian 8 or Ubuntu 14 or later may require configuration of systemd instead of `update-rc.d`. In those cases, please refer to the <<using-systemd>> section.
+
 [float]
 ===== Installing the oracle JDK
 
@@ -69,11 +71,11 @@ sudo /sbin/chkconfig --add elasticsearch
 sudo service elasticsearch start
 --------------------------------------------------
 
-
+[[using-systemd]]
 [float]
 ==== Using systemd
 
-Distributions like SUSE do not use the `chkconfig` tool to register services, but rather `systemd` and its command `/bin/systemctl` to start and stop services (at least in newer versions, otherwise use the `chkconfig` commands above). The configuration file is also placed at `/etc/sysconfig/elasticsearch` if the system is rpm based and `/etc/default/elasticsearch` if it is deb. After installing the RPM, you have to change the systemd configuration and then start up elasticsearch
+Distributions like Debian Jessie, Ubuntu 14, and many of the SUSE derivitives do not use the `chkconfig` tool to register services, but rather `systemd` and its command `/bin/systemctl` to start and stop services (at least in newer versions, otherwise use the `chkconfig` commands above). The configuration file is also placed at `/etc/sysconfig/elasticsearch` if the system is rpm based and `/etc/default/elasticsearch` if it is deb. After installing the RPM, you have to change the systemd configuration and then start up elasticsearch
 
 [source,sh]
 --------------------------------------------------
diff --git a/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java b/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java
index 433d03d..9515971 100644
--- a/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java
+++ b/plugins/analysis-kuromoji/src/main/java/org/elasticsearch/index/analysis/JapaneseStopTokenFilterFactory.java
@@ -25,7 +25,6 @@ import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.ja.JapaneseAnalyzer;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.search.suggest.analyzing.SuggestStopFilter;
-import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.settings.Settings;
@@ -36,8 +35,10 @@ import org.elasticsearch.index.settings.IndexSettings;
 import java.util.Map;
 import java.util.Set;
 
-public class JapaneseStopTokenFilterFactory extends AbstractTokenFilterFactory{
+import static java.util.Collections.singletonMap;
 
+public class JapaneseStopTokenFilterFactory extends AbstractTokenFilterFactory{
+    private static final Map<String, Set<?>> NAMED_STOP_WORDS = singletonMap("_japanese_", JapaneseAnalyzer.getDefaultStopSet());
 
     private final CharArraySet stopWords;
 
@@ -50,10 +51,7 @@ public class JapaneseStopTokenFilterFactory extends AbstractTokenFilterFactory{
         super(index, indexSettings, name, settings);
         this.ignoreCase = settings.getAsBoolean("ignore_case", false);
         this.removeTrailing = settings.getAsBoolean("remove_trailing", true);
-        Map<String, Set<?>> namedStopWords = MapBuilder.<String, Set<?>>newMapBuilder()
-            .put("_japanese_", JapaneseAnalyzer.getDefaultStopSet())
-            .immutableMap();
-        this.stopWords = Analysis.parseWords(env, settings, "stopwords", JapaneseAnalyzer.getDefaultStopSet(), namedStopWords, ignoreCase);
+        this.stopWords = Analysis.parseWords(env, settings, "stopwords", JapaneseAnalyzer.getDefaultStopSet(), NAMED_STOP_WORDS, ignoreCase);
     }
 
     @Override
diff --git a/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-integTest-execution-times.log b/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-integTest-execution-times.log
new file mode 100644
index 0000000..7636f46
--- /dev/null
+++ b/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-integTest-execution-times.log
@@ -0,0 +1 @@
+org.elasticsearch.cloud.gce.CloudGCERestIT=1367
diff --git a/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-test-execution-times.log b/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-test-execution-times.log
new file mode 100644
index 0000000..40b2939
--- /dev/null
+++ b/plugins/cloud-gce/.local-3.0.0-SNAPSHOT-test-execution-times.log
@@ -0,0 +1,2 @@
+org.elasticsearch.discovery.gce.GceDiscoverySettingsTests=700
+org.elasticsearch.discovery.gce.GceDiscoveryTests=929
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
index e3faeb1..4c29e7c 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java
@@ -19,19 +19,27 @@
 
 package org.elasticsearch.action.deletebyquery;
 
+import org.elasticsearch.ElasticsearchGenerationException;
 import org.elasticsearch.action.ActionRequest;
 import org.elasticsearch.action.ActionRequestValidationException;
 import org.elasticsearch.action.IndicesRequest;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
+import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.bytes.BytesArray;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.common.xcontent.XContentBuilder;
+import org.elasticsearch.common.xcontent.XContentFactory;
+import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.search.Scroll;
 
 import java.io.IOException;
 import java.util.Arrays;
+import java.util.Map;
 
 import static org.elasticsearch.action.ValidateActions.addValidationError;
 import static org.elasticsearch.search.Scroll.readScroll;
@@ -69,7 +77,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
 
     private String[] types = Strings.EMPTY_ARRAY;
 
-    private QueryBuilder<?> query;
+    private BytesReference source;
 
     private String routing;
 
@@ -93,7 +101,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
     @Override
     public ActionRequestValidationException validate() {
         ActionRequestValidationException validationException = null;
-        if (query == null) {
+        if (source == null) {
             validationException = addValidationError("source is missing", validationException);
         }
         return validationException;
@@ -132,12 +140,45 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         return this;
     }
 
-    public QueryBuilder<?> query() {
-        return query;
+    public BytesReference source() {
+        return source;
     }
 
-    public DeleteByQueryRequest query(QueryBuilder<?> queryBuilder) {
-        this.query = queryBuilder;
+    public DeleteByQueryRequest source(QuerySourceBuilder sourceBuilder) {
+        this.source = sourceBuilder.buildAsBytes(Requests.CONTENT_TYPE);
+        return this;
+    }
+
+    public DeleteByQueryRequest source(Map<String,?> querySource) {
+        try {
+            XContentBuilder builder = XContentFactory.contentBuilder(Requests.CONTENT_TYPE);
+            builder.map(querySource);
+            return source(builder);
+        } catch (IOException e) {
+            throw new ElasticsearchGenerationException("Failed to generate [" + querySource + "]", e);
+        }
+    }
+
+    public DeleteByQueryRequest source(XContentBuilder builder) {
+        this.source = builder.bytes();
+        return this;
+    }
+
+    public DeleteByQueryRequest source(String querySource) {
+        this.source = new BytesArray(querySource);
+        return this;
+    }
+
+    public DeleteByQueryRequest source(byte[] querySource) {
+        return source(querySource, 0, querySource.length);
+    }
+
+    public DeleteByQueryRequest source(byte[] querySource, int offset, int length) {
+        return source(new BytesArray(querySource, offset, length));
+    }
+
+    public DeleteByQueryRequest source(BytesReference querySource) {
+        this.source = querySource;
         return this;
     }
 
@@ -208,7 +249,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         indices = in.readStringArray();
         indicesOptions = IndicesOptions.readIndicesOptions(in);
         types = in.readStringArray();
-        query = in.readQuery();
+        source = in.readBytesReference();
         routing = in.readOptionalString();
         size = in.readVInt();
         if (in.readBoolean()) {
@@ -225,7 +266,7 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
         out.writeStringArray(indices);
         indicesOptions.writeIndicesOptions(out);
         out.writeStringArray(types);
-        out.writeQuery(query);
+        out.writeBytesReference(source);
         out.writeOptionalString(routing);
         out.writeVInt(size);
         out.writeOptionalStreamable(scroll);
@@ -234,11 +275,12 @@ public class DeleteByQueryRequest extends ActionRequest<DeleteByQueryRequest> im
 
     @Override
     public String toString() {
-        return "delete-by-query indices:" + Arrays.toString(indices) +
-                ", types:" + Arrays.toString(types) +
-                ", size:" + size +
-                ", timeout:" + timeout +
-                ", routing:" + routing +
-                ", query:" + query.toString();
+        String sSource = "_na_";
+        try {
+            sSource = XContentHelper.convertToJson(source, false);
+        } catch (Exception e) {
+            // ignore
+        }
+        return "delete-by-query [" + Arrays.toString(indices) + "][" + Arrays.toString(types) + "], source[" + sSource + "]";
     }
 }
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
index 7560e1e..d30cfaa 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java
@@ -20,17 +20,25 @@
 package org.elasticsearch.action.deletebyquery;
 
 import org.elasticsearch.action.ActionRequestBuilder;
+import org.elasticsearch.action.ListenableActionFuture;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
+import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 
+import java.util.Map;
+
 /**
  * Creates a new {@link DeleteByQueryRequestBuilder}
  * @see DeleteByQueryRequest
  */
 public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQueryRequest, DeleteByQueryResponse, DeleteByQueryRequestBuilder> {
 
+    private QuerySourceBuilder sourceBuilder;
+
     public DeleteByQueryRequestBuilder(ElasticsearchClient client, DeleteByQueryAction action) {
         super(client, action, new DeleteByQueryRequest());
     }
@@ -56,11 +64,26 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
      * @see org.elasticsearch.index.query.QueryBuilders
      */
     public DeleteByQueryRequestBuilder setQuery(QueryBuilder<?> queryBuilder) {
-        request.query(queryBuilder);
+        sourceBuilder().setQuery(queryBuilder);
+        return this;
+    }
+
+    /**
+     * The query binary used to delete documents.
+     */
+    public DeleteByQueryRequestBuilder setQuery(BytesReference queryBinary) {
+        sourceBuilder().setQuery(queryBinary);
         return this;
     }
 
     /**
+     * Constructs a new builder with a raw search query.
+     */
+    public DeleteByQueryRequestBuilder setQuery(XContentBuilder query) {
+        return setQuery(query.bytes());
+    }
+
+    /**
      * A comma separated list of routing values to control the shards the action will be executed on.
      */
     public DeleteByQueryRequestBuilder setRouting(String routing) {
@@ -77,6 +100,47 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
     }
 
     /**
+     * The source to execute. It is preferable to use either {@link #setSource(byte[])}
+     * or {@link #setQuery(QueryBuilder)}.
+     */
+    public DeleteByQueryRequestBuilder setSource(String source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute in the form of a map.
+     */
+    public DeleteByQueryRequestBuilder setSource(Map<String, Object> source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute in the form of a builder.
+     */
+    public DeleteByQueryRequestBuilder setSource(XContentBuilder builder) {
+        request().source(builder);
+        return this;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public DeleteByQueryRequestBuilder setSource(byte[] source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
+     * The source to execute.
+     */
+    public DeleteByQueryRequestBuilder setSource(BytesReference source) {
+        request().source(source);
+        return this;
+    }
+
+    /**
      * An optional timeout to control how long the delete by query is allowed to take.
      */
     public DeleteByQueryRequestBuilder setTimeout(TimeValue timeout) {
@@ -100,4 +164,19 @@ public class DeleteByQueryRequestBuilder extends ActionRequestBuilder<DeleteByQu
         return this;
     }
 
+    @Override
+    public ListenableActionFuture<DeleteByQueryResponse> execute() {
+        if (sourceBuilder != null) {
+            request.source(sourceBuilder);
+        }
+        return super.execute();
+    }
+
+    private QuerySourceBuilder sourceBuilder() {
+        if (sourceBuilder == null) {
+            sourceBuilder = new QuerySourceBuilder();
+        }
+        return sourceBuilder;
+    }
+
 }
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
index 83a3015..252befd 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java
@@ -27,13 +27,7 @@ import org.elasticsearch.action.bulk.BulkRequest;
 import org.elasticsearch.action.bulk.BulkResponse;
 import org.elasticsearch.action.delete.DeleteRequest;
 import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.search.ClearScrollResponse;
-import org.elasticsearch.action.search.SearchRequest;
-import org.elasticsearch.action.search.SearchResponse;
-import org.elasticsearch.action.search.SearchScrollRequest;
-import org.elasticsearch.action.search.ShardSearchFailure;
-import org.elasticsearch.action.search.TransportSearchAction;
-import org.elasticsearch.action.search.TransportSearchScrollAction;
+import org.elasticsearch.action.search.*;
 import org.elasticsearch.action.support.ActionFilters;
 import org.elasticsearch.action.support.HandledTransportAction;
 import org.elasticsearch.client.Client;
@@ -48,9 +42,7 @@ import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.threadpool.ThreadPool;
 import org.elasticsearch.transport.TransportService;
 
-import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.List;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
@@ -115,11 +107,9 @@ public class TransportDeleteByQueryAction extends HandledTransportAction<DeleteB
                     scanRequest.routing(request.routing());
                 }
 
-                List<String> fields = new ArrayList<>();
-                fields.add("_routing");
-                fields.add("_parent");
                 SearchSourceBuilder source = new SearchSourceBuilder()
-.query(request.query()).fields(fields)
+                        .query(request.source())
+                        .fields("_routing", "_parent")
                         .sort("_doc") // important for performance
                         .fetchSource(false)
                         .version(true);
diff --git a/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java b/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
index aa2fbcc..251953d 100644
--- a/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
+++ b/plugins/delete-by-query/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java
@@ -22,15 +22,11 @@ package org.elasticsearch.rest.action.deletebyquery;
 import org.elasticsearch.action.deletebyquery.DeleteByQueryRequest;
 import org.elasticsearch.action.deletebyquery.DeleteByQueryResponse;
 import org.elasticsearch.action.support.IndicesOptions;
+import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.client.Client;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryParseContext;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
 import org.elasticsearch.rest.BaseRestHandler;
 import org.elasticsearch.rest.RestChannel;
 import org.elasticsearch.rest.RestController;
@@ -38,8 +34,6 @@ import org.elasticsearch.rest.RestRequest;
 import org.elasticsearch.rest.action.support.RestActions;
 import org.elasticsearch.rest.action.support.RestToXContentListener;
 
-import java.io.IOException;
-
 import static org.elasticsearch.action.deletebyquery.DeleteByQueryAction.INSTANCE;
 import static org.elasticsearch.rest.RestRequest.Method.DELETE;
 
@@ -48,19 +42,15 @@ import static org.elasticsearch.rest.RestRequest.Method.DELETE;
  */
 public class RestDeleteByQueryAction extends BaseRestHandler {
 
-    private IndicesQueriesRegistry indicesQueriesRegistry;
-
     @Inject
-    public RestDeleteByQueryAction(Settings settings, RestController controller, Client client,
-            IndicesQueriesRegistry indicesQueriesRegistry) {
+    public RestDeleteByQueryAction(Settings settings, RestController controller, Client client) {
         super(settings, controller, client);
-        this.indicesQueriesRegistry = indicesQueriesRegistry;
         controller.registerHandler(DELETE, "/{index}/_query", this);
         controller.registerHandler(DELETE, "/{index}/{type}/_query", this);
     }
 
     @Override
-    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) throws IOException {
+    public void handleRequest(final RestRequest request, final RestChannel channel, final Client client) {
         DeleteByQueryRequest delete = new DeleteByQueryRequest(Strings.splitStringByCommaToArray(request.param("index")));
         delete.indicesOptions(IndicesOptions.fromRequest(request, delete.indicesOptions()));
         delete.routing(request.param("routing"));
@@ -68,23 +58,15 @@ public class RestDeleteByQueryAction extends BaseRestHandler {
             delete.timeout(request.paramAsTime("timeout", null));
         }
         if (request.hasContent()) {
-            XContentParser requestParser = XContentFactory.xContent(request.content()).createParser(request.content());
-            QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-            context.reset(requestParser);
-            final QueryBuilder<?> builder = context.parseInnerQueryBuilder();
-            delete.query(builder);
+            delete.source(request.content());
         } else {
             String source = request.param("source");
             if (source != null) {
-                XContentParser requestParser = XContentFactory.xContent(source).createParser(source);
-                QueryParseContext context = new QueryParseContext(indicesQueriesRegistry);
-                context.reset(requestParser);
-                final QueryBuilder<?> builder = context.parseInnerQueryBuilder();
-                delete.query(builder);
+                delete.source(source);
             } else {
-                QueryBuilder<?> queryBuilder = RestActions.urlParamsToQueryBuilder(request);
-                if (queryBuilder != null) {
-                    delete.query(queryBuilder);
+                QuerySourceBuilder querySourceBuilder = RestActions.parseQuerySource(request);
+                if (querySourceBuilder != null) {
+                    delete.source(querySourceBuilder);
                 }
             }
         }
diff --git a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
index 853b6cf..c9d3f44 100644
--- a/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
+++ b/plugins/delete-by-query/src/test/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryActionTests.java
@@ -42,8 +42,7 @@ import org.junit.Test;
 import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
 import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.*;
 
 public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
 
@@ -60,6 +59,20 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
     }
 
     @Test
+    public void testExecuteScanFailsOnMalformedQuery() {
+        createIndex("test");
+
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).source("{...}");
+        TestActionListener listener = new TestActionListener();
+
+        newAsyncAction(delete, listener).executeScan();
+        waitForCompletion("scan request should fail on malformed query", listener);
+
+        assertFailure(listener, "all shards failed");
+        assertSearchContextsClosed();
+    }
+
+    @Test
     public void testExecuteScan() {
         createIndex("test");
         final int numDocs = randomIntBetween(1, 200);
@@ -70,7 +83,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
         assertHitCount(client().prepareCount("test").get(), numDocs);
 
         final long limit = randomIntBetween(0, numDocs);
-        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).query(boolQuery().must(rangeQuery("num").lte(limit)));
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).source(boolQuery().must(rangeQuery("num").lte(limit)).buildAsBytes());
         TestActionListener listener = new TestActionListener();
 
         newAsyncAction(delete, listener).executeScan();
@@ -206,7 +219,7 @@ public class TransportDeleteByQueryActionTests extends ESSingleNodeTestCase {
         assertTrue(Strings.hasText(scrollId));
         assertThat(searchResponse.getHits().getTotalHits(), equalTo(limit));
 
-        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).size(100).query(boolQuery().must(rangeQuery("num").lte(limit)));
+        DeleteByQueryRequest delete = new DeleteByQueryRequest().indices(new String[]{"test"}).size(100).source(boolQuery().must(rangeQuery("num").lte(limit)).buildAsBytes());
         TestActionListener listener = new TestActionListener();
 
         newAsyncAction(delete, listener).executeScroll(searchResponse.getScrollId());
diff --git a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
index 5b61787..e05fc31 100644
--- a/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
+++ b/plugins/discovery-multicast/src/main/java/org/elasticsearch/plugin/discovery/multicast/MulticastZenPing.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.plugin.discovery.multicast;
 
+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;
+
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
@@ -44,7 +46,13 @@ import org.elasticsearch.common.xcontent.XContentType;
 import org.elasticsearch.discovery.zen.ping.PingContextProvider;
 import org.elasticsearch.discovery.zen.ping.ZenPing;
 import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.transport.*;
+import org.elasticsearch.transport.EmptyTransportResponseHandler;
+import org.elasticsearch.transport.TransportChannel;
+import org.elasticsearch.transport.TransportException;
+import org.elasticsearch.transport.TransportRequest;
+import org.elasticsearch.transport.TransportRequestHandler;
+import org.elasticsearch.transport.TransportResponse;
+import org.elasticsearch.transport.TransportService;
 
 import java.io.IOException;
 import java.net.SocketAddress;
@@ -483,8 +491,8 @@ public class MulticastZenPing extends AbstractLifecycleComponent<ZenPing> implem
                 }
 
                 builder.startObject("attributes");
-                for (Map.Entry<String, String> attr : localNode.attributes().entrySet()) {
-                    builder.field(attr.getKey(), attr.getValue());
+                for (ObjectObjectCursor<String, String> attr : localNode.attributes()) {
+                    builder.field(attr.key, attr.value);
                 }
                 builder.endObject();
 
diff --git a/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
index b8d7044..b91450f 100644
--- a/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
+++ b/plugins/lang-expression/src/test/java/org/elasticsearch/script/expression/IndexedExpressionTests.java
@@ -19,13 +19,11 @@
 
 package org.elasticsearch.script.expression;
 
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -37,7 +35,7 @@ import static org.hamcrest.Matchers.containsString;
 
 //TODO: please convert to unit tests!
 public class IndexedExpressionTests extends ESIntegTestCase {
-
+    
     @Override
     protected Settings nodeSettings(int nodeOrdinal) {
         Settings.Builder builder = Settings.builder().put(super.nodeSettings(nodeOrdinal));
@@ -47,7 +45,7 @@ public class IndexedExpressionTests extends ESIntegTestCase {
         builder.put("script.engine.expression.indexed.mapping", "off");
         return builder.build();
     }
-
+    
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(ExpressionPlugin.class);
@@ -70,20 +68,16 @@ public class IndexedExpressionTests extends ESIntegTestCase {
             assertThat(e.getCause().getMessage(), containsString("scripts of type [indexed], operation [update] and lang [expression] are disabled"));
         }
         try {
-            client().prepareSearch()
-                    .setSource(
-                            new SearchSourceBuilder().scriptField("test1", new Script("script1", ScriptType.INDEXED, "expression", null)))
-                    .setIndices("test").setTypes("scriptTest").get();
+            String query = "{ \"script_fields\" : { \"test1\" : { \"script_id\" : \"script1\", \"lang\":\"expression\" }}}";
+            client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
             fail("search script should have been rejected");
         } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [indexed], operation [search] and lang [expression] are disabled"));
         }
         try {
-            client().prepareSearch("test")
-                    .setSource(
-                            new SearchSourceBuilder().aggregation(AggregationBuilders.terms("test").script(
-                                    new Script("script1", ScriptType.INDEXED, "expression", null)))).get();
-        } catch (Exception e) {
+            String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_id\":\"script1\", \"script_lang\":\"expression\" } } } }";
+            client().prepareSearch("test").setSource(new BytesArray(source)).get();
+        } catch(Exception e) {
             assertThat(e.toString(), containsString("scripts of type [indexed], operation [aggs] and lang [expression] are disabled"));
         }
     }
diff --git a/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml b/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml
index 36ff7f5..a0953a25 100644
--- a/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml
+++ b/plugins/lang-expression/src/test/resources/rest-api-spec/test/lang_expression/20_search.yaml
@@ -22,6 +22,6 @@ setup:
 ---
 "Expressions scripting test":
 
-  - do: { search: { body: { script_fields : { my_field : { script: { lang: expression, inline: 'doc["age"].value + 19' } } } } } }
+  - do: { search: { body: { script_fields : { my_field : { lang: expression, script: 'doc["age"].value + 19' } } } } }
   - match:  { hits.hits.0.fields.my_field.0: 42.0 }
 
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
index 81971d6..a1ed4b7 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ContextAndHeaderTransportTests.java
@@ -45,7 +45,6 @@ import org.elasticsearch.client.FilterClient;
 import org.elasticsearch.common.inject.AbstractModule;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
@@ -56,7 +55,6 @@ import org.elasticsearch.index.query.MoreLikeThisQueryBuilder;
 import org.elasticsearch.index.query.MoreLikeThisQueryBuilder.Item;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.TermsQueryBuilder;
-import org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionBuilder;
 import org.elasticsearch.indices.cache.query.terms.TermsLookup;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestController;
@@ -69,7 +67,6 @@ import org.elasticsearch.script.mustache.MustacheScriptEngineService;
 import org.elasticsearch.search.aggregations.AggregationBuilders;
 import org.elasticsearch.search.aggregations.pipeline.PipelineAggregatorBuilders;
 import org.elasticsearch.search.suggest.Suggest;
-import org.elasticsearch.search.suggest.SuggestBuilder;
 import org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ClusterScope;
@@ -277,12 +274,14 @@ public class ContextAndHeaderTransportTests extends ESIntegTestCase {
                 .get();
         transportClient().admin().indices().prepareRefresh(queryIndex).get();
 
+        // custom content, not sure how to specify "script_id" otherwise in the API
+        XContentBuilder builder = jsonBuilder().startObject().startObject("function_score").field("boost_mode", "replace").startArray("functions")
+                .startObject().startObject("script_score").field("script_id", "my_script").field("lang", "groovy").endObject().endObject().endArray().endObject().endObject();
+
         SearchResponse searchResponse = transportClient()
                 .prepareSearch(queryIndex)
-                .setQuery(
-                        QueryBuilders.functionScoreQuery(
-                                new ScriptScoreFunctionBuilder(new Script("my_script", ScriptType.INDEXED, "groovy", null))).boostMode(
-                                CombineFunction.REPLACE)).get();
+                .setQuery(builder)
+                .get();
         assertNoFailures(searchResponse);
         assertHitCount(searchResponse, 1);
         assertThat(searchResponse.getHits().getMaxScore(), is(10.0f));
@@ -444,13 +443,11 @@ public class ContextAndHeaderTransportTests extends ESIntegTestCase {
                 MustacheScriptEngineService.NAME, null, null));
 
         SearchRequestBuilder searchRequestBuilder = transportClient().prepareSearch("test").setSize(0);
-        SuggestBuilder suggestBuilder = new SuggestBuilder();
         String suggestText = "united states house of representatives elections in washington 2006";
         if (suggestText != null) {
-            suggestBuilder.setText(suggestText);
+            searchRequestBuilder.setSuggestText(suggestText);
         }
-        suggestBuilder.addSuggestion(filteredFilterSuggest);
-        searchRequestBuilder.suggest(suggestBuilder);
+        searchRequestBuilder.addSuggestion(filteredFilterSuggest);
         SearchResponse actionGet = searchRequestBuilder.execute().actionGet();
         assertThat(Arrays.toString(actionGet.getShardFailures()), actionGet.getFailedShards(), equalTo(0));
         Suggest searchSuggest = actionGet.getSuggest();
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
index 8b7cdba..51fc5a4 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/FunctionScoreTests.java
@@ -60,7 +60,7 @@ public class FunctionScoreTests extends ESIntegTestCase {
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-    
+
 
     @Test
     public void testScriptScoresNested() throws IOException {
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
index d5c2f55..1bba7bf 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/IndexedScriptTests.java
@@ -24,27 +24,23 @@ import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptService;
 import org.elasticsearch.script.groovy.GroovyPlugin;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
 import org.elasticsearch.search.SearchHit;
-import org.elasticsearch.search.aggregations.AggregationBuilders;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
@@ -58,7 +54,7 @@ public class IndexedScriptTests extends ESIntegTestCase {
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-
+    
     @Override
     protected Settings nodeSettings(int nodeOrdinal) {
         Settings.Builder builder = Settings.builder().put(super.nodeSettings(nodeOrdinal));
@@ -95,20 +91,13 @@ public class IndexedScriptTests extends ESIntegTestCase {
         builders.add(client().prepareIndex("test", "scriptTest", "5").setSource("{\"theField\":\"bar\"}"));
 
         indexRandom(true, builders);
-        Map<String, Object> script2Params = new HashMap<>();
-        script2Params.put("factor", 3);
-        SearchResponse searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).size(1)
-                                .scriptField("test1", new Script("script1", ScriptType.INDEXED, "groovy", null))
-                                .scriptField("test2", new Script("script2", ScriptType.INDEXED, "groovy", script2Params)))
-                .setIndices("test").setTypes("scriptTest").get();
+        String query = "{ \"query\" : { \"match_all\": {}} , \"script_fields\" : { \"test1\" : { \"script_id\" : \"script1\", \"lang\":\"groovy\" }, \"test2\" : { \"script_id\" : \"script2\", \"lang\":\"groovy\", \"params\":{\"factor\":3}  }}, size:1}";
+        SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test").setTypes("scriptTest").get();
         assertHitCount(searchResponse, 5);
         assertTrue(searchResponse.getHits().hits().length == 1);
         SearchHit sh = searchResponse.getHits().getAt(0);
-        assertThat((Integer) sh.field("test1").getValue(), equalTo(2));
-        assertThat((Integer) sh.field("test2").getValue(), equalTo(6));
+        assertThat((Integer)sh.field("test1").getValue(), equalTo(2));
+        assertThat((Integer)sh.field("test2").getValue(), equalTo(6));
     }
 
     // Relates to #10397
@@ -124,12 +113,11 @@ public class IndexedScriptTests extends ESIntegTestCase {
             PutIndexedScriptResponse response = 
                     client().preparePutIndexedScript(GroovyScriptEngineService.NAME, "script1", "{\"script\":\"" + i + "\"}").get();
             assertEquals(i, response.getVersion());
-            SearchResponse searchResponse = client()
-                    .prepareSearch()
-                    .setSource(
-                            new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).scriptField("test_field",
-                                    new Script("script1", ScriptType.INDEXED, "groovy", null))).setIndices("test_index")
-                    .setTypes("test_type").get();
+            
+            String query = "{"
+                    + " \"query\" : { \"match_all\": {}}, "
+                    + " \"script_fields\" : { \"test_field\" : { \"script_id\" : \"script1\", \"lang\":\"groovy\" } } }";    
+            SearchResponse searchResponse = client().prepareSearch().setSource(new BytesArray(query)).setIndices("test_index").setTypes("test_type").get();
             assertHitCount(searchResponse, 1);
             SearchHit sh = searchResponse.getHits().getAt(0);
             assertThat((Integer)sh.field("test_field").getValue(), equalTo(i));
@@ -165,11 +153,8 @@ public class IndexedScriptTests extends ESIntegTestCase {
         }
         client().prepareIndex("test", "scriptTest", "1").setSource("{\"theField\":\"foo\"}").get();
         refresh();
-        SearchResponse searchResponse = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().aggregation(AggregationBuilders.terms("test").script(
-                                new Script("script1", ScriptType.INDEXED, null, null)))).get();
+        String source = "{\"aggs\": {\"test\": { \"terms\" : { \"script_id\":\"script1\" } } } }";
+        SearchResponse searchResponse = client().prepareSearch("test").setSource(new BytesArray(source)).get();
         assertHitCount(searchResponse, 1);
         assertThat(searchResponse.getAggregations().get("test"), notNullValue());
     }
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
index a7618f0..7d9a080 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java
@@ -19,7 +19,9 @@
 
 package org.elasticsearch.messy.tests;
 
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.action.index.IndexRequestBuilder;
+import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.Base64;
@@ -31,7 +33,6 @@ import org.elasticsearch.common.joda.Joda;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.internal.TimestampFieldMapper;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
@@ -39,7 +40,6 @@ import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHitField;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
 import org.elasticsearch.search.sort.SortOrder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.joda.time.DateTime;
@@ -80,7 +80,7 @@ public class SearchFieldsTests extends ESIntegTestCase {
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-
+    
     @Test
     public void testStoredFields() throws Exception {
         createIndex("test");
@@ -529,13 +529,22 @@ public class SearchFieldsTests extends ESIntegTestCase {
         createIndex("test");
         indexRandom(true, client().prepareIndex("test", "type", "1").setSource("test_field", "foobar"));
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type").setSource(
-                new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).fieldDataField("test_field")).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": \"test_field\"}"))).get();
         assertHitCount(searchResponse, 1);
         Map<String,SearchHitField> fields = searchResponse.getHits().getHits()[0].getFields();
         assertThat((String)fields.get("test_field").value(), equalTo("foobar"));
     }
 
+    @Test(expected = SearchPhaseExecutionException.class)
+    public void testInvalidFieldDataField() throws ExecutionException, InterruptedException {
+        createIndex("test");
+        if (randomBoolean()) {
+            client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": {}}"))).get();
+        } else {
+            client().prepareSearch("test").setTypes("type").setSource(new BytesArray(new BytesRef("{\"query\":{\"match_all\":{}},\"fielddata_fields\": 1.0}"))).get();
+        }
+    }
+
     @Test
     public void testFieldsPulledFromFieldData() throws Exception {
         createIndex("test");
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchStatsTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchStatsTests.java
index 89d1670..d9cfb7d 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchStatsTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchStatsTests.java
@@ -34,7 +34,6 @@ import org.elasticsearch.index.search.stats.SearchStats.Stats;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
-import org.elasticsearch.search.highlight.HighlightBuilder;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -66,7 +65,7 @@ public class SearchStatsTests extends ESIntegTestCase {
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-
+    
     @Override
     protected int numberOfReplicas() {
         return 0;
@@ -110,7 +109,7 @@ public class SearchStatsTests extends ESIntegTestCase {
         for (int i = 0; i < iters; i++) {
             SearchResponse searchResponse = internalCluster().clientNodeClient().prepareSearch()
                     .setQuery(QueryBuilders.termQuery("field", "value")).setStats("group1", "group2")
-                    .highlighter(new HighlightBuilder().field("field"))
+                    .addHighlightedField("field")
                     .addScriptField("scrip1", new Script("_source.field"))
                     .setSize(100)
                     .execute().actionGet();
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchTimeoutTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchTimeoutTests.java
index 1b53550..2a982df 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchTimeoutTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchTimeoutTests.java
@@ -22,7 +22,6 @@ package org.elasticsearch.messy.tests;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.common.unit.TimeValue;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
@@ -30,8 +29,8 @@ import org.junit.Test;
 
 import java.util.Collection;
 import java.util.Collections;
-import java.util.concurrent.TimeUnit;
 
+import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.index.query.QueryBuilders.scriptQuery;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -55,7 +54,7 @@ public class SearchTimeoutTests extends ESIntegTestCase {
         client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).execute().actionGet();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setTimeout(new TimeValue(10, TimeUnit.MILLISECONDS))
+                .setTimeout("10ms")
                 .setQuery(scriptQuery(new Script("Thread.sleep(500); return true;")))
                 .execute().actionGet();
         assertThat(searchResponse.isTimedOut(), equalTo(true));
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
index 5f51b65..47bfb49 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SimpleSortTests.java
@@ -30,6 +30,7 @@ import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.geo.GeoDistance;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.text.StringAndBytesText;
@@ -45,12 +46,7 @@ import org.elasticsearch.script.Script;
 import org.elasticsearch.script.groovy.GroovyPlugin;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHitField;
-import org.elasticsearch.search.sort.FieldSortBuilder;
-import org.elasticsearch.search.sort.GeoDistanceSortBuilder;
-import org.elasticsearch.search.sort.ScriptSortBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
-import org.elasticsearch.search.sort.SortOrder;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
+import org.elasticsearch.search.sort.*;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.junit.annotations.TestLogging;
 import org.hamcrest.Matchers;
@@ -1852,6 +1848,50 @@ public class SimpleSortTests extends ESIntegTestCase {
         assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(3.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
         assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(5.25, 4, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
 
+        //test all the different formats in one
+        createQPoints(qHashes, qPoints);
+        XContentBuilder searchSourceBuilder = jsonBuilder();
+        searchSourceBuilder.startObject().startArray("sort").startObject().startObject("_geo_distance").startArray("location");
+
+        for (int i = 0; i < 4; i++) {
+            int at = randomInt(qPoints.size() - 1);
+            int format = randomInt(3);
+            switch (format) {
+                case 0: {
+                    searchSourceBuilder.value(qHashes.get(at));
+                    break;
+                }
+                case 1: {
+                    searchSourceBuilder.value(qPoints.get(at).lat() + "," + qPoints.get(at).lon());
+                    break;
+                }
+                case 2: {
+                    searchSourceBuilder.value(qPoints.get(at));
+                    break;
+                }
+                case 3: {
+                    searchSourceBuilder.startArray().value(qPoints.get(at).lon()).value(qPoints.get(at).lat()).endArray();
+                    break;
+                }
+            }
+            qHashes.remove(at);
+            qPoints.remove(at);
+        }
+
+        searchSourceBuilder.endArray();
+        searchSourceBuilder.field("order", "asc");
+        searchSourceBuilder.field("unit", "km");
+        searchSourceBuilder.field("sort_mode", "min");
+        searchSourceBuilder.field("distance_type", "plane");
+        searchSourceBuilder.endObject();
+        searchSourceBuilder.endObject();
+        searchSourceBuilder.endArray();
+        searchSourceBuilder.endObject();
+
+        searchResponse = client().prepareSearch().setSource(searchSourceBuilder.bytes()).execute().actionGet();
+        assertOrderedSearchHits(searchResponse, "d1", "d2");
+        assertThat((Double) searchResponse.getHits().getAt(0).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(2.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
+        assertThat((Double) searchResponse.getHits().getAt(1).getSortValues()[0], closeTo(GeoDistance.PLANE.calculate(4.5, 1, 2, 1, DistanceUnit.KILOMETERS), 1.e-4));
     }
 
     public void testSinglePointGeoDistanceSort() throws ExecutionException, InterruptedException, IOException {
@@ -1890,25 +1930,40 @@ public class SimpleSortTests extends ESIntegTestCase {
                 .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").point(2.0, 2.0)
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        String geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .startArray("location").value(2f).value(2f).endArray()
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").geohashes("s037ms06g7h0")
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .field("location", "s037ms06g7h0")
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
 
-        searchResponse = client()
-                .prepareSearch()
-                .setSource(
-                        new SearchSourceBuilder().sort(SortBuilders.geoDistanceSort("location").point(2.0, 2.0)
-                                .unit(DistanceUnit.KILOMETERS).geoDistance(GeoDistance.PLANE))).execute().actionGet();
+        geoSortRequest = jsonBuilder().startObject().startArray("sort").startObject()
+                .startObject("_geo_distance")
+                .startObject("location")
+                .field("lat", 2)
+                .field("lon", 2)
+                .endObject()
+                .field("unit", "km")
+                .field("distance_type", "plane")
+                .endObject()
+                .endObject().endArray().string();
+        searchResponse = client().prepareSearch().setSource(new BytesArray(geoSortRequest))
+                .execute().actionGet();
         checkCorrectSortOrderForGeoSort(searchResponse);
     }
 
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java
index ace6f68..69da9c7 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java
@@ -19,8 +19,6 @@
 
 package org.elasticsearch.messy.tests;
 
-import com.google.common.collect.ImmutableMap;
-
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.elasticsearch.action.get.GetResponse;
 import org.elasticsearch.action.search.SearchResponse;
@@ -42,6 +40,7 @@ import java.util.Collections;
 import java.util.Map;
 import java.util.concurrent.ExecutionException;
 
+import static java.util.Collections.singletonMap;
 import static org.elasticsearch.index.query.QueryBuilders.termQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertExists;
@@ -58,12 +57,12 @@ import static org.hamcrest.Matchers.not;
  */
 @SuppressCodecs("*") // requires custom completion format
 public class TransformOnIndexMapperTests extends ESIntegTestCase {
-    
+
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
     }
-    
+
     @Test
     public void searchOnTransformed() throws Exception {
         setup(true);
@@ -171,7 +170,7 @@ public class TransformOnIndexMapperTests extends ESIntegTestCase {
         if (getRandom().nextBoolean()) {
             script = script.replace("sourceField", "'content'");
         } else {
-            builder.field("params", ImmutableMap.of("sourceField", "content"));
+            builder.field("params", singletonMap("sourceField", "content"));
         }
         builder.field("script", script);
     }
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
deleted file mode 100644
index dc76eda..0000000
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
+++ /dev/null
@@ -1,850 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.messy.tests;
-
-import org.elasticsearch.ElasticsearchTimeoutException;
-import org.elasticsearch.action.ActionListener;
-import org.elasticsearch.action.ActionRequestValidationException;
-import org.elasticsearch.action.admin.indices.alias.Alias;
-import org.elasticsearch.action.delete.DeleteRequest;
-import org.elasticsearch.action.delete.DeleteResponse;
-import org.elasticsearch.action.get.GetResponse;
-import org.elasticsearch.action.update.UpdateRequest;
-import org.elasticsearch.action.update.UpdateRequestBuilder;
-import org.elasticsearch.action.update.UpdateResponse;
-import org.elasticsearch.client.transport.NoNodeAvailableException;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.unit.TimeValue;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.VersionType;
-import org.elasticsearch.index.engine.DocumentMissingException;
-import org.elasticsearch.index.engine.VersionConflictEngineException;
-import org.elasticsearch.index.shard.MergePolicyConfig;
-import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.script.groovy.GroovyPlugin;
-import org.elasticsearch.test.ESIntegTestCase;
-import org.junit.Test;
-
-import java.util.*;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.Semaphore;
-import java.util.concurrent.TimeUnit;
-
-import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThrows;
-import static org.hamcrest.Matchers.*;
-
-public class UpdateTests extends ESIntegTestCase {
-
-    @Override
-    protected Collection<Class<? extends Plugin>> nodePlugins() {
-        return Collections.singleton(GroovyPlugin.class);
-    }
-    
-    private void createTestIndex() throws Exception {
-        logger.info("--> creating index test");
-
-        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
-                .addMapping("type1", XContentFactory.jsonBuilder()
-                        .startObject()
-                        .startObject("type1")
-                        .startObject("_timestamp").field("enabled", true).endObject()
-                        .startObject("_ttl").field("enabled", true).endObject()
-                        .endObject()
-                        .endObject()));
-    }
-
-    @Test
-    public void testUpsert() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                .execute().actionGet();
-        assertTrue(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("1"));
-        }
-
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                .execute().actionGet();
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("2"));
-        }
-    }
-
-    @Test
-    public void testScriptedUpsert() throws Exception {
-        createTestIndex();
-        ensureGreen();
-        
-        // Script logic is 
-        // 1) New accounts take balance from "balance" in upsert doc and first payment is charged at 50%
-        // 2) Existing accounts subtract full payment from balance stored in elasticsearch
-        
-        String script="int oldBalance=ctx._source.balance;"+
-                      "int deduction=ctx.op == \"create\" ? (payment/2) :  payment;"+
-                      "ctx._source.balance=oldBalance-deduction;";
-        int openingBalance=10;
-
-        Map<String, Object> params = new HashMap<>();
-        params.put("payment", 2);
-
-        // Pay money from what will be a new account and opening balance comes from upsert doc
-        // provided by client
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("balance", openingBalance).endObject())
-                .setScriptedUpsert(true)
-.setScript(new Script(script, ScriptService.ScriptType.INLINE, null, params))
-                .execute().actionGet();
-        assertTrue(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("balance").toString(), equalTo("9"));
-        }
-
-        // Now pay money for an existing account where balance is stored in es 
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("balance", openingBalance).endObject())
-                .setScriptedUpsert(true)
-.setScript(new Script(script, ScriptService.ScriptType.INLINE, null, params))
-                .execute().actionGet();
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("balance").toString(), equalTo("7"));
-        }
-    }
-
-    @Test
-    public void testUpsertDoc() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setDocAsUpsert(true)
-                .setFields("_source")
-                .execute().actionGet();
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
-    }
-
-    @Test
-    // See: https://github.com/elasticsearch/elasticsearch/issues/3265
-    public void testNotUpsertDoc() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        assertThrows(client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setDoc(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setDocAsUpsert(false)
-                .setFields("_source")
-                .execute(), DocumentMissingException.class);
-    }
-
-    @Test
-    public void testUpsertFields() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setScript(new Script("ctx._source.extra = \"foo\"", ScriptService.ScriptType.INLINE, null, null))
-                .setFields("_source")
-                .execute().actionGet();
-
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra"), nullValue());
-
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setScript(new Script("ctx._source.extra = \"foo\"", ScriptService.ScriptType.INLINE, null, null))
-                .setFields("_source")
-                .execute().actionGet();
-
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra").toString(), equalTo("foo"));
-    }
-
-    @Test
-    public void testVersionedUpdate() throws Exception {
-        assertAcked(prepareCreate("test").addAlias(new Alias("alias")));
-        ensureGreen();
-
-        index("test", "type", "1", "text", "value"); // version is now 1
-
-        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1")
-                        .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null)).setVersion(2)
-                        .execute(),
-                VersionConflictEngineException.class);
-
-        client().prepareUpdate(indexOrAlias(), "type", "1")
-                .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null)).setVersion(1).get();
-        assertThat(client().prepareGet("test", "type", "1").get().getVersion(), equalTo(2l));
-
-        // and again with a higher version..
-        client().prepareUpdate(indexOrAlias(), "type", "1")
-                .setScript(new Script("ctx._source.text = 'v3'", ScriptService.ScriptType.INLINE, null, null)).setVersion(2).get();
-
-        assertThat(client().prepareGet("test", "type", "1").get().getVersion(), equalTo(3l));
-
-        // after delete
-        client().prepareDelete("test", "type", "1").get();
-        assertThrows(client().prepareUpdate("test", "type", "1")
-                        .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null)).setVersion(3)
-                        .execute(),
-                DocumentMissingException.class);
-
-        // external versioning
-        client().prepareIndex("test", "type", "2").setSource("text", "value").setVersion(10).setVersionType(VersionType.EXTERNAL).get();
-
-        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "2")
-                        .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null)).setVersion(2)
-                        .setVersionType(VersionType.EXTERNAL).execute(),
-                ActionRequestValidationException.class);
-
-
-        // With force version
-        client().prepareUpdate(indexOrAlias(), "type", "2")
-                .setScript(new Script("ctx._source.text = 'v10'", ScriptService.ScriptType.INLINE, null, null))
-                .setVersion(10).setVersionType(VersionType.FORCE).get();
-
-        GetResponse get = get("test", "type", "2");
-        assertThat(get.getVersion(), equalTo(10l));
-        assertThat((String) get.getSource().get("text"), equalTo("v10"));
-
-        // upserts - the combination with versions is a bit weird. Test are here to ensure we do not change our behavior unintentionally
-
-        // With internal versions, tt means "if object is there with version X, update it or explode. If it is not there, index.
-        client().prepareUpdate(indexOrAlias(), "type", "3")
-                .setScript(new Script("ctx._source.text = 'v2'", ScriptService.ScriptType.INLINE, null, null))
-                .setVersion(10).setUpsert("{ \"text\": \"v0\" }").get();
-        get = get("test", "type", "3");
-        assertThat(get.getVersion(), equalTo(1l));
-        assertThat((String) get.getSource().get("text"), equalTo("v0"));
-
-
-        // retry on conflict is rejected:
-        assertThrows(client().prepareUpdate(indexOrAlias(), "type", "1").setVersion(10).setRetryOnConflict(5), ActionRequestValidationException.class);
-    }
-
-    @Test
-    public void testIndexAutoCreation() throws Exception {
-        UpdateResponse updateResponse = client().prepareUpdate("test", "type1", "1")
-                .setUpsert(XContentFactory.jsonBuilder().startObject().field("bar", "baz").endObject())
-                .setScript(new Script("ctx._source.extra = \"foo\"", ScriptService.ScriptType.INLINE, null, null))
-                .setFields("_source")
-                .execute().actionGet();
-
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("bar").toString(), equalTo("baz"));
-        assertThat(updateResponse.getGetResult().sourceAsMap().get("extra"), nullValue());
-    }
-
-    @Test
-    public void testUpdate() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        try {
-            client().prepareUpdate(indexOrAlias(), "type1", "1")
-                    .setScript(new Script("ctx._source.field++", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-            fail();
-        } catch (DocumentMissingException e) {
-            // all is well
-        }
-
-        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
-
-        UpdateResponse updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        assertThat(updateResponse.getVersion(), equalTo(2L));
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("2"));
-        }
-
-        Map<String, Object> params = new HashMap<>();
-        params.put("count", 3);
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx._source.field += count", ScriptService.ScriptType.INLINE, null, params)).execute().actionGet();
-        assertThat(updateResponse.getVersion(), equalTo(3L));
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("5"));
-        }
-
-        // check noop
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx.op = 'none'", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        assertThat(updateResponse.getVersion(), equalTo(3L));
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("5"));
-        }
-
-        // check delete
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx.op = 'delete'", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        assertThat(updateResponse.getVersion(), equalTo(4L));
-        assertFalse(updateResponse.isCreated());
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-
-        for (int i = 0; i < 5; i++) {
-            GetResponse getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.isExists(), equalTo(false));
-        }
-
-        // check TTL is kept after an update without TTL
-        client().prepareIndex("test", "type1", "2").setSource("field", 1).setTTL(86400000L).setRefresh(true).execute().actionGet();
-        GetResponse getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
-        long ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
-        assertThat(ttl, greaterThan(0L));
-        client().prepareUpdate(indexOrAlias(), "type1", "2")
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
-        ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
-        assertThat(ttl, greaterThan(0L));
-
-        // check TTL update
-        client().prepareUpdate(indexOrAlias(), "type1", "2")
-                .setScript(new Script("ctx._ttl = 3600000", ScriptService.ScriptType.INLINE, null, null)).execute().actionGet();
-        getResponse = client().prepareGet("test", "type1", "2").setFields("_ttl").execute().actionGet();
-        ttl = ((Number) getResponse.getField("_ttl").getValue()).longValue();
-        assertThat(ttl, greaterThan(0L));
-        assertThat(ttl, lessThanOrEqualTo(3600000L));
-
-        // check timestamp update
-        client().prepareIndex("test", "type1", "3").setSource("field", 1).setRefresh(true).execute().actionGet();
-        client().prepareUpdate(indexOrAlias(), "type1", "3")
-                .setScript(new Script("ctx._timestamp = \"2009-11-15T14:12:12\"", ScriptService.ScriptType.INLINE, null, null)).execute()
-                .actionGet();
-        getResponse = client().prepareGet("test", "type1", "3").setFields("_timestamp").execute().actionGet();
-        long timestamp = ((Number) getResponse.getField("_timestamp").getValue()).longValue();
-        assertThat(timestamp, equalTo(1258294332000L));
-
-        // check fields parameter
-        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1")
-                .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null)).setFields("_source", "field")
-                .execute().actionGet();
-        assertThat(updateResponse.getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult(), notNullValue());
-        assertThat(updateResponse.getGetResult().getIndex(), equalTo("test"));
-        assertThat(updateResponse.getGetResult().sourceRef(), notNullValue());
-        assertThat(updateResponse.getGetResult().field("field").getValue(), notNullValue());
-
-        // check updates without script
-        // add new field
-        client().prepareIndex("test", "type1", "1").setSource("field", 1).execute().actionGet();
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("field2", 2).endObject()).execute().actionGet();
-        for (int i = 0; i < 5; i++) {
-            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("1"));
-            assertThat(getResponse.getSourceAsMap().get("field2").toString(), equalTo("2"));
-        }
-
-        // change existing field
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("field", 3).endObject()).execute().actionGet();
-        for (int i = 0; i < 5; i++) {
-            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            assertThat(getResponse.getSourceAsMap().get("field").toString(), equalTo("3"));
-            assertThat(getResponse.getSourceAsMap().get("field2").toString(), equalTo("2"));
-        }
-
-        // recursive map
-        Map<String, Object> testMap = new HashMap<>();
-        Map<String, Object> testMap2 = new HashMap<>();
-        Map<String, Object> testMap3 = new HashMap<>();
-        testMap3.put("commonkey", testMap);
-        testMap3.put("map3", 5);
-        testMap2.put("map2", 6);
-        testMap.put("commonkey", testMap2);
-        testMap.put("map1", 8);
-
-        client().prepareIndex("test", "type1", "1").setSource("map", testMap).execute().actionGet();
-        updateResponse = client().prepareUpdate(indexOrAlias(), "type1", "1").setDoc(XContentFactory.jsonBuilder().startObject().field("map", testMap3).endObject()).execute().actionGet();
-        for (int i = 0; i < 5; i++) {
-            getResponse = client().prepareGet("test", "type1", "1").execute().actionGet();
-            Map map1 = (Map) getResponse.getSourceAsMap().get("map");
-            assertThat(map1.size(), equalTo(3));
-            assertThat(map1.containsKey("map1"), equalTo(true));
-            assertThat(map1.containsKey("map3"), equalTo(true));
-            assertThat(map1.containsKey("commonkey"), equalTo(true));
-            Map map2 = (Map) map1.get("commonkey");
-            assertThat(map2.size(), equalTo(3));
-            assertThat(map2.containsKey("map1"), equalTo(true));
-            assertThat(map2.containsKey("map2"), equalTo(true));
-            assertThat(map2.containsKey("commonkey"), equalTo(true));
-        }
-    }
-
-    @Test
-    public void testUpdateRequestWithBothScriptAndDoc() throws Exception {
-        createTestIndex();
-        ensureGreen();
-
-        try {
-            client().prepareUpdate(indexOrAlias(), "type1", "1")
-                    .setDoc(XContentFactory.jsonBuilder().startObject().field("field", 1).endObject())
-                    .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                    .execute().actionGet();
-            fail("Should have thrown ActionRequestValidationException");
-        } catch (ActionRequestValidationException e) {
-            assertThat(e.validationErrors().size(), equalTo(1));
-            assertThat(e.validationErrors().get(0), containsString("can't provide both script and doc"));
-            assertThat(e.getMessage(), containsString("can't provide both script and doc"));
-        }
-    }
-
-    @Test
-    public void testUpdateRequestWithScriptAndShouldUpsertDoc() throws Exception {
-        createTestIndex();
-        ensureGreen();
-        try {
-            client().prepareUpdate(indexOrAlias(), "type1", "1")
-                    .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                    .setDocAsUpsert(true)
-                    .execute().actionGet();
-            fail("Should have thrown ActionRequestValidationException");
-        } catch (ActionRequestValidationException e) {
-            assertThat(e.validationErrors().size(), equalTo(1));
-            assertThat(e.validationErrors().get(0), containsString("doc must be specified if doc_as_upsert is enabled"));
-            assertThat(e.getMessage(), containsString("doc must be specified if doc_as_upsert is enabled"));
-        }
-    }
-
-    @Test
-    public void testContextVariables() throws Exception {
-        assertAcked(prepareCreate("test").addAlias(new Alias("alias"))
-                        .addMapping("type1", XContentFactory.jsonBuilder()
-                                .startObject()
-                                .startObject("type1")
-                                .startObject("_timestamp").field("enabled", true).endObject()
-                                .startObject("_ttl").field("enabled", true).endObject()
-                                .endObject()
-                                .endObject())
-                        .addMapping("subtype1", XContentFactory.jsonBuilder()
-                                .startObject()
-                                .startObject("subtype1")
-                                .startObject("_parent").field("type", "type1").endObject()
-                                .startObject("_timestamp").field("enabled", true).endObject()
-                                .startObject("_ttl").field("enabled", true).endObject()
-                                .endObject()
-                                .endObject())
-        );
-        ensureGreen();
-
-        // Index some documents
-        long timestamp = System.currentTimeMillis();
-        client().prepareIndex()
-                .setIndex("test")
-                .setType("type1")
-                .setId("parentId1")
-                .setTimestamp(String.valueOf(timestamp-1))
-                .setSource("field1", 0, "content", "bar")
-                .execute().actionGet();
-
-        long ttl = 10000;
-        client().prepareIndex()
-                .setIndex("test")
-                .setType("subtype1")
-                .setId("id1")
-                .setParent("parentId1")
-                .setRouting("routing1")
-                .setTimestamp(String.valueOf(timestamp))
-                .setTTL(ttl)
-                .setSource("field1", 1, "content", "foo")
-                .execute().actionGet();
-
-        // Update the first object and note context variables values
-        Map<String, Object> scriptParams = new HashMap<>();
-        scriptParams.put("delim", "_");
-        UpdateResponse updateResponse = client().prepareUpdate("test", "subtype1", "id1")
-                .setRouting("routing1")
-                .setScript(
-                        new Script("assert ctx._index == \"test\" : \"index should be \\\"test\\\"\"\n"
-                                +
-                                "assert ctx._type == \"subtype1\" : \"type should be \\\"subtype1\\\"\"\n" +
-                                "assert ctx._id == \"id1\" : \"id should be \\\"id1\\\"\"\n" +
-                                "assert ctx._version == 1 : \"version should be 1\"\n" +
-                                "assert ctx._parent == \"parentId1\" : \"parent should be \\\"parentId1\\\"\"\n" +
-                                "assert ctx._routing == \"routing1\" : \"routing should be \\\"routing1\\\"\"\n" +
-                                "assert ctx._timestamp == " + timestamp + " : \"timestamp should be " + timestamp + "\"\n" +
-                                // ttl has a 3-second leeway, because it's always counting down
-                                "assert ctx._ttl <= " + ttl + " : \"ttl should be <= " + ttl + " but was \" + ctx._ttl\n" +
-                                "assert ctx._ttl >= " + (ttl-3000) + " : \"ttl should be <= " + (ttl-3000) + " but was \" + ctx._ttl\n" +
-                                "ctx._source.content = ctx._source.content + delim + ctx._source.content;\n" +
-                                "ctx._source.field1 += 1;\n",
- ScriptService.ScriptType.INLINE, null, scriptParams))
-                .execute().actionGet();
-
-        assertEquals(2, updateResponse.getVersion());
-
-        GetResponse getResponse = client().prepareGet("test", "subtype1", "id1").setRouting("routing1").execute().actionGet();
-        assertEquals(2, getResponse.getSourceAsMap().get("field1"));
-        assertEquals("foo_foo", getResponse.getSourceAsMap().get("content"));
-
-        // Idem with the second object
-        scriptParams = new HashMap<>();
-        scriptParams.put("delim", "_");
-        updateResponse = client().prepareUpdate("test", "type1", "parentId1")
-                .setScript(
-                        new Script(
-                        "assert ctx._index == \"test\" : \"index should be \\\"test\\\"\"\n" +
-                        "assert ctx._type == \"type1\" : \"type should be \\\"type1\\\"\"\n" +
-                        "assert ctx._id == \"parentId1\" : \"id should be \\\"parentId1\\\"\"\n" +
-                        "assert ctx._version == 1 : \"version should be 1\"\n" +
-                        "assert ctx._parent == null : \"parent should be null\"\n" +
-                        "assert ctx._routing == null : \"routing should be null\"\n" +
-                        "assert ctx._timestamp == " + (timestamp - 1) + " : \"timestamp should be " + (timestamp - 1) + "\"\n" +
-                        "assert ctx._ttl == null : \"ttl should be null\"\n" +
-                        "ctx._source.content = ctx._source.content + delim + ctx._source.content;\n" +
-                        "ctx._source.field1 += 1;\n",
- ScriptService.ScriptType.INLINE, null, scriptParams))
-                .execute().actionGet();
-
-        assertEquals(2, updateResponse.getVersion());
-
-        getResponse = client().prepareGet("test", "type1", "parentId1").execute().actionGet();
-        assertEquals(1, getResponse.getSourceAsMap().get("field1"));
-        assertEquals("bar_bar", getResponse.getSourceAsMap().get("content"));
-    }
-
-    @Test
-    public void testConcurrentUpdateWithRetryOnConflict() throws Exception {
-        final boolean useBulkApi = randomBoolean();
-        createTestIndex();
-        ensureGreen();
-
-        int numberOfThreads = scaledRandomIntBetween(2,5);
-        final CountDownLatch latch = new CountDownLatch(numberOfThreads);
-        final CountDownLatch startLatch = new CountDownLatch(1);
-        final int numberOfUpdatesPerThread = scaledRandomIntBetween(100, 10000);
-        final List<Throwable> failures = new CopyOnWriteArrayList<>();
-        for (int i = 0; i < numberOfThreads; i++) {
-            Runnable r = new Runnable() {
-
-                @Override
-                public void run() {
-                    try {
-                        startLatch.await();
-                        for (int i = 0; i < numberOfUpdatesPerThread; i++) {
-                            if (useBulkApi) {
-                                UpdateRequestBuilder updateRequestBuilder = client().prepareUpdate(indexOrAlias(), "type1", Integer.toString(i))
-                                        .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                                        .setRetryOnConflict(Integer.MAX_VALUE)
-                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject());
-                                client().prepareBulk().add(updateRequestBuilder).execute().actionGet();
-                            } else {
-                                client().prepareUpdate(indexOrAlias(), "type1", Integer.toString(i))
-                                        .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                                        .setRetryOnConflict(Integer.MAX_VALUE)
-                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
-                                        .execute().actionGet();
-                            }
-                        }
-                    } catch (Throwable e) {
-                        failures.add(e);
-                    } finally {
-                        latch.countDown();
-                    }
-                }
-
-            };
-            new Thread(r).start();
-        }
-        startLatch.countDown();
-        latch.await();
-        for (Throwable throwable : failures) {
-            logger.info("Captured failure on concurrent update:", throwable);
-        }
-        assertThat(failures.size(), equalTo(0));
-        for (int i = 0; i < numberOfUpdatesPerThread; i++) {
-            GetResponse response = client().prepareGet("test", "type1", Integer.toString(i)).execute().actionGet();
-            assertThat(response.getId(), equalTo(Integer.toString(i)));
-            assertThat(response.isExists(), equalTo(true));
-            assertThat(response.getVersion(), equalTo((long) numberOfThreads));
-            assertThat((Integer) response.getSource().get("field"), equalTo(numberOfThreads));
-        }
-    }
-
-    @Test
-    public void stressUpdateDeleteConcurrency() throws Exception {
-        //We create an index with merging disabled so that deletes don't get merged away
-        assertAcked(prepareCreate("test")
-                .addMapping("type1", XContentFactory.jsonBuilder()
-                        .startObject()
-                        .startObject("type1")
-                        .startObject("_timestamp").field("enabled", true).endObject()
-                        .startObject("_ttl").field("enabled", true).endObject()
-                        .endObject()
-                        .endObject())
-                .setSettings(Settings.builder().put(MergePolicyConfig.INDEX_MERGE_ENABLED, false)));
-        ensureGreen();
-
-        final int numberOfThreads = scaledRandomIntBetween(3,5);
-        final int numberOfIdsPerThread = scaledRandomIntBetween(3,10);
-        final int numberOfUpdatesPerId = scaledRandomIntBetween(10,100);
-        final int retryOnConflict = randomIntBetween(0,1);
-        final CountDownLatch latch = new CountDownLatch(numberOfThreads);
-        final CountDownLatch startLatch = new CountDownLatch(1);
-        final List<Throwable> failures = new CopyOnWriteArrayList<>();
-
-        final class UpdateThread extends Thread {
-            final Map<Integer,Integer> failedMap = new HashMap<>();
-            final int numberOfIds;
-            final int updatesPerId;
-            final int maxUpdateRequests = numberOfIdsPerThread*numberOfUpdatesPerId;
-            final int maxDeleteRequests = numberOfIdsPerThread*numberOfUpdatesPerId;
-            private final Semaphore updateRequestsOutstanding = new Semaphore(maxUpdateRequests);
-            private final Semaphore deleteRequestsOutstanding = new Semaphore(maxDeleteRequests);
-
-            public UpdateThread(int numberOfIds, int updatesPerId) {
-                this.numberOfIds = numberOfIds;
-                this.updatesPerId = updatesPerId;
-            }
-
-            final class UpdateListener implements ActionListener<UpdateResponse> {
-                int id;
-
-                public UpdateListener(int id) {
-                    this.id = id;
-                }
-
-                @Override
-                public void onResponse(UpdateResponse updateResponse) {
-                    updateRequestsOutstanding.release(1);
-                }
-
-                @Override
-                public void onFailure(Throwable e) {
-                    synchronized (failedMap) {
-                        incrementMapValue(id, failedMap);
-                    }
-                    updateRequestsOutstanding.release(1);
-                }
-
-            }
-
-            final class DeleteListener implements ActionListener<DeleteResponse> {
-                int id;
-
-                public DeleteListener(int id) {
-                    this.id = id;
-                }
-
-                @Override
-                public void onResponse(DeleteResponse deleteResponse) {
-                    deleteRequestsOutstanding.release(1);
-                }
-
-                @Override
-                public void onFailure(Throwable e) {
-                    synchronized (failedMap) {
-                        incrementMapValue(id, failedMap);
-                    }
-                    deleteRequestsOutstanding.release(1);
-                }
-            }
-
-            @Override
-            public void run(){
-                try {
-                    startLatch.await();
-                    boolean hasWaitedForNoNode = false;
-                    for (int j = 0; j < numberOfIds; j++) {
-                        for (int k = 0; k < numberOfUpdatesPerId; ++k) {
-                            updateRequestsOutstanding.acquire();
-                            try {
-                                UpdateRequest ur = client().prepareUpdate("test", "type1", Integer.toString(j))
-                                        .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                                        .setRetryOnConflict(retryOnConflict)
-                                        .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
-                                        .request();
-                                client().update(ur, new UpdateListener(j));
-                            } catch (NoNodeAvailableException nne) {
-                                updateRequestsOutstanding.release();
-                                synchronized (failedMap) {
-                                    incrementMapValue(j, failedMap);
-                                }
-                                if (hasWaitedForNoNode) {
-                                    throw nne;
-                                }
-                                logger.warn("Got NoNodeException waiting for 1 second for things to recover.");
-                                hasWaitedForNoNode = true;
-                                Thread.sleep(1000);
-                            }
-
-                            try {
-                                deleteRequestsOutstanding.acquire();
-                                DeleteRequest dr = client().prepareDelete("test", "type1", Integer.toString(j)).request();
-                                client().delete(dr, new DeleteListener(j));
-                            } catch (NoNodeAvailableException nne) {
-                                deleteRequestsOutstanding.release();
-                                synchronized (failedMap) {
-                                    incrementMapValue(j, failedMap);
-                                }
-                                if (hasWaitedForNoNode) {
-                                    throw nne;
-                                }
-                                logger.warn("Got NoNodeException waiting for 1 second for things to recover.");
-                                hasWaitedForNoNode = true;
-                                Thread.sleep(1000); //Wait for no-node to clear
-                            }
-                        }
-                    }
-                } catch (Throwable e) {
-                    logger.error("Something went wrong", e);
-                    failures.add(e);
-                } finally {
-                    try {
-                        waitForOutstandingRequests(TimeValue.timeValueSeconds(60), updateRequestsOutstanding, maxUpdateRequests, "Update");
-                        waitForOutstandingRequests(TimeValue.timeValueSeconds(60), deleteRequestsOutstanding, maxDeleteRequests, "Delete");
-                    } catch (ElasticsearchTimeoutException ete) {
-                        failures.add(ete);
-                    }
-                    latch.countDown();
-                }
-            }
-
-            private void incrementMapValue(int j, Map<Integer,Integer> map) {
-                if (!map.containsKey(j)) {
-                    map.put(j, 0);
-                }
-                map.put(j, map.get(j) + 1);
-            }
-
-            private void waitForOutstandingRequests(TimeValue timeOut, Semaphore requestsOutstanding, int maxRequests, String name) {
-                long start = System.currentTimeMillis();
-                do {
-                    long msRemaining = timeOut.getMillis() - (System.currentTimeMillis() - start);
-                    logger.info("[{}] going to try and acquire [{}] in [{}]ms [{}] available to acquire right now",name, maxRequests,msRemaining, requestsOutstanding.availablePermits());
-                    try {
-                        requestsOutstanding.tryAcquire(maxRequests, msRemaining, TimeUnit.MILLISECONDS );
-                        return;
-                    } catch (InterruptedException ie) {
-                        //Just keep swimming
-                    }
-                } while ((System.currentTimeMillis() - start) < timeOut.getMillis());
-                throw new ElasticsearchTimeoutException("Requests were still outstanding after the timeout [" + timeOut + "] for type [" + name + "]" );
-            }
-        }
-        final List<UpdateThread> threads = new ArrayList<>();
-
-        for (int i = 0; i < numberOfThreads; i++) {
-            UpdateThread ut = new UpdateThread(numberOfIdsPerThread, numberOfUpdatesPerId);
-            ut.start();
-            threads.add(ut);
-        }
-
-        startLatch.countDown();
-        latch.await();
-
-        for (UpdateThread ut : threads){
-            ut.join(); //Threads should have finished because of the latch.await
-        }
-
-        //If are no errors every request received a response otherwise the test would have timedout
-        //aquiring the request outstanding semaphores.
-        for (Throwable throwable : failures) {
-            logger.info("Captured failure on concurrent update:", throwable);
-        }
-
-        assertThat(failures.size(), equalTo(0));
-
-        //Upsert all the ids one last time to make sure they are available at get time
-        //This means that we add 1 to the expected versions and attempts
-        //All the previous operations should be complete or failed at this point
-        for (int i = 0; i < numberOfIdsPerThread; ++i) {
-            UpdateResponse ur = client().prepareUpdate("test", "type1", Integer.toString(i))
-                    .setScript(new Script("ctx._source.field += 1", ScriptService.ScriptType.INLINE, null, null))
-                .setRetryOnConflict(Integer.MAX_VALUE)
-                .setUpsert(jsonBuilder().startObject().field("field", 1).endObject())
-                .execute().actionGet();
-        }
-
-        refresh();
-
-        for (int i = 0; i < numberOfIdsPerThread; ++i) {
-            int totalFailures = 0;
-            GetResponse response = client().prepareGet("test", "type1", Integer.toString(i)).execute().actionGet();
-            if (response.isExists()) {
-                assertThat(response.getId(), equalTo(Integer.toString(i)));
-                int expectedVersion = (numberOfThreads * numberOfUpdatesPerId * 2) + 1;
-                for (UpdateThread ut : threads) {
-                    if (ut.failedMap.containsKey(i)) {
-                        totalFailures += ut.failedMap.get(i);
-                    }
-                }
-                expectedVersion -= totalFailures;
-                logger.error("Actual version [{}] Expected version [{}] Total failures [{}]", response.getVersion(), expectedVersion, totalFailures);
-                assertThat(response.getVersion(), equalTo((long) expectedVersion));
-                assertThat(response.getVersion() + totalFailures,
-                        equalTo(
-                                (long)((numberOfUpdatesPerId * numberOfThreads * 2) + 1)
-                ));
-            }
-        }
-    }
-
-    private static String indexOrAlias() {
-        return randomBoolean() ? "test" : "alias";
-    }
-}
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
index c4d9366..a0d2c78 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
@@ -80,7 +80,6 @@
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentilesIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TDigestPercentilesTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/bucket/TopHitsIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TopHitsTests.java
   renamed:    core/src/test/java/org/elasticsearch/index/mapper/TransformOnIndexMapperIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/TransformOnIndexMapperTests.java
-  renamed:    core/src/test/java/org/elasticsearch/update/UpdateIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/UpdateTests.java
   renamed:    core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/ValueCountTests.java
   renamed:    core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptCompilationException.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyRestIT.java
   renamed:    core/src/test/java/org/elasticsearch/script/GroovyScriptIT.java -> plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java
index 337f4eb..c9fabd7 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java
@@ -22,14 +22,12 @@ package org.elasticsearch.script.groovy;
 import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.search.SearchPhaseExecutionException;
 import org.elasticsearch.action.search.SearchResponse;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.plugins.Plugin;
 import org.elasticsearch.script.Script;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.script.groovy.GroovyScriptEngineService;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -38,14 +36,9 @@ import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.scriptQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertOrderedSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
 /**
@@ -64,16 +57,16 @@ public class GroovyScriptTests extends ESIntegTestCase {
         client().prepareIndex("test", "doc", "1").setSource("foo", 5).setRefresh(true).get();
 
         // Test that something that would usually be a BigDecimal is transformed into a Double
-        assertScript("def n = 1.23; assert n instanceof Double; return n;");
-        assertScript("def n = 1.23G; assert n instanceof Double; return n;");
-        assertScript("def n = BigDecimal.ONE; assert n instanceof BigDecimal; return n;");
+        assertScript("def n = 1.23; assert n instanceof Double;");
+        assertScript("def n = 1.23G; assert n instanceof Double;");
+        assertScript("def n = BigDecimal.ONE; assert n instanceof BigDecimal;");
     }
 
-    public void assertScript(String scriptString) {
-        Script script = new Script(scriptString, ScriptType.INLINE, "groovy", null);
+    public void assertScript(String script) {
         SearchResponse resp = client().prepareSearch("test")
-                .setSource(new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(SortBuilders.scriptSort(script, "number")))
-                .get();
+                .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                        "\"sort\":{\"_script\": {\"script\": \""+ script +
+                        "; 1\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertNoFailures(resp);
     }
 
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
index f002bd1..043a5d1 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
@@ -22,14 +22,10 @@ package org.elasticsearch.script.groovy;
 import org.apache.lucene.util.Constants;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.action.search.ShardSearchFailure;
+import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.plugins.Plugin;
-import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptException;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.elasticsearch.search.builder.SearchSourceBuilder;
-import org.elasticsearch.search.sort.SortBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
 
@@ -54,7 +50,7 @@ public class GroovySecurityTests extends ESIntegTestCase {
         super.setUp();
         assumeTrue("test requires security manager to be enabled", System.getSecurityManager() != null);
     }
-
+    
     @Override
     protected Collection<Class<? extends Plugin>> nodePlugins() {
         return Collections.singleton(GroovyPlugin.class);
@@ -83,7 +79,7 @@ public class GroovySecurityTests extends ESIntegTestCase {
         // Ranges
         assertSuccess("def range = 1..doc['foo'].value; def v = range.get(0)");
         // Maps
-        assertSuccess("def v = doc['foo'].value; def m = [:]; m.put(\"value\", v)");
+        assertSuccess("def v = doc['foo'].value; def m = [:]; m.put(\\\"value\\\", v)");
         // Times
         assertSuccess("def t = Instant.now().getMillis()");
         // GroovyCollections
@@ -91,42 +87,40 @@ public class GroovySecurityTests extends ESIntegTestCase {
 
         // Fail cases:
         // AccessControlException[access denied ("java.io.FilePermission" "<<ALL FILES>>" "execute")]
-        assertFailure("pr = Runtime.getRuntime().exec(\"touch /tmp/gotcha\"); pr.waitFor()");
+        assertFailure("pr = Runtime.getRuntime().exec(\\\"touch /tmp/gotcha\\\"); pr.waitFor()");
 
         // AccessControlException[access denied ("java.lang.RuntimePermission" "accessClassInPackage.sun.reflect")]
-        assertFailure("d = new DateTime(); d.getClass().getDeclaredMethod(\"year\").setAccessible(true)");
-        assertFailure("d = new DateTime(); d.\"${'get' + 'Class'}\"()." +
-                        "\"${'getDeclared' + 'Method'}\"(\"year\").\"${'set' + 'Accessible'}\"(false)");
-        assertFailure("Class.forName(\"org.joda.time.DateTime\").getDeclaredMethod(\"year\").setAccessible(true)");
+        assertFailure("d = new DateTime(); d.getClass().getDeclaredMethod(\\\"year\\\").setAccessible(true)");
+        assertFailure("d = new DateTime(); d.\\\"${'get' + 'Class'}\\\"()." +
+                        "\\\"${'getDeclared' + 'Method'}\\\"(\\\"year\\\").\\\"${'set' + 'Accessible'}\\\"(false)");
+        assertFailure("Class.forName(\\\"org.joda.time.DateTime\\\").getDeclaredMethod(\\\"year\\\").setAccessible(true)");
 
         // AccessControlException[access denied ("groovy.security.GroovyCodeSourcePermission" "/groovy/shell")]
         assertFailure("Eval.me('2 + 2')");
         assertFailure("Eval.x(5, 'x + 2')");
 
         // AccessControlException[access denied ("java.lang.RuntimePermission" "accessDeclaredMembers")]
-        assertFailure("d = new Date(); java.lang.reflect.Field f = Date.class.getDeclaredField(\"fastTime\");" +
-                " f.setAccessible(true); f.get(\"fastTime\")");
+        assertFailure("d = new Date(); java.lang.reflect.Field f = Date.class.getDeclaredField(\\\"fastTime\\\");" +
+                " f.setAccessible(true); f.get(\\\"fastTime\\\")");
 
         // AccessControlException[access denied ("java.io.FilePermission" "<<ALL FILES>>" "execute")]
-        assertFailure("def methodName = 'ex'; Runtime.\"${'get' + 'Runtime'}\"().\"${methodName}ec\"(\"touch /tmp/gotcha2\")");
+        assertFailure("def methodName = 'ex'; Runtime.\\\"${'get' + 'Runtime'}\\\"().\\\"${methodName}ec\\\"(\\\"touch /tmp/gotcha2\\\")");
         
         // test a directory we normally have access to, but the groovy script does not.
         Path dir = createTempDir();
         // TODO: figure out the necessary escaping for windows paths here :)
         if (!Constants.WINDOWS) {
             // access denied ("java.io.FilePermission" ".../tempDir-00N" "read")
-            assertFailure("new File(\"" + dir + "\").exists()");
+            assertFailure("new File(\\\"" + dir + "\\\").exists()");
         }
     }
 
     private void assertSuccess(String script) {
         logger.info("--> script: " + script);
-        SearchResponse resp = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(
-                                SortBuilders.scriptSort(new Script(script + "; doc['foo'].value + 2", ScriptType.INLINE, "groovy", null),
-                                        "number"))).get();
+        SearchResponse resp = client().prepareSearch("test")
+                .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                        "\"sort\":{\"_script\": {\"script\": \"" + script +
+                        "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertNoFailures(resp);
         assertEquals(1, resp.getHits().getTotalHits());
         assertThat(resp.getHits().getAt(0).getSortValues(), equalTo(new Object[]{7.0}));
@@ -134,12 +128,10 @@ public class GroovySecurityTests extends ESIntegTestCase {
 
     private void assertFailure(String script) {
         logger.info("--> script: " + script);
-        SearchResponse resp = client()
-                .prepareSearch("test")
-                .setSource(
-                        new SearchSourceBuilder().query(QueryBuilders.matchAllQuery()).sort(
-                                SortBuilders.scriptSort(new Script(script + "; doc['foo'].value + 2", ScriptType.INLINE, "groovy", null),
-                                        "number"))).get();
+        SearchResponse resp = client().prepareSearch("test")
+                 .setSource(new BytesArray("{\"query\": {\"match_all\": {}}," +
+                         "\"sort\":{\"_script\": {\"script\": \"" + script +
+                         "; doc['foo'].value + 2\", \"type\": \"number\", \"lang\": \"groovy\"}}}")).get();
         assertEquals(0, resp.getHits().getTotalHits());
         ShardSearchFailure fails[] = resp.getShardFailures();
         // TODO: GroovyScriptExecutionException needs work:
diff --git a/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index 8f0dedc..008a83e 100644
--- a/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/plugins/lang-groovy/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -32,13 +32,7 @@ import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.query.BoolQueryBuilder;
-import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
-import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.TermQueryBuilder;
-import org.elasticsearch.index.query.WrapperQueryBuilder;
+import org.elasticsearch.index.query.*;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.index.search.MatchQuery;
 import org.elasticsearch.index.search.MatchQuery.Type;
@@ -60,54 +54,10 @@ import java.util.concurrent.ExecutionException;
 import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
-import static org.elasticsearch.index.query.QueryBuilders.commonTermsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.constantScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.existsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.functionScoreQuery;
-import static org.elasticsearch.index.query.QueryBuilders.fuzzyQuery;
-import static org.elasticsearch.index.query.QueryBuilders.hasChildQuery;
-import static org.elasticsearch.index.query.QueryBuilders.idsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.indicesQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
-import static org.elasticsearch.index.query.QueryBuilders.matchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.missingQuery;
-import static org.elasticsearch.index.query.QueryBuilders.multiMatchQuery;
-import static org.elasticsearch.index.query.QueryBuilders.notQuery;
-import static org.elasticsearch.index.query.QueryBuilders.prefixQuery;
-import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
-import static org.elasticsearch.index.query.QueryBuilders.rangeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.regexpQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanMultiTermQueryBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.spanNearQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanNotQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanOrQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termsLookupQuery;
-import static org.elasticsearch.index.query.QueryBuilders.termsQuery;
-import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
-import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
-import static org.elasticsearch.index.query.QueryBuilders.wrapperQuery;
+import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThirdHit;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasScore;
-import static org.hamcrest.Matchers.allOf;
-import static org.hamcrest.Matchers.closeTo;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.greaterThan;
-import static org.hamcrest.Matchers.is;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.hamcrest.Matchers.*;
 
 public class SearchQueryIT extends ESIntegTestCase {
 
@@ -198,6 +148,15 @@ public class SearchQueryIT extends ESIntegTestCase {
     }
 
     @Test
+    public void passQueryAsStringTest() throws Exception {
+        createIndex("test");
+        client().prepareIndex("test", "type1", "1").setSource("field1", "value1_1", "field2", "value2_1").setRefresh(true).get();
+
+        SearchResponse searchResponse = client().prepareSearch().setQuery("{ \"term\" : { \"field1\" : \"value1_1\" }}").get();
+        assertHitCount(searchResponse, 1l);
+    }
+
+    @Test
     public void testIndexOptions() throws Exception {
         assertAcked(prepareCreate("test")
                 .addMapping("type1", "field1", "type=string,index_options=docs"));
@@ -352,6 +311,10 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("2"));
 
+        searchResponse = client().prepareSearch().setQuery("{ \"common\" : { \"field1\" : { \"query\" : \"the lazy fox brown\", \"cutoff_frequency\" : 1, \"minimum_should_match\" : { \"high_freq\" : 4 } } } }").get();
+        assertHitCount(searchResponse, 1l);
+        assertFirstHit(searchResponse, hasId("2"));
+
         // Default
         searchResponse = client().prepareSearch().setQuery(commonTermsQuery("field1", "the lazy fox brown").cutoffFrequency(1)).get();
         assertHitCount(searchResponse, 1l);
@@ -440,6 +403,10 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("2"));
 
+        searchResponse = client().prepareSearch().setQuery("{ \"common\" : { \"field1\" : { \"query\" : \"the fast lazy fox brown\", \"cutoff_frequency\" : 1, \"minimum_should_match\" : { \"high_freq\" : 6 } } } }").get();
+        assertHitCount(searchResponse, 1l);
+        assertFirstHit(searchResponse, hasId("2"));
+
         // Default
         searchResponse = client().prepareSearch().setQuery(commonTermsQuery("field1", "the fast lazy fox brown").cutoffFrequency(1)).get();
         assertHitCount(searchResponse, 1l);
@@ -1489,6 +1456,14 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 2l);
     }
 
+    @Test
+    public void testEmptyTopLevelFilter() {
+        client().prepareIndex("test", "type", "1").setSource("field", "value").setRefresh(true).get();
+
+        SearchResponse searchResponse = client().prepareSearch().setPostFilter("{}").get();
+        assertHitCount(searchResponse, 1l);
+    }
+
     @Test // see #2926
     public void testMustNot() throws IOException, ExecutionException, InterruptedException {
         assertAcked(prepareCreate("test")
@@ -2209,4 +2184,26 @@ public class SearchQueryIT extends ESIntegTestCase {
             assertThat(i + " expected: " + first + " actual: " + actual, Float.compare(first, actual), equalTo(0));
         }
     }
+
+    @Test // see #7686.
+    public void testIdsQueryWithInvalidValues() throws Exception {
+        createIndex("test");
+        indexRandom(true, false, client().prepareIndex("test", "type", "1").setSource("body", "foo"));
+
+        try {
+            client().prepareSearch("test")
+                    .setTypes("type")
+                    .setQuery("{\n" +
+                            "  \"ids\": {\n" +
+                            "    \"values\": [[\"1\"]]\n" +
+                            "  }\n" +
+                            "}")
+                    .get();
+            fail("query is invalid and should have produced a parse exception");
+        } catch (Exception e) {
+            assertThat("query could not be parsed due to bad format: " + e.toString(),
+                    e.toString().contains("Illegal value for id, expecting a string or number, got: START_ARRAY"),
+                    equalTo(true));
+        }
+    }
 }
diff --git a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
index 6d1625a..ee77a84 100644
--- a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
+++ b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/10_basic.yaml
@@ -18,10 +18,9 @@ setup:
             body:
                 script_fields:
                     bar:
-                        script: 
-                            inline: "doc['foo'].value + x"
-                            lang: javascript
-                            params:
-                                x: "bbb"
+                        lang: javascript
+                        script: "doc['foo'].value + x"
+                        params:
+                            x: "bbb"
 
     - match: { hits.hits.0.fields.bar.0: "aaabbb"}
diff --git a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
index 742c8f0..24a6c8b 100644
--- a/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
+++ b/plugins/lang-javascript/src/test/resources/rest-api-spec/test/lang_javascript/20_search.yaml
@@ -33,9 +33,8 @@
                             lang: js
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -58,9 +57,8 @@
 
                 script_fields:
                     sNum1:
-                        script:
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -83,9 +81,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: js
+                        lang: js
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -121,21 +118,17 @@
             body:
                 script_fields:
                     s_obj1:
-                        script: 
-                            inline: "_source.obj1"
-                            lang: js
+                        lang: js
+                        script: "_source.obj1"
                     s_obj1_test:
-                        script: 
-                            inline: "_source.obj1.test"
-                            lang: js
+                        lang: js
+                        script: "_source.obj1.test"
                     s_obj2:
-                        script: 
-                            inline: "_source.obj2"
-                            lang: js
+                        lang: js
+                        script: "_source.obj2"
                     s_obj2_arr2:
-                        script: 
-                            inline: "_source.obj2.arr2"
-                            lang: js
+                        lang: js
+                        script: "_source.obj2.arr2"
 
     - match: { hits.total: 1 }
     - match: { hits.hits.0.fields.s_obj1.0.test: something }
@@ -406,9 +399,8 @@
             body:
                 script_fields:
                     foobar:
-                        script: 
-                            inline: "doc['f'].values.length"
-                            lang: js
+                        lang: js
+                        script: "doc['f'].values.length"
 
 
     - match: { hits.total: 1 }
diff --git a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
index 4a811d1..ba7b733 100644
--- a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
+++ b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/10_basic.yaml
@@ -18,10 +18,9 @@ setup:
             body:
                 script_fields:
                     bar:
-                        script: 
-                            inline: "doc['foo'].value + x"
-                            lang: python
-                            params:
-                                x: "bbb"
+                        lang: python
+                        script: "doc['foo'].value + x"
+                        params:
+                            x: "bbb"
 
     - match: { hits.hits.0.fields.bar.0: "aaabbb"}
diff --git a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
index b0f18e1..d19561a 100644
--- a/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
+++ b/plugins/lang-python/src/test/resources/rest-api-spec/test/lang_python/20_search.yaml
@@ -33,9 +33,8 @@
                             lang: python
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -58,9 +57,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -83,9 +81,8 @@
 
                 script_fields:
                     sNum1:
-                        script: 
-                            inline: "doc['num1'].value"
-                            lang: python
+                        lang: python
+                        script: "doc['num1'].value"
                 sort:
                     num1:
                         order: asc
@@ -121,21 +118,17 @@
             body:
                 script_fields:
                     s_obj1:
-                        script: 
-                            inline: "_source['obj1']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj1']"
                     s_obj1_test:
-                        script: 
-                            inline: "_source['obj1']['test']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj1']['test']"
                     s_obj2:
-                        script: 
-                            inline: "_source['obj2']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj2']"
                     s_obj2_arr2:
-                        script: 
-                            inline: "_source['obj2']['arr2']"
-                            lang: python
+                        lang: python
+                        script: "_source['obj2']['arr2']"
 
     - match: { hits.total: 1 }
     - match: { hits.hits.0.fields.s_obj1.0.test: something }
diff --git a/plugins/pom.xml b/plugins/pom.xml
index d080a55..1502b8b 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -120,11 +120,6 @@
             <scope>provided</scope>
         </dependency>
         <dependency>
-            <groupId>com.google.guava</groupId>
-            <artifactId>guava</artifactId>
-            <scope>provided</scope>
-        </dependency>
-        <dependency>
             <groupId>com.carrotsearch</groupId>
             <artifactId>hppc</artifactId>
             <scope>provided</scope>
diff --git a/pom.xml b/pom.xml
index f966923..41b0f6e 100644
--- a/pom.xml
+++ b/pom.xml
@@ -302,7 +302,7 @@
             <dependency>
                 <groupId>com.github.spullara.mustache.java</groupId>
                 <artifactId>compiler</artifactId>
-                <version>0.8.13</version>
+                <version>0.9.1</version>
             </dependency>
 
             <!-- Used in plugins -->
@@ -333,12 +333,6 @@
             </dependency>
 
             <dependency>
-                <groupId>com.google.guava</groupId>
-                <artifactId>guava</artifactId>
-                <version>18.0</version>
-            </dependency>
-
-            <dependency>
                 <groupId>com.carrotsearch</groupId>
                 <artifactId>hppc</artifactId>
                 <version>0.7.1</version>
@@ -986,8 +980,7 @@
                             <headerDefinition>${elasticsearch.license.headerDefinition}</headerDefinition>
                         </headerDefinitions>
                         <includes>
-                            <include>src/main/java/org/elasticsearch/**/*.java</include>
-                            <include>src/test/java/org/elasticsearch/**/*.java</include>
+                            <include>src/**/*.java</include>
                         </includes>
                     </configuration>
                     <executions>
diff --git a/qa/smoke-test-multinode/pom.xml b/qa/smoke-test-multinode/pom.xml
index a4788dd..75bd37f 100644
--- a/qa/smoke-test-multinode/pom.xml
+++ b/qa/smoke-test-multinode/pom.xml
@@ -106,11 +106,6 @@
       <scope>provided</scope>
     </dependency>
     <dependency>
-      <groupId>com.google.guava</groupId>
-      <artifactId>guava</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
       <groupId>com.carrotsearch</groupId>
       <artifactId>hppc</artifactId>
       <scope>provided</scope>
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index f9d4b79..3e55631 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -111,11 +111,6 @@
       <scope>provided</scope>
     </dependency>
     <dependency>
-      <groupId>com.google.guava</groupId>
-      <artifactId>guava</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
       <groupId>com.carrotsearch</groupId>
       <artifactId>hppc</artifactId>
       <scope>provided</scope>
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/os_package.bash b/qa/vagrant/src/test/resources/packaging/scripts/os_package.bash
index 4907cdf..f48532c 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/os_package.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/os_package.bash
@@ -72,38 +72,39 @@ verify_package_installation() {
 
     getent group elasticsearch
 
-    assert_file "$ESHOME" d root 755
-    assert_file "$ESHOME/bin" d root 755
-    assert_file "$ESHOME/lib" d root 755
-    assert_file "$ESCONFIG" d root 755
-    assert_file "$ESCONFIG/elasticsearch.yml" f root 644
-    assert_file "$ESCONFIG/logging.yml" f root 644
-    assert_file "$ESDATA" d elasticsearch 755
-    assert_file "$ESLOG" d elasticsearch 755
-    assert_file "$ESPLUGINS" d elasticsearch 755
-    assert_file "$ESPIDDIR" d elasticsearch 755
-    assert_file "$ESHOME/NOTICE.txt" f root 644
-    assert_file "$ESHOME/README.textile" f root 644
+    assert_file "$ESHOME" d root root 755
+    assert_file "$ESHOME/bin" d root root 755
+    assert_file "$ESHOME/lib" d root root 755
+    assert_file "$ESCONFIG" d root elasticsearch 750
+    assert_file "$ESCONFIG/elasticsearch.yml" f root elasticsearch 750
+    assert_file "$ESCONFIG/logging.yml" f root elasticsearch 750
+    assert_file "$ESSCRIPTS" d root elasticsearch 750
+    assert_file "$ESDATA" d elasticsearch elasticsearch 755
+    assert_file "$ESLOG" d elasticsearch elasticsearch 755
+    assert_file "$ESPLUGINS" d elasticsearch elasticsearch 755
+    assert_file "$ESPIDDIR" d elasticsearch elasticsearch 755
+    assert_file "$ESHOME/NOTICE.txt" f root root 644
+    assert_file "$ESHOME/README.textile" f root root 644
 
     if is_dpkg; then
         # Env file
-        assert_file "/etc/default/elasticsearch" f root 644
+        assert_file "/etc/default/elasticsearch" f root root 644
 
         # Doc files
-        assert_file "/usr/share/doc/elasticsearch" d root 755
-        assert_file "/usr/share/doc/elasticsearch/copyright" f root 644
+        assert_file "/usr/share/doc/elasticsearch" d root root 755
+        assert_file "/usr/share/doc/elasticsearch/copyright" f root root 644
     fi
 
     if is_rpm; then
         # Env file
-        assert_file "/etc/sysconfig/elasticsearch" f root 644
+        assert_file "/etc/sysconfig/elasticsearch" f root root 644
         # License file
-        assert_file "/usr/share/elasticsearch/LICENSE.txt" f root 644
+        assert_file "/usr/share/elasticsearch/LICENSE.txt" f root root 644
     fi
 
     if is_systemd; then
-        assert_file "/usr/lib/systemd/system/elasticsearch.service" f root 644
-        assert_file "/usr/lib/tmpfiles.d/elasticsearch.conf" f root 644
-        assert_file "/usr/lib/sysctl.d/elasticsearch.conf" f root 644
+        assert_file "/usr/lib/systemd/system/elasticsearch.service" f root root 644
+        assert_file "/usr/lib/tmpfiles.d/elasticsearch.conf" f root root 644
+        assert_file "/usr/lib/sysctl.d/elasticsearch.conf" f root root 644
     fi
 }
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
index c81af43..599f6ba 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
@@ -150,7 +150,8 @@ assert_file() {
     local file="$1"
     local type=$2
     local user=$3
-    local privileges=$4
+    local group=$4
+    local privileges=$5
 
     assert_file_exist "$file"
 
@@ -167,6 +168,11 @@ assert_file() {
         [ "$realuser" = "$user" ]
     fi
 
+    if [ "x$group" != "x" ]; then
+        realgroup=$(find "$file" -maxdepth 0 -printf "%g")
+        [ "$realgroup" = "$group" ]
+    fi
+
     if [ "x$privileges" != "x" ]; then
         realprivileges=$(find "$file" -maxdepth 0 -printf "%m")
         [ "$realprivileges" = "$privileges" ]
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
index 7e4c574..44313aa 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/10_basic.yaml
@@ -35,7 +35,7 @@ setup:
         index: test_index
         name: test_warmer
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
 
   - do:
       indices.delete_warmer:
@@ -55,8 +55,8 @@ setup:
   - do:
       indices.get_warmer: {}
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 
 ---
@@ -67,8 +67,8 @@ setup:
         index: '*'
         name: '*'
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using _all":
@@ -78,8 +78,8 @@ setup:
         index: _all
         name: _all
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting all warmers without specifying index should work":
@@ -88,8 +88,8 @@ setup:
       indices.get_warmer:
         name: _all
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using prefix*":
@@ -99,8 +99,8 @@ setup:
         index: test_i*
         name: test_w*
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting warmers for several indices should work using comma-separated lists":
@@ -110,8 +110,8 @@ setup:
         index: test_index,test_idx
         name: test_warmer,test_warmer2
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
+  - match: {test_idx.warmers.test_warmer2.source.query.match_all: {}}
 
 ---
 "Getting a non-existent warmer on an existing index should return an empty body":
@@ -131,7 +131,7 @@ setup:
         index: test_index
         name: test_warmer,non-existent
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
   - is_false: test_index.warmers.non-existent
 
 --- 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
index b8a2fa6..96d7344 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/20_aliases.yaml
@@ -26,5 +26,5 @@
       indices.get_warmer:
           index: test_alias
 
-  - match: {test_index.warmers.test_warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index.warmers.test_warmer.source.query.match_all: {}}
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
index ffad427..b9c64f7 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/indices.put_warmer/all_path_options.yaml
@@ -38,8 +38,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -54,9 +54,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer in * index":
@@ -70,9 +70,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer prefix* index":
@@ -86,8 +86,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -102,8 +102,8 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
   - is_false: foo
 
 ---
@@ -117,9 +117,9 @@ setup:
   - do:
       indices.get_warmer: { index: _all, name: '*' }
 
-  - match: {test_index1.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {test_index2.warmers.warmer.source.query.match_all: {boost: 1.0}}
-  - match: {foo.warmers.warmer.source.query.match_all: {boost: 1.0}}
+  - match: {test_index1.warmers.warmer.source.query.match_all: {}}
+  - match: {test_index2.warmers.warmer.source.query.match_all: {}}
+  - match: {foo.warmers.warmer.source.query.match_all: {}}
 
 ---
 "put warmer with missing name":
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
index b49d659..a78a5a2 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/search/10_source_filtering.yaml
@@ -14,12 +14,12 @@
   - do:
       search:
       # stringified for boolean value
-        body: { _source: true, query: { match_all: {} } }
+        body: "{ _source: true, query: { match_all: {} } }"
 
   - length:   { hits.hits: 1  }
   - match: { hits.hits.0._source.count: 1 }
 
-  - do: { search: { body: { _source: false, query: { match_all: {} } } } }
+  - do: { search: { body: "{ _source: false, query: { match_all: {} } }" } }
   - length:   { hits.hits: 1  }
   - is_false: hits.hits.0._source
 
diff --git a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml b/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
index 4da748a..5153f6c 100644
--- a/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
+++ b/rest-api-spec/src/main/resources/rest-api-spec/test/template/20_search.yaml
@@ -28,11 +28,16 @@
 
   - do:
       search_template:
+        body: { "template": { "id" : "1" }, "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
+  - match: { hits.total: 1 }
+
+  - do:
+      search_template:
         body: {  "id" : "1", "params" : { "my_value" : "value1_foo", "my_size" : 1 } }
   - match: { hits.total: 1 }
 
   - do:
       catch: /Unable.to.find.on.disk.file.script.\[simple1\].using.lang.\[mustache\]/
       search_template:
-        body: { "file" : "simple1"}
+        body: { "template" : "simple1" }
 
