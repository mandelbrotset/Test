diff --git a/README.textile b/README.textile
index 63f1841..f48e40c 100644
--- a/README.textile
+++ b/README.textile
@@ -42,7 +42,7 @@ h3. Installation
 
 * "Download":https://www.elastic.co/downloads/elasticsearch and unzip the Elasticsearch official distribution.
 * Run @bin/elasticsearch@ on unix, or @bin\elasticsearch.bat@ on windows.
-* Run @curl -X GET http://localhost:9200/@.
+* Run @curl -X GET http://127.0.0.1:9200/@.
 * Start more servers ...
 
 h3. Indexing
@@ -50,16 +50,16 @@ h3. Indexing
 Let's try and index some twitter like information. First, let's create a twitter user, and add some tweets (the @twitter@ index will be created automatically):
 
 <pre>
-curl -XPUT 'http://localhost:9200/twitter/user/kimchy' -d '{ "name" : "Shay Banon" }'
+curl -XPUT 'http://127.0.0.1:9200/twitter/user/kimchy' -d '{ "name" : "Shay Banon" }'
 
-curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '
+curl -XPUT 'http://127.0.0.1:9200/twitter/tweet/1' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T13:12:00",
     "message": "Trying out Elasticsearch, so far so good?"
 }'
 
-curl -XPUT 'http://localhost:9200/twitter/tweet/2' -d '
+curl -XPUT 'http://127.0.0.1:9200/twitter/tweet/2' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T14:12:12",
@@ -70,9 +70,9 @@ curl -XPUT 'http://localhost:9200/twitter/tweet/2' -d '
 Now, let's see if the information was added by GETting it:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/user/kimchy?pretty=true'
-curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty=true'
-curl -XGET 'http://localhost:9200/twitter/tweet/2?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/user/kimchy?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/1?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/2?pretty=true'
 </pre>
 
 h3. Searching
@@ -81,13 +81,13 @@ Mmm search..., shouldn't it be elastic?
 Let's find all the tweets that @kimchy@ posted:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/tweet/_search?q=user:kimchy&pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/_search?q=user:kimchy&pretty=true'
 </pre>
 
 We can also use the JSON query language Elasticsearch provides instead of a query string:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/_search?pretty=true' -d '
 {
     "query" : {
         "match" : { "user": "kimchy" }
@@ -98,7 +98,7 @@ curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
 Just for kicks, let's get all the documents stored (we should see the user as well):
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
@@ -109,7 +109,7 @@ curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
 We can also do range search (the @postDate@ was automatically identified as date)
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/_search?pretty=true' -d '
 {
     "query" : {
         "range" : {
@@ -130,16 +130,16 @@ Elasticsearch supports multiple indices, as well as multiple types per index. In
 Another way to define our simple twitter system is to have a different index per user (note, though that each index has an overhead). Here is the indexing curl's in this case:
 
 <pre>
-curl -XPUT 'http://localhost:9200/kimchy/info/1' -d '{ "name" : "Shay Banon" }'
+curl -XPUT 'http://127.0.0.1:9200/kimchy/info/1' -d '{ "name" : "Shay Banon" }'
 
-curl -XPUT 'http://localhost:9200/kimchy/tweet/1' -d '
+curl -XPUT 'http://127.0.0.1:9200/kimchy/tweet/1' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T13:12:00",
     "message": "Trying out Elasticsearch, so far so good?"
 }'
 
-curl -XPUT 'http://localhost:9200/kimchy/tweet/2' -d '
+curl -XPUT 'http://127.0.0.1:9200/kimchy/tweet/2' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T14:12:12",
@@ -152,7 +152,7 @@ The above will index information into the @kimchy@ index, with two types, @info@
 Complete control on the index level is allowed. As an example, in the above case, we would want to change from the default 5 shards with 1 replica per index, to only 1 shard with 1 replica per index (== per twitter user). Here is how this can be done (the configuration can be in yaml as well):
 
 <pre>
-curl -XPUT http://localhost:9200/another_user/ -d '
+curl -XPUT http://127.0.0.1:9200/another_user/ -d '
 {
     "index" : {
         "numberOfShards" : 1,
@@ -165,7 +165,7 @@ Search (and similar operations) are multi index aware. This means that we can ea
 index (twitter user), for example:
 
 <pre>
-curl -XGET 'http://localhost:9200/kimchy,another_user/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/kimchy,another_user/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
@@ -176,7 +176,7 @@ curl -XGET 'http://localhost:9200/kimchy,another_user/_search?pretty=true' -d '
 Or on all the indices:
 
 <pre>
-curl -XGET 'http://localhost:9200/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
diff --git a/TESTING.asciidoc b/TESTING.asciidoc
index 2a480c5..f35d32d 100644
--- a/TESTING.asciidoc
+++ b/TESTING.asciidoc
@@ -473,11 +473,9 @@ and in another window:
 ----------------------------------------------------
 vagrant up centos-7 && vagrant ssh centos-7
 cd $RPM
-sudo ES_CLEAN_BEFORE_TEST=true bats $BATS/*rpm*.bats
+sudo bats $BATS/*rpm*.bats
 ----------------------------------------------------
 
-At this point `ES_CLEAN_BEFORE_TEST=true` is required or tests fail spuriously.
-
 If you wanted to retest all the release artifacts on a single VM you could:
 -------------------------------------------------
 # Build all the distributions fresh but skip recompiling elasticsearch:
@@ -486,7 +484,7 @@ mvn -amd -pl distribution install -DskipTests
 mvn -Dtests.vagrant -pl qa/vagrant pre-integration-test
 vagrant up trusty && vagrant ssh trusty
 cd $TESTROOT
-sudo ES_CLEAN_BEFORE_TEST=true bats $BATS/*.bats
+sudo bats $BATS/*.bats
 -------------------------------------------------
 
 == Coverage analysis
diff --git a/core/README.textile b/core/README.textile
index 720f357..b2873e8 100644
--- a/core/README.textile
+++ b/core/README.textile
@@ -42,7 +42,7 @@ h3. Installation
 
 * "Download":https://www.elastic.co/downloads/elasticsearch and unzip the Elasticsearch official distribution.
 * Run @bin/elasticsearch@ on unix, or @bin\elasticsearch.bat@ on windows.
-* Run @curl -X GET http://localhost:9200/@.
+* Run @curl -X GET http://127.0.0.1:9200/@.
 * Start more servers ...
 
 h3. Indexing
@@ -50,16 +50,16 @@ h3. Indexing
 Let's try and index some twitter like information. First, let's create a twitter user, and add some tweets (the @twitter@ index will be created automatically):
 
 <pre>
-curl -XPUT 'http://localhost:9200/twitter/user/kimchy' -d '{ "name" : "Shay Banon" }'
+curl -XPUT 'http://127.0.0.1:9200/twitter/user/kimchy' -d '{ "name" : "Shay Banon" }'
 
-curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '
+curl -XPUT 'http://127.0.0.1:9200/twitter/tweet/1' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T13:12:00",
     "message": "Trying out Elasticsearch, so far so good?"
 }'
 
-curl -XPUT 'http://localhost:9200/twitter/tweet/2' -d '
+curl -XPUT 'http://127.0.0.1:9200/twitter/tweet/2' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T14:12:12",
@@ -70,9 +70,9 @@ curl -XPUT 'http://localhost:9200/twitter/tweet/2' -d '
 Now, let's see if the information was added by GETting it:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/user/kimchy?pretty=true'
-curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty=true'
-curl -XGET 'http://localhost:9200/twitter/tweet/2?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/user/kimchy?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/1?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/2?pretty=true'
 </pre>
 
 h3. Searching
@@ -81,13 +81,13 @@ Mmm search..., shouldn't it be elastic?
 Let's find all the tweets that @kimchy@ posted:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/tweet/_search?q=user:kimchy&pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/_search?q=user:kimchy&pretty=true'
 </pre>
 
 We can also use the JSON query language Elasticsearch provides instead of a query string:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/_search?pretty=true' -d '
 {
     "query" : {
         "match" : { "user": "kimchy" }
@@ -98,7 +98,7 @@ curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
 Just for kicks, let's get all the documents stored (we should see the user as well):
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
@@ -109,7 +109,7 @@ curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
 We can also do range search (the @postDate@ was automatically identified as date)
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/_search?pretty=true' -d '
 {
     "query" : {
         "range" : {
@@ -130,16 +130,16 @@ Elasticsearch supports multiple indices, as well as multiple types per index. In
 Another way to define our simple twitter system is to have a different index per user (note, though that each index has an overhead). Here is the indexing curl's in this case:
 
 <pre>
-curl -XPUT 'http://localhost:9200/kimchy/info/1' -d '{ "name" : "Shay Banon" }'
+curl -XPUT 'http://127.0.0.1:9200/kimchy/info/1' -d '{ "name" : "Shay Banon" }'
 
-curl -XPUT 'http://localhost:9200/kimchy/tweet/1' -d '
+curl -XPUT 'http://127.0.0.1:9200/kimchy/tweet/1' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T13:12:00",
     "message": "Trying out Elasticsearch, so far so good?"
 }'
 
-curl -XPUT 'http://localhost:9200/kimchy/tweet/2' -d '
+curl -XPUT 'http://127.0.0.1:9200/kimchy/tweet/2' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T14:12:12",
@@ -152,7 +152,7 @@ The above will index information into the @kimchy@ index, with two types, @info@
 Complete control on the index level is allowed. As an example, in the above case, we would want to change from the default 5 shards with 1 replica per index, to only 1 shard with 1 replica per index (== per twitter user). Here is how this can be done (the configuration can be in yaml as well):
 
 <pre>
-curl -XPUT http://localhost:9200/another_user/ -d '
+curl -XPUT http://127.0.0.1:9200/another_user/ -d '
 {
     "index" : {
         "numberOfShards" : 1,
@@ -165,7 +165,7 @@ Search (and similar operations) are multi index aware. This means that we can ea
 index (twitter user), for example:
 
 <pre>
-curl -XGET 'http://localhost:9200/kimchy,another_user/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/kimchy,another_user/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
@@ -176,7 +176,7 @@ curl -XGET 'http://localhost:9200/kimchy,another_user/_search?pretty=true' -d '
 Or on all the indices:
 
 <pre>
-curl -XGET 'http://localhost:9200/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
diff --git a/core/pom.xml b/core/pom.xml
index e039cc8..dbc1b02 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch</groupId>
         <artifactId>elasticsearch-parent</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch</groupId>
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
index cb4bee3..6cac629 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java
@@ -21,8 +21,8 @@ package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.ExistsQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.ExistsQueryParser;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
@@ -32,7 +32,7 @@ public class ExistsFieldQueryExtension implements FieldQueryExtension {
     public static final String NAME = "_exists_";
 
     @Override
-    public Query query(QueryShardContext context, String queryText) {
-        return new ConstantScoreQuery(ExistsQueryBuilder.newFilter(context, queryText));
+    public Query query(QueryParseContext parseContext, String queryText) {
+        return new ConstantScoreQuery(ExistsQueryParser.newFilter(parseContext, queryText, null));
     }
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
index 299a37a..003ff18 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java
@@ -20,12 +20,12 @@
 package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
  */
 public interface FieldQueryExtension {
 
-    Query query(QueryShardContext context, String queryText);
+    Query query(QueryParseContext parseContext, String queryText);
 }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
index 6974dc0..4cfdc25 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java
@@ -39,7 +39,7 @@ import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import com.google.common.base.Objects;
@@ -70,7 +70,7 @@ public class MapperQueryParser extends QueryParser {
                 .build();
     }
 
-    private final QueryShardContext context;
+    private final QueryParseContext parseContext;
 
     private QueryParserSettings settings;
 
@@ -85,9 +85,15 @@ public class MapperQueryParser extends QueryParser {
 
     private String quoteFieldSuffix;
 
-    public MapperQueryParser(QueryShardContext context) {
+    public MapperQueryParser(QueryParseContext parseContext) {
         super(null, null);
-        this.context = context;
+        this.parseContext = parseContext;
+    }
+
+    public MapperQueryParser(QueryParserSettings settings, QueryParseContext parseContext) {
+        super(settings.defaultField(), settings.defaultAnalyzer());
+        this.parseContext = parseContext;
+        reset(settings);
     }
 
     public void reset(QueryParserSettings settings) {
@@ -162,7 +168,7 @@ public class MapperQueryParser extends QueryParser {
     public Query getFieldQuery(String field, String queryText, boolean quoted) throws ParseException {
         FieldQueryExtension fieldQueryExtension = fieldQueryExtensions.get(field);
         if (fieldQueryExtension != null) {
-            return fieldQueryExtension.query(context, queryText);
+            return fieldQueryExtension.query(parseContext, queryText);
         }
         Collection<String> fields = extractMultiFields(field);
         if (fields != null) {
@@ -226,27 +232,27 @@ public class MapperQueryParser extends QueryParser {
             if (quoted) {
                 setAnalyzer(quoteAnalyzer);
                 if (quoteFieldSuffix != null) {
-                    currentFieldType = context.fieldMapper(field + quoteFieldSuffix);
+                    currentFieldType = parseContext.fieldMapper(field + quoteFieldSuffix);
                 }
             }
             if (currentFieldType == null) {
-                currentFieldType = context.fieldMapper(field);
+                currentFieldType = parseContext.fieldMapper(field);
             }
             if (currentFieldType != null) {
                 if (quoted) {
                     if (!forcedQuoteAnalyzer) {
-                        setAnalyzer(context.getSearchQuoteAnalyzer(currentFieldType));
+                        setAnalyzer(parseContext.getSearchQuoteAnalyzer(currentFieldType));
                     }
                 } else {
                     if (!forcedAnalyzer) {
-                        setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                        setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                     }
                 }
                 if (currentFieldType != null) {
                     Query query = null;
                     if (currentFieldType.useTermQueryWithQueryString()) {
                         try {
-                            query = currentFieldType.termQuery(queryText, context);
+                            query = currentFieldType.termQuery(queryText, parseContext);
                         } catch (RuntimeException e) {
                             if (settings.lenient()) {
                                 return null;
@@ -357,7 +363,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getRangeQuerySingle(String field, String part1, String part2, boolean startInclusive, boolean endInclusive) {
-        currentFieldType = context.fieldMapper(field);
+        currentFieldType = parseContext.fieldMapper(field);
         if (currentFieldType != null) {
             if (lowercaseExpandedTerms && !currentFieldType.isNumeric()) {
                 part1 = part1 == null ? null : part1.toLowerCase(locale);
@@ -422,7 +428,7 @@ public class MapperQueryParser extends QueryParser {
     }
 
     private Query getFuzzyQuerySingle(String field, String termStr, String minSimilarity) throws ParseException {
-        currentFieldType = context.fieldMapper(field);
+        currentFieldType = parseContext.fieldMapper(field);
         if (currentFieldType != null) {
             try {
                 return currentFieldType.fuzzyQuery(termStr, Fuzziness.build(minSimilarity), fuzzyPrefixLength, settings.fuzzyMaxExpansions(), FuzzyQuery.defaultTranspositions);
@@ -492,14 +498,14 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 Query query = null;
                 if (currentFieldType.useTermQueryWithQueryString()) {
-                    query = currentFieldType.prefixQuery(termStr, multiTermRewriteMethod, context);
+                    query = currentFieldType.prefixQuery(termStr, multiTermRewriteMethod, parseContext);
                 }
                 if (query == null) {
                     query = getPossiblyAnalyzedPrefixQuery(currentFieldType.names().indexName(), termStr);
@@ -584,7 +590,7 @@ public class MapperQueryParser extends QueryParser {
                     return newMatchAllDocsQuery();
                 }
                 // effectively, we check if a field exists or not
-                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(context, actualField);
+                return fieldQueryExtensions.get(ExistsFieldQueryExtension.NAME).query(parseContext, actualField);
             }
         }
         if (lowercaseExpandedTerms) {
@@ -633,10 +639,10 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 indexedNameField = currentFieldType.names().indexName();
                 return getPossiblyAnalyzedWildcardQuery(indexedNameField, termStr);
@@ -774,14 +780,14 @@ public class MapperQueryParser extends QueryParser {
         currentFieldType = null;
         Analyzer oldAnalyzer = getAnalyzer();
         try {
-            currentFieldType = context.fieldMapper(field);
+            currentFieldType = parseContext.fieldMapper(field);
             if (currentFieldType != null) {
                 if (!forcedAnalyzer) {
-                    setAnalyzer(context.getSearchAnalyzer(currentFieldType));
+                    setAnalyzer(parseContext.getSearchAnalyzer(currentFieldType));
                 }
                 Query query = null;
                 if (currentFieldType.useTermQueryWithQueryString()) {
-                    query = currentFieldType.regexpQuery(termStr, RegExp.ALL, maxDeterminizedStates, multiTermRewriteMethod, context);
+                    query = currentFieldType.regexpQuery(termStr, RegExp.ALL, maxDeterminizedStates, multiTermRewriteMethod, parseContext);
                 }
                 if (query == null) {
                     query = super.getRegexpQuery(field, termStr);
@@ -829,7 +835,7 @@ public class MapperQueryParser extends QueryParser {
     private Collection<String> extractMultiFields(String field) {
         Collection<String> fields = null;
         if (field != null) {
-            fields = context.simpleMatchToIndexNames(field);
+            fields = parseContext.simpleMatchToIndexNames(field);
         } else {
             fields = settings.fields();
         }
diff --git a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
index f9fc8c9..ed1b704 100644
--- a/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
+++ b/core/src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java
@@ -21,8 +21,8 @@ package org.apache.lucene.queryparser.classic;
 
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.Query;
-import org.elasticsearch.index.query.MissingQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.MissingQueryParser;
+import org.elasticsearch.index.query.QueryParseContext;
 
 /**
  *
@@ -32,11 +32,8 @@ public class MissingFieldQueryExtension implements FieldQueryExtension {
     public static final String NAME = "_missing_";
 
     @Override
-    public Query query(QueryShardContext context, String queryText) {
-        Query query = MissingQueryBuilder.newFilter(context, queryText, MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE, MissingQueryBuilder.DEFAULT_NULL_VALUE);
-        if (query != null) {
-            return new ConstantScoreQuery(query);
-        }
-        return null;
+    public Query query(QueryParseContext parseContext, String queryText) {
+        return new ConstantScoreQuery(MissingQueryParser.newFilter(parseContext, queryText,
+                MissingQueryParser.DEFAULT_EXISTENCE_VALUE, MissingQueryParser.DEFAULT_NULL_VALUE, null));
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/ElasticsearchException.java b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
index 0a23ebd..a4def64 100644
--- a/core/src/main/java/org/elasticsearch/ElasticsearchException.java
+++ b/core/src/main/java/org/elasticsearch/ElasticsearchException.java
@@ -496,7 +496,6 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
                 org.elasticsearch.http.HttpException.class,
                 org.elasticsearch.index.shard.IndexShardNotRecoveringException.class,
                 org.elasticsearch.indices.IndexPrimaryShardNotAllocatedException.class,
-                org.elasticsearch.env.FailedToResolveConfigException.class,
                 org.elasticsearch.action.UnavailableShardsException.class,
                 org.elasticsearch.transport.ActionNotFoundTransportException.class,
                 org.elasticsearch.index.shard.TranslogRecoveryPerformer.BatchOperationException.class,
@@ -581,7 +580,6 @@ public class ElasticsearchException extends RuntimeException implements ToXConte
                 org.elasticsearch.index.engine.RecoveryEngineException.class,
                 org.elasticsearch.common.blobstore.BlobStoreException.class,
                 org.elasticsearch.index.snapshots.IndexShardRestoreException.class,
-                org.elasticsearch.index.query.QueryShardException.class,
                 org.elasticsearch.index.query.QueryParsingException.class,
                 org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnPrimaryException.class,
                 org.elasticsearch.index.engine.DeleteByQueryFailedEngineException.class,
diff --git a/core/src/main/java/org/elasticsearch/Version.java b/core/src/main/java/org/elasticsearch/Version.java
index d820a1b..d12fcd3 100644
--- a/core/src/main/java/org/elasticsearch/Version.java
+++ b/core/src/main/java/org/elasticsearch/Version.java
@@ -257,8 +257,10 @@ public class Version {
     public static final Version V_2_0_0_beta1 = new Version(V_2_0_0_beta1_ID, true, org.apache.lucene.util.Version.LUCENE_5_2_1);
     public static final int V_2_0_0_ID = 2000099;
     public static final Version V_2_0_0 = new Version(V_2_0_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_2_1);
+    public static final int V_2_1_0_ID = 2010099;
+    public static final Version V_2_1_0 = new Version(V_2_1_0_ID, true, org.apache.lucene.util.Version.LUCENE_5_2_1);
 
-    public static final Version CURRENT = V_2_0_0;
+    public static final Version CURRENT = V_2_1_0;
 
     static {
         assert CURRENT.luceneVersion.equals(Lucene.VERSION) : "Version must be upgraded to [" + Lucene.VERSION + "] is still set to [" + CURRENT.luceneVersion + "]";
@@ -270,6 +272,8 @@ public class Version {
 
     public static Version fromId(int id) {
         switch (id) {
+            case V_2_1_0_ID:
+                return V_2_1_0;
             case V_2_0_0_ID:
                 return V_2_0_0;
             case V_2_0_0_beta1_ID:
diff --git a/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java b/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java
index ef64bbd..ff91635 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java
@@ -183,7 +183,7 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
                 transientSettings.put(currentState.metaData().transientSettings());
                 for (Map.Entry<String, String> entry : request.transientSettings().getAsMap().entrySet()) {
                     if (dynamicSettings.isDynamicOrLoggingSetting(entry.getKey())) {
-                        String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue());
+                        String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue(), clusterService.state());
                         if (error == null) {
                             transientSettings.put(entry.getKey(), entry.getValue());
                             transientUpdates.put(entry.getKey(), entry.getValue());
@@ -200,7 +200,7 @@ public class TransportClusterUpdateSettingsAction extends TransportMasterNodeAct
                 persistentSettings.put(currentState.metaData().persistentSettings());
                 for (Map.Entry<String, String> entry : request.persistentSettings().getAsMap().entrySet()) {
                     if (dynamicSettings.isDynamicOrLoggingSetting(entry.getKey())) {
-                        String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue());
+                        String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue(), clusterService.state());
                         if (error == null) {
                             persistentSettings.put(entry.getKey(), entry.getValue());
                             persistentUpdates.put(entry.getKey(), entry.getValue());
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
index 807a638..bed778b 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java
@@ -42,7 +42,6 @@ import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
@@ -186,12 +185,12 @@ public class TransportValidateQueryAction extends TransportBroadcastAction<Valid
 
             valid = true;
             if (request.explain()) {
-                explanation = searchContext.query().toString();
+                explanation = searchContext.parsedQuery().query().toString();
             }
             if (request.rewrite()) {
                 explanation = getRewrittenQuery(searcher.searcher(), searchContext.query());
-            }
-        } catch (QueryShardException|QueryParsingException e) {
+            }   
+        } catch (QueryParsingException e) {
             valid = false;
             error = e.getDetailedMessage();
         } catch (AssertionError|IOException e) {
diff --git a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
index 515ecd1..4acdfdc 100644
--- a/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
+++ b/core/src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.action.admin.indices.validate.query;
 
+import org.elasticsearch.action.ActionListener;
 import org.elasticsearch.action.support.QuerySourceBuilder;
 import org.elasticsearch.action.support.broadcast.BroadcastOperationRequestBuilder;
 import org.elasticsearch.client.ElasticsearchClient;
diff --git a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
index 43e4331..ab4366d 100644
--- a/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
+++ b/core/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java
@@ -145,6 +145,10 @@ public class BulkProcessor implements Closeable {
     }
 
     public static Builder builder(Client client, Listener listener) {
+        if (client == null) {
+            throw new NullPointerException("The client you specified while building a BulkProcessor is null");
+        }
+        
         return new Builder(client, listener);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
index fa2fd31..9d51904 100644
--- a/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
+++ b/core/src/main/java/org/elasticsearch/action/exists/TransportExistsAction.java
@@ -41,7 +41,7 @@ import org.elasticsearch.common.lucene.Lucene;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.util.BigArrays;
 import org.elasticsearch.index.IndexService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.indices.IndicesService;
 import org.elasticsearch.script.ScriptService;
@@ -166,10 +166,10 @@ public class TransportExistsAction extends TransportBroadcastAction<ExistsReques
             BytesReference source = request.querySource();
             if (source != null && source.length() > 0) {
                 try {
-                    QueryShardContext.setTypes(request.types());
+                    QueryParseContext.setTypes(request.types());
                     context.parsedQuery(indexService.queryParserService().parseQuery(source));
                 } finally {
-                    QueryShardContext.removeTypes();
+                    QueryParseContext.removeTypes();
                 }
             }
             context.preProcess();
diff --git a/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java b/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java
index c11162a..fa57763 100644
--- a/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java
+++ b/core/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java
@@ -180,7 +180,7 @@ public abstract class TransportSearchTypeAction extends TransportAction<SearchRe
 
         void onFirstPhaseResult(int shardIndex, ShardRouting shard, FirstResult result, ShardIterator shardIt) {
             result.shardTarget(new SearchShardTarget(shard.currentNodeId(), shard.index(), shard.id()));
-            processFirstPhaseResult(shardIndex, shard, result);
+            processFirstPhaseResult(shardIndex, result);
             // we need to increment successful ops first before we compare the exit condition otherwise if we
             // are fast we could concurrently update totalOps but then preempt one of the threads which can
             // cause the successor to read a wrong value from successfulOps if second phase is very fast ie. count etc.
@@ -358,7 +358,7 @@ public abstract class TransportSearchTypeAction extends TransportAction<SearchRe
 
         protected abstract void sendExecuteFirstPhase(DiscoveryNode node, ShardSearchTransportRequest request, ActionListener<FirstResult> listener);
 
-        protected final void processFirstPhaseResult(int shardIndex, ShardRouting shard, FirstResult result) {
+        protected final void processFirstPhaseResult(int shardIndex, FirstResult result) {
             firstResults.set(shardIndex, result);
 
             if (logger.isTraceEnabled()) {
diff --git a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
index e57d956..20333e9 100644
--- a/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
+++ b/core/src/main/java/org/elasticsearch/cluster/ClusterModule.java
@@ -19,59 +19,301 @@
 
 package org.elasticsearch.cluster;
 
-import com.google.common.collect.ImmutableList;
+import org.elasticsearch.action.support.DestructiveOperations;
 import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
 import org.elasticsearch.cluster.action.index.NodeIndexDeletedAction;
 import org.elasticsearch.cluster.action.index.NodeMappingRefreshAction;
 import org.elasticsearch.cluster.action.shard.ShardStateAction;
-import org.elasticsearch.cluster.metadata.*;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.cluster.metadata.IndexTemplateFilter;
+import org.elasticsearch.cluster.metadata.MetaData;
+import org.elasticsearch.cluster.metadata.MetaDataCreateIndexService;
+import org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService;
+import org.elasticsearch.cluster.metadata.MetaDataIndexAliasesService;
+import org.elasticsearch.cluster.metadata.MetaDataIndexStateService;
+import org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService;
+import org.elasticsearch.cluster.metadata.MetaDataMappingService;
+import org.elasticsearch.cluster.metadata.MetaDataService;
+import org.elasticsearch.cluster.metadata.MetaDataUpdateSettingsService;
 import org.elasticsearch.cluster.node.DiscoveryNodeService;
 import org.elasticsearch.cluster.routing.OperationRouting;
 import org.elasticsearch.cluster.routing.RoutingService;
-import org.elasticsearch.cluster.routing.allocation.AllocationModule;
+import org.elasticsearch.cluster.routing.UnassignedInfo;
+import org.elasticsearch.cluster.routing.allocation.AllocationService;
+import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
+import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;
+import org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.FilterAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.NodeVersionAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.RebalanceOnlyWhenActiveAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.ReplicaAfterPrimaryActiveAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.SameShardAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.ShardsLimitAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.SnapshotInProgressAllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;
 import org.elasticsearch.cluster.service.InternalClusterService;
-import org.elasticsearch.cluster.settings.ClusterDynamicSettingsModule;
-import org.elasticsearch.common.Classes;
+import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
+import org.elasticsearch.cluster.settings.DynamicSettings;
+import org.elasticsearch.cluster.settings.Validator;
 import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Module;
-import org.elasticsearch.common.inject.SpawnModules;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
+import org.elasticsearch.common.logging.ESLogger;
+import org.elasticsearch.common.logging.Loggers;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.settings.IndexDynamicSettingsModule;
+import org.elasticsearch.common.util.ExtensionPoint;
+import org.elasticsearch.discovery.DiscoverySettings;
+import org.elasticsearch.discovery.zen.ZenDiscovery;
+import org.elasticsearch.discovery.zen.elect.ElectMasterService;
+import org.elasticsearch.gateway.GatewayAllocator;
+import org.elasticsearch.gateway.PrimaryShardAllocator;
+import org.elasticsearch.index.engine.EngineConfig;
+import org.elasticsearch.index.indexing.IndexingSlowLog;
+import org.elasticsearch.index.search.stats.SearchSlowLog;
+import org.elasticsearch.index.settings.IndexDynamicSettings;
+import org.elasticsearch.index.shard.IndexShard;
+import org.elasticsearch.index.shard.MergePolicyConfig;
+import org.elasticsearch.index.shard.MergeSchedulerConfig;
+import org.elasticsearch.index.store.IndexStore;
+import org.elasticsearch.index.translog.TranslogConfig;
+import org.elasticsearch.index.translog.TranslogService;
+import org.elasticsearch.indices.IndicesWarmer;
+import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
+import org.elasticsearch.indices.cache.request.IndicesRequestCache;
+import org.elasticsearch.indices.recovery.RecoverySettings;
+import org.elasticsearch.indices.store.IndicesStore;
+import org.elasticsearch.indices.ttl.IndicesTTLService;
+import org.elasticsearch.search.SearchService;
+import org.elasticsearch.threadpool.ThreadPool;
+import org.elasticsearch.transport.TransportService;
 
-import java.util.HashSet;
-import java.util.Set;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
 
 /**
- *
+ * Configures classes and services that affect the entire cluster.
  */
-public class ClusterModule extends AbstractModule implements SpawnModules {
+public class ClusterModule extends AbstractModule {
 
-    private final Settings settings;
-    public static final String CLUSTER_SERVICE_IMPL = "cluster.info.service.type";
+    public static final String EVEN_SHARD_COUNT_ALLOCATOR = "even_shard";
+    public static final String BALANCED_ALLOCATOR = "balanced"; // default
+    public static final String SHARDS_ALLOCATOR_TYPE_KEY = "cluster.routing.allocation.type";
+    public static final List<Class<? extends AllocationDecider>> DEFAULT_ALLOCATION_DECIDERS =
+        Collections.unmodifiableList(Arrays.asList(
+            SameShardAllocationDecider.class,
+            FilterAllocationDecider.class,
+            ReplicaAfterPrimaryActiveAllocationDecider.class,
+            ThrottlingAllocationDecider.class,
+            RebalanceOnlyWhenActiveAllocationDecider.class,
+            ClusterRebalanceAllocationDecider.class,
+            ConcurrentRebalanceAllocationDecider.class,
+            EnableAllocationDecider.class, // new enable allocation logic should proceed old disable allocation logic
+            DisableAllocationDecider.class,
+            AwarenessAllocationDecider.class,
+            ShardsLimitAllocationDecider.class,
+            NodeVersionAllocationDecider.class,
+            DiskThresholdDecider.class,
+            SnapshotInProgressAllocationDecider.class));
 
-    private Set<Class<? extends IndexTemplateFilter>> indexTemplateFilters = new HashSet<>();
+    private final Settings settings;
+    private final DynamicSettings.Builder clusterDynamicSettings = new DynamicSettings.Builder();
+    private final DynamicSettings.Builder indexDynamicSettings = new DynamicSettings.Builder();
+    private final ExtensionPoint.TypeExtensionPoint<ShardsAllocator> shardsAllocators = new ExtensionPoint.TypeExtensionPoint<>("shards_allocator", ShardsAllocator.class);
+    private final ExtensionPoint.SetExtensionPoint<AllocationDecider> allocationDeciders = new ExtensionPoint.SetExtensionPoint<>("allocation_decider", AllocationDecider.class, AllocationDeciders.class);
+    private final ExtensionPoint.SetExtensionPoint<IndexTemplateFilter> indexTemplateFilters = new ExtensionPoint.SetExtensionPoint<>("index_template_filter", IndexTemplateFilter.class);
 
     // pkg private so tests can mock
     Class<? extends ClusterInfoService> clusterInfoServiceImpl = InternalClusterInfoService.class;
 
     public ClusterModule(Settings settings) {
         this.settings = settings;
+
+        registerBuiltinClusterSettings();
+        registerBuiltinIndexSettings();
+
+        for (Class<? extends AllocationDecider> decider : ClusterModule.DEFAULT_ALLOCATION_DECIDERS) {
+            registerAllocationDecider(decider);
+        }
+        registerShardsAllocator(ClusterModule.BALANCED_ALLOCATOR, BalancedShardsAllocator.class);
+        registerShardsAllocator(ClusterModule.EVEN_SHARD_COUNT_ALLOCATOR, BalancedShardsAllocator.class);
     }
 
-    public void registerIndexTemplateFilter(Class<? extends IndexTemplateFilter> indexTemplateFilter) {
-        indexTemplateFilters.add(indexTemplateFilter);
+    private void registerBuiltinClusterSettings() {
+        registerClusterDynamicSetting(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTES, Validator.EMPTY);
+        registerClusterDynamicSetting(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(BalancedShardsAllocator.SETTING_INDEX_BALANCE_FACTOR, Validator.FLOAT);
+        registerClusterDynamicSetting(BalancedShardsAllocator.SETTING_SHARD_BALANCE_FACTOR, Validator.FLOAT);
+        registerClusterDynamicSetting(BalancedShardsAllocator.SETTING_THRESHOLD, Validator.NON_NEGATIVE_FLOAT);
+        registerClusterDynamicSetting(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE, ClusterRebalanceAllocationDecider.ALLOCATION_ALLOW_REBALANCE_VALIDATOR);
+        registerClusterDynamicSetting(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, Validator.INTEGER);
+        registerClusterDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, Validator.EMPTY);
+        registerClusterDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE, Validator.EMPTY);
+        registerClusterDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, Validator.EMPTY);
+        registerClusterDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION, Validator.EMPTY);
+        registerClusterDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION, Validator.EMPTY);
+        registerClusterDynamicSetting(ZenDiscovery.SETTING_REJOIN_ON_MASTER_GONE, Validator.BOOLEAN);
+        registerClusterDynamicSetting(DiscoverySettings.NO_MASTER_BLOCK, Validator.EMPTY);
+        registerClusterDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(IndicesStore.INDICES_STORE_THROTTLE_TYPE, Validator.EMPTY);
+        registerClusterDynamicSetting(IndicesStore.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
+        registerClusterDynamicSetting(IndicesTTLService.INDICES_TTL_INTERVAL, Validator.TIME);
+        registerClusterDynamicSetting(MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT, Validator.TIME);
+        registerClusterDynamicSetting(MetaData.SETTING_READ_ONLY, Validator.EMPTY);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE, Validator.POSITIVE_BYTES_SIZE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_TRANSLOG_OPS, Validator.INTEGER);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_TRANSLOG_SIZE, Validator.BYTES_SIZE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_COMPRESS, Validator.EMPTY);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, Validator.POSITIVE_INTEGER);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, Validator.POSITIVE_INTEGER);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(RecoverySettings.INDICES_RECOVERY_MAX_SIZE_PER_SEC, Validator.BYTES_SIZE);
+        registerClusterDynamicSetting(ThreadPool.THREADPOOL_GROUP + "*", Validator.EMPTY);
+        registerClusterDynamicSetting(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES, Validator.INTEGER);
+        registerClusterDynamicSetting(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, Validator.INTEGER);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK, Validator.EMPTY);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK, Validator.EMPTY);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, Validator.BOOLEAN);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, Validator.BOOLEAN);
+        registerClusterDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(SnapshotInProgressAllocationDecider.CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED, Validator.EMPTY);
+        registerClusterDynamicSetting(DestructiveOperations.REQUIRES_NAME, Validator.EMPTY);
+        registerClusterDynamicSetting(DiscoverySettings.PUBLISH_TIMEOUT, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(DiscoverySettings.PUBLISH_DIFF_ENABLE, Validator.BOOLEAN);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, Validator.NON_NEGATIVE_DOUBLE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
+        registerClusterDynamicSetting(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING, Validator.NON_NEGATIVE_DOUBLE);
+        registerClusterDynamicSetting(InternalClusterService.SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD, Validator.TIME_NON_NEGATIVE);
+        registerClusterDynamicSetting(SearchService.DEFAULT_SEARCH_TIMEOUT, Validator.TIMEOUT);
+        registerClusterDynamicSetting(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_VALIDATOR);
+        registerClusterDynamicSetting(TransportService.SETTING_TRACE_LOG_INCLUDE, Validator.EMPTY);
+        registerClusterDynamicSetting(TransportService.SETTING_TRACE_LOG_INCLUDE + ".*", Validator.EMPTY);
+        registerClusterDynamicSetting(TransportService.SETTING_TRACE_LOG_EXCLUDE, Validator.EMPTY);
+        registerClusterDynamicSetting(TransportService.SETTING_TRACE_LOG_EXCLUDE + ".*", Validator.EMPTY);
     }
 
-    @Override
-    public Iterable<? extends Module> spawnModules() {
-        return ImmutableList.of(new AllocationModule(settings),
-                new ClusterDynamicSettingsModule(),
-                new IndexDynamicSettingsModule());
+    private void registerBuiltinIndexSettings() {
+        registerIndexDynamicSetting(IndexStore.INDEX_STORE_THROTTLE_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
+        registerIndexDynamicSetting(IndexStore.INDEX_STORE_THROTTLE_TYPE, Validator.EMPTY);
+        registerIndexDynamicSetting(MergeSchedulerConfig.MAX_THREAD_COUNT, Validator.EMPTY);
+        registerIndexDynamicSetting(MergeSchedulerConfig.MAX_MERGE_COUNT, Validator.EMPTY);
+        registerIndexDynamicSetting(MergeSchedulerConfig.AUTO_THROTTLE, Validator.EMPTY);
+        registerIndexDynamicSetting(FilterAllocationDecider.INDEX_ROUTING_REQUIRE_GROUP + "*", Validator.EMPTY);
+        registerIndexDynamicSetting(FilterAllocationDecider.INDEX_ROUTING_INCLUDE_GROUP + "*", Validator.EMPTY);
+        registerIndexDynamicSetting(FilterAllocationDecider.INDEX_ROUTING_EXCLUDE_GROUP + "*", Validator.EMPTY);
+        registerIndexDynamicSetting(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, Validator.EMPTY);
+        registerIndexDynamicSetting(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE, Validator.EMPTY);
+        registerIndexDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_ALLOCATION, Validator.EMPTY);
+        registerIndexDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION, Validator.EMPTY);
+        registerIndexDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION, Validator.EMPTY);
+        registerIndexDynamicSetting(TranslogConfig.INDEX_TRANSLOG_FS_TYPE, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, Validator.NON_NEGATIVE_INTEGER);
+        registerIndexDynamicSetting(IndexMetaData.SETTING_AUTO_EXPAND_REPLICAS, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexMetaData.SETTING_READ_ONLY, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexMetaData.SETTING_BLOCKS_READ, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexMetaData.SETTING_BLOCKS_WRITE, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexMetaData.SETTING_BLOCKS_METADATA, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexMetaData.SETTING_SHARED_FS_ALLOW_RECOVERY_ON_ANY_NODE, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexMetaData.SETTING_PRIORITY, Validator.NON_NEGATIVE_INTEGER);
+        registerIndexDynamicSetting(IndicesTTLService.INDEX_TTL_DISABLE_PURGE, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexShard.INDEX_REFRESH_INTERVAL, Validator.TIME);
+        registerIndexDynamicSetting(PrimaryShardAllocator.INDEX_RECOVERY_INITIAL_SHARDS, Validator.EMPTY);
+        registerIndexDynamicSetting(EngineConfig.INDEX_COMPOUND_ON_FLUSH, Validator.BOOLEAN);
+        registerIndexDynamicSetting(EngineConfig.INDEX_GC_DELETES_SETTING, Validator.TIME);
+        registerIndexDynamicSetting(IndexShard.INDEX_FLUSH_ON_CLOSE, Validator.BOOLEAN);
+        registerIndexDynamicSetting(EngineConfig.INDEX_VERSION_MAP_SIZE, Validator.BYTES_SIZE_OR_PERCENTAGE);
+        registerIndexDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_WARN, Validator.TIME);
+        registerIndexDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_INFO, Validator.TIME);
+        registerIndexDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_DEBUG, Validator.TIME);
+        registerIndexDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_TRACE, Validator.TIME);
+        registerIndexDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_REFORMAT, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_LEVEL, Validator.EMPTY);
+        registerIndexDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_MAX_SOURCE_CHARS_TO_LOG, Validator.EMPTY);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_QUERY_WARN, Validator.TIME);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_QUERY_INFO, Validator.TIME);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_QUERY_DEBUG, Validator.TIME);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_QUERY_TRACE, Validator.TIME);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_FETCH_WARN, Validator.TIME);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_FETCH_INFO, Validator.TIME);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_FETCH_DEBUG, Validator.TIME);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_FETCH_TRACE, Validator.TIME);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_REFORMAT, Validator.EMPTY);
+        registerIndexDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_LEVEL, Validator.EMPTY);
+        registerIndexDynamicSetting(ShardsLimitAllocationDecider.INDEX_TOTAL_SHARDS_PER_NODE, Validator.INTEGER);
+        registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_EXPUNGE_DELETES_ALLOWED, Validator.DOUBLE);
+        registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_FLOOR_SEGMENT, Validator.BYTES_SIZE);
+        registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGE_AT_ONCE, Validator.INTEGER_GTE_2);
+        registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGE_AT_ONCE_EXPLICIT, Validator.INTEGER_GTE_2);
+        registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGED_SEGMENT, Validator.BYTES_SIZE);
+        registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_SEGMENTS_PER_TIER, Validator.DOUBLE_GTE_2);
+        registerIndexDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_RECLAIM_DELETES_WEIGHT, Validator.NON_NEGATIVE_DOUBLE);
+        registerIndexDynamicSetting(MergePolicyConfig.INDEX_COMPOUND_FORMAT, Validator.EMPTY);
+        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_INTERVAL, Validator.TIME);
+        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, Validator.INTEGER);
+        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, Validator.BYTES_SIZE);
+        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_PERIOD, Validator.TIME);
+        registerIndexDynamicSetting(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH, Validator.EMPTY);
+        registerIndexDynamicSetting(TranslogConfig.INDEX_TRANSLOG_DURABILITY, Validator.EMPTY);
+        registerIndexDynamicSetting(IndicesWarmer.INDEX_WARMER_ENABLED, Validator.EMPTY);
+        registerIndexDynamicSetting(IndicesRequestCache.INDEX_CACHE_REQUEST_ENABLED, Validator.BOOLEAN);
+        registerIndexDynamicSetting(IndicesRequestCache.DEPRECATED_INDEX_CACHE_REQUEST_ENABLED, Validator.BOOLEAN);
+        registerIndexDynamicSetting(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, Validator.TIME);
+    }
+
+    public void registerIndexDynamicSetting(String setting, Validator validator) {
+        indexDynamicSettings.addSetting(setting, validator);
+    }
+
+    public void registerClusterDynamicSetting(String setting, Validator validator) {
+        clusterDynamicSettings.addSetting(setting, validator);
+    }
+
+    public void registerAllocationDecider(Class<? extends AllocationDecider> allocationDecider) {
+        allocationDeciders.registerExtension(allocationDecider);
+    }
+
+    public void registerShardsAllocator(String name, Class<? extends ShardsAllocator> clazz) {
+        shardsAllocators.registerExtension(name, clazz);
+    }
+
+    public void registerIndexTemplateFilter(Class<? extends IndexTemplateFilter> indexTemplateFilter) {
+        indexTemplateFilters.registerExtension(indexTemplateFilter);
     }
 
     @Override
     protected void configure() {
+        bind(DynamicSettings.class).annotatedWith(ClusterDynamicSettings.class).toInstance(clusterDynamicSettings.build());
+        bind(DynamicSettings.class).annotatedWith(IndexDynamicSettings.class).toInstance(indexDynamicSettings.build());
+
+        // bind ShardsAllocator
+        String shardsAllocatorType = shardsAllocators.bindType(binder(), settings, ClusterModule.SHARDS_ALLOCATOR_TYPE_KEY, ClusterModule.BALANCED_ALLOCATOR);
+        if (shardsAllocatorType.equals(ClusterModule.EVEN_SHARD_COUNT_ALLOCATOR)) {
+            final ESLogger logger = Loggers.getLogger(getClass(), settings);
+            logger.warn("{} allocator has been removed in 2.0 using {} instead", ClusterModule.EVEN_SHARD_COUNT_ALLOCATOR, ClusterModule.BALANCED_ALLOCATOR);
+        }
+        allocationDeciders.bind(binder());
+        indexTemplateFilters.bind(binder());
+
+        bind(ClusterInfoService.class).to(clusterInfoServiceImpl).asEagerSingleton();
+        bind(GatewayAllocator.class).asEagerSingleton();
+        bind(AllocationService.class).asEagerSingleton();
         bind(DiscoveryNodeService.class).asEagerSingleton();
         bind(ClusterService.class).to(InternalClusterService.class).asEagerSingleton();
         bind(OperationRouting.class).asEagerSingleton();
@@ -84,18 +326,10 @@ public class ClusterModule extends AbstractModule implements SpawnModules {
         bind(MetaDataUpdateSettingsService.class).asEagerSingleton();
         bind(MetaDataIndexTemplateService.class).asEagerSingleton();
         bind(IndexNameExpressionResolver.class).asEagerSingleton();
-
         bind(RoutingService.class).asEagerSingleton();
-
         bind(ShardStateAction.class).asEagerSingleton();
         bind(NodeIndexDeletedAction.class).asEagerSingleton();
         bind(NodeMappingRefreshAction.class).asEagerSingleton();
         bind(MappingUpdatedAction.class).asEagerSingleton();
-        bind(ClusterInfoService.class).to(clusterInfoServiceImpl).asEagerSingleton();
-
-        Multibinder<IndexTemplateFilter> mbinder = Multibinder.newSetBinder(binder(), IndexTemplateFilter.class);
-        for (Class<? extends IndexTemplateFilter> indexTemplateFilter : indexTemplateFilters) {
-            mbinder.addBinding().to(indexTemplateFilter);
-        }
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
index d5b398c..f12824d 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.indices.InvalidAliasNameException;
 
 import java.io.IOException;
@@ -142,10 +142,10 @@ public class AliasValidator extends AbstractComponent {
     }
 
     private void validateAliasFilter(XContentParser parser, IndexQueryParserService indexQueryParserService) throws IOException {
-        QueryShardContext context = indexQueryParserService.getShardContext();
+        QueryParseContext context = indexQueryParserService.getParseContext();
         try {
             context.reset(parser);
-            context.parseContext().parseInnerFilter();
+            context.parseInnerFilter();
         } finally {
             context.reset(null);
             parser.close();
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
index 840532e..96f7915 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java
@@ -34,6 +34,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.indices.IndexCreationException;
 import org.elasticsearch.indices.IndexTemplateAlreadyExistsException;
 import org.elasticsearch.indices.IndexTemplateMissingException;
 import org.elasticsearch.indices.InvalidIndexTemplateException;
@@ -50,12 +51,14 @@ public class MetaDataIndexTemplateService extends AbstractComponent {
 
     private final ClusterService clusterService;
     private final AliasValidator aliasValidator;
+    private final MetaDataCreateIndexService metaDataCreateIndexService;
 
     @Inject
-    public MetaDataIndexTemplateService(Settings settings, ClusterService clusterService, AliasValidator aliasValidator) {
+    public MetaDataIndexTemplateService(Settings settings, ClusterService clusterService, MetaDataCreateIndexService metaDataCreateIndexService, AliasValidator aliasValidator) {
         super(settings);
         this.clusterService = clusterService;
         this.aliasValidator = aliasValidator;
+        this.metaDataCreateIndexService = metaDataCreateIndexService;
     }
 
     public void removeTemplates(final RemoveRequest request, final RemoveListener listener) {
@@ -207,6 +210,12 @@ public class MetaDataIndexTemplateService extends AbstractComponent {
             throw new InvalidIndexTemplateException(request.name, "template must not container the following characters " + Strings.INVALID_FILENAME_CHARS);
         }
 
+        try {
+            metaDataCreateIndexService.validateIndexSettings(request.name, request.settings);
+        } catch (IndexCreationException exception) {
+            throw new InvalidIndexTemplateException(request.name, exception.getDetailedMessage());
+        }
+
         for (Alias alias : request.aliases) {
             //we validate the alias only partially, as we don't know yet to which index it'll get applied to
             aliasValidator.validateAliasStandalone(alias);
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
index 9fbe2c5..4665633 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java
@@ -191,7 +191,7 @@ public class MetaDataUpdateSettingsService extends AbstractComponent implements
             if (!dynamicSettings.hasDynamicSetting(setting.getKey())) {
                 removedSettings.add(setting.getKey());
             } else {
-                String error = dynamicSettings.validateDynamicSetting(setting.getKey(), setting.getValue());
+                String error = dynamicSettings.validateDynamicSetting(setting.getKey(), setting.getValue(), clusterService.state());
                 if (error != null) {
                     errors.add("[" + setting.getKey() + "] - " + error);
                 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationModule.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationModule.java
deleted file mode 100644
index 62e14ec..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationModule.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.routing.allocation;
-
-import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
-import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
-import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
-import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders;
-import org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.ClusterRebalanceAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.ConcurrentRebalanceAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.FilterAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.NodeVersionAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.RebalanceOnlyWhenActiveAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.ReplicaAfterPrimaryActiveAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.SameShardAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.ShardsLimitAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.SnapshotInProgressAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.ThrottlingAllocationDecider;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.logging.ESLogger;
-import org.elasticsearch.common.logging.Loggers;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.util.ExtensionPoint;
-import org.elasticsearch.gateway.GatewayAllocator;
-
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-/**
- * A module to setup classes related to shard allocation.
- *
- * There are two basic concepts for allocation.
- * <ul>
- * <li>An {@link AllocationDecider} decides *when* an allocation should be attempted.</li>
- * <li>A {@link ShardsAllocator} determins *how* an allocation takes place</li>
- * </ul>
- */
-public class AllocationModule extends AbstractModule {
-
-    public static final String EVEN_SHARD_COUNT_ALLOCATOR = "even_shard";
-    public static final String BALANCED_ALLOCATOR = "balanced"; // default
-    public static final String SHARDS_ALLOCATOR_TYPE_KEY = "cluster.routing.allocation.type";
-
-    public static final List<Class<? extends AllocationDecider>> DEFAULT_ALLOCATION_DECIDERS =
-        Collections.unmodifiableList(Arrays.asList(
-            SameShardAllocationDecider.class,
-            FilterAllocationDecider.class,
-            ReplicaAfterPrimaryActiveAllocationDecider.class,
-            ThrottlingAllocationDecider.class,
-            RebalanceOnlyWhenActiveAllocationDecider.class,
-            ClusterRebalanceAllocationDecider.class,
-            ConcurrentRebalanceAllocationDecider.class,
-            EnableAllocationDecider.class, // new enable allocation logic should proceed old disable allocation logic
-            DisableAllocationDecider.class,
-            AwarenessAllocationDecider.class,
-            ShardsLimitAllocationDecider.class,
-            NodeVersionAllocationDecider.class,
-            DiskThresholdDecider.class,
-            SnapshotInProgressAllocationDecider.class));
-
-
-    private final Settings settings;
-    private final ExtensionPoint.TypeExtensionPoint<ShardsAllocator> shardsAllocators = new ExtensionPoint.TypeExtensionPoint<>("shards_allocator", ShardsAllocator.class);
-    private final ExtensionPoint.SetExtensionPoint<AllocationDecider> allocationDeciders = new ExtensionPoint.SetExtensionPoint<>("allocation_decider", AllocationDecider.class, AllocationDeciders.class);
-
-    public AllocationModule(Settings settings) {
-        this.settings = settings;
-        for (Class<? extends AllocationDecider> decider : DEFAULT_ALLOCATION_DECIDERS) {
-            allocationDeciders.registerExtension(decider);
-        }
-        shardsAllocators.registerExtension(BALANCED_ALLOCATOR, BalancedShardsAllocator.class);
-        shardsAllocators.registerExtension(EVEN_SHARD_COUNT_ALLOCATOR, BalancedShardsAllocator.class);
-    }
-
-    /** Register a custom allocation decider */
-    public void registerAllocationDecider(Class<? extends AllocationDecider> allocationDecider) {
-        allocationDeciders.registerExtension(allocationDecider);
-    }
-
-    /** Register a custom shard allocator with the given name */
-    public void registerShardAllocator(String name, Class<? extends ShardsAllocator> clazz) {
-        shardsAllocators.registerExtension(name, clazz);
-    }
-
-    @Override
-    protected void configure() {
-        // bind ShardsAllocator
-        String shardsAllocatorType = shardsAllocators.bindType(binder(), settings, AllocationModule.SHARDS_ALLOCATOR_TYPE_KEY, AllocationModule.BALANCED_ALLOCATOR);
-        if (shardsAllocatorType.equals(EVEN_SHARD_COUNT_ALLOCATOR)) {
-            final ESLogger logger = Loggers.getLogger(getClass(), settings);
-            logger.warn("{} allocator has been removed in 2.0 using {} instead", AllocationModule.EVEN_SHARD_COUNT_ALLOCATOR, AllocationModule.BALANCED_ALLOCATOR);
-        }
-        // bind AllocationDeciders
-        allocationDeciders.bind(binder());
-
-        bind(GatewayAllocator.class).asEagerSingleton();
-        bind(AllocationService.class).asEagerSingleton();
-    }
-
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java
index b057307..00f6575 100644
--- a/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java
+++ b/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.cluster.routing.allocation.decider;
 
+import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.routing.ShardRouting;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.cluster.settings.Validator;
@@ -52,7 +53,7 @@ public class ClusterRebalanceAllocationDecider extends AllocationDecider {
     public static final String CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE = "cluster.routing.allocation.allow_rebalance";
     public static final Validator ALLOCATION_ALLOW_REBALANCE_VALIDATOR = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 ClusterRebalanceType.parseString(value);
                 return null;
diff --git a/core/src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java b/core/src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java
deleted file mode 100644
index a68ef64..0000000
--- a/core/src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.cluster.settings;
-
-import org.elasticsearch.action.support.DestructiveOperations;
-import org.elasticsearch.cluster.InternalClusterInfoService;
-import org.elasticsearch.cluster.action.index.MappingUpdatedAction;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
-import org.elasticsearch.cluster.routing.allocation.decider.*;
-import org.elasticsearch.cluster.service.InternalClusterService;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.discovery.DiscoverySettings;
-import org.elasticsearch.discovery.zen.ZenDiscovery;
-import org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService;
-import org.elasticsearch.indices.recovery.RecoverySettings;
-import org.elasticsearch.indices.store.IndicesStore;
-import org.elasticsearch.indices.ttl.IndicesTTLService;
-import org.elasticsearch.search.SearchService;
-import org.elasticsearch.threadpool.ThreadPool;
-
-/**
- */
-public class ClusterDynamicSettingsModule extends AbstractModule {
-
-    private final DynamicSettings clusterDynamicSettings;
-
-    public ClusterDynamicSettingsModule() {
-        clusterDynamicSettings = new DynamicSettings();
-        clusterDynamicSettings.addDynamicSetting(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_ATTRIBUTES);
-        clusterDynamicSettings.addDynamicSetting(AwarenessAllocationDecider.CLUSTER_ROUTING_ALLOCATION_AWARENESS_FORCE_GROUP + "*");
-        clusterDynamicSettings.addDynamicSetting(BalancedShardsAllocator.SETTING_INDEX_BALANCE_FACTOR, Validator.FLOAT);
-        clusterDynamicSettings.addDynamicSetting(BalancedShardsAllocator.SETTING_SHARD_BALANCE_FACTOR, Validator.FLOAT);
-        clusterDynamicSettings.addDynamicSetting(BalancedShardsAllocator.SETTING_THRESHOLD, Validator.NON_NEGATIVE_FLOAT);
-        clusterDynamicSettings.addDynamicSetting(ClusterRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ALLOW_REBALANCE,
-                ClusterRebalanceAllocationDecider.ALLOCATION_ALLOW_REBALANCE_VALIDATOR);
-        clusterDynamicSettings.addDynamicSetting(ConcurrentRebalanceAllocationDecider.CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, Validator.INTEGER);
-        clusterDynamicSettings.addDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE);
-        clusterDynamicSettings.addDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_REBALANCE_ENABLE);
-        clusterDynamicSettings.addDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION);
-        clusterDynamicSettings.addDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_ALLOCATION);
-        clusterDynamicSettings.addDynamicSetting(DisableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION);
-        clusterDynamicSettings.addDynamicSetting(ZenDiscovery.SETTING_REJOIN_ON_MASTER_GONE, Validator.BOOLEAN);
-        clusterDynamicSettings.addDynamicSetting(DiscoverySettings.NO_MASTER_BLOCK);
-        clusterDynamicSettings.addDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_INCLUDE_GROUP + "*");
-        clusterDynamicSettings.addDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_EXCLUDE_GROUP + "*");
-        clusterDynamicSettings.addDynamicSetting(FilterAllocationDecider.CLUSTER_ROUTING_REQUIRE_GROUP + "*");
-        clusterDynamicSettings.addDynamicSetting(IndicesStore.INDICES_STORE_THROTTLE_TYPE);
-        clusterDynamicSettings.addDynamicSetting(IndicesStore.INDICES_STORE_THROTTLE_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
-        clusterDynamicSettings.addDynamicSetting(IndicesTTLService.INDICES_TTL_INTERVAL, Validator.TIME);
-        clusterDynamicSettings.addDynamicSetting(MappingUpdatedAction.INDICES_MAPPING_DYNAMIC_TIMEOUT, Validator.TIME);
-        clusterDynamicSettings.addDynamicSetting(MetaData.SETTING_READ_ONLY);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_FILE_CHUNK_SIZE, Validator.BYTES_SIZE);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_TRANSLOG_OPS, Validator.INTEGER);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_TRANSLOG_SIZE, Validator.BYTES_SIZE);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_COMPRESS);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_CONCURRENT_STREAMS, Validator.POSITIVE_INTEGER);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_CONCURRENT_SMALL_FILE_STREAMS, Validator.POSITIVE_INTEGER);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_STATE_SYNC, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_ACTIVITY_TIMEOUT, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_INTERNAL_ACTION_TIMEOUT, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_INTERNAL_LONG_ACTION_TIMEOUT, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(RecoverySettings.INDICES_RECOVERY_MAX_SIZE_PER_SEC, Validator.BYTES_SIZE);
-        clusterDynamicSettings.addDynamicSetting(ThreadPool.THREADPOOL_GROUP + "*");
-        clusterDynamicSettings.addDynamicSetting(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_INITIAL_PRIMARIES_RECOVERIES, Validator.INTEGER);
-        clusterDynamicSettings.addDynamicSetting(ThrottlingAllocationDecider.CLUSTER_ROUTING_ALLOCATION_NODE_CONCURRENT_RECOVERIES, Validator.INTEGER);
-        clusterDynamicSettings.addDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK);
-        clusterDynamicSettings.addDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK);
-        clusterDynamicSettings.addDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_DISK_THRESHOLD_ENABLED, Validator.BOOLEAN);
-        clusterDynamicSettings.addDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_INCLUDE_RELOCATIONS, Validator.BOOLEAN);
-        clusterDynamicSettings.addDynamicSetting(DiskThresholdDecider.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_UPDATE_INTERVAL, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(InternalClusterInfoService.INTERNAL_CLUSTER_INFO_TIMEOUT, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(SnapshotInProgressAllocationDecider.CLUSTER_ROUTING_ALLOCATION_SNAPSHOT_RELOCATION_ENABLED);
-        clusterDynamicSettings.addDynamicSetting(DestructiveOperations.REQUIRES_NAME);
-        clusterDynamicSettings.addDynamicSetting(DiscoverySettings.PUBLISH_TIMEOUT, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(DiscoverySettings.PUBLISH_DIFF_ENABLE, Validator.BOOLEAN);
-        clusterDynamicSettings.addDynamicSetting(HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
-        clusterDynamicSettings.addDynamicSetting(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
-        clusterDynamicSettings.addDynamicSetting(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_OVERHEAD_SETTING, Validator.NON_NEGATIVE_DOUBLE);
-        clusterDynamicSettings.addDynamicSetting(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING, Validator.MEMORY_SIZE);
-        clusterDynamicSettings.addDynamicSetting(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_OVERHEAD_SETTING, Validator.NON_NEGATIVE_DOUBLE);
-        clusterDynamicSettings.addDynamicSetting(InternalClusterService.SETTING_CLUSTER_SERVICE_SLOW_TASK_LOGGING_THRESHOLD, Validator.TIME_NON_NEGATIVE);
-        clusterDynamicSettings.addDynamicSetting(SearchService.DEFAULT_SEARCH_TIMEOUT, Validator.TIMEOUT);
-    }
-
-    public void addDynamicSettings(String... settings) {
-        clusterDynamicSettings.addDynamicSettings(settings);
-    }
-
-    public void addDynamicSetting(String setting, Validator validator) {
-        clusterDynamicSettings.addDynamicSetting(setting, validator);
-    }
-
-    @Override
-    protected void configure() {
-        bind(DynamicSettings.class).annotatedWith(ClusterDynamicSettings.class).toInstance(clusterDynamicSettings);
-
-        // Bind to null provider just in case somebody will forget to supply @ClusterDynamicSetting or @IndexDynamicSetting annotations
-        // This will cause any attempt to inject a unannotated DynamicSettings to fail with Guice error, instead of silently
-        // injecting an empty copy of dynamic settings
-        bind(DynamicSettings.class).toProvider(Providers.<DynamicSettings>of(null));
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java b/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
index 1848a92..c4137fc 100644
--- a/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
+++ b/core/src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java
@@ -19,18 +19,39 @@
 
 package org.elasticsearch.cluster.settings;
 
-import com.google.common.collect.ImmutableMap;
+import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.regex.Regex;
 
+import java.util.Collections;
+import java.util.HashMap;
 import java.util.Map;
 
 /**
+ * A container for setting names and validation methods for those settings.
  */
 public class DynamicSettings {
 
-    private ImmutableMap<String, Validator> dynamicSettings = ImmutableMap.of();
+    private final Map<String, Validator> dynamicSettings;
 
+    public static class Builder {
+        private Map<String, Validator> settings = new HashMap<>();
+
+        public void addSetting(String setting, Validator validator) {
+            Validator old = settings.put(setting, validator);
+            if (old != null) {
+                throw new IllegalArgumentException("Cannot register setting [" + setting + "] twice");
+            }
+        }
+
+        public DynamicSettings build() {
+            return new DynamicSettings(settings);
+        }
+    }
+
+    private DynamicSettings(Map<String, Validator> settings) {
+        this.dynamicSettings = Collections.unmodifiableMap(settings);
+    }
 
     public boolean isDynamicOrLoggingSetting(String key) {
         return hasDynamicSetting(key) || key.startsWith("logger.");
@@ -45,32 +66,12 @@ public class DynamicSettings {
         return false;
     }
 
-    public String validateDynamicSetting(String dynamicSetting, String value) {
+    public String validateDynamicSetting(String dynamicSetting, String value, ClusterState clusterState) {
         for (Map.Entry<String, Validator> setting : dynamicSettings.entrySet()) {
             if (Regex.simpleMatch(setting.getKey(), dynamicSetting)) {
-                return setting.getValue().validate(dynamicSetting, value);
+                return setting.getValue().validate(dynamicSetting, value, clusterState);
             }
         }
         return null;
     }
-
-    public synchronized void addDynamicSetting(String setting, Validator validator) {
-        MapBuilder<String, Validator> updatedSettings = MapBuilder.newMapBuilder(dynamicSettings);
-        updatedSettings.put(setting, validator);
-        dynamicSettings = updatedSettings.immutableMap();
-    }
-
-    public synchronized void addDynamicSetting(String setting) {
-        addDynamicSetting(setting, Validator.EMPTY);
-    }
-
-
-    public synchronized void addDynamicSettings(String... settings) {
-        MapBuilder<String, Validator> updatedSettings = MapBuilder.newMapBuilder(dynamicSettings);
-        for (String setting : settings) {
-            updatedSettings.put(setting, Validator.EMPTY);
-        }
-        dynamicSettings = updatedSettings.immutableMap();
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/cluster/settings/Validator.java b/core/src/main/java/org/elasticsearch/cluster/settings/Validator.java
index 822b8c7..cb253dc 100644
--- a/core/src/main/java/org/elasticsearch/cluster/settings/Validator.java
+++ b/core/src/main/java/org/elasticsearch/cluster/settings/Validator.java
@@ -20,7 +20,9 @@
 package org.elasticsearch.cluster.settings;
 
 import org.elasticsearch.ElasticsearchParseException;
+import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.common.Booleans;
+import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.unit.TimeValue;
 
 import static org.elasticsearch.common.unit.ByteSizeValue.parseBytesSizeValue;
@@ -32,18 +34,18 @@ import static org.elasticsearch.common.unit.MemorySizeValue.parseBytesSizeValueO
  */
 public interface Validator {
 
-    String validate(String setting, String value);
+    String validate(String setting, String value, ClusterState clusterState);
 
-    public static final Validator EMPTY = new Validator() {
+    Validator EMPTY = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             return null;
         }
     };
 
-    public static final Validator TIME = new Validator() {
+    Validator TIME = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             if (value == null) {
                 throw new NullPointerException("value must not be null");
             }
@@ -57,9 +59,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator TIMEOUT = new Validator() {
+    Validator TIMEOUT = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (value == null) {
                     throw new NullPointerException("value must not be null");
@@ -76,9 +78,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator TIME_NON_NEGATIVE = new Validator() {
+    Validator TIME_NON_NEGATIVE = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (value == null) {
                     throw new NullPointerException("value must not be null");
@@ -95,9 +97,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator FLOAT = new Validator() {
+    Validator FLOAT = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 Float.parseFloat(value);
             } catch (NumberFormatException ex) {
@@ -107,9 +109,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator NON_NEGATIVE_FLOAT = new Validator() {
+    Validator NON_NEGATIVE_FLOAT = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (Float.parseFloat(value) < 0.0) {
                     return "the value of the setting " + setting + " must be a non negative float";
@@ -121,9 +123,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator DOUBLE = new Validator() {
+    Validator DOUBLE = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 Double.parseDouble(value);
             } catch (NumberFormatException ex) {
@@ -133,9 +135,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator NON_NEGATIVE_DOUBLE = new Validator() {
+    Validator NON_NEGATIVE_DOUBLE = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (Double.parseDouble(value) < 0.0) {
                     return "the value of the setting " + setting + " must be a non negative double";
@@ -147,9 +149,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator DOUBLE_GTE_2 = new Validator() {
+    Validator DOUBLE_GTE_2 = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (Double.parseDouble(value) < 2.0) {
                     return "the value of the setting " + setting + " must be >= 2.0";
@@ -161,9 +163,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator INTEGER = new Validator() {
+    Validator INTEGER = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 Integer.parseInt(value);
             } catch (NumberFormatException ex) {
@@ -173,9 +175,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator POSITIVE_INTEGER = new Validator() {
+    Validator POSITIVE_INTEGER = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (Integer.parseInt(value) <= 0) {
                     return "the value of the setting " + setting + " must be a positive integer";
@@ -187,9 +189,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator NON_NEGATIVE_INTEGER = new Validator() {
+    Validator NON_NEGATIVE_INTEGER = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (Integer.parseInt(value) < 0) {
                     return "the value of the setting " + setting + " must be a non negative integer";
@@ -201,9 +203,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator INTEGER_GTE_2 = new Validator() {
+    Validator INTEGER_GTE_2 = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (Integer.parseInt(value) < 2) {
                     return "the value of the setting " + setting + " must be >= 2";
@@ -215,9 +217,9 @@ public interface Validator {
         }
     };
 
-    public static final Validator BYTES_SIZE = new Validator() {
+    Validator BYTES_SIZE = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 parseBytesSizeValue(value, setting);
             } catch (ElasticsearchParseException ex) {
@@ -227,9 +229,24 @@ public interface Validator {
         }
     };
 
-    public static final Validator PERCENTAGE = new Validator() {
+    Validator POSITIVE_BYTES_SIZE = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState state) {
+            try {
+                ByteSizeValue byteSizeValue = parseBytesSizeValue(value, setting);
+                if (byteSizeValue.getBytes() <= 0) {
+                    return setting + " must be a positive byte size value";
+                }
+            } catch (ElasticsearchParseException ex) {
+                return ex.getMessage();
+            }
+            return null;
+        }
+    };
+
+    Validator PERCENTAGE = new Validator() {
+        @Override
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 if (value == null) {
                     return "the value of " + setting + " can not be null";
@@ -249,12 +266,12 @@ public interface Validator {
     };
 
 
-    public static final Validator BYTES_SIZE_OR_PERCENTAGE = new Validator() {
+    Validator BYTES_SIZE_OR_PERCENTAGE = new Validator() {
         @Override
-        public String validate(String setting, String value) {
-            String byteSize = BYTES_SIZE.validate(setting, value);
+        public String validate(String setting, String value, ClusterState clusterState) {
+            String byteSize = BYTES_SIZE.validate(setting, value, clusterState);
             if (byteSize != null) {
-                String percentage = PERCENTAGE.validate(setting, value);
+                String percentage = PERCENTAGE.validate(setting, value, clusterState);
                 if (percentage == null) {
                     return null;
                 }
@@ -265,9 +282,9 @@ public interface Validator {
     };
 
 
-    public static final Validator MEMORY_SIZE = new Validator() {
+    Validator MEMORY_SIZE = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
             try {
                 parseBytesSizeValueOrHeapRatio(value, setting);
             } catch (ElasticsearchParseException ex) {
@@ -279,7 +296,7 @@ public interface Validator {
 
     public static final Validator BOOLEAN = new Validator() {
         @Override
-        public String validate(String setting, String value) {
+        public String validate(String setting, String value, ClusterState clusterState) {
 
             if (value != null && (Booleans.isExplicitFalse(value) || Booleans.isExplicitTrue(value))) {
                 return null;
diff --git a/core/src/main/java/org/elasticsearch/common/Names.java b/core/src/main/java/org/elasticsearch/common/Names.java
deleted file mode 100644
index 3210245..0000000
--- a/core/src/main/java/org/elasticsearch/common/Names.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.common;
-
-import com.google.common.base.Charsets;
-import org.elasticsearch.common.io.FileSystemUtils;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.net.URL;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.util.concurrent.ThreadLocalRandom;
-
-/**
- *
- */
-public abstract class Names {
-
-    public static String randomNodeName(URL nodeNames) {
-        try {
-            int numberOfNames = 0;
-            try (BufferedReader reader = FileSystemUtils.newBufferedReader(nodeNames, Charsets.UTF_8)) {
-                while (reader.readLine() != null) {
-                    numberOfNames++;
-                }
-            }
-            try (BufferedReader reader = FileSystemUtils.newBufferedReader(nodeNames, Charsets.UTF_8)) {
-                int number = ((ThreadLocalRandom.current().nextInt(numberOfNames)) % numberOfNames);
-                for (int i = 0; i < number; i++) {
-                    reader.readLine();
-                }
-                return reader.readLine();
-            }
-        } catch (IOException e) {
-            return null;
-        }
-    }
-
-    private Names() {}
-}
diff --git a/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java b/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
index 4d177d7..0ee003c 100644
--- a/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
+++ b/core/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java
@@ -21,20 +21,25 @@ package org.elasticsearch.common.http.client;
 
 import com.google.common.base.Charsets;
 import com.google.common.base.Strings;
+import com.google.common.hash.Hashing;
 import org.apache.lucene.util.IOUtils;
-import org.elasticsearch.ElasticsearchTimeoutException;
-import org.elasticsearch.Version;
+import org.elasticsearch.*;
 import org.elasticsearch.common.Base64;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.unit.TimeValue;
+import org.elasticsearch.common.util.ByteArray;
 
 import java.io.*;
 import java.net.HttpURLConnection;
 import java.net.URL;
 import java.net.URLConnection;
 import java.nio.file.Files;
+import java.nio.file.NoSuchFileException;
 import java.nio.file.Path;
 import java.nio.file.attribute.FileTime;
+import java.util.Arrays;
+import java.util.List;
+import java.util.concurrent.Callable;
 
 /**
  *
@@ -83,6 +88,81 @@ public class HttpDownloadHelper {
         return getThread.wasSuccessful();
     }
 
+    public interface Checksummer {
+        /** Return the hex string for the given byte array */
+        String checksum(byte[] filebytes);
+        /** Human-readable name for the checksum format */
+        String name();
+    }
+
+    /** Checksummer for SHA1 */
+    public static Checksummer SHA1_CHECKSUM = new Checksummer() {
+        @Override
+        public String checksum(byte[] filebytes) {
+            return Hashing.sha1().hashBytes(filebytes).toString();
+        }
+
+        @Override
+        public String name() {
+            return "SHA1";
+        }
+    };
+
+    /** Checksummer for MD5 */
+    public static Checksummer MD5_CHECKSUM = new Checksummer() {
+        @Override
+        public String checksum(byte[] filebytes) {
+            return Hashing.md5().hashBytes(filebytes).toString();
+        }
+
+        @Override
+        public String name() {
+            return "MD5";
+        }
+    };
+
+    /**
+     * Download the given checksum URL to the destination and check the checksum
+     * @param checksumURL URL for the checksum file
+     * @param originalFile original file to calculate checksum of
+     * @param checksumFile destination to download the checksum file to
+     * @param hashFunc class used to calculate the checksum of the file
+     * @return true if the checksum was validated, false if it did not exist
+     * @throws Exception if the checksum failed to match
+     */
+    public boolean downloadAndVerifyChecksum(URL checksumURL, Path originalFile, Path checksumFile,
+                                             @Nullable DownloadProgress progress,
+                                             TimeValue timeout, Checksummer hashFunc) throws Exception {
+        try {
+            if (download(checksumURL, checksumFile, progress, timeout)) {
+                byte[] fileBytes = Files.readAllBytes(originalFile);
+                List<String> checksumLines = Files.readAllLines(checksumFile, Charsets.UTF_8);
+                if (checksumLines.size() != 1) {
+                    throw new ElasticsearchCorruptionException("invalid format for checksum file (" +
+                            hashFunc.name() + "), expected 1 line, got: " + checksumLines.size());
+                }
+                String checksumHex = checksumLines.get(0);
+                String fileHex = hashFunc.checksum(fileBytes);
+                if (fileHex.equals(checksumHex) == false) {
+                    throw new ElasticsearchCorruptionException("incorrect hash (" + hashFunc.name() +
+                            "), file hash: [" + fileHex + "], expected: [" + checksumHex + "]");
+                }
+                return true;
+            }
+        } catch (FileNotFoundException | NoSuchFileException e) {
+            // checksum file doesn't exist
+            return false;
+        } catch (IOException e) {
+            if (ExceptionsHelper.unwrapCause(e) instanceof FileNotFoundException) {
+                // checksum file didn't exist
+                return false;
+            }
+            throw e;
+        } finally {
+            IOUtils.deleteFilesIgnoringExceptions(checksumFile);
+        }
+        return false;
+    }
 
     /**
      * Interface implemented for reporting
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
index 5f3bd01..0dac786 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/FilterStreamInput.java
@@ -68,4 +68,4 @@ public abstract class FilterStreamInput extends StreamInput {
     public void setVersion(Version version) {
         delegate.setVersion(version);
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
index ecef804..cb42a9f 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.text.StringAndBytesText;
 import org.elasticsearch.common.text.Text;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 
@@ -46,18 +45,8 @@ import static org.elasticsearch.ElasticsearchException.readStackTrace;
 
 public abstract class StreamInput extends InputStream {
 
-    private final NamedWriteableRegistry namedWriteableRegistry;
-
     private Version version = Version.CURRENT;
 
-    protected StreamInput() {
-        this.namedWriteableRegistry = new NamedWriteableRegistry();
-    }
-
-    protected StreamInput(NamedWriteableRegistry namedWriteableRegistry) {
-        this.namedWriteableRegistry = namedWriteableRegistry;
-    }
-
     public Version getVersion() {
         return this.version;
     }
@@ -570,13 +559,6 @@ public abstract class StreamInput extends InputStream {
         throw new UnsupportedOperationException();
     }
 
-    /**
-     * Reads a {@link QueryBuilder} from the current stream
-     */
-    public QueryBuilder readQuery() throws IOException {
-        return readNamedWriteable(QueryBuilder.class);
-    }
-
     public static StreamInput wrap(BytesReference reference) {
         if (reference.hasArray() == false) {
             reference = reference.toBytesArray();
diff --git a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
index 84cd303..afd6073 100644
--- a/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
+++ b/core/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java
@@ -32,7 +32,6 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.regex.Regex;
 import org.elasticsearch.common.text.Text;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.joda.time.ReadableInstant;
 
 import java.io.EOFException;
@@ -622,11 +621,4 @@ public abstract class StreamOutput extends OutputStream {
         writeString(namedWriteable.getWriteableName());
         namedWriteable.writeTo(this);
     }
-
-    /**
-     * Writes a {@link QueryBuilder} to the current stream
-     */
-    public void writeQuery(QueryBuilder queryBuilder) throws IOException {
-        writeNamedWriteable(queryBuilder);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
index a435949..0b4cdbd 100644
--- a/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
+++ b/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java
@@ -21,17 +21,19 @@ package org.elasticsearch.common.logging.log4j;
 
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
-
 import org.apache.log4j.PropertyConfigurator;
 import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.FailedToResolveConfigException;
 
 import java.io.IOException;
-import java.net.MalformedURLException;
-import java.nio.file.*;
+import java.nio.file.FileVisitOption;
+import java.nio.file.FileVisitResult;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.SimpleFileVisitor;
 import java.nio.file.attribute.BasicFileAttributes;
 import java.util.EnumSet;
 import java.util.List;
@@ -143,8 +145,8 @@ public class LogConfigurator {
 
     public static void loadConfig(Path file, Settings.Builder settingsBuilder) {
         try {
-            settingsBuilder.loadFromUrl(file.toUri().toURL());
-        } catch (FailedToResolveConfigException | NoClassDefFoundError | MalformedURLException e) {
+            settingsBuilder.loadFromPath(file);
+        } catch (SettingsException | NoClassDefFoundError e) {
             // ignore
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/common/settings/Settings.java b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
index 16eca5c..9309e1c 100644
--- a/core/src/main/java/org/elasticsearch/common/settings/Settings.java
+++ b/core/src/main/java/org/elasticsearch/common/settings/Settings.java
@@ -1058,18 +1058,6 @@ public final class Settings implements ToXContent {
          * Loads settings from a url that represents them using the
          * {@link SettingsLoaderFactory#loaderFromSource(String)}.
          */
-        public Builder loadFromUrl(URL url) throws SettingsException {
-            try {
-                return loadFromStream(url.toExternalForm(), url.openStream());
-            } catch (IOException e) {
-                throw new SettingsException("Failed to open stream for url [" + url.toExternalForm() + "]", e);
-            }
-        }
-
-        /**
-         * Loads settings from a url that represents them using the
-         * {@link SettingsLoaderFactory#loaderFromSource(String)}.
-         */
         public Builder loadFromPath(Path path) throws SettingsException {
             try {
                 return loadFromStream(path.getFileName().toString(), Files.newInputStream(path));
diff --git a/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java b/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
index bc48042..cfcd209 100644
--- a/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
+++ b/core/src/main/java/org/elasticsearch/common/unit/Fuzziness.java
@@ -19,26 +19,22 @@
 package org.elasticsearch.common.unit;
 
 import com.google.common.base.Preconditions;
-
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.util.automaton.LevenshteinAutomata;
 import org.elasticsearch.common.ParseField;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
-import java.util.Locale;
-import java.util.Objects;
 
 /**
  * A unit class that encapsulates all in-exact search
  * parsing and conversion from similarities to edit distances
  * etc.
  */
-public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
+public final class Fuzziness implements ToXContent {
 
     public static final XContentBuilderString X_FIELD_NAME = new XContentBuilderString("fuzziness");
     public static final Fuzziness ZERO = new Fuzziness(0);
@@ -49,20 +45,13 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
 
     private final String fuzziness;
 
-    /** the prototype constant is intended for deserialization when used with
-     * {@link org.elasticsearch.common.io.stream.StreamableReader#readFrom(StreamInput)} */
-    static final Fuzziness PROTOTYPE = AUTO;
-
     private Fuzziness(int fuzziness) {
         Preconditions.checkArgument(fuzziness >= 0 && fuzziness <= 2, "Valid edit distances are [0, 1, 2] but was [" + fuzziness + "]");
         this.fuzziness = Integer.toString(fuzziness);
     }
 
     private Fuzziness(String fuzziness) {
-        if (fuzziness == null) {
-            throw new IllegalArgumentException("fuzziness can't be null!");
-        }
-        this.fuzziness = fuzziness.toUpperCase(Locale.ROOT);
+        this.fuzziness = fuzziness;
     }
 
     /**
@@ -132,7 +121,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public int asDistance(String text) {
-        if (this.equals(AUTO)) { //AUTO
+        if (this == AUTO) { //AUTO
             final int len = termLen(text);
             if (len <= 2) {
                 return 0;
@@ -146,7 +135,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public TimeValue asTimeValue() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return TimeValue.timeValueMillis(1);
         } else {
             return TimeValue.parseTimeValue(fuzziness.toString(), null, "fuzziness");
@@ -154,7 +143,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public long asLong() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -165,7 +154,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public int asInt() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -176,7 +165,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public short asShort() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -187,7 +176,7 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public byte asByte() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1;
         }
         try {
@@ -198,14 +187,14 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     }
 
     public double asDouble() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1d;
         }
         return Double.parseDouble(fuzziness.toString());
     }
 
     public float asFloat() {
-        if (this.equals(AUTO)) {
+        if (this == AUTO) {
             return 1f;
         }
         return Float.parseFloat(fuzziness.toString());
@@ -218,35 +207,4 @@ public final class Fuzziness implements ToXContent, Writeable<Fuzziness> {
     public String asString() {
         return fuzziness.toString();
     }
-
-    @Override
-    public boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        Fuzziness other = (Fuzziness) obj;
-        return Objects.equals(fuzziness, other.fuzziness);
-    }
-
-    @Override
-    public int hashCode() {
-        return fuzziness.hashCode();
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeString(fuzziness);
-    }
-
-    @Override
-    public Fuzziness readFrom(StreamInput in) throws IOException {
-        return new Fuzziness(in.readString());
-    }
-
-    public static Fuzziness readFuzzinessFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java b/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
index 435c3ae..5414c4e 100644
--- a/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
+++ b/core/src/main/java/org/elasticsearch/common/util/ExtensionPoint.java
@@ -39,7 +39,7 @@ public abstract class ExtensionPoint<T> {
     /**
      * Creates a new extension point
      *
-     * @param name           the human readable underscore case name of the extension poing. This is used in error messages etc.
+     * @param name           the human readable underscore case name of the extension point. This is used in error messages etc.
      * @param extensionClass the base class that should be extended
      * @param singletons     a list of singletons to bind with this extension point - these are bound in {@link #bind(Binder)}
      */
@@ -55,9 +55,6 @@ public abstract class ExtensionPoint<T> {
      * @param binder the binder to use
      */
     public final void bind(Binder binder) {
-        if (singletons == null || singletons.length == 0) {
-            throw new IllegalStateException("Can't bind empty or null singletons");
-        }
         for (Class<?> c : singletons) {
             binder.bind(c).asEagerSingleton();
         }
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
index 8917e67..d1b7906 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java
@@ -150,7 +150,7 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
     public ZenDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool,
                         TransportService transportService, final ClusterService clusterService, NodeSettingsService nodeSettingsService,
                         ZenPingService pingService, ElectMasterService electMasterService,
-                        DiscoverySettings discoverySettings, @ClusterDynamicSettings DynamicSettings dynamicSettings) {
+                        DiscoverySettings discoverySettings) {
         super(settings);
         this.clusterName = clusterName;
         this.clusterService = clusterService;
@@ -198,24 +198,6 @@ public class ZenDiscovery extends AbstractLifecycleComponent<Discovery> implemen
         this.joinThreadControl = new JoinThreadControl(threadPool);
 
         transportService.registerRequestHandler(DISCOVERY_REJOIN_ACTION_NAME, RejoinClusterRequest.class, ThreadPool.Names.SAME, new RejoinClusterRequestHandler());
-
-        dynamicSettings.addDynamicSetting(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES, new Validator() {
-            @Override
-            public String validate(String setting, String value) {
-                int intValue;
-                try {
-                    intValue = Integer.parseInt(value);
-                } catch (NumberFormatException ex) {
-                    return "cannot parse value [" + value + "] as an integer";
-                }
-                int masterNodes = clusterService.state().nodes().masterNodes().size();
-                if (intValue > masterNodes) {
-                    return "cannot set " + ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES + " to more than the current master nodes count [" + masterNodes + "]";
-                }
-                return null;
-            }
-        });
-
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java b/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
index 6d80d25..ce65861 100644
--- a/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
+++ b/core/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java
@@ -23,7 +23,9 @@ import com.carrotsearch.hppc.ObjectContainer;
 import com.google.common.collect.Lists;
 import org.apache.lucene.util.CollectionUtil;
 import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.node.DiscoveryNode;
+import org.elasticsearch.cluster.settings.Validator;
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.settings.Settings;
@@ -36,6 +38,22 @@ import java.util.*;
 public class ElectMasterService extends AbstractComponent {
 
     public static final String DISCOVERY_ZEN_MINIMUM_MASTER_NODES = "discovery.zen.minimum_master_nodes";
+    public static final Validator DISCOVERY_ZEN_MINIMUM_MASTER_NODES_VALIDATOR = new Validator() {
+        @Override
+        public String validate(String setting, String value, ClusterState clusterState) {
+            int intValue;
+            try {
+                intValue = Integer.parseInt(value);
+            } catch (NumberFormatException ex) {
+                return "cannot parse value [" + value + "] as an integer";
+            }
+            int masterNodes = clusterState.nodes().masterNodes().size();
+            if (intValue > masterNodes) {
+                return "cannot set " + ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES + " to more than the current master nodes count [" + masterNodes + "]";
+            }
+            return null;
+        }
+    };
 
     // This is the minimum version a master needs to be on, otherwise it gets ignored
     // This is based on the minimum compatible version of the current version this node is on
diff --git a/core/src/main/java/org/elasticsearch/env/Environment.java b/core/src/main/java/org/elasticsearch/env/Environment.java
index 8ec9a7e..f50405e 100644
--- a/core/src/main/java/org/elasticsearch/env/Environment.java
+++ b/core/src/main/java/org/elasticsearch/env/Environment.java
@@ -307,30 +307,4 @@ public class Environment {
     public FileStore getFileStore(Path path) throws IOException {
         return ESFileStore.getMatchingFileStore(path, fileStores);
     }
-
-    public URL resolveConfig(String path) throws FailedToResolveConfigException {
-        // first, try it as a path in the config directory
-        Path f = configFile.resolve(path);
-        if (Files.exists(f)) {
-            try {
-                return f.toUri().toURL();
-            } catch (MalformedURLException e) {
-                throw new FailedToResolveConfigException("Failed to resolve path [" + f + "]", e);
-            }
-        }
-        // try and load it from the classpath directly
-        // TODO: remove this, callers can look up their own config on classpath
-        URL resource = getClass().getClassLoader().getResource(path);
-        if (resource != null) {
-            return resource;
-        }
-        // try and load it from the classpath with config/ prefix
-        if (!path.startsWith("config/")) {
-            resource = getClass().getClassLoader().getResource("config/" + path);
-            if (resource != null) {
-                return resource;
-            }
-        }
-        throw new FailedToResolveConfigException("Failed to resolve config path [" + path + "], tried config path [" + f + "] and classpath");
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/env/FailedToResolveConfigException.java b/core/src/main/java/org/elasticsearch/env/FailedToResolveConfigException.java
deleted file mode 100644
index f61ac3e..0000000
--- a/core/src/main/java/org/elasticsearch/env/FailedToResolveConfigException.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.env;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class FailedToResolveConfigException extends ElasticsearchException {
-
-    public FailedToResolveConfigException(String msg) {
-        super(msg);
-    }
-
-    public FailedToResolveConfigException(String msg, Throwable cause) {
-        super(msg, cause);
-    }
-
-    public FailedToResolveConfigException(StreamInput in) throws IOException{
-        super(in);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java b/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
index 322d7c0..a58b0f2 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/Analysis.java
@@ -58,7 +58,6 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tr.TurkishAnalyzer;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.util.Version;
-
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.collect.MapBuilder;
 import org.elasticsearch.common.io.FileSystemUtils;
@@ -71,8 +70,14 @@ import org.elasticsearch.index.settings.IndexSettings;
 import java.io.BufferedReader;
 import java.io.IOException;
 import java.io.Reader;
-import java.net.URL;
-import java.util.*;
+import java.nio.file.Path;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
 
 /**
  *
@@ -226,9 +231,9 @@ public class Analysis {
             }
         }
 
-        final URL wordListFile = env.resolveConfig(wordListPath);
+        final Path wordListFile = env.configFile().resolve(wordListPath);
 
-        try (BufferedReader reader = FileSystemUtils.newBufferedReader(wordListFile, Charsets.UTF_8)) {
+        try (BufferedReader reader = FileSystemUtils.newBufferedReader(wordListFile.toUri().toURL(), Charsets.UTF_8)) {
             return loadWordList(reader, "#");
         } catch (IOException ioe) {
             String message = String.format(Locale.ROOT, "IOException while reading %s_path: %s", settingPrefix, ioe.getMessage());
@@ -273,10 +278,10 @@ public class Analysis {
             return null;
         }
 
-        final URL fileUrl = env.resolveConfig(filePath);
+        final Path path = env.configFile().resolve(filePath);
 
         try {
-            return FileSystemUtils.newBufferedReader(fileUrl, Charsets.UTF_8);
+            return FileSystemUtils.newBufferedReader(path.toUri().toURL(), Charsets.UTF_8);
         } catch (IOException ioe) {
             String message = String.format(Locale.ROOT, "IOException while reading %s_path: %s", settingPrefix, ioe.getMessage());
             throw new IllegalArgumentException(message);
diff --git a/core/src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java b/core/src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
index 4224c9c..a0c7ef5 100644
--- a/core/src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
+++ b/core/src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
@@ -35,6 +35,8 @@ import org.elasticsearch.index.settings.IndexSettings;
 import org.xml.sax.InputSource;
 
 import java.net.URL;
+import java.nio.file.Files;
+import java.nio.file.Path;
 
 /**
  * Uses the {@link org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter} to decompound tokens based on hyphenation rules.
@@ -55,10 +57,10 @@ public class HyphenationCompoundWordTokenFilterFactory extends AbstractCompoundW
             throw new IllegalArgumentException("hyphenation_patterns_path is a required setting.");
         }
 
-        URL hyphenationPatternsFile = env.resolveConfig(hyphenationPatternsPath);
+        Path hyphenationPatternsFile = env.configFile().resolve(hyphenationPatternsPath);
 
         try {
-            hyphenationTree = HyphenationCompoundWordTokenFilter.getHyphenationTree(new InputSource(hyphenationPatternsFile.toExternalForm()));
+            hyphenationTree = HyphenationCompoundWordTokenFilter.getHyphenationTree(new InputSource(Files.newInputStream(hyphenationPatternsFile)));
         } catch (Exception e) {
             throw new IllegalArgumentException("Exception while reading hyphenation_patterns_path: " + e.getMessage());
         }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java b/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
index e55f3b1..65113e6 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java
@@ -33,7 +33,7 @@ import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.analysis.NamedAnalyzer;
 import org.elasticsearch.index.fielddata.FieldDataType;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.similarity.SimilarityProvider;
 
 import java.io.IOException;
@@ -425,7 +425,7 @@ public abstract class MappedFieldType extends FieldType {
     }
 
     /**
-     * Should the field query {@link #termQuery(Object, org.elasticsearch.index.query.QueryShardContext)}  be used when detecting this
+     * Should the field query {@link #termQuery(Object, org.elasticsearch.index.query.QueryParseContext)}  be used when detecting this
      * field in query string.
      */
     public boolean useTermQueryWithQueryString() {
@@ -437,11 +437,11 @@ public abstract class MappedFieldType extends FieldType {
         return new Term(names().indexName(), indexedValueForSearch(value));
     }
 
-    public Query termQuery(Object value, @Nullable QueryShardContext context) {
+    public Query termQuery(Object value, @Nullable QueryParseContext context) {
         return new TermQuery(createTerm(value));
     }
 
-    public Query termsQuery(List values, @Nullable QueryShardContext context) {
+    public Query termsQuery(List values, @Nullable QueryParseContext context) {
         BytesRef[] bytesRefs = new BytesRef[values.size()];
         for (int i = 0; i < bytesRefs.length; i++) {
             bytesRefs[i] = indexedValueForSearch(values.get(i));
@@ -460,7 +460,7 @@ public abstract class MappedFieldType extends FieldType {
         return new FuzzyQuery(createTerm(value), fuzziness.asDistance(BytesRefs.toString(value)), prefixLength, maxExpansions, transpositions);
     }
 
-    public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+    public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
         PrefixQuery query = new PrefixQuery(createTerm(value));
         if (method != null) {
             query.setRewriteMethod(method);
@@ -468,7 +468,7 @@ public abstract class MappedFieldType extends FieldType {
         return query;
     }
 
-    public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+    public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
         RegexpQuery query = new RegexpQuery(createTerm(value), flags, maxDeterminizedStates);
         if (method != null) {
             query.setRewriteMethod(method);
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
index e538a00..f872207 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java
@@ -40,7 +40,7 @@ import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.similarity.SimilarityLookupService;
 
 import java.io.IOException;
@@ -186,7 +186,7 @@ public class AllFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, QueryShardContext context) {
+        public Query termQuery(Object value, QueryParseContext context) {
             return queryStringTermQuery(createTerm(value));
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
index f6e09b2..63fa41f 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java
@@ -49,7 +49,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.Collection;
@@ -167,7 +167,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.termQuery(value, context);
             }
@@ -176,7 +176,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termsQuery(List values, @Nullable QueryShardContext context) {
+        public Query termsQuery(List values, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.termsQuery(values, context);
             }
@@ -184,7 +184,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+        public Query prefixQuery(String value, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.prefixQuery(value, method, context);
             }
@@ -201,7 +201,7 @@ public class IdFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryShardContext context) {
+        public Query regexpQuery(String value, int flags, int maxDeterminizedStates, @Nullable MultiTermQuery.RewriteMethod method, @Nullable QueryParseContext context) {
             if (indexOptions() != IndexOptions.NONE || context == null) {
                 return super.regexpQuery(value, flags, maxDeterminizedStates, method, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
index 1b7168a..3f395a8 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java
@@ -38,7 +38,7 @@ import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.Iterator;
@@ -157,7 +157,7 @@ public class IndexFieldMapper extends MetadataFieldMapper {
          * indices
          */
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (context == null) {
                 return super.termQuery(value, context);
             }
@@ -171,7 +171,7 @@ public class IndexFieldMapper extends MetadataFieldMapper {
         
 
         @Override
-        public Query termsQuery(List values, QueryShardContext context) {
+        public Query termsQuery(List values, QueryParseContext context) {
             if (context == null) {
                 return super.termsQuery(values, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
index 7cd4ac0..5fcd10c 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -189,12 +189,12 @@ public class ParentFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             return termsQuery(Collections.singletonList(value), context);
         }
 
         @Override
-        public Query termsQuery(List values, @Nullable QueryShardContext context) {
+        public Query termsQuery(List values, @Nullable QueryParseContext context) {
             if (context == null) {
                 return super.termsQuery(values, context);
             }
diff --git a/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java b/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
index 12e40de..480d2a4 100644
--- a/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
+++ b/core/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MergeResult;
 import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext;
 import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.List;
@@ -137,7 +137,7 @@ public class TypeFieldMapper extends MetadataFieldMapper {
         }
 
         @Override
-        public Query termQuery(Object value, @Nullable QueryShardContext context) {
+        public Query termQuery(Object value, @Nullable QueryParseContext context) {
             if (indexOptions() == IndexOptions.NONE) {
                 return new ConstantScoreQuery(new PrefixQuery(new Term(UidFieldMapper.NAME, Uid.typePrefixAsBytes(BytesRefs.toBytesRef(value)))));
             }
diff --git a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
index 52930e8..91ff1de 100644
--- a/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java
@@ -42,7 +42,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 import org.elasticsearch.index.percolator.stats.ShardPercolateService;
 import org.elasticsearch.index.query.IndexQueryParserService;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.settings.IndexSettings;
 import org.elasticsearch.index.shard.AbstractIndexShardComponent;
@@ -185,13 +185,12 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
         }
     }
 
-    //norelease this method parses from xcontent to lucene query, need to re-investigate how to split context here
     private Query parseQuery(String type, XContentParser parser) {
         String[] previousTypes = null;
         if (type != null) {
-            QueryShardContext.setTypesWithPrevious(new String[]{type});
+            QueryParseContext.setTypesWithPrevious(new String[]{type});
         }
-        QueryShardContext context = queryParserService.getShardContext();
+        QueryParseContext context = queryParserService.getParseContext();
         try {
             context.reset(parser);
             // This means that fields in the query need to exist in the mapping prior to registering this query
@@ -210,10 +209,10 @@ public class PercolatorQueriesRegistry extends AbstractIndexShardComponent imple
             context.setMapUnmappedFieldAsString(mapUnmappedFieldsAsString ? true : false);
             return queryParserService.parseInnerQuery(context);
         } catch (IOException e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         } finally {
             if (type != null) {
-                QueryShardContext.setTypes(previousTypes);
+                QueryParseContext.setTypes(previousTypes);
             }
             context.reset(null);
         }
diff --git a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
deleted file mode 100644
index 1dbf666..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/AbstractQueryBuilder.java
+++ /dev/null
@@ -1,311 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentType;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Objects;
-
-/**
- * Base class for all classes producing lucene queries.
- * Supports conversion to BytesReference and creation of lucene Query objects.
- */
-public abstract class AbstractQueryBuilder<QB extends AbstractQueryBuilder> extends ToXContentToBytes implements QueryBuilder<QB> {
-
-    /** Default for boost to apply to resulting Lucene query. Defaults to 1.0*/
-    public static final float DEFAULT_BOOST = 1.0f;
-
-    protected String queryName;
-    protected float boost = DEFAULT_BOOST;
-
-    protected AbstractQueryBuilder() {
-        super(XContentType.JSON);
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        doXContent(builder, params);
-        builder.endObject();
-        return builder;
-    }
-
-    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
-
-    protected void printBoostAndQueryName(XContentBuilder builder) throws IOException {
-        builder.field("boost", boost);
-        if (queryName != null) {
-            builder.field("_name", queryName);
-        }
-    }
-
-    @Override
-    public final Query toQuery(QueryShardContext context) throws IOException {
-        Query query = doToQuery(context);
-        if (query != null) {
-            query.setBoost(boost);
-            if (queryName != null) {
-                context.addNamedQuery(queryName, query);
-            }
-        }
-        return query;
-    }
-
-    @Override
-    public final Query toFilter(QueryShardContext context) throws IOException {
-        Query result = null;
-            final boolean originalIsFilter = context.isFilter;
-            try {
-                context.isFilter = true;
-                result = toQuery(context);
-            } finally {
-                context.isFilter = originalIsFilter;
-            }
-        return result;
-    }
-
-    //norelease to be made abstract once all query builders override doToQuery providing their own specific implementation.
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return context.indexQueryParserService().queryParser(getName()).parse(context);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // default impl does not validate, subclasses should override.
-        //norelease to be possibly made abstract once all queries support validation
-        return null;
-    }
-
-    /**
-     * Returns the query name for the query.
-     */
-    @SuppressWarnings("unchecked")
-    @Override
-    public final QB queryName(String queryName) {
-        this.queryName = queryName;
-        return (QB) this;
-    }
-
-    /**
-     * Sets the query name for the query.
-     */
-    @Override
-    public final String queryName() {
-        return queryName;
-    }
-
-    /**
-     * Returns the boost for this query.
-     */
-    @Override
-    public final float boost() {
-        return this.boost;
-    }
-
-    /**
-     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
-     * weightings) have their score multiplied by the boost provided.
-     */
-    @SuppressWarnings("unchecked")
-    @Override
-    public final QB boost(float boost) {
-        this.boost = boost;
-        return (QB) this;
-    }
-
-    @Override
-    public final QB readFrom(StreamInput in) throws IOException {
-        QB queryBuilder = doReadFrom(in);
-        queryBuilder.boost = in.readFloat();
-        queryBuilder.queryName = in.readOptionalString();
-        return queryBuilder;
-    }
-
-    //norelease make this abstract once all builders implement doReadFrom themselves
-    protected QB doReadFrom(StreamInput in) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public final void writeTo(StreamOutput out) throws IOException {
-        doWriteTo(out);
-        out.writeFloat(boost);
-        out.writeOptionalString(queryName);
-    }
-
-    //norelease make this abstract once all builders implement doWriteTo themselves
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    protected final QueryValidationException addValidationError(String validationError, QueryValidationException validationException) {
-        return QueryValidationException.addValidationError(getName(), validationError, validationException);
-    }
-
-    @Override
-    public final boolean equals(Object obj) {
-        if (this == obj) {
-            return true;
-        }
-        if (obj == null || getClass() != obj.getClass()) {
-            return false;
-        }
-        @SuppressWarnings("unchecked")
-        QB other = (QB) obj;
-        return Objects.equals(queryName, other.queryName) &&
-                Objects.equals(boost, other.boost) &&
-                doEquals(other);
-    }
-
-    /**
-     * Indicates whether some other {@link QueryBuilder} object of the same type is "equal to" this one.
-     */
-    //norelease to be made abstract once all queries are refactored
-    protected boolean doEquals(QB other) {
-        return super.equals(other);
-    }
-
-    @Override
-    public final int hashCode() {
-        return Objects.hash(getClass(), queryName, boost, doHashCode());
-    }
-
-    //norelease to be made abstract once all queries are refactored
-    protected int doHashCode() {
-        return super.hashCode();
-    }
-
-    /**
-     * This helper method checks if the object passed in is a string, if so it
-     * converts it to a {@link BytesRef}.
-     * @param obj the input object
-     * @return the same input object or a {@link BytesRef} representation if input was of type string
-     */
-    protected static Object convertToBytesRefIfString(Object obj) {
-        if (obj instanceof String) {
-            return BytesRefs.toBytesRef(obj);
-        }
-        return obj;
-    }
-
-    /**
-     * This helper method checks if the object passed in is a {@link BytesRef}, if so it
-     * converts it to a utf8 string.
-     * @param obj the input object
-     * @return the same input object or a utf8 string if input was of type {@link BytesRef}
-     */
-    protected static Object convertToStringIfBytesRef(Object obj) {
-        if (obj instanceof BytesRef) {
-            return ((BytesRef) obj).utf8ToString();
-        }
-        return obj;
-    }
-
-    /**
-     * Helper method to convert collection of {@link QueryBuilder} instances to lucene
-     * {@link Query} instances. {@link QueryBuilder} that return <tt>null</tt> calling
-     * their {@link QueryBuilder#toQuery(QueryShardContext)} method are not added to the
-     * resulting collection.
-     *
-     * @throws IOException
-     * @throws QueryShardException
-     */
-    protected static Collection<Query> toQueries(Collection<QueryBuilder> queryBuilders, QueryShardContext context) throws QueryShardException,
-            IOException {
-        List<Query> queries = new ArrayList<>(queryBuilders.size());
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            Query query = queryBuilder.toQuery(context);
-            if (query != null) {
-                queries.add(query);
-            }
-        }
-        return queries;
-    }
-
-    protected QueryValidationException validateInnerQueries(List<QueryBuilder> queryBuilders, QueryValidationException initialValidationException) {
-        QueryValidationException validationException = initialValidationException;
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            validationException = validateInnerQuery(queryBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    protected QueryValidationException validateInnerQuery(QueryBuilder queryBuilder, QueryValidationException initialValidationException) {
-        QueryValidationException validationException = initialValidationException;
-        if (queryBuilder != null) {
-            QueryValidationException queryValidationException = queryBuilder.validate();
-            if (queryValidationException != null) {
-                validationException = QueryValidationException.addValidationErrors(queryValidationException.validationErrors(), validationException);
-            }
-        } else {
-            validationException = addValidationError("inner query cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public String getName() {
-        //default impl returns the same as writeable name, but we keep the distinction between the two just to make sure
-        return getWriteableName();
-    }
-
-    protected final void writeQueries(StreamOutput out, List<? extends QueryBuilder> queries) throws IOException {
-        out.writeVInt(queries.size());
-        for (QueryBuilder query : queries) {
-            out.writeQuery(query);
-        }
-    }
-
-    protected final List<QueryBuilder> readQueries(StreamInput in) throws IOException {
-        List<QueryBuilder> queries = new ArrayList<>();
-        int size = in.readVInt();
-        for (int i = 0; i < size; i++) {
-            queries.add(in.readQuery());
-        }
-        return queries;
-    }
-
-    protected final void writeOptionalQuery(StreamOutput out, QueryBuilder query) throws IOException {
-        if (query == null) {
-            out.writeBoolean(false);
-        } else {
-            out.writeBoolean(true);
-            out.writeQuery(query);
-        }
-    }
-
-    protected final QueryBuilder readOptionalQuery(StreamInput in) throws IOException {
-        if (in.readBoolean()) {
-            return in.readQuery();
-        }
-        return null;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
index 1f9c842..e0ecafc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/AndQueryBuilder.java
@@ -20,43 +20,30 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.Lists;
-
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Objects;
 
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  * @deprecated Use {@link BoolQueryBuilder} instead
  */
 @Deprecated
-public class AndQueryBuilder extends AbstractQueryBuilder<AndQueryBuilder> {
-
-    public static final String NAME = "and";
+public class AndQueryBuilder extends QueryBuilder {
 
-    private final ArrayList<QueryBuilder> filters = Lists.newArrayList();
+    private ArrayList<QueryBuilder> filters = Lists.newArrayList();
 
-    static final AndQueryBuilder PROTOTYPE = new AndQueryBuilder();
+    private String queryName;
 
-    /**
-     * @param filters nested filters, no <tt>null</tt> values are allowed
-     */
     public AndQueryBuilder(QueryBuilder... filters) {
-        Collections.addAll(this.filters, filters);
+        for (QueryBuilder filter : filters) {
+            this.filters.add(filter);
+        }
     }
 
     /**
      * Adds a filter to the list of filters to "and".
-     * @param filterBuilder nested filter, no <tt>null</tt> value allowed
      */
     public AndQueryBuilder add(QueryBuilder filterBuilder) {
         filters.add(filterBuilder);
@@ -64,79 +51,24 @@ public class AndQueryBuilder extends AbstractQueryBuilder<AndQueryBuilder> {
     }
 
     /**
-     * @return the list of queries added to "and".
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public List<QueryBuilder> innerQueries() {
-        return this.filters;
+    public AndQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(AndQueryParser.NAME);
         builder.startArray("filters");
         for (QueryBuilder filter : filters) {
             filter.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        if (filters.isEmpty()) {
-            // no filters provided, this should be ignored upstream
-            return null;
-        }
-
-        BooleanQuery query = new BooleanQuery();
-        for (QueryBuilder f : filters) {
-            Query innerQuery = f.toFilter(context);
-            // ignore queries that are null
-            if (innerQuery != null) {
-                query.add(innerQuery, Occur.MUST);
-            }
-        }
-        if (query.clauses().isEmpty()) {
-            // no inner lucene query exists, ignore upstream
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(filters, null);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filters);
-    }
-
-    @Override
-    protected boolean doEquals(AndQueryBuilder other) {
-        return Objects.equals(filters, other.filters);
-    }
-
-    @Override
-    protected AndQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        AndQueryBuilder andQueryBuilder = new AndQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            andQueryBuilder.add(queryBuilder);
-        }
-        return andQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, filters);
+        builder.endObject();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
index 35fbb94..8c1969d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/AndQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -28,11 +31,12 @@ import java.util.ArrayList;
 import static com.google.common.collect.Lists.newArrayList;
 
 /**
- * Parser for and query
- * @deprecated use bool query instead
+ *
  */
 @Deprecated
-public class AndQueryParser extends BaseQueryParser<AndQueryBuilder> {
+public class AndQueryParser implements QueryParser {
+
+    public static final String NAME = "and";
 
     @Inject
     public AndQueryParser() {
@@ -40,25 +44,26 @@ public class AndQueryParser extends BaseQueryParser<AndQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{AndQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public AndQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        final ArrayList<QueryBuilder> queries = newArrayList();
+        ArrayList<Query> queries = newArrayList();
         boolean queriesFound = false;
 
         String queryName = null;
         String currentFieldName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_ARRAY) {
             while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                 queriesFound = true;
-                QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
-                queries.add(filter);
+                Query filter = parseContext.parseInnerFilter();
+                if (filter != null) {
+                    queries.add(filter);
+                }
             }
         } else {
             while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -70,15 +75,23 @@ public class AndQueryParser extends BaseQueryParser<AndQueryBuilder> {
                     if ("filters".equals(currentFieldName)) {
                         queriesFound = true;
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
-                            queries.add(filter);
+                            Query filter = parseContext.parseInnerFilter();
+                            if (filter != null) {
+                                queries.add(filter);
+                            }
+                        }
+                    } else {
+                        queriesFound = true;
+                        while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
+                            Query filter = parseContext.parseInnerFilter();
+                            if (filter != null) {
+                                queries.add(filter);
+                            }
                         }
                     }
                 } else if (token.isValue()) {
                     if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
-                    } else if ("boost".equals(currentFieldName)) {
-                        boost = parser.floatValue();
                     } else {
                         throw new QueryParsingException(parseContext, "[and] query does not support [" + currentFieldName + "]");
                     }
@@ -90,17 +103,18 @@ public class AndQueryParser extends BaseQueryParser<AndQueryBuilder> {
             throw new QueryParsingException(parseContext, "[and] query requires 'filters' to be set on it'");
         }
 
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        for (QueryBuilder query : queries) {
-            andQuery.add(query);
+        if (queries.isEmpty()) {
+            // no filters provided, this should be ignored upstream
+            return null;
         }
-        andQuery.queryName(queryName);
-        andQuery.boost(boost);
-        return andQuery;
-    }
 
-    @Override
-    public AndQueryBuilder getBuilderPrototype() {
-        return AndQueryBuilder.PROTOTYPE;
+        BooleanQuery query = new BooleanQuery();
+        for (Query f : queries) {
+            query.add(f, Occur.MUST);
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java
deleted file mode 100644
index 4ff02df..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParser.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-/**
- * Class used during the query parsers refactoring. Will be removed once we can parse search requests on the coordinating node.
- * All query parsers that have a refactored "fromXContent" method can be changed to extend this instead of {@link BaseQueryParserTemp}.
- * Keeps old {@link QueryParser#parse(QueryShardContext)} method as a stub delegating to
- * {@link QueryParser#fromXContent(QueryParseContext)} and {@link QueryBuilder#toQuery(QueryShardContext)}}
- */
-//norelease needs to be removed once we parse search requests on the coordinating node, as the parse method is not needed anymore at that point.
-public abstract class BaseQueryParser<QB extends QueryBuilder<QB>> implements QueryParser<QB> {
-
-    @Override
-    public final Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        return fromXContent(context.parseContext()).toQuery(context);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java b/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java
deleted file mode 100644
index 4dc3eae..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseQueryParserTemp.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-/**
- * This class with method impl is an intermediate step in the query parsers refactoring.
- * Provides a fromXContent default implementation for query parsers that don't have yet a
- * specific fromXContent implementation that returns a QueryBuilder.
- */
-//norelease to be removed once all queries are moved over to extend BaseQueryParser
-public abstract class BaseQueryParserTemp implements QueryParser {
-
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        Query query = parse(parseContext.shardContext());
-        return new QueryWrappingQueryBuilder(query);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java
deleted file mode 100644
index 6444184..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/BaseTermQueryBuilder.java
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-import java.util.Objects;
-
-public abstract class BaseTermQueryBuilder<QB extends BaseTermQueryBuilder<QB>> extends AbstractQueryBuilder<QB> {
-
-    /** Name of field to match against. */
-    protected final String fieldName;
-
-    /** Value to find matches for. */
-    protected final Object value;
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, String value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, int value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, long value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, float value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, double value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, boolean value) {
-        this(fieldName, (Object) value);
-    }
-
-    /**
-     * Constructs a new base term query.
-     * In case value is assigned to a string, we internally convert it to a {@link BytesRef}
-     * because in {@link TermQueryParser} and {@link SpanTermQueryParser} string values are parsed to {@link BytesRef}
-     * and we want internal representation of query to be equal regardless of whether it was created from XContent or via Java API.
-     *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
-     */
-    public BaseTermQueryBuilder(String fieldName, Object value) {
-        this.fieldName = fieldName;
-        this.value = convertToBytesRefIfString(value);
-    }
-
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    /**
-     *  Returns the value used in this query.
-     *  If necessary, converts internal {@link BytesRef} representation back to string.
-     */
-    public Object value() {
-        return convertToStringIfBytesRef(this.value);
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(getName());
-        builder.startObject(fieldName);
-        builder.field("value", convertToStringIfBytesRef(this.value));
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    /** Returns a {@link QueryValidationException} if fieldName is null or empty, or if value is null. */
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (fieldName == null || fieldName.isEmpty()) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (value == null) {
-            validationException = addValidationError("value cannot be null.", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(fieldName, value);
-    }
-
-    @Override
-    protected final boolean doEquals(BaseTermQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(value, other.value);
-    }
-
-    @Override
-    protected final QB doReadFrom(StreamInput in) throws IOException {
-        return createBuilder(in.readString(), in.readGenericValue());
-    }
-
-    protected abstract QB createBuilder(String fieldName, Object value);
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeGenericValue(value);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
index 32d9c85..c377667 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java
@@ -19,35 +19,17 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Objects;
-
-import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
  * A Query that matches documents matching boolean combinations of other queries.
  */
-public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
-
-    public static final String NAME = "bool";
-
-    public static final boolean ADJUST_PURE_NEGATIVE_DEFAULT = true;
-
-    public static final boolean DISABLE_COORD_DEFAULT = false;
-
-    static final BoolQueryBuilder PROTOTYPE = new BoolQueryBuilder();
+public class BoolQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<BoolQueryBuilder> {
 
     private final List<QueryBuilder> mustClauses = new ArrayList<>();
 
@@ -57,15 +39,19 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
 
     private final List<QueryBuilder> shouldClauses = new ArrayList<>();
 
-    private boolean disableCoord = DISABLE_COORD_DEFAULT;
+    private float boost = -1;
 
-    private boolean adjustPureNegative = ADJUST_PURE_NEGATIVE_DEFAULT;
+    private Boolean disableCoord;
 
     private String minimumShouldMatch;
+    
+    private Boolean adjustPureNegative;
+
+    private String queryName;
 
     /**
      * Adds a query that <b>must</b> appear in the matching documents and will
-     * contribute to scoring. No <tt>null</tt> value allowed.
+     * contribute to scoring.
      */
     public BoolQueryBuilder must(QueryBuilder queryBuilder) {
         mustClauses.add(queryBuilder);
@@ -73,15 +59,8 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must</b> appear in the matching documents.
-     */
-    public List<QueryBuilder> must() {
-        return this.mustClauses;
-    }
-
-    /**
      * Adds a query that <b>must</b> appear in the matching documents but will
-     * not contribute to scoring. No <tt>null</tt> value allowed.
+     * not contribute to scoring.
      */
     public BoolQueryBuilder filter(QueryBuilder queryBuilder) {
         filterClauses.add(queryBuilder);
@@ -89,15 +68,8 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must</b> appear in the matching documents but don't conntribute to scoring
-     */
-    public List<QueryBuilder> filter() {
-        return this.filterClauses;
-    }
-
-    /**
-     * Adds a query that <b>must not</b> appear in the matching documents.
-     * No <tt>null</tt> value allowed.
+     * Adds a query that <b>must not</b> appear in the matching documents and
+     * will not contribute to scoring.
      */
     public BoolQueryBuilder mustNot(QueryBuilder queryBuilder) {
         mustNotClauses.add(queryBuilder);
@@ -105,16 +77,9 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the queries that <b>must not</b> appear in the matching documents.
-     */
-    public List<QueryBuilder> mustNot() {
-        return this.mustNotClauses;
-    }
-
-    /**
-     * Adds a clause that <i>should</i> be matched by the returned documents. For a boolean query with no
+     * Adds a query that <i>should</i> appear in the matching documents. For a boolean query with no
      * <tt>MUST</tt> clauses one or more <code>SHOULD</code> clauses must match a document
-     * for the BooleanQuery to match. No <tt>null</tt> value allowed.
+     * for the BooleanQuery to match.
      *
      * @see #minimumNumberShouldMatch(int)
      */
@@ -124,13 +89,13 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * Gets the list of clauses that <b>should</b> be matched by the returned documents.
-     *
-     * @see #should(QueryBuilder)
-     *  @see #minimumNumberShouldMatch(int)
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public List<QueryBuilder> should() {
-        return this.shouldClauses;
+    @Override
+    public BoolQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -142,13 +107,6 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * @return whether the <tt>Similarity#coord(int,int)</tt> in scoring are disabled. Defaults to <tt>false</tt>.
-     */
-    public boolean disableCoord() {
-        return this.disableCoord;
-    }
-
-    /**
      * Specifies a minimum number of the optional (should) boolean clauses which must be satisfied.
      * <p/>
      * <p>By default no optional clauses are necessary for a match
@@ -166,23 +124,6 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
         return this;
     }
 
-
-    /**
-     * Specifies a minimum number of the optional (should) boolean clauses which must be satisfied.
-     * @see BoolQueryBuilder#minimumNumberShouldMatch(int)
-     */
-    public BoolQueryBuilder minimumNumberShouldMatch(String minimumNumberShouldMatch) {
-        this.minimumShouldMatch = minimumNumberShouldMatch;
-        return this;
-    }
-
-    /**
-     * @return the string representation of the minimumShouldMatch settings for this query
-     */
-    public String minimumNumberShouldMatch() {
-        return this.minimumShouldMatch;
-    }
-
     /**
      * Sets the minimum should match using the special syntax (for example, supporting percentage).
      */
@@ -198,7 +139,7 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     public boolean hasClauses() {
         return !(mustClauses.isEmpty() && shouldClauses.isEmpty() && mustNotClauses.isEmpty() && filterClauses.isEmpty());
     }
-
+    
     /**
      * If a boolean query contains only negative ("must not") clauses should the
      * BooleanQuery be enhanced with a {@link MatchAllDocsQuery} in order to act
@@ -210,29 +151,39 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
     }
 
     /**
-     * @return the setting for the adjust_pure_negative setting in this query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean adjustPureNegative() {
-        return this.adjustPureNegative;
+    public BoolQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject("bool");
         doXArrayContent("must", mustClauses, builder, params);
         doXArrayContent("filter", filterClauses, builder, params);
         doXArrayContent("must_not", mustNotClauses, builder, params);
         doXArrayContent("should", shouldClauses, builder, params);
-        builder.field("disable_coord", disableCoord);
-        builder.field("adjust_pure_negative", adjustPureNegative);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
+        }
         if (minimumShouldMatch != null) {
             builder.field("minimum_should_match", minimumShouldMatch);
         }
-        printBoostAndQueryName(builder);
+        if (adjustPureNegative != null) {
+            builder.field("adjust_pure_negative", adjustPureNegative);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
 
-    private static void doXArrayContent(String field, List<QueryBuilder> clauses, XContentBuilder builder, Params params) throws IOException {
+    private void doXArrayContent(String field, List<QueryBuilder> clauses, XContentBuilder builder, Params params) throws IOException {
         if (clauses.isEmpty()) {
             return;
         }
@@ -248,103 +199,4 @@ public class BoolQueryBuilder extends AbstractQueryBuilder<BoolQueryBuilder> {
         }
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        BooleanQuery booleanQuery = new BooleanQuery(disableCoord);
-        addBooleanClauses(context, booleanQuery, mustClauses, BooleanClause.Occur.MUST);
-        addBooleanClauses(context, booleanQuery, mustNotClauses, BooleanClause.Occur.MUST_NOT);
-        addBooleanClauses(context, booleanQuery, shouldClauses, BooleanClause.Occur.SHOULD);
-        addBooleanClauses(context, booleanQuery, filterClauses, BooleanClause.Occur.FILTER);
-
-        if (booleanQuery.clauses().isEmpty()) {
-            return new MatchAllDocsQuery();
-        }
-
-        Queries.applyMinimumShouldMatch(booleanQuery, minimumShouldMatch);
-        return adjustPureNegative ? fixNegativeQueryIfNeeded(booleanQuery) : booleanQuery;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        validationException = validateInnerQueries(mustClauses, validationException);
-        validationException = validateInnerQueries(shouldClauses, validationException);
-        validationException = validateInnerQueries(mustNotClauses, validationException);
-        validationException = validateInnerQueries(filterClauses, validationException);
-        return validationException;
-    }
-
-    private void addBooleanClauses(QueryShardContext context, BooleanQuery booleanQuery, List<QueryBuilder> clauses, Occur occurs) throws IOException {
-        for (QueryBuilder query : clauses) {
-            Query luceneQuery = null;
-            switch (occurs) {
-            case SHOULD:
-                if (context.isFilter() && minimumShouldMatch == null) {
-                    minimumShouldMatch = "1";
-                }
-                luceneQuery = query.toQuery(context);
-                break;
-            case FILTER:
-            case MUST_NOT:
-                luceneQuery = query.toFilter(context);
-                break;
-            case MUST:
-                luceneQuery = query.toQuery(context);
-            }
-            if (luceneQuery != null) {
-                booleanQuery.add(new BooleanClause(luceneQuery, occurs));
-            }
-        }
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(adjustPureNegative, disableCoord,
-                minimumShouldMatch, mustClauses, shouldClauses, mustNotClauses, filterClauses);
-    }
-
-    @Override
-    protected boolean doEquals(BoolQueryBuilder other) {
-        return Objects.equals(adjustPureNegative, other.adjustPureNegative) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(minimumShouldMatch, other.minimumShouldMatch) &&
-                Objects.equals(mustClauses, other.mustClauses) &&
-                Objects.equals(shouldClauses, other.shouldClauses) &&
-                Objects.equals(mustNotClauses, other.mustNotClauses) &&
-                Objects.equals(filterClauses, other.filterClauses);
-    }
-
-    @Override
-    protected BoolQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        boolQueryBuilder.mustClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.mustNotClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.shouldClauses.addAll(queryBuilders);
-        queryBuilders = readQueries(in);
-        boolQueryBuilder.filterClauses.addAll(queryBuilders);
-        boolQueryBuilder.adjustPureNegative = in.readBoolean();
-        boolQueryBuilder.disableCoord = in.readBoolean();
-        boolQueryBuilder.minimumShouldMatch = in.readOptionalString();
-        return boolQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, mustClauses);
-        writeQueries(out, mustNotClauses);
-        writeQueries(out, shouldClauses);
-        writeQueries(out, filterClauses);
-        out.writeBoolean(adjustPureNegative);
-        out.writeBoolean(disableCoord);
-        out.writeOptionalString(minimumShouldMatch);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
index 60b5973..6fa7faa 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java
@@ -19,8 +19,12 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -28,11 +32,14 @@ import java.io.IOException;
 import java.util.List;
 
 import static com.google.common.collect.Lists.newArrayList;
+import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfNeeded;
 
 /**
- * Parser for bool query
+ *
  */
-public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
+public class BoolQueryParser implements QueryParser {
+
+    public static final String NAME = "bool";
 
     @Inject
     public BoolQueryParser(Settings settings) {
@@ -41,27 +48,23 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{BoolQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public BoolQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        boolean disableCoord = BoolQueryBuilder.DISABLE_COORD_DEFAULT;
-        boolean adjustPureNegative = BoolQueryBuilder.ADJUST_PURE_NEGATIVE_DEFAULT;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean disableCoord = false;
+        float boost = 1.0f;
         String minimumShouldMatch = null;
 
-        final List<QueryBuilder> mustClauses = newArrayList();
-        final List<QueryBuilder> mustNotClauses = newArrayList();
-        final List<QueryBuilder> shouldClauses = newArrayList();
-        final List<QueryBuilder> filterClauses = newArrayList();
+        List<BooleanClause> clauses = newArrayList();
+        boolean adjustPureNegative = true;
         String queryName = null;
-
+        
         String currentFieldName = null;
         XContentParser.Token token;
-        QueryBuilder query;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -70,21 +73,32 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 switch (currentFieldName) {
                 case "must":
-                    query = parseContext.parseInnerQueryBuilder();
-                    mustClauses.add(query);
+                    Query query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST));
+                    }
                     break;
                 case "should":
-                    query = parseContext.parseInnerQueryBuilder();
-                    shouldClauses.add(query);
+                    query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.SHOULD));
+                        if (parseContext.isFilter() && minimumShouldMatch == null) {
+                            minimumShouldMatch = "1";
+                        }
+                    }
                     break;
                 case "filter":
-                    query = parseContext.parseInnerFilterToQueryBuilder();
-                    filterClauses.add(query);
+                    query = parseContext.parseInnerFilter();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.FILTER));
+                    }
                     break;
                 case "must_not":
                 case "mustNot":
-                    query = parseContext.parseInnerFilterToQueryBuilder();
-                    mustNotClauses.add(query);
+                    query = parseContext.parseInnerFilter();
+                    if (query != null) {
+                        clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST_NOT));
+                    }
                     break;
                 default:
                     throw new QueryParsingException(parseContext, "[bool] query does not support [" + currentFieldName + "]");
@@ -93,21 +107,32 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
                 while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                     switch (currentFieldName) {
                     case "must":
-                        query = parseContext.parseInnerQueryBuilder();
-                        mustClauses.add(query);
+                        Query query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST));
+                        }
                         break;
                     case "should":
-                        query = parseContext.parseInnerQueryBuilder();
-                        shouldClauses.add(query);
+                        query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.SHOULD));
+                            if (parseContext.isFilter() && minimumShouldMatch == null) {
+                                minimumShouldMatch = "1";
+                            }
+                        }
                         break;
                     case "filter":
-                        query = parseContext.parseInnerFilterToQueryBuilder();
-                        filterClauses.add(query);
+                        query = parseContext.parseInnerFilter();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.FILTER));
+                        }
                         break;
                     case "must_not":
                     case "mustNot":
-                        query = parseContext.parseInnerFilterToQueryBuilder();
-                        mustNotClauses.add(query);
+                        query = parseContext.parseInnerFilter();
+                        if (query != null) {
+                            clauses.add(new BooleanClause(query, BooleanClause.Occur.MUST_NOT));
+                        }
                         break;
                     default:
                         throw new QueryParsingException(parseContext, "bool query does not support [" + currentFieldName + "]");
@@ -131,29 +156,21 @@ public class BoolQueryParser extends BaseQueryParser<BoolQueryBuilder> {
                 }
             }
         }
-        BoolQueryBuilder boolQuery = new BoolQueryBuilder();
-        for (QueryBuilder queryBuilder : mustClauses) {
-            boolQuery.must(queryBuilder);
-        }
-        for (QueryBuilder queryBuilder : mustNotClauses) {
-            boolQuery.mustNot(queryBuilder);
+
+        if (clauses.isEmpty()) {
+            return new MatchAllDocsQuery();
         }
-        for (QueryBuilder queryBuilder : shouldClauses) {
-            boolQuery.should(queryBuilder);
+
+        BooleanQuery booleanQuery = new BooleanQuery(disableCoord);
+        for (BooleanClause clause : clauses) {
+            booleanQuery.add(clause);
         }
-        for (QueryBuilder queryBuilder : filterClauses) {
-            boolQuery.filter(queryBuilder);
+        booleanQuery.setBoost(boost);
+        Queries.applyMinimumShouldMatch(booleanQuery, minimumShouldMatch);
+        Query query = adjustPureNegative ? fixNegativeQueryIfNeeded(booleanQuery) : booleanQuery;
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        boolQuery.boost(boost);
-        boolQuery.disableCoord(disableCoord);
-        boolQuery.adjustPureNegative(adjustPureNegative);
-        boolQuery.minimumNumberShouldMatch(minimumShouldMatch);
-        boolQuery.queryName(queryName);
-        return boolQuery;
-    }
-
-    @Override
-    public BoolQueryBuilder getBuilderPrototype() {
-        return BoolQueryBuilder.PROTOTYPE;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java
new file mode 100644
index 0000000..31572ce
--- /dev/null
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostableQueryBuilder.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.index.query;
+
+/**
+ * Query builder which allow setting some boost
+ */
+public interface BoostableQueryBuilder<B extends BoostableQueryBuilder<B>> {
+
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    B boost(float boost);
+
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
index 69ab70a..9d67469 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java
@@ -19,14 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.queries.BoostingQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * The BoostingQuery class can be used to effectively demote results that match a given query.
@@ -40,132 +35,63 @@ import java.util.Objects;
  * multiplied by the supplied "boost" parameter, so this should be less than 1 to achieve a
  * demoting effect
  */
-public class BoostingQueryBuilder extends AbstractQueryBuilder<BoostingQueryBuilder> {
+public class BoostingQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<BoostingQueryBuilder> {
 
-    public static final String NAME = "boosting";
+    private QueryBuilder positiveQuery;
 
-    private final QueryBuilder positiveQuery;
-
-    private final QueryBuilder negativeQuery;
+    private QueryBuilder negativeQuery;
 
     private float negativeBoost = -1;
 
-    static final BoostingQueryBuilder PROTOTYPE = new BoostingQueryBuilder(null, null);
+    private float boost = -1;
+
+    public BoostingQueryBuilder() {
 
-    /**
-     * Create a new {@link BoostingQueryBuilder}
-     *
-     * @param positiveQuery the positive query for this boosting query.
-     * @param negativeQuery the negative query for this boosting query.
-     */
-    public BoostingQueryBuilder(QueryBuilder positiveQuery, QueryBuilder negativeQuery) {
-        this.positiveQuery = positiveQuery;
-        this.negativeQuery = negativeQuery;
     }
 
-    /**
-     * Get the positive query for this boosting query.
-     */
-    public QueryBuilder positiveQuery() {
-        return this.positiveQuery;
+    public BoostingQueryBuilder positive(QueryBuilder positiveQuery) {
+        this.positiveQuery = positiveQuery;
+        return this;
     }
 
-    /**
-     * Get the negative query for this boosting query.
-     */
-    public QueryBuilder negativeQuery() {
-        return this.negativeQuery;
+    public BoostingQueryBuilder negative(QueryBuilder negativeQuery) {
+        this.negativeQuery = negativeQuery;
+        return this;
     }
 
-    /**
-     * Set the negative boost factor.
-     */
     public BoostingQueryBuilder negativeBoost(float negativeBoost) {
         this.negativeBoost = negativeBoost;
         return this;
     }
 
-    /**
-     * Get the negative boost factor.
-     */
-    public float negativeBoost() {
-        return this.negativeBoost;
-    }
-
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("positive");
-        positiveQuery.toXContent(builder, params);
-        builder.field("negative");
-        negativeQuery.toXContent(builder, params);
-        builder.field("negative_boost", negativeBoost);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public BoostingQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (negativeBoost < 0) {
-            validationException = addValidationError("query requires negativeBoost to be set to positive value", validationException);
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        if (positiveQuery == null) {
+            throw new IllegalArgumentException("boosting query requires positive query to be set");
         }
         if (negativeQuery == null) {
-            validationException = addValidationError("inner clause [negative] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(negativeQuery, validationException);
+            throw new IllegalArgumentException("boosting query requires negative query to be set");
         }
-        if (positiveQuery == null) {
-            validationException = addValidationError("inner clause [positive] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(positiveQuery, validationException);
+        if (negativeBoost == -1) {
+            throw new IllegalArgumentException("boosting query requires negativeBoost to be set");
         }
-        return validationException;
-    }
+        builder.startObject(BoostingQueryParser.NAME);
+        builder.field("positive");
+        positiveQuery.toXContent(builder, params);
+        builder.field("negative");
+        negativeQuery.toXContent(builder, params);
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
+        builder.field("negative_boost", negativeBoost);
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query positive = positiveQuery.toQuery(context);
-        Query negative = negativeQuery.toQuery(context);
-        // make upstream queries ignore this query by returning `null`
-        // if either inner query builder returns null
-        if (positive == null || negative == null) {
-            return null;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-
-        return new BoostingQuery(positive, negative, negativeBoost);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(negativeBoost, positiveQuery, negativeQuery);
-    }
-
-    @Override
-    protected boolean doEquals(BoostingQueryBuilder other) {
-        return Objects.equals(negativeBoost, other.negativeBoost) &&
-                Objects.equals(positiveQuery, other.positiveQuery) &&
-                Objects.equals(negativeQuery, other.negativeQuery);
-    }
-
-    @Override
-    protected BoostingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder positiveQuery = in.readQuery();
-        QueryBuilder negativeQuery = in.readQuery();
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positiveQuery, negativeQuery);
-        boostingQuery.negativeBoost = in.readFloat();
-        return boostingQuery;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(positiveQuery);
-        out.writeQuery(negativeQuery);
-        out.writeFloat(negativeBoost);
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
index 699d23d..c160b2f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java
@@ -19,15 +19,19 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.queries.BoostingQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for boosting query
+ *
  */
-public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
+public class BoostingQueryParser implements QueryParser {
+
+    public static final String NAME = "boosting";
 
     @Inject
     public BoostingQueryParser() {
@@ -35,20 +39,19 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{BoostingQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public BoostingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder positiveQuery = null;
+        Query positiveQuery = null;
         boolean positiveQueryFound = false;
-        QueryBuilder negativeQuery = null;
+        Query negativeQuery = null;
         boolean negativeQueryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = -1;
         float negativeBoost = -1;
-        String queryName = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -57,10 +60,10 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("positive".equals(currentFieldName)) {
-                    positiveQuery = parseContext.parseInnerQueryBuilder();
+                    positiveQuery = parseContext.parseInnerQuery();
                     positiveQueryFound = true;
                 } else if ("negative".equals(currentFieldName)) {
-                    negativeQuery = parseContext.parseInnerQueryBuilder();
+                    negativeQuery = parseContext.parseInnerQuery();
                     negativeQueryFound = true;
                 } else {
                     throw new QueryParsingException(parseContext, "[boosting] query does not support [" + currentFieldName + "]");
@@ -68,8 +71,6 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
             } else if (token.isValue()) {
                 if ("negative_boost".equals(currentFieldName) || "negativeBoost".equals(currentFieldName)) {
                     negativeBoost = parser.floatValue();
-                } else if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
@@ -78,25 +79,25 @@ public class BoostingQueryParser extends BaseQueryParser<BoostingQueryBuilder> {
             }
         }
 
-        if (!positiveQueryFound) {
+        if (positiveQuery == null && !positiveQueryFound) {
             throw new QueryParsingException(parseContext, "[boosting] query requires 'positive' query to be set'");
         }
-        if (!negativeQueryFound) {
+        if (negativeQuery == null && !negativeQueryFound) {
             throw new QueryParsingException(parseContext, "[boosting] query requires 'negative' query to be set'");
         }
-        if (negativeBoost < 0) {
-            throw new QueryParsingException(parseContext, "[boosting] query requires 'negative_boost' to be set to be a positive value'");
+        if (negativeBoost == -1) {
+            throw new QueryParsingException(parseContext, "[boosting] query requires 'negative_boost' to be set'");
         }
 
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positiveQuery, negativeQuery);
-        boostingQuery.negativeBoost(negativeBoost);
-        boostingQuery.boost(boost);
-        boostingQuery.queryName(queryName);
-        return boostingQuery;
-    }
+        // parsers returned null
+        if (positiveQuery == null || negativeQuery == null) {
+            return null;
+        }
 
-    @Override
-    public BoostingQueryBuilder getBuilderPrototype() {
-        return BoostingQueryBuilder.PROTOTYPE;
+        BoostingQuery boostingQuery = new BoostingQuery(positiveQuery, negativeQuery, negativeBoost);
+        if (boost != -1) {
+            boostingQuery.setBoost(boost);
+        }
+        return boostingQuery;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
index c7e68d2..ae9c10d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryBuilder.java
@@ -19,31 +19,18 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
 import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * CommonTermsQuery query is a query that executes high-frequency terms in a
  * optional sub-query to prevent slow queries due to "common" terms like
- * stopwords. This query basically builds 2 queries off the
- * {@link org.apache.lucene.queries.CommonTermsQuery#add(Term) added} terms
- * where low-frequency terms are added to a required boolean clause
+ * stopwords. This query basically builds 2 queries off the {@link #add(Term)
+ * added} terms where low-frequency terms are added to a required boolean clause
  * and high-frequency terms are added to an optional boolean clause. The
  * optional clause is only executed if the required "low-frequency' clause
  * matches. Scores produced by this query will be slightly different to plain
@@ -55,52 +42,46 @@ import java.util.Objects;
  * execution times significantly if applicable.
  * <p>
  */
-public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQueryBuilder> {
+public class CommonTermsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<CommonTermsQueryBuilder> {
 
-    public static final String NAME = "common";
-
-    public static final float DEFAULT_CUTOFF_FREQ = 0.01f;
-
-    public static final Operator DEFAULT_HIGH_FREQ_OCCUR = Operator.OR;
-
-    public static final Operator DEFAULT_LOW_FREQ_OCCUR = Operator.OR;
-
-    public static final boolean DEFAULT_DISABLE_COORD = true;
+    public static enum Operator {
+        OR, AND
+    }
 
-    private final String fieldName;
+    private final String name;
 
     private final Object text;
 
-    private Operator highFreqOperator = DEFAULT_HIGH_FREQ_OCCUR;
+    private Operator highFreqOperator = null;
 
-    private Operator lowFreqOperator = DEFAULT_LOW_FREQ_OCCUR;
+    private Operator lowFreqOperator = null;
 
     private String analyzer = null;
 
+    private Float boost = null;
+
     private String lowFreqMinimumShouldMatch = null;
 
     private String highFreqMinimumShouldMatch = null;
 
-    private boolean disableCoord = DEFAULT_DISABLE_COORD;
+    private Boolean disableCoord = null;
 
-    private float cutoffFrequency = DEFAULT_CUTOFF_FREQ;
+    private Float cutoffFrequency = null;
 
-    static final CommonTermsQueryBuilder PROTOTYPE = new CommonTermsQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new common terms query.
      */
-    public CommonTermsQueryBuilder(String fieldName, Object text) {
-        this.fieldName = fieldName;
+    public CommonTermsQueryBuilder(String name, Object text) {
+        if (name == null) {
+            throw new IllegalArgumentException("Field name must not be null");
+        }
+        if (text == null) {
+            throw new IllegalArgumentException("Query must not be null");
+        }
         this.text = text;
-    }
-
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public Object value() {
-        return this.text;
+        this.name = name;
     }
 
     /**
@@ -109,27 +90,19 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
      * <tt>AND</tt>.
      */
     public CommonTermsQueryBuilder highFreqOperator(Operator operator) {
-        this.highFreqOperator = (operator == null) ? DEFAULT_HIGH_FREQ_OCCUR : operator;
+        this.highFreqOperator = operator;
         return this;
     }
 
-    public Operator highFreqOperator() {
-        return highFreqOperator;
-    }
-
     /**
      * Sets the operator to use for terms with a low document frequency (less
      * than {@link #cutoffFrequency(float)}. Defaults to <tt>AND</tt>.
      */
     public CommonTermsQueryBuilder lowFreqOperator(Operator operator) {
-        this.lowFreqOperator = (operator == null) ? DEFAULT_LOW_FREQ_OCCUR : operator;
+        this.lowFreqOperator = operator;
         return this;
     }
 
-    public Operator lowFreqOperator() {
-        return lowFreqOperator;
-    }
-
     /**
      * Explicitly set the analyzer to use. Defaults to use explicit mapping
      * config for the field, or, if not set, the default search analyzer.
@@ -139,8 +112,13 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         return this;
     }
 
-    public String analyzer() {
-        return this.analyzer;
+    /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public CommonTermsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -148,17 +126,13 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
      * in [0..1] (or absolute number >=1) representing the maximum threshold of
      * a terms document frequency to be considered a low frequency term.
      * Defaults to
-     * <tt>{@value #DEFAULT_CUTOFF_FREQ}</tt>
+     * <tt>{@value CommonTermsQueryParser#DEFAULT_MAX_TERM_DOC_FREQ}</tt>
      */
     public CommonTermsQueryBuilder cutoffFrequency(float cutoffFrequency) {
         this.cutoffFrequency = cutoffFrequency;
         return this;
     }
 
-    public float cutoffFrequency() {
-        return this.cutoffFrequency;
-    }
-
     /**
      * Sets the minimum number of high frequent query terms that need to match in order to
      * produce a hit when there are no low frequen terms.
@@ -168,10 +142,6 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         return this;
     }
 
-    public String highFreqMinimumShouldMatch() {
-        return this.highFreqMinimumShouldMatch;
-    }
-
     /**
      * Sets the minimum number of low frequent query terms that need to match in order to
      * produce a hit.
@@ -180,33 +150,44 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
         this.lowFreqMinimumShouldMatch = lowFreqMinimumShouldMatch;
         return this;
     }
-
-    public String lowFreqMinimumShouldMatch() {
-        return this.lowFreqMinimumShouldMatch;
-    }
-
+    
     public CommonTermsQueryBuilder disableCoord(boolean disableCoord) {
         this.disableCoord = disableCoord;
         return this;
     }
 
-    public boolean disableCoord() {
-        return this.disableCoord;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public CommonTermsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(CommonTermsQueryParser.NAME);
+        builder.startObject(name);
 
         builder.field("query", text);
-        builder.field("disable_coord", disableCoord);
-        builder.field("high_freq_operator", highFreqOperator.toString());
-        builder.field("low_freq_operator", lowFreqOperator.toString());
+        if (disableCoord != null) {
+            builder.field("disable_coord", disableCoord);
+        }
+        if (highFreqOperator != null) {
+            builder.field("high_freq_operator", highFreqOperator.toString());
+        }
+        if (lowFreqOperator != null) {
+            builder.field("low_freq_operator", lowFreqOperator.toString());
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
-        builder.field("cutoff_frequency", cutoffFrequency);
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (cutoffFrequency != null) {
+            builder.field("cutoff_frequency", cutoffFrequency);
+        }
         if (lowFreqMinimumShouldMatch != null || highFreqMinimumShouldMatch != null) {
             builder.startObject("minimum_should_match");
             if (lowFreqMinimumShouldMatch != null) {
@@ -217,125 +198,11 @@ public class CommonTermsQueryBuilder extends AbstractQueryBuilder<CommonTermsQue
             }
             builder.endObject();
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        String field;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            field = fieldType.names().indexName();
-        } else {
-            field = fieldName;
-        }
-
-        Analyzer analyzerObj;
-        if (analyzer == null) {
-            if (fieldType != null) {
-                analyzerObj = context.getSearchAnalyzer(fieldType);
-            } else {
-                analyzerObj = context.mapperService().searchAnalyzer();
-            }
-        } else {
-            analyzerObj = context.mapperService().analysisService().analyzer(analyzer);
-            if (analyzerObj == null) {
-                throw new QueryShardException(context, "[common] analyzer [" + analyzer + "] not found");
-            }
-        }
-
-        Occur highFreqOccur = highFreqOperator.toBooleanClauseOccur();
-        Occur lowFreqOccur = lowFreqOperator.toBooleanClauseOccur();
-
-        ExtendedCommonTermsQuery commonsQuery = new ExtendedCommonTermsQuery(highFreqOccur, lowFreqOccur, cutoffFrequency, disableCoord, fieldType);
-        return parseQueryString(commonsQuery, text, field, analyzerObj, lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch);
-    }
-
-    static Query parseQueryString(ExtendedCommonTermsQuery query, Object queryString, String field, Analyzer analyzer,
-                                         String lowFreqMinimumShouldMatch, String highFreqMinimumShouldMatch) throws IOException {
-        // Logic similar to QueryParser#getFieldQuery
-        int count = 0;
-        try (TokenStream source = analyzer.tokenStream(field, queryString.toString())) {
-            source.reset();
-            CharTermAttribute termAtt = source.addAttribute(CharTermAttribute.class);
-            BytesRefBuilder builder = new BytesRefBuilder();
-            while (source.incrementToken()) {
-                // UTF-8
-                builder.copyChars(termAtt);
-                query.add(new Term(field, builder.toBytesRef()));
-                count++;
-            }
-        }
-
-        if (count == 0) {
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        query.setLowFreqMinimumNumberShouldMatch(lowFreqMinimumShouldMatch);
-        query.setHighFreqMinimumNumberShouldMatch(highFreqMinimumShouldMatch);
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.text == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected CommonTermsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        CommonTermsQueryBuilder commonTermsQueryBuilder = new CommonTermsQueryBuilder(in.readString(), in.readGenericValue());
-        commonTermsQueryBuilder.highFreqOperator = Operator.readOperatorFrom(in);
-        commonTermsQueryBuilder.lowFreqOperator = Operator.readOperatorFrom(in);
-        commonTermsQueryBuilder.analyzer = in.readOptionalString();
-        commonTermsQueryBuilder.lowFreqMinimumShouldMatch = in.readOptionalString();
-        commonTermsQueryBuilder.highFreqMinimumShouldMatch = in.readOptionalString();
-        commonTermsQueryBuilder.disableCoord = in.readBoolean();
-        commonTermsQueryBuilder.cutoffFrequency = in.readFloat();
-        return commonTermsQueryBuilder;
-    }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.text);
-        highFreqOperator.writeTo(out);
-        lowFreqOperator.writeTo(out);
-        out.writeOptionalString(analyzer);
-        out.writeOptionalString(lowFreqMinimumShouldMatch);
-        out.writeOptionalString(highFreqMinimumShouldMatch);
-        out.writeBoolean(disableCoord);
-        out.writeFloat(cutoffFrequency);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, text, highFreqOperator, lowFreqOperator, analyzer,
-                lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch, disableCoord, cutoffFrequency);
-    }
-
-    @Override
-    protected boolean doEquals(CommonTermsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(text, other.text) &&
-                Objects.equals(highFreqOperator, other.highFreqOperator) &&
-                Objects.equals(lowFreqOperator, other.lowFreqOperator) &&
-                Objects.equals(analyzer, other.analyzer) &&
-                Objects.equals(lowFreqMinimumShouldMatch, other.lowFreqMinimumShouldMatch) &&
-                Objects.equals(highFreqMinimumShouldMatch, other.highFreqMinimumShouldMatch) &&
-                Objects.equals(disableCoord, other.disableCoord) &&
-                Objects.equals(cutoffFrequency, other.cutoffFrequency);
+        builder.endObject();
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
index 65f4fa3..c18229e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java
@@ -19,15 +19,36 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.ExtendedCommonTermsQuery;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRefBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for common terms query
+ *
  */
-public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuilder> {
+public class CommonTermsQueryParser implements QueryParser {
+
+    public static final String NAME = "common";
+
+    static final float DEFAULT_MAX_TERM_DOC_FREQ = 0.01f;
+
+    static final Occur DEFAULT_HIGH_FREQ_OCCUR = Occur.SHOULD;
+
+    static final Occur DEFAULT_LOW_FREQ_OCCUR = Occur.SHOULD;
+
+    static final boolean DEFAULT_DISABLE_COORD = true;
+
 
     @Inject
     public CommonTermsQueryParser() {
@@ -35,26 +56,26 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
 
     @Override
     public String[] names() {
-        return new String[] { CommonTermsQueryBuilder.NAME };
+        return new String[] { NAME };
     }
 
     @Override
-    public CommonTermsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new QueryParsingException(parseContext, "[common] query malformed, no field");
         }
         String fieldName = parser.currentName();
-        Object text = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String analyzer = null;
+        Object value = null;
+        float boost = 1.0f;
+        String queryAnalyzer = null;
         String lowFreqMinimumShouldMatch = null;
         String highFreqMinimumShouldMatch = null;
-        boolean disableCoord = CommonTermsQueryBuilder.DEFAULT_DISABLE_COORD;
-        Operator highFreqOperator = CommonTermsQueryBuilder.DEFAULT_HIGH_FREQ_OCCUR;
-        Operator lowFreqOperator = CommonTermsQueryBuilder.DEFAULT_LOW_FREQ_OCCUR;
-        float cutoffFrequency = CommonTermsQueryBuilder.DEFAULT_CUTOFF_FREQ;
+        boolean disableCoord = DEFAULT_DISABLE_COORD;
+        Occur highFreqOccur = DEFAULT_HIGH_FREQ_OCCUR;
+        Occur lowFreqOccur = DEFAULT_LOW_FREQ_OCCUR;
+        float maxTermFrequency = DEFAULT_MAX_TERM_DOC_FREQ;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -84,21 +105,41 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
                     }
                 } else if (token.isValue()) {
                     if ("query".equals(currentFieldName)) {
-                        text = parser.objectText();
+                        value = parser.objectText();
                     } else if ("analyzer".equals(currentFieldName)) {
-                        analyzer = parser.text();
+                        String analyzer = parser.text();
+                        if (parseContext.analysisService().analyzer(analyzer) == null) {
+                            throw new QueryParsingException(parseContext, "[common] analyzer [" + parser.text() + "] not found");
+                        }
+                        queryAnalyzer = analyzer;
                     } else if ("disable_coord".equals(currentFieldName) || "disableCoord".equals(currentFieldName)) {
                         disableCoord = parser.booleanValue();
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("high_freq_operator".equals(currentFieldName) || "highFreqOperator".equals(currentFieldName)) {
-                        highFreqOperator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            highFreqOccur = BooleanClause.Occur.SHOULD;
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            highFreqOccur = BooleanClause.Occur.MUST;
+                        } else {
+                            throw new QueryParsingException(parseContext,
+                                    "[common] query requires operator to be either 'and' or 'or', not [" + op + "]");
+                        }
                     } else if ("low_freq_operator".equals(currentFieldName) || "lowFreqOperator".equals(currentFieldName)) {
-                        lowFreqOperator = Operator.fromString(parser.text());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            lowFreqOccur = BooleanClause.Occur.SHOULD;
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            lowFreqOccur = BooleanClause.Occur.MUST;
+                        } else {
+                            throw new QueryParsingException(parseContext,
+                                    "[common] query requires operator to be either 'and' or 'or', not [" + op + "]");
+                        }
                     } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                         lowFreqMinimumShouldMatch = parser.text();
                     } else if ("cutoff_frequency".equals(currentFieldName)) {
-                        cutoffFrequency = parser.floatValue();
+                        maxTermFrequency = parser.floatValue();
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -108,7 +149,7 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
             }
             parser.nextToken();
         } else {
-            text = parser.objectText();
+            value = parser.objectText();
             // move to the next token
             token = parser.nextToken();
             if (token != XContentParser.Token.END_OBJECT) {
@@ -118,23 +159,66 @@ public class CommonTermsQueryParser extends BaseQueryParser<CommonTermsQueryBuil
             }
         }
 
-        if (text == null) {
+        if (value == null) {
             throw new QueryParsingException(parseContext, "No text specified for text query");
         }
-        return new CommonTermsQueryBuilder(fieldName, text)
-                .lowFreqMinimumShouldMatch(lowFreqMinimumShouldMatch)
-                .highFreqMinimumShouldMatch(highFreqMinimumShouldMatch)
-                .analyzer(analyzer)
-                .highFreqOperator(highFreqOperator)
-                .lowFreqOperator(lowFreqOperator)
-                .disableCoord(disableCoord)
-                .cutoffFrequency(cutoffFrequency)
-                .boost(boost)
-                .queryName(queryName);
+        String field;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            field = fieldType.names().indexName();
+        } else {
+            field = fieldName;
+        }
+
+        Analyzer analyzer = null;
+        if (queryAnalyzer == null) {
+            if (fieldType != null) {
+                analyzer = fieldType.searchAnalyzer();
+            }
+            if (analyzer == null && fieldType != null) {
+                analyzer = parseContext.getSearchAnalyzer(fieldType);
+            }
+            if (analyzer == null) {
+                analyzer = parseContext.mapperService().searchAnalyzer();
+            }
+        } else {
+            analyzer = parseContext.mapperService().analysisService().analyzer(queryAnalyzer);
+            if (analyzer == null) {
+                throw new IllegalArgumentException("No analyzer found for [" + queryAnalyzer + "]");
+            }
+        }
+
+        ExtendedCommonTermsQuery commonsQuery = new ExtendedCommonTermsQuery(highFreqOccur, lowFreqOccur, maxTermFrequency, disableCoord, fieldType);
+        commonsQuery.setBoost(boost);
+        Query query = parseQueryString(commonsQuery, value.toString(), field, parseContext, analyzer, lowFreqMinimumShouldMatch, highFreqMinimumShouldMatch);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public CommonTermsQueryBuilder getBuilderPrototype() {
-        return CommonTermsQueryBuilder.PROTOTYPE;
+
+    private final Query parseQueryString(ExtendedCommonTermsQuery query, String queryString, String field, QueryParseContext parseContext,
+            Analyzer analyzer, String lowFreqMinimumShouldMatch, String highFreqMinimumShouldMatch) throws IOException {
+        // Logic similar to QueryParser#getFieldQuery
+        int count = 0;
+        try (TokenStream source = analyzer.tokenStream(field, queryString.toString())) {
+            source.reset();
+            CharTermAttribute termAtt = source.addAttribute(CharTermAttribute.class);
+            BytesRefBuilder builder = new BytesRefBuilder();
+            while (source.incrementToken()) {
+                // UTF-8
+                builder.copyChars(termAtt);
+                query.add(new Term(field, builder.toBytesRef()));
+                count++;
+            }
+        }
+
+        if (count == 0) {
+            return null;
+        }
+        query.setLowFreqMinimumNumberShouldMatch(lowFreqMinimumShouldMatch);
+        query.setHighFreqMinimumNumberShouldMatch(highFreqMinimumShouldMatch);
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
index 10b14e0..bdcbe9c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -32,84 +28,41 @@ import java.util.Objects;
  * A query that wraps a filter and simply returns a constant score equal to the
  * query boost for every document in the filter.
  */
-public class ConstantScoreQueryBuilder extends AbstractQueryBuilder<ConstantScoreQueryBuilder> {
-
-    public static final String NAME = "constant_score";
+public class ConstantScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<ConstantScoreQueryBuilder> {
 
     private final QueryBuilder filterBuilder;
 
-    static final ConstantScoreQueryBuilder PROTOTYPE = new ConstantScoreQueryBuilder(null);
+    private float boost = -1;
 
     /**
-     * A query that wraps another query and simply returns a constant score equal to the
+     * A query that wraps a query and simply returns a constant score equal to the
      * query boost for every document in the query.
      *
      * @param filterBuilder The query to wrap in a constant score query
      */
     public ConstantScoreQueryBuilder(QueryBuilder filterBuilder) {
-        this.filterBuilder = filterBuilder;
+        this.filterBuilder = Objects.requireNonNull(filterBuilder);
     }
 
     /**
-     * @return the query that was wrapped in this constant score query
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public QueryBuilder innerQuery() {
-        return this.filterBuilder;
+    @Override
+    public ConstantScoreQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(ConstantScoreQueryParser.NAME);
         builder.field("filter");
         filterBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerFilter = filterBuilder.toFilter(context);
-        if (innerFilter == null ) {
-            // return null so that parent queries (e.g. bool) also ignore this
-            return null;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return new ConstantScoreQuery(innerFilter);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (filterBuilder == null) {
-            validationException = addValidationError("inner clause [filter] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(filterBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filterBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(ConstantScoreQueryBuilder other) {
-        return Objects.equals(filterBuilder, other.filterBuilder);
-    }
-
-    @Override
-    protected ConstantScoreQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerFilterBuilder = in.readQuery();
-        return new ConstantScoreQueryBuilder(innerFilterBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(filterBuilder);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
index ba261e8..d8a34b9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
@@ -27,10 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for constant_score query
+ *
  */
-public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQueryBuilder> {
+public class ConstantScoreQueryParser implements QueryParser {
 
+    public static final String NAME = "constant_score";
     private static final ParseField INNER_QUERY_FIELD = new ParseField("filter", "query");
 
     @Inject
@@ -39,17 +42,16 @@ public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQuery
 
     @Override
     public String[] names() {
-        return new String[]{ConstantScoreQueryBuilder.NAME, Strings.toCamelCase(ConstantScoreQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public ConstantScoreQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
+        Query filter = null;
         boolean queryFound = false;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -60,15 +62,13 @@ public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQuery
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, INNER_QUERY_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    filter = parseContext.parseInnerFilter();
                     queryFound = true;
                 } else {
                     throw new QueryParsingException(parseContext, "[constant_score] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[constant_score] query does not support [" + currentFieldName + "]");
@@ -79,14 +79,12 @@ public class ConstantScoreQueryParser extends BaseQueryParser<ConstantScoreQuery
             throw new QueryParsingException(parseContext, "[constant_score] requires a 'filter' element");
         }
 
-        ConstantScoreQueryBuilder constantScoreBuilder = new ConstantScoreQueryBuilder(query);
-        constantScoreBuilder.boost(boost);
-        constantScoreBuilder.queryName(queryName);
-        return constantScoreBuilder;
-    }
+        if (filter == null) {
+            return null;
+        }
 
-    @Override
-    public ConstantScoreQueryBuilder getBuilderPrototype() {
-        return ConstantScoreQueryBuilder.PROTOTYPE;
+        filter = new ConstantScoreQuery(filter);
+        filter.setBoost(boost);
+        return filter;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
index 9b43de6..ddf9d95 100644
--- a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java
@@ -19,34 +19,27 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Objects;
+
+import static com.google.common.collect.Lists.newArrayList;
 
 /**
  * A query that generates the union of documents produced by its sub-queries, and that scores each document
  * with the maximum score for that document as produced by any sub-query, plus a tie breaking increment for any
  * additional matching sub-queries.
  */
-public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder> {
+public class DisMaxQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<DisMaxQueryBuilder> {
 
-    public static final String NAME = "dis_max";
+    private ArrayList<QueryBuilder> queries = newArrayList();
 
-    private final ArrayList<QueryBuilder> queries = new ArrayList<>();
+    private float boost = -1;
 
-    /** Default multiplication factor for breaking ties in document scores.*/
-    public static float DEFAULT_TIE_BREAKER = 0.0f;
-    private float tieBreaker = DEFAULT_TIE_BREAKER;
+    private float tieBreaker = -1;
 
-    static final DisMaxQueryBuilder PROTOTYPE = new DisMaxQueryBuilder();
+    private String queryName;
 
     /**
      * Add a sub-query to this disjunction.
@@ -57,10 +50,13 @@ public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder>
     }
 
     /**
-     * @return an immutable list copy of the current sub-queries of this disjunction
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public List<QueryBuilder> innerQueries() {
-        return this.queries;
+    @Override
+    public DisMaxQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
@@ -75,70 +71,30 @@ public class DisMaxQueryBuilder extends AbstractQueryBuilder<DisMaxQueryBuilder>
     }
 
     /**
-     * @return the tie breaker score
-     * @see DisMaxQueryBuilder#tieBreaker(float)
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public float tieBreaker() {
-        return this.tieBreaker;
+    public DisMaxQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("tie_breaker", tieBreaker);
+        builder.startObject(DisMaxQueryParser.NAME);
+        if (tieBreaker != -1) {
+            builder.field("tie_breaker", tieBreaker);
+        }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.startArray("queries");
         for (QueryBuilder queryBuilder : queries) {
             queryBuilder.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // return null if there are no queries at all
-        Collection<Query> luceneQueries = toQueries(queries, context);
-        if (luceneQueries.isEmpty()) {
-            return null;
-        }
-
-        return new DisjunctionMaxQuery(luceneQueries, tieBreaker);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(queries, null);
-    }
-
-    @Override
-    protected DisMaxQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        DisMaxQueryBuilder disMax = new DisMaxQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        disMax.queries.addAll(queryBuilders);
-        disMax.tieBreaker = in.readFloat();
-        return disMax;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, queries);
-        out.writeFloat(tieBreaker);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queries, tieBreaker);
-    }
-
-    @Override
-    protected boolean doEquals(DisMaxQueryBuilder other) {
-        return Objects.equals(queries, other.queries) &&
-               Objects.equals(tieBreaker, other.tieBreaker);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
index 704328f..2747387 100644
--- a/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -29,9 +31,11 @@ import java.util.List;
 import static com.google.common.collect.Lists.newArrayList;
 
 /**
- * Parser for dis_max query
+ *
  */
-public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
+public class DisMaxQueryParser implements QueryParser {
+
+    public static final String NAME = "dis_max";
 
     @Inject
     public DisMaxQueryParser() {
@@ -39,17 +43,17 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{DisMaxQueryBuilder.NAME, Strings.toCamelCase(DisMaxQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public DisMaxQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        float tieBreaker = DisMaxQueryBuilder.DEFAULT_TIE_BREAKER;
+        float boost = 1.0f;
+        float tieBreaker = 0.0f;
 
-        final List<QueryBuilder> queries = newArrayList();
+        List<Query> queries = newArrayList();
         boolean queriesFound = false;
         String queryName = null;
 
@@ -61,8 +65,10 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("queries".equals(currentFieldName)) {
                     queriesFound = true;
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    queries.add(query);
+                    Query query = parseContext.parseInnerQuery();
+                    if (query != null) {
+                        queries.add(query);
+                    }
                 } else {
                     throw new QueryParsingException(parseContext, "[dis_max] query does not support [" + currentFieldName + "]");
                 }
@@ -70,8 +76,10 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
                 if ("queries".equals(currentFieldName)) {
                     queriesFound = true;
                     while (token != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        queries.add(query);
+                        Query query = parseContext.parseInnerQuery();
+                        if (query != null) {
+                            queries.add(query);
+                        }
                         token = parser.nextToken();
                     }
                 } else {
@@ -94,18 +102,15 @@ public class DisMaxQueryParser extends BaseQueryParser<DisMaxQueryBuilder> {
             throw new QueryParsingException(parseContext, "[dis_max] requires 'queries' field");
         }
 
-        DisMaxQueryBuilder disMaxQuery = new DisMaxQueryBuilder();
-        disMaxQuery.tieBreaker(tieBreaker);
-        disMaxQuery.queryName(queryName);
-        disMaxQuery.boost(boost);
-        for (QueryBuilder query : queries) {
-            disMaxQuery.add(query);
+        if (queries.isEmpty()) {
+            return null;
         }
-        return disMaxQuery;
-    }
 
-    @Override
-    public DisMaxQueryBuilder getBuilderPrototype() {
-        return DisMaxQueryBuilder.PROTOTYPE;
+        DisjunctionMaxQuery query = new DisjunctionMaxQuery(queries, tieBreaker);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java
deleted file mode 100644
index c59d8d3..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/EmptyQueryBuilder.java
+++ /dev/null
@@ -1,118 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.action.support.ToXContentToBytes;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentType;
-
-import java.io.IOException;
-
-/**
- * A {@link QueryBuilder} that is a stand in replacement for an empty query clause in the DSL.
- * The current DSL allows parsing inner queries / filters like "{ }", in order to have a
- * valid non-null representation of these clauses that actually do nothing we can use this class.
- *
- * This builder has no corresponding parser and it is not registered under the query name. It is
- * intended to be used internally as a stand-in for nested queries that are left empty and should
- * be ignored upstream.
- */
-public class EmptyQueryBuilder extends ToXContentToBytes implements QueryBuilder<EmptyQueryBuilder> {
-
-    public static final String NAME = "empty_query";
-
-    /** the one and only empty query builder */
-    public static final EmptyQueryBuilder PROTOTYPE = new EmptyQueryBuilder();
-
-    // prevent instances other than prototype
-    private EmptyQueryBuilder() {
-        super(XContentType.JSON);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public String getName() {
-        return getWriteableName();
-    }
-
-    @Override
-    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject();
-        builder.endObject();
-        return builder;
-    }
-
-    @Override
-    public Query toQuery(QueryShardContext context) throws IOException {
-        // empty
-        return null;
-    }
-
-    @Override
-    public Query toFilter(QueryShardContext context) throws IOException {
-        // empty
-        return null;
-    }
-
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-    }
-
-    @Override
-    public EmptyQueryBuilder readFrom(StreamInput in) throws IOException {
-        return EmptyQueryBuilder.PROTOTYPE;
-    }
-
-    @Override
-    public EmptyQueryBuilder queryName(String queryName) {
-        //no-op
-        return this;
-    }
-
-    @Override
-    public String queryName() {
-        return null;
-    }
-
-    @Override
-    public float boost() {
-        return -1;
-    }
-
-    @Override
-    public EmptyQueryBuilder boost(float boost) {
-        //no-op
-        return this;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
index 6808793..9980d81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryBuilder.java
@@ -19,126 +19,38 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
 
 /**
  * Constructs a query that only match on documents that the field has a value in them.
  */
-public class ExistsQueryBuilder extends AbstractQueryBuilder<ExistsQueryBuilder> {
+public class ExistsQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "exists";
+    private String name;
 
-    private final String fieldName;
+    private String queryName;
 
-    static final ExistsQueryBuilder PROTOTYPE = new ExistsQueryBuilder(null);
-
-    public ExistsQueryBuilder(String fieldName) {
-        this.fieldName = fieldName;
+    public ExistsQueryBuilder(String name) {
+        this.name = name;
     }
 
     /**
-     * @return the field name that has to exist for this query to match
+     * Sets the query name for the query that can be used when searching for matched_queries per hit.
      */
-    public String fieldName() {
-        return this.fieldName;
+    public ExistsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("field", fieldName);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, fieldName);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern) {
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)context.mapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            // no fields exists, so we should not match anything
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        BooleanQuery boolFilter = new BooleanQuery();
-        for (String field : fields) {
-            MappedFieldType fieldType = context.fieldMapper(field);
-            Query filter = null;
-            if (fieldNamesFieldType.isEnabled()) {
-                final String f;
-                if (fieldType != null) {
-                    f = fieldType.names().indexName();
-                } else {
-                    f = field;
-                }
-                filter = fieldNamesFieldType.termQuery(f, context);
-            }
-            // if _field_names are not indexed, we need to go the slow way
-            if (filter == null && fieldType != null) {
-                filter = fieldType.rangeQuery(null, null, true, true);
-            }
-            if (filter == null) {
-                filter = new TermRangeQuery(field, null, null, true, true);
-            }
-            boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+        builder.startObject(ExistsQueryParser.NAME);
+        builder.field("field", name);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return new ConstantScoreQuery(boolFilter);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName);
-    }
-
-    @Override
-    protected boolean doEquals(ExistsQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName);
-    }
-
-    @Override
-    protected ExistsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new ExistsQueryBuilder(in.readString());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
index bd584bc..0ce578c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ExistsQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.*;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
+import java.util.Collection;
 
 /**
- * Parser for exists query
+ *
  */
-public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
+public class ExistsQueryParser implements QueryParser {
+
+    public static final String NAME = "exists";
 
     @Inject
     public ExistsQueryParser() {
@@ -35,16 +43,15 @@ public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{ExistsQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public ExistsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldPattern = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
         XContentParser.Token token;
         String currentFieldName = null;
@@ -56,8 +63,6 @@ public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
                     fieldPattern = parser.text();
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[exists] query does not support [" + currentFieldName + "]");
                 }
@@ -68,14 +73,55 @@ public class ExistsQueryParser extends BaseQueryParser<ExistsQueryBuilder> {
             throw new QueryParsingException(parseContext, "exists must be provided with a [field]");
         }
 
-        ExistsQueryBuilder builder = new ExistsQueryBuilder(fieldPattern);
-        builder.queryName(queryName);
-        builder.boost(boost);
-        return builder;
+        return newFilter(parseContext, fieldPattern, queryName);
     }
 
-    @Override
-    public ExistsQueryBuilder getBuilderPrototype() {
-        return ExistsQueryBuilder.PROTOTYPE;
+    public static Query newFilter(QueryParseContext parseContext, String fieldPattern, String queryName) {
+        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)parseContext.mapperService().fullName(FieldNamesFieldMapper.NAME);
+        if (fieldNamesFieldType == null) {
+            // can only happen when no types exist, so no docs exist either
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        ObjectMapper objectMapper = parseContext.getObjectMapper(fieldPattern);
+        if (objectMapper != null) {
+            // automatic make the object mapper pattern
+            fieldPattern = fieldPattern + ".*";
+        }
+
+        Collection<String> fields = parseContext.simpleMatchToIndexNames(fieldPattern);
+        if (fields.isEmpty()) {
+            // no fields exists, so we should not match anything
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        BooleanQuery boolFilter = new BooleanQuery();
+        for (String field : fields) {
+            MappedFieldType fieldType = parseContext.fieldMapper(field);
+            Query filter = null;
+            if (fieldNamesFieldType.isEnabled()) {
+                final String f;
+                if (fieldType != null) {
+                    f = fieldType.names().indexName();
+                } else {
+                    f = field;
+                }
+                filter = fieldNamesFieldType.termQuery(f, parseContext);
+            }
+            // if _field_names are not indexed, we need to go the slow way
+            if (filter == null && fieldType != null) {
+                filter = fieldType.rangeQuery(null, null, true, true);
+            }
+            if (filter == null) {
+                filter = new TermRangeQuery(field, null, null, true, true);
+            }
+            boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+        }
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, boolFilter);
+        }
+        return new ConstantScoreQuery(boolFilter);
     }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java
deleted file mode 100644
index 85fdad1..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterBuilder.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-import java.util.Objects;
-
-/**
- * A filter that simply wraps a query. Same as the {@link QueryFilterBuilder} except that it allows also to
- * associate a name with the query filter.
- * @deprecated Useless now that queries and filters are merged: pass the
- *             query as a filter directly.
- */
-@Deprecated
-public class FQueryFilterBuilder extends AbstractQueryBuilder<FQueryFilterBuilder> {
-
-    public static final String NAME = "fquery";
-
-    static final FQueryFilterBuilder PROTOTYPE = new FQueryFilterBuilder(null);
-
-    private final QueryBuilder queryBuilder;
-
-    /**
-     * A filter that simply wraps a query.
-     *
-     * @param queryBuilder The query to wrap as a filter
-     */
-    public FQueryFilterBuilder(QueryBuilder queryBuilder) {
-        this.queryBuilder = queryBuilder;
-    }
-
-    /**
-     * @return the query builder that is wrapped by this {@link FQueryFilterBuilder}
-     */
-    public QueryBuilder innerQuery() {
-        return this.queryBuilder;
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(FQueryFilterBuilder.NAME);
-        builder.field("query");
-        queryBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerQuery = this.queryBuilder.toQuery(context);
-        if (innerQuery == null) {
-            return null;
-        }
-        return new ConstantScoreQuery(innerQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(queryBuilder, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(FQueryFilterBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder);
-    }
-
-    @Override
-    protected FQueryFilterBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        FQueryFilterBuilder fquery = new FQueryFilterBuilder(innerQueryBuilder);
-        return fquery;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
index 46ab50f..4c0f782 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java
@@ -19,6 +19,8 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -27,11 +29,11 @@ import java.io.IOException;
 /**
  * The "fquery" filter is the same as the {@link QueryFilterParser} except that it allows also to
  * associate a name with the query filter.
- * @deprecated Useless now that queries and filters are merged: pass the
- *             query as a filter directly.
  */
 @Deprecated
-public class FQueryFilterParser extends BaseQueryParser<FQueryFilterBuilder> {
+public class FQueryFilterParser implements QueryParser {
+
+    public static final String NAME = "fquery";
 
     @Inject
     public FQueryFilterParser() {
@@ -39,17 +41,16 @@ public class FQueryFilterParser extends BaseQueryParser<FQueryFilterBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{FQueryFilterBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public FQueryFilterBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder wrappedQuery = null;
+        Query query = null;
         boolean queryFound = false;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -61,15 +62,13 @@ public class FQueryFilterParser extends BaseQueryParser<FQueryFilterBuilder> {
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
                     queryFound = true;
-                    wrappedQuery = parseContext.parseInnerQueryBuilder();
+                    query = parseContext.parseInnerQuery();
                 } else {
                     throw new QueryParsingException(parseContext, "[fquery] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[fquery] query does not support [" + currentFieldName + "]");
                 }
@@ -78,14 +77,13 @@ public class FQueryFilterParser extends BaseQueryParser<FQueryFilterBuilder> {
         if (!queryFound) {
             throw new QueryParsingException(parseContext, "[fquery] requires 'query' element");
         }
-        FQueryFilterBuilder queryBuilder = new FQueryFilterBuilder(wrappedQuery);
-        queryBuilder.queryName(queryName);
-        queryBuilder.boost(boost);
-        return queryBuilder;
-    }
-
-    @Override
-    public FQueryFilterBuilder getBuilderPrototype() {
-        return FQueryFilterBuilder.PROTOTYPE;
+        if (query == null) {
+            return null;
+        }
+        query = new ConstantScoreQuery(query);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
index a577225..c118416 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java
@@ -19,113 +19,52 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class FieldMaskingSpanQueryBuilder extends AbstractQueryBuilder<FieldMaskingSpanQueryBuilder> implements SpanQueryBuilder<FieldMaskingSpanQueryBuilder>{
-
-    public static final String NAME = "field_masking_span";
+public class FieldMaskingSpanQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<FieldMaskingSpanQueryBuilder> {
 
     private final SpanQueryBuilder queryBuilder;
 
-    private final String fieldName;
+    private final String field;
 
-    static final FieldMaskingSpanQueryBuilder PROTOTYPE = new FieldMaskingSpanQueryBuilder(null, null);
+    private float boost = -1;
 
-    /**
-     * Constructs a new {@link FieldMaskingSpanQueryBuilder} given an inner {@link SpanQueryBuilder} for
-     * a given field
-     * @param queryBuilder inner {@link SpanQueryBuilder}
-     * @param fieldName the field name
-     */
-    public FieldMaskingSpanQueryBuilder(SpanQueryBuilder queryBuilder, String fieldName) {
+    private String queryName;
+
+
+    public FieldMaskingSpanQueryBuilder(SpanQueryBuilder queryBuilder, String field) {
         this.queryBuilder = queryBuilder;
-        this.fieldName = fieldName;
+        this.field = field;
     }
 
-    /**
-     * @return the field name for this query
-     */
-    public String fieldName() {
-        return this.fieldName;
+    @Override
+    public FieldMaskingSpanQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the inner {@link QueryBuilder}
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder innerQuery() {
-        return this.queryBuilder;
+    public FieldMaskingSpanQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(FieldMaskingSpanQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
-        builder.field("field", fieldName);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected SpanQuery doToQuery(QueryShardContext context) throws IOException {
-        String fieldInQuery = fieldName;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            fieldInQuery = fieldType.names().indexName();
-        }
-        Query innerQuery = queryBuilder.toQuery(context);
-        assert innerQuery instanceof SpanQuery;
-        return new FieldMaskingSpanQuery((SpanQuery)innerQuery, fieldInQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (queryBuilder == null) {
-            validationException = addValidationError("inner clause [query] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(queryBuilder, validationException);
+        builder.field("field", field);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (fieldName == null || fieldName.isEmpty()) {
-            validationException = addValidationError("field name is null or empty", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected FieldMaskingSpanQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        return new FieldMaskingSpanQueryBuilder((SpanQueryBuilder) innerQueryBuilder, in.readString());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-        out.writeString(fieldName);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder, fieldName);
-    }
-
-    @Override
-    protected boolean doEquals(FieldMaskingSpanQueryBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder) &&
-               Objects.equals(fieldName, other.fieldName);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
index ad77039..2980be1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.MappedFieldType;
+
 import java.io.IOException;
 
 /**
- * Parser for field_masking_span query
+ *
  */
-public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpanQueryBuilder> {
+public class FieldMaskingSpanQueryParser implements QueryParser {
+
+    public static final String NAME = "field_masking_span";
 
     @Inject
     public FieldMaskingSpanQueryParser() {
@@ -35,16 +43,16 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpa
 
     @Override
     public String[] names() {
-        return new String[]{FieldMaskingSpanQueryBuilder.NAME, Strings.toCamelCase(FieldMaskingSpanQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public FieldMaskingSpanQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder inner = null;
+        SpanQuery inner = null;
         String field = null;
         String queryName = null;
 
@@ -55,11 +63,11 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpa
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
-                        throw new QueryParsingException(parseContext, "[field_masking_span] query must be of type span query");
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
+                        throw new QueryParsingException(parseContext, "[field_masking_span] query] must be of type span query");
                     }
-                    inner = (SpanQueryBuilder) query;
+                    inner = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[field_masking_span] query does not support ["
                             + currentFieldName + "]");
@@ -83,14 +91,16 @@ public class FieldMaskingSpanQueryParser extends BaseQueryParser<FieldMaskingSpa
             throw new QueryParsingException(parseContext, "field_masking_span must have [field] set for it");
         }
 
-        FieldMaskingSpanQueryBuilder queryBuilder = new FieldMaskingSpanQueryBuilder(inner, field);
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
+        MappedFieldType fieldType = parseContext.fieldMapper(field);
+        if (fieldType != null) {
+            field = fieldType.names().indexName();
+        }
 
-    @Override
-    public FieldMaskingSpanQueryBuilder getBuilderPrototype() {
-        return FieldMaskingSpanQueryBuilder.PROTOTYPE;
+        FieldMaskingSpanQuery query = new FieldMaskingSpanQuery(inner, field);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
index 471f138..93507cf 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java
@@ -19,140 +19,72 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A query that applies a filter to the results of another query.
  * @deprecated Use {@link BoolQueryBuilder} instead.
  */
 @Deprecated
-public class FilteredQueryBuilder extends AbstractQueryBuilder<FilteredQueryBuilder> {
+public class FilteredQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<FilteredQueryBuilder> {
 
-    /** Name of the query in the REST API. */
-    public static final String NAME = "filtered";
-    /** The query to filter. */
     private final QueryBuilder queryBuilder;
-    /** The filter to apply to the query. */
+
     private final QueryBuilder filterBuilder;
 
-    static final FilteredQueryBuilder PROTOTYPE = new FilteredQueryBuilder(null, null);
+    private float boost = -1;
 
-    /**
-     * Returns a {@link MatchAllQueryBuilder} instance that will be used as
-     * default queryBuilder if none is supplied by the user. Feel free to
-     * set queryName and boost on that instance - it's always a new one.
-     * */
-    private static QueryBuilder generateDefaultQuery() {
-        return new MatchAllQueryBuilder();
-    }
-
-    /**
-     * A query that applies a filter to the results of a match_all query.
-     * @param filterBuilder The filter to apply on the query (Can be null)
-     * */
-    public FilteredQueryBuilder(QueryBuilder filterBuilder) {
-        this(generateDefaultQuery(), filterBuilder);
-    }
+    private String queryName;
 
     /**
      * A query that applies a filter to the results of another query.
      *
-     * @param queryBuilder  The query to apply the filter to
+     * @param queryBuilder  The query to apply the filter to (Can be null)
      * @param filterBuilder The filter to apply on the query (Can be null)
      */
-    public FilteredQueryBuilder(QueryBuilder queryBuilder, QueryBuilder filterBuilder) {
-        this.queryBuilder = (queryBuilder != null) ? queryBuilder : generateDefaultQuery();
-        this.filterBuilder = (filterBuilder != null) ? filterBuilder : EmptyQueryBuilder.PROTOTYPE;
-    }
-
-    /** Returns the query to apply the filter to. */
-    public QueryBuilder innerQuery() {
-        return queryBuilder;
-    }
-
-    /** Returns the filter to apply to the query results. */
-    public QueryBuilder innerFilter() {
-        return filterBuilder;
+    public FilteredQueryBuilder(@Nullable QueryBuilder queryBuilder, @Nullable QueryBuilder filterBuilder) {
+        this.queryBuilder = queryBuilder;
+        this.filterBuilder = filterBuilder;
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected boolean doEquals(FilteredQueryBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder) &&
-                Objects.equals(filterBuilder, other.filterBuilder);
+    public FilteredQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    public int doHashCode() {
-        return Objects.hash(queryBuilder, filterBuilder);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public FilteredQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public Query doToQuery(QueryShardContext context) throws QueryShardException, IOException {
-        Query query = queryBuilder.toQuery(context);
-        Query filter = filterBuilder.toFilter(context);
-
-        if (query == null) {
-            // Most likely this query was generated from the JSON query DSL - it parsed to an EmptyQueryBuilder so we ignore
-            // the whole filtered query as there is nothing to filter on. See FilteredQueryParser for an example.
-            return null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(FilteredQueryParser.NAME);
+        if (queryBuilder != null) {
+            builder.field("query");
+            queryBuilder.toXContent(builder, params);
         }
-
-        if (filter == null || Queries.isConstantMatchAllQuery(filter)) {
-            // no filter, or match all filter
-            return query;
-        } else if (Queries.isConstantMatchAllQuery(query)) {
-            // if its a match_all query, use constant_score
-            return new ConstantScoreQuery(filter);
+        if (filterBuilder != null) {
+            builder.field("filter");
+            filterBuilder.toXContent(builder, params);
+        }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        // use a BooleanQuery
-        return Queries.filtered(query, filter);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        validationException = validateInnerQuery(queryBuilder, validationException);
-        validationException = validateInnerQuery(filterBuilder, validationException);
-        return validationException;
-
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("query");
-        queryBuilder.toXContent(builder, params);
-        builder.field("filter");
-        filterBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public FilteredQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder query = in.readQuery();
-        QueryBuilder filter = in.readQuery();
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(query, filter);
-        return qb;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-        out.writeQuery(filterBuilder);
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
index 6b94d6d..774ff74 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java
@@ -19,17 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for filtered query.
- * @deprecated Use {@link BoolQueryParser} instead.
+ *
  */
 @Deprecated
-public class FilteredQueryParser extends BaseQueryParser<FilteredQueryBuilder> {
+public class FilteredQueryParser implements QueryParser {
+
+    public static final String NAME = "filtered";
 
     @Inject
     public FilteredQueryParser() {
@@ -37,16 +43,17 @@ public class FilteredQueryParser extends BaseQueryParser<FilteredQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{FilteredQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public FilteredQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
-        QueryBuilder filter = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        Query query = Queries.newMatchAllQuery();
+        Query filter = null;
+        boolean filterFound = false;
+        float boost = 1.0f;
         String queryName = null;
 
         String currentFieldName = null;
@@ -59,9 +66,10 @@ public class FilteredQueryParser extends BaseQueryParser<FilteredQueryBuilder> {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(currentFieldName)) {
-                    query = parseContext.parseInnerQueryBuilder();
+                    query = parseContext.parseInnerQuery();
                 } else if ("filter".equals(currentFieldName)) {
-                    filter = parseContext.parseInnerFilterToQueryBuilder();
+                    filterFound = true;
+                    filter = parseContext.parseInnerFilter();
                 } else {
                     throw new QueryParsingException(parseContext, "[filtered] query does not support [" + currentFieldName + "]");
                 }
@@ -78,15 +86,39 @@ public class FilteredQueryParser extends BaseQueryParser<FilteredQueryBuilder> {
             }
         }
 
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(query, filter);
-        qb.boost(boost);
-        qb.queryName(queryName);
-        return qb;
-    }
+        // parsed internally, but returned null during parsing...
+        if (query == null) {
+            return null;
+        }
 
-    @Override
-    public FilteredQueryBuilder getBuilderPrototype() {
-        return FilteredQueryBuilder.PROTOTYPE;
-    }
+        if (filter == null) {
+            if (!filterFound) {
+                // we allow for null filter, so it makes compositions on the client side to be simpler
+                return query;
+            } else {
+                // even if the filter is not found, and its null, we should simply ignore it, and go
+                // by the query
+                return query;
+            }
+        }
+        if (Queries.isConstantMatchAllQuery(filter)) {
+            // this is an instance of match all filter, just execute the query
+            return query;
+        }
+
+        // if its a match_all query, use constant_score
+        if (Queries.isConstantMatchAllQuery(query)) {
+            Query q = new ConstantScoreQuery(filter);
+            q.setBoost(boost);
+            return q;
+        }
 
+        BooleanQuery filteredQuery = Queries.filtered(query, filter);
+
+        filteredQuery.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, filteredQuery);
+        }
+        return filteredQuery;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
index 237b415..23557b1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java
@@ -19,273 +19,177 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that does fuzzy matching for a specific value.
  */
-public class FuzzyQueryBuilder extends AbstractQueryBuilder<FuzzyQueryBuilder> implements MultiTermQueryBuilder<FuzzyQueryBuilder> {
+public class FuzzyQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<FuzzyQueryBuilder> {
 
-    public static final String NAME = "fuzzy";
-
-    /** Default maximum edit distance. Defaults to AUTO. */
-    public static final Fuzziness DEFAULT_FUZZINESS = Fuzziness.AUTO;
-
-    /** Default number of initial characters which will not be “fuzzified”. Defaults to 0. */
-    public static final int DEFAULT_PREFIX_LENGTH = FuzzyQuery.defaultPrefixLength;
-
-    /** Default maximum number of terms that the fuzzy query will expand to. Defaults to 50. */
-    public static final int DEFAULT_MAX_EXPANSIONS = FuzzyQuery.defaultMaxExpansions;
-
-    /** Default as to whether transpositions should be treated as a primitive edit operation, 
-     * instead of classic Levenshtein algorithm. Defaults to false. */
-    public static final boolean DEFAULT_TRANSPOSITIONS = false;
-
-    private final String fieldName;
+    private final String name;
 
     private final Object value;
 
-    private Fuzziness fuzziness = DEFAULT_FUZZINESS;
+    private float boost = -1;
 
-    private int prefixLength = DEFAULT_PREFIX_LENGTH;
+    private Fuzziness fuzziness;
 
-    private int maxExpansions = DEFAULT_MAX_EXPANSIONS;
+    private Integer prefixLength;
 
+    private Integer maxExpansions;
+    
     //LUCENE 4 UPGRADE  we need a testcase for this + documentation
-    private boolean transpositions = DEFAULT_TRANSPOSITIONS;
+    private Boolean transpositions;
 
     private String rewrite;
 
-    static final FuzzyQueryBuilder PROTOTYPE = new FuzzyQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, String value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, int value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, String value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, long value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, int value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, float value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, long value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, double value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, float value) {
+        this(name, (Object) value);
     }
 
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
+     * @param name  The name of the field
      * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, boolean value) {
-        this(fieldName, (Object) value);
+    public FuzzyQueryBuilder(String name, double value) {
+        this(name, (Object) value);
     }
 
+    // NO COMMIT: not sure we should also allow boolean?
     /**
      * Constructs a new fuzzy query.
      *
-     * @param fieldName  The name of the field
-     * @param value The value of the term
+     * @param name  The name of the field
+     * @param value The value of the text
      */
-    public FuzzyQueryBuilder(String fieldName, Object value) {
-        this.fieldName = fieldName;
-        this.value = convertToBytesRefIfString(value);
+    public FuzzyQueryBuilder(String name, boolean value) {
+        this(name, (Object) value);
     }
 
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public Object value() {
-        return convertToStringIfBytesRef(this.value);
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public FuzzyQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public FuzzyQueryBuilder fuzziness(Fuzziness fuzziness) {
-        this.fuzziness = (fuzziness == null) ? DEFAULT_FUZZINESS : fuzziness;
+        this.fuzziness = fuzziness;
         return this;
     }
-    
-    public Fuzziness fuzziness() {
-        return this.fuzziness;
-    }
 
     public FuzzyQueryBuilder prefixLength(int prefixLength) {
         this.prefixLength = prefixLength;
         return this;
     }
-    
-    public int prefixLength() {
-        return this.prefixLength;
-    }
 
     public FuzzyQueryBuilder maxExpansions(int maxExpansions) {
         this.maxExpansions = maxExpansions;
         return this;
     }
-
-    public int maxExpansions() {
-        return this.maxExpansions;
-    }
-
+    
     public FuzzyQueryBuilder transpositions(boolean transpositions) {
       this.transpositions = transpositions;
       return this;
     }
 
-    public boolean transpositions() {
-        return this.transpositions;
-    }
-
     public FuzzyQueryBuilder rewrite(String rewrite) {
         this.rewrite = rewrite;
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public FuzzyQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("value", convertToStringIfBytesRef(this.value));
-        fuzziness.toXContent(builder, params);
-        builder.field("prefix_length", prefixLength);
-        builder.field("max_expansions", maxExpansions);
-        builder.field("transpositions", transpositions);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
+        builder.startObject(FuzzyQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("value", value);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public Query doToQuery(QueryShardContext context) throws QueryParsingException, IOException {
-        Query query = null;
-        if (rewrite == null && context.isFilter()) {
-            rewrite = QueryParsers.CONSTANT_SCORE.getPreferredName();
+        if (transpositions != null) {
+            builder.field("transpositions", transpositions);
         }
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.fuzzyQuery(value, fuzziness, prefixLength, maxExpansions, transpositions);
+        if (fuzziness != null) {
+            fuzziness.toXContent(builder, params);
         }
-        if (query == null) {
-            int maxEdits = fuzziness.asDistance(BytesRefs.toString(value));
-            query = new FuzzyQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), maxEdits, prefixLength, maxExpansions, transpositions);
+        if (prefixLength != null) {
+            builder.field("prefix_length", prefixLength);
         }
-        if (query instanceof MultiTermQuery) {
-            MultiTermQuery.RewriteMethod rewriteMethod = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-            QueryParsers.setRewriteMethod((MultiTermQuery) query, rewriteMethod);
+        if (maxExpansions != null) {
+            builder.field("max_expansions", maxExpansions);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
+        if (rewrite != null) {
+            builder.field("rewrite", rewrite);
         }
-        if (this.value == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    public FuzzyQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        FuzzyQueryBuilder fuzzyQueryBuilder = new FuzzyQueryBuilder(in.readString(), in.readGenericValue());
-        fuzzyQueryBuilder.fuzziness = Fuzziness.readFuzzinessFrom(in);
-        fuzzyQueryBuilder.prefixLength = in.readVInt();
-        fuzzyQueryBuilder.maxExpansions = in.readVInt();
-        fuzzyQueryBuilder.transpositions = in.readBoolean();
-        fuzzyQueryBuilder.rewrite = in.readOptionalString();
-        return fuzzyQueryBuilder;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.value);
-        this.fuzziness.writeTo(out);
-        out.writeVInt(this.prefixLength);
-        out.writeVInt(this.maxExpansions);
-        out.writeBoolean(this.transpositions);
-        out.writeOptionalString(this.rewrite);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(fieldName, value, fuzziness, prefixLength, maxExpansions, transpositions, rewrite);
-    }
-
-    @Override
-    public boolean doEquals(FuzzyQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(fuzziness, other.fuzziness) &&
-                Objects.equals(prefixLength, other.prefixLength) &&
-                Objects.equals(maxExpansions, other.maxExpansions) &&
-                Objects.equals(transpositions, other.transpositions) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
index 694a303..aefdb4c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java
@@ -19,48 +19,60 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
-public class FuzzyQueryParser extends BaseQueryParser {
+/**
+ *
+ */
+public class FuzzyQueryParser implements QueryParser {
 
+    public static final String NAME = "fuzzy";
+    private static final Fuzziness DEFAULT_FUZZINESS = Fuzziness.AUTO;
     private static final ParseField FUZZINESS = Fuzziness.FIELD.withDeprecation("min_similarity");
 
+
     @Inject
     public FuzzyQueryParser() {
     }
 
     @Override
     public String[] names() {
-        return new String[]{ FuzzyQueryBuilder.NAME };
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new QueryParsingException(parseContext, "[fuzzy] query malformed, no field");
         }
-        
         String fieldName = parser.currentName();
-        Object value = null;
-
-        Fuzziness fuzziness = FuzzyQueryBuilder.DEFAULT_FUZZINESS;
-        int prefixLength = FuzzyQueryBuilder.DEFAULT_PREFIX_LENGTH;
-        int maxExpansions = FuzzyQueryBuilder.DEFAULT_MAX_EXPANSIONS;
-        boolean transpositions = FuzzyQueryBuilder.DEFAULT_TRANSPOSITIONS;
-        String rewrite = null;
 
+        Object value = null;
+        float boost = 1.0f;
+        Fuzziness fuzziness = DEFAULT_FUZZINESS;
+        int prefixLength = FuzzyQuery.defaultPrefixLength;
+        int maxExpansions = FuzzyQuery.defaultMaxExpansions;
+        boolean transpositions = FuzzyQuery.defaultTranspositions;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-
+        MultiTermQuery.RewriteMethod rewriteMethod = null;
+        if (parseContext.isFilter()) {
+            rewriteMethod = MultiTermQuery.CONSTANT_SCORE_REWRITE;
+        }
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
             String currentFieldName = null;
@@ -81,9 +93,9 @@ public class FuzzyQueryParser extends BaseQueryParser {
                     } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                         maxExpansions = parser.intValue();
                     } else if ("transpositions".equals(currentFieldName)) {
-                        transpositions = parser.booleanValue();
+                      transpositions = parser.booleanValue();
                     } else if ("rewrite".equals(currentFieldName)) {
-                        rewrite = parser.textOrNull();
+                        rewriteMethod = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), parser.textOrNull(), null);
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -99,20 +111,26 @@ public class FuzzyQueryParser extends BaseQueryParser {
         }
 
         if (value == null) {
-            throw new QueryParsingException(parseContext, "no value specified for fuzzy query");
+            throw new QueryParsingException(parseContext, "No value specified for fuzzy query");
         }
-        return new FuzzyQueryBuilder(fieldName, value)
-                .fuzziness(fuzziness)
-                .prefixLength(prefixLength)
-                .maxExpansions(maxExpansions)
-                .transpositions(transpositions)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
+        
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.fuzzyQuery(value, fuzziness, prefixLength, maxExpansions, transpositions);
+        }
+        if (query == null) {
+            int maxEdits = fuzziness.asDistance(BytesRefs.toString(value));
+            query = new FuzzyQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), maxEdits, prefixLength, maxExpansions, transpositions);
+        }
+        if (query instanceof MultiTermQuery) {
+            QueryParsers.setRewriteMethod((MultiTermQuery) query, rewriteMethod);
+        }
+        query.setBoost(boost);
 
-    @Override
-    public FuzzyQueryBuilder getBuilderPrototype() {
-        return FuzzyQueryBuilder.PROTOTYPE;
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
index 4e61528..9b376ca 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
-public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBoundingBoxQueryBuilder> {
-
-    public static final String NAME = "geo_bbox";
+public class GeoBoundingBoxQueryBuilder extends QueryBuilder {
 
     public static final String TOP_LEFT = GeoBoundingBoxQueryParser.TOP_LEFT;
     public static final String BOTTOM_RIGHT = GeoBoundingBoxQueryParser.BOTTOM_RIGHT;
@@ -36,15 +34,14 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     private static final int LEFT = 1;
     private static final int BOTTOM = 2;
     private static final int RIGHT = 3;
-
+    
     private final String name;
 
     private double[] box = {Double.NaN, Double.NaN, Double.NaN, Double.NaN};
 
+    private String queryName;
     private String type;
 
-    static final GeoBoundingBoxQueryBuilder PROTOTYPE = new GeoBoundingBoxQueryBuilder(null);
-
     public GeoBoundingBoxQueryBuilder(String name) {
         this.name = name;
     }
@@ -108,7 +105,7 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     public GeoBoundingBoxQueryBuilder bottomLeft(String geohash) {
         return bottomLeft(GeoHashUtils.decode(geohash));
     }
-
+    
     /**
      * Adds top right point.
      *
@@ -130,6 +127,14 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
     }
 
     /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoBoundingBoxQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Sets the type of executing of the geo bounding box. Can be either `memory` or `indexed`. Defaults
      * to `memory`.
      */
@@ -150,25 +155,21 @@ public class GeoBoundingBoxQueryBuilder extends AbstractQueryBuilder<GeoBounding
         } else if(Double.isNaN(box[LEFT])) {
             throw new IllegalArgumentException("geo_bounding_box requires left longitude to be set");
         }
-
-        builder.startObject(NAME);
+                
+        builder.startObject(GeoBoundingBoxQueryParser.NAME);
 
         builder.startObject(name);
         builder.array(TOP_LEFT, box[LEFT], box[TOP]);
         builder.array(BOTTOM_RIGHT, box[RIGHT], box[BOTTOM]);
         builder.endObject();
 
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (type != null) {
             builder.field("type", type);
         }
 
-        printBoostAndQueryName(builder);
-
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
index 6ec143d..8901257 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxQueryParser.java
@@ -26,6 +26,7 @@ import org.elasticsearch.common.geo.GeoUtils;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.search.geo.InMemoryGeoBoundingBoxQuery;
@@ -36,7 +37,7 @@ import java.io.IOException;
 /**
  *
  */
-public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
+public class GeoBoundingBoxQueryParser implements QueryParser {
 
     public static final String NAME = "geo_bbox";
 
@@ -63,12 +64,11 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoBoundingBoxQueryBuilder.NAME, "geoBbox", "geo_bounding_box", "geoBoundingBox"};
+        return new String[]{NAME, "geoBbox", "geo_bounding_box", "geoBoundingBox"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -77,15 +77,14 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
         double bottom = Double.NaN;
         double left = Double.NaN;
         double right = Double.NaN;
-
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
         boolean normalize = true;
 
         GeoPoint sparse = new GeoPoint();
-
+        
         String type = "memory";
 
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -138,8 +137,6 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("normalize".equals(currentFieldName)) {
                     normalize = parser.booleanValue();
                 } else if ("type".equals(currentFieldName)) {
@@ -154,7 +151,7 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
         final GeoPoint bottomRight = new GeoPoint(bottom, right);
 
         if (normalize) {
-            // Special case: if the difference bettween the left and right is 360 and the right is greater than the left, we are asking for
+            // Special case: if the difference bettween the left and right is 360 and the right is greater than the left, we are asking for 
             // the complete longitude range so need to set longitude to the complete longditude range
             boolean completeLonRange = ((right - left) % 360 == 0 && right > left);
             GeoUtils.normalizePoint(topLeft, true, !completeLonRange);
@@ -165,7 +162,7 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
             }
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to parse [{}] query. could not find [{}] field [{}]", NAME, GeoPointFieldMapper.CONTENT_TYPE, fieldName);
         }
@@ -178,22 +175,15 @@ public class GeoBoundingBoxQueryParser extends BaseQueryParserTemp {
         if ("indexed".equals(type)) {
             filter = IndexedGeoBoundingBoxQuery.create(topLeft, bottomRight, geoFieldType);
         } else if ("memory".equals(type)) {
-            IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+            IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
             filter = new InMemoryGeoBoundingBoxQuery(topLeft, bottomRight, indexFieldData);
         } else {
             throw new QueryParsingException(parseContext, "failed to parse [{}] query. geo bounding box type [{}] is not supported. either [indexed] or [memory] are allowed", NAME, type);
         }
-        if (filter != null) {
-            filter.setBoost(boost);
-        }
+
         if (queryName != null) {
-            context.addNamedQuery(queryName, filter);
+            parseContext.addNamedQuery(queryName, filter);
         }
         return filter;
-    }
-
-    @Override
-    public GeoBoundingBoxQueryBuilder getBuilderPrototype() {
-        return GeoBoundingBoxQueryBuilder.PROTOTYPE;
-    }
+    }    
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
index 895679a..0995a5e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryBuilder.java
@@ -26,9 +26,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 import java.util.Locale;
 
-public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQueryBuilder> {
-
-    public static final String NAME = "geo_distance";
+public class GeoDistanceQueryBuilder extends QueryBuilder {
 
     private final String name;
 
@@ -44,7 +42,7 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
 
     private String optimizeBbox;
 
-    static final GeoDistanceQueryBuilder PROTOTYPE = new GeoDistanceQueryBuilder(null);
+    private String queryName;
 
     public GeoDistanceQueryBuilder(String name) {
         this.name = name;
@@ -91,9 +89,17 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         return this;
     }
 
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoDistanceQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoDistanceQueryParser.NAME);
         if (geohash != null) {
             builder.field(name, geohash);
         } else {
@@ -106,12 +112,9 @@ public class GeoDistanceQueryBuilder extends AbstractQueryBuilder<GeoDistanceQue
         if (optimizeBbox != null) {
             builder.field("optimize_bbox", optimizeBbox);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
index 77df31d..b785625 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceQueryParser.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
@@ -42,7 +43,9 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class GeoDistanceQueryParser extends BaseQueryParserTemp {
+public class GeoDistanceQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_distance";
 
     @Inject
     public GeoDistanceQueryParser() {
@@ -50,17 +53,15 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoDistanceQueryBuilder.NAME, "geoDistance"};
+        return new String[]{NAME, "geoDistance"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         GeoPoint point = new GeoPoint();
@@ -122,8 +123,6 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("optimize_bbox".equals(currentFieldName) || "optimizeBbox".equals(currentFieldName)) {
                     optimizeBbox = parser.textOrNull();
                 } else if ("normalize".equals(currentFieldName)) {
@@ -149,7 +148,7 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
             GeoUtils.normalizePoint(point, normalizeLat, normalizeLon);
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -159,17 +158,11 @@ public class GeoDistanceQueryParser extends BaseQueryParserTemp {
         GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
 
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoDistanceRangeQuery(point, null, distance, true, false, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoDistanceQueryBuilder getBuilderPrototype() {
-        return GeoDistanceQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
index b97263a..d21810f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 import java.util.Locale;
 
-public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistanceRangeQueryBuilder> {
-
-    public static final String NAME = "geo_distance_range";
+public class GeoDistanceRangeQueryBuilder extends QueryBuilder {
 
     private final String name;
 
@@ -44,9 +42,9 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
 
     private GeoDistance geoDistance;
 
-    private String optimizeBbox;
+    private String queryName;
 
-    static final GeoDistanceRangeQueryBuilder PROTOTYPE = new GeoDistanceRangeQueryBuilder(null);
+    private String optimizeBbox;
 
     public GeoDistanceRangeQueryBuilder(String name) {
         this.name = name;
@@ -127,9 +125,17 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         return this;
     }
 
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoDistanceRangeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoDistanceRangeQueryParser.NAME);
         if (geohash != null) {
             builder.field(name, geohash);
         } else {
@@ -145,12 +151,9 @@ public class GeoDistanceRangeQueryBuilder extends AbstractQueryBuilder<GeoDistan
         if (optimizeBbox != null) {
             builder.field("optimize_bbox", optimizeBbox);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
index 63d11ff..6c8479b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeQueryParser.java
@@ -28,6 +28,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.search.geo.GeoDistanceRangeQuery;
@@ -42,7 +43,9 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
+public class GeoDistanceRangeQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_distance_range";
 
     @Inject
     public GeoDistanceRangeQueryParser() {
@@ -50,17 +53,15 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoDistanceRangeQueryBuilder.NAME, "geoDistanceRange"};
+        return new String[]{NAME, "geoDistanceRange"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         GeoPoint point = new GeoPoint();
@@ -152,8 +153,6 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
                     fieldName = currentFieldName.substring(0, currentFieldName.length() - GeoPointFieldMapper.Names.GEOHASH_SUFFIX.length());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("optimize_bbox".equals(currentFieldName) || "optimizeBbox".equals(currentFieldName)) {
                     optimizeBbox = parser.textOrNull();
                 } else if ("normalize".equals(currentFieldName)) {
@@ -189,7 +188,7 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
             GeoUtils.normalizePoint(point, normalizeLat, normalizeLon);
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -198,17 +197,11 @@ public class GeoDistanceRangeQueryParser extends BaseQueryParserTemp {
         }
         GeoPointFieldMapper.GeoPointFieldType geoFieldType = ((GeoPointFieldMapper.GeoPointFieldType) fieldType);
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoDistanceRangeQuery(point, from, to, includeLower, includeUpper, geoDistance, geoFieldType, indexFieldData, optimizeBbox);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoDistanceRangeQueryBuilder getBuilderPrototype() {
-        return GeoDistanceRangeQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
index 3ce56bb..4fd2f41 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryBuilder.java
@@ -28,17 +28,15 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 import java.util.List;
 
-public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQueryBuilder> {
-
-    public static final String NAME = "geo_polygon";
+public class GeoPolygonQueryBuilder extends QueryBuilder {
 
     public static final String POINTS = GeoPolygonQueryParser.POINTS;
-
+    
     private final String name;
 
     private final List<GeoPoint> shell = Lists.newArrayList();
 
-    static final GeoPolygonQueryBuilder PROTOTYPE = new GeoPolygonQueryBuilder(null);
+    private String queryName;
 
     public GeoPolygonQueryBuilder(String name) {
         this.name = name;
@@ -49,7 +47,7 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
      *
      * @param lat The latitude
      * @param lon The longitude
-     * @return the current builder
+     * @return
      */
     public GeoPolygonQueryBuilder addPoint(double lat, double lon) {
         return addPoint(new GeoPoint(lat, lon));
@@ -63,10 +61,18 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         shell.add(point);
         return this;
     }
+    
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public GeoPolygonQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoPolygonQueryParser.NAME);
 
         builder.startObject(name);
         builder.startArray(POINTS);
@@ -76,13 +82,10 @@ public class GeoPolygonQueryBuilder extends AbstractQueryBuilder<GeoPolygonQuery
         builder.endArray();
         builder.endObject();
 
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
index f36c6d7..43d3686 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoPolygonQueryParser.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.Lists;
+
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.geo.GeoPoint;
 import org.elasticsearch.common.geo.GeoUtils;
@@ -27,6 +28,7 @@ import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentParser.Token;
 import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 import org.elasticsearch.index.search.geo.GeoPolygonQuery;
@@ -46,8 +48,9 @@ import java.util.List;
  * }
  * </pre>
  */
-public class GeoPolygonQueryParser extends BaseQueryParserTemp {
+public class GeoPolygonQueryParser implements QueryParser {
 
+    public static final String NAME = "geo_polygon";
     public static final String POINTS = "points";
 
     @Inject
@@ -56,12 +59,11 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoPolygonQueryBuilder.NAME, "geoPolygon"};
+        return new String[]{NAME, "geoPolygon"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -71,7 +73,6 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
         boolean normalizeLon = true;
         boolean normalizeLat = true;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -107,8 +108,6 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if ("normalize".equals(currentFieldName)) {
                     normalizeLat = parser.booleanValue();
                     normalizeLon = parser.booleanValue();
@@ -141,7 +140,7 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             }
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "failed to find geo_point field [" + fieldName + "]");
         }
@@ -149,17 +148,11 @@ public class GeoPolygonQueryParser extends BaseQueryParserTemp {
             throw new QueryParsingException(parseContext, "field [" + fieldName + "] is not a geo_point field");
         }
 
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         Query query = new GeoPolygonQuery(indexFieldData, shell.toArray(new GeoPoint[shell.size()]));
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
-        query.setBoost(boost);
         return query;
     }
-
-    @Override
-    public GeoPolygonQueryBuilder getBuilderPrototype() {
-        return GeoPolygonQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
index 9180d0e..3887874 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java
@@ -29,11 +29,7 @@ import java.io.IOException;
 /**
  * {@link QueryBuilder} that builds a GeoShape Filter
  */
-public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuilder> {
-
-    public static final String NAME = "geo_shape";
-
-    static final GeoShapeQueryBuilder PROTOTYPE = new GeoShapeQueryBuilder(null, null);
+public class GeoShapeQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<GeoShapeQueryBuilder> {
 
     private final String name;
 
@@ -41,6 +37,8 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
     private SpatialStrategy strategy = null;
 
+    private String queryName;
+
     private final String indexedShapeId;
     private final String indexedShapeType;
 
@@ -49,6 +47,8 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
     private ShapeRelation relation = null;
 
+    private float boost = -1;
+    
     /**
      * Creates a new GeoShapeQueryBuilder whose Filter will be against the
      * given field name using the given Shape
@@ -93,6 +93,17 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     /**
+     * Sets the name of the filter
+     *
+     * @param queryName Name of the filter
+     * @return this
+     */
+    public GeoShapeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Defines which spatial strategy will be used for building the geo shape filter. When not set, the strategy that
      * will be used will be the one that is associated with the geo shape field in the mappings.
      *
@@ -138,8 +149,14 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
     }
 
     @Override
+    public GeoShapeQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(GeoShapeQueryParser.NAME);
 
         builder.startObject(name);
 
@@ -168,13 +185,14 @@ public class GeoShapeQueryBuilder extends AbstractQueryBuilder<GeoShapeQueryBuil
 
         builder.endObject();
 
-        printBoostAndQueryName(builder);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+
+        if (name != null) {
+            builder.field("_name", queryName);
+        }
 
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
index 693db14..286fa1c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java
@@ -31,6 +31,7 @@ import org.elasticsearch.common.geo.ShapeRelation;
 import org.elasticsearch.common.geo.builders.ShapeBuilder;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper;
 import org.elasticsearch.index.search.shape.ShapeFetchService;
@@ -38,7 +39,9 @@ import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
 
-public class GeoShapeQueryParser extends BaseQueryParserTemp {
+public class GeoShapeQueryParser implements QueryParser {
+
+    public static final String NAME = "geo_shape";
 
     private ShapeFetchService fetchService;
 
@@ -49,12 +52,11 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{GeoShapeQueryBuilder.NAME, Strings.toCamelCase(GeoShapeQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
@@ -137,7 +139,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             throw new QueryParsingException(parseContext, "No Shape Relation defined");
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
             throw new QueryParsingException(parseContext, "Failed to find geo_shape field [" + fieldName + "]");
         }
@@ -158,7 +160,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             // this strategy doesn't support disjoint anymore: but it did before, including creating lucene fieldcache (!)
             // in this case, execute disjoint as exists && !intersects
             BooleanQuery bool = new BooleanQuery();
-            Query exists = ExistsQueryBuilder.newFilter(context, fieldName);
+            Query exists = ExistsQueryParser.newFilter(parseContext, fieldName, null);
             Filter intersects = strategy.makeFilter(getArgs(shape, ShapeRelation.INTERSECTS));
             bool.add(exists, BooleanClause.Occur.MUST);
             bool.add(intersects, BooleanClause.Occur.MUST_NOT);
@@ -168,7 +170,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
         }
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
@@ -188,11 +190,7 @@ public class GeoShapeQueryParser extends BaseQueryParserTemp {
             return new SpatialArgs(SpatialOperation.IsWithin, shape.build());
         default:
             throw new IllegalArgumentException("");
-        }
-    }
 
-    @Override
-    public GeoShapeQueryBuilder getBuilderPrototype() {
-        return GeoShapeQueryBuilder.PROTOTYPE;
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
index 5ea66b9..814aca4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/query/GeohashCellQuery.java
@@ -31,7 +31,9 @@ import org.elasticsearch.common.unit.DistanceUnit;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentParser.Token;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
 
 import java.io.IOException;
@@ -69,7 +71,7 @@ public class GeohashCellQuery {
      * @param geohashes   optional array of additional geohashes
      * @return a new GeoBoundinboxfilter
      */
-    public static Query create(QueryShardContext context, GeoPointFieldMapper.GeoPointFieldType fieldType, String geohash, @Nullable List<CharSequence> geohashes) {
+    public static Query create(QueryParseContext context, GeoPointFieldMapper.GeoPointFieldType fieldType, String geohash, @Nullable List<CharSequence> geohashes) {
         MappedFieldType geoHashMapper = fieldType.geohashFieldType();
         if (geoHashMapper == null) {
             throw new IllegalArgumentException("geohash filter needs geohash_prefix to be enabled");
@@ -88,7 +90,7 @@ public class GeohashCellQuery {
      * <code>geohash</code> to be set. the default for a neighbor filteing is
      * <code>false</code>.
      */
-    public static class Builder extends AbstractQueryBuilder<Builder> {
+    public static class Builder extends QueryBuilder {
         // we need to store the geohash rather than the corresponding point,
         // because a transformation from a geohash to a point an back to the
         // geohash will extend the accuracy of the hash to max precision
@@ -97,7 +99,6 @@ public class GeohashCellQuery {
         private String geohash;
         private int levels = -1;
         private boolean neighbors;
-        private static final Builder PROTOTYPE = new Builder(null);
 
 
         public Builder(String field) {
@@ -164,17 +165,12 @@ public class GeohashCellQuery {
                 builder.field(PRECISION, levels);
             }
             builder.field(field, geohash);
-            printBoostAndQueryName(builder);
-            builder.endObject();
-        }
 
-        @Override
-        public String getWriteableName() {
-            return NAME;
+            builder.endObject();
         }
     }
 
-    public static class Parser extends BaseQueryParserTemp {
+    public static class Parser implements QueryParser {
 
         @Inject
         public Parser() {
@@ -186,16 +182,14 @@ public class GeohashCellQuery {
         }
 
         @Override
-        public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-            QueryParseContext parseContext = context.parseContext();
+        public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
             XContentParser parser = parseContext.parser();
 
             String fieldName = null;
             String geohash = null;
             int levels = -1;
             boolean neighbors = false;
-            String queryName = null;
-            float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
 
             XContentParser.Token token;
             if ((token = parser.currentToken()) != Token.START_OBJECT) {
@@ -219,17 +213,11 @@ public class GeohashCellQuery {
                     } else if (NEIGHBORS.equals(field)) {
                         parser.nextToken();
                         neighbors = parser.booleanValue();
-                    } else if ("_name".equals(field)) {
-                        parser.nextToken();
-                        queryName = parser.text();
-                    } else if ("boost".equals(field)) {
-                        parser.nextToken();
-                        boost = parser.floatValue();
                     } else {
                         fieldName = field;
                         token = parser.nextToken();
                         if(token == Token.VALUE_STRING) {
-                            // A string indicates either a geohash or a lat/lon string
+                            // A string indicates either a gehash or a lat/lon string
                             String location = parser.text();
                             if(location.indexOf(",")>0) {
                                 geohash = GeoUtils.parseGeoPoint(parser).geohash();
@@ -249,7 +237,7 @@ public class GeohashCellQuery {
                 throw new QueryParsingException(parseContext, "failed to parse [{}] query. missing geohash value", NAME);
             }
 
-            MappedFieldType fieldType = context.fieldMapper(fieldName);
+            MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
             if (fieldType == null) {
                 throw new QueryParsingException(parseContext, "failed to parse [{}] query. missing [{}] field [{}]", NAME, GeoPointFieldMapper.CONTENT_TYPE, fieldName);
             }
@@ -270,22 +258,12 @@ public class GeohashCellQuery {
 
             Query filter;
             if (neighbors) {
-                filter = create(context, geoFieldType, geohash, GeoHashUtils.addNeighbors(geohash, new ArrayList<CharSequence>(8)));
+                filter = create(parseContext, geoFieldType, geohash, GeoHashUtils.addNeighbors(geohash, new ArrayList<CharSequence>(8)));
             } else {
-                filter = create(context, geoFieldType, geohash, null);
+                filter = create(parseContext, geoFieldType, geohash, null);
             }
-            if (queryName != null) {
-                context.addNamedQuery(queryName, filter);
-            }
-            if (filter != null) {
-                filter.setBoost(boost);
-            }
-            return filter;
-        }
 
-        @Override
-        public GeohashCellQuery.Builder getBuilderPrototype() {
-            return Builder.PROTOTYPE;
+            return filter;
         }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
index 64b852d..74a6a5c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java
@@ -23,14 +23,14 @@ import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 
 import java.io.IOException;
 
-public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuilder> {
-
-    public static final String NAME = "has_child";
+public class HasChildQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<HasChildQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
     private String childType;
 
+    private float boost = 1.0f;
+
     private String scoreType;
 
     private Integer minChildren;
@@ -39,9 +39,9 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
 
     private Integer shortCircuitCutoff;
 
-    private QueryInnerHitBuilder innerHit = null;
+    private String queryName;
 
-    static final HasChildQueryBuilder PROTOTYPE = new HasChildQueryBuilder(null, null);
+    private QueryInnerHitBuilder innerHit = null;
 
     public HasChildQueryBuilder(String type, QueryBuilder queryBuilder) {
         this.childType = type;
@@ -49,6 +49,16 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
     }
 
     /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public HasChildQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Defines how the scores from the matching child documents are mapped into the parent document.
      */
     public HasChildQueryBuilder scoreType(String scoreType) {
@@ -82,6 +92,14 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
     }
 
     /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public HasChildQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Sets inner hit definition in the scope of this query and reusing the defined type and query.
      */
     public HasChildQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
@@ -91,10 +109,13 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(HasChildQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("child_type", childType);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
         if (scoreType != null) {
             builder.field("score_type", scoreType);
         }
@@ -107,7 +128,9 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
         if (shortCircuitCutoff != null) {
             builder.field("short_circuit_cutoff", shortCircuitCutoff);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -115,9 +138,4 @@ public class HasChildQueryBuilder extends AbstractQueryBuilder<HasChildQueryBuil
         }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
index ba2ef65..c5388b1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java
@@ -22,6 +22,9 @@ package org.elasticsearch.index.query;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiDocValues;
 import org.apache.lucene.search.*;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.QueryWrapperFilter;
 import org.apache.lucene.search.join.BitDocIdSetFilter;
 import org.elasticsearch.common.ParseField;
 import org.apache.lucene.search.join.JoinUtil;
@@ -50,8 +53,9 @@ import java.io.IOException;
 /**
  *
  */
-public class HasChildQueryParser extends BaseQueryParserTemp {
+public class HasChildQueryParser implements QueryParser {
 
+    public static final String NAME = "has_child";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -63,16 +67,15 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[] { HasChildQueryBuilder.NAME, Strings.toCamelCase(HasChildQueryBuilder.NAME) };
+        return new String[] { NAME, Strings.toCamelCase(NAME) };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         boolean queryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String childType = null;
         ScoreType scoreType = ScoreType.NONE;
         int minChildren = 0;
@@ -138,7 +141,7 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
         }
         innerQuery.setBoost(boost);
 
-        DocumentMapper childDocMapper = context.mapperService().documentMapper(childType);
+        DocumentMapper childDocMapper = parseContext.mapperService().documentMapper(childType);
         if (childDocMapper == null) {
             throw new QueryParsingException(parseContext, "[has_child] No mapping for for type [" + childType + "]");
         }
@@ -148,14 +151,14 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
         }
 
         if (innerHits != null) {
-            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.v2(), parsedQuery, null, context.mapperService(), childDocMapper);
+            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.v2(), parsedQuery, null, parseContext.mapperService(), childDocMapper);
             String name = innerHits.v1() != null ? innerHits.v1() : childType;
-            context.addInnerHits(name, parentChildInnerHits);
+            parseContext.addInnerHits(name, parentChildInnerHits);
         }
 
         String parentType = parentFieldMapper.type();
-        DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+        DocumentMapper parentDocMapper = parseContext.mapperService().documentMapper(parentType);
         if (parentDocMapper == null) {
             throw new QueryParsingException(parseContext, "[has_child]  Type [" + childType + "] points to a non existent parent type ["
                     + parentType + "]");
@@ -167,15 +170,15 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
 
         BitDocIdSetFilter nonNestedDocsFilter = null;
         if (parentDocMapper.hasNestedObjects()) {
-            nonNestedDocsFilter = context.bitsetFilter(Queries.newNonNestedFilter());
+            nonNestedDocsFilter = parseContext.bitsetFilter(Queries.newNonNestedFilter());
         }
 
         // wrap the query with type query
         innerQuery = Queries.filtered(innerQuery, childDocMapper.typeFilter());
 
         final Query query;
-        final ParentChildIndexFieldData parentChildIndexFieldData = context.getForField(parentFieldMapper.fieldType());
-        if (context.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
+        final ParentChildIndexFieldData parentChildIndexFieldData = parseContext.getForField(parentFieldMapper.fieldType());
+        if (parseContext.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
             query = joinUtilHelper(parentType, parentChildIndexFieldData, parentDocMapper.typeFilter(), scoreType, innerQuery, minChildren, maxChildren);
         } else {
             // TODO: use the query API
@@ -189,7 +192,7 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
             }
         }
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         query.setBoost(boost);
         return query;
@@ -286,9 +289,4 @@ public class HasChildQueryParser extends BaseQueryParserTemp {
             return "LateParsingQuery {parentType=" + parentType + "}";
         }
     }
-
-    @Override
-    public HasChildQueryBuilder getBuilderPrototype() {
-        return HasChildQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
index 67bfe07..743ad76 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java
@@ -26,14 +26,14 @@ import java.io.IOException;
 /**
  * Builder for the 'has_parent' query.
  */
-public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBuilder> {
+public class HasParentQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<HasParentQueryBuilder> {
 
-    public static final String NAME = "has_parent";
     private final QueryBuilder queryBuilder;
     private final String parentType;
     private String scoreType;
+    private float boost = 1.0f;
+    private String queryName;
     private QueryInnerHitBuilder innerHit = null;
-    static final HasParentQueryBuilder PROTOTYPE = new HasParentQueryBuilder(null, null);
 
     /**
      * @param parentType  The parent type
@@ -44,6 +44,12 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
         this.queryBuilder = parentQuery;
     }
 
+    @Override
+    public HasParentQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
      * Defines how the parent score is mapped into the child documents.
      */
@@ -53,6 +59,14 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
     }
 
     /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public HasParentQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
      * Sets inner hit definition in the scope of this query and reusing the defined type and query.
      */
     public HasParentQueryBuilder innerHit(QueryInnerHitBuilder innerHit) {
@@ -62,14 +76,19 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(HasParentQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("parent_type", parentType);
         if (scoreType != null) {
             builder.field("score_type", scoreType);
         }
-        printBoostAndQueryName(builder);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -77,10 +96,5 @@ public class HasParentQueryBuilder extends AbstractQueryBuilder<HasParentQueryBu
         }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
index 00f8884..3d3a662 100644
--- a/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java
@@ -47,8 +47,9 @@ import java.util.Set;
 
 import static org.elasticsearch.index.query.HasChildQueryParser.joinUtilHelper;
 
-public class HasParentQueryParser extends BaseQueryParserTemp {
+public class HasParentQueryParser implements QueryParser {
 
+    public static final String NAME = "has_parent";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -60,16 +61,15 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{HasParentQueryBuilder.NAME, Strings.toCamelCase(HasParentQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         boolean queryFound = false;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String parentType = null;
         boolean score = false;
         String queryName = null;
@@ -134,40 +134,40 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         }
 
         innerQuery.setBoost(boost);
-        Query query = createParentQuery(innerQuery, parentType, score, context, innerHits);
+        Query query = createParentQuery(innerQuery, parentType, score, parseContext, innerHits);
         if (query == null) {
             return null;
         }
 
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
 
-    static Query createParentQuery(Query innerQuery, String parentType, boolean score, QueryShardContext context, Tuple<String, SubSearchContext> innerHits) throws IOException {
-        DocumentMapper parentDocMapper = context.mapperService().documentMapper(parentType);
+    static Query createParentQuery(Query innerQuery, String parentType, boolean score, QueryParseContext parseContext, Tuple<String, SubSearchContext> innerHits) throws IOException {
+        DocumentMapper parentDocMapper = parseContext.mapperService().documentMapper(parentType);
         if (parentDocMapper == null) {
-            throw new QueryParsingException(context.parseContext(), "[has_parent] query configured 'parent_type' [" + parentType
+            throw new QueryParsingException(parseContext, "[has_parent] query configured 'parent_type' [" + parentType
                     + "] is not a valid type");
         }
 
         if (innerHits != null) {
-            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, context.copyNamedQueries());
-            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.v2(), parsedQuery, null, context.mapperService(), parentDocMapper);
+            ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
+            InnerHitsContext.ParentChildInnerHits parentChildInnerHits = new InnerHitsContext.ParentChildInnerHits(innerHits.v2(), parsedQuery, null, parseContext.mapperService(), parentDocMapper);
             String name = innerHits.v1() != null ? innerHits.v1() : parentType;
-            context.addInnerHits(name, parentChildInnerHits);
+            parseContext.addInnerHits(name, parentChildInnerHits);
         }
 
         Set<String> parentTypes = new HashSet<>(5);
         parentTypes.add(parentDocMapper.type());
         ParentChildIndexFieldData parentChildIndexFieldData = null;
-        for (DocumentMapper documentMapper : context.mapperService().docMappers(false)) {
+        for (DocumentMapper documentMapper : parseContext.mapperService().docMappers(false)) {
             ParentFieldMapper parentFieldMapper = documentMapper.parentFieldMapper();
             if (parentFieldMapper.active()) {
-                DocumentMapper parentTypeDocumentMapper = context.mapperService().documentMapper(parentFieldMapper.type());
-                parentChildIndexFieldData = context.getForField(parentFieldMapper.fieldType());
+                DocumentMapper parentTypeDocumentMapper = parseContext.mapperService().documentMapper(parentFieldMapper.type());
+                parentChildIndexFieldData = parseContext.getForField(parentFieldMapper.fieldType());
                 if (parentTypeDocumentMapper == null) {
                     // Only add this, if this parentFieldMapper (also a parent)  isn't a child of another parent.
                     parentTypes.add(parentFieldMapper.type());
@@ -175,19 +175,19 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
             }
         }
         if (parentChildIndexFieldData == null) {
-            throw new QueryParsingException(context.parseContext(), "[has_parent] no _parent field configured");
+            throw new QueryParsingException(parseContext, "[has_parent] no _parent field configured");
         }
 
         Query parentFilter = null;
         if (parentTypes.size() == 1) {
-            DocumentMapper documentMapper = context.mapperService().documentMapper(parentTypes.iterator().next());
+            DocumentMapper documentMapper = parseContext.mapperService().documentMapper(parentTypes.iterator().next());
             if (documentMapper != null) {
                 parentFilter = documentMapper.typeFilter();
             }
         } else {
             BooleanQuery parentsFilter = new BooleanQuery();
             for (String parentTypeStr : parentTypes) {
-                DocumentMapper documentMapper = context.mapperService().documentMapper(parentTypeStr);
+                DocumentMapper documentMapper = parseContext.mapperService().documentMapper(parentTypeStr);
                 if (documentMapper != null) {
                     parentsFilter.add(documentMapper.typeFilter(), BooleanClause.Occur.SHOULD);
                 }
@@ -202,7 +202,7 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         // wrap the query with type query
         innerQuery = Queries.filtered(innerQuery, parentDocMapper.typeFilter());
         Filter childrenFilter = new QueryWrapperFilter(Queries.not(parentFilter));
-        if (context.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
+        if (parseContext.indexVersionCreated().onOrAfter(Version.V_2_0_0_beta1)) {
             ScoreType scoreMode = score ? ScoreType.MAX : ScoreType.NONE;
             return joinUtilHelper(parentType, parentChildIndexFieldData, childrenFilter, scoreMode, innerQuery, 0, Integer.MAX_VALUE);
         } else {
@@ -214,9 +214,4 @@ public class HasParentQueryParser extends BaseQueryParserTemp {
         }
     }
 
-    @Override
-    public HasParentQueryBuilder getBuilderPrototype() {
-        return HasParentQueryBuilder.PROTOTYPE;
-    }
-
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
index 461a800..02c2a17 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java
@@ -19,62 +19,44 @@
 
 package org.elasticsearch.index.query;
 
-import com.google.common.collect.Sets;
-
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.Uid;
-import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
 
 /**
  * A query that will return only documents matching specific ids (and a type).
  */
-public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
+public class IdsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<IdsQueryBuilder> {
 
-    public static final String NAME = "ids";
+    private final List<String> types;
 
-    private final Set<String> ids = Sets.newHashSet();
+    private List<String> values = new ArrayList<>();
 
-    private final String[] types;
+    private float boost = -1;
 
-    static final IdsQueryBuilder PROTOTYPE = new IdsQueryBuilder();
+    private String queryName;
 
-    /**
-     * Creates a new IdsQueryBuilder by optionally providing the types of the documents to look for
-     */
-    public IdsQueryBuilder(@Nullable String... types) {
-        this.types = types;
-    }
-
-    /**
-     * Returns the types used in this query
-     */
-    public String[] types() {
-        return this.types;
+    public IdsQueryBuilder(String... types) {
+        this.types = types == null ? null : Arrays.asList(types);
     }
 
     /**
-     * Adds ids to the query.
+     * Adds ids to the filter.
      */
     public IdsQueryBuilder addIds(String... ids) {
-        Collections.addAll(this.ids, ids);
+        values.addAll(Arrays.asList(ids));
         return this;
     }
 
     /**
-     * Adds ids to the query.
+     * Adds ids to the filter.
      */
     public IdsQueryBuilder addIds(Collection<String> ids) {
-        this.ids.addAll(ids);
+        values.addAll(ids);
         return this;
     }
 
@@ -93,83 +75,48 @@ public class IdsQueryBuilder extends AbstractQueryBuilder<IdsQueryBuilder> {
     }
 
     /**
-     * Returns the ids for the query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public Set<String> ids() {
-        return this.ids;
+    @Override
+    public IdsQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public IdsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(IdsQueryParser.NAME);
         if (types != null) {
-            if (types.length == 1) {
-                builder.field("type", types[0]);
+            if (types.size() == 1) {
+                builder.field("type", types.get(0));
             } else {
-                builder.array("types", types);
+                builder.startArray("types");
+                for (Object type : types) {
+                    builder.value(type);
+                }
+                builder.endArray();
             }
         }
         builder.startArray("values");
-        for (String value : ids) {
+        for (Object value : values) {
             builder.value(value);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query query;
-        if (this.ids.isEmpty()) {
-             query = Queries.newMatchNoDocsQuery();
-        } else {
-            Collection<String> typesForQuery;
-            if (types == null || types.length == 0) {
-                typesForQuery = context.queryTypes();
-            } else if (types.length == 1 && MetaData.ALL.equals(types[0])) {
-                typesForQuery = context.mapperService().types();
-            } else {
-                typesForQuery = Sets.newHashSet(types);
-            }
-
-            query = new TermsQuery(UidFieldMapper.NAME, Uid.createUidsForTypesAndIds(typesForQuery, ids));
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // all fields can be empty or null
-        return null;
-    }
-
-    @Override
-    protected IdsQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        IdsQueryBuilder idsQueryBuilder = new IdsQueryBuilder(in.readStringArray());
-        idsQueryBuilder.addIds(in.readStringArray());
-        return idsQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeStringArray(types);
-        out.writeStringArray(ids.toArray(new String[ids.size()]));
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(ids, Arrays.hashCode(types));
-    }
-
-    @Override
-    protected boolean doEquals(IdsQueryBuilder other) {
-        return Objects.equals(ids, other.ids) &&
-               Arrays.equals(types, other.types);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
index 3403517..340eb81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java
@@ -20,17 +20,28 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Iterables;
+
+import org.apache.lucene.queries.TermsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.Uid;
+import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.List;
 
 /**
- * Parser for ids query
+ *
  */
-public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
+public class IdsQueryParser implements QueryParser {
+
+    public static final String NAME = "ids";
 
     @Inject
     public IdsQueryParser() {
@@ -38,21 +49,18 @@ public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{IdsQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
-    /**
-     * @return a QueryBuilder representation of the query passed in as XContent in the parse context
-     */
     @Override
-    public IdsQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        List<String> ids = new ArrayList<>();
-        List<String> types = new ArrayList<>();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String queryName = null;
 
+        List<BytesRef> ids = new ArrayList<>();
+        Collection<String> types = null;
         String currentFieldName = null;
+        float boost = 1.0f;
+        String queryName = null;
         XContentParser.Token token;
         boolean idsProvided = false;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -64,17 +72,18 @@ public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         if ((token == XContentParser.Token.VALUE_STRING) ||
                                 (token == XContentParser.Token.VALUE_NUMBER)) {
-                            String id = parser.textOrNull();
-                            if (id == null) {
+                            BytesRef value = parser.utf8BytesOrNull();
+                            if (value == null) {
                                 throw new QueryParsingException(parseContext, "No value specified for term filter");
                             }
-                            ids.add(id);
+                            ids.add(value);
                         } else {
                             throw new QueryParsingException(parseContext, "Illegal value for id, expecting a string or number, got: "
                                     + token);
                         }
                     }
                 } else if ("types".equals(currentFieldName) || "type".equals(currentFieldName)) {
+                    types = new ArrayList<>();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         String value = parser.textOrNull();
                         if (value == null) {
@@ -97,18 +106,26 @@ public class IdsQueryParser extends BaseQueryParser<IdsQueryBuilder> {
                 }
             }
         }
+
         if (!idsProvided) {
             throw new QueryParsingException(parseContext, "[ids] query, no ids values provided");
         }
 
-        IdsQueryBuilder query = new IdsQueryBuilder(types.toArray(new String[types.size()]));
-        query.addIds(ids.toArray(new String[ids.size()]));
-        query.boost(boost).queryName(queryName);
-        return query;
-    }
+        if (ids.isEmpty()) {
+            return Queries.newMatchNoDocsQuery();
+        }
 
-    @Override
-    public IdsQueryBuilder getBuilderPrototype() {
-        return IdsQueryBuilder.PROTOTYPE;
+        if (types == null || types.isEmpty()) {
+            types = parseContext.queryTypes();
+        } else if (types.size() == 1 && Iterables.getFirst(types, null).equals("_all")) {
+            types = parseContext.mapperService().types();
+        }
+
+        TermsQuery query = new TermsQuery(UidFieldMapper.NAME, Uid.createUidsForTypesAndIds(types, ids));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
index 58a3de6..810504a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java
@@ -53,10 +53,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     public static final String PARSE_STRICT = "index.query.parse.strict";
     public static final String ALLOW_UNMAPPED = "index.query.parse.allow_unmapped_fields";
 
-    private CloseableThreadLocal<QueryShardContext> cache = new CloseableThreadLocal<QueryShardContext>() {
+    private CloseableThreadLocal<QueryParseContext> cache = new CloseableThreadLocal<QueryParseContext>() {
         @Override
-        protected QueryShardContext initialValue() {
-            return new QueryShardContext(index, IndexQueryParserService.this);
+        protected QueryParseContext initialValue() {
+            return new QueryParseContext(index, IndexQueryParserService.this);
         }
     };
 
@@ -116,26 +116,20 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return this.queryStringLenient;
     }
 
-    //norelease we might want to get rid of this as it was temporarily introduced for our default doToQuery impl
-    //seems to be used only in tests
-    public QueryParser<?> queryParser(String name) {
+    public QueryParser queryParser(String name) {
         return indicesQueriesRegistry.queryParsers().get(name);
     }
 
-    public IndicesQueriesRegistry indicesQueriesRegistry() {
-        return indicesQueriesRegistry;
-    }
-
     public ParsedQuery parse(QueryBuilder queryBuilder) {
         XContentParser parser = null;
         try {
             BytesReference bytes = queryBuilder.buildAsBytes();
             parser = XContentFactory.xContent(bytes).createParser(bytes);
             return parse(cache.get(), parser);
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -152,10 +146,10 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         try {
             parser = XContentFactory.xContent(source, offset, length).createParser(source, offset, length);
             return parse(cache.get(), parser);
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -167,8 +161,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return parse(cache.get(), source);
     }
 
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, BytesReference source) {
+    public ParsedQuery parse(QueryParseContext context, BytesReference source) {
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
@@ -176,7 +169,7 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -184,15 +177,15 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         }
     }
 
-    public ParsedQuery parse(String source) throws QueryParsingException, QueryShardException {
+    public ParsedQuery parse(String source) throws QueryParsingException {
         XContentParser parser = null;
         try {
             parser = XContentFactory.xContent(source).createParser(source);
             return innerParse(cache.get(), parser);
-        } catch (QueryShardException|QueryParsingException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Exception e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse [" + source + "]", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse [" + source + "]", e);
         } finally {
             if (parser != null) {
                 parser.close();
@@ -204,12 +197,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
         return parse(cache.get(), parser);
     }
 
-    //norelease
-    public ParsedQuery parse(QueryShardContext context, XContentParser parser) {
+    public ParsedQuery parse(QueryParseContext context, XContentParser parser) {
         try {
             return innerParse(context, parser);
         } catch (IOException e) {
-            throw new QueryParsingException(context.parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(context, "Failed to parse", e);
         }
     }
 
@@ -217,12 +209,11 @@ public class IndexQueryParserService extends AbstractIndexComponent {
      * Parses an inner filter, returning null if the filter should be ignored.
      */
     @Nullable
-    //norelease
     public ParsedQuery parseInnerFilter(XContentParser parser) throws IOException {
-        QueryShardContext context = cache.get();
+        QueryParseContext context = cache.get();
         context.reset(parser);
         try {
-            Query filter = context.parseContext().parseInnerFilter();
+            Query filter = context.parseInnerFilter();
             if (filter == null) {
                 return null;
             }
@@ -233,22 +224,27 @@ public class IndexQueryParserService extends AbstractIndexComponent {
     }
 
     @Nullable
-    public QueryBuilder parseInnerQueryBuilder(QueryParseContext parseContext) throws IOException {
-        parseContext.parseFieldMatcher(parseFieldMatcher);
-        return parseContext.parseInnerQueryBuilder();
+    public Query parseInnerQuery(XContentParser parser) throws IOException {
+        QueryParseContext context = cache.get();
+        context.reset(parser);
+        try {
+            return context.parseInnerQuery();
+        } finally {
+            context.reset(null);
+        }
     }
 
     @Nullable
-    //norelease
-    public Query parseInnerQuery(QueryShardContext context) throws IOException {
-        Query query = context.parseContext().parseInnerQueryBuilder().toQuery(context);
+    public Query parseInnerQuery(QueryParseContext parseContext) throws IOException {
+        parseContext.parseFieldMatcher(parseFieldMatcher);
+        Query query = parseContext.parseInnerQuery();
         if (query == null) {
             query = Queries.newMatchNoDocsQuery();
         }
         return query;
     }
 
-    public QueryShardContext getShardContext() {
+    public QueryParseContext getParseContext() {
         return cache.get();
     }
 
@@ -280,39 +276,34 @@ public class IndexQueryParserService extends AbstractIndexComponent {
                         XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource);
                         parsedQuery = parse(qSourceParser);
                     } else {
-                        throw new QueryParsingException(getShardContext().parseContext(), "request does not support [" + fieldName + "]");
+                        throw new QueryParsingException(getParseContext(), "request does not support [" + fieldName + "]");
                     }
                 }
             }
             if (parsedQuery != null) {
                 return parsedQuery;
             }
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             throw e;
         } catch (Throwable e) {
-            throw new QueryParsingException(getShardContext().parseContext(), "Failed to parse", e);
+            throw new QueryParsingException(getParseContext(), "Failed to parse", e);
         }
 
-        throw new QueryParsingException(getShardContext().parseContext(), "Required query is missing");
+        throw new QueryParsingException(getParseContext(), "Required query is missing");
     }
 
-    //norelease
-    private ParsedQuery innerParse(QueryShardContext context, XContentParser parser) throws IOException, QueryShardException {
-        context.reset(parser);
+    private ParsedQuery innerParse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
+        parseContext.reset(parser);
         try {
-            context.parseFieldMatcher(parseFieldMatcher);
-            return innerParse(context, context.parseContext().parseInnerQueryBuilder());
+            parseContext.parseFieldMatcher(parseFieldMatcher);
+            Query query = parseContext.parseInnerQuery();
+            if (query == null) {
+                query = Queries.newMatchNoDocsQuery();
+            }
+            return new ParsedQuery(query, parseContext.copyNamedQueries());
         } finally {
-            context.reset(null);
-        }
-    }
-
-    private static ParsedQuery innerParse(QueryShardContext context, QueryBuilder queryBuilder) throws IOException, QueryShardException {
-        Query query = queryBuilder.toQuery(context);
-        if (query == null) {
-            query = Queries.newMatchNoDocsQuery();
+            parseContext.reset(null);
         }
-        return new ParsedQuery(query, context.copyNamedQueries());
     }
 
     public ParseFieldMatcher parseFieldMatcher() {
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
index d7de7c1..7c2af81 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java
@@ -27,9 +27,7 @@ import java.io.IOException;
  * A query that will execute the wrapped query only for the specified indices, and "match_all" when
  * it does not match those indices (by default).
  */
-public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilder> {
-
-    public static final String NAME = "indices";
+public class IndicesQueryBuilder extends QueryBuilder {
 
     private final QueryBuilder queryBuilder;
 
@@ -38,7 +36,7 @@ public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilde
     private String sNoMatchQuery;
     private QueryBuilder noMatchQuery;
 
-    static final IndicesQueryBuilder PROTOTYPE = new IndicesQueryBuilder(null);
+    private String queryName;
 
     public IndicesQueryBuilder(QueryBuilder queryBuilder, String... indices) {
         this.queryBuilder = queryBuilder;
@@ -61,9 +59,17 @@ public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilde
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public IndicesQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(IndicesQueryParser.NAME);
         builder.field("indices", indices);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
@@ -73,12 +79,9 @@ public class IndicesQueryBuilder extends AbstractQueryBuilder<IndicesQueryBuilde
         } else if (sNoMatchQuery != null) {
             builder.field("no_match_query", sNoMatchQuery);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
index d368d6b..a18c865 100644
--- a/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java
@@ -37,8 +37,9 @@ import java.util.Collection;
 
 /**
  */
-public class IndicesQueryParser extends BaseQueryParserTemp {
+public class IndicesQueryParser implements QueryParser {
 
+    public static final String NAME = "indices";
     private static final ParseField QUERY_FIELD = new ParseField("query", "filter");
     private static final ParseField NO_MATCH_QUERY = new ParseField("no_match_query", "no_match_filter");
 
@@ -54,12 +55,11 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{IndicesQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         Query noMatchQuery = null;
@@ -67,7 +67,6 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
         boolean indicesFound = false;
         boolean currentIndexMatchesIndices = false;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -119,8 +118,6 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
                     }
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[indices] query does not support [" + currentFieldName + "]");
                 }
@@ -150,9 +147,8 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
             }
         }
         if (queryName != null) {
-            context.addNamedQuery(queryName, chosenQuery);
+            parseContext.addNamedQuery(queryName, chosenQuery);
         }
-        chosenQuery.setBoost(boost);
         return chosenQuery;
     }
 
@@ -165,9 +161,4 @@ public class IndicesQueryParser extends BaseQueryParserTemp {
         }
         return false;
     }
-
-    @Override
-    public IndicesQueryBuilder getBuilderPrototype() {
-        return IndicesQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
index b217a5e..9d44f39 100644
--- a/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/LimitQueryBuilder.java
@@ -19,11 +19,7 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
 import org.elasticsearch.action.search.SearchRequestBuilder;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -32,62 +28,18 @@ import java.io.IOException;
  * @deprecated Use {@link SearchRequestBuilder#setTerminateAfter(int)} instead.
  */
 @Deprecated
-public class LimitQueryBuilder extends AbstractQueryBuilder<LimitQueryBuilder> {
+public class LimitQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "limit";
     private final int limit;
-    static final LimitQueryBuilder PROTOTYPE = new LimitQueryBuilder(-1);
 
     public LimitQueryBuilder(int limit) {
         this.limit = limit;
     }
 
-    public int limit() {
-        return limit;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(LimitQueryParser.NAME);
         builder.field("value", limit);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // this filter is deprecated and parses to a filter that matches everything
-        return Queries.newMatchAllQuery();
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
-
-    @Override
-    protected boolean doEquals(LimitQueryBuilder other) {
-        return Integer.compare(other.limit, limit) == 0;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return this.limit;
-    }
-
-    @Override
-    protected LimitQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new LimitQueryBuilder(in.readInt());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeInt(limit);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
index ed47198..3419f61 100644
--- a/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/LimitQueryParser.java
@@ -19,17 +19,17 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
-/**
- * Parser for limit query
- * @deprecated use terminate_after feature instead
- */
 @Deprecated
-public class LimitQueryParser extends BaseQueryParser<LimitQueryBuilder> {
+public class LimitQueryParser implements QueryParser {
+
+    public static final String NAME = "limit";
 
     @Inject
     public LimitQueryParser() {
@@ -37,16 +37,14 @@ public class LimitQueryParser extends BaseQueryParser<LimitQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{LimitQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public LimitQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         int limit = -1;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -55,10 +53,6 @@ public class LimitQueryParser extends BaseQueryParser<LimitQueryBuilder> {
             } else if (token.isValue()) {
                 if ("value".equals(currentFieldName)) {
                     limit = parser.intValue();
-                } else if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[limit] query does not support [" + currentFieldName + "]");
                 }
@@ -69,11 +63,7 @@ public class LimitQueryParser extends BaseQueryParser<LimitQueryBuilder> {
             throw new QueryParsingException(parseContext, "No value specified for limit query");
         }
 
-        return new LimitQueryBuilder(limit).boost(boost).queryName(queryName);
-    }
-
-    @Override
-    public LimitQueryBuilder getBuilderPrototype() {
-        return LimitQueryBuilder.PROTOTYPE;
+        // this filter is deprecated and parses to a filter that matches everything
+        return Queries.newMatchAllQuery();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
index 00c5019..b09bc9f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -30,52 +26,26 @@ import java.io.IOException;
 /**
  * A query that matches on all documents.
  */
-public class MatchAllQueryBuilder extends AbstractQueryBuilder<MatchAllQueryBuilder> {
+public class MatchAllQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MatchAllQueryBuilder> {
 
-    public static final String NAME = "match_all";
-
-    static final MatchAllQueryBuilder PROTOTYPE = new MatchAllQueryBuilder();
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return Queries.newMatchAllQuery();
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        // nothing to validate
-        return null;
-    }
+    private float boost = -1;
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected boolean doEquals(MatchAllQueryBuilder other) {
-        return true;
+    public MatchAllQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     @Override
-    protected int doHashCode() {
-        return 0;
-    }
-
-    @Override
-    protected MatchAllQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new MatchAllQueryBuilder();
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        //nothing to write really
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(MatchAllQueryParser.NAME);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
index 4066c75..933d3d3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java
@@ -19,16 +19,21 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for match_all query
+ *
  */
-public class MatchAllQueryParser extends BaseQueryParser<MatchAllQueryBuilder> {
+public class MatchAllQueryParser implements QueryParser {
+
+    public static final String NAME = "match_all";
 
     @Inject
     public MatchAllQueryParser() {
@@ -36,38 +41,35 @@ public class MatchAllQueryParser extends BaseQueryParser<MatchAllQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{MatchAllQueryBuilder.NAME, Strings.toCamelCase(MatchAllQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public MatchAllQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
+        float boost = 1.0f;
         String currentFieldName = null;
+
         XContentParser.Token token;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         while (((token = parser.nextToken()) != XContentParser.Token.END_OBJECT && token != XContentParser.Token.END_ARRAY)) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
             } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
+                if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[match_all] query does not support [" + currentFieldName + "]");
                 }
             }
         }
-        MatchAllQueryBuilder queryBuilder = new MatchAllQueryBuilder();
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
 
-    @Override
-    public MatchAllQueryBuilder getBuilderPrototype() {
-        return MatchAllQueryBuilder.PROTOTYPE;
+        if (boost == 1.0f) {
+            return Queries.newMatchAllQuery();
+        }
+
+        MatchAllDocsQuery query = new MatchAllDocsQuery();
+        query.setBoost(boost);
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
index e31a8cf..e4a643f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryBuilder.java
@@ -29,9 +29,12 @@ import java.util.Locale;
  * Match query is a query that analyzes the text and constructs a query as the result of the analysis. It
  * can construct different queries based on the type provided.
  */
-public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
+public class MatchQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MatchQueryBuilder> {
 
-    public static final String NAME = "match";
+    public enum Operator {
+        OR,
+        AND
+    }
 
     public enum Type {
         /**
@@ -63,6 +66,8 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
 
     private String analyzer;
 
+    private Float boost;
+
     private Integer slop;
 
     private Fuzziness fuzziness;
@@ -85,7 +90,7 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
 
     private Float cutoff_Frequency = null;
 
-    static final MatchQueryBuilder PROTOTYPE = new MatchQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Constructs a new text query.
@@ -121,6 +126,15 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
     }
 
     /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public MatchQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Set the phrase slop if evaluated to a phrase query type.
      */
     public MatchQueryBuilder slop(int slop) {
@@ -194,9 +208,17 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MatchQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(MatchQueryParser.NAME);
         builder.startObject(name);
 
         builder.field("query", text);
@@ -209,6 +231,9 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (slop != null) {
             builder.field("slop", slop);
         }
@@ -243,13 +268,12 @@ public class MatchQueryBuilder extends AbstractQueryBuilder<MatchQueryBuilder> {
         if (cutoff_Frequency != null) {
             builder.field("cutoff_frequency", cutoff_Frequency);
         }
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+
+
         builder.endObject();
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
index 5d9ba66..62177ab 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.index.query;
 
 import org.apache.lucene.queries.ExtendedCommonTermsQuery;
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
@@ -34,7 +35,9 @@ import java.io.IOException;
 /**
  *
  */
-public class MatchQueryParser extends BaseQueryParserTemp {
+public class MatchQueryParser implements QueryParser {
+
+    public static final String NAME = "match";
 
     @Inject
     public MatchQueryParser() {
@@ -43,13 +46,12 @@ public class MatchQueryParser extends BaseQueryParserTemp {
     @Override
     public String[] names() {
         return new String[]{
-                MatchQueryBuilder.NAME, "match_phrase", "matchPhrase", "match_phrase_prefix", "matchPhrasePrefix", "matchFuzzy", "match_fuzzy", "fuzzy_match"
+                NAME, "match_phrase", "matchPhrase", "match_phrase_prefix", "matchPhrasePrefix", "matchFuzzy", "match_fuzzy", "fuzzy_match"
         };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         MatchQuery.Type type = MatchQuery.Type.BOOLEAN;
@@ -68,8 +70,8 @@ public class MatchQueryParser extends BaseQueryParserTemp {
         String fieldName = parser.currentName();
 
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        MatchQuery matchQuery = new MatchQuery(context);
+        float boost = 1.0f;
+        MatchQuery matchQuery = new MatchQuery(parseContext);
         String minimumShouldMatch = null;
         String queryName = null;
 
@@ -95,7 +97,7 @@ public class MatchQueryParser extends BaseQueryParserTemp {
                         }
                     } else if ("analyzer".equals(currentFieldName)) {
                         String analyzer = parser.text();
-                        if (context.analysisService().analyzer(analyzer) == null) {
+                        if (parseContext.analysisService().analyzer(analyzer) == null) {
                             throw new QueryParsingException(parseContext, "[match] analyzer [" + parser.text() + "] not found");
                         }
                         matchQuery.setAnalyzer(analyzer);
@@ -110,7 +112,15 @@ public class MatchQueryParser extends BaseQueryParserTemp {
                     } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                         matchQuery.setMaxExpansions(parser.intValue());
                     } else if ("operator".equals(currentFieldName)) {
-                        matchQuery.setOccur(Operator.fromString(parser.text()).toBooleanClauseOccur());
+                        String op = parser.text();
+                        if ("or".equalsIgnoreCase(op)) {
+                            matchQuery.setOccur(BooleanClause.Occur.SHOULD);
+                        } else if ("and".equalsIgnoreCase(op)) {
+                            matchQuery.setOccur(BooleanClause.Occur.MUST);
+                        } else {
+                            throw new QueryParsingException(parseContext, "text query requires operator to be either 'and' or 'or', not ["
+                                    + op + "]");
+                        }
                     } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                         minimumShouldMatch = parser.textOrNull();
                     } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
@@ -164,13 +174,8 @@ public class MatchQueryParser extends BaseQueryParserTemp {
         }
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
-
-    @Override
-    public MatchQueryBuilder getBuilderPrototype() {
-        return MatchQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
index 253af16..ac3f279 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MissingQueryBuilder.java
@@ -19,45 +19,25 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.*;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
-import java.util.Collection;
-import java.util.Objects;
 
 /**
- * Constructs a filter that have only null values or no value in the original field.
+ * Constructs a filter that only match on documents that the field has a value in them.
  */
-public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilder> {
+public class MissingQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "missing";
+    private String name;
 
-    public static final boolean DEFAULT_NULL_VALUE = false;
+    private String queryName;
 
-    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
+    private Boolean nullValue;
 
-    private final String fieldPattern;
+    private Boolean existence;
 
-    private boolean nullValue = DEFAULT_NULL_VALUE;
-
-    private boolean existence = DEFAULT_EXISTENCE_VALUE;
-
-    static final MissingQueryBuilder PROTOTYPE = new MissingQueryBuilder(null);
-
-    public MissingQueryBuilder(String fieldPattern) {
-        this.fieldPattern = fieldPattern;
-    }
-
-    public String fieldPattern() {
-        return this.fieldPattern;
+    public MissingQueryBuilder(String name) {
+        this.name = name;
     }
 
     /**
@@ -70,15 +50,7 @@ public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilde
     }
 
     /**
-     * Returns true if the missing filter will include documents where the field contains a null value, otherwise
-     * these documents will not be included.
-     */
-    public boolean nullValue() {
-        return this.nullValue;
-    }
-
-    /**
-     * Should the missing filter include documents where the field doesn't exist in the docs.
+     * Should the missing filter include documents where the field doesn't exists in the docs.
      * Defaults to <tt>true</tt>.
      */
     public MissingQueryBuilder existence(boolean existence) {
@@ -87,157 +59,26 @@ public class MissingQueryBuilder extends AbstractQueryBuilder<MissingQueryBuilde
     }
 
     /**
-     * Returns true if the missing filter will include documents where the field has no values, otherwise
-     * these documents will not be included.
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean existence() {
-        return this.existence;
+    public MissingQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("field", fieldPattern);
-        builder.field("null_value", nullValue);
-        builder.field("existence", existence);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return newFilter(context, fieldPattern, existence, nullValue);
-    }
-
-    public static Query newFilter(QueryShardContext context, String fieldPattern, boolean existence, boolean nullValue) {
-        if (!existence && !nullValue) {
-            throw new QueryShardException(context, "missing must have either existence, or null_value, or both set to true");
-        }
-
-        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType) context.mapperService().fullName(FieldNamesFieldMapper.NAME);
-        if (fieldNamesFieldType == null) {
-            // can only happen when no types exist, so no docs exist either
-            return Queries.newMatchNoDocsQuery();
-        }
-
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (fields.isEmpty()) {
-            if (existence) {
-                // if we ask for existence of fields, and we found none, then we should match on all
-                return Queries.newMatchAllQuery();
-            }
-            return null;
-        }
-
-        Query existenceFilter = null;
-        Query nullFilter = null;
-
-        if (existence) {
-            BooleanQuery boolFilter = new BooleanQuery();
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                Query filter = null;
-                if (fieldNamesFieldType.isEnabled()) {
-                    final String f;
-                    if (fieldType != null) {
-                        f = fieldType.names().indexName();
-                    } else {
-                        f = field;
-                    }
-                    filter = fieldNamesFieldType.termQuery(f, context);
-                }
-                // if _field_names are not indexed, we need to go the slow way
-                if (filter == null && fieldType != null) {
-                    filter = fieldType.rangeQuery(null, null, true, true);
-                }
-                if (filter == null) {
-                    filter = new TermRangeQuery(field, null, null, true, true);
-                }
-                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
-            }
-
-            existenceFilter = boolFilter;
-            existenceFilter = Queries.not(existenceFilter);;
-        }
-
-        if (nullValue) {
-            for (String field : fields) {
-                MappedFieldType fieldType = context.fieldMapper(field);
-                if (fieldType != null) {
-                    nullFilter = fieldType.nullValueQuery();
-                }
-            }
-        }
-
-        Query filter;
-        if (nullFilter != null) {
-            if (existenceFilter != null) {
-                BooleanQuery combined = new BooleanQuery();
-                combined.add(existenceFilter, BooleanClause.Occur.SHOULD);
-                combined.add(nullFilter, BooleanClause.Occur.SHOULD);
-                // cache the not filter as well, so it will be faster
-                filter = combined;
-            } else {
-                filter = nullFilter;
-            }
-        } else {
-            filter = existenceFilter;
+        builder.startObject(MissingQueryParser.NAME);
+        builder.field("field", name);
+        if (nullValue != null) {
+            builder.field("null_value", nullValue);
         }
-
-        if (filter == null) {
-            return null;
-        }
-
-        return new ConstantScoreQuery(filter);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldPattern)) {
-            validationException = addValidationError("missing must be provided with a [field]", validationException);
+        if (existence != null) {
+            builder.field("existence", existence);
         }
-        if (!existence && !nullValue) {
-            validationException = addValidationError("missing must have either existence, or null_value, or both set to true", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected MissingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MissingQueryBuilder missingQueryBuilder = new MissingQueryBuilder(in.readString());
-        missingQueryBuilder.nullValue = in.readBoolean();
-        missingQueryBuilder.existence = in.readBoolean();
-        return missingQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldPattern);
-        out.writeBoolean(nullValue);
-        out.writeBoolean(existence);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldPattern, nullValue, existence);
-    }
-
-    @Override
-    protected boolean doEquals(MissingQueryBuilder other) {
-        return Objects.equals(fieldPattern, other.fieldPattern) &&
-                Objects.equals(nullValue, other.nullValue) &&
-                Objects.equals(existence, other.existence);
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
index 1dd6bd1..6ef19d7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MissingQueryParser.java
@@ -19,15 +19,29 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.internal.FieldNamesFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
 
 import java.io.IOException;
+import java.util.Collection;
 
 /**
- * Parser for missing query
+ *
  */
-public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
+public class MissingQueryParser implements QueryParser {
+
+    public static final String NAME = "missing";
+    public static final boolean DEFAULT_NULL_VALUE = false;
+    public static final boolean DEFAULT_EXISTENCE_VALUE = true;
 
     @Inject
     public MissingQueryParser() {
@@ -35,18 +49,17 @@ public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{MissingQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public MissingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldPattern = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        boolean nullValue = MissingQueryBuilder.DEFAULT_NULL_VALUE;
-        boolean existence = MissingQueryBuilder.DEFAULT_EXISTENCE_VALUE;
+        boolean nullValue = DEFAULT_NULL_VALUE;
+        boolean existence = DEFAULT_EXISTENCE_VALUE;
 
         XContentParser.Token token;
         String currentFieldName = null;
@@ -62,8 +75,6 @@ public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
                     existence = parser.booleanValue();
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[missing] query does not support [" + currentFieldName + "]");
                 }
@@ -73,15 +84,98 @@ public class MissingQueryParser extends BaseQueryParser<MissingQueryBuilder> {
         if (fieldPattern == null) {
             throw new QueryParsingException(parseContext, "missing must be provided with a [field]");
         }
-        return new MissingQueryBuilder(fieldPattern)
-                .nullValue(nullValue)
-                .existence(existence)
-                .boost(boost)
-                .queryName(queryName);
+
+        return newFilter(parseContext, fieldPattern, existence, nullValue, queryName);
     }
 
-    @Override
-    public MissingQueryBuilder getBuilderPrototype() {
-        return MissingQueryBuilder.PROTOTYPE;
+    public static Query newFilter(QueryParseContext parseContext, String fieldPattern, boolean existence, boolean nullValue, String queryName) {
+        if (!existence && !nullValue) {
+            throw new QueryParsingException(parseContext, "missing must have either existence, or null_value, or both set to true");
+        }
+
+        final FieldNamesFieldMapper.FieldNamesFieldType fieldNamesFieldType = (FieldNamesFieldMapper.FieldNamesFieldType)parseContext.mapperService().fullName(FieldNamesFieldMapper.NAME);
+        if (fieldNamesFieldType == null) {
+            // can only happen when no types exist, so no docs exist either
+            return Queries.newMatchNoDocsQuery();
+        }
+
+        ObjectMapper objectMapper = parseContext.getObjectMapper(fieldPattern);
+        if (objectMapper != null) {
+            // automatic make the object mapper pattern
+            fieldPattern = fieldPattern + ".*";
+        }
+
+        Collection<String> fields = parseContext.simpleMatchToIndexNames(fieldPattern);
+        if (fields.isEmpty()) {
+            if (existence) {
+                // if we ask for existence of fields, and we found none, then we should match on all
+                return Queries.newMatchAllQuery();
+            }
+            return null;
+        }
+
+        Query existenceFilter = null;
+        Query nullFilter = null;
+
+        if (existence) {
+            BooleanQuery boolFilter = new BooleanQuery();
+            for (String field : fields) {
+                MappedFieldType fieldType = parseContext.fieldMapper(field);
+                Query filter = null;
+                if (fieldNamesFieldType.isEnabled()) {
+                    final String f;
+                    if (fieldType != null) {
+                        f = fieldType.names().indexName();
+                    } else {
+                        f = field;
+                    }
+                    filter = fieldNamesFieldType.termQuery(f, parseContext);
+                }
+                // if _field_names are not indexed, we need to go the slow way
+                if (filter == null && fieldType != null) {
+                    filter = fieldType.rangeQuery(null, null, true, true);
+                }
+                if (filter == null) {
+                    filter = new TermRangeQuery(field, null, null, true, true);
+                }
+                boolFilter.add(filter, BooleanClause.Occur.SHOULD);
+            }
+
+            existenceFilter = boolFilter;
+            existenceFilter = Queries.not(existenceFilter);;
+        }
+
+        if (nullValue) {
+            for (String field : fields) {
+                MappedFieldType fieldType = parseContext.fieldMapper(field);
+                if (fieldType != null) {
+                    nullFilter = fieldType.nullValueQuery();
+                }
+            }
+        }
+
+        Query filter;
+        if (nullFilter != null) {
+            if (existenceFilter != null) {
+                BooleanQuery combined = new BooleanQuery();
+                combined.add(existenceFilter, BooleanClause.Occur.SHOULD);
+                combined.add(nullFilter, BooleanClause.Occur.SHOULD);
+                // cache the not filter as well, so it will be faster
+                filter = combined;
+            } else {
+                filter = nullFilter;
+            }
+        } else {
+            filter = existenceFilter;
+        }
+
+        if (filter == null) {
+            return null;
+        }
+
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, existenceFilter);
+        }
+        return new ConstantScoreQuery(filter);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
index fbd13ea..19d65d9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java
@@ -23,11 +23,7 @@ import org.elasticsearch.action.get.MultiGetRequest;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.common.xcontent.XContentType;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.index.VersionType;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
 
@@ -41,7 +37,7 @@ import java.util.Locale;
  * A more like this query that finds documents that are "like" the provided {@link #likeText(String)}
  * which is checked against the fields the query is constructed with.
  */
-public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQueryBuilder> {
+public class MoreLikeThisQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MoreLikeThisQueryBuilder> {
 
     /**
      * A single get item. Pure delegate to multi get.
@@ -132,8 +128,6 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         }
     }
 
-    public static final String NAME = "mlt";
-
     private final String[] fields;
     private List<Item> docs = new ArrayList<>();
     private List<Item> unlikeDocs = new ArrayList<>();
@@ -147,10 +141,10 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
     private int minWordLength = -1;
     private int maxWordLength = -1;
     private float boostTerms = -1;
+    private float boost = -1;
     private String analyzer;
     private Boolean failOnUnsupportedField;
-
-    static final MoreLikeThisQueryBuilder PROTOTYPE = new MoreLikeThisQueryBuilder();
+    private String queryName;
 
     /**
      * Constructs a new more like this query which uses the "_all" field.
@@ -346,6 +340,12 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
+    @Override
+    public MoreLikeThisQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
      * Whether to fail or return no result when this query is run against a field which is not supported such as binary/numeric fields.
      */
@@ -354,10 +354,18 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MoreLikeThisQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         String likeFieldName = MoreLikeThisQueryParser.Fields.LIKE.getPreferredName();
-        builder.startObject(NAME);
+        builder.startObject(MoreLikeThisQueryParser.NAME);
         if (fields != null) {
             builder.startArray("fields");
             for (String field : fields) {
@@ -404,21 +412,21 @@ public class MoreLikeThisQueryBuilder extends AbstractQueryBuilder<MoreLikeThisQ
         if (boostTerms != -1) {
             builder.field(MoreLikeThisQueryParser.Fields.BOOST_TERMS.getPreferredName(), boostTerms);
         }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
         if (failOnUnsupportedField != null) {
             builder.field(MoreLikeThisQueryParser.Fields.FAIL_ON_UNSUPPORTED_FIELD.getPreferredName(), failOnUnsupportedField);
         }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (include != null) {
             builder.field("include", include);
         }
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
index 6f505c5..d56ce40 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.query;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
-
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.queries.TermsQuery;
 import org.apache.lucene.search.BooleanClause;
@@ -54,8 +53,9 @@ import static org.elasticsearch.index.mapper.Uid.createUidAsBytes;
 /**
  *
  */
-public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
+public class MoreLikeThisQueryParser implements QueryParser {
 
+    public static final String NAME = "mlt";
     private MoreLikeThisFetchService fetchService = null;
 
     public static class Fields {
@@ -88,16 +88,15 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{MoreLikeThisQueryBuilder.NAME, "more_like_this", "moreLikeThis"};
+        return new String[]{NAME, "more_like_this", "moreLikeThis"};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         MoreLikeThisQuery mltQuery = new MoreLikeThisQuery();
-        mltQuery.setSimilarity(context.searchSimilarity());
+        mltQuery.setSimilarity(parseContext.searchSimilarity());
         Analyzer analyzer = null;
         List<String> moreLikeFields = null;
         boolean failOnUnsupportedField = true;
@@ -144,7 +143,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.MINIMUM_SHOULD_MATCH)) {
                     mltQuery.setMinimumShouldMatch(parser.text());
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzer = context.analysisService().analyzer(parser.text());
+                    analyzer = parseContext.analysisService().analyzer(parser.text());
                 } else if ("boost".equals(currentFieldName)) {
                     mltQuery.setBoost(parser.floatValue());
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.FAIL_ON_UNSUPPORTED_FIELD)) {
@@ -167,7 +166,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
                     moreLikeFields = Lists.newLinkedList();
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                         String field = parser.text();
-                        MappedFieldType fieldType = context.fieldMapper(field);
+                        MappedFieldType fieldType = parseContext.fieldMapper(field);
                         moreLikeFields.add(fieldType == null ? field : fieldType.names().indexName());
                     }
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, Fields.DOCUMENT_IDS)) {
@@ -216,14 +215,14 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
         // set analyzer
         if (analyzer == null) {
-            analyzer = context.mapperService().searchAnalyzer();
+            analyzer = parseContext.mapperService().searchAnalyzer();
         }
         mltQuery.setAnalyzer(analyzer);
 
         // set like text fields
         boolean useDefaultField = (moreLikeFields == null);
         if (useDefaultField) {
-            moreLikeFields = Lists.newArrayList(context.defaultField());
+            moreLikeFields = Lists.newArrayList(parseContext.defaultField());
         }
         // possibly remove unsupported fields
         removeUnsupportedFields(moreLikeFields, analyzer, failOnUnsupportedField);
@@ -234,7 +233,7 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
 
         // support for named query
         if (queryName != null) {
-            context.addNamedQuery(queryName, mltQuery);
+            parseContext.addNamedQuery(queryName, mltQuery);
         }
 
         // handle like texts
@@ -258,12 +257,12 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
                     item.index(parseContext.index().name());
                 }
                 if (item.type() == null) {
-                    if (context.queryTypes().size() > 1) {
+                    if (parseContext.queryTypes().size() > 1) {
                         throw new QueryParsingException(parseContext,
                                     "ambiguous type for item with id: " + item.id()
                                 + " and index: " + item.index());
                     } else {
-                        item.type(context.queryTypes().iterator().next());
+                        item.type(parseContext.queryTypes().iterator().next());
                     }
                 }
                 // default fields if not present but don't override for artificial docs
@@ -356,9 +355,4 @@ public class MoreLikeThisQueryParser extends BaseQueryParserTemp {
             boolQuery.add(query, BooleanClause.Occur.MUST_NOT);
         }
     }
-
-    @Override
-    public MoreLikeThisQueryBuilder getBuilderPrototype() {
-        return MoreLikeThisQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
index bdcd886..1f7f960 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java
@@ -21,7 +21,6 @@ package org.elasticsearch.index.query;
 
 import com.carrotsearch.hppc.ObjectFloatHashMap;
 import com.google.common.collect.Lists;
-
 import org.elasticsearch.ElasticsearchParseException;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
@@ -37,9 +36,7 @@ import java.util.Locale;
 /**
  * Same as {@link MatchQueryBuilder} but supports multiple fields.
  */
-public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQueryBuilder> {
-
-    public static final String NAME = "multi_match";
+public class MultiMatchQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<MultiMatchQueryBuilder> {
 
     private final Object text;
 
@@ -48,10 +45,12 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
 
     private MultiMatchQueryBuilder.Type type;
 
-    private Operator operator;
+    private MatchQueryBuilder.Operator operator;
 
     private String analyzer;
 
+    private Float boost;
+
     private Integer slop;
 
     private Fuzziness fuzziness;
@@ -76,7 +75,8 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
 
     private MatchQueryBuilder.ZeroTermsQuery zeroTermsQuery = null;
 
-    static final MultiMatchQueryBuilder PROTOTYPE = new MultiMatchQueryBuilder(null);
+    private String queryName;
+
 
     public enum Type {
 
@@ -143,7 +143,7 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
                 }
             }
             if (type == null) {
-                throw new ElasticsearchParseException("failed to parse [{}] query type [{}]. unknown type.", NAME, value);
+                throw new ElasticsearchParseException("failed to parse [{}] query type [{}]. unknown type.", MultiMatchQueryParser.NAME, value);
             }
             return type;
         }
@@ -197,7 +197,7 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
     /**
      * Sets the operator to use when using a boolean query. Defaults to <tt>OR</tt>.
      */
-    public MultiMatchQueryBuilder operator(Operator operator) {
+    public MultiMatchQueryBuilder operator(MatchQueryBuilder.Operator operator) {
         this.operator = operator;
         return this;
     }
@@ -212,6 +212,15 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
     }
 
     /**
+     * Set the boost to apply to the query.
+     */
+    @Override
+    public MultiMatchQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * Set the phrase slop if evaluated to a phrase query type.
      */
     public MultiMatchQueryBuilder slop(int slop) {
@@ -308,9 +317,17 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public MultiMatchQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(MultiMatchQueryParser.NAME);
 
         builder.field("query", text);
         builder.startArray("fields");
@@ -332,6 +349,9 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (slop != null) {
             builder.field("slop", slop);
         }
@@ -374,13 +394,11 @@ public class MultiMatchQueryBuilder extends AbstractQueryBuilder<MultiMatchQuery
             builder.field("zero_terms_query", zeroTermsQuery.toString());
         }
 
-        printBoostAndQueryName(builder);
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
         builder.endObject();
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
index fcd79d8..5922f52 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java
@@ -21,6 +21,7 @@ package org.elasticsearch.index.query;
 
 import com.google.common.collect.Maps;
 
+import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.regex.Regex;
@@ -36,7 +37,9 @@ import java.util.Map;
 /**
  * Same as {@link MatchQueryParser} but has support for multiple fields.
  */
-public class MultiMatchQueryParser extends BaseQueryParserTemp {
+public class MultiMatchQueryParser implements QueryParser {
+
+    public static final String NAME = "multi_match";
 
     @Inject
     public MultiMatchQueryParser() {
@@ -45,20 +48,19 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
     @Override
     public String[] names() {
         return new String[]{
-                MultiMatchQueryBuilder.NAME, "multiMatch"
+                NAME, "multiMatch"
         };
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         Float tieBreaker = null;
         MultiMatchQueryBuilder.Type type = null;
-        MultiMatchQuery multiMatchQuery = new MultiMatchQuery(context);
+        MultiMatchQuery multiMatchQuery = new MultiMatchQuery(parseContext);
         String minimumShouldMatch = null;
         Map<String, Float> fieldNameWithBoosts = Maps.newHashMap();
         String queryName = null;
@@ -71,12 +73,12 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
             } else if ("fields".equals(currentFieldName)) {
                 if (token == XContentParser.Token.START_ARRAY) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        extractFieldAndBoost(context, parser, fieldNameWithBoosts);
+                        extractFieldAndBoost(parseContext, parser, fieldNameWithBoosts);
                     }
                 } else if (token.isValue()) {
-                    extractFieldAndBoost(context, parser, fieldNameWithBoosts);
+                    extractFieldAndBoost(parseContext, parser, fieldNameWithBoosts);
                 } else {
-                    throw new QueryParsingException(parseContext, "[" + MultiMatchQueryBuilder.NAME + "] query does not support [" + currentFieldName + "]");
+                    throw new QueryParsingException(parseContext, "[" + NAME + "] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("query".equals(currentFieldName)) {
@@ -85,8 +87,8 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
                     type = MultiMatchQueryBuilder.Type.parse(parser.text(), parseContext.parseFieldMatcher());
                 } else if ("analyzer".equals(currentFieldName)) {
                     String analyzer = parser.text();
-                    if (context.analysisService().analyzer(analyzer) == null) {
-                        throw new QueryParsingException(parseContext, "[" + MultiMatchQueryBuilder.NAME + "] analyzer [" + parser.text() + "] not found");
+                    if (parseContext.analysisService().analyzer(analyzer) == null) {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] analyzer [" + parser.text() + "] not found");
                     }
                     multiMatchQuery.setAnalyzer(analyzer);
                 } else if ("boost".equals(currentFieldName)) {
@@ -100,7 +102,15 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
                 } else if ("max_expansions".equals(currentFieldName) || "maxExpansions".equals(currentFieldName)) {
                     multiMatchQuery.setMaxExpansions(parser.intValue());
                 } else if ("operator".equals(currentFieldName)) {
-                    multiMatchQuery.setOccur(Operator.fromString(parser.text()).toBooleanClauseOccur());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        multiMatchQuery.setOccur(BooleanClause.Occur.SHOULD);
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        multiMatchQuery.setOccur(BooleanClause.Occur.MUST);
+                    } else {
+                        throw new QueryParsingException(parseContext, "text query requires operator to be either 'and' or 'or', not [" + op
+                                + "]");
+                    }
                 } else if ("minimum_should_match".equals(currentFieldName) || "minimumShouldMatch".equals(currentFieldName)) {
                     minimumShouldMatch = parser.textOrNull();
                 } else if ("fuzzy_rewrite".equals(currentFieldName) || "fuzzyRewrite".equals(currentFieldName)) {
@@ -157,12 +167,12 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
 
         query.setBoost(boost);
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
 
-    private void extractFieldAndBoost(QueryShardContext context, XContentParser parser, Map<String, Float> fieldNameWithBoosts) throws IOException {
+    private void extractFieldAndBoost(QueryParseContext parseContext, XContentParser parser, Map<String, Float> fieldNameWithBoosts) throws IOException {
         String fField = null;
         Float fBoost = null;
         char[] fieldText = parser.textCharacters();
@@ -180,16 +190,11 @@ public class MultiMatchQueryParser extends BaseQueryParserTemp {
         }
 
         if (Regex.isSimpleMatchPattern(fField)) {
-            for (String field : context.mapperService().simpleMatchToIndexNames(fField)) {
+            for (String field : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
                 fieldNameWithBoosts.put(field, fBoost);
             }
         } else {
             fieldNameWithBoosts.put(fField, fBoost);
         }
     }
-
-    @Override
-    public MultiMatchQueryBuilder getBuilderPrototype() {
-        return MultiMatchQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
index 0e946d6..9c7383d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/MultiTermQueryBuilder.java
@@ -18,6 +18,6 @@
  */
 package org.elasticsearch.index.query;
 
-public interface MultiTermQueryBuilder<QB extends MultiTermQueryBuilder<QB>> extends QueryBuilder<QB> {
+public abstract class MultiTermQueryBuilder extends QueryBuilder {
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
index bb2e4b4..63b40dc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java
@@ -25,9 +25,7 @@ import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import java.io.IOException;
 import java.util.Objects;
 
-public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder> {
-
-    public static final String NAME = "nested";
+public class NestedQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<NestedQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
@@ -35,28 +33,39 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
 
     private String scoreMode;
 
-    private QueryInnerHitBuilder innerHit;
+    private float boost = 1.0f;
 
-    static final NestedQueryBuilder PROTOTYPE = new NestedQueryBuilder();
+    private String queryName;
+
+    private QueryInnerHitBuilder innerHit;
 
     public NestedQueryBuilder(String path, QueryBuilder queryBuilder) {
         this.path = path;
         this.queryBuilder = Objects.requireNonNull(queryBuilder);
     }
+    /**
+     * The score mode.
+     */
+    public NestedQueryBuilder scoreMode(String scoreMode) {
+        this.scoreMode = scoreMode;
+        return this;
+    }
 
     /**
-     * private constructor only used internally
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    private NestedQueryBuilder() {
-        this.path = null;
-        this.queryBuilder = null;
+    @Override
+    public NestedQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * The score mode.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public NestedQueryBuilder scoreMode(String scoreMode) {
-        this.scoreMode = scoreMode;
+    public NestedQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
@@ -70,14 +79,19 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(NestedQueryParser.NAME);
         builder.field("query");
         queryBuilder.toXContent(builder, params);
         builder.field("path", path);
         if (scoreMode != null) {
             builder.field("score_mode", scoreMode);
         }
-        printBoostAndQueryName(builder);
+        if (boost != 1.0f) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (innerHit != null) {
             builder.startObject("inner_hits");
             builder.value(innerHit);
@@ -86,8 +100,4 @@ public class NestedQueryBuilder extends AbstractQueryBuilder<NestedQueryBuilder>
         builder.endObject();
     }
 
-    @Override
-    public final String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
index 7e4b376..4dc71f9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java
@@ -37,8 +37,9 @@ import org.elasticsearch.search.internal.SubSearchContext;
 
 import java.io.IOException;
 
-public class NestedQueryParser extends BaseQueryParserTemp {
+public class NestedQueryParser implements QueryParser {
 
+    public static final String NAME = "nested";
     private static final ParseField FILTER_FIELD = new ParseField("filter").withAllDeprecated("query");
 
     private final InnerHitsQueryParserHelper innerHitsQueryParserHelper;
@@ -50,16 +51,15 @@ public class NestedQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{NestedQueryBuilder.NAME, Strings.toCamelCase(NestedQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        final ToBlockJoinQueryBuilder builder = new ToBlockJoinQueryBuilder(context);
+        final ToBlockJoinQueryBuilder builder = new ToBlockJoinQueryBuilder(parseContext);
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         ScoreMode scoreMode = ScoreMode.Avg;
         String queryName = null;
 
@@ -111,7 +111,7 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         if (joinQuery != null) {
             joinQuery.setBoost(boost);
             if (queryName != null) {
-                context.addNamedQuery(queryName, joinQuery);
+                parseContext.addNamedQuery(queryName, joinQuery);
             }
         }
         return joinQuery;
@@ -122,8 +122,8 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         private ScoreMode scoreMode;
         private Tuple<String, SubSearchContext> innerHits;
 
-        public ToBlockJoinQueryBuilder(QueryShardContext context) throws IOException {
-            super(context);
+        public ToBlockJoinQueryBuilder(QueryParseContext parseContext) throws IOException {
+            super(parseContext);
         }
 
         public void setScoreMode(ScoreMode scoreMode) {
@@ -147,14 +147,14 @@ public class NestedQueryParser extends BaseQueryParserTemp {
                     innerQuery = null;
                 }
             } else {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             if (innerHits != null) {
-                ParsedQuery parsedQuery = new ParsedQuery(innerQuery, shardContext.copyNamedQueries());
+                ParsedQuery parsedQuery = new ParsedQuery(innerQuery, parseContext.copyNamedQueries());
                 InnerHitsContext.NestedInnerHits nestedInnerHits = new InnerHitsContext.NestedInnerHits(innerHits.v2(), parsedQuery, null, getParentObjectMapper(), nestedObjectMapper);
                 String name = innerHits.v1() != null ? innerHits.v1() : path;
-                shardContext.addInnerHits(name, nestedInnerHits);
+                parseContext.addInnerHits(name, nestedInnerHits);
             }
 
             if (innerQuery != null) {
@@ -165,9 +165,4 @@ public class NestedQueryParser extends BaseQueryParserTemp {
         }
 
     }
-
-    @Override
-    public NestedQueryBuilder getBuilderPrototype() {
-        return NestedQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
index a26ebb7..c16cf64 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NotQueryBuilder.java
@@ -19,10 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
@@ -31,71 +27,29 @@ import java.util.Objects;
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  */
-public class NotQueryBuilder extends AbstractQueryBuilder<NotQueryBuilder> {
-
-    public static final String NAME = "not";
+public class NotQueryBuilder extends QueryBuilder {
 
     private final QueryBuilder filter;
 
-    static final NotQueryBuilder PROTOTYPE = new NotQueryBuilder(null);
+    private String queryName;
 
     public NotQueryBuilder(QueryBuilder filter) {
-        this.filter = filter;
+        this.filter = Objects.requireNonNull(filter);
     }
 
-    /**
-     * @return the query added to "not".
-     */
-    public QueryBuilder innerQuery() {
-        return this.filter;
+    public NotQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(NotQueryParser.NAME);
         builder.field("query");
         filter.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query luceneQuery = filter.toFilter(context);
-        if (luceneQuery == null) {
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return Queries.not(luceneQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(filter, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filter);
-    }
-
-    @Override
-    protected boolean doEquals(NotQueryBuilder other) {
-        return Objects.equals(filter, other.filter);
-    }
-
-    @Override
-    protected NotQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder queryBuilder = in.readQuery();
-        return new NotQueryBuilder(queryBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(filter);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
index 0bb38b9..68ffe43 100644
--- a/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/NotQueryParser.java
@@ -19,17 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
 
 /**
- * Parser for not query
+ *
  */
-public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
+public class NotQueryParser implements QueryParser {
 
+    public static final String NAME = "not";
     private static final ParseField QUERY_FIELD = new ParseField("filter", "query");
 
     @Inject
@@ -38,19 +41,18 @@ public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{NotQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public NotQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        QueryBuilder query = null;
+        Query query = null;
         boolean queryFound = false;
 
         String queryName = null;
         String currentFieldName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
@@ -59,22 +61,20 @@ public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
                 // skip
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if (parseContext.parseFieldMatcher().match(currentFieldName, QUERY_FIELD)) {
-                    query = parseContext.parseInnerFilterToQueryBuilder();
+                    query = parseContext.parseInnerFilter();
                     queryFound = true;
                 } else {
                     queryFound = true;
                     // its the filter, and the name is the field
-                    query = parseContext.parseInnerFilterToQueryBuilder(currentFieldName);
+                    query = parseContext.parseInnerFilter(currentFieldName);
                 }
             } else if (token == XContentParser.Token.START_ARRAY) {
                 queryFound = true;
                 // its the filter, and the name is the field
-                query = parseContext.parseInnerFilterToQueryBuilder(currentFieldName);
+                query = parseContext.parseInnerFilter(currentFieldName);
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else {
                     throw new QueryParsingException(parseContext, "[not] query does not support [" + currentFieldName + "]");
                 }
@@ -85,14 +85,14 @@ public class NotQueryParser extends BaseQueryParser<NotQueryBuilder> {
             throw new QueryParsingException(parseContext, "filter is required when using `not` query");
         }
 
-        NotQueryBuilder notQueryBuilder = new NotQueryBuilder(query);
-        notQueryBuilder.queryName(queryName);
-        notQueryBuilder.boost(boost);
-        return notQueryBuilder;
-    }
+        if (query == null) {
+            return null;
+        }
 
-    @Override
-    public NotQueryBuilder getBuilderPrototype() {
-        return NotQueryBuilder.PROTOTYPE;
+        Query notQuery = Queries.not(query);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, notQuery);
+        }
+        return notQuery;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/Operator.java b/core/src/main/java/org/elasticsearch/index/query/Operator.java
deleted file mode 100644
index ce143eb..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/Operator.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.index.query;
-
-import com.google.common.collect.Lists;
-import org.apache.lucene.search.BooleanClause;
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.io.stream.Writeable;
-
-import java.io.IOException;
-
-public enum Operator implements Writeable<Operator> {
-    OR(0), AND(1);
-
-    private final int ordinal;
-
-    private static final Operator PROTOTYPE = OR;
-
-    private Operator(int ordinal) {
-        this.ordinal = ordinal;
-    }
-
-    public BooleanClause.Occur toBooleanClauseOccur() {
-        switch (this) {
-            case OR:
-                return BooleanClause.Occur.SHOULD;
-            case AND:
-                return BooleanClause.Occur.MUST;
-            default:
-                throw Operator.newOperatorException(this.toString());
-        }
-    }
-
-    @Override
-    public Operator readFrom(StreamInput in) throws IOException {
-        int ord = in.readVInt();
-        for (Operator operator : Operator.values()) {
-            if (operator.ordinal == ord) {
-                return operator;
-            }
-        }
-        throw new ElasticsearchException("unknown serialized operator [" + ord + "]");
-    }
-
-    public static Operator readOperatorFrom(StreamInput in) throws IOException {
-        return PROTOTYPE.readFrom(in);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        out.writeVInt(this.ordinal);
-    }
-
-    public static Operator fromString(String op) {
-        for (Operator operator : Operator.values()) {
-            if (operator.name().equalsIgnoreCase(op)) {
-                return operator;
-            }
-        }
-        throw Operator.newOperatorException(op);
-    }
-
-    private static IllegalArgumentException newOperatorException(String op) {
-        return new IllegalArgumentException("operator needs to be either " + Lists.newArrayList(Operator.values()) +
-                ", but not [" + op + "]");
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
index 1a53771..19ad3e4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/OrQueryBuilder.java
@@ -20,32 +20,22 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.collect.Lists;
-
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.List;
-import java.util.Objects;
 
 /**
  * A filter that matches documents matching boolean combinations of other filters.
  * @deprecated Use {@link BoolQueryBuilder} instead
  */
 @Deprecated
-public class OrQueryBuilder extends AbstractQueryBuilder<OrQueryBuilder> {
-
-    public static final String NAME = "or";
+public class OrQueryBuilder extends QueryBuilder {
 
-    private final ArrayList<QueryBuilder> filters = Lists.newArrayList();
+    private ArrayList<QueryBuilder> filters = Lists.newArrayList();
 
-    static final OrQueryBuilder PROTOTYPE = new OrQueryBuilder();
+    private String queryName;
 
     public OrQueryBuilder(QueryBuilder... filters) {
         Collections.addAll(this.filters, filters);
@@ -53,87 +43,28 @@ public class OrQueryBuilder extends AbstractQueryBuilder<OrQueryBuilder> {
 
     /**
      * Adds a filter to the list of filters to "or".
-     * No <tt>null</tt> value allowed.
      */
     public OrQueryBuilder add(QueryBuilder filterBuilder) {
         filters.add(filterBuilder);
         return this;
     }
 
-    /**
-     * @return the list of queries added to "or".
-     */
-    public List<QueryBuilder> innerQueries() {
-        return this.filters;
+    public OrQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(OrQueryParser.NAME);
         builder.startArray("filters");
         for (QueryBuilder filter : filters) {
             filter.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        if (filters.isEmpty()) {
-            // no filters provided, this should be ignored upstream
-            return null;
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-
-        BooleanQuery query = new BooleanQuery();
-        for (QueryBuilder f : filters) {
-            Query innerQuery = f.toFilter(context);
-            // ignore queries that are null
-            if (innerQuery != null) {
-                query.add(innerQuery, Occur.SHOULD);
-            }
-        }
-        if (query.clauses().isEmpty()) {
-            // no inner lucene query exists, ignore upstream
-            return null;
-        }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQueries(filters, null);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(filters);
-    }
-
-    @Override
-    protected boolean doEquals(OrQueryBuilder other) {
-        return Objects.equals(filters, other.filters);
-    }
-
-    @Override
-    protected OrQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        OrQueryBuilder orQueryBuilder = new OrQueryBuilder();
-        List<QueryBuilder> queryBuilders = readQueries(in);
-        for (QueryBuilder queryBuilder : queryBuilders) {
-            orQueryBuilder.add(queryBuilder);
-        }
-        return orQueryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, filters);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
index fb6e935..acc55e5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/OrQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
@@ -28,11 +31,12 @@ import java.util.ArrayList;
 import static com.google.common.collect.Lists.newArrayList;
 
 /**
- * Parser for or query
- * @deprecated use bool query instead
+ *
  */
 @Deprecated
-public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
+public class OrQueryParser implements QueryParser {
+
+    public static final String NAME = "or";
 
     @Inject
     public OrQueryParser() {
@@ -40,24 +44,23 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{OrQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public OrQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        final ArrayList<QueryBuilder> queries = newArrayList();
+        ArrayList<Query> queries = newArrayList();
         boolean queriesFound = false;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_ARRAY) {
             while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                 queriesFound = true;
-                QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
+                Query filter = parseContext.parseInnerFilter();
                 if (filter != null) {
                     queries.add(filter);
                 }
@@ -70,7 +73,7 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
                     if ("filters".equals(currentFieldName)) {
                         queriesFound = true;
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
+                            Query filter = parseContext.parseInnerFilter();
                             if (filter != null) {
                                 queries.add(filter);
                             }
@@ -78,7 +81,7 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
                     } else {
                         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
                             queriesFound = true;
-                            QueryBuilder filter = parseContext.parseInnerFilterToQueryBuilder();
+                            Query filter = parseContext.parseInnerFilter();
                             if (filter != null) {
                                 queries.add(filter);
                             }
@@ -87,8 +90,6 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
                 } else if (token.isValue()) {
                     if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
-                    } else if ("boost".equals(currentFieldName)) {
-                        boost = parser.floatValue();
                     } else {
                         throw new QueryParsingException(parseContext, "[or] query does not support [" + currentFieldName + "]");
                     }
@@ -100,17 +101,17 @@ public class OrQueryParser extends BaseQueryParser<OrQueryBuilder> {
             throw new QueryParsingException(parseContext, "[or] query requires 'filters' to be set on it'");
         }
 
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        for (QueryBuilder query : queries) {
-            orQuery.add(query);
+        if (queries.isEmpty()) {
+            return null;
         }
-        orQuery.queryName(queryName);
-        orQuery.boost(boost);
-        return orQuery;
-    }
 
-    @Override
-    public OrQueryBuilder getBuilderPrototype() {
-        return OrQueryBuilder.PROTOTYPE;
+        BooleanQuery query = new BooleanQuery();
+        for (Query f : queries) {
+            query.add(f, Occur.SHOULD);
+        }
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
index a49580c..e0e5b2f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java
@@ -19,53 +19,44 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that matches documents containing terms with a specified prefix.
  */
-public class PrefixQueryBuilder extends AbstractQueryBuilder<PrefixQueryBuilder> implements MultiTermQueryBuilder<PrefixQueryBuilder> {
+public class PrefixQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<PrefixQueryBuilder> {
 
-    public static final String NAME = "prefix";
+    private final String name;
 
-    private final String fieldName;
+    private final String prefix;
 
-    private final String value;
+    private float boost = -1;
 
     private String rewrite;
 
-    static final PrefixQueryBuilder PROTOTYPE = new PrefixQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * A Query that matches documents containing terms with a specified prefix.
      *
-     * @param fieldName The name of the field
-     * @param value The prefix query
+     * @param name   The name of the field
+     * @param prefix The prefix query
      */
-    public PrefixQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
+    public PrefixQueryBuilder(String name, String prefix) {
+        this.name = name;
+        this.prefix = prefix;
     }
 
-    public String fieldName() {
-        return this.fieldName;
-    }
-
-    public String value() {
-        return this.value;
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public PrefixQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public PrefixQueryBuilder rewrite(String rewrite) {
@@ -73,83 +64,33 @@ public class PrefixQueryBuilder extends AbstractQueryBuilder<PrefixQueryBuilder>
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public PrefixQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("prefix", this.value);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-
-        Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.prefixQuery(value, method, context);
-        }
-        if (query == null) {
-            PrefixQuery prefixQuery = new PrefixQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
-            if (method != null) {
-                prefixQuery.setRewriteMethod(method);
+        builder.startObject(PrefixQueryParser.NAME);
+        if (boost == -1 && rewrite == null && queryName == null) {
+            builder.field(name, prefix);
+        } else {
+            builder.startObject(name);
+            builder.field("prefix", prefix);
+            if (boost != -1) {
+                builder.field("boost", boost);
             }
-            query = prefixQuery;
-        }
-
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.value == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
+            if (rewrite != null) {
+                builder.field("rewrite", rewrite);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-        return validationException;
-    }
-
-    @Override
-    protected PrefixQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        PrefixQueryBuilder prefixQueryBuilder = new PrefixQueryBuilder(in.readString(), in.readString());
-        prefixQueryBuilder.rewrite = in.readOptionalString();
-        return prefixQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    protected final int doHashCode() {
-        return Objects.hash(fieldName, value, rewrite);
-    }
-
-    @Override
-    protected boolean doEquals(PrefixQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
index 3eae400..56387d9 100644
--- a/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java
@@ -19,15 +19,24 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for prefix query
+ *
  */
-public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
+public class PrefixQueryParser implements QueryParser {
+
+    public static final String NAME = "prefix";
 
     @Inject
     public PrefixQueryParser() {
@@ -35,19 +44,19 @@ public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{PrefixQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public PrefixQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = parser.currentName();
-        String value = null;
-        String rewrite = null;
-
+        String rewriteMethod = null;
         String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        String value = null;
+        float boost = 1.0f;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -68,7 +77,7 @@ public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
                         } else if ("boost".equals(currentFieldName)) {
                             boost = parser.floatValue();
                         } else if ("rewrite".equals(currentFieldName)) {
-                            rewrite = parser.textOrNull();
+                            rewriteMethod = parser.textOrNull();
                         } else {
                             throw new QueryParsingException(parseContext, "[regexp] query does not support [" + currentFieldName + "]");
                         }
@@ -87,14 +96,25 @@ public class PrefixQueryParser extends BaseQueryParser<PrefixQueryBuilder> {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for prefix query");
         }
-        return new PrefixQueryBuilder(fieldName, value)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public PrefixQueryBuilder getBuilderPrototype() {
-        return PrefixQueryBuilder.PROTOTYPE;
+        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null);
+
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.prefixQuery(value, method, parseContext);
+        }
+        if (query == null) {
+            PrefixQuery prefixQuery = new PrefixQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
+            if (method != null) {
+                prefixQuery.setRewriteMethod(method);
+            }
+            query = prefixQuery;
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return  query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
index 3f69375..fa11d32 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilder.java
@@ -19,79 +19,25 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.elasticsearch.client.Requests;
-import org.elasticsearch.common.bytes.BytesReference;
-import org.elasticsearch.common.io.stream.NamedWriteable;
-import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.action.support.ToXContentToBytes;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentType;
 
 import java.io.IOException;
 
-public interface QueryBuilder<QB extends QueryBuilder> extends NamedWriteable<QB>, ToXContent {
+public abstract class QueryBuilder extends ToXContentToBytes {
 
-    /**
-     * Validate the query.
-     * @return a {@link QueryValidationException} containing error messages, {@code null} if query is valid.
-     * e.g. if fields that are needed to create the lucene query are missing.
-     */
-    QueryValidationException validate();
+    protected QueryBuilder() {
+        super(XContentType.JSON);
+    }
 
-    /**
-     * Converts this QueryBuilder to a lucene {@link Query}.
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * parent queries.
-     *
-     * @param context additional information needed to construct the queries
-     * @return the {@link Query} or <tt>null</tt> if this query should be ignored upstream
-     * @throws QueryShardException
-     * @throws IOException
-     */
-    Query toQuery(QueryShardContext context) throws IOException;
+    @Override
+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject();
+        doXContent(builder, params);
+        builder.endObject();
+        return builder;
+    }
 
-    /**
-     * Converts this QueryBuilder to an unscored lucene {@link Query} that acts as a filter.
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * parent queries.
-     *
-     * @param context additional information needed to construct the queries
-     * @return the {@link Query} or <tt>null</tt> if this query should be ignored upstream
-     * @throws QueryShardException
-     * @throws IOException
-     */
-    Query toFilter(QueryShardContext context) throws IOException;
-
-    /**
-     * Returns a {@link org.elasticsearch.common.bytes.BytesReference}
-     * containing the {@link ToXContent} output in binary format.
-     * Builds the request based on the default {@link XContentType}, either {@link Requests#CONTENT_TYPE} or provided as a constructor argument
-     */
-    //norelease once we move to serializing queries over the wire in Streamable format, this method shouldn't be needed anymore
-    BytesReference buildAsBytes();
-
-    /**
-     * Sets the arbitrary name to be assigned to the query (see named queries).
-     */
-    QB queryName(String queryName);
-
-    /**
-     * Returns the arbitrary name assigned to the query (see named queries).
-     */
-    String queryName();
-
-    /**
-     * Returns the boost for this query.
-     */
-    float boost();
-
-    /**
-     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
-     * weightings) have their score multiplied by the boost provided.
-     */
-    QB boost(float boost);
-
-    /**
-     * Returns the name that identifies uniquely the query
-     */
-    String getName();
+    protected abstract void doXContent(XContentBuilder builder, Params params) throws IOException;
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
index 75bc64b..fe2852d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryBuilders.java
@@ -59,11 +59,11 @@ public abstract class QueryBuilders {
     /**
      * Creates a common query for the provided field name and text.
      *
-     * @param fieldName The field name.
+     * @param name The field name.
      * @param text The query text (to be analyzed).
      */
-    public static CommonTermsQueryBuilder commonTermsQuery(String fieldName, Object text) {
-        return new CommonTermsQueryBuilder(fieldName, text);
+    public static CommonTermsQueryBuilder commonTermsQuery(String name, Object text) {
+        return new CommonTermsQueryBuilder(name, text);
     }
 
     /**
@@ -277,8 +277,8 @@ public abstract class QueryBuilders {
      * Unlike the "NOT" clause, this still selects documents that contain undesirable terms,
      * but reduces their overall score:
      */
-    public static BoostingQueryBuilder boostingQuery(QueryBuilder positiveQuery, QueryBuilder negativeQuery) {
-        return new BoostingQueryBuilder(positiveQuery, negativeQuery);
+    public static BoostingQueryBuilder boostingQuery() {
+        return new BoostingQueryBuilder();
     }
 
     /**
@@ -312,33 +312,26 @@ public abstract class QueryBuilders {
         return new SpanFirstQueryBuilder(match, end);
     }
 
-    public static SpanNearQueryBuilder spanNearQuery(int slop) {
-        return new SpanNearQueryBuilder(slop);
+    public static SpanNearQueryBuilder spanNearQuery() {
+        return new SpanNearQueryBuilder();
     }
 
-    public static SpanNotQueryBuilder spanNotQuery(SpanQueryBuilder include, SpanQueryBuilder exclude) {
-        return new SpanNotQueryBuilder(include, exclude);
+    public static SpanNotQueryBuilder spanNotQuery() {
+        return new SpanNotQueryBuilder();
     }
 
     public static SpanOrQueryBuilder spanOrQuery() {
         return new SpanOrQueryBuilder();
     }
 
-    /** Creates a new {@code span_within} builder.
-    * @param big the big clause, it must enclose {@code little} for a match.
-    * @param little the little clause, it must be contained within {@code big} for a match.
-    */
-    public static SpanWithinQueryBuilder spanWithinQuery(SpanQueryBuilder big, SpanQueryBuilder little) {
-        return new SpanWithinQueryBuilder(big, little);
+    /** Creates a new {@code span_within} builder. */
+    public static SpanWithinQueryBuilder spanWithinQuery() {
+        return new SpanWithinQueryBuilder();
     }
 
-    /**
-     * Creates a new {@code span_containing} builder.
-     * @param big the big clause, it must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
-     */
-    public static SpanContainingQueryBuilder spanContainingQuery(SpanQueryBuilder big, SpanQueryBuilder little) {
-        return new SpanContainingQueryBuilder(big, little);
+    /** Creates a new {@code span_containing} builder. */
+    public static SpanContainingQueryBuilder spanContainingQuery() {
+        return new SpanContainingQueryBuilder();
     }
 
     /**
@@ -600,10 +593,11 @@ public abstract class QueryBuilders {
     }
 
     /**
-     * A terms query that can extract the terms from another doc in an index.
+     * A terms lookup filter for the provided field name. A lookup terms filter can
+     * extract the terms to filter by from another doc in an index.
      */
-    public static TermsQueryBuilder termsLookupQuery(String name) {
-        return new TermsQueryBuilder(name, (Object[]) null);
+    public static TermsLookupQueryBuilder termsLookupQuery(String name) {
+        return new TermsLookupQueryBuilder(name);
     }
 
     /**
@@ -690,7 +684,7 @@ public abstract class QueryBuilders {
     public static GeohashCellQuery.Builder geoHashCellQuery(String name, String geohash, boolean neighbors) {
         return new GeohashCellQuery.Builder(name, geohash, neighbors);
     }
-
+    
     /**
      * A filter to filter based on a polygon defined by a set of locations  / points.
      *
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
index 6dc2d39..936e466 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java
@@ -19,14 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A filter that simply wraps a query.
@@ -34,13 +29,11 @@ import java.util.Objects;
  *             query as a filter directly.
  */
 @Deprecated
-public class QueryFilterBuilder extends AbstractQueryBuilder<QueryFilterBuilder> {
-
-    public static final String NAME = "query";
+public class QueryFilterBuilder extends QueryBuilder {
 
     private final QueryBuilder queryBuilder;
 
-    static final QueryFilterBuilder PROTOTYPE = new QueryFilterBuilder(null);
+    private String queryName;
 
     /**
      * A filter that simply wraps a query.
@@ -52,56 +45,26 @@ public class QueryFilterBuilder extends AbstractQueryBuilder<QueryFilterBuilder>
     }
 
     /**
-     * @return the query builder that is wrapped by this {@link QueryFilterBuilder}
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public QueryBuilder innerQuery() {
-        return this.queryBuilder;
+    public QueryFilterBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.field(NAME);
-        queryBuilder.toXContent(builder, params);
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // inner query builder can potentially be `null`, in that case we ignore it
-        Query innerQuery = this.queryBuilder.toQuery(context);
-        if (innerQuery == null) {
-            return null;
+        if (queryName == null) {
+            builder.field(QueryFilterParser.NAME);
+            queryBuilder.toXContent(builder, params);
+        } else {
+            builder.startObject(FQueryFilterParser.NAME);
+            builder.field("query");
+            queryBuilder.toXContent(builder, params);
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-        return new ConstantScoreQuery(innerQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        return validateInnerQuery(queryBuilder, null);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(queryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(QueryFilterBuilder other) {
-        return Objects.equals(queryBuilder, other.queryBuilder);
-    }
-
-    @Override
-    protected QueryFilterBuilder doReadFrom(StreamInput in) throws IOException {
-        QueryBuilder innerQueryBuilder = in.readQuery();
-        return new QueryFilterBuilder(innerQueryBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(queryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
index 03513ab..fdb9cb3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java
@@ -19,16 +19,16 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.inject.Inject;
 
 import java.io.IOException;
 
-/**
- * Parser for query filter
- * @deprecated use any query instead directly, possible since queries and filters are merged.
- */
 @Deprecated
-public class QueryFilterParser extends BaseQueryParser<QueryFilterBuilder> {
+public class QueryFilterParser implements QueryParser {
+
+    public static final String NAME = "query";
 
     @Inject
     public QueryFilterParser() {
@@ -36,16 +36,11 @@ public class QueryFilterParser extends BaseQueryParser<QueryFilterBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{QueryFilterBuilder.NAME};
-    }
-
-    @Override
-    public QueryFilterBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        return new QueryFilterBuilder(parseContext.parseInnerQueryBuilder());
+        return new String[]{NAME};
     }
 
     @Override
-    public QueryFilterBuilder getBuilderPrototype() {
-        return QueryFilterBuilder.PROTOTYPE;
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
+        return new ConstantScoreQuery(parseContext.parseInnerQuery());
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
index 90b45e3..4b12200 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParseContext.java
@@ -19,117 +19,207 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Maps;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.queryparser.classic.MapperQueryParser;
+import org.apache.lucene.queryparser.classic.QueryParserSettings;
+import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.Query;
+import org.apache.lucene.search.join.BitDocIdSetFilter;
+import org.apache.lucene.search.similarities.Similarity;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.ParseFieldMatcher;
+import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.Index;
-import org.elasticsearch.indices.query.IndicesQueriesRegistry;
+import org.elasticsearch.index.analysis.AnalysisService;
+import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.mapper.*;
+import org.elasticsearch.index.mapper.core.StringFieldMapper;
+import org.elasticsearch.index.mapper.object.ObjectMapper;
+import org.elasticsearch.index.query.support.NestedScope;
+import org.elasticsearch.index.similarity.SimilarityService;
+import org.elasticsearch.script.ScriptService;
+import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
+import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
+import java.util.*;
 
 public class QueryParseContext {
 
     private static final ParseField CACHE = new ParseField("_cache").withAllDeprecated("Elasticsearch makes its own caching decisions");
     private static final ParseField CACHE_KEY = new ParseField("_cache_key").withAllDeprecated("Filters are always used as cache keys");
 
-    private XContentParser parser;
+    private static ThreadLocal<String[]> typesContext = new ThreadLocal<>();
+
+    public static void setTypes(String[] types) {
+        typesContext.set(types);
+    }
+
+    public static String[] getTypes() {
+        return typesContext.get();
+    }
+
+    public static String[] setTypesWithPrevious(String[] types) {
+        String[] old = typesContext.get();
+        setTypes(types);
+        return old;
+    }
+
+    public static void removeTypes() {
+        typesContext.remove();
+    }
+
     private final Index index;
-    //norelease this flag is also used in the QueryShardContext, we need to make sure we set it there correctly in doToQuery()
+
+    private final Version indexVersionCreated;
+
+    private final IndexQueryParserService indexQueryParser;
+
+    private final Map<String, Query> namedQueries = Maps.newHashMap();
+
+    private final MapperQueryParser queryParser = new MapperQueryParser(this);
+
+    private XContentParser parser;
+
     private ParseFieldMatcher parseFieldMatcher;
 
-    //norelease this can eventually be deleted when context() method goes away
-    private final QueryShardContext shardContext;
-    private IndicesQueriesRegistry indicesQueriesRegistry;
+    private boolean allowUnmappedFields;
+
+    private boolean mapUnmappedFieldAsString;
 
-    public QueryParseContext(Index index, IndicesQueriesRegistry registry) {
+    private NestedScope nestedScope;
+
+    private boolean isFilter;
+
+    public QueryParseContext(Index index, IndexQueryParserService indexQueryParser) {
         this.index = index;
-        this.indicesQueriesRegistry = registry;
-        this.shardContext = null;
+        this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
+        this.indexQueryParser = indexQueryParser;
+    }
+
+    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
+        this.parseFieldMatcher = parseFieldMatcher;
     }
 
-    QueryParseContext(QueryShardContext context) {
-        this.shardContext = context;
-        this.index = context.index();
-        this.indicesQueriesRegistry = context.indexQueryParserService().indicesQueriesRegistry();
+    public ParseFieldMatcher parseFieldMatcher() {
+        return parseFieldMatcher;
     }
 
     public void reset(XContentParser jp) {
+        allowUnmappedFields = indexQueryParser.defaultAllowUnmappedFields();
         this.parseFieldMatcher = ParseFieldMatcher.EMPTY;
+        this.lookup = null;
         this.parser = jp;
+        this.namedQueries.clear();
+        this.nestedScope = new NestedScope();
+        this.isFilter = false;
     }
 
-    //norelease this is still used in BaseQueryParserTemp and FunctionScoreQueryParse, remove if not needed there anymore
-    @Deprecated
-    public QueryShardContext shardContext() {
-        return this.shardContext;
+    public Index index() {
+        return this.index;
+    }
+
+    public void parser(XContentParser parser) {
+        this.parser = parser;
     }
 
     public XContentParser parser() {
-        return this.parser;
+        return parser;
+    }
+    
+    public IndexQueryParserService indexQueryParserService() {
+        return indexQueryParser;
     }
 
-    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        this.parseFieldMatcher = parseFieldMatcher;
+    public AnalysisService analysisService() {
+        return indexQueryParser.analysisService;
     }
 
-    public boolean isDeprecatedSetting(String setting) {
-        return parseFieldMatcher.match(setting, CACHE) || parseFieldMatcher.match(setting, CACHE_KEY);
+    public ScriptService scriptService() {
+        return indexQueryParser.scriptService;
     }
 
-    public Index index() {
-        return this.index;
+    public MapperService mapperService() {
+        return indexQueryParser.mapperService;
     }
 
-    /**
-     * @deprecated replaced by calls to parseInnerFilterToQueryBuilder(String queryName) for the resulting queries
-     */
     @Nullable
-    @Deprecated
-    //norelease should be possible to remove after refactoring all queries
-    public Query parseInnerFilter(String queryName) throws IOException, QueryShardException {
-        assert this.shardContext != null;
-        QueryBuilder builder = parseInnerFilterToQueryBuilder(queryName);
-        return (builder != null) ? builder.toQuery(this.shardContext) : null;
+    public SimilarityService similarityService() {
+        return indexQueryParser.similarityService;
     }
 
-    /**
-     * @deprecated replaced by calls to parseInnerFilterToQueryBuilder() for the resulting queries
-     */
-    @Nullable
-    @Deprecated
-    //norelease should be possible to remove after refactoring all queries
-    public Query parseInnerFilter() throws QueryShardException, IOException {
-        assert this.shardContext != null;
-        QueryBuilder builder = parseInnerFilterToQueryBuilder();
-        Query result = null;
-        if (builder != null) {
-            result = builder.toQuery(this.shardContext);
+    public Similarity searchSimilarity() {
+        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
+    }
+
+    public String defaultField() {
+        return indexQueryParser.defaultField();
+    }
+
+    public boolean queryStringLenient() {
+        return indexQueryParser.queryStringLenient();
+    }
+
+    public MapperQueryParser queryParser(QueryParserSettings settings) {
+        queryParser.reset(settings);
+        return queryParser;
+    }
+
+    public BitDocIdSetFilter bitsetFilter(Filter filter) {
+        return indexQueryParser.bitsetFilterCache.getBitDocIdSetFilter(filter);
+    }
+
+    public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType mapper) {
+        return indexQueryParser.fieldDataService.getForField(mapper);
+    }
+
+    public void addNamedQuery(String name, Query query) {
+        if (query != null) {
+            namedQueries.put(name, query);
         }
-        return result;
+    }
+
+    public ImmutableMap<String, Query> copyNamedQueries() {
+        return ImmutableMap.copyOf(namedQueries);
+    }
+
+    public void combineNamedQueries(QueryParseContext context) {
+        namedQueries.putAll(context.namedQueries);
     }
 
     /**
-     * @deprecated replaced by calls to parseInnerQueryBuilder() for the resulting queries
+     * Return whether we are currently parsing a filter or a query.
      */
-    @Nullable
-    @Deprecated
-    //norelease should be possible to remove after refactoring all queries
-    public Query parseInnerQuery() throws IOException, QueryShardException {
-        QueryBuilder builder = parseInnerQueryBuilder();
-        Query result = null;
-        if (builder != null) {
-            result = builder.toQuery(this.shardContext);
+    public boolean isFilter() {
+        return isFilter;
+    }
+
+    public void addInnerHits(String name, InnerHitsContext.BaseInnerHits context) {
+        SearchContext sc = SearchContext.current();
+        if (sc == null) {
+            throw new QueryParsingException(this, "inner_hits unsupported");
         }
-        return result;
+
+        InnerHitsContext innerHitsContext;
+        if (sc.innerHits() == null) {
+            innerHitsContext = new InnerHitsContext(new HashMap<String, InnerHitsContext.BaseInnerHits>());
+            sc.innerHits(innerHitsContext);
+        } else {
+            innerHitsContext = sc.innerHits();
+        }
+        innerHitsContext.addInnerHitDefinition(name, context);
     }
 
-    /**
-     * @return a new QueryBuilder based on the current state of the parser
-     * @throws IOException
-     */
-    public QueryBuilder parseInnerQueryBuilder() throws IOException {
+    @Nullable
+    public Query parseInnerQuery() throws QueryParsingException, IOException {
         // move to START object
         XContentParser.Token token;
         if (parser.currentToken() != XContentParser.Token.START_OBJECT) {
@@ -141,7 +231,7 @@ public class QueryParseContext {
         token = parser.nextToken();
         if (token == XContentParser.Token.END_OBJECT) {
             // empty query
-            return EmptyQueryBuilder.PROTOTYPE;
+            return null;
         }
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new QueryParsingException(this, "[_na] query malformed, no field after start_object");
@@ -153,11 +243,11 @@ public class QueryParseContext {
             throw new QueryParsingException(this, "[_na] query malformed, no field after start_object");
         }
 
-        QueryParser queryParser = queryParser(queryName);
+        QueryParser queryParser = indexQueryParser.queryParser(queryName);
         if (queryParser == null) {
             throw new QueryParsingException(this, "No query registered for [" + queryName + "]");
         }
-        QueryBuilder result = queryParser.fromXContent(this);
+        Query result = queryParser.parse(this);
         if (parser.currentToken() == XContentParser.Token.END_OBJECT || parser.currentToken() == XContentParser.Token.END_ARRAY) {
             // if we are at END_OBJECT, move to the next one...
             parser.nextToken();
@@ -165,47 +255,138 @@ public class QueryParseContext {
         return result;
     }
 
-    /**
-     * @return a new QueryBuilder based on the current state of the parser, but does so that the inner query
-     * is parsed to a filter
-     * @throws IOException
-     */
     @Nullable
-    //norelease setting and checking the isFilter Flag should completely be moved to toQuery/toFilter after query refactoring
-    public QueryBuilder parseInnerFilterToQueryBuilder() throws IOException {
-        final boolean originalIsFilter = this.shardContext.isFilter;
+    public Query parseInnerFilter() throws QueryParsingException, IOException {
+        final boolean originalIsFilter = isFilter;
         try {
-            this.shardContext.isFilter = true;
-            return parseInnerQueryBuilder();
+            isFilter = true;
+            return parseInnerQuery();
         } finally {
-            this.shardContext.isFilter = originalIsFilter;
+            isFilter = originalIsFilter;
         }
     }
 
-    //norelease setting and checking the isFilter Flag should completely be moved to toQuery/toFilter after query refactoring
-    QueryBuilder parseInnerFilterToQueryBuilder(String queryName) throws IOException, QueryParsingException {
-        final boolean originalIsFilter = this.shardContext.isFilter;
+    public Query parseInnerFilter(String queryName) throws IOException, QueryParsingException {
+        final boolean originalIsFilter = isFilter;
         try {
-            this.shardContext.isFilter = true;
-            QueryParser queryParser = queryParser(queryName);
+            isFilter = true;
+            QueryParser queryParser = indexQueryParser.queryParser(queryName);
             if (queryParser == null) {
                 throw new QueryParsingException(this, "No query registered for [" + queryName + "]");
             }
-            return queryParser.fromXContent(this);
+            return queryParser.parse(this);
         } finally {
-            this.shardContext.isFilter = originalIsFilter;
+            isFilter = originalIsFilter;
         }
     }
 
-    public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
+    public Collection<String> simpleMatchToIndexNames(String pattern) {
+        return indexQueryParser.mapperService.simpleMatchToIndexNames(pattern, getTypes());
+    }
+
+    public MappedFieldType fieldMapper(String name) {
+        return failIfFieldMappingNotFound(name, indexQueryParser.mapperService.smartNameFieldType(name, getTypes()));
+    }
+
+    public ObjectMapper getObjectMapper(String name) {
+        return indexQueryParser.mapperService.getObjectMapper(name, getTypes());
+    }
+
+    /** Gets the search analyzer for the given field, or the default if there is none present for the field
+     * TODO: remove this by moving defaults into mappers themselves
+     */
+    public Analyzer getSearchAnalyzer(MappedFieldType fieldType) {
+        if (fieldType.searchAnalyzer() != null) {
+            return fieldType.searchAnalyzer();
+        }
+        return mapperService().searchAnalyzer();
     }
 
-    public void parser(XContentParser innerParser) {
-        this.parser = innerParser;
+    /** Gets the search quote nalyzer for the given field, or the default if there is none present for the field
+     * TODO: remove this by moving defaults into mappers themselves
+     */
+    public Analyzer getSearchQuoteAnalyzer(MappedFieldType fieldType) {
+        if (fieldType.searchQuoteAnalyzer() != null) {
+            return fieldType.searchQuoteAnalyzer();
+        }
+        return mapperService().searchQuoteAnalyzer();
     }
 
-    QueryParser queryParser(String name) {
-        return indicesQueriesRegistry.queryParsers().get(name);
+    public void setAllowUnmappedFields(boolean allowUnmappedFields) {
+        this.allowUnmappedFields = allowUnmappedFields;
     }
+
+    public void setMapUnmappedFieldAsString(boolean mapUnmappedFieldAsString) {
+        this.mapUnmappedFieldAsString = mapUnmappedFieldAsString;
+    }
+
+    private MappedFieldType failIfFieldMappingNotFound(String name, MappedFieldType fieldMapping) {
+        if (allowUnmappedFields) {
+            return fieldMapping;
+        } else if (mapUnmappedFieldAsString){
+            StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
+            // it would be better to pass the real index settings, but they are not easily accessible from here...
+            Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexQueryParser.getIndexCreatedVersion()).build();
+            return builder.build(new Mapper.BuilderContext(settings, new ContentPath(1))).fieldType();
+        } else {
+            Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion();
+            if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) {
+                throw new QueryParsingException(this, "Strict field resolution and no field mapping can be found for the field with name ["
+                        + name + "]");
+            } else {
+                return fieldMapping;
+            }
+        }
+    }
+
+    /**
+     * Returns the narrowed down explicit types, or, if not set, all types.
+     */
+    public Collection<String> queryTypes() {
+        String[] types = getTypes();
+        if (types == null || types.length == 0) {
+            return mapperService().types();
+        }
+        if (types.length == 1 && types[0].equals("_all")) {
+            return mapperService().types();
+        }
+        return Arrays.asList(types);
+    }
+
+    private SearchLookup lookup = null;
+
+    public SearchLookup lookup() {
+        SearchContext current = SearchContext.current();
+        if (current != null) {
+            return current.lookup();
+        }
+        if (lookup == null) {
+            lookup = new SearchLookup(mapperService(), indexQueryParser.fieldDataService, null);
+        }
+        return lookup;
+    }
+
+    public long nowInMillis() {
+        SearchContext current = SearchContext.current();
+        if (current != null) {
+            return current.nowInMillis();
+        }
+        return System.currentTimeMillis();
+    }
+
+    public NestedScope nestedScope() {
+        return nestedScope;
+    }
+
+    /**
+     * Return whether the setting is deprecated.
+     */
+    public boolean isDeprecatedSetting(String setting) {
+        return parseFieldMatcher.match(setting, CACHE) || parseFieldMatcher.match(setting, CACHE_KEY);
+    }
+
+    public Version indexVersionCreated() {
+        return indexVersionCreated;
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
index d54971b..eff585a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParser.java
@@ -25,10 +25,9 @@ import org.elasticsearch.common.Nullable;
 import java.io.IOException;
 
 /**
- * Defines a query parser that is able to read and parse a query object in {@link org.elasticsearch.common.xcontent.XContent}
- * format and create an internal object representing the query, implementing {@link QueryBuilder}, which can be streamed to other nodes.
+ *
  */
-public interface QueryParser<QB extends QueryBuilder<QB>> {
+public interface QueryParser {
 
     /**
      * The names this query parser is registered under.
@@ -36,33 +35,11 @@ public interface QueryParser<QB extends QueryBuilder<QB>> {
     String[] names();
 
     /**
-     * Parses the into a query from the current parser location. Will be at
-     * "START_OBJECT" location, and should end when the token is at the matching
-     * "END_OBJECT".
+     * Parses the into a query from the current parser location. Will be at "START_OBJECT" location,
+     * and should end when the token is at the matching "END_OBJECT".
      * <p/>
-     * Returns <tt>null</tt> if this query should be ignored in the context of
-     * the DSL.
+     * Returns <tt>null</tt> if this query should be ignored in the context of the DSL.
      */
-    //norelease can be removed in favour of fromXContent once search requests can be parsed on the coordinating node
     @Nullable
-    Query parse(QueryShardContext context) throws IOException, QueryParsingException;
-
-    /**
-     * Creates a new {@link QueryBuilder} from the query held by the {@link QueryShardContext}
-     * in {@link org.elasticsearch.common.xcontent.XContent} format
-     *
-     * @param parseContext
-     *            the input parse context. The state on the parser contained in
-     *            this context will be changed as a side effect of this method
-     *            call
-     * @return the new QueryBuilder
-     * @throws IOException
-     * @throws QueryParsingException
-     */
-    QB fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException;
-
-    /**
-     * @return an empty {@link QueryBuilder} instance for this parser that can be used for deserialization
-     */
-    QB getBuilderPrototype();
+    Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException;
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java b/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
index 80acae7..c606953 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryParsingException.java
@@ -31,8 +31,7 @@ import org.elasticsearch.rest.RestStatus;
 import java.io.IOException;
 
 /**
- * Exception that can be used when parsing queries with a given {@link QueryParseContext}.
- * Can contain information about location of the error.
+ *
  */
 public class QueryParsingException extends ElasticsearchException {
 
@@ -72,15 +71,9 @@ public class QueryParsingException extends ElasticsearchException {
         this.columnNumber = col;
     }
 
-    public QueryParsingException(StreamInput in) throws IOException{
-        super(in);
-        lineNumber = in.readInt();
-        columnNumber = in.readInt();
-    }
-
     /**
      * Line number of the location of the error
-     *
+     * 
      * @return the line number or -1 if unknown
      */
     public int getLineNumber() {
@@ -89,7 +82,7 @@ public class QueryParsingException extends ElasticsearchException {
 
     /**
      * Column number of the location of the error
-     *
+     * 
      * @return the column number or -1 if unknown
      */
     public int getColumnNumber() {
@@ -116,4 +109,11 @@ public class QueryParsingException extends ElasticsearchException {
         out.writeInt(lineNumber);
         out.writeInt(columnNumber);
     }
+
+    public QueryParsingException(StreamInput in) throws IOException{
+        super(in);
+        lineNumber = in.readInt();
+        columnNumber = in.readInt();
+    }
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
deleted file mode 100644
index a55d545..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
+++ /dev/null
@@ -1,329 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Maps;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryparser.classic.MapperQueryParser;
-import org.apache.lucene.queryparser.classic.QueryParserSettings;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.join.BitDocIdSetFilter;
-import org.apache.lucene.search.similarities.Similarity;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.common.Nullable;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.analysis.AnalysisService;
-import org.elasticsearch.index.fielddata.IndexFieldData;
-import org.elasticsearch.index.mapper.ContentPath;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.Mapper;
-import org.elasticsearch.index.mapper.MapperBuilders;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.core.StringFieldMapper;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.support.NestedScope;
-import org.elasticsearch.index.similarity.SimilarityService;
-import org.elasticsearch.script.ScriptService;
-import org.elasticsearch.search.fetch.innerhits.InnerHitsContext;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.search.lookup.SearchLookup;
-
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Context object used to create lucene queries on the shard level.
- */
-public class QueryShardContext {
-
-    private static ThreadLocal<String[]> typesContext = new ThreadLocal<>();
-
-    public static void setTypes(String[] types) {
-        typesContext.set(types);
-    }
-
-    public static String[] getTypes() {
-        return typesContext.get();
-    }
-
-    public static String[] setTypesWithPrevious(String[] types) {
-        String[] old = typesContext.get();
-        setTypes(types);
-        return old;
-    }
-
-    public static void removeTypes() {
-        typesContext.remove();
-    }
-
-    private final Index index;
-
-    private final Version indexVersionCreated;
-
-    private final IndexQueryParserService indexQueryParser;
-
-    private final Map<String, Query> namedQueries = Maps.newHashMap();
-
-    private final MapperQueryParser queryParser = new MapperQueryParser(this);
-
-    private ParseFieldMatcher parseFieldMatcher;
-
-    private boolean allowUnmappedFields;
-
-    private boolean mapUnmappedFieldAsString;
-
-    private NestedScope nestedScope;
-
-    //norelease this should be possible to remove once query context are completely separated
-    private QueryParseContext parseContext;
-
-    boolean isFilter;
-
-    public QueryShardContext(Index index, IndexQueryParserService indexQueryParser) {
-        this.index = index;
-        this.indexVersionCreated = Version.indexCreated(indexQueryParser.indexSettings());
-        this.indexQueryParser = indexQueryParser;
-        this.parseContext = new QueryParseContext(this);
-    }
-
-    public void parseFieldMatcher(ParseFieldMatcher parseFieldMatcher) {
-        this.parseFieldMatcher = parseFieldMatcher;
-    }
-
-    public ParseFieldMatcher parseFieldMatcher() {
-        return parseFieldMatcher;
-    }
-
-    private void reset() {
-        allowUnmappedFields = indexQueryParser.defaultAllowUnmappedFields();
-        this.parseFieldMatcher = ParseFieldMatcher.EMPTY;
-        this.lookup = null;
-        this.namedQueries.clear();
-        this.nestedScope = new NestedScope();
-    }
-
-    //norelease remove parser argument once query contexts are separated
-    public void reset(XContentParser jp) {
-        this.reset();
-        this.parseContext.reset(jp);
-    }
-
-    public Index index() {
-        return this.index;
-    }
-
-    public IndexQueryParserService indexQueryParserService() {
-        return indexQueryParser;
-    }
-
-    public AnalysisService analysisService() {
-        return indexQueryParser.analysisService;
-    }
-
-    public ScriptService scriptService() {
-        return indexQueryParser.scriptService;
-    }
-
-    public MapperService mapperService() {
-        return indexQueryParser.mapperService;
-    }
-
-    @Nullable
-    public SimilarityService similarityService() {
-        return indexQueryParser.similarityService;
-    }
-
-    public Similarity searchSimilarity() {
-        return indexQueryParser.similarityService != null ? indexQueryParser.similarityService.similarity() : null;
-    }
-
-    public String defaultField() {
-        return indexQueryParser.defaultField();
-    }
-
-    public boolean queryStringLenient() {
-        return indexQueryParser.queryStringLenient();
-    }
-
-    public MapperQueryParser queryParser(QueryParserSettings settings) {
-        queryParser.reset(settings);
-        return queryParser;
-    }
-
-    public BitDocIdSetFilter bitsetFilter(Filter filter) {
-        return indexQueryParser.bitsetFilterCache.getBitDocIdSetFilter(filter);
-    }
-
-    public <IFD extends IndexFieldData<?>> IFD getForField(MappedFieldType mapper) {
-        return indexQueryParser.fieldDataService.getForField(mapper);
-    }
-
-    public void addNamedQuery(String name, Query query) {
-        if (query != null) {
-            namedQueries.put(name, query);
-        }
-    }
-
-    public ImmutableMap<String, Query> copyNamedQueries() {
-        return ImmutableMap.copyOf(namedQueries);
-    }
-
-    public void combineNamedQueries(QueryShardContext context) {
-        namedQueries.putAll(context.namedQueries);
-    }
-
-    /**
-     * Return whether we are currently parsing a filter or a query.
-     */
-    public boolean isFilter() {
-        return isFilter;
-    }
-
-    public void addInnerHits(String name, InnerHitsContext.BaseInnerHits context) {
-        SearchContext sc = SearchContext.current();
-        if (sc == null) {
-            throw new QueryShardException(this, "inner_hits unsupported");
-        }
-
-        InnerHitsContext innerHitsContext;
-        if (sc.innerHits() == null) {
-            innerHitsContext = new InnerHitsContext(new HashMap<String, InnerHitsContext.BaseInnerHits>());
-            sc.innerHits(innerHitsContext);
-        } else {
-            innerHitsContext = sc.innerHits();
-        }
-        innerHitsContext.addInnerHitDefinition(name, context);
-    }
-
-    public Collection<String> simpleMatchToIndexNames(String pattern) {
-        return indexQueryParser.mapperService.simpleMatchToIndexNames(pattern, getTypes());
-    }
-
-    public MappedFieldType fieldMapper(String name) {
-        return failIfFieldMappingNotFound(name, indexQueryParser.mapperService.smartNameFieldType(name, getTypes()));
-    }
-
-    public ObjectMapper getObjectMapper(String name) {
-        return indexQueryParser.mapperService.getObjectMapper(name, getTypes());
-    }
-
-    /** Gets the search analyzer for the given field, or the default if there is none present for the field
-     * TODO: remove this by moving defaults into mappers themselves
-     */
-    public Analyzer getSearchAnalyzer(MappedFieldType fieldType) {
-        if (fieldType.searchAnalyzer() != null) {
-            return fieldType.searchAnalyzer();
-        }
-        return mapperService().searchAnalyzer();
-    }
-
-    /** Gets the search quote nalyzer for the given field, or the default if there is none present for the field
-     * TODO: remove this by moving defaults into mappers themselves
-     */
-    public Analyzer getSearchQuoteAnalyzer(MappedFieldType fieldType) {
-        if (fieldType.searchQuoteAnalyzer() != null) {
-            return fieldType.searchQuoteAnalyzer();
-        }
-        return mapperService().searchQuoteAnalyzer();
-    }
-
-    public void setAllowUnmappedFields(boolean allowUnmappedFields) {
-        this.allowUnmappedFields = allowUnmappedFields;
-    }
-
-    public void setMapUnmappedFieldAsString(boolean mapUnmappedFieldAsString) {
-        this.mapUnmappedFieldAsString = mapUnmappedFieldAsString;
-    }
-
-    private MappedFieldType failIfFieldMappingNotFound(String name, MappedFieldType fieldMapping) {
-        if (allowUnmappedFields) {
-            return fieldMapping;
-        } else if (mapUnmappedFieldAsString){
-            StringFieldMapper.Builder builder = MapperBuilders.stringField(name);
-            // it would be better to pass the real index settings, but they are not easily accessible from here...
-            Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, indexQueryParser.getIndexCreatedVersion()).build();
-            return builder.build(new Mapper.BuilderContext(settings, new ContentPath(1))).fieldType();
-        } else {
-            Version indexCreatedVersion = indexQueryParser.getIndexCreatedVersion();
-            if (fieldMapping == null && indexCreatedVersion.onOrAfter(Version.V_1_4_0_Beta1)) {
-                throw new QueryShardException(this, "Strict field resolution and no field mapping can be found for the field with name ["
-                        + name + "]");
-            } else {
-                return fieldMapping;
-            }
-        }
-    }
-
-    /**
-     * Returns the narrowed down explicit types, or, if not set, all types.
-     */
-    public Collection<String> queryTypes() {
-        String[] types = getTypes();
-        if (types == null || types.length == 0) {
-            return mapperService().types();
-        }
-        if (types.length == 1 && types[0].equals("_all")) {
-            return mapperService().types();
-        }
-        return Arrays.asList(types);
-    }
-
-    private SearchLookup lookup = null;
-
-    public SearchLookup lookup() {
-        SearchContext current = SearchContext.current();
-        if (current != null) {
-            return current.lookup();
-        }
-        if (lookup == null) {
-            lookup = new SearchLookup(mapperService(), indexQueryParser.fieldDataService, null);
-        }
-        return lookup;
-    }
-
-    public long nowInMillis() {
-        SearchContext current = SearchContext.current();
-        if (current != null) {
-            return current.nowInMillis();
-        }
-        return System.currentTimeMillis();
-    }
-
-    public NestedScope nestedScope() {
-        return nestedScope;
-    }
-
-    public Version indexVersionCreated() {
-        return indexVersionCreated;
-    }
-
-    public QueryParseContext parseContext() {
-        return this.parseContext;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java b/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java
deleted file mode 100644
index da4c8c6..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryShardException.java
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.elasticsearch.ElasticsearchException;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.rest.RestStatus;
-
-import java.io.IOException;
-
-/**
- * Exception that is thrown when creating lucene queries on the shard
- */
-public class QueryShardException extends ElasticsearchException {
-
-    public QueryShardException(QueryShardContext context, String msg, Object... args) {
-        this(context, msg, null, args);
-    }
-
-    public QueryShardException(QueryShardContext context, String msg, Throwable cause, Object... args) {
-        super(msg, cause, args);
-        setIndex(context.index());
-    }
-
-    /**
-     * This constructor is provided for use in unit tests where a
-     * {@link QueryShardContext} may not be available
-     */
-    public QueryShardException(Index index, int line, int col, String msg, Throwable cause) {
-        super(msg, cause);
-        setIndex(index);
-    }
-
-    public QueryShardException(StreamInput in) throws IOException{
-        super(in);
-    }
-
-    @Override
-    public RestStatus status() {
-        return RestStatus.BAD_REQUEST;
-    }
-
-    @Override
-    protected void innerToXContent(XContentBuilder builder, Params params) throws IOException {
-        super.innerToXContent(builder, params);
-    }
-
-    @Override
-    public void writeTo(StreamOutput out) throws IOException {
-        super.writeTo(out);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
index baff082..7965eec 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java
@@ -38,9 +38,12 @@ import static com.google.common.collect.Lists.newArrayList;
  * them either using DisMax or a plain boolean query (see {@link #useDisMax(boolean)}).
  * <p/>
  */
-public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQueryBuilder> {
+public class QueryStringQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<QueryStringQueryBuilder> {
 
-    public static final String NAME = "query_string";
+    public enum Operator {
+        OR,
+        AND
+    }
 
     private final String queryString;
 
@@ -65,6 +68,9 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     private Locale locale;
 
+
+    private float boost = -1;
+
     private Fuzziness fuzziness;
     private int fuzzyPrefixLength = -1;
     private int fuzzyMaxExpansions = -1;
@@ -86,13 +92,13 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     private Boolean lenient;
 
+    private String queryName;
+
     private String timeZone;
 
     /** To limit effort spent determinizing regexp queries. */
     private Integer maxDeterminizedStates;
 
-    static final QueryStringQueryBuilder PROTOTYPE = new QueryStringQueryBuilder(null);
-
     public QueryStringQueryBuilder(String queryString) {
         this.queryString = queryString;
     }
@@ -153,11 +159,11 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
     /**
      * Sets the boolean operator of the query parser used to parse the query string.
      * <p/>
-     * <p>In default mode ({@link Operator#OR}) terms without any modifiers
+     * <p>In default mode ({@link FieldQueryBuilder.Operator#OR}) terms without any modifiers
      * are considered optional: for example <code>capital of Hungary</code> is equal to
      * <code>capital OR of OR Hungary</code>.
      * <p/>
-     * <p>In {@link Operator#AND} mode terms are considered to be in conjunction: the
+     * <p>In {@link FieldQueryBuilder.Operator#AND} mode terms are considered to be in conjunction: the
      * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
      */
     public QueryStringQueryBuilder defaultOperator(Operator defaultOperator) {
@@ -289,6 +295,16 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
     }
 
     /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
+    @Override
+    public QueryStringQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
      * An optional field name suffix to automatically try and add to the field searched when using quoted text.
      */
     public QueryStringQueryBuilder quoteFieldSuffix(String quoteFieldSuffix) {
@@ -305,6 +321,14 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         return this;
     }
 
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public QueryStringQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
     public QueryStringQueryBuilder locale(Locale locale) {
         this.locale = locale;
         return this;
@@ -320,7 +344,7 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(QueryStringQueryParser.NAME);
         builder.field("query", queryString);
         if (defaultField != null) {
             builder.field("default_field", defaultField);
@@ -368,6 +392,9 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         if (fuzziness != null) {
             fuzziness.toXContent(builder, params);
         }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
         if (fuzzyPrefixLength != -1) {
             builder.field("fuzzy_prefix_length", fuzzyPrefixLength);
         }
@@ -395,18 +422,15 @@ public class QueryStringQueryBuilder extends AbstractQueryBuilder<QueryStringQue
         if (lenient != null) {
             builder.field("lenient", lenient);
         }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
         if (locale != null) {
             builder.field("locale", locale.toString());
         }
         if (timeZone != null) {
             builder.field("time_zone", timeZone);
         }
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
index 20ab078..2a21d3d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java
@@ -47,8 +47,9 @@ import static org.elasticsearch.common.lucene.search.Queries.fixNegativeQueryIfN
 /**
  *
  */
-public class QueryStringQueryParser extends BaseQueryParserTemp {
+public class QueryStringQueryParser implements QueryParser {
 
+    public static final String NAME = "query_string";
     private static final ParseField FUZZINESS = Fuzziness.FIELD.withDeprecation("fuzzy_min_sim");
 
     private final boolean defaultAnalyzeWildcard;
@@ -62,18 +63,17 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{QueryStringQueryBuilder.NAME, Strings.toCamelCase(QueryStringQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
         QueryParserSettings qpSettings = new QueryParserSettings();
-        qpSettings.defaultField(context.defaultField());
-        qpSettings.lenient(context.queryStringLenient());
+        qpSettings.defaultField(parseContext.defaultField());
+        qpSettings.lenient(parseContext.queryStringLenient());
         qpSettings.analyzeWildcard(defaultAnalyzeWildcard);
         qpSettings.allowLeadingWildcard(defaultAllowLeadingWildcard);
         qpSettings.locale(Locale.ROOT);
@@ -106,7 +106,7 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                         }
 
                         if (Regex.isSimpleMatchPattern(fField)) {
-                            for (String field : context.mapperService().simpleMatchToIndexNames(fField)) {
+                            for (String field : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
                                 qpSettings.fields().add(field);
                                 if (fBoost != -1) {
                                     if (qpSettings.boosts() == null) {
@@ -144,13 +144,13 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                         throw new QueryParsingException(parseContext, "Query default operator [" + op + "] is not allowed");
                     }
                 } else if ("analyzer".equals(currentFieldName)) {
-                    NamedAnalyzer analyzer = context.analysisService().analyzer(parser.text());
+                    NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());
                     if (analyzer == null) {
                         throw new QueryParsingException(parseContext, "[query_string] analyzer [" + parser.text() + "] not found");
                     }
                     qpSettings.forcedAnalyzer(analyzer);
                 } else if ("quote_analyzer".equals(currentFieldName) || "quoteAnalyzer".equals(currentFieldName)) {
-                    NamedAnalyzer analyzer = context.analysisService().analyzer(parser.text());
+                    NamedAnalyzer analyzer = parseContext.analysisService().analyzer(parser.text());
                     if (analyzer == null) {
                         throw new QueryParsingException(parseContext, "[query_string] quote_analyzer [" + parser.text()
                                 + "] not found");
@@ -215,16 +215,16 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
         if (qpSettings.queryString() == null) {
             throw new QueryParsingException(parseContext, "query_string must be provided with a [query]");
         }
-        qpSettings.defaultAnalyzer(context.mapperService().searchAnalyzer());
-        qpSettings.defaultQuoteAnalyzer(context.mapperService().searchQuoteAnalyzer());
+        qpSettings.defaultAnalyzer(parseContext.mapperService().searchAnalyzer());
+        qpSettings.defaultQuoteAnalyzer(parseContext.mapperService().searchQuoteAnalyzer());
 
         if (qpSettings.escape()) {
             qpSettings.queryString(org.apache.lucene.queryparser.classic.QueryParser.escape(qpSettings.queryString()));
         }
 
-        qpSettings.queryTypes(context.queryTypes());
+        qpSettings.queryTypes(parseContext.queryTypes());
 
-        MapperQueryParser queryParser = context.queryParser(qpSettings);
+        MapperQueryParser queryParser = parseContext.queryParser(qpSettings);
 
         try {
             Query query = queryParser.parse(qpSettings.queryString());
@@ -239,16 +239,11 @@ public class QueryStringQueryParser extends BaseQueryParserTemp {
                 Queries.applyMinimumShouldMatch((BooleanQuery) query, qpSettings.minimumShouldMatch());
             }
             if (queryName != null) {
-                context.addNamedQuery(queryName, query);
+                parseContext.addNamedQuery(queryName, query);
             }
             return query;
         } catch (org.apache.lucene.queryparser.classic.ParseException e) {
             throw new QueryParsingException(parseContext, "Failed to parse query [" + qpSettings.queryString() + "]", e);
         }
     }
-
-    @Override
-    public QueryStringQueryBuilder getBuilderPrototype() {
-        return QueryStringQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java b/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java
deleted file mode 100644
index 3761a82..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryValidationException.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import java.util.ArrayList;
-import java.util.List;
-
-/**
- * This exception can be used to indicate various reasons why validation of a query has failed.
- */
-public class QueryValidationException extends IllegalArgumentException {
-
-    private final List<String> validationErrors = new ArrayList<>();
-
-    public QueryValidationException(String error) {
-        super("query validation failed");
-        validationErrors.add(error);
-    }
-
-    public QueryValidationException(List<String> errors) {
-        super("query validation failed");
-        validationErrors.addAll(errors);
-    }
-
-    public void addValidationError(String error) {
-        validationErrors.add(error);
-    }
-
-    public void addValidationErrors(Iterable<String> errors) {
-        for (String error : errors) {
-            validationErrors.add(error);
-        }
-    }
-
-    public List<String> validationErrors() {
-        return validationErrors;
-    }
-
-    @Override
-    public String getMessage() {
-        StringBuilder sb = new StringBuilder();
-        sb.append("Validation Failed: ");
-        int index = 0;
-        for (String error : validationErrors) {
-            sb.append(++index).append(": ").append(error).append(";");
-        }
-        return sb.toString();
-    }
-
-    /**
-     * Helper method than can be used to add error messages to an existing {@link QueryValidationException}.
-     * When passing {@code null} as the initial exception, a new exception is created.
-     *
-     * @param queryId
-     * @param validationError the error message to add to an initial exception
-     * @param validationException an initial exception. Can be {@code null}, in which case a new exception is created.
-     * @return a {@link QueryValidationException} with added validation error message
-     */
-    public static QueryValidationException addValidationError(String queryId, String validationError, QueryValidationException validationException) {
-        if (validationException == null) {
-            validationException = new QueryValidationException("[" + queryId + "] " + validationError);
-        } else {
-            validationException.addValidationError(validationError);
-        }
-        return validationException;
-    }
-
-    /**
-     * Helper method than can be used to add error messages to an existing {@link QueryValidationException}.
-     * When passing {@code null} as the initial exception, a new exception is created.
-     * @param validationErrors the error messages to add to an initial exception
-     * @param validationException an initial exception. Can be {@code null}, in which case a new exception is created.
-     * @return a {@link QueryValidationException} with added validation error message
-     */
-    public static QueryValidationException addValidationErrors(List<String> validationErrors, QueryValidationException validationException) {
-        if (validationException == null) {
-            validationException = new QueryValidationException(validationErrors);
-        } else {
-            validationException.addValidationErrors(validationErrors);
-        }
-        return validationException;
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java
deleted file mode 100644
index d3be9da..0000000
--- a/core/src/main/java/org/elasticsearch/index/query/QueryWrappingQueryBuilder.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-
-import java.io.IOException;
-
-/**
- * QueryBuilder implementation that  holds a lucene query, which can be returned by {@link QueryBuilder#toQuery(QueryShardContext)}.
- * Doesn't support conversion to {@link org.elasticsearch.common.xcontent.XContent} via {@link #doXContent(XContentBuilder, Params)}.
- */
-//norelease to be removed once all queries support separate fromXContent and toQuery methods. Make AbstractQueryBuilder#toQuery final as well then.
-public class QueryWrappingQueryBuilder extends AbstractQueryBuilder<QueryWrappingQueryBuilder> implements SpanQueryBuilder<QueryWrappingQueryBuilder>, MultiTermQueryBuilder<QueryWrappingQueryBuilder>{
-
-    private Query query;
-
-    public QueryWrappingQueryBuilder(Query query) {
-        this.query = query;
-        //hack to make sure that the boost from the wrapped query is used, otherwise it gets overwritten.
-        if (query != null) {
-            this.boost = query.getBoost();
-        }
-    }
-
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        throw new UnsupportedOperationException();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return query;
-    }
-
-    @Override
-    public String getWriteableName() {
-        // this should not be called since we overwrite BaseQueryBuilder#toQuery() in this class
-        throw new UnsupportedOperationException();
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
index 0db4152..da23698 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java
@@ -19,111 +19,187 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.joda.DateMathParser;
-import org.elasticsearch.common.joda.Joda;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.mapper.core.DateFieldMapper;
-import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that matches documents within an range of terms.
  */
-public class RangeQueryBuilder extends AbstractQueryBuilder<RangeQueryBuilder> implements MultiTermQueryBuilder<RangeQueryBuilder> {
+public class RangeQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<RangeQueryBuilder> {
 
-    public static final boolean DEFAULT_INCLUDE_UPPER = true;
+    private final String name;
+    private Object from;
+    private Object to;
+    private String timeZone;
+    private boolean includeLower = true;
+    private boolean includeUpper = true;
+    private float boost = -1;
+    private String queryName;
+    private String format;
 
-    public static final boolean DEFAULT_INCLUDE_LOWER = true;
+    /**
+     * A Query that matches documents within an range of terms.
+     *
+     * @param name The field name
+     */
+    public RangeQueryBuilder(String name) {
+        this.name = name;
+    }
 
-    public static final String NAME = "range";
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(Object from) {
+        this.from = from;
+        return this;
+    }
 
-    private final String fieldName;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(String from) {
+        this.from = from;
+        return this;
+    }
 
-    private Object from;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(int from) {
+        this.from = from;
+        return this;
+    }
 
-    private Object to;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(long from) {
+        this.from = from;
+        return this;
+    }
 
-    private String timeZone;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(float from) {
+        this.from = from;
+        return this;
+    }
 
-    private boolean includeLower = DEFAULT_INCLUDE_LOWER;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder from(double from) {
+        this.from = from;
+        return this;
+    }
 
-    private boolean includeUpper = DEFAULT_INCLUDE_UPPER;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(String from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
-    private String format;
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(Object from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
-    static final RangeQueryBuilder PROTOTYPE = new RangeQueryBuilder(null);
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gt(int from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
+    }
 
     /**
-     * A Query that matches documents within an range of terms.
-     *
-     * @param fieldName The field name
+     * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder(String fieldName) {
-        this.fieldName = fieldName;
+    public RangeQueryBuilder gt(long from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
     /**
-     * Get the field name for this query.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public String fieldName() {
-        return this.fieldName;
+    public RangeQueryBuilder gt(float from) {
+        this.from = from;
+        this.includeLower = false;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
-     * In case lower bound is assigned to a string, we internally convert it to a {@link BytesRef} because
-     * in {@link RangeQueryParser} field are later parsed as {@link BytesRef} and we need internal representation
-     * of query to be equal regardless of whether it was created from XContent or via Java API.
      */
-    public RangeQueryBuilder from(Object from, boolean includeLower) {
-        this.from = convertToBytesRefIfString(from);
-        this.includeLower = includeLower;
+    public RangeQueryBuilder gt(double from) {
+        this.from = from;
+        this.includeLower = false;
         return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder from(Object from) {
-        return from(from, this.includeLower);
+    public RangeQueryBuilder gte(String from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
-     * Gets the lower range value for this query.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public Object from() {
-        return convertToStringIfBytesRef(this.from);
+    public RangeQueryBuilder gte(Object from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder gt(Object from) {
-        return from(from, false);
+    public RangeQueryBuilder gte(int from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
      * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder gte(Object from) {
-        return from(from, true);
+    public RangeQueryBuilder gte(long from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
     }
 
     /**
-     * The to part of the range query. Null indicates unbounded.
+     * The from part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder to(Object to, boolean includeUpper) {
-        this.to = convertToBytesRefIfString(to);
-        this.includeUpper = includeUpper;
+    public RangeQueryBuilder gte(float from) {
+        this.from = from;
+        this.includeLower = true;
+        return this;
+    }
+
+    /**
+     * The from part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder gte(double from) {
+        this.from = from;
+        this.includeLower = true;
         return this;
     }
 
@@ -131,214 +207,229 @@ public class RangeQueryBuilder extends AbstractQueryBuilder<RangeQueryBuilder> i
      * The to part of the range query. Null indicates unbounded.
      */
     public RangeQueryBuilder to(Object to) {
-        return to(to, this.includeUpper);
+        this.to = to;
+        return this;
     }
 
     /**
-     * Gets the upper range value for this query.
-     * In case upper bound is assigned to a string, we internally convert it to a {@link BytesRef} because
-     * in {@link RangeQueryParser} field are later parsed as {@link BytesRef} and we need internal representation
-     * of query to be equal regardless of whether it was created from XContent or via Java API.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public Object to() {
-        return convertToStringIfBytesRef(this.to);
+    public RangeQueryBuilder to(String to) {
+        this.to = to;
+        return this;
     }
 
     /**
      * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder lt(Object to) {
-        return to(to, false);
+    public RangeQueryBuilder to(int to) {
+        this.to = to;
+        return this;
     }
 
     /**
      * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder lte(Object to) {
-        return to(to, true);
+    public RangeQueryBuilder to(long to) {
+        this.to = to;
+        return this;
     }
 
     /**
-     * Should the lower bound be included or not. Defaults to <tt>true</tt>.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder includeLower(boolean includeLower) {
-        this.includeLower = includeLower;
+    public RangeQueryBuilder to(float to) {
+        this.to = to;
         return this;
     }
 
     /**
-     * Gets the includeLower flag for this query.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public boolean includeLower() {
-        return this.includeLower;
+    public RangeQueryBuilder to(double to) {
+        this.to = to;
+        return this;
     }
 
     /**
-     * Should the upper bound be included or not. Defaults to <tt>true</tt>.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder includeUpper(boolean includeUpper) {
-        this.includeUpper = includeUpper;
+    public RangeQueryBuilder lt(String to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * Gets the includeUpper flag for this query.
+     * The to part of the range query. Null indicates unbounded.
      */
-    public boolean includeUpper() {
-        return this.includeUpper;
+    public RangeQueryBuilder lt(Object to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
     /**
-     * In case of date field, we can adjust the from/to fields using a timezone
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder timeZone(String timezone) {
-        this.timeZone = timezone;
+    public RangeQueryBuilder lt(int to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * In case of date field, gets the from/to fields timezone adjustment
+     * The to part of the range query. Null indicates unbounded.
      */
-    public String timeZone() {
-        return this.timeZone;
+    public RangeQueryBuilder lt(long to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
     /**
-     * In case of format field, we can parse the from/to fields using this time format
+     * The to part of the range query. Null indicates unbounded.
      */
-    public RangeQueryBuilder format(String format) {
-        this.format = format;
+    public RangeQueryBuilder lt(float to) {
+        this.to = to;
+        this.includeUpper = false;
         return this;
     }
 
     /**
-     * Gets the format field to parse the from/to fields
+     * The to part of the range query. Null indicates unbounded.
      */
-    public String format() {
-        return this.format;
+    public RangeQueryBuilder lt(double to) {
+        this.to = to;
+        this.includeUpper = false;
+        return this;
     }
 
-    @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("from", convertToStringIfBytesRef(this.from));
-        builder.field("to", convertToStringIfBytesRef(this.to));
-        builder.field("include_lower", includeLower);
-        builder.field("include_upper", includeUpper);
-        if (timeZone != null) {
-            builder.field("time_zone", timeZone);
-        }
-        if (format != null) {
-            builder.field("format", format);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(String to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(Object to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        MappedFieldType mapper = context.fieldMapper(this.fieldName);
-        if (mapper != null) {
-            if (mapper instanceof DateFieldMapper.DateFieldType) {
-                DateMathParser forcedDateParser = null;
-                if (this.format  != null) {
-                    forcedDateParser = new DateMathParser(Joda.forPattern(this.format));
-                }
-                DateTimeZone dateTimeZone = null;
-                if (this.timeZone != null) {
-                    dateTimeZone = DateTimeZone.forID(this.timeZone);
-                }
-                query = ((DateFieldMapper.DateFieldType) mapper).rangeQuery(from, to, includeLower, includeUpper, dateTimeZone, forcedDateParser);
-            } else  {
-                if (timeZone != null) {
-                    throw new QueryShardException(context, "[range] time_zone can not be applied to non date field ["
-                            + fieldName + "]");
-                }
-                //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well?
-                query = mapper.rangeQuery(from, to, includeLower, includeUpper);
-            }
-        } else {
-            if (timeZone != null) {
-                throw new QueryShardException(context, "[range] time_zone can not be applied to non unmapped field ["
-                        + fieldName + "]");
-            }
-        }
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(int to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
+    }
 
-        if (query == null) {
-            query = new TermRangeQuery(this.fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper);
-        }
-        return query;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(long to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.fieldName == null || this.fieldName.isEmpty()) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.timeZone != null) {
-            try {
-                DateTimeZone.forID(this.timeZone);
-            } catch (Exception e) {
-                validationException = addValidationError("error parsing timezone." + e.getMessage(),
-                        validationException);
-            }
-        }
-        if (this.format != null) {
-            try {
-                Joda.forPattern(this.format);
-            } catch (Exception e) {
-                validationException = addValidationError("error parsing format." + e.getMessage(),
-                        validationException);
-            }
-        }
-        return validationException;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(float to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected RangeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder(in.readString());
-        rangeQueryBuilder.from = in.readGenericValue();
-        rangeQueryBuilder.to = in.readGenericValue();
-        rangeQueryBuilder.includeLower = in.readBoolean();
-        rangeQueryBuilder.includeUpper = in.readBoolean();
-        rangeQueryBuilder.timeZone = in.readOptionalString();
-        rangeQueryBuilder.format = in.readOptionalString();
-        return rangeQueryBuilder;
+    /**
+     * The to part of the range query. Null indicates unbounded.
+     */
+    public RangeQueryBuilder lte(double to) {
+        this.to = to;
+        this.includeUpper = true;
+        return this;
     }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(this.fieldName);
-        out.writeGenericValue(this.from);
-        out.writeGenericValue(this.to);
-        out.writeBoolean(this.includeLower);
-        out.writeBoolean(this.includeUpper);
-        out.writeOptionalString(this.timeZone);
-        out.writeOptionalString(this.format);
+    /**
+     * Should the lower bound be included or not. Defaults to <tt>true</tt>.
+     */
+    public RangeQueryBuilder includeLower(boolean includeLower) {
+        this.includeLower = includeLower;
+        return this;
     }
 
+    /**
+     * Should the upper bound be included or not. Defaults to <tt>true</tt>.
+     */
+    public RangeQueryBuilder includeUpper(boolean includeUpper) {
+        this.includeUpper = includeUpper;
+        return this;
+    }
+
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, from, to, timeZone, includeLower, includeUpper, format);
+    public RangeQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public RangeQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
+     * In case of date field, we can adjust the from/to fields using a timezone
+     */
+    public RangeQueryBuilder timeZone(String timezone) {
+        this.timeZone = timezone;
+        return this;
+    }
+
+    /**
+     * In case of date field, we can set the format to be used instead of the mapper format
+     */
+    public RangeQueryBuilder format(String format) {
+        this.format = format;
+        return this;
     }
 
     @Override
-    protected boolean doEquals(RangeQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-               Objects.equals(from, other.from) &&
-               Objects.equals(to, other.to) &&
-               Objects.equals(timeZone, other.timeZone) &&
-               Objects.equals(includeLower, other.includeLower) &&
-               Objects.equals(includeUpper, other.includeUpper) &&
-               Objects.equals(format, other.format);
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(RangeQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("from", from);
+        builder.field("to", to);
+        if (timeZone != null) {
+            builder.field("time_zone", timeZone);
+        }
+        if (format != null) {
+            builder.field("format", format);
+        }
+        builder.field("include_lower", includeLower);
+        builder.field("include_upper", includeUpper);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
+        builder.endObject();
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
index 5a6d85f..6e4cd45 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java
@@ -19,17 +19,26 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
 import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.joda.DateMathParser;
+import org.elasticsearch.common.joda.Joda;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.mapper.core.DateFieldMapper;
+import org.joda.time.DateTimeZone;
 
 import java.io.IOException;
 
 /**
- * Parser for range query
+ *
  */
-public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
+public class RangeQueryParser implements QueryParser {
 
+    public static final String NAME = "range";
     private static final ParseField FIELDDATA_FIELD = new ParseField("fielddata").withAllDeprecated("[no replacement]");
 
     @Inject
@@ -38,22 +47,22 @@ public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{RangeQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public RangeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = null;
         Object from = null;
         Object to = null;
-        boolean includeLower = RangeQueryBuilder.DEFAULT_INCLUDE_LOWER;
-        boolean includeUpper = RangeQueryBuilder.DEFAULT_INCLUDE_UPPER;
-        String timeZone = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        boolean includeLower = true;
+        boolean includeUpper = true;
+        DateTimeZone timeZone = null;
+        DateMathParser forcedDateParser = null;
+        float boost = 1.0f;
         String queryName = null;
-        String format = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -91,11 +100,9 @@ public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
                             to = parser.objectBytes();
                             includeUpper = true;
                         } else if ("time_zone".equals(currentFieldName) || "timeZone".equals(currentFieldName)) {
-                            timeZone = parser.text();
+                            timeZone = DateTimeZone.forID(parser.text());
                         } else if ("format".equals(currentFieldName)) {
-                            format = parser.text();
-                        } else if ("_name".equals(currentFieldName)) {
-                            queryName = parser.text();
+                            forcedDateParser = new DateMathParser(Joda.forPattern(parser.text()));
                         } else {
                             throw new QueryParsingException(parseContext, "[range] query does not support [" + currentFieldName + "]");
                         }
@@ -112,20 +119,27 @@ public class RangeQueryParser extends BaseQueryParser<RangeQueryBuilder> {
             }
         }
 
-        RangeQueryBuilder rangeQuery = new RangeQueryBuilder(fieldName);
-        rangeQuery.from(from);
-        rangeQuery.to(to);
-        rangeQuery.includeLower(includeLower);
-        rangeQuery.includeUpper(includeUpper);
-        rangeQuery.timeZone(timeZone);
-        rangeQuery.boost(boost);
-        rangeQuery.queryName(queryName);
-        rangeQuery.format(format);
-        return rangeQuery;
-    }
-
-    @Override
-    public RangeQueryBuilder getBuilderPrototype() {
-        return RangeQueryBuilder.PROTOTYPE;
+        Query query = null;
+        MappedFieldType mapper = parseContext.fieldMapper(fieldName);
+        if (mapper != null) {
+            if (mapper instanceof DateFieldMapper.DateFieldType) {
+                query = ((DateFieldMapper.DateFieldType) mapper).rangeQuery(from, to, includeLower, includeUpper, timeZone, forcedDateParser);
+            } else  {
+                if (timeZone != null) {
+                    throw new QueryParsingException(parseContext, "[range] time_zone can not be applied to non date field ["
+                            + fieldName + "]");
+                }
+                //LUCENE 4 UPGRADE Mapper#rangeQuery should use bytesref as well?
+                query = mapper.rangeQuery(from, to, includeLower, includeUpper);
+            }
+        }
+        if (query == null) {
+            query = new TermRangeQuery(fieldName, BytesRefs.toBytesRef(from), BytesRefs.toBytesRef(to), includeLower, includeUpper);
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
index 6399089..ee143eb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java
@@ -19,73 +19,48 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
 import org.apache.lucene.util.automaton.Operations;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * A Query that does fuzzy matching for a specific value.
  */
-public class RegexpQueryBuilder extends AbstractQueryBuilder<RegexpQueryBuilder> implements MultiTermQueryBuilder<RegexpQueryBuilder> {
+public class RegexpQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<RegexpQueryBuilder> {
 
-    public static final String NAME = "regexp";
+    private final String name;
+    private final String regexp;
 
-    public static final int DEFAULT_FLAGS_VALUE = RegexpFlag.ALL.value();
-
-    public static final int DEFAULT_MAX_DETERMINIZED_STATES = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
-
-    private final String fieldName;
-    
-    private final String value;
-    
-    private int flagsValue = DEFAULT_FLAGS_VALUE;
-    
-    private int maxDeterminizedStates = DEFAULT_MAX_DETERMINIZED_STATES;
-    
+    private int flags = RegexpQueryParser.DEFAULT_FLAGS_VALUE;
+    private float boost = -1;
     private String rewrite;
-    
-    static final RegexpQueryBuilder PROTOTYPE = new RegexpQueryBuilder(null, null);
+    private String queryName;
+    private int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
+    private boolean maxDetermizedStatesSet;
 
     /**
-     * Constructs a new regex query.
-     * 
-     * @param fieldName  The name of the field
-     * @param value The regular expression
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param regexp The regular expression
      */
-    public RegexpQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
-    }
-
-    /** Returns the field name used in this query. */
-    public String fieldName() {
-        return this.fieldName;
+    public RegexpQueryBuilder(String name, String regexp) {
+        this.name = name;
+        this.regexp = regexp;
     }
 
     /**
-     *  Returns the value used in this query.
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
      */
-    public String value() {
-        return this.value;
+    @Override
+    public RegexpQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     public RegexpQueryBuilder flags(RegexpFlag... flags) {
-        if (flags == null) {
-            this.flagsValue = DEFAULT_FLAGS_VALUE;
-            return this;
-        }
         int value = 0;
         if (flags.length == 0) {
             value = RegexpFlag.ALL.value;
@@ -94,120 +69,53 @@ public class RegexpQueryBuilder extends AbstractQueryBuilder<RegexpQueryBuilder>
                 value |= flag.value;
             }
         }
-        this.flagsValue = value;
-        return this;
-    }
-
-    public RegexpQueryBuilder flags(int flags) {
-        this.flagsValue = flags;
+        this.flags = value;
         return this;
     }
 
-    public int flags() {
-        return this.flagsValue;
-    }
-
     /**
      * Sets the regexp maxDeterminizedStates.
      */
     public RegexpQueryBuilder maxDeterminizedStates(int value) {
         this.maxDeterminizedStates = value;
+        this.maxDetermizedStatesSet = true;
         return this;
     }
-    
-    public int maxDeterminizedStates() {
-        return this.maxDeterminizedStates;
-    }
 
     public RegexpQueryBuilder rewrite(String rewrite) {
         this.rewrite = rewrite;
         return this;
     }
-    
-    public String rewrite() {
-        return this.rewrite;
+
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public RegexpQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("value", this.value);
-        builder.field("flags_value", flagsValue);
-        builder.field("max_determinized_states", maxDeterminizedStates);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
+        builder.startObject(RegexpQueryParser.NAME);
+        builder.startObject(name);
+        builder.field("value", regexp);
+        if (flags != -1) {
+            builder.field("flags_value", flags);
         }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    public Query doToQuery(QueryShardContext context) throws QueryShardException, IOException {
-        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-
-        Query query = null;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, context);
+        if (maxDetermizedStatesSet) {
+            builder.field("max_determinized_states", maxDeterminizedStates);
         }
-        if (query == null) {
-            RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates);
-            if (method != null) {
-                regexpQuery.setRewriteMethod(method);
-            }
-            query = regexpQuery;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
+        if (rewrite != null) {
+            builder.field("rewrite", rewrite);
         }
-        if (this.value == null) {
-            validationException = addValidationError("query text cannot be null", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    public RegexpQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        RegexpQueryBuilder regexpQueryBuilder = new RegexpQueryBuilder(in.readString(), in.readString());
-        regexpQueryBuilder.flagsValue = in.readVInt();
-        regexpQueryBuilder.maxDeterminizedStates = in.readVInt();
-        regexpQueryBuilder.rewrite = in.readOptionalString();
-        return regexpQueryBuilder;
-    }
-
-    @Override
-    public void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeVInt(flagsValue);
-        out.writeVInt(maxDeterminizedStates);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    public int doHashCode() {
-        return Objects.hash(fieldName, value, flagsValue, maxDeterminizedStates, rewrite);
-    }
-
-    @Override
-    public boolean doEquals(RegexpQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(flagsValue, other.flagsValue) &&
-                Objects.equals(maxDeterminizedStates, other.maxDeterminizedStates) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
index 066b302..533e3cd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java
@@ -19,15 +19,27 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RegexpQuery;
+import org.apache.lucene.util.automaton.Operations;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for regexp query
+ *
  */
-public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
+public class RegexpQueryParser implements QueryParser {
+
+    public static final String NAME = "regexp";
+
+    public static final int DEFAULT_FLAGS_VALUE = RegexpFlag.ALL.value();
 
     @Inject
     public RegexpQueryParser() {
@@ -35,20 +47,20 @@ public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{RegexpQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public RegexpQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String fieldName = parser.currentName();
-        String rewrite = null;
+        String rewriteMethod = null;
 
         String value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        int flagsValue = RegexpQueryBuilder.DEFAULT_FLAGS_VALUE;
-        int maxDeterminizedStates = RegexpQueryBuilder.DEFAULT_MAX_DETERMINIZED_STATES;
+        float boost = 1.0f;
+        int flagsValue = DEFAULT_FLAGS_VALUE;
+        int maxDeterminizedStates = Operations.DEFAULT_MAX_DETERMINIZED_STATES;
         String queryName = null;
         String currentFieldName = null;
         XContentParser.Token token;
@@ -68,7 +80,7 @@ public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
                         } else if ("boost".equals(currentFieldName)) {
                             boost = parser.floatValue();
                         } else if ("rewrite".equals(currentFieldName)) {
-                            rewrite = parser.textOrNull();
+                            rewriteMethod = parser.textOrNull();
                         } else if ("flags".equals(currentFieldName)) {
                             String flags = parser.textOrNull();
                             flagsValue = RegexpFlag.resolveValue(flags);
@@ -96,16 +108,27 @@ public class RegexpQueryParser extends BaseQueryParser<RegexpQueryBuilder> {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for regexp query");
         }
-        return new RegexpQueryBuilder(fieldName, value)
-                .flags(flagsValue)
-                .maxDeterminizedStates(maxDeterminizedStates)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public RegexpQueryBuilder getBuilderPrototype() {
-        return RegexpQueryBuilder.PROTOTYPE;
+        MultiTermQuery.RewriteMethod method = QueryParsers.parseRewriteMethod(parseContext.parseFieldMatcher(), rewriteMethod, null);
+
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.regexpQuery(value, flagsValue, maxDeterminizedStates, method, parseContext);
+        }
+        if (query == null) {
+            RegexpQuery regexpQuery = new RegexpQuery(new Term(fieldName, BytesRefs.toBytesRef(value)), flagsValue, maxDeterminizedStates);
+            if (method != null) {
+                regexpQuery.setRewriteMethod(method);
+            }
+            query = regexpQuery;
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
+
+
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
index 519f065..a9a35ac 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
@@ -19,155 +19,40 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RandomAccessWeight;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.Bits;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.script.*;
+import org.elasticsearch.script.Script;
 import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
-import java.util.Objects;
+import java.util.HashMap;
+import java.util.Map;
 
-public class ScriptQueryBuilder extends AbstractQueryBuilder<ScriptQueryBuilder> {
+public class ScriptQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "script";
+    private Script script;
 
-    static final ScriptQueryBuilder PROTOTYPE = new ScriptQueryBuilder(null);
-
-    private final Script script;
+    private String queryName;
 
     public ScriptQueryBuilder(Script script) {
         this.script = script;
     }
 
-    public Script script() {
-        return this.script;
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public ScriptQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.startObject(NAME);
-        builder.field(ScriptField.SCRIPT.getPreferredName(), script);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        return new ScriptQuery(script, context.scriptService(), context.lookup());
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (this.script == null) {
-            validationException = addValidationError("script cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    static class ScriptQuery extends Query {
-
-        private final Script script;
-
-        private final SearchScript searchScript;
-
-        public ScriptQuery(Script script, ScriptService scriptService, SearchLookup searchLookup) {
-            this.script = script;
-            this.searchScript = scriptService.search(searchLookup, script, ScriptContext.Standard.SEARCH);
-        }
-
-        @Override
-        public String toString(String field) {
-            StringBuilder buffer = new StringBuilder();
-            buffer.append("ScriptFilter(");
-            buffer.append(script);
-            buffer.append(")");
-            return buffer.toString();
-        }
 
-        @Override
-        public boolean equals(Object obj) {
-            if (this == obj)
-                return true;
-            if (!super.equals(obj))
-                return false;
-            ScriptQuery other = (ScriptQuery) obj;
-            return Objects.equals(script, other.script);
-        }
-
-        @Override
-        public int hashCode() {
-            final int prime = 31;
-            int result = super.hashCode();
-            result = prime * result + Objects.hashCode(script);
-            return result;
-        }
-
-        @Override
-        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
-            return new RandomAccessWeight(this) {
-                @Override
-                protected Bits getMatchingDocs(final LeafReaderContext context) throws IOException {
-                    final LeafSearchScript leafScript = searchScript.getLeafSearchScript(context);
-                    return new Bits() {
-
-                        @Override
-                        public boolean get(int doc) {
-                            leafScript.setDocument(doc);
-                            Object val = leafScript.run();
-                            if (val == null) {
-                                return false;
-                            }
-                            if (val instanceof Boolean) {
-                                return (Boolean) val;
-                            }
-                            if (val instanceof Number) {
-                                return ((Number) val).longValue() != 0;
-                            }
-                            throw new IllegalArgumentException("Can't handle type [" + val + "] in script filter");
-                        }
-
-                        @Override
-                        public int length() {
-                            return context.reader().maxDoc();
-                        }
-
-                    };
-                }
-            };
+        builder.startObject(ScriptQueryParser.NAME);
+        builder.field(ScriptField.SCRIPT.getPreferredName(), script);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-    }
-
-    @Override
-    protected ScriptQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new ScriptQueryBuilder(Script.readScript(in));
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        script.writeTo(out);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(script);
-    }
-
-    @Override
-    protected boolean doEquals(ScriptQueryBuilder other) {
-        return Objects.equals(script, other.script);
+        builder.endObject();
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
index ccbfd66..62561f3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/ScriptQueryParser.java
@@ -19,12 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import com.google.common.base.Objects;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RandomAccessWeight;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.Bits;
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.script.Script;
+import org.elasticsearch.script.*;
 import org.elasticsearch.script.Script.ScriptField;
-import org.elasticsearch.script.ScriptParameterParser;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
+import org.elasticsearch.search.lookup.SearchLookup;
 
 import java.io.IOException;
 import java.util.Map;
@@ -32,9 +40,11 @@ import java.util.Map;
 import static com.google.common.collect.Maps.newHashMap;
 
 /**
- * Parser for script query
+ *
  */
-public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
+public class ScriptQueryParser implements QueryParser {
+
+    public static final String NAME = "script";
 
     @Inject
     public ScriptQueryParser() {
@@ -42,23 +52,23 @@ public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{ScriptQueryBuilder.NAME};
+        return new String[] { NAME };
     }
 
     @Override
-    public ScriptQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
         ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
-        
+
+        XContentParser.Token token;
+
         // also, when caching, since its isCacheable is false, will result in loading all bit set...
         Script script = null;
         Map<String, Object> params = null;
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
         String queryName = null;
-
-        XContentParser.Token token;
         String currentFieldName = null;
+
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
             if (token == XContentParser.Token.FIELD_NAME) {
                 currentFieldName = parser.currentName();
@@ -75,8 +85,6 @@ public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
             } else if (token.isValue()) {
                 if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
                 } else if (!scriptParameterParser.token(currentFieldName, token, parser, parseContext.parseFieldMatcher())) {
                     throw new QueryParsingException(parseContext, "[script] query does not support [" + currentFieldName + "]");
                 }
@@ -99,13 +107,83 @@ public class ScriptQueryParser extends BaseQueryParser<ScriptQueryBuilder> {
             throw new QueryParsingException(parseContext, "script must be provided with a [script] filter");
         }
 
-        return new ScriptQueryBuilder(script)
-                .boost(boost)
-                .queryName(queryName);
+        Query query = new ScriptQuery(script, parseContext.scriptService(), parseContext.lookup());
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 
-    @Override
-    public ScriptQueryBuilder getBuilderPrototype() {
-        return ScriptQueryBuilder.PROTOTYPE;
+    static class ScriptQuery extends Query {
+
+        private final Script script;
+
+        private final SearchScript searchScript;
+
+        public ScriptQuery(Script script, ScriptService scriptService, SearchLookup searchLookup) {
+            this.script = script;
+            this.searchScript = scriptService.search(searchLookup, script, ScriptContext.Standard.SEARCH);
+        }
+
+        @Override
+        public String toString(String field) {
+            StringBuilder buffer = new StringBuilder();
+            buffer.append("ScriptFilter(");
+            buffer.append(script);
+            buffer.append(")");
+            return buffer.toString();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (this == obj)
+                return true;
+            if (!super.equals(obj))
+                return false;
+            ScriptQuery other = (ScriptQuery) obj;
+            return Objects.equal(script, other.script);
+        }
+
+        @Override
+        public int hashCode() {
+            final int prime = 31;
+            int result = super.hashCode();
+            result = prime * result + Objects.hashCode(script);
+            return result;
+        }
+
+        @Override
+        public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {
+            return new RandomAccessWeight(this) {
+                @Override
+                protected Bits getMatchingDocs(final LeafReaderContext context) throws IOException {
+                    final LeafSearchScript leafScript = searchScript.getLeafSearchScript(context);
+                    return new Bits() {
+
+                        @Override
+                        public boolean get(int doc) {
+                            leafScript.setDocument(doc);
+                            Object val = leafScript.run();
+                            if (val == null) {
+                                return false;
+                            }
+                            if (val instanceof Boolean) {
+                                return (Boolean) val;
+                            }
+                            if (val instanceof Number) {
+                                return ((Number) val).longValue() != 0;
+                            }
+                            throw new IllegalArgumentException("Can't handle type [" + val + "] in script filter");
+                        }
+
+                        @Override
+                        public int length() {
+                            return context.reader().maxDoc();
+                        }
+
+                    };
+                }
+            };
+        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
index 06a3ccb..fc916f5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java
@@ -29,7 +29,6 @@ import org.apache.lucene.util.BytesRef;
 import java.io.IOException;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Objects;
 
 /**
  * Wrapper class for Lucene's SimpleQueryParser that allows us to redefine
@@ -203,102 +202,51 @@ public class SimpleQueryParser extends org.apache.lucene.queryparser.simple.Simp
             return new PrefixQuery(new Term(field, termStr));
         }
     }
+
     /**
      * Class encapsulating the settings for the SimpleQueryString query, with
      * their default values
      */
-    static class Settings {
-        /** Locale to use for parsing. */
-        private Locale locale = SimpleQueryStringBuilder.DEFAULT_LOCALE;
-        /** Specifies whether parsed terms should be lowercased. */
-        private boolean lowercaseExpandedTerms = SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        /** Specifies whether lenient query parsing should be used. */
-        private boolean lenient = SimpleQueryStringBuilder.DEFAULT_LENIENT;
-        /** Specifies whether wildcards should be analyzed. */
-        private boolean analyzeWildcard = SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD;
+    public static class Settings {
+        private Locale locale = Locale.ROOT;
+        private boolean lowercaseExpandedTerms = true;
+        private boolean lenient = false;
+        private boolean analyzeWildcard = false;
 
-        /**
-         * Generates default {@link Settings} object (uses ROOT locale, does
-         * lowercase terms, no lenient parsing, no wildcard analysis).
-         * */
         public Settings() {
-        }
 
-        public Settings(Locale locale, Boolean lowercaseExpandedTerms, Boolean lenient, Boolean analyzeWildcard) {
-            this.locale = locale;
-            this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-            this.lenient = lenient;
-            this.analyzeWildcard = analyzeWildcard;
         }
 
-        /** Specifies the locale to use for parsing, Locale.ROOT by default. */
         public void locale(Locale locale) {
-            this.locale = (locale != null) ? locale : SimpleQueryStringBuilder.DEFAULT_LOCALE;
+            this.locale = locale;
         }
 
-        /** Returns the locale to use for parsing. */
         public Locale locale() {
             return this.locale;
         }
 
-        /**
-         * Specifies whether to lowercase parse terms, defaults to true if
-         * unset.
-         */
         public void lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
             this.lowercaseExpandedTerms = lowercaseExpandedTerms;
         }
 
-        /** Returns whether to lowercase parse terms. */
         public boolean lowercaseExpandedTerms() {
             return this.lowercaseExpandedTerms;
         }
 
-        /** Specifies whether to use lenient parsing, defaults to false. */
         public void lenient(boolean lenient) {
             this.lenient = lenient;
         }
 
-        /** Returns whether to use lenient parsing. */
         public boolean lenient() {
             return this.lenient;
         }
 
-        /** Specifies whether to analyze wildcards. Defaults to false if unset. */
         public void analyzeWildcard(boolean analyzeWildcard) {
             this.analyzeWildcard = analyzeWildcard;
         }
 
-        /** Returns whether to analyze wildcards. */
         public boolean analyzeWildcard() {
             return analyzeWildcard;
         }
-
-        @Override
-        public int hashCode() {
-            // checking the return value of toLanguageTag() for locales only.
-            // For further reasoning see
-            // https://issues.apache.org/jira/browse/LUCENE-4021
-            return Objects.hash(locale.toLanguageTag(), lowercaseExpandedTerms, lenient, analyzeWildcard);
-        }
-
-        @Override
-        public boolean equals(Object obj) {
-            if (this == obj) {
-                return true;
-            }
-            if (obj == null || getClass() != obj.getClass()) {
-                return false;
-            }
-            Settings other = (Settings) obj;
-
-            // checking the return value of toLanguageTag() for locales only.
-            // For further reasoning see
-            // https://issues.apache.org/jira/browse/LUCENE-4021
-            return (Objects.equals(locale.toLanguageTag(), other.locale.toLanguageTag())
-                    && Objects.equals(lowercaseExpandedTerms, other.lowercaseExpandedTerms) 
-                    && Objects.equals(lenient, other.lenient)
-                    && Objects.equals(analyzeWildcard, other.analyzeWildcard));
-        }
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
index 1387980..700ad41 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java
@@ -19,304 +19,150 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.xcontent.ToXContent.Params;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.SimpleQueryParser.Settings;
 
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Objects;
-import java.util.TreeMap;
 
 /**
- * SimpleQuery is a query parser that acts similar to a query_string query, but
- * won't throw exceptions for any weird string syntax.
- *
- * For more detailed explanation of the query string syntax see also the <a
- * href=
- * "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html"
- * > online documentation</a>.
+ * SimpleQuery is a query parser that acts similar to a query_string
+ * query, but won't throw exceptions for any weird string syntax.
  */
-public class SimpleQueryStringBuilder extends AbstractQueryBuilder<SimpleQueryStringBuilder> {
-    /** Default locale used for parsing.*/
-    public static final Locale DEFAULT_LOCALE = Locale.ROOT;
-    /** Default for lowercasing parsed terms.*/
-    public static final boolean DEFAULT_LOWERCASE_EXPANDED_TERMS = true;
-    /** Default for using lenient query parsing.*/
-    public static final boolean DEFAULT_LENIENT = false;
-    /** Default for wildcard analysis.*/
-    public static final boolean DEFAULT_ANALYZE_WILDCARD = false;
-    /** Default for default operator to use for linking boolean clauses.*/
-    public static final Operator DEFAULT_OPERATOR = Operator.OR;
-    /** Default for search flags to use. */
-    public static final int DEFAULT_FLAGS = SimpleQueryStringFlag.ALL.value;
-    /** Name for (de-)serialization. */
-    public static final String NAME = "simple_query_string";
-
-    /** Query text to parse. */
-    private final String queryText;
-    /**
-     * Fields to query against. If left empty will query default field,
-     * currently _ALL. Uses a TreeMap to hold the fields so boolean clauses are
-     * always sorted in same order for generated Lucene query for easier
-     * testing.
-     *
-     * Can be changed back to HashMap once https://issues.apache.org/jira/browse/LUCENE-6305 is fixed.
-     */
-    private final Map<String, Float> fieldsAndWeights = new TreeMap<>();
-    /** If specified, analyzer to use to parse the query text, defaults to registered default in toQuery. */
+public class SimpleQueryStringBuilder extends QueryBuilder implements BoostableQueryBuilder<SimpleQueryStringBuilder> {
+    private Map<String, Float> fields = new HashMap<>();
     private String analyzer;
-    /** Default operator to use for linking boolean clauses. Defaults to OR according to docs. */
-    private Operator defaultOperator = DEFAULT_OPERATOR;
-    /** If result is a boolean query, minimumShouldMatch parameter to apply. Ignored otherwise. */
+    private Operator operator;
+    private final String queryText;
+    private String queryName;
     private String minimumShouldMatch;
-    /** Any search flags to be used, ALL by default. */
-    private int flags = DEFAULT_FLAGS;
-
-    /** Further search settings needed by the ES specific query string parser only. */
-    private Settings settings = new Settings();
+    private int flags = -1;
+    private float boost = -1.0f;
+    private Boolean lowercaseExpandedTerms;
+    private Boolean lenient;
+    private Boolean analyzeWildcard;
+    private Locale locale;
 
-    static final SimpleQueryStringBuilder PROTOTYPE = new SimpleQueryStringBuilder(null);
+    /**
+     * Operators for the default_operator
+     */
+    public static enum Operator {
+        AND,
+        OR
+    }
 
-    /** Construct a new simple query with this query string. */
-    public SimpleQueryStringBuilder(String queryText) {
-        this.queryText = queryText;
+    /**
+     * Construct a new simple query with the given text
+     */
+    public SimpleQueryStringBuilder(String text) {
+        this.queryText = text;
     }
 
-    /** Returns the text to parse the query from. */
-    public String value() {
-        return this.queryText;
+    /** Set the boost of this query. */
+    @Override
+    public SimpleQueryStringBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+    
+    /** Returns the boost of this query. */
+    public float boost() {
+        return this.boost;
     }
 
-    /** Add a field to run the query against. */
+    /**
+     * Add a field to run the query against
+     */
     public SimpleQueryStringBuilder field(String field) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsAndWeights.put(field, AbstractQueryBuilder.DEFAULT_BOOST);
+        this.fields.put(field, null);
         return this;
     }
 
-    /** Add a field to run the query against with a specific boost. */
+    /**
+     * Add a field to run the query against with a specific boost
+     */
     public SimpleQueryStringBuilder field(String field, float boost) {
-        if (Strings.isEmpty(field)) {
-            throw new IllegalArgumentException("supplied field is null or empty.");
-        }
-        this.fieldsAndWeights.put(field, boost);
+        this.fields.put(field, boost);
         return this;
     }
 
-    /** Add several fields to run the query against with a specific boost. */
-    public SimpleQueryStringBuilder fields(Map<String, Float> fields) {
-        this.fieldsAndWeights.putAll(fields);
+    /**
+     * Specify a name for the query
+     */
+    public SimpleQueryStringBuilder queryName(String name) {
+        this.queryName = name;
         return this;
     }
 
-    /** Returns the fields including their respective boosts to run the query against. */
-    public Map<String, Float> fields() {
-        return this.fieldsAndWeights;
-    }
-
-    /** Specify an analyzer to use for the query. */
+    /**
+     * Specify an analyzer to use for the query
+     */
     public SimpleQueryStringBuilder analyzer(String analyzer) {
         this.analyzer = analyzer;
         return this;
     }
 
-    /** Returns the analyzer to use for the query. */
-    public String analyzer() {
-        return this.analyzer;
-    }
-
     /**
      * Specify the default operator for the query. Defaults to "OR" if no
-     * operator is specified.
+     * operator is specified
      */
     public SimpleQueryStringBuilder defaultOperator(Operator defaultOperator) {
-        this.defaultOperator = (defaultOperator != null) ? defaultOperator : DEFAULT_OPERATOR;
+        this.operator = defaultOperator;
         return this;
     }
 
-    /** Returns the default operator for the query. */
-    public Operator defaultOperator() {
-        return this.defaultOperator;
-    }
-
     /**
-     * Specify the enabled features of the SimpleQueryString. Defaults to ALL if
-     * none are specified.
+     * Specify the enabled features of the SimpleQueryString.
      */
     public SimpleQueryStringBuilder flags(SimpleQueryStringFlag... flags) {
-        if (flags != null && flags.length > 0) {
-            int value = 0;
+        int value = 0;
+        if (flags.length == 0) {
+            value = SimpleQueryStringFlag.ALL.value;
+        } else {
             for (SimpleQueryStringFlag flag : flags) {
                 value |= flag.value;
             }
-            this.flags = value;
-        } else {
-            this.flags = DEFAULT_FLAGS;
         }
-
+        this.flags = value;
         return this;
     }
 
-    /** For testing and serialisation only. */
-    SimpleQueryStringBuilder flags(int flags) {
-        this.flags = flags;
-        return this;
-    }
-
-    /** For testing only: Return the flags set for this query. */
-    int flags() {
-        return this.flags;
-    }
-
-    /**
-     * Specifies whether parsed terms for this query should be lower-cased.
-     * Defaults to true if not set.
-     */
     public SimpleQueryStringBuilder lowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-        this.settings.lowercaseExpandedTerms(lowercaseExpandedTerms);
+        this.lowercaseExpandedTerms = lowercaseExpandedTerms;
         return this;
     }
 
-    /** Returns whether parsed terms should be lower cased for this query. */
-    public boolean lowercaseExpandedTerms() {
-        return this.settings.lowercaseExpandedTerms();
-    }
-
-    /** Specifies the locale for parsing terms. Defaults to ROOT if none is set. */
     public SimpleQueryStringBuilder locale(Locale locale) {
-        this.settings.locale(locale);
+        this.locale = locale;
         return this;
     }
 
-    /** Returns the locale for parsing terms for this query. */
-    public Locale locale() {
-        return this.settings.locale();
-    }
-
-    /** Specifies whether query parsing should be lenient. Defaults to false. */
     public SimpleQueryStringBuilder lenient(boolean lenient) {
-        this.settings.lenient(lenient);
+        this.lenient = lenient;
         return this;
     }
 
-    /** Returns whether query parsing should be lenient. */
-    public boolean lenient() {
-        return this.settings.lenient();
-    }
-
-    /** Specifies whether wildcards should be analyzed. Defaults to false. */
     public SimpleQueryStringBuilder analyzeWildcard(boolean analyzeWildcard) {
-        this.settings.analyzeWildcard(analyzeWildcard);
+        this.analyzeWildcard = analyzeWildcard;
         return this;
     }
 
-    /** Returns whether wildcards should by analyzed. */
-    public boolean analyzeWildcard() {
-        return this.settings.analyzeWildcard();
-    }
-
-    /**
-     * Specifies the minimumShouldMatch to apply to the resulting query should
-     * that be a Boolean query.
-     */
     public SimpleQueryStringBuilder minimumShouldMatch(String minimumShouldMatch) {
         this.minimumShouldMatch = minimumShouldMatch;
         return this;
     }
 
-    /**
-     * Returns the minimumShouldMatch to apply to the resulting query should
-     * that be a Boolean query.
-     */
-    public String minimumShouldMatch() {
-        return minimumShouldMatch;
-    }
-
-    /**
-     * {@inheritDoc}
-     *
-     * Checks that mandatory queryText is neither null nor empty.
-     * */
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        // Query text is required
-        if (queryText == null) {
-            validationException = addValidationError("query text missing", validationException);
-        }
-
-        return validationException;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        // Use the default field if no fields specified
-        if (fieldsAndWeights.isEmpty()) {
-            fieldsAndWeights.put(context.defaultField(), AbstractQueryBuilder.DEFAULT_BOOST);
-        }
-
-        // field names in builder can have wildcards etc, need to resolve them here
-        Map<String, Float> resolvedFieldsAndWeights = new TreeMap<>();
-        for (String fField : fieldsAndWeights.keySet()) {
-            if (Regex.isSimpleMatchPattern(fField)) {
-                for (String fieldName : context.mapperService().simpleMatchToIndexNames(fField)) {
-                    resolvedFieldsAndWeights.put(fieldName, fieldsAndWeights.get(fField));
-                }
-            } else {
-                MappedFieldType fieldType = context.fieldMapper(fField);
-                if (fieldType != null) {
-                    resolvedFieldsAndWeights.put(fieldType.names().indexName(), fieldsAndWeights.get(fField));
-                } else {
-                    resolvedFieldsAndWeights.put(fField, fieldsAndWeights.get(fField));
-                }
-            }
-        }
-
-        // Use standard analyzer by default if none specified
-        Analyzer luceneAnalyzer;
-        if (analyzer == null) {
-            luceneAnalyzer = context.mapperService().searchAnalyzer();
-        } else {
-            luceneAnalyzer = context.analysisService().analyzer(analyzer);
-            if (luceneAnalyzer == null) {
-                throw new QueryShardException(context, "[" + SimpleQueryStringBuilder.NAME + "] analyzer [" + analyzer
-                        + "] not found");
-            }
-
-        }
-
-        SimpleQueryParser sqp = new SimpleQueryParser(luceneAnalyzer, resolvedFieldsAndWeights, flags, settings);
-        sqp.setDefaultOperator(defaultOperator.toBooleanClauseOccur());
-
-        Query query = sqp.parse(queryText);
-        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
-            Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
-        }
-        return query;
-    }
-
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(SimpleQueryStringParser.NAME);
 
         builder.field("query", queryText);
 
-        if (fieldsAndWeights.size() > 0) {
+        if (fields.size() > 0) {
             builder.startArray("fields");
-            for (Map.Entry<String, Float> entry : fieldsAndWeights.entrySet()) {
+            for (Map.Entry<String, Float> entry : fields.entrySet()) {
                 String field = entry.getKey();
                 Float boost = entry.getValue();
                 if (boost != null) {
@@ -328,82 +174,47 @@ public class SimpleQueryStringBuilder extends AbstractQueryBuilder<SimpleQuerySt
             builder.endArray();
         }
 
+        if (flags != -1) {
+            builder.field("flags", flags);
+        }
+
         if (analyzer != null) {
             builder.field("analyzer", analyzer);
         }
 
-        builder.field("flags", flags);
-        builder.field("default_operator", defaultOperator.name().toLowerCase(Locale.ROOT));
-        builder.field("lowercase_expanded_terms", settings.lowercaseExpandedTerms());
-        builder.field("lenient", settings.lenient());
-        builder.field("analyze_wildcard", settings.analyzeWildcard());
-        builder.field("locale", (settings.locale().toLanguageTag()));
+        if (operator != null) {
+            builder.field("default_operator", operator.name().toLowerCase(Locale.ROOT));
+        }
 
-        if (minimumShouldMatch != null) {
-            builder.field("minimum_should_match", minimumShouldMatch);
+        if (lowercaseExpandedTerms != null) {
+            builder.field("lowercase_expanded_terms", lowercaseExpandedTerms);
         }
 
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
+        if (lenient != null) {
+            builder.field("lenient", lenient);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
+        if (analyzeWildcard != null) {
+            builder.field("analyze_wildcard", analyzeWildcard);
+        }
 
-    @Override
-    protected SimpleQueryStringBuilder doReadFrom(StreamInput in) throws IOException {
-        SimpleQueryStringBuilder result = new SimpleQueryStringBuilder(in.readString());
-        int size = in.readInt();
-        Map<String, Float> fields = new HashMap<>();
-        for (int i = 0; i < size; i++) {
-            String field = in.readString();
-            Float weight = in.readFloat();
-            fields.put(field, weight);
+        if (locale != null) {
+            builder.field("locale", locale.toString());
         }
-        result.fieldsAndWeights.putAll(fields);
-        result.flags = in.readInt();
-        result.analyzer = in.readOptionalString();
-        result.defaultOperator = Operator.readOperatorFrom(in);
-        result.settings.lowercaseExpandedTerms(in.readBoolean());
-        result.settings.lenient(in.readBoolean());
-        result.settings.analyzeWildcard(in.readBoolean());
-        String localeStr = in.readString();
-        result.settings.locale(Locale.forLanguageTag(localeStr));
-        result.minimumShouldMatch = in.readOptionalString();
-        return result;
-    }
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(queryText);
-        out.writeInt(fieldsAndWeights.size());
-        for (Map.Entry<String, Float> entry : fieldsAndWeights.entrySet()) {
-            out.writeString(entry.getKey());
-            out.writeFloat(entry.getValue());
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        out.writeInt(flags);
-        out.writeOptionalString(analyzer);
-        defaultOperator.writeTo(out);
-        out.writeBoolean(settings.lowercaseExpandedTerms());
-        out.writeBoolean(settings.lenient());
-        out.writeBoolean(settings.analyzeWildcard());
-        out.writeString(settings.locale().toLanguageTag());
-        out.writeOptionalString(minimumShouldMatch);
-    }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldsAndWeights, analyzer, defaultOperator, queryText, minimumShouldMatch, settings, flags);
-    }
+        if (minimumShouldMatch != null) {
+            builder.field("minimum_should_match", minimumShouldMatch);
+        }
+        
+        if (boost != -1.0f) {
+            builder.field("boost", boost);
+        }
 
-    @Override
-    protected boolean doEquals(SimpleQueryStringBuilder other) {
-        return Objects.equals(fieldsAndWeights, other.fieldsAndWeights) && Objects.equals(analyzer, other.analyzer)
-                && Objects.equals(defaultOperator, other.defaultOperator) && Objects.equals(queryText, other.queryText)
-                && Objects.equals(minimumShouldMatch, other.minimumShouldMatch)
-                && Objects.equals(settings, other.settings) && (flags == other.flags);
+        builder.endObject();
     }
-}
 
+}
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
index 68d19db..ce0ce88 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java
@@ -71,7 +71,7 @@ public enum SimpleQueryStringFlag {
                         magic |= flag.value();
                 }
             } catch (IllegalArgumentException iae) {
-                throw new IllegalArgumentException("Unknown " + SimpleQueryStringBuilder.NAME + " flag [" + s + "]");
+                throw new IllegalArgumentException("Unknown " + SimpleQueryStringParser.NAME + " flag [" + s + "]");
             }
         }
         return magic;
diff --git a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
index e45659e..d80423d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java
@@ -19,11 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.common.regex.Regex;
+import org.elasticsearch.common.util.LocaleUtils;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Locale;
 import java.util.Map;
@@ -59,7 +68,9 @@ import java.util.Map;
  * {@code fields} - fields to search, defaults to _all if not set, allows
  * boosting a field with ^n
  */
-public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBuilder> {
+public class SimpleQueryStringParser implements QueryParser {
+
+    public static final String NAME = "simple_query_string";
 
     @Inject
     public SimpleQueryStringParser() {
@@ -68,26 +79,23 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
 
     @Override
     public String[] names() {
-        return new String[]{SimpleQueryStringBuilder.NAME, Strings.toCamelCase(SimpleQueryStringBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SimpleQueryStringBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String currentFieldName = null;
         String queryBody = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f; 
         String queryName = null;
         String minimumShouldMatch = null;
-        Map<String, Float> fieldsAndWeights = new HashMap<>();
-        Operator defaultOperator = null;
-        String analyzerName = null;
-        int flags = SimpleQueryStringFlag.ALL.value();
-        boolean lenient = SimpleQueryStringBuilder.DEFAULT_LENIENT;
-        boolean lowercaseExpandedTerms = SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS;
-        boolean analyzeWildcard = SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD;
-        Locale locale = null;
+        Map<String, Float> fieldsAndWeights = null;
+        BooleanClause.Occur defaultOperator = null;
+        Analyzer analyzer = null;
+        int flags = -1;
+        SimpleQueryParser.Settings sqsSettings = new SimpleQueryParser.Settings();
 
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -111,10 +119,26 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                         if (fField == null) {
                             fField = parser.text();
                         }
-                        fieldsAndWeights.put(fField, fBoost);
+
+                        if (fieldsAndWeights == null) {
+                            fieldsAndWeights = new HashMap<>();
+                        }
+
+                        if (Regex.isSimpleMatchPattern(fField)) {
+                            for (String fieldName : parseContext.mapperService().simpleMatchToIndexNames(fField)) {
+                                fieldsAndWeights.put(fieldName, fBoost);
+                            }
+                        } else {
+                            MappedFieldType fieldType = parseContext.fieldMapper(fField);
+                            if (fieldType != null) {
+                                fieldsAndWeights.put(fieldType.names().indexName(), fBoost);
+                            } else {
+                                fieldsAndWeights.put(fField, fBoost);
+                            }
+                        }
                     }
                 } else {
-                    throw new QueryParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] query does not support [" + currentFieldName + "]");
+                    throw new QueryParsingException(parseContext, "[" + NAME + "] query does not support [" + currentFieldName + "]");
                 }
             } else if (token.isValue()) {
                 if ("query".equals(currentFieldName)) {
@@ -122,9 +146,19 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("analyzer".equals(currentFieldName)) {
-                    analyzerName = parser.text();
+                    analyzer = parseContext.analysisService().analyzer(parser.text());
+                    if (analyzer == null) {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] analyzer [" + parser.text() + "] not found");
+                    }
                 } else if ("default_operator".equals(currentFieldName) || "defaultOperator".equals(currentFieldName)) {
-                    defaultOperator = Operator.fromString(parser.text());
+                    String op = parser.text();
+                    if ("or".equalsIgnoreCase(op)) {
+                        defaultOperator = BooleanClause.Occur.SHOULD;
+                    } else if ("and".equalsIgnoreCase(op)) {
+                        defaultOperator = BooleanClause.Occur.MUST;
+                    } else {
+                        throw new QueryParsingException(parseContext, "[" + NAME + "] default operator [" + op + "] is not allowed");
+                    }
                 } else if ("flags".equals(currentFieldName)) {
                     if (parser.currentToken() != XContentParser.Token.VALUE_NUMBER) {
                         // Possible options are:
@@ -138,38 +172,56 @@ public class SimpleQueryStringParser extends BaseQueryParser<SimpleQueryStringBu
                     }
                 } else if ("locale".equals(currentFieldName)) {
                     String localeStr = parser.text();
-                    locale = Locale.forLanguageTag(localeStr);
+                    Locale locale = LocaleUtils.parse(localeStr);
+                    sqsSettings.locale(locale);
                 } else if ("lowercase_expanded_terms".equals(currentFieldName)) {
-                    lowercaseExpandedTerms = parser.booleanValue();
+                    sqsSettings.lowercaseExpandedTerms(parser.booleanValue());
                 } else if ("lenient".equals(currentFieldName)) {
-                    lenient = parser.booleanValue();
+                    sqsSettings.lenient(parser.booleanValue());
                 } else if ("analyze_wildcard".equals(currentFieldName)) {
-                    analyzeWildcard = parser.booleanValue();
+                    sqsSettings.analyzeWildcard(parser.booleanValue());
                 } else if ("_name".equals(currentFieldName)) {
                     queryName = parser.text();
                 } else if ("minimum_should_match".equals(currentFieldName)) {
                     minimumShouldMatch = parser.textOrNull();
                 } else {
-                    throw new QueryParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] unsupported field [" + parser.currentName() + "]");
+                    throw new QueryParsingException(parseContext, "[" + NAME + "] unsupported field [" + parser.currentName() + "]");
                 }
             }
         }
 
         // Query text is required
         if (queryBody == null) {
-            throw new QueryParsingException(parseContext, "[" + SimpleQueryStringBuilder.NAME + "] query text missing");
+            throw new QueryParsingException(parseContext, "[" + NAME + "] query text missing");
+        }
+
+        // Use standard analyzer by default
+        if (analyzer == null) {
+            analyzer = parseContext.mapperService().searchAnalyzer();
         }
 
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder(queryBody);
-        qb.boost(boost).fields(fieldsAndWeights).analyzer(analyzerName).queryName(queryName).minimumShouldMatch(minimumShouldMatch);
-        qb.flags(flags).defaultOperator(defaultOperator).locale(locale).lowercaseExpandedTerms(lowercaseExpandedTerms);
-        qb.lenient(lenient).analyzeWildcard(analyzeWildcard).boost(boost);
+        if (fieldsAndWeights == null) {
+            fieldsAndWeights = Collections.singletonMap(parseContext.defaultField(), 1.0F);
+        }
+        SimpleQueryParser sqp = new SimpleQueryParser(analyzer, fieldsAndWeights, flags, sqsSettings);
 
-        return qb;
-    }
+        if (defaultOperator != null) {
+            sqp.setDefaultOperator(defaultOperator);
+        }
 
-    @Override
-    public SimpleQueryStringBuilder getBuilderPrototype() {
-        return SimpleQueryStringBuilder.PROTOTYPE;
+        Query query = sqp.parse(queryBody);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+
+        if (minimumShouldMatch != null && query instanceof BooleanQuery) {
+            Queries.applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);
+        }
+
+        if (query != null) {
+            query.setBoost(boost);
+        }
+
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
index 81aa01c..0b7a3cd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryBuilder.java
@@ -19,111 +19,74 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanContainingQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Builder for {@link org.apache.lucene.search.spans.SpanContainingQuery}.
  */
-public class SpanContainingQueryBuilder extends AbstractQueryBuilder<SpanContainingQueryBuilder> implements SpanQueryBuilder<SpanContainingQueryBuilder> {
+public class SpanContainingQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanContainingQueryBuilder> {
 
-    public static final String NAME = "span_containing";
-    private final SpanQueryBuilder big;
-    private final SpanQueryBuilder little;
-    static final SpanContainingQueryBuilder PROTOTYPE = new SpanContainingQueryBuilder(null, null);
+    private SpanQueryBuilder big;
+    private SpanQueryBuilder little;
+    private float boost = -1;
+    private String queryName;
 
-    /**
-     * @param big the big clause, it must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
+    /** 
+     * Sets the little clause, it must be contained within {@code big} for a match.
      */
-    public SpanContainingQueryBuilder(SpanQueryBuilder big, SpanQueryBuilder little) {
-        this.little = little;
-        this.big = big;
+    public SpanContainingQueryBuilder little(SpanQueryBuilder clause) {
+        this.little = clause;
+        return this;
     }
 
-    /**
-     * @return the big clause, it must enclose {@code little} for a match.
+    /** 
+     * Sets the big clause, it must enclose {@code little} for a match.
      */
-    public SpanQueryBuilder bigQuery() {
-        return this.big;
-    }
-
-    /**
-     * @return the little clause, it must be contained within {@code big} for a match.
-     */
-    public SpanQueryBuilder littleQuery() {
-        return this.little;
+    public SpanContainingQueryBuilder big(SpanQueryBuilder clause) {
+        this.big = clause;
+        return this;
     }
 
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("big");
-        big.toXContent(builder, params);
-        builder.field("little");
-        little.toXContent(builder, params);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public SpanContainingQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerBig = big.toQuery(context);
-        assert innerBig instanceof SpanQuery;
-        Query innerLittle = little.toQuery(context);
-        assert innerLittle instanceof SpanQuery;
-        return new SpanContainingQuery((SpanQuery) innerBig, (SpanQuery) innerLittle);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public SpanContainingQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         if (big == null) {
-            validationException = addValidationError("inner clause [big] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(big, validationException);
+            throw new IllegalArgumentException("Must specify big clause when building a span_containing query");
         }
         if (little == null) {
-            validationException = addValidationError("inner clause [little] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(little, validationException);
+            throw new IllegalArgumentException("Must specify little clause when building a span_containing query");
         }
-        return validationException;
-    }
+        builder.startObject(SpanContainingQueryParser.NAME);
 
-    @Override
-    protected SpanContainingQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder big = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder little = (SpanQueryBuilder)in.readQuery();
-        return new SpanContainingQueryBuilder(big, little);
-    }
+        builder.field("big");
+        big.toXContent(builder, params);
 
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(big);
-        out.writeQuery(little);
-    }
+        builder.field("little");
+        little.toXContent(builder, params);
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(big, little);
-    }
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
 
-    @Override
-    protected boolean doEquals(SpanContainingQueryBuilder other) {
-        return Objects.equals(big, other.big) &&
-               Objects.equals(little, other.little);
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
index affc853..63e312b 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanContainingQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanContainingQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,9 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_containing query
+ * Parser for {@link SpanContainingQuery}
  */
-public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQueryBuilder> {
+public class SpanContainingQueryParser implements QueryParser {
+
+    public static final String NAME = "span_containing";
 
     @Inject
     public SpanContainingQueryParser() {
@@ -36,16 +41,17 @@ public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQue
 
     @Override
     public String[] names() {
-        return new String[]{SpanContainingQueryBuilder.NAME, Strings.toCamelCase(SpanContainingQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanContainingQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+
+        float boost = 1.0f;
         String queryName = null;
-        SpanQueryBuilder<?> big = null;
-        SpanQueryBuilder<?> little = null;
+        SpanQuery big = null;
+        SpanQuery little = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -54,17 +60,17 @@ public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQue
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("big".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder<?>)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "span_containing [big] must be of type span query");
                     }
-                    big = (SpanQueryBuilder<?>) query;
+                    big = (SpanQuery) query;
                 } else if ("little".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder<?>)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "span_containing [little] must be of type span query");
                     }
-                    little = (SpanQueryBuilder<?>) query;
+                    little = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_containing] query does not support [" + currentFieldName + "]");
                 }
@@ -75,15 +81,20 @@ public class SpanContainingQueryParser extends BaseQueryParser<SpanContainingQue
             } else {
                 throw new QueryParsingException(parseContext, "[span_containing] query does not support [" + currentFieldName + "]");
             }
+        }        
+        
+        if (big == null) {
+            throw new QueryParsingException(parseContext, "span_containing must include [big]");
+        }
+        if (little == null) {
+            throw new QueryParsingException(parseContext, "span_containing must include [little]");
         }
 
-        SpanContainingQueryBuilder query = new SpanContainingQueryBuilder(big, little);
-        query.boost(boost).queryName(queryName);
+        Query query = new SpanContainingQuery(big, little);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
         return query;
     }
-
-    @Override
-    public SpanContainingQueryBuilder getBuilderPrototype() {
-        return SpanContainingQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
index 1115ffc..f967a1c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java
@@ -19,109 +19,51 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class SpanFirstQueryBuilder extends AbstractQueryBuilder<SpanFirstQueryBuilder> implements SpanQueryBuilder<SpanFirstQueryBuilder>{
-
-    public static final String NAME = "span_first";
+public class SpanFirstQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanFirstQueryBuilder> {
 
     private final SpanQueryBuilder matchBuilder;
 
     private final int end;
 
-    static final SpanFirstQueryBuilder SPAN_FIRST_QUERY_BUILDER = new SpanFirstQueryBuilder(null, -1);
+    private float boost = -1;
+
+    private String queryName;
 
-    /**
-     * Query that matches spans queries defined in <code>matchBuilder</code>
-     * whose end position is less than or equal to <code>end</code>.
-     * @param matchBuilder inner {@link SpanQueryBuilder}
-     * @param end maximum end position of the match, needs to be positive
-     * @throws IllegalArgumentException for negative <code>end</code> positions
-     */
     public SpanFirstQueryBuilder(SpanQueryBuilder matchBuilder, int end) {
         this.matchBuilder = matchBuilder;
         this.end = end;
     }
 
-    /**
-     * @return the inner {@link SpanQueryBuilder} defined in this query
-     */
-    public SpanQueryBuilder innerQuery() {
-        return this.matchBuilder;
+    @Override
+    public SpanFirstQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return maximum end position of the matching inner span query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public int end() {
-        return this.end;
+    public SpanFirstQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(SpanFirstQueryParser.NAME);
         builder.field("match");
         matchBuilder.toXContent(builder, params);
         builder.field("end", end);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerSpanQuery = matchBuilder.toQuery(context);
-        assert innerSpanQuery instanceof SpanQuery;
-        return new SpanFirstQuery((SpanQuery) innerSpanQuery, end);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (matchBuilder == null) {
-            validationException = addValidationError("inner clause [match] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(matchBuilder, validationException);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (end < 0) {
-            validationException = addValidationError("parameter [end] needs to be positive.", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanFirstQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder matchBuilder = (SpanQueryBuilder)in.readQuery();
-        int end = in.readInt();
-        return new SpanFirstQueryBuilder(matchBuilder, end);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(matchBuilder);
-        out.writeInt(end);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(matchBuilder, end);
-    }
-
-    @Override
-    protected boolean doEquals(SpanFirstQueryBuilder other) {
-        return Objects.equals(matchBuilder, other.matchBuilder) &&
-               Objects.equals(end, other.end);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
index 987dbc0..5a302eb 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanFirstQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,9 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_first query
+ *
  */
-public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder> {
+public class SpanFirstQueryParser implements QueryParser {
+
+    public static final String NAME = "span_first";
 
     @Inject
     public SpanFirstQueryParser() {
@@ -36,17 +41,17 @@ public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder>
 
     @Override
     public String[] names() {
-        return new String[]{SpanFirstQueryBuilder.NAME, Strings.toCamelCase(SpanFirstQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanFirstQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder match = null;
-        Integer end = null;
+        SpanQuery match = null;
+        int end = -1;
         String queryName = null;
 
         String currentFieldName = null;
@@ -56,11 +61,11 @@ public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder>
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("match".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanFirst [match] must be of type span query");
                     }
-                    match = (SpanQueryBuilder) query;
+                    match = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_first] query does not support [" + currentFieldName + "]");
                 }
@@ -79,16 +84,15 @@ public class SpanFirstQueryParser extends BaseQueryParser<SpanFirstQueryBuilder>
         if (match == null) {
             throw new QueryParsingException(parseContext, "spanFirst must have [match] span query clause");
         }
-        if (end == null) {
+        if (end == -1) {
             throw new QueryParsingException(parseContext, "spanFirst must have [end] set for it");
         }
-        SpanFirstQueryBuilder queryBuilder = new SpanFirstQueryBuilder(match, end);
-        queryBuilder.boost(boost).queryName(queryName);
-        return queryBuilder;
-    }
 
-    @Override
-    public SpanFirstQueryBuilder getBuilderPrototype() {
-        return SpanFirstQueryBuilder.SPAN_FIRST_QUERY_BUILDER;
+        SpanFirstQuery query = new SpanFirstQuery(match, end);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
index a31b17e..11b9897 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilder.java
@@ -18,88 +18,25 @@
  */
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-/**
- * Query that allows wraping a {@link MultiTermQueryBuilder} (one of wildcard, fuzzy, prefix, term, range or regexp query)
- * as a {@link SpanQueryBuilder} so it can be nested.
- */
-public class SpanMultiTermQueryBuilder extends AbstractQueryBuilder<SpanMultiTermQueryBuilder> implements SpanQueryBuilder<SpanMultiTermQueryBuilder> {
+public class SpanMultiTermQueryBuilder extends SpanQueryBuilder {
 
-    public static final String NAME = "span_multi";
-    private final MultiTermQueryBuilder multiTermQueryBuilder;
-    static final SpanMultiTermQueryBuilder PROTOTYPE = new SpanMultiTermQueryBuilder(null);
+    private MultiTermQueryBuilder multiTermQueryBuilder;
 
     public SpanMultiTermQueryBuilder(MultiTermQueryBuilder multiTermQueryBuilder) {
         this.multiTermQueryBuilder = multiTermQueryBuilder;
     }
 
-    public MultiTermQueryBuilder innerQuery() {
-        return this.multiTermQueryBuilder;
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params)
             throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(SpanMultiTermQueryParser.NAME);
         builder.field(SpanMultiTermQueryParser.MATCH_NAME);
         multiTermQueryBuilder.toXContent(builder, params);
-        printBoostAndQueryName(builder);
         builder.endObject();
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query subQuery = multiTermQueryBuilder.toQuery(context);
-        if (subQuery instanceof MultiTermQuery == false) {
-            throw new UnsupportedOperationException("unsupported inner query, should be " + MultiTermQuery.class.getName() +" but was "
-                    + subQuery.getClass().getName());
-        }
-        return new SpanMultiTermQueryWrapper<>((MultiTermQuery) subQuery);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (multiTermQueryBuilder == null) {
-            validationException = addValidationError("inner clause ["+ SpanMultiTermQueryParser.MATCH_NAME +"] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(multiTermQueryBuilder, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanMultiTermQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        MultiTermQueryBuilder multiTermBuilder = (MultiTermQueryBuilder)in.readQuery();
-        return new SpanMultiTermQueryBuilder(multiTermBuilder);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected boolean doEquals(SpanMultiTermQueryBuilder other) {
-        return Objects.equals(multiTermQueryBuilder, other.multiTermQueryBuilder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
index 77e9def..a44580a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanMultiTermQueryParser.java
@@ -18,17 +18,22 @@
  */
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.common.xcontent.XContentParser.Token;
 
 import java.io.IOException;
 
 /**
- * Parser for span_multi query
+ *
  */
-public class SpanMultiTermQueryParser extends BaseQueryParser<SpanMultiTermQueryBuilder> {
+public class SpanMultiTermQueryParser implements QueryParser {
 
+    public static final String NAME = "span_multi";
     public static final String MATCH_NAME = "match";
 
     @Inject
@@ -37,50 +42,29 @@ public class SpanMultiTermQueryParser extends BaseQueryParser<SpanMultiTermQuery
 
     @Override
     public String[] names() {
-        return new String[]{SpanMultiTermQueryBuilder.NAME, Strings.toCamelCase(SpanMultiTermQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanMultiTermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        String currentFieldName = null;
-        MultiTermQueryBuilder subQuery = null;
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token == XContentParser.Token.START_OBJECT) {
-                if (MATCH_NAME.equals(currentFieldName)) {
-                    QueryBuilder innerQuery = parseContext.parseInnerQueryBuilder();
-                    if (innerQuery instanceof MultiTermQueryBuilder == false) {
-                        throw new QueryParsingException(parseContext, "[span_multi] [" + MATCH_NAME + "] must be of type multi term query");
-                    }
-                    subQuery = (MultiTermQueryBuilder) innerQuery;
-                } else {
-                    throw new QueryParsingException(parseContext, "[span_multi] query does not support [" + currentFieldName + "]");
-                }
-            } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else {
-                    throw new QueryParsingException(parseContext, "[span_multi] query does not support [" + currentFieldName + "]");
-                }
-            }
+
+        Token token = parser.nextToken();
+        if (!MATCH_NAME.equals(parser.currentName()) || token != XContentParser.Token.FIELD_NAME) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm must have [" + MATCH_NAME + "] multi term query clause");
         }
 
-        if (subQuery == null) {
-            throw new QueryParsingException(parseContext, "[span_multi] must have [" + MATCH_NAME + "] multi term query clause");
+        token = parser.nextToken();
+        if (token != XContentParser.Token.START_OBJECT) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm must have [" + MATCH_NAME + "] multi term query clause");
         }
 
-        return new SpanMultiTermQueryBuilder(subQuery).queryName(queryName).boost(boost);
-    }
+        Query subQuery = parseContext.parseInnerQuery();
+        if (!(subQuery instanceof MultiTermQuery)) {
+            throw new QueryParsingException(parseContext, "spanMultiTerm [" + MATCH_NAME + "] must be of type multi term query");
+        }
 
-    @Override
-    public SpanMultiTermQueryBuilder getBuilderPrototype() {
-        return SpanMultiTermQueryBuilder.PROTOTYPE;
+        parser.nextToken();
+        return new SpanMultiTermQueryWrapper<>((MultiTermQuery) subQuery);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
index e00cc32..cb05e08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java
@@ -19,179 +19,86 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
-/**
- * Matches spans which are near one another. One can specify slop, the maximum number
- * of intervening unmatched positions, as well as whether matches are required to be in-order.
- * The span near query maps to Lucene {@link SpanNearQuery}.
- */
-public class SpanNearQueryBuilder extends AbstractQueryBuilder<SpanNearQueryBuilder> implements SpanQueryBuilder<SpanNearQueryBuilder> {
-
-    public static final String NAME = "span_near";
-
-    /** Default for flag controlling whether matches are required to be in-order */
-    public static boolean DEFAULT_IN_ORDER = true;
-
-    /** Default for flag controlling whether payloads are collected */
-    public static boolean DEFAULT_COLLECT_PAYLOADS = true;
+public class SpanNearQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanNearQueryBuilder> {
 
-    private final List<SpanQueryBuilder> clauses = new ArrayList<>();
+    private ArrayList<SpanQueryBuilder> clauses = new ArrayList<>();
 
-    private final int slop;
+    private Integer slop = null;
 
-    private boolean inOrder = DEFAULT_IN_ORDER;
+    private Boolean inOrder;
 
-    private boolean collectPayloads = DEFAULT_COLLECT_PAYLOADS;
+    private Boolean collectPayloads;
 
-    static final SpanNearQueryBuilder PROTOTYPE = new SpanNearQueryBuilder(0);
+    private float boost = -1;
 
-    /**
-     * @param slop controls the maximum number of intervening unmatched positions permitted
-     */
-    public SpanNearQueryBuilder(int slop) {
-        this.slop = slop;
-    }
-
-    /**
-     * @return the maximum number of intervening unmatched positions permitted
-     */
-    public int slop() {
-        return this.slop;
-    }
+    private String queryName;
 
     public SpanNearQueryBuilder clause(SpanQueryBuilder clause) {
         clauses.add(clause);
         return this;
     }
 
-    /**
-     * @return the {@link SpanQueryBuilder} clauses that were set for this query
-     */
-    public List<SpanQueryBuilder> clauses() {
-        return this.clauses;
+    public SpanNearQueryBuilder slop(int slop) {
+        this.slop = slop;
+        return this;
     }
 
-    /**
-     * When <code>inOrder</code> is true, the spans from each clause
-     * must be in the same order as in <code>clauses</code> and must be non-overlapping.
-     * Defaults to <code>true</code>
-     */
     public SpanNearQueryBuilder inOrder(boolean inOrder) {
         this.inOrder = inOrder;
         return this;
     }
 
-    /**
-     * @see SpanNearQueryBuilder#inOrder(boolean))
-     */
-    public boolean inOrder() {
-        return this.inOrder;
-    }
-
-    /**
-     * @param collectPayloads flag controlling whether payloads are collected
-     */
     public SpanNearQueryBuilder collectPayloads(boolean collectPayloads) {
         this.collectPayloads = collectPayloads;
         return this;
     }
 
+    @Override
+    public SpanNearQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @see SpanNearQueryBuilder#collectPayloads(boolean))
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public boolean collectPayloads() {
-        return this.collectPayloads;
+    public SpanNearQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (clauses.isEmpty()) {
+            throw new IllegalArgumentException("Must have at least one clause when building a spanNear query");
+        }
+        if (slop == null) {
+            throw new IllegalArgumentException("Must set the slop when building a spanNear query");
+        }
+        builder.startObject(SpanNearQueryParser.NAME);
         builder.startArray("clauses");
         for (SpanQueryBuilder clause : clauses) {
             clause.toXContent(builder, params);
         }
         builder.endArray();
-        builder.field("slop", slop);
-        builder.field("in_order", inOrder);
-        builder.field("collect_payloads", collectPayloads);
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        SpanQuery[] spanQueries = new SpanQuery[clauses.size()];
-        for (int i = 0; i < clauses.size(); i++) {
-            Query query = clauses.get(i).toQuery(context);
-            assert query instanceof SpanQuery;
-            spanQueries[i] = (SpanQuery) query;
+        builder.field("slop", slop.intValue());
+        if (inOrder != null) {
+            builder.field("in_order", inOrder);
         }
-        return new SpanNearQuery(spanQueries, slop, inOrder, collectPayloads);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (clauses.isEmpty()) {
-            validationException =  addValidationError("query must include [clauses]", validationException);
+        if (collectPayloads != null) {
+            builder.field("collect_payloads", collectPayloads);
         }
-        for (SpanQueryBuilder innerClause : clauses) {
-            if (innerClause == null) {
-                validationException =  addValidationError("[clauses] contains null element", validationException);
-            } else {
-                validationException = validateInnerQuery(innerClause, validationException);
-            }
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanNearQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(in.readVInt());
-        List<QueryBuilder> clauses = readQueries(in);
-        for (QueryBuilder subClause : clauses) {
-            queryBuilder.clauses.add((SpanQueryBuilder)subClause);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        queryBuilder.collectPayloads = in.readBoolean();
-        queryBuilder.inOrder = in.readBoolean();
-        return queryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeVInt(slop);
-        writeQueries(out, clauses);
-        out.writeBoolean(collectPayloads);
-        out.writeBoolean(inOrder);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(clauses, slop, collectPayloads, inOrder);
-    }
-
-    @Override
-    protected boolean doEquals(SpanNearQueryBuilder other) {
-        return Objects.equals(clauses, other.clauses) &&
-               Objects.equals(slop, other.slop) &&
-               Objects.equals(collectPayloads, other.collectPayloads) &&
-               Objects.equals(inOrder, other.inOrder);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
index f74b803..6ecf1b7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -29,9 +32,11 @@ import java.util.List;
 import static com.google.common.collect.Lists.newArrayList;
 
 /**
- * Parser for span_near query
+ *
  */
-public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
+public class SpanNearQueryParser implements QueryParser {
+
+    public static final String NAME = "span_near";
 
     @Inject
     public SpanNearQueryParser() {
@@ -39,20 +44,20 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{SpanNearQueryBuilder.NAME, Strings.toCamelCase(SpanNearQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanNearQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         Integer slop = null;
-        boolean inOrder = SpanNearQueryBuilder.DEFAULT_IN_ORDER;
-        boolean collectPayloads = SpanNearQueryBuilder.DEFAULT_COLLECT_PAYLOADS;
+        boolean inOrder = true;
+        boolean collectPayloads = true;
         String queryName = null;
 
-        List<SpanQueryBuilder> clauses = newArrayList();
+        List<SpanQuery> clauses = newArrayList();
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -62,11 +67,11 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("clauses".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        if (!(query instanceof SpanQueryBuilder)) {
+                        Query query = parseContext.parseInnerQuery();
+                        if (!(query instanceof SpanQuery)) {
                             throw new QueryParsingException(parseContext, "spanNear [clauses] must be of type span query");
                         }
-                        clauses.add((SpanQueryBuilder) query);
+                        clauses.add((SpanQuery) query);
                     }
                 } else {
                     throw new QueryParsingException(parseContext, "[span_near] query does not support [" + currentFieldName + "]");
@@ -77,7 +82,7 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
                 } else if ("collect_payloads".equals(currentFieldName) || "collectPayloads".equals(currentFieldName)) {
                     collectPayloads = parser.booleanValue();
                 } else if ("slop".equals(currentFieldName)) {
-                    slop = parser.intValue();
+                    slop = Integer.valueOf(parser.intValue());
                 } else if ("boost".equals(currentFieldName)) {
                     boost = parser.floatValue();
                 } else if ("_name".equals(currentFieldName)) {
@@ -89,24 +94,18 @@ public class SpanNearQueryParser extends BaseQueryParser<SpanNearQueryBuilder> {
                 throw new QueryParsingException(parseContext, "[span_near] query does not support [" + currentFieldName + "]");
             }
         }
-
+        if (clauses.isEmpty()) {
+            throw new QueryParsingException(parseContext, "span_near must include [clauses]");
+        }
         if (slop == null) {
             throw new QueryParsingException(parseContext, "span_near must include [slop]");
         }
 
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(slop);
-        for (SpanQueryBuilder subQuery : clauses) {
-            queryBuilder.clause(subQuery);
+        SpanNearQuery query = new SpanNearQuery(clauses.toArray(new SpanQuery[clauses.size()]), slop.intValue(), inOrder, collectPayloads);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        queryBuilder.inOrder(inOrder);
-        queryBuilder.collectPayloads(collectPayloads);
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
-
-    @Override
-    public SpanNearQueryBuilder getBuilderPrototype() {
-        return SpanNearQueryBuilder.PROTOTYPE;
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
index 3af88e3..e37cd80 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java
@@ -19,176 +19,100 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class SpanNotQueryBuilder extends AbstractQueryBuilder<SpanNotQueryBuilder> implements SpanQueryBuilder<SpanNotQueryBuilder> {
+public class SpanNotQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanNotQueryBuilder> {
 
-    public static final String NAME = "span_not";
+    private SpanQueryBuilder include;
 
-    /** the default pre parameter size */
-    public static final int DEFAULT_PRE = 0;
-    /** the default post parameter size */
-    public static final int DEFAULT_POST = 0;
+    private SpanQueryBuilder exclude;
 
-    private final SpanQueryBuilder include;
+    private Integer dist;
 
-    private final SpanQueryBuilder exclude;
+    private Integer pre;
 
-    private int pre = DEFAULT_PRE;
+    private Integer post;
 
-    private int post = DEFAULT_POST;
+    private Float boost;
 
-    static final SpanNotQueryBuilder PROTOTYPE = new SpanNotQueryBuilder(null, null);
+    private String queryName;
 
-    /**
-     * Construct a span query matching spans from <code>include</code> which
-     * have no overlap with spans from <code>exclude</code>.
-     * @param include the span query whose matches are filtered
-     * @param exclude the span query whose matches must not overlap
-     */
-    public SpanNotQueryBuilder(SpanQueryBuilder include, SpanQueryBuilder exclude) {
+    public SpanNotQueryBuilder include(SpanQueryBuilder include) {
         this.include = include;
-        this.exclude = exclude;
-    }
-
-    /**
-     * @return the span query whose matches are filtered
-     */
-    public SpanQueryBuilder includeQuery() {
-        return this.include;
+        return this;
     }
 
-    /**
-     * @return the span query whose matches must not overlap
-     */
-    public SpanQueryBuilder excludeQuery() {
-        return this.exclude;
+    public SpanNotQueryBuilder exclude(SpanQueryBuilder exclude) {
+        this.exclude = exclude;
+        return this;
     }
 
-    /**
-     * @param dist the amount of tokens from within the include span can’t have overlap with the exclude span.
-     * Equivalent to setting both pre and post parameter.
-     */
     public SpanNotQueryBuilder dist(int dist) {
-        pre(dist);
-        post(dist);
+        this.dist = dist;
         return this;
     }
 
-    /**
-     * @param pre the amount of tokens before the include span that can’t have overlap with the exclude span. Values
-     * smaller than 0 will be ignored and 0 used instead.
-     */
     public SpanNotQueryBuilder pre(int pre) {
-        this.pre = (pre >= 0) ? pre : 0;
+        this.pre = (pre >=0) ? pre : 0;
         return this;
     }
 
-    /**
-     * @return the amount of tokens before the include span that can’t have overlap with the exclude span.
-     * @see SpanNotQueryBuilder#pre(int)
-     */
-    public Integer pre() {
-        return this.pre;
-    }
-
-    /**
-     * @param post the amount of tokens after the include span that can’t have overlap with the exclude span.
-     */
     public SpanNotQueryBuilder post(int post) {
         this.post = (post >= 0) ? post : 0;
         return this;
     }
 
-    /**
-     * @return the amount of tokens after the include span that can’t have overlap with the exclude span.
-     * @see SpanNotQueryBuilder#post(int)
-     */
-    public Integer post() {
-        return this.post;
-    }
-
     @Override
-    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("include");
-        include.toXContent(builder, params);
-        builder.field("exclude");
-        exclude.toXContent(builder, params);
-        builder.field("pre", pre);
-        builder.field("post", post);
-        printBoostAndQueryName(builder);
-        builder.endObject();
+    public SpanNotQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-
-        Query includeQuery = this.include.toQuery(context);
-        assert includeQuery instanceof SpanQuery;
-        Query excludeQuery = this.exclude.toQuery(context);
-        assert excludeQuery instanceof SpanQuery;
-
-        return new SpanNotQuery((SpanQuery) includeQuery, (SpanQuery) excludeQuery, pre, post);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     * @param queryName The query name
+     * @return this
+     */
+    public SpanNotQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
+    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         if (include == null) {
-            validationException = addValidationError("inner clause [include] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(include, validationException);
+            throw new IllegalArgumentException("Must specify include when using spanNot query");
         }
         if (exclude == null) {
-            validationException = addValidationError("inner clause [exclude] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(exclude, validationException);
+            throw new IllegalArgumentException("Must specify exclude when using spanNot query");
         }
-        return validationException;
-    }
-
-    @Override
-    protected SpanNotQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder include = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder exclude = (SpanQueryBuilder)in.readQuery();
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(include, exclude);
-        queryBuilder.pre(in.readVInt());
-        queryBuilder.post(in.readVInt());
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(include);
-        out.writeQuery(exclude);
-        out.writeVInt(pre);
-        out.writeVInt(post);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(include, exclude, pre, post);
-    }
 
-    @Override
-    protected boolean doEquals(SpanNotQueryBuilder other) {
-        return Objects.equals(include, other.include) &&
-               Objects.equals(exclude, other.exclude) &&
-               (pre == other.pre) &&
-               (post == other.post);
-    }
+        if (dist != null && (pre != null || post != null)) {
+             throw new IllegalArgumentException("spanNot can either use [dist] or [pre] & [post] (or none)");
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.startObject(SpanNotQueryParser.NAME);
+        builder.field("include");
+        include.toXContent(builder, params);
+        builder.field("exclude");
+        exclude.toXContent(builder, params);
+        if (dist != null) {
+            builder.field("dist", dist);
+        }
+        if (pre != null) {
+            builder.field("pre", pre);
+        }
+        if (post != null) {
+            builder.field("post", post);
+        }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
index bc9ee51..bcb62e7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanNotQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,9 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_not query
+ *
  */
-public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
+public class SpanNotQueryParser implements QueryParser {
+
+    public static final String NAME = "span_not";
 
     @Inject
     public SpanNotQueryParser() {
@@ -36,17 +41,17 @@ public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{SpanNotQueryBuilder.NAME, Strings.toCamelCase(SpanNotQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanNotQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
 
-        SpanQueryBuilder include = null;
-        SpanQueryBuilder exclude = null;
+        SpanQuery include = null;
+        SpanQuery exclude = null;
 
         Integer dist = null;
         Integer pre  = null;
@@ -61,17 +66,17 @@ public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("include".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanNot [include] must be of type span query");
                     }
-                    include = (SpanQueryBuilder) query;
+                    include = (SpanQuery) query;
                 } else if ("exclude".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (!(query instanceof SpanQueryBuilder)) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (!(query instanceof SpanQuery)) {
                         throw new QueryParsingException(parseContext, "spanNot [exclude] must be of type span query");
                     }
-                    exclude = (SpanQueryBuilder) query;
+                    exclude = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_not] query does not support [" + currentFieldName + "]");
                 }
@@ -101,23 +106,26 @@ public class SpanNotQueryParser extends BaseQueryParser<SpanNotQueryBuilder> {
             throw new QueryParsingException(parseContext, "spanNot can either use [dist] or [pre] & [post] (or none)");
         }
 
-        SpanNotQueryBuilder spanNotQuery = new SpanNotQueryBuilder(include, exclude);
-        if (dist != null) {
-            spanNotQuery.dist(dist);
+        // set appropriate defaults
+        if (pre != null && post == null) {
+            post = 0;
+        } else if (pre == null && post != null){
+            pre = 0;
         }
-        if (pre != null) {
-            spanNotQuery.pre(pre);
-        }
-        if (post != null) {
-            spanNotQuery.post(post);
+
+        SpanNotQuery query;
+        if (pre != null && post != null) {
+            query = new SpanNotQuery(include, exclude, pre, post);
+        } else if (dist != null) {
+            query = new SpanNotQuery(include, exclude, dist);
+        } else {
+            query = new SpanNotQuery(include, exclude);
         }
-        spanNotQuery.boost(boost);
-        spanNotQuery.queryName(queryName);
-        return spanNotQuery;
-    }
 
-    @Override
-    public SpanNotQueryBuilder getBuilderPrototype() {
-        return SpanNotQueryBuilder.PROTOTYPE;
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
index 8e9b7ae..0042aa7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java
@@ -19,108 +19,55 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
 
-/**
- * Span query that matches the union of its clauses. Maps to {@link SpanOrQuery}.
- */
-public class SpanOrQueryBuilder extends AbstractQueryBuilder<SpanOrQueryBuilder> implements SpanQueryBuilder<SpanOrQueryBuilder> {
+public class SpanOrQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanOrQueryBuilder> {
 
-    public static final String NAME = "span_or";
+    private ArrayList<SpanQueryBuilder> clauses = new ArrayList<>();
 
-    private final List<SpanQueryBuilder> clauses = new ArrayList<>();
+    private float boost = -1;
 
-    static final SpanOrQueryBuilder PROTOTYPE = new SpanOrQueryBuilder();
+    private String queryName;
 
     public SpanOrQueryBuilder clause(SpanQueryBuilder clause) {
         clauses.add(clause);
         return this;
     }
 
+    @Override
+    public SpanOrQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     /**
-     * @return the {@link SpanQueryBuilder} clauses that were set for this query
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public List<SpanQueryBuilder> clauses() {
-        return this.clauses;
+    public SpanOrQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (clauses.isEmpty()) {
+            throw new IllegalArgumentException("Must have at least one clause when building a spanOr query");
+        }
+        builder.startObject(SpanOrQueryParser.NAME);
         builder.startArray("clauses");
         for (SpanQueryBuilder clause : clauses) {
             clause.toXContent(builder, params);
         }
         builder.endArray();
-        printBoostAndQueryName(builder);
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        SpanQuery[] spanQueries = new SpanQuery[clauses.size()];
-        for (int i = 0; i < clauses.size(); i++) {
-            Query query = clauses.get(i).toQuery(context);
-            assert query instanceof SpanQuery;
-            spanQueries[i] = (SpanQuery) query;
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        return new SpanOrQuery(spanQueries);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (clauses.isEmpty()) {
-            validationException =  addValidationError("query must include [clauses]", validationException);
+        if (queryName != null) {
+            builder.field("_name", queryName);
         }
-        for (SpanQueryBuilder innerClause : clauses) {
-            if (innerClause == null) {
-                validationException =  addValidationError("[clauses] contains null element", validationException);
-            } else {
-                validationException = validateInnerQuery(innerClause, validationException);
-            }
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanOrQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        List<QueryBuilder> clauses = readQueries(in);
-        for (QueryBuilder subClause : clauses) {
-            queryBuilder.clauses.add((SpanQueryBuilder)subClause);
-        }
-        return queryBuilder;
-
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        writeQueries(out, clauses);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(clauses);
-    }
-
-    @Override
-    protected boolean doEquals(SpanOrQueryBuilder other) {
-        return Objects.equals(clauses, other.clauses);
-    }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
index 6489f9b..db58d4c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java
@@ -19,7 +19,11 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanOrQuery;
+import org.apache.lucene.search.spans.SpanQuery;
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
 
 import java.io.IOException;
@@ -28,23 +32,29 @@ import java.util.List;
 import static com.google.common.collect.Lists.newArrayList;
 
 /**
- * Parser for span_or query
+ *
  */
-public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
+public class SpanOrQueryParser implements QueryParser {
+
+    public static final String NAME = "span_or";
+
+    @Inject
+    public SpanOrQueryParser() {
+    }
 
     @Override
     public String[] names() {
-        return new String[]{SpanOrQueryBuilder.NAME, Strings.toCamelCase(SpanOrQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanOrQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
 
-        List<SpanQueryBuilder> clauses = newArrayList();
+        List<SpanQuery> clauses = newArrayList();
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -54,11 +64,11 @@ public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
             } else if (token == XContentParser.Token.START_ARRAY) {
                 if ("clauses".equals(currentFieldName)) {
                     while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
-                        QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                        if (!(query instanceof SpanQueryBuilder)) {
+                        Query query = parseContext.parseInnerQuery();
+                        if (!(query instanceof SpanQuery)) {
                             throw new QueryParsingException(parseContext, "spanOr [clauses] must be of type span query");
                         }
-                        clauses.add((SpanQueryBuilder) query);
+                        clauses.add((SpanQuery) query);
                     }
                 } else {
                     throw new QueryParsingException(parseContext, "[span_or] query does not support [" + currentFieldName + "]");
@@ -77,17 +87,11 @@ public class SpanOrQueryParser extends BaseQueryParser<SpanOrQueryBuilder> {
             throw new QueryParsingException(parseContext, "spanOr must include [clauses]");
         }
 
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        for (SpanQueryBuilder clause : clauses) {
-            queryBuilder.clause(clause);
+        SpanOrQuery query = new SpanOrQuery(clauses.toArray(new SpanQuery[clauses.size()]));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
         }
-        queryBuilder.boost(boost);
-        queryBuilder.queryName(queryName);
-        return queryBuilder;
-    }
-
-    @Override
-    public SpanOrQueryBuilder getBuilderPrototype() {
-        return SpanOrQueryBuilder.PROTOTYPE;
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
index d35dcbc..4216f22 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java
@@ -19,9 +19,6 @@
 
 package org.elasticsearch.index.query;
 
-/**
- * Marker interface for a specific type of {@link QueryBuilder} that allows to build span queries
- */
-public interface SpanQueryBuilder<QB extends SpanQueryBuilder> extends QueryBuilder<QB> {
+public abstract class SpanQueryBuilder extends QueryBuilder {
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
index 24cd816..9d0176e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java
@@ -19,76 +19,75 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
-/**
- * A Span Query that matches documents containing a term.
- * @see SpanTermQuery
- */
-public class SpanTermQueryBuilder extends BaseTermQueryBuilder<SpanTermQueryBuilder> implements SpanQueryBuilder<SpanTermQueryBuilder> {
+public class SpanTermQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanTermQueryBuilder> {
+
+    private final String name;
+
+    private final Object value;
+
+    private float boost = -1;
 
-    public static final String NAME = "span_term";
-    static final SpanTermQueryBuilder PROTOTYPE = new SpanTermQueryBuilder(null, null);
+    private String queryName;
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
     public SpanTermQueryBuilder(String name, String value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
     public SpanTermQueryBuilder(String name, int value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
     public SpanTermQueryBuilder(String name, long value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
     public SpanTermQueryBuilder(String name, float value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
     public SpanTermQueryBuilder(String name, double value) {
-        super(name, (Object) value);
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
-    public SpanTermQueryBuilder(String name, Object value) {
-        super(name, value);
+    private SpanTermQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
     @Override
-    public SpanQuery doToQuery(QueryShardContext context) throws IOException {
-        BytesRef valueBytes = null;
-        String fieldName = this.fieldName;
-        MappedFieldType mapper = context.fieldMapper(fieldName);
-        if (mapper != null) {
-            fieldName = mapper.names().indexName();
-            valueBytes = mapper.indexedValueForSearch(value);
-        }
-        if (valueBytes == null) {
-            valueBytes = BytesRefs.toBytesRef(this.value);
-        }
-        return new SpanTermQuery(new Term(fieldName, valueBytes));
+    public SpanTermQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected SpanTermQueryBuilder createBuilder(String fieldName, Object value) {
-        return new SpanTermQueryBuilder(fieldName, value);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public SpanTermQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(SpanTermQueryParser.NAME);
+        if (boost == -1 && queryName != null) {
+            builder.field(name, value);
+        } else {
+            builder.startObject(name);
+            builder.field("value", value);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
+        }
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
index 824b474..c4ff2ee 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java
@@ -19,16 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for span_term query
+ *
  */
-public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
+public class SpanTermQueryParser implements QueryParser {
+
+    public static final String NAME = "span_term";
 
     @Inject
     public SpanTermQueryParser() {
@@ -36,24 +43,23 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{SpanTermQueryBuilder.NAME, Strings.toCamelCase(SpanTermQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanTermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.currentToken();
         if (token == XContentParser.Token.START_OBJECT) {
             token = parser.nextToken();
         }
-
         assert token == XContentParser.Token.FIELD_NAME;
         String fieldName = parser.currentName();
 
 
-        Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        String value = null;
+        float boost = 1.0f;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -63,9 +69,9 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
                     currentFieldName = parser.currentName();
                 } else {
                     if ("term".equals(currentFieldName)) {
-                        value = parser.objectBytes();
+                        value = parser.text();
                     } else if ("value".equals(currentFieldName)) {
-                        value = parser.objectBytes();
+                        value = parser.text();
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("_name".equals(currentFieldName)) {
@@ -77,7 +83,7 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
             }
             parser.nextToken();
         } else {
-            value = parser.objectBytes();
+            value = parser.text();
             // move to the next token
             parser.nextToken();
         }
@@ -86,13 +92,21 @@ public class SpanTermQueryParser extends BaseQueryParser<SpanTermQueryBuilder> {
             throw new QueryParsingException(parseContext, "No value specified for term query");
         }
 
-        SpanTermQueryBuilder result = new SpanTermQueryBuilder(fieldName, value);
-        result.boost(boost).queryName(queryName);
-        return result;
-    }
+        BytesRef valueBytes = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+            valueBytes = fieldType.indexedValueForSearch(value);
+        }
+        if (valueBytes == null) {
+            valueBytes = new BytesRef(value);
+        }
 
-    @Override
-    public SpanTermQueryBuilder getBuilderPrototype() {
-        return SpanTermQueryBuilder.PROTOTYPE;
+        SpanTermQuery query = new SpanTermQuery(new Term(fieldName, valueBytes));
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
index 83c7716..d2b2fdc 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryBuilder.java
@@ -19,53 +19,59 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanWithinQuery;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Builder for {@link org.apache.lucene.search.spans.SpanWithinQuery}.
  */
-public class SpanWithinQueryBuilder extends AbstractQueryBuilder<SpanWithinQueryBuilder> implements SpanQueryBuilder<SpanWithinQueryBuilder> {
+public class SpanWithinQueryBuilder extends SpanQueryBuilder implements BoostableQueryBuilder<SpanWithinQueryBuilder> {
 
-    public static final String NAME = "span_within";
-    private final SpanQueryBuilder big;
-    private final SpanQueryBuilder little;
-    static final SpanWithinQueryBuilder PROTOTYPE = new SpanWithinQueryBuilder(null, null);
+    private SpanQueryBuilder big;
+    private SpanQueryBuilder little;
+    private float boost = -1;
+    private String queryName;
 
-    /**
-     * Query that returns spans from <code>little</code> that are contained in a spans from <code>big</code>.
-     * @param big clause that must enclose {@code little} for a match.
-     * @param little the little clause, it must be contained within {@code big} for a match.
+    /** 
+     * Sets the little clause, it must be contained within {@code big} for a match.
      */
-    public SpanWithinQueryBuilder(SpanQueryBuilder big, SpanQueryBuilder little) {
-        this.little = little;
-        this.big = big;
+    public SpanWithinQueryBuilder little(SpanQueryBuilder clause) {
+        this.little = clause;
+        return this;
     }
 
-    /**
-     * @return the little clause, contained within {@code big} for a match.
+    /** 
+     * Sets the big clause, it must enclose {@code little} for a match.
      */
-    public SpanQueryBuilder littleQuery() {
-        return this.little;
+    public SpanWithinQueryBuilder big(SpanQueryBuilder clause) {
+        this.big = clause;
+        return this;
+    }
+
+    @Override
+    public SpanWithinQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
     /**
-     * @return the big clause that must enclose {@code little} for a match.
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
      */
-    public SpanQueryBuilder bigQuery() {
-        return this.big;
+    public SpanWithinQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        if (big == null) {
+            throw new IllegalArgumentException("Must specify big clause when building a span_within query");
+        }
+        if (little == null) {
+            throw new IllegalArgumentException("Must specify little clause when building a span_within query");
+        }
+        builder.startObject(SpanWithinQueryParser.NAME);
 
         builder.field("big");
         big.toXContent(builder, params);
@@ -73,62 +79,14 @@ public class SpanWithinQueryBuilder extends AbstractQueryBuilder<SpanWithinQuery
         builder.field("little");
         little.toXContent(builder, params);
 
-        printBoostAndQueryName(builder);
-
-        builder.endObject();
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query innerBig = big.toQuery(context);
-        assert innerBig instanceof SpanQuery;
-        Query innerLittle = little.toQuery(context);
-        assert innerLittle instanceof SpanQuery;
-        return new SpanWithinQuery((SpanQuery) innerBig, (SpanQuery) innerLittle);
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (big == null) {
-            validationException = addValidationError("inner clause [big] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(big, validationException);
+        if (boost != -1) {
+            builder.field("boost", boost);
         }
-        if (little == null) {
-            validationException = addValidationError("inner clause [little] cannot be null.", validationException);
-        } else {
-            validationException = validateInnerQuery(little, validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected SpanWithinQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        SpanQueryBuilder big = (SpanQueryBuilder)in.readQuery();
-        SpanQueryBuilder little = (SpanQueryBuilder)in.readQuery();
-        return new SpanWithinQueryBuilder(big, little);
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeQuery(big);
-        out.writeQuery(little);
-    }
 
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(big, little);
-    }
-
-    @Override
-    protected boolean doEquals(SpanWithinQueryBuilder other) {
-        return Objects.equals(big, other.big) &&
-               Objects.equals(little, other.little);
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
index 00ddb0e..9194cbd 100644
--- a/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/SpanWithinQueryParser.java
@@ -19,6 +19,9 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.search.spans.SpanWithinQuery;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -26,9 +29,11 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import java.io.IOException;
 
 /**
- * Parser for span_within query
+ * Parser for {@link SpanWithinQuery}
  */
-public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilder> {
+public class SpanWithinQueryParser implements QueryParser {
+
+    public static final String NAME = "span_within";
 
     @Inject
     public SpanWithinQueryParser() {
@@ -36,17 +41,17 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
 
     @Override
     public String[] names() {
-        return new String[]{SpanWithinQueryBuilder.NAME, Strings.toCamelCase(SpanWithinQueryBuilder.NAME)};
+        return new String[]{NAME, Strings.toCamelCase(NAME)};
     }
 
     @Override
-    public SpanWithinQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
-        SpanQueryBuilder big = null;
-        SpanQueryBuilder little = null;
+        SpanQuery big = null;
+        SpanQuery little = null;
 
         String currentFieldName = null;
         XContentParser.Token token;
@@ -55,17 +60,17 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
                 currentFieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("big".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (query instanceof SpanQueryBuilder == false) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (query instanceof SpanQuery == false) {
                         throw new QueryParsingException(parseContext, "span_within [big] must be of type span query");
                     }
-                    big = (SpanQueryBuilder) query;
+                    big = (SpanQuery) query;
                 } else if ("little".equals(currentFieldName)) {
-                    QueryBuilder query = parseContext.parseInnerQueryBuilder();
-                    if (query instanceof SpanQueryBuilder == false) {
+                    Query query = parseContext.parseInnerQuery();
+                    if (query instanceof SpanQuery == false) {
                         throw new QueryParsingException(parseContext, "span_within [little] must be of type span query");
                     }
-                    little = (SpanQueryBuilder) query;
+                    little = (SpanQuery) query;
                 } else {
                     throw new QueryParsingException(parseContext, "[span_within] query does not support [" + currentFieldName + "]");
                 }
@@ -76,8 +81,8 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
             } else {
                 throw new QueryParsingException(parseContext, "[span_within] query does not support [" + currentFieldName + "]");
             }
-        }
-
+        }        
+        
         if (big == null) {
             throw new QueryParsingException(parseContext, "span_within must include [big]");
         }
@@ -85,13 +90,11 @@ public class SpanWithinQueryParser extends BaseQueryParser<SpanWithinQueryBuilde
             throw new QueryParsingException(parseContext, "span_within must include [little]");
         }
 
-        SpanWithinQueryBuilder query = new SpanWithinQueryBuilder(big, little);
-        query.boost(boost).queryName(queryName);
+        Query query = new SpanWithinQuery(big, little);
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
         return query;
     }
-
-    @Override
-    public SpanWithinQueryBuilder getBuilderPrototype() {
-        return SpanWithinQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
index 63721c0..852977f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java
@@ -28,10 +28,7 @@ import java.util.Map;
 /**
  * Facilitates creating template query requests.
  * */
-public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuilder> {
-
-    /** Name to reference this type of query. */
-    public static final String NAME = "template";
+public class TemplateQueryBuilder extends QueryBuilder {
 
     /** Template to fill. */
     private Template template;
@@ -42,8 +39,6 @@ public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuil
 
     private ScriptService.ScriptType templateType;
 
-    static final TemplateQueryBuilder PROTOTYPE = new TemplateQueryBuilder(null, null);
-
     /**
      * @param template
      *            the template to use for that query.
@@ -82,16 +77,11 @@ public class TemplateQueryBuilder extends AbstractQueryBuilder<TemplateQueryBuil
 
     @Override
     protected void doXContent(XContentBuilder builder, Params builderParams) throws IOException {
-        builder.field(TemplateQueryBuilder.NAME);
+        builder.field(TemplateQueryParser.NAME);
         if (template == null) {
             new Template(templateString, templateType, null, null, this.vars).toXContent(builder, builderParams);
         } else {
             template.toXContent(builder, builderParams);
         }
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
index 1a75071..9b289f3 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java
@@ -38,8 +38,10 @@ import java.util.Map;
  * In the simplest case, parse template string and variables from the request,
  * compile the template and execute the template against the given variables.
  * */
-public class TemplateQueryParser extends BaseQueryParserTemp {
+public class TemplateQueryParser implements QueryParser {
 
+    /** Name to reference this type of query. */
+    public static final String NAME = "template";
     /** Name of query parameter containing the template string. */
     public static final String QUERY = "query";
 
@@ -59,21 +61,20 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[] {TemplateQueryBuilder.NAME};
+        return new String[] { NAME };
     }
 
     /**
      * Parses the template query replacing template parameters with provided
      * values. Handles both submitting the template as part of the request as
      * well as referencing only the template name.
-     *
-     * @param context
+     * 
+     * @param parseContext
      *            parse context containing the templated query.
      */
     @Override
     @Nullable
-    public Query parse(QueryShardContext context) throws IOException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException {
         XContentParser parser = parseContext.parser();
         Template template = parse(parser, parseContext.parseFieldMatcher());
         ExecutableScript executable = this.scriptService.executable(template, ScriptContext.Standard.SEARCH);
@@ -81,9 +82,9 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
         BytesReference querySource = (BytesReference) executable.run();
 
         try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            return contextCopy.parseContext().parseInnerQuery();
+            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParserService());
+            context.reset(qSourceParser);
+            return context.parseInnerQuery();
         }
     }
 
@@ -112,9 +113,4 @@ public class TemplateQueryParser extends BaseQueryParserTemp {
     public static Template parse(XContentParser parser, Map<String, ScriptService.ScriptType> parameterMap, ParseFieldMatcher parseFieldMatcher) throws IOException {
         return Template.parse(parser, parameterMap, parseFieldMatcher);
     }
-
-    @Override
-    public TemplateQueryBuilder getBuilderPrototype() {
-        return TemplateQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
index 5c8bf3f..5bd911a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java
@@ -19,77 +19,128 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.common.xcontent.XContentBuilder;
 
 import java.io.IOException;
 
 /**
  * A Query that matches documents containing a term.
  */
-public class TermQueryBuilder extends BaseTermQueryBuilder<TermQueryBuilder> {
+public class TermQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<TermQueryBuilder> {
 
-    public static final String NAME = "term";
-    static final TermQueryBuilder PROTOTYPE = new TermQueryBuilder(null, null);
+    private final String name;
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, String) */
-    public TermQueryBuilder(String fieldName, String value) {
-        super(fieldName, (Object) value);
+    private final Object value;
+
+    private float boost = -1;
+
+    private String queryName;
+
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, String value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, int) */
-    public TermQueryBuilder(String fieldName, int value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, int value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, long) */
-    public TermQueryBuilder(String fieldName, long value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, long value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, float) */
-    public TermQueryBuilder(String fieldName, float value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, float value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, double) */
-    public TermQueryBuilder(String fieldName, double value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, double value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, boolean) */
-    public TermQueryBuilder(String fieldName, boolean value) {
-        super(fieldName, (Object) value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, boolean value) {
+        this(name, (Object) value);
     }
 
-    /** @see BaseTermQueryBuilder#BaseTermQueryBuilder(String, Object) */
-    public TermQueryBuilder(String fieldName, Object value) {
-        super(fieldName, value);
+    /**
+     * Constructs a new term query.
+     *
+     * @param name  The name of the field
+     * @param value The value of the term
+     */
+    public TermQueryBuilder(String name, Object value) {
+        this.name = name;
+        this.value = value;
     }
 
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    public Query doToQuery(QueryShardContext context) throws IOException {
-        Query query = null;
-        MappedFieldType mapper = context.fieldMapper(this.fieldName);
-        if (mapper != null) {
-            query = mapper.termQuery(this.value, context);
-        }
-        if (query == null) {
-            query = new TermQuery(new Term(this.fieldName, BytesRefs.toBytesRef(this.value)));
-        }
-        return query;
+    public TermQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    protected TermQueryBuilder createBuilder(String fieldName, Object value) {
-        return new TermQueryBuilder(fieldName, value);
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public TermQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(TermQueryParser.NAME);
+        if (boost == -1 && queryName == null) {
+            builder.field(name, value);
+        } else {
+            builder.startObject(name);
+            builder.field("value", value);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
+        }
+        builder.endObject();
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
index 4de2910..be74053 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
 import org.elasticsearch.common.inject.Inject;
+import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.FieldMapper;
+import org.elasticsearch.index.mapper.MappedFieldType;
 
 import java.io.IOException;
 
 /**
- * Parser for the term query
+ *
  */
-public class TermQueryParser extends BaseQueryParser<TermQueryBuilder> {
+public class TermQueryParser implements QueryParser {
+
+    public static final String NAME = "term";
 
     @Inject
     public TermQueryParser() {
@@ -35,17 +43,17 @@ public class TermQueryParser extends BaseQueryParser<TermQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{TermQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public TermQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
         String fieldName = null;
         Object value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String currentFieldName = null;
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -93,16 +101,22 @@ public class TermQueryParser extends BaseQueryParser<TermQueryBuilder> {
             }
         }
 
-        TermQueryBuilder termQuery = new TermQueryBuilder(fieldName, value);
-        termQuery.boost(boost);
-        if (queryName != null) {
-            termQuery.queryName(queryName);
+        if (value == null) {
+            throw new QueryParsingException(parseContext, "No value specified for term query");
         }
-        return termQuery;
-    }
 
-    @Override
-    public TermQueryBuilder getBuilderPrototype() {
-        return TermQueryBuilder.PROTOTYPE;
+        Query query = null;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            query = fieldType.termQuery(value, parseContext);
+        }
+        if (query == null) {
+            query = new TermQuery(new Term(fieldName, BytesRefs.toBytesRef(value)));
+        }
+        query.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, query);
+        }
+        return query;
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
index a074e2a..4bdd0da 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsLookupQueryBuilder.java
@@ -19,20 +19,93 @@
 
 package org.elasticsearch.index.query;
 
+import org.elasticsearch.common.xcontent.XContentBuilder;
+
+import java.io.IOException;
 
 /**
- * A filter for a field based on several terms matching on any of them.
- * @deprecated use {@link TermsQueryBuilder} instead.
+ * A filer for a field based on several terms matching on any of them.
  */
-@Deprecated
-public class TermsLookupQueryBuilder extends TermsQueryBuilder {
+public class TermsLookupQueryBuilder extends QueryBuilder {
+
+    private final String name;
+    private String lookupIndex;
+    private String lookupType;
+    private String lookupId;
+    private String lookupRouting;
+    private String lookupPath;
+
+    private String queryName;
 
     public TermsLookupQueryBuilder(String name) {
-        super(name, (Object[]) null);
+        this.name = name;
+    }
+
+    /**
+     * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public TermsLookupQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
+    }
+
+    /**
+     * Sets the index name to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupIndex(String lookupIndex) {
+        this.lookupIndex = lookupIndex;
+        return this;
+    }
+
+    /**
+     * Sets the index type to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupType(String lookupType) {
+        this.lookupType = lookupType;
+        return this;
+    }
+
+    /**
+     * Sets the doc id to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupId(String lookupId) {
+        this.lookupId = lookupId;
+        return this;
+    }
+
+    /**
+     * Sets the path within the document to lookup the terms from.
+     */
+    public TermsLookupQueryBuilder lookupPath(String lookupPath) {
+        this.lookupPath = lookupPath;
+        return this;
+    }
+
+    public TermsLookupQueryBuilder lookupRouting(String lookupRouting) {
+        this.lookupRouting = lookupRouting;
+        return this;
     }
 
     @Override
-    public String getWriteableName() {
-        return TermsQueryBuilder.NAME;
-   }
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(TermsQueryParser.NAME);
+
+        builder.startObject(name);
+        if (lookupIndex != null) {
+            builder.field("index", lookupIndex);
+        }
+        builder.field("type", lookupType);
+        builder.field("id", lookupId);
+        if (lookupRouting != null) {
+            builder.field("routing", lookupRouting);
+        }
+        builder.field("path", lookupPath);
+        builder.endObject();
+
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
+
+        builder.endObject();
+    }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
index 24644e9..9ffdb0c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java
@@ -24,13 +24,9 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import java.io.IOException;
 
 /**
- * A filter for a field based on several terms matching on any of them.
+ * A filer for a field based on several terms matching on any of them.
  */
-public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
-
-    public static final String NAME = "terms";
-
-    static final TermsQueryBuilder PROTOTYPE = new TermsQueryBuilder(null, (Object) null);
+public class TermsQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<TermsQueryBuilder> {
 
     private final String name;
 
@@ -40,16 +36,14 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
 
     private Boolean disableCoord;
 
+    private String queryName;
+
     private String execution;
 
-    private String lookupIndex;
-    private String lookupType;
-    private String lookupId;
-    private String lookupRouting;
-    private String lookupPath;
+    private float boost = -1;
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -59,7 +53,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -70,7 +64,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -81,7 +75,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -92,7 +86,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -103,7 +97,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -114,7 +108,7 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-     * A filter for a field based on several terms matching on any of them.
+     * A filer for a field based on several terms matching on any of them.
      *
      * @param name   The field name
      * @param values The terms
@@ -136,9 +130,6 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
     }
 
     /**
-<<<<<<< HEAD
-     * Sets the index name to lookup the terms from.
-=======
      * Sets the minimum number of matches across the provided terms. Defaults to <tt>1</tt>.
      * @deprecated use [bool] query instead
      */
@@ -160,60 +151,23 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
 
     /**
      * Sets the filter name for the filter that can be used when searching for matched_filters per hit.
->>>>>>> master
-     */
-    public TermsQueryBuilder lookupIndex(String lookupIndex) {
-        this.lookupIndex = lookupIndex;
-        return this;
-    }
-
-    /**
-     * Sets the index type to lookup the terms from.
      */
-    public TermsQueryBuilder lookupType(String lookupType) {
-        this.lookupType = lookupType;
+    public TermsQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
         return this;
     }
 
-    /**
-     * Sets the doc id to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupId(String lookupId) {
-        this.lookupId = lookupId;
-        return this;
-    }
-
-    /**
-     * Sets the path within the document to lookup the terms from.
-     */
-    public TermsQueryBuilder lookupPath(String lookupPath) {
-        this.lookupPath = lookupPath;
-        return this;
-    }
-
-    public TermsQueryBuilder lookupRouting(String lookupRouting) {
-        this.lookupRouting = lookupRouting;
+    @Override
+    public TermsQueryBuilder boost(float boost) {
+        this.boost = boost;
         return this;
     }
 
     @Override
     public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        if (values == null) {
-            builder.startObject(name);
-            if (lookupIndex != null) {
-                builder.field("index", lookupIndex);
-            }
-            builder.field("type", lookupType);
-            builder.field("id", lookupId);
-            if (lookupRouting != null) {
-                builder.field("routing", lookupRouting);
-            }
-            builder.field("path", lookupPath);
-            builder.endObject();
-        } else {
-            builder.field(name, values);
-        }
+        builder.startObject(TermsQueryParser.NAME);
+        builder.field(name, values);
+
         if (execution != null) {
             builder.field("execution", execution);
         }
@@ -226,13 +180,14 @@ public class TermsQueryBuilder extends AbstractQueryBuilder<TermsQueryBuilder> {
             builder.field("disable_coord", disableCoord);
         }
 
-        printBoostAndQueryName(builder);
+        if (boost != -1) {
+            builder.field("boost", boost);
+        }
 
-        builder.endObject();
-    }
+        if (queryName != null) {
+            builder.field("_name", queryName);
+        }
 
-    @Override
-    public String getWriteableName() {
-        return NAME;
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
index 0dc450e..fa64389 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java
@@ -47,8 +47,9 @@ import java.util.List;
 /**
  *
  */
-public class TermsQueryParser extends BaseQueryParserTemp {
+public class TermsQueryParser implements QueryParser {
 
+    public static final String NAME = "terms";
     private static final ParseField MIN_SHOULD_MATCH_FIELD = new ParseField("min_match", "min_should_match").withAllDeprecated("Use [bool] query instead");
     private static final ParseField DISABLE_COORD_FIELD = new ParseField("disable_coord").withAllDeprecated("Use [bool] query instead");
     private Client client;
@@ -62,7 +63,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{TermsQueryBuilder.NAME, "in"};
+        return new String[]{NAME, "in"};
     }
 
     @Inject(optional = true)
@@ -71,8 +72,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         String queryName = null;
@@ -145,7 +145,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
                     // ignore
                 } else if (parseContext.parseFieldMatcher().match(currentFieldName, MIN_SHOULD_MATCH_FIELD)) {
                     if (minShouldMatch != null) {
-                        throw new IllegalArgumentException("[" + currentFieldName + "] is not allowed in a filter context for the [" + TermsQueryBuilder.NAME + "] query");
+                        throw new IllegalArgumentException("[" + currentFieldName + "] is not allowed in a filter context for the [" + NAME + "] query");
                     }
                     minShouldMatch = parser.textOrNull();
                 } else if ("boost".equals(currentFieldName)) {
@@ -164,7 +164,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
             throw new QueryParsingException(parseContext, "terms query requires a field name, followed by array of terms");
         }
 
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType != null) {
             fieldName = fieldType.names().indexName();
         }
@@ -185,9 +185,9 @@ public class TermsQueryParser extends BaseQueryParserTemp {
         }
 
         Query query;
-        if (context.isFilter()) {
+        if (parseContext.isFilter()) {
             if (fieldType != null) {
-                query = fieldType.termsQuery(terms, context);
+                query = fieldType.termsQuery(terms, parseContext);
             } else {
                 BytesRef[] filterValues = new BytesRef[terms.size()];
                 for (int i = 0; i < filterValues.length; i++) {
@@ -199,7 +199,7 @@ public class TermsQueryParser extends BaseQueryParserTemp {
             BooleanQuery bq = new BooleanQuery(disableCoord);
             for (Object term : terms) {
                 if (fieldType != null) {
-                    bq.add(fieldType.termQuery(term, context), Occur.SHOULD);
+                    bq.add(fieldType.termQuery(term, parseContext), Occur.SHOULD);
                 } else {
                     bq.add(new TermQuery(new Term(fieldName, BytesRefs.toBytesRef(term))), Occur.SHOULD);
                 }
@@ -210,13 +210,8 @@ public class TermsQueryParser extends BaseQueryParserTemp {
         query.setBoost(boost);
 
         if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            parseContext.addNamedQuery(queryName, query);
         }
         return query;
     }
-
-    @Override
-    public TermsQueryBuilder getBuilderPrototype() {
-        return TermsQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
index 9f89a94..2a9a6c5 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TypeQueryBuilder.java
@@ -19,92 +19,22 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
-import org.elasticsearch.common.lucene.BytesRefs;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 
 import java.io.IOException;
-import java.util.Objects;
 
-public class TypeQueryBuilder extends AbstractQueryBuilder<TypeQueryBuilder> {
+public class TypeQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "type";
-
-    private final BytesRef type;
-
-    static final TypeQueryBuilder PROTOTYPE = new TypeQueryBuilder((BytesRef) null);
+    private final String type;
 
     public TypeQueryBuilder(String type) {
-        this.type = BytesRefs.toBytesRef(type);
-    }
-
-    TypeQueryBuilder(BytesRef type) {
         this.type = type;
     }
 
-    public String type() {
-        return BytesRefs.toString(this.type);
-    }
-
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.field("value", type.utf8ToString());
-        printBoostAndQueryName(builder);
+        builder.startObject(TypeQueryParser.NAME);
+        builder.field("value", type);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
-
-    @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        Query filter;
-        //LUCENE 4 UPGRADE document mapper should use bytesref as well?
-        DocumentMapper documentMapper = context.mapperService().documentMapper(type.utf8ToString());
-        if (documentMapper == null) {
-            filter = new TermQuery(new Term(TypeFieldMapper.NAME, type));
-        } else {
-            filter = documentMapper.typeFilter();
-        }
-        return filter;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (type == null) {
-            validationException = addValidationError("[type] cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected TypeQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        return new TypeQueryBuilder(in.readBytesRef());
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeBytesRef(type);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(type);
-    }
-
-    @Override
-    protected boolean doEquals(TypeQueryBuilder other) {
-        return Objects.equals(type, other.type);
-    }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
index ee5e772..e4b7889 100644
--- a/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/TypeQueryParser.java
@@ -19,16 +19,20 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
 
 import java.io.IOException;
 
-/**
- * Parser for type query
- */
-public class TypeQueryParser extends BaseQueryParser<TypeQueryBuilder> {
+public class TypeQueryParser implements QueryParser {
+
+    public static final String NAME = "type";
 
     @Inject
     public TypeQueryParser() {
@@ -36,45 +40,37 @@ public class TypeQueryParser extends BaseQueryParser<TypeQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{TypeQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public TypeQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
-        BytesRef type = null;
 
-        String queryName = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-
-        String currentFieldName = null;
-        XContentParser.Token token;
-        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
-            if (token == XContentParser.Token.FIELD_NAME) {
-                currentFieldName = parser.currentName();
-            } else if (token.isValue()) {
-                if ("_name".equals(currentFieldName)) {
-                    queryName = parser.text();
-                } else if ("boost".equals(currentFieldName)) {
-                    boost = parser.floatValue();
-                } else if ("value".equals(currentFieldName)) {
-                    type = parser.utf8Bytes();
-                }
-            } else {
-                throw new QueryParsingException(parseContext, "[type] filter doesn't support [" + currentFieldName + "]");
-            }
+        XContentParser.Token token = parser.nextToken();
+        if (token != XContentParser.Token.FIELD_NAME) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
         }
-
-        if (type == null) {
-            throw new QueryParsingException(parseContext, "[type] filter needs to be provided with a value for the type");
+        String fieldName = parser.currentName();
+        if (!fieldName.equals("value")) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
         }
-        return new TypeQueryBuilder(type)
-                .boost(boost)
-                .queryName(queryName);
-    }
+        token = parser.nextToken();
+        if (token != XContentParser.Token.VALUE_STRING) {
+            throw new QueryParsingException(parseContext, "[type] filter should have a value field, and the type name");
+        }
+        BytesRef type = parser.utf8Bytes();
+        // move to the next token
+        parser.nextToken();
 
-    @Override
-    public TypeQueryBuilder getBuilderPrototype() {
-        return TypeQueryBuilder.PROTOTYPE;
+        Query filter;
+        //LUCENE 4 UPGRADE document mapper should use bytesref as well? 
+        DocumentMapper documentMapper = parseContext.mapperService().documentMapper(type.utf8ToString());
+        if (documentMapper == null) {
+            filter = new TermQuery(new Term(TypeFieldMapper.NAME, type));
+        } else {
+            filter = documentMapper.typeFilter();
+        }
+        return filter;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
index 89b753e..654f14e 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java
@@ -19,20 +19,9 @@
 
 package org.elasticsearch.index.query;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.Strings;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
-import java.util.Objects;
 
 /**
  * Implements the wildcard search query. Supported wildcards are <tt>*</tt>, which
@@ -42,17 +31,17 @@ import java.util.Objects;
  * a Wildcard term should not start with one of the wildcards <tt>*</tt> or
  * <tt>?</tt>.
  */
-public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuilder> implements MultiTermQueryBuilder<WildcardQueryBuilder> {
+public class WildcardQueryBuilder extends MultiTermQueryBuilder implements BoostableQueryBuilder<WildcardQueryBuilder> {
 
-    public static final String NAME = "wildcard";
+    private final String name;
 
-    private final String fieldName;
+    private final String wildcard;
 
-    private final String value;
+    private float boost = -1;
 
     private String rewrite;
 
-    static final WildcardQueryBuilder PROTOTYPE = new WildcardQueryBuilder(null, null);
+    private String queryName;
 
     /**
      * Implements the wildcard search query. Supported wildcards are <tt>*</tt>, which
@@ -62,20 +51,12 @@ public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuil
      * a Wildcard term should not start with one of the wildcards <tt>*</tt> or
      * <tt>?</tt>.
      *
-     * @param fieldName The field name
-     * @param value The wildcard query string
+     * @param name     The field name
+     * @param wildcard The wildcard query string
      */
-    public WildcardQueryBuilder(String fieldName, String value) {
-        this.fieldName = fieldName;
-        this.value = value;
-    }
-
-    public String fieldName() {
-        return fieldName;
-    }
-
-    public String value() {
-        return value;
+    public WildcardQueryBuilder(String name, String wildcard) {
+        this.name = name;
+        this.wildcard = wildcard;
     }
 
     public WildcardQueryBuilder rewrite(String rewrite) {
@@ -83,83 +64,43 @@ public class WildcardQueryBuilder extends AbstractQueryBuilder<WildcardQueryBuil
         return this;
     }
 
-    public String rewrite() {
-        return this.rewrite;
-    }
-
+    /**
+     * Sets the boost for this query.  Documents matching this query will (in addition to the normal
+     * weightings) have their score multiplied by the boost provided.
+     */
     @Override
-    public String getWriteableName() {
-        return NAME;
+    public WildcardQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
     }
 
-    @Override
-    public void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
-        builder.startObject(fieldName);
-        builder.field("wildcard", value);
-        if (rewrite != null) {
-            builder.field("rewrite", rewrite);
-        }
-        printBoostAndQueryName(builder);
-        builder.endObject();
-        builder.endObject();
+    /**
+     * Sets the query name for the filter that can be used when searching for matched_filters per hit.
+     */
+    public WildcardQueryBuilder queryName(String queryName) {
+        this.queryName = queryName;
+        return this;
     }
 
     @Override
-    protected Query doToQuery(QueryShardContext context) throws IOException {
-        String indexFieldName;
-        BytesRef valueBytes;
-
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
-        if (fieldType != null) {
-            indexFieldName = fieldType.names().indexName();
-            valueBytes = fieldType.indexedValueForSearch(value);
+    public void doXContent(XContentBuilder builder, Params params) throws IOException {
+        builder.startObject(WildcardQueryParser.NAME);
+        if (boost == -1 && rewrite == null && queryName == null) {
+            builder.field(name, wildcard);
         } else {
-            indexFieldName = fieldName;
-            valueBytes = new BytesRef(value);
+            builder.startObject(name);
+            builder.field("wildcard", wildcard);
+            if (boost != -1) {
+                builder.field("boost", boost);
+            }
+            if (rewrite != null) {
+                builder.field("rewrite", rewrite);
+            }
+            if (queryName != null) {
+                builder.field("_name", queryName);
+            }
+            builder.endObject();
         }
-
-        WildcardQuery query = new WildcardQuery(new Term(indexFieldName, valueBytes));
-        MultiTermQuery.RewriteMethod rewriteMethod = QueryParsers.parseRewriteMethod(context.parseFieldMatcher(), rewrite, null);
-        QueryParsers.setRewriteMethod(query, rewriteMethod);
-        return query;
-    }
-
-    @Override
-    public QueryValidationException validate() {
-        QueryValidationException validationException = null;
-        if (Strings.isEmpty(this.fieldName)) {
-            validationException = addValidationError("field name cannot be null or empty.", validationException);
-        }
-        if (this.value == null) {
-            validationException = addValidationError("wildcard cannot be null", validationException);
-        }
-        return validationException;
-    }
-
-    @Override
-    protected WildcardQueryBuilder doReadFrom(StreamInput in) throws IOException {
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(in.readString(), in.readString());
-        wildcardQueryBuilder.rewrite = in.readOptionalString();
-        return wildcardQueryBuilder;
-    }
-
-    @Override
-    protected void doWriteTo(StreamOutput out) throws IOException {
-        out.writeString(fieldName);
-        out.writeString(value);
-        out.writeOptionalString(rewrite);
-    }
-
-    @Override
-    protected int doHashCode() {
-        return Objects.hash(fieldName, value, rewrite);
-    }
-
-    @Override
-    protected boolean doEquals(WildcardQueryBuilder other) {
-        return Objects.equals(fieldName, other.fieldName) &&
-                Objects.equals(value, other.value) &&
-                Objects.equals(rewrite, other.rewrite);
+        builder.endObject();
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
index d3b3e26..da92db4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java
@@ -19,15 +19,23 @@
 
 package org.elasticsearch.index.query;
 
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MappedFieldType;
+import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
 
 /**
- * Parser for wildcard query
+ *
  */
-public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
+public class WildcardQueryParser implements QueryParser {
+
+    public static final String NAME = "wildcard";
 
     @Inject
     public WildcardQueryParser() {
@@ -35,11 +43,11 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
 
     @Override
     public String[] names() {
-        return new String[]{WildcardQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public WildcardQueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -47,10 +55,10 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
             throw new QueryParsingException(parseContext, "[wildcard] query malformed, no field");
         }
         String fieldName = parser.currentName();
-        String rewrite = null;
+        String rewriteMethod = null;
 
         String value = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
+        float boost = 1.0f;
         String queryName = null;
         token = parser.nextToken();
         if (token == XContentParser.Token.START_OBJECT) {
@@ -66,7 +74,7 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
                     } else if ("boost".equals(currentFieldName)) {
                         boost = parser.floatValue();
                     } else if ("rewrite".equals(currentFieldName)) {
-                        rewrite = parser.textOrNull();
+                        rewriteMethod = parser.textOrNull();
                     } else if ("_name".equals(currentFieldName)) {
                         queryName = parser.text();
                     } else {
@@ -83,14 +91,22 @@ public class WildcardQueryParser extends BaseQueryParser<WildcardQueryBuilder> {
         if (value == null) {
             throw new QueryParsingException(parseContext, "No value specified for prefix query");
         }
-        return new WildcardQueryBuilder(fieldName, value)
-                .rewrite(rewrite)
-                .boost(boost)
-                .queryName(queryName);
-    }
 
-    @Override
-    public WildcardQueryBuilder getBuilderPrototype() {
-        return WildcardQueryBuilder.PROTOTYPE;
+        BytesRef valueBytes;
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
+        if (fieldType != null) {
+            fieldName = fieldType.names().indexName();
+            valueBytes = fieldType.indexedValueForSearch(value);
+        } else {
+            valueBytes = new BytesRef(value);
+        }
+
+        WildcardQuery wildcardQuery = new WildcardQuery(new Term(fieldName, valueBytes));
+        QueryParsers.setRewriteMethod(wildcardQuery, parseContext.parseFieldMatcher(), rewriteMethod);
+        wildcardQuery.setBoost(boost);
+        if (queryName != null) {
+            parseContext.addNamedQuery(queryName, wildcardQuery);
+        }
+        return wildcardQuery;
     }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
index aac8176..6fde3c7 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryBuilder.java
@@ -20,7 +20,6 @@
 package org.elasticsearch.index.query;
 
 import com.google.common.base.Charsets;
-
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 
@@ -40,13 +39,11 @@ import java.io.IOException;
  * }
  * </pre>
  */
-public class WrapperQueryBuilder extends AbstractQueryBuilder<WrapperQueryBuilder> {
+public class WrapperQueryBuilder extends QueryBuilder {
 
-    public static final String NAME = "wrapper";
     private final byte[] source;
     private final int offset;
     private final int length;
-    static final WrapperQueryBuilder PROTOTYPE = new WrapperQueryBuilder(null, -1, -1);
 
     /**
      * Creates a query builder given a query provided as a string
@@ -77,13 +74,8 @@ public class WrapperQueryBuilder extends AbstractQueryBuilder<WrapperQueryBuilde
 
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-        builder.startObject(NAME);
+        builder.startObject(WrapperQueryParser.NAME);
         builder.field("query", source, offset, length);
         builder.endObject();
     }
-
-    @Override
-    public String getWriteableName() {
-        return NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
index 02309da..331ba78 100644
--- a/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java
@@ -29,7 +29,9 @@ import java.io.IOException;
 /**
  * Query parser for JSON Queries.
  */
-public class WrapperQueryParser extends BaseQueryParserTemp {
+public class WrapperQueryParser implements QueryParser {
+
+    public static final String NAME = "wrapper";
 
     @Inject
     public WrapperQueryParser() {
@@ -37,12 +39,11 @@ public class WrapperQueryParser extends BaseQueryParserTemp {
 
     @Override
     public String[] names() {
-        return new String[]{WrapperQueryBuilder.NAME};
+        return new String[]{NAME};
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         XContentParser.Token token = parser.nextToken();
@@ -57,17 +58,12 @@ public class WrapperQueryParser extends BaseQueryParserTemp {
 
         byte[] querySource = parser.binaryValue();
         try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
-            final QueryShardContext contextCopy = new QueryShardContext(context.index(), context.indexQueryParserService());
-            contextCopy.reset(qSourceParser);
-            Query result = contextCopy.parseContext().parseInnerQuery();
+            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParserService());
+            context.reset(qSourceParser);
+            Query result = context.parseInnerQuery();
             parser.nextToken();
-            context.combineNamedQueries(contextCopy);
+            parseContext.combineNamedQueries(context);
             return result;
         }
     }
-
-    @Override
-    public WrapperQueryBuilder getBuilderPrototype() {
-        return WrapperQueryBuilder.PROTOTYPE;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
index 7580c84..3dc2427 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java
@@ -43,7 +43,7 @@ import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.core.DateFieldMapper;
 import org.elasticsearch.index.mapper.core.NumberFieldMapper;
 import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionBuilder;
 import org.elasticsearch.index.query.functionscore.gauss.GaussDecayFunctionParser;
@@ -119,7 +119,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
      *
      * */
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         String currentFieldName;
         XContentParser.Token token;
         AbstractDistanceScoreFunction scoreFunction;
@@ -132,7 +132,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
             if (token == XContentParser.Token.START_OBJECT) {
                 variableContent.copyCurrentStructure(parser);
                 fieldName = currentFieldName;
-            } else if (context.parseFieldMatcher().match(currentFieldName, MULTI_VALUE_MODE)) {
+            } else if (parseContext.parseFieldMatcher().match(currentFieldName, MULTI_VALUE_MODE)) {
                 multiValueMode = parser.text();
             } else {
                 throw new ElasticsearchParseException("malformed score function score parameters.");
@@ -142,34 +142,34 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
             throw new ElasticsearchParseException("malformed score function score parameters.");
         }
         XContentParser variableParser = XContentFactory.xContent(variableContent.string()).createParser(variableContent.string());
-        scoreFunction = parseVariable(fieldName, variableParser, context, MultiValueMode.fromString(multiValueMode.toUpperCase(Locale.ROOT)));
+        scoreFunction = parseVariable(fieldName, variableParser, parseContext, MultiValueMode.fromString(multiValueMode.toUpperCase(Locale.ROOT)));
         return scoreFunction;
     }
 
     // parses origin and scale parameter for field "fieldName"
-    private AbstractDistanceScoreFunction parseVariable(String fieldName, XContentParser parser, QueryShardContext context, MultiValueMode mode) throws IOException {
+    private AbstractDistanceScoreFunction parseVariable(String fieldName, XContentParser parser, QueryParseContext parseContext, MultiValueMode mode) throws IOException {
 
         // now, the field must exist, else we cannot read the value for
         // the doc later
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType == null) {
-            throw new QueryParsingException(context.parseContext(), "unknown field [{}]", fieldName);
+            throw new QueryParsingException(parseContext, "unknown field [{}]", fieldName);
         }
 
         // dates and time need special handling
         parser.nextToken();
         if (fieldType instanceof DateFieldMapper.DateFieldType) {
-            return parseDateVariable(fieldName, parser, context, (DateFieldMapper.DateFieldType) fieldType, mode);
+            return parseDateVariable(fieldName, parser, parseContext, (DateFieldMapper.DateFieldType) fieldType, mode);
         } else if (fieldType instanceof GeoPointFieldMapper.GeoPointFieldType) {
-            return parseGeoVariable(fieldName, parser, context, (GeoPointFieldMapper.GeoPointFieldType) fieldType, mode);
+            return parseGeoVariable(fieldName, parser, parseContext, (GeoPointFieldMapper.GeoPointFieldType) fieldType, mode);
         } else if (fieldType instanceof NumberFieldMapper.NumberFieldType) {
-            return parseNumberVariable(fieldName, parser, context, (NumberFieldMapper.NumberFieldType) fieldType, mode);
+            return parseNumberVariable(fieldName, parser, parseContext, (NumberFieldMapper.NumberFieldType) fieldType, mode);
         } else {
-            throw new QueryParsingException(context.parseContext(), "field [{}] is of type [{}], but only numeric types are supported.", fieldName, fieldType);
+            throw new QueryParsingException(parseContext, "field [{}] is of type [{}], but only numeric types are supported.", fieldName, fieldType);
         }
     }
 
-    private AbstractDistanceScoreFunction parseNumberVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseNumberVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             NumberFieldMapper.NumberFieldType fieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -199,11 +199,11 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         if (!scaleFound || !refFound) {
             throw new ElasticsearchParseException("both [{}] and [{}] must be set for numeric fields.", DecayFunctionBuilder.SCALE, DecayFunctionBuilder.ORIGIN);
         }
-        IndexNumericFieldData numericFieldData = context.getForField(fieldType);
+        IndexNumericFieldData numericFieldData = parseContext.getForField(fieldType);
         return new NumericFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), numericFieldData, mode);
     }
 
-    private AbstractDistanceScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseGeoVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             GeoPointFieldMapper.GeoPointFieldType fieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -231,12 +231,12 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         }
         double scale = DistanceUnit.DEFAULT.parse(scaleString, DistanceUnit.DEFAULT);
         double offset = DistanceUnit.DEFAULT.parse(offsetString, DistanceUnit.DEFAULT);
-        IndexGeoPointFieldData indexFieldData = context.getForField(fieldType);
+        IndexGeoPointFieldData indexFieldData = parseContext.getForField(fieldType);
         return new GeoFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), indexFieldData, mode);
 
     }
 
-    private AbstractDistanceScoreFunction parseDateVariable(String fieldName, XContentParser parser, QueryShardContext context,
+    private AbstractDistanceScoreFunction parseDateVariable(String fieldName, XContentParser parser, QueryParseContext parseContext,
             DateFieldMapper.DateFieldType dateFieldType, MultiValueMode mode) throws IOException {
         XContentParser.Token token;
         String parameterName = null;
@@ -271,7 +271,7 @@ public abstract class DecayFunctionParser implements ScoreFunctionParser {
         double scale = val.getMillis();
         val = TimeValue.parseTimeValue(offsetString, TimeValue.timeValueHours(24), getClass().getSimpleName() + ".offset");
         double offset = val.getMillis();
-        IndexNumericFieldData numericFieldData = context.getForField(dateFieldType);
+        IndexNumericFieldData numericFieldData = parseContext.getForField(dateFieldType);
         return new NumericFieldDataScoreFunction(origin, scale, decay, offset, getDecayFunction(), numericFieldData, mode);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
index 3880592..dc7571a 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java
@@ -21,7 +21,7 @@ package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.common.lucene.search.function.CombineFunction;
 import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.index.query.AbstractQueryBuilder;
+import org.elasticsearch.index.query.BoostableQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
 
 import java.io.IOException;
@@ -31,12 +31,14 @@ import java.util.ArrayList;
  * A query that uses a filters with a script associated with them to compute the
  * score.
  */
-public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScoreQueryBuilder> {
+public class FunctionScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder<FunctionScoreQueryBuilder> {
 
     private final QueryBuilder queryBuilder;
 
     private final QueryBuilder filterBuilder;
 
+    private Float boost;
+
     private Float maxBoost;
 
     private String scoreMode;
@@ -47,8 +49,6 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
     private ArrayList<ScoreFunctionBuilder> scoreFunctions = new ArrayList<>();
     private Float minScore = null;
 
-    static final FunctionScoreQueryBuilder PROTOTYPE = new FunctionScoreQueryBuilder();
-
     /**
      * Creates a function_score query that executes on documents that match query a query.
      * Query and filter will be wrapped into a filtered_query.
@@ -143,6 +143,17 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         return this;
     }
 
+    /**
+     * Sets the boost for this query. Documents matching this query will (in
+     * addition to the normal weightings) have their score multiplied by the
+     * boost provided.
+     */
+    @Override
+    public FunctionScoreQueryBuilder boost(float boost) {
+        this.boost = boost;
+        return this;
+    }
+
     @Override
     protected void doXContent(XContentBuilder builder, Params params) throws IOException {
         builder.startObject(FunctionScoreQueryParser.NAME);
@@ -175,10 +186,13 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         if (maxBoost != null) {
             builder.field("max_boost", maxBoost);
         }
+        if (boost != null) {
+            builder.field("boost", boost);
+        }
         if (minScore != null) {
             builder.field("min_score", minScore);
         }
-        printBoostAndQueryName(builder);
+
         builder.endObject();
     }
 
@@ -186,9 +200,4 @@ public class FunctionScoreQueryBuilder extends AbstractQueryBuilder<FunctionScor
         this.minScore = minScore;
         return this;
     }
-
-    @Override
-    public String getWriteableName() {
-        return FunctionScoreQueryParser.NAME;
-    }
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
index 3df6b78..02fc425 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java
@@ -37,7 +37,9 @@ import org.elasticsearch.common.lucene.search.function.FunctionScoreQuery;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.lucene.search.function.WeightFactorFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.factor.FactorParser;
 
 import java.io.IOException;
@@ -45,7 +47,7 @@ import java.util.ArrayList;
 import java.util.Arrays;
 
 /**
- * Parser for function_score query
+ *
  */
 public class FunctionScoreQueryParser implements QueryParser {
 
@@ -82,14 +84,12 @@ public class FunctionScoreQueryParser implements QueryParser {
     }
 
     @Override
-    public Query parse(QueryShardContext context) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
         XContentParser parser = parseContext.parser();
 
         Query query = null;
         Query filter = null;
-        float boost = AbstractQueryBuilder.DEFAULT_BOOST;
-        String queryName = null;
+        float boost = 1.0f;
 
         FiltersFunctionScoreQuery.ScoreMode scoreMode = FiltersFunctionScoreQuery.ScoreMode.Multiply;
         ArrayList<FiltersFunctionScoreQuery.FilterFunction> filterFunctions = new ArrayList<>();
@@ -119,8 +119,6 @@ public class FunctionScoreQueryParser implements QueryParser {
                 maxBoost = parser.floatValue();
             } else if ("boost".equals(currentFieldName)) {
                 boost = parser.floatValue();
-            } else if ("_name".equals(currentFieldName)) {
-                queryName = parser.text();
             } else if ("min_score".equals(currentFieldName) || "minScore".equals(currentFieldName)) {
                 minScore = parser.floatValue();
             } else if ("functions".equals(currentFieldName)) {
@@ -128,7 +126,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                     String errorString = "already found [" + singleFunctionName + "], now encountering [functions].";
                     handleMisplacedFunctionsDeclaration(errorString, singleFunctionName);
                 }
-                currentFieldName = parseFiltersAndFunctions(context, parser, filterFunctions, currentFieldName);
+                currentFieldName = parseFiltersAndFunctions(parseContext, parser, filterFunctions, currentFieldName);
                 functionArrayFound = true;
             } else {
                 ScoreFunction scoreFunction;
@@ -139,7 +137,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                     // we try to parse a score function. If there is no score
                     // function for the current field name,
                     // functionParserMapper.get() will throw an Exception.
-                    scoreFunction = functionParserMapper.get(parseContext, currentFieldName).parse(context, parser);
+                    scoreFunction = functionParserMapper.get(parseContext, currentFieldName).parse(parseContext, parser);
                 }
                 if (functionArrayFound) {
                     String errorString = "already found [functions] array, now encountering [" + currentFieldName + "].";
@@ -170,7 +168,6 @@ public class FunctionScoreQueryParser implements QueryParser {
         if (maxBoost == null) {
             maxBoost = Float.MAX_VALUE;
         }
-        Query result;
         // handle cases where only one score function and no filter was
         // provided. In this case we create a FunctionScoreQuery.
         if (filterFunctions.size() == 0 || filterFunctions.size() == 1 && (filterFunctions.get(0).filter == null || Queries.isConstantMatchAllQuery(filterFunctions.get(0).filter))) {
@@ -179,8 +176,9 @@ public class FunctionScoreQueryParser implements QueryParser {
             if (combineFunction != null) {
                 theQuery.setCombineFunction(combineFunction);
             }
+            theQuery.setBoost(boost);
             theQuery.setMaxBoost(maxBoost);
-            result = theQuery;
+            return theQuery;
             // in all other cases we create a FiltersFunctionScoreQuery.
         } else {
             FiltersFunctionScoreQuery functionScoreQuery = new FiltersFunctionScoreQuery(query, scoreMode,
@@ -188,13 +186,9 @@ public class FunctionScoreQueryParser implements QueryParser {
             if (combineFunction != null) {
                 functionScoreQuery.setCombineFunction(combineFunction);
             }
-            result = functionScoreQuery;
-        }
-        result.setBoost(boost);
-        if (queryName != null) {
-            context.addNamedQuery(queryName, query);
+            functionScoreQuery.setBoost(boost);
+            return functionScoreQuery;
         }
-        return result;
     }
 
     private void handleMisplacedFunctionsDeclaration(String errorString, String functionName) {
@@ -205,9 +199,8 @@ public class FunctionScoreQueryParser implements QueryParser {
         throw new ElasticsearchParseException("failed to parse [{}] query. [{}]", NAME, errorString);
     }
 
-    private String parseFiltersAndFunctions(QueryShardContext context, XContentParser parser,
+    private String parseFiltersAndFunctions(QueryParseContext parseContext, XContentParser parser,
                                             ArrayList<FiltersFunctionScoreQuery.FilterFunction> filterFunctions, String currentFieldName) throws IOException {
-        QueryParseContext parseContext = context.parseContext();
         XContentParser.Token token;
         while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {
             Query filter = null;
@@ -229,7 +222,7 @@ public class FunctionScoreQueryParser implements QueryParser {
                             // functionParserMapper throws exception if parser
                             // non-existent
                             ScoreFunctionParser functionParser = functionParserMapper.get(parseContext, currentFieldName);
-                            scoreFunction = functionParser.parse(context, parser);
+                            scoreFunction = functionParser.parse(parseContext, parser);
                         }
                     }
                 }
@@ -276,16 +269,4 @@ public class FunctionScoreQueryParser implements QueryParser {
         }
         return cf;
     }
-
-    //norelease to be removed once all queries are moved over to extend BaseQueryParser
-    @Override
-    public QueryBuilder fromXContent(QueryParseContext parseContext) throws IOException, QueryParsingException {
-        Query query = parse(parseContext.shardContext());
-        return new QueryWrappingQueryBuilder(query);
-    }
-
-    @Override
-    public FunctionScoreQueryBuilder getBuilderPrototype() {
-        return FunctionScoreQueryBuilder.PROTOTYPE;
-    }
-}
+}
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
index 4065f08..74c3d08 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionParser.java
@@ -21,14 +21,14 @@ package org.elasticsearch.index.query.functionscore;
 
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
 public interface ScoreFunctionParser {
 
-    ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException;
+    ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException;
 
     /**
      * Returns the name of the function, for example "linear", "gauss" etc. This
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
index 2635c2b..a1c8d20 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/factor/FactorParser.java
@@ -19,13 +19,14 @@
 
 package org.elasticsearch.index.query.functionscore.factor;
 
+import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
+
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.function.BoostScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
 
 import java.io.IOException;
 
@@ -42,7 +43,7 @@ public class FactorParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         float boostFactor = parser.floatValue();
         return new BoostScoreFunction(boostFactor);
     }
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
index a91d954..e6a8f2d 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/fieldvaluefactor/FieldValueFactorFunctionParser.java
@@ -24,8 +24,8 @@ import org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexNumericFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -52,8 +52,7 @@ public class FieldValueFactorFunctionParser implements ScoreFunctionParser {
     public static String[] NAMES = { "field_value_factor", "fieldValueFactor" };
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
 
         String currentFieldName = null;
         String field = null;
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
index 20c2f55..124336c 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java
@@ -27,8 +27,8 @@ import org.elasticsearch.common.lucene.search.function.RandomScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.fielddata.IndexFieldData;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -51,8 +51,8 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
+
         int seed = -1;
 
         String currentFieldName = null;
@@ -90,7 +90,7 @@ public class RandomScoreFunctionParser implements ScoreFunctionParser {
         }
 
         if (seed == -1) {
-            seed = Longs.hashCode(context.nowInMillis());
+            seed = Longs.hashCode(parseContext.nowInMillis());
         }
         final ShardId shardId = SearchContext.current().indexShard().shardId();
         final int salt = (shardId.index().name().hashCode() << 10) | shardId.id();
diff --git a/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java b/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
index 38a29f3..2cf066f 100644
--- a/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
+++ b/core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionParser.java
@@ -21,11 +21,11 @@
 
 package org.elasticsearch.index.query.functionscore.script;
 
+import org.elasticsearch.common.ParseField;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.search.function.ScoreFunction;
 import org.elasticsearch.common.lucene.search.function.ScriptScoreFunction;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
@@ -58,8 +58,7 @@ public class ScriptScoreFunctionParser implements ScoreFunctionParser {
     }
 
     @Override
-    public ScoreFunction parse(QueryShardContext context, XContentParser parser) throws IOException, QueryParsingException {
-        QueryParseContext parseContext = context.parseContext();
+    public ScoreFunction parse(QueryParseContext parseContext, XContentParser parser) throws IOException, QueryParsingException {
         ScriptParameterParser scriptParameterParser = new ScriptParameterParser();
         Script script = null;
         Map<String, Object> vars = null;
@@ -101,7 +100,7 @@ public class ScriptScoreFunctionParser implements ScoreFunctionParser {
 
         SearchScript searchScript;
         try {
-            searchScript = context.scriptService().search(context.lookup(), script, ScriptContext.Standard.SEARCH);
+            searchScript = parseContext.scriptService().search(parseContext.lookup(), script, ScriptContext.Standard.SEARCH);
             return new ScriptScoreFunction(script, searchScript);
         } catch (Exception e) {
             throw new QueryParsingException(parseContext, NAMES[0] + " the script could not be loaded", e);
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java b/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
index 6e59d01..ae839c4 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java
@@ -22,7 +22,6 @@ package org.elasticsearch.index.query.support;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
@@ -52,7 +51,7 @@ public class InnerHitsQueryParserHelper {
         this.fieldDataFieldsParseElement = fieldDataFieldsParseElement;
     }
 
-    public Tuple<String, SubSearchContext> parse(QueryParseContext parserContext) throws IOException, QueryShardException {
+    public Tuple<String, SubSearchContext> parse(QueryParseContext parserContext) throws IOException, QueryParsingException {
         String fieldName = null;
         XContentParser.Token token;
         String innerHitName = null;
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
index 1e941bb..63da8a1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/NestedInnerQueryParseSupport.java
@@ -26,10 +26,10 @@ import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
+import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
-import org.elasticsearch.index.query.QueryShardContext;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.search.internal.SearchContext;
 
 import java.io.IOException;
@@ -41,7 +41,6 @@ import java.io.IOException;
  */
 public class NestedInnerQueryParseSupport {
 
-    protected final QueryShardContext shardContext;
     protected final QueryParseContext parseContext;
 
     private BytesReference source;
@@ -61,15 +60,12 @@ public class NestedInnerQueryParseSupport {
     private ObjectMapper parentObjectMapper;
 
     public NestedInnerQueryParseSupport(XContentParser parser, SearchContext searchContext) {
-        parseContext = searchContext.queryParserService().getShardContext().parseContext();
-        shardContext = searchContext.queryParserService().getShardContext();
-        shardContext.reset(parser);
-
+        parseContext = searchContext.queryParserService().getParseContext();
+        parseContext.reset(parser);
     }
 
-    public NestedInnerQueryParseSupport(QueryShardContext context) {
-        this.parseContext = context.parseContext();
-        this.shardContext = context;
+    public NestedInnerQueryParseSupport(QueryParseContext parseContext) {
+        this.parseContext = parseContext;
     }
 
     public void query() throws IOException {
@@ -107,10 +103,10 @@ public class NestedInnerQueryParseSupport {
             return innerQuery;
         } else {
             if (path == null) {
-                throw new QueryShardException(shardContext, "[nested] requires 'path' field");
+                throw new QueryParsingException(parseContext, "[nested] requires 'path' field");
             }
             if (!queryFound) {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             XContentParser old = parseContext.parser();
@@ -136,10 +132,10 @@ public class NestedInnerQueryParseSupport {
             return innerFilter;
         } else {
             if (path == null) {
-                throw new QueryShardException(shardContext, "[nested] requires 'path' field");
+                throw new QueryParsingException(parseContext, "[nested] requires 'path' field");
             }
             if (!filterFound) {
-                throw new QueryShardException(shardContext, "[nested] requires either 'query' or 'filter' field");
+                throw new QueryParsingException(parseContext, "[nested] requires either 'query' or 'filter' field");
             }
 
             setPathLevel();
@@ -159,12 +155,12 @@ public class NestedInnerQueryParseSupport {
 
     public void setPath(String path) {
         this.path = path;
-        nestedObjectMapper = shardContext.getObjectMapper(path);
+        nestedObjectMapper = parseContext.getObjectMapper(path);
         if (nestedObjectMapper == null) {
-            throw new QueryShardException(shardContext, "[nested] failed to find nested object under path [" + path + "]");
+            throw new QueryParsingException(parseContext, "[nested] failed to find nested object under path [" + path + "]");
         }
         if (!nestedObjectMapper.nested().isNested()) {
-            throw new QueryShardException(shardContext, "[nested] nested object under path [" + path + "] is not of nested type");
+            throw new QueryParsingException(parseContext, "[nested] nested object under path [" + path + "] is not of nested type");
         }
     }
 
@@ -189,18 +185,18 @@ public class NestedInnerQueryParseSupport {
     }
 
     private void setPathLevel() {
-        ObjectMapper objectMapper = shardContext.nestedScope().getObjectMapper();
+        ObjectMapper objectMapper = parseContext.nestedScope().getObjectMapper();
         if (objectMapper == null) {
-            parentFilter = shardContext.bitsetFilter(Queries.newNonNestedFilter());
+            parentFilter = parseContext.bitsetFilter(Queries.newNonNestedFilter());
         } else {
-            parentFilter = shardContext.bitsetFilter(objectMapper.nestedTypeFilter());
+            parentFilter = parseContext.bitsetFilter(objectMapper.nestedTypeFilter());
         }
-        childFilter = shardContext.bitsetFilter(nestedObjectMapper.nestedTypeFilter());
-        parentObjectMapper = shardContext.nestedScope().nextLevel(nestedObjectMapper);
+        childFilter = parseContext.bitsetFilter(nestedObjectMapper.nestedTypeFilter());
+        parentObjectMapper = parseContext.nestedScope().nextLevel(nestedObjectMapper);
     }
 
     private void resetPathLevel() {
-        shardContext.nestedScope().previousLevel();
+        parseContext.nestedScope().previousLevel();
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java b/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
index a500393..1a12c74 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java
@@ -29,12 +29,12 @@ import org.elasticsearch.common.ParseFieldMatcher;
  */
 public final class QueryParsers {
 
-    public static final ParseField CONSTANT_SCORE = new ParseField("constant_score", "constant_score_auto", "constant_score_filter");
-    public static final ParseField SCORING_BOOLEAN = new ParseField("scoring_boolean");
-    public static final ParseField CONSTANT_SCORE_BOOLEAN = new ParseField("constant_score_boolean");
-    public static final ParseField TOP_TERMS = new ParseField("top_terms_");
-    public static final ParseField TOP_TERMS_BOOST = new ParseField("top_terms_boost_");
-    public static final ParseField TOP_TERMS_BLENDED_FREQS = new ParseField("top_terms_blended_freqs_");
+    private static final ParseField CONSTANT_SCORE = new ParseField("constant_score", "constant_score_auto", "constant_score_filter");
+    private static final ParseField SCORING_BOOLEAN = new ParseField("scoring_boolean");
+    private static final ParseField CONSTANT_SCORE_BOOLEAN = new ParseField("constant_score_boolean");
+    private static final ParseField TOP_TERMS = new ParseField("top_terms_");
+    private static final ParseField TOP_TERMS_BOOST = new ParseField("top_terms_boost_");
+    private static final ParseField TOP_TERMS_BLENDED_FREQS = new ParseField("top_terms_blended_freqs_");
 
     private QueryParsers() {
 
diff --git a/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java b/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
index cd8fd27..37716d1 100644
--- a/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
+++ b/core/src/main/java/org/elasticsearch/index/query/support/XContentStructure.java
@@ -25,7 +25,6 @@ import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardContext;
 import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
@@ -85,14 +84,14 @@ public abstract class XContentStructure {
         BytesReference br = this.bytes();
         assert br != null : "innerBytes must be set with .bytes(bytes) or .freeze() before parsing";
         XContentParser innerParser = XContentHelper.createParser(br);
-        String[] origTypes = QueryShardContext.setTypesWithPrevious(types);
+        String[] origTypes = QueryParseContext.setTypesWithPrevious(types);
         XContentParser old = parseContext.parser();
         parseContext.parser(innerParser);
         try {
             return parseContext.parseInnerQuery();
         } finally {
             parseContext.parser(old);
-            QueryShardContext.setTypes(origTypes);
+            QueryParseContext.setTypes(origTypes);
         }
     }
 
@@ -107,12 +106,12 @@ public abstract class XContentStructure {
         public InnerQuery(QueryParseContext parseContext1, @Nullable String... types) throws IOException {
             super(parseContext1);
             if (types != null) {
-                String[] origTypes = QueryShardContext.setTypesWithPrevious(types);
+                String[] origTypes = QueryParseContext.setTypesWithPrevious(types);
                 try {
                     query = parseContext1.parseInnerQuery();
                     queryParsed = true;
                 } finally {
-                    QueryShardContext.setTypes(origTypes);
+                    QueryParseContext.setTypes(origTypes);
                 }
             } else {
                 BytesReference innerBytes = XContentFactory.smileBuilder().copyCurrentStructure(parseContext1.parser()).bytes();
diff --git a/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
index 0b5dae6..fb5fff8 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MatchQuery.java
@@ -30,7 +30,7 @@ import org.elasticsearch.common.lucene.search.MultiPhrasePrefixQuery;
 import org.elasticsearch.common.lucene.search.Queries;
 import org.elasticsearch.common.unit.Fuzziness;
 import org.elasticsearch.index.mapper.MappedFieldType;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.query.support.QueryParsers;
 
 import java.io.IOException;
@@ -49,7 +49,7 @@ public class MatchQuery {
         ALL
     }
 
-    protected final QueryShardContext context;
+    protected final QueryParseContext parseContext;
 
     protected String analyzer;
 
@@ -60,9 +60,9 @@ public class MatchQuery {
     protected int phraseSlop = 0;
 
     protected Fuzziness fuzziness = null;
-
+    
     protected int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-
+    
     protected int maxExpansions = FuzzyQuery.defaultMaxExpansions;
 
     protected boolean transpositions = FuzzyQuery.defaultTranspositions;
@@ -72,11 +72,11 @@ public class MatchQuery {
     protected boolean lenient;
 
     protected ZeroTermsQuery zeroTermsQuery = ZeroTermsQuery.NONE;
-
+    
     protected Float commonTermsCutoff = null;
-
-    public MatchQuery(QueryShardContext context) {
-        this.context = context;
+    
+    public MatchQuery(QueryParseContext parseContext) {
+        this.parseContext = parseContext;
     }
 
     public void setAnalyzer(String analyzer) {
@@ -86,7 +86,7 @@ public class MatchQuery {
     public void setOccur(BooleanClause.Occur occur) {
         this.occur = occur;
     }
-
+    
     public void setCommonTermsCutoff(float cutoff) {
         this.commonTermsCutoff = Float.valueOf(cutoff);
     }
@@ -134,11 +134,11 @@ public class MatchQuery {
     protected Analyzer getAnalyzer(MappedFieldType fieldType) {
         if (this.analyzer == null) {
             if (fieldType != null) {
-                return context.getSearchAnalyzer(fieldType);
+                return parseContext.getSearchAnalyzer(fieldType);
             }
-            return context.mapperService().searchAnalyzer();
+            return parseContext.mapperService().searchAnalyzer();
         } else {
-            Analyzer analyzer = context.mapperService().analysisService().analyzer(this.analyzer);
+            Analyzer analyzer = parseContext.mapperService().analysisService().analyzer(this.analyzer);
             if (analyzer == null) {
                 throw new IllegalArgumentException("No analyzer found for [" + this.analyzer + "]");
             }
@@ -148,7 +148,7 @@ public class MatchQuery {
 
     public Query parse(Type type, String fieldName, Object value) throws IOException {
         final String field;
-        MappedFieldType fieldType = context.fieldMapper(fieldName);
+        MappedFieldType fieldType = parseContext.fieldMapper(fieldName);
         if (fieldType != null) {
             field = fieldType.names().indexName();
         } else {
@@ -157,14 +157,14 @@ public class MatchQuery {
 
         if (fieldType != null && fieldType.useTermQueryWithQueryString() && !forceAnalyzeQueryString()) {
             try {
-                return fieldType.termQuery(value, context);
+                return fieldType.termQuery(value, parseContext);
             } catch (RuntimeException e) {
                 if (lenient) {
                     return null;
                 }
                 throw e;
             }
-
+            
         }
         Analyzer analyzer = getAnalyzer(fieldType);
         assert analyzer != null;
diff --git a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
index b85dcfd..621e7d0 100644
--- a/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
+++ b/core/src/main/java/org/elasticsearch/index/search/MultiMatchQuery.java
@@ -29,9 +29,10 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.util.BytesRef;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.lucene.search.Queries;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -47,10 +48,10 @@ public class MultiMatchQuery extends MatchQuery {
         this.groupTieBreaker = tieBreaker;
     }
 
-    public MultiMatchQuery(QueryShardContext context) {
-        super(context);
+    public MultiMatchQuery(QueryParseContext parseContext) {
+        super(parseContext);
     }
-
+    
     private Query parseAndApply(Type type, String fieldName, Object value, String minimumShouldMatch, Float boostValue) throws IOException {
         Query query = parse(type, fieldName, value);
         if (query instanceof BooleanQuery) {
@@ -162,7 +163,7 @@ public class MultiMatchQuery extends MatchQuery {
             List<Tuple<String, Float>> missing = new ArrayList<>();
             for (Map.Entry<String, Float> entry : fieldNames.entrySet()) {
                 String name = entry.getKey();
-                MappedFieldType fieldType = context.fieldMapper(name);
+                MappedFieldType fieldType = parseContext.fieldMapper(name);
                 if (fieldType != null) {
                     Analyzer actualAnalyzer = getAnalyzer(fieldType);
                     name = fieldType.names().indexName();
diff --git a/core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java b/core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java
deleted file mode 100644
index c9fe0b4..0000000
--- a/core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java
+++ /dev/null
@@ -1,139 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.settings;
-
-import org.elasticsearch.cluster.routing.UnassignedInfo;
-import org.elasticsearch.gateway.PrimaryShardAllocator;
-import org.elasticsearch.index.shard.MergeSchedulerConfig;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.routing.allocation.decider.DisableAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.FilterAllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.ShardsLimitAllocationDecider;
-import org.elasticsearch.cluster.settings.DynamicSettings;
-import org.elasticsearch.cluster.settings.Validator;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.index.engine.EngineConfig;
-import org.elasticsearch.index.indexing.IndexingSlowLog;
-import org.elasticsearch.index.search.stats.SearchSlowLog;
-import org.elasticsearch.index.shard.IndexShard;
-import org.elasticsearch.index.shard.MergePolicyConfig;
-import org.elasticsearch.index.store.IndexStore;
-import org.elasticsearch.index.translog.TranslogConfig;
-import org.elasticsearch.index.translog.TranslogService;
-import org.elasticsearch.indices.IndicesWarmer;
-import org.elasticsearch.indices.cache.request.IndicesRequestCache;
-import org.elasticsearch.indices.ttl.IndicesTTLService;
-
-/**
- */
-public class IndexDynamicSettingsModule extends AbstractModule {
-
-    private final DynamicSettings indexDynamicSettings;
-
-    public IndexDynamicSettingsModule() {
-        indexDynamicSettings = new DynamicSettings();
-        indexDynamicSettings.addDynamicSetting(IndexStore.INDEX_STORE_THROTTLE_MAX_BYTES_PER_SEC, Validator.BYTES_SIZE);
-        indexDynamicSettings.addDynamicSetting(IndexStore.INDEX_STORE_THROTTLE_TYPE);
-        indexDynamicSettings.addDynamicSetting(MergeSchedulerConfig.MAX_THREAD_COUNT);
-        indexDynamicSettings.addDynamicSetting(MergeSchedulerConfig.MAX_MERGE_COUNT);
-        indexDynamicSettings.addDynamicSetting(MergeSchedulerConfig.AUTO_THROTTLE);
-        indexDynamicSettings.addDynamicSetting(FilterAllocationDecider.INDEX_ROUTING_REQUIRE_GROUP + "*");
-        indexDynamicSettings.addDynamicSetting(FilterAllocationDecider.INDEX_ROUTING_INCLUDE_GROUP + "*");
-        indexDynamicSettings.addDynamicSetting(FilterAllocationDecider.INDEX_ROUTING_EXCLUDE_GROUP + "*");
-        indexDynamicSettings.addDynamicSetting(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE);
-        indexDynamicSettings.addDynamicSetting(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE);
-        indexDynamicSettings.addDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_ALLOCATION);
-        indexDynamicSettings.addDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_NEW_ALLOCATION);
-        indexDynamicSettings.addDynamicSetting(DisableAllocationDecider.INDEX_ROUTING_ALLOCATION_DISABLE_REPLICA_ALLOCATION);
-        indexDynamicSettings.addDynamicSetting(TranslogConfig.INDEX_TRANSLOG_FS_TYPE);
-        indexDynamicSettings.addDynamicSetting(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, Validator.NON_NEGATIVE_INTEGER);
-        indexDynamicSettings.addDynamicSetting(IndexMetaData.SETTING_AUTO_EXPAND_REPLICAS);
-        indexDynamicSettings.addDynamicSetting(IndexMetaData.SETTING_READ_ONLY);
-        indexDynamicSettings.addDynamicSetting(IndexMetaData.SETTING_BLOCKS_READ);
-        indexDynamicSettings.addDynamicSetting(IndexMetaData.SETTING_BLOCKS_WRITE);
-        indexDynamicSettings.addDynamicSetting(IndexMetaData.SETTING_BLOCKS_METADATA);
-        indexDynamicSettings.addDynamicSetting(IndexMetaData.SETTING_SHARED_FS_ALLOW_RECOVERY_ON_ANY_NODE);
-        indexDynamicSettings.addDynamicSetting(IndexMetaData.SETTING_PRIORITY, Validator.NON_NEGATIVE_INTEGER);
-        indexDynamicSettings.addDynamicSetting(IndicesTTLService.INDEX_TTL_DISABLE_PURGE);
-        indexDynamicSettings.addDynamicSetting(IndexShard.INDEX_REFRESH_INTERVAL, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(PrimaryShardAllocator.INDEX_RECOVERY_INITIAL_SHARDS);
-        indexDynamicSettings.addDynamicSetting(EngineConfig.INDEX_COMPOUND_ON_FLUSH, Validator.BOOLEAN);
-        indexDynamicSettings.addDynamicSetting(EngineConfig.INDEX_GC_DELETES_SETTING, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(IndexShard.INDEX_FLUSH_ON_CLOSE, Validator.BOOLEAN);
-        indexDynamicSettings.addDynamicSetting(EngineConfig.INDEX_VERSION_MAP_SIZE, Validator.BYTES_SIZE_OR_PERCENTAGE);
-        indexDynamicSettings.addDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_WARN, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_INFO, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_DEBUG, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_THRESHOLD_INDEX_TRACE, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_REFORMAT);
-        indexDynamicSettings.addDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_LEVEL);
-        indexDynamicSettings.addDynamicSetting(IndexingSlowLog.INDEX_INDEXING_SLOWLOG_MAX_SOURCE_CHARS_TO_LOG);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_QUERY_WARN, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_QUERY_INFO, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_QUERY_DEBUG, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_QUERY_TRACE, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_FETCH_WARN, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_FETCH_INFO, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_FETCH_DEBUG, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_THRESHOLD_FETCH_TRACE, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_REFORMAT);
-        indexDynamicSettings.addDynamicSetting(SearchSlowLog.INDEX_SEARCH_SLOWLOG_LEVEL);
-        indexDynamicSettings.addDynamicSetting(ShardsLimitAllocationDecider.INDEX_TOTAL_SHARDS_PER_NODE, Validator.INTEGER);
-        indexDynamicSettings.addDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_EXPUNGE_DELETES_ALLOWED, Validator.DOUBLE);
-        indexDynamicSettings.addDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_FLOOR_SEGMENT, Validator.BYTES_SIZE);
-        indexDynamicSettings.addDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGE_AT_ONCE, Validator.INTEGER_GTE_2);
-        indexDynamicSettings.addDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGE_AT_ONCE_EXPLICIT, Validator.INTEGER_GTE_2);
-        indexDynamicSettings.addDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_MAX_MERGED_SEGMENT, Validator.BYTES_SIZE);
-        indexDynamicSettings.addDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_SEGMENTS_PER_TIER, Validator.DOUBLE_GTE_2);
-        indexDynamicSettings.addDynamicSetting(MergePolicyConfig.INDEX_MERGE_POLICY_RECLAIM_DELETES_WEIGHT, Validator.NON_NEGATIVE_DOUBLE);
-        indexDynamicSettings.addDynamicSetting(MergePolicyConfig.INDEX_COMPOUND_FORMAT);
-        indexDynamicSettings.addDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_INTERVAL, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_OPS, Validator.INTEGER);
-        indexDynamicSettings.addDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE, Validator.BYTES_SIZE);
-        indexDynamicSettings.addDynamicSetting(TranslogService.INDEX_TRANSLOG_FLUSH_THRESHOLD_PERIOD, Validator.TIME);
-        indexDynamicSettings.addDynamicSetting(TranslogService.INDEX_TRANSLOG_DISABLE_FLUSH);
-        indexDynamicSettings.addDynamicSetting(TranslogConfig.INDEX_TRANSLOG_DURABILITY);
-        indexDynamicSettings.addDynamicSetting(IndicesWarmer.INDEX_WARMER_ENABLED);
-        indexDynamicSettings.addDynamicSetting(IndicesRequestCache.INDEX_CACHE_REQUEST_ENABLED, Validator.BOOLEAN);
-        indexDynamicSettings.addDynamicSetting(IndicesRequestCache.DEPRECATED_INDEX_CACHE_REQUEST_ENABLED, Validator.BOOLEAN);
-        indexDynamicSettings.addDynamicSetting(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING, Validator.TIME);
-    }
-
-    public void addDynamicSettings(String... settings) {
-        indexDynamicSettings.addDynamicSettings(settings);
-    }
-
-    public void addDynamicSetting(String setting, Validator validator) {
-        indexDynamicSettings.addDynamicSetting(setting, validator);
-    }
-
-    @Override
-    protected void configure() {
-        bind(DynamicSettings.class).annotatedWith(IndexDynamicSettings.class).toInstance(indexDynamicSettings);
-    }
-
-    /**
-     * Returns <code>true</code> iff the given setting is in the dynamic settings map. Otherwise <code>false</code>.
-     */
-    public boolean containsSetting(String setting) {
-        return indexDynamicSettings.hasDynamicSetting(setting);
-    }
-}
diff --git a/core/src/main/java/org/elasticsearch/index/settings/IndexSettingsService.java b/core/src/main/java/org/elasticsearch/index/settings/IndexSettingsService.java
index 62964b5..39d1437 100644
--- a/core/src/main/java/org/elasticsearch/index/settings/IndexSettingsService.java
+++ b/core/src/main/java/org/elasticsearch/index/settings/IndexSettingsService.java
@@ -63,7 +63,7 @@ public class IndexSettingsService extends AbstractIndexComponent {
     }
 
     /**
-     * Only settings registered in {@link IndexDynamicSettingsModule} can be changed dynamically.
+     * Only settings registered in {@link org.elasticsearch.cluster.ClusterModule} can be changed dynamically.
      */
     public void addListener(Listener listener) {
         this.listeners.add(listener);
@@ -73,7 +73,7 @@ public class IndexSettingsService extends AbstractIndexComponent {
         this.listeners.remove(listener);
     }
 
-    public static interface Listener {
+    public interface Listener {
         void onRefreshSettings(Settings settings);
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
index d8bea76..46c03de 100644
--- a/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
+++ b/core/src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java
@@ -35,12 +35,7 @@ import org.elasticsearch.index.aliases.IndexAliasesService;
 import org.elasticsearch.index.cache.IndexCache;
 import org.elasticsearch.index.engine.Engine;
 import org.elasticsearch.index.engine.IgnoreOnRecoveryEngineException;
-import org.elasticsearch.index.mapper.DocumentMapper;
-import org.elasticsearch.index.mapper.MapperException;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.mapper.MapperUtils;
-import org.elasticsearch.index.mapper.Mapping;
-import org.elasticsearch.index.mapper.Uid;
+import org.elasticsearch.index.mapper.*;
 import org.elasticsearch.index.query.IndexQueryParserService;
 import org.elasticsearch.index.query.ParsedQuery;
 import org.elasticsearch.index.query.QueryParsingException;
diff --git a/core/src/main/java/org/elasticsearch/index/store/Store.java b/core/src/main/java/org/elasticsearch/index/store/Store.java
index db642f7..2736717 100644
--- a/core/src/main/java/org/elasticsearch/index/store/Store.java
+++ b/core/src/main/java/org/elasticsearch/index/store/Store.java
@@ -1487,7 +1487,7 @@ public class Store extends AbstractIndexShardComponent implements Closeable, Ref
             try {
                 return new StoreStats(estimateSize(directory), directoryService.throttleTimeInNanos());
             } catch (IOException ex) {
-                throw new ElasticsearchException("failed to refresh store stats");
+                throw new ElasticsearchException("failed to refresh store stats", ex);
             }
         }
 
diff --git a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java
index 665ddf9..fb7ca17 100644
--- a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java
+++ b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java
@@ -32,9 +32,6 @@ public class IndicesQueriesModule extends AbstractModule {
 
     private Set<Class<? extends QueryParser>> queryParsersClasses = Sets.newHashSet();
 
-    /**
-     * Registers a {@link QueryParser} given its class
-     */
     public synchronized IndicesQueriesModule addQuery(Class<? extends QueryParser> queryParser) {
         queryParsersClasses.add(queryParser);
         return this;
@@ -86,6 +83,7 @@ public class IndicesQueriesModule extends AbstractModule {
         qpBinders.addBinding().to(TemplateQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(TypeQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(LimitQueryParser.class).asEagerSingleton();
+        qpBinders.addBinding().to(TermsQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(ScriptQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(GeoDistanceQueryParser.class).asEagerSingleton();
         qpBinders.addBinding().to(GeoDistanceRangeQueryParser.class).asEagerSingleton();
diff --git a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
index ef9df4b..7d13fe0 100644
--- a/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
+++ b/core/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java
@@ -24,10 +24,7 @@ import com.google.common.collect.Maps;
 
 import org.elasticsearch.common.component.AbstractComponent;
 import org.elasticsearch.common.inject.Inject;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.index.query.EmptyQueryBuilder;
-import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.index.query.QueryParser;
 
 import java.util.Map;
@@ -35,28 +32,24 @@ import java.util.Set;
 
 public class IndicesQueriesRegistry extends AbstractComponent {
 
-    private ImmutableMap<String, QueryParser<?>> queryParsers;
+    private ImmutableMap<String, QueryParser> queryParsers;
 
     @Inject
-    public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers, NamedWriteableRegistry namedWriteableRegistry) {
+    public IndicesQueriesRegistry(Settings settings, Set<QueryParser> injectedQueryParsers) {
         super(settings);
-        Map<String, QueryParser<?>> queryParsers = Maps.newHashMap();
-        for (QueryParser<?> queryParser : injectedQueryParsers) {
+        Map<String, QueryParser> queryParsers = Maps.newHashMap();
+        for (QueryParser queryParser : injectedQueryParsers) {
             for (String name : queryParser.names()) {
                 queryParsers.put(name, queryParser);
             }
-            namedWriteableRegistry.registerPrototype(QueryBuilder.class, queryParser.getBuilderPrototype());
         }
-        // EmptyQueryBuilder is not registered as query parser but used internally.
-        // We need to register it with the NamedWriteableRegistry in order to serialize it
-        namedWriteableRegistry.registerPrototype(QueryBuilder.class, EmptyQueryBuilder.PROTOTYPE);
         this.queryParsers = ImmutableMap.copyOf(queryParsers);
     }
 
     /**
      * Returns all the registered query parsers
      */
-    public ImmutableMap<String, QueryParser<?>> queryParsers() {
+    public ImmutableMap<String, QueryParser> queryParsers() {
         return queryParsers;
     }
 }
\ No newline at end of file
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
index 295ab49..9d0439d 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java
@@ -293,7 +293,7 @@ public class RecoverySourceHandler {
                             store.incRef();
                             final StoreFileMetaData md = recoverySourceMetadata.get(name);
                             try (final IndexInput indexInput = store.directory().openInput(name, IOContext.READONCE)) {
-                                final int BUFFER_SIZE = (int) recoverySettings.fileChunkSize().bytes();
+                                final int BUFFER_SIZE = (int) Math.max(1, recoverySettings.fileChunkSize().bytes()); // at least one!
                                 final byte[] buf = new byte[BUFFER_SIZE];
                                 boolean shouldCompressRequest = recoverySettings.compress();
                                 if (CompressorFactory.isCompressed(indexInput)) {
diff --git a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java
index b4d5bf6..cef2454 100644
--- a/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java
+++ b/core/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java
@@ -226,6 +226,9 @@ public class RecoveryStatus extends AbstractRefCounted {
     public IndexOutput openAndPutIndexOutput(String fileName, StoreFileMetaData metaData, Store store) throws IOException {
         ensureRefCount();
         String tempFileName = getTempNameForFile(fileName);
+        if (tempFileNames.containsKey(tempFileName)) {
+            throw new IllegalStateException("output for file [" + fileName + "] has already been created");
+        }
         // add first, before it's created
         tempFileNames.put(tempFileName, fileName);
         IndexOutput indexOutput = store.createVerifyingOutput(tempFileName, metaData, IOContext.DEFAULT);
diff --git a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
index 7a824cd..0f46027 100644
--- a/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
+++ b/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java
@@ -19,20 +19,28 @@
 
 package org.elasticsearch.node.internal;
 
+import com.google.common.base.Charsets;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.UnmodifiableIterator;
 import org.elasticsearch.cluster.ClusterName;
 import org.elasticsearch.common.Booleans;
-import org.elasticsearch.common.Names;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.cli.Terminal;
 import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.FailedToResolveConfigException;
 
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.ThreadLocalRandom;
 
 import static org.elasticsearch.common.Strings.cleanPath;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
@@ -92,23 +100,23 @@ public class InternalSettingsPreparer {
                 // if its default, then load it, but also load form env
                 if (Strings.hasText(System.getProperty("es.default.config"))) {
                     loadFromEnv = true;
-                    settingsBuilder.loadFromUrl(environment.resolveConfig(System.getProperty("es.default.config")));
+                    settingsBuilder.loadFromPath(environment.configFile().resolve(System.getProperty("es.default.config")));
                 }
                 // if explicit, just load it and don't load from env
                 if (Strings.hasText(System.getProperty("es.config"))) {
                     loadFromEnv = false;
-                    settingsBuilder.loadFromUrl(environment.resolveConfig(System.getProperty("es.config")));
+                    settingsBuilder.loadFromPath(environment.configFile().resolve(System.getProperty("es.config")));
                 }
                 if (Strings.hasText(System.getProperty("elasticsearch.config"))) {
                     loadFromEnv = false;
-                    settingsBuilder.loadFromUrl(environment.resolveConfig(System.getProperty("elasticsearch.config")));
+                    settingsBuilder.loadFromPath(environment.configFile().resolve(System.getProperty("elasticsearch.config")));
                 }
             }
             if (loadFromEnv) {
                 for (String allowedSuffix : ALLOWED_SUFFIXES) {
                     try {
-                        settingsBuilder.loadFromUrl(environment.resolveConfig("elasticsearch" + allowedSuffix));
-                    } catch (FailedToResolveConfigException e) {
+                        settingsBuilder.loadFromPath(environment.configFile().resolve("elasticsearch" + allowedSuffix));
+                    } catch (SettingsException e) {
                         // ignore
                     }
                 }
@@ -154,16 +162,11 @@ public class InternalSettingsPreparer {
         // all settings placeholders have been resolved. resolve the value for the name setting by checking for name,
         // then looking for node.name, and finally generate one if needed
         if (settings.get("name") == null) {
-            final String name = settings.get("node.name");
+            String name = settings.get("node.name");
             if (name == null || name.isEmpty()) {
-                settings = settingsBuilder().put(settings)
-                        .put("name", Names.randomNodeName(environment.resolveConfig("names.txt")))
-                        .build();
-            } else {
-                settings = settingsBuilder().put(settings)
-                        .put("name", name)
-                        .build();
+                name = randomNodeName(environment);
             }
+            settings = settingsBuilder().put(settings).put("name", name).build();
         }
 
         environment = new Environment(settings);
@@ -178,6 +181,35 @@ public class InternalSettingsPreparer {
         return new Tuple<>(settings, environment);
     }
 
+    static String randomNodeName(Environment environment) {
+        InputStream input;
+        Path namesPath = environment.configFile().resolve("names.txt");
+        if (Files.exists(namesPath)) {
+            try {
+                input = Files.newInputStream(namesPath);
+            } catch (IOException e) {
+                throw new RuntimeException("Failed to load custom names.txt from " + namesPath, e);
+            }
+        } else {
+            input = InternalSettingsPreparer.class.getResourceAsStream("/config/names.txt");
+        }
+
+        try {
+            List<String> names = new ArrayList<>();
+            try (BufferedReader reader = new BufferedReader(new InputStreamReader(input, Charsets.UTF_8))) {
+                String name = reader.readLine();
+                while (name != null) {
+                    names.add(name);
+                    name = reader.readLine();
+                }
+            }
+            int index = ThreadLocalRandom.current().nextInt(names.size());
+            return names.get(index);
+        } catch (IOException e) {
+            throw new RuntimeException("Could not read node names list", e);
+        }
+    }
+
     static Settings replacePromptPlaceholders(Settings settings, Terminal terminal) {
         UnmodifiableIterator<Map.Entry<String, String>> iter = settings.getAsMap().entrySet().iterator();
         Settings.Builder builder = Settings.builder();
diff --git a/core/src/main/java/org/elasticsearch/node/settings/NodeSettingsService.java b/core/src/main/java/org/elasticsearch/node/settings/NodeSettingsService.java
index 8a96e9f..dbe6a33 100644
--- a/core/src/main/java/org/elasticsearch/node/settings/NodeSettingsService.java
+++ b/core/src/main/java/org/elasticsearch/node/settings/NodeSettingsService.java
@@ -106,7 +106,7 @@ public class NodeSettingsService extends AbstractComponent implements ClusterSta
     }
 
     /**
-     * Only settings registered in {@link org.elasticsearch.cluster.settings.ClusterDynamicSettingsModule} can be changed dynamically.
+     * Only settings registered in {@link org.elasticsearch.cluster.ClusterModule} can be changed dynamically.
      */
     public void addListener(Listener listener) {
         this.listeners.add(listener);
@@ -116,7 +116,7 @@ public class NodeSettingsService extends AbstractComponent implements ClusterSta
         this.listeners.remove(listener);
     }
 
-    public static interface Listener {
+    public interface Listener {
         void onRefreshSettings(Settings settings);
     }
 }
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
index bf3b060..3e79ef4 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateContext.java
@@ -20,9 +20,11 @@ package org.elasticsearch.percolator;
 
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
 import com.google.common.collect.ImmutableList;
+
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
@@ -32,7 +34,6 @@ import org.apache.lucene.util.Counter;
 import org.elasticsearch.action.percolate.PercolateShardRequest;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.cache.recycler.PageCacheRecycler;
-import org.elasticsearch.client.Client;
 import org.elasticsearch.common.*;
 import org.elasticsearch.common.collect.ImmutableOpenMap;
 import org.elasticsearch.common.lease.Releasables;
@@ -111,13 +112,13 @@ public class PercolateContext extends SearchContext {
     private SearchLookup searchLookup;
     private ParsedQuery parsedQuery;
     private Query query;
-    private boolean queryRewritten;
     private Query percolateQuery;
     private FetchSubPhase.HitContext hitContext;
     private SearchContextAggregations aggregations;
     private QuerySearchResult querySearchResult;
     private Sort sort;
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
+    private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
 
     public PercolateContext(PercolateShardRequest request, SearchShardTarget searchShardTarget, IndexShard indexShard,
                             IndexService indexService, PageCacheRecycler pageCacheRecycler,
@@ -232,7 +233,6 @@ public class PercolateContext extends SearchContext {
     public SearchContext parsedQuery(ParsedQuery query) {
         this.parsedQuery = query;
         this.query = query.query();
-        this.queryRewritten = false;
         return this;
     }
 
@@ -247,18 +247,6 @@ public class PercolateContext extends SearchContext {
     }
 
     @Override
-    public boolean queryRewritten() {
-        return queryRewritten;
-    }
-
-    @Override
-    public SearchContext updateRewriteQuery(Query rewriteQuery) {
-        queryRewritten = true;
-        query = rewriteQuery;
-        return this;
-    }
-
-    @Override
     public String[] types() {
         return types;
     }
@@ -768,4 +756,9 @@ public class PercolateContext extends SearchContext {
     public void copyContextAndHeadersFrom(HasContextAndHeaders other) {
         assert false : "percolatecontext does not support contexts & headers";
     }
+
+    @Override
+    public Map<Class<?>, Collector> queryCollectors() {
+        return queryCollectors;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
index 1e7cbb7..fd140ec 100644
--- a/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
+++ b/core/src/main/java/org/elasticsearch/plugins/PluginManager.java
@@ -23,11 +23,14 @@ import com.google.common.base.Strings;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterators;
 import org.apache.lucene.util.IOUtils;
+import org.elasticsearch.Build;
+import org.elasticsearch.ElasticsearchCorruptionException;
 import org.elasticsearch.ElasticsearchTimeoutException;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.Version;
 import org.elasticsearch.bootstrap.JarHell;
 import org.elasticsearch.common.cli.Terminal;
+import org.elasticsearch.common.collect.Tuple;
 import org.elasticsearch.common.http.client.HttpDownloadHelper;
 import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.unit.TimeValue;
@@ -82,7 +85,8 @@ public class PluginManager {
                     "elasticsearch-cloud-gce",
                     "elasticsearch-delete-by-query",
                     "elasticsearch-lang-javascript",
-                    "elasticsearch-lang-python"
+                    "elasticsearch-lang-python",
+                    "elasticsearch-mapper-size"
             ).build();
 
     private final Environment environment;
@@ -123,6 +127,7 @@ public class PluginManager {
 
         HttpDownloadHelper downloadHelper = new HttpDownloadHelper();
         boolean downloaded = false;
+        boolean verified = false;
         HttpDownloadHelper.DownloadProgress progress;
         if (outputMode == OutputMode.SILENT) {
             progress = new HttpDownloadHelper.NullProgress();
@@ -143,7 +148,14 @@ public class PluginManager {
             try {
                 downloadHelper.download(pluginUrl, pluginFile, progress, this.timeout);
                 downloaded = true;
-            } catch (ElasticsearchTimeoutException e) {
+                terminal.println("Verifying %s checksums if available ...", pluginUrl.toExternalForm());
+                Tuple<URL, Path> sha1Info = pluginHandle.newChecksumUrlAndFile(environment, pluginUrl, "sha1");
+                verified = downloadHelper.downloadAndVerifyChecksum(sha1Info.v1(), pluginFile,
+                        sha1Info.v2(), progress, this.timeout, HttpDownloadHelper.SHA1_CHECKSUM);
+                Tuple<URL, Path> md5Info = pluginHandle.newChecksumUrlAndFile(environment, pluginUrl, "md5");
+                verified = verified || downloadHelper.downloadAndVerifyChecksum(md5Info.v1(), pluginFile,
+                        md5Info.v2(), progress, this.timeout, HttpDownloadHelper.MD5_CHECKSUM);
+            } catch (ElasticsearchTimeoutException | ElasticsearchCorruptionException e) {
                 throw e;
             } catch (Exception e) {
                 // ignore
@@ -162,8 +174,15 @@ public class PluginManager {
                 try {
                     downloadHelper.download(url, pluginFile, progress, this.timeout);
                     downloaded = true;
+                    terminal.println("Verifying %s checksums if available ...", url.toExternalForm());
+                    Tuple<URL, Path> sha1Info = pluginHandle.newChecksumUrlAndFile(environment, url, "sha1");
+                    verified = downloadHelper.downloadAndVerifyChecksum(sha1Info.v1(), pluginFile,
+                            sha1Info.v2(), progress, this.timeout, HttpDownloadHelper.SHA1_CHECKSUM);
+                    Tuple<URL, Path> md5Info = pluginHandle.newChecksumUrlAndFile(environment, url, "md5");
+                    verified = verified || downloadHelper.downloadAndVerifyChecksum(md5Info.v1(), pluginFile,
+                            md5Info.v2(), progress, this.timeout, HttpDownloadHelper.MD5_CHECKSUM);
                     break;
-                } catch (ElasticsearchTimeoutException e) {
+                } catch (ElasticsearchTimeoutException | ElasticsearchCorruptionException e) {
                     throw e;
                 } catch (Exception e) {
                     terminal.println(VERBOSE, "Failed: %s", ExceptionsHelper.detailedMessage(e));
@@ -176,6 +195,10 @@ public class PluginManager {
             IOUtils.deleteFilesIgnoringExceptions(pluginFile);
             throw new IOException("failed to download out of all possible locations..., use --verbose to get detailed information");
         }
+
+        if (verified == false) {
+            terminal.println("NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)");
+        }
         return pluginFile;
     }
 
@@ -433,9 +456,9 @@ public class PluginManager {
                 if (user == null) {
                     // TODO Update to https
                     if (!Strings.isNullOrEmpty(System.getProperty(PROPERTY_SUPPORT_STAGING_URLS))) {
-                        addUrl(urls, String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/staging/org/elasticsearch/plugin/%s/%s/%s-%s.zip", repo, version, repo, version));
+                        addUrl(urls, String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/staging/%s/org/elasticsearch/plugin/elasticsearch-%s/%s/elasticsearch-%s-%s.zip", Build.CURRENT.hashShort(), repo, version, repo, version));
                     }
-                    addUrl(urls, String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/%s/%s/%s-%s.zip", repo, version, repo, version));
+                    addUrl(urls, String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/elasticsearch-%s/%s/elasticsearch-%s-%s.zip", repo, version, repo, version));
                 } else {
                     // Elasticsearch old download service
                     // TODO Update to https
@@ -467,6 +490,11 @@ public class PluginManager {
             return Files.createTempFile(env.tmpFile(), name, ".zip");
         }
 
+        Tuple<URL, Path> newChecksumUrlAndFile(Environment env, URL originalUrl, String suffix) throws IOException {
+            URL newUrl = new URL(originalUrl.toString() + "." + suffix);
+            return new Tuple<>(newUrl, Files.createTempFile(env.tmpFile(), name, ".zip." + suffix));
+        }
+
         Path extractedDir(Environment env) {
             return env.pluginsFile().resolve(name);
         }
diff --git a/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java b/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
index 7c01fdd..ce306c6 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java
@@ -30,7 +30,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.index.get.GetResult;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.*;
@@ -75,7 +74,13 @@ public class RestExplainAction extends BaseRestHandler {
             queryStringBuilder.lenient(request.paramAsBoolean("lenient", null));
             String defaultOperator = request.param("default_operator");
             if (defaultOperator != null) {
-                queryStringBuilder.defaultOperator(Operator.fromString(defaultOperator));
+                if ("OR".equals(defaultOperator)) {
+                    queryStringBuilder.defaultOperator(QueryStringQueryBuilder.Operator.OR);
+                } else if ("AND".equals(defaultOperator)) {
+                    queryStringBuilder.defaultOperator(QueryStringQueryBuilder.Operator.AND);
+                } else {
+                    throw new IllegalArgumentException("Unsupported defaultOperator [" + defaultOperator + "], can either be [OR] or [AND]");
+                }
             }
 
             QuerySourceBuilder querySourceBuilder = new QuerySourceBuilder();
diff --git a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
index 674aa69..bd17c1d 100644
--- a/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
+++ b/core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
@@ -27,7 +27,6 @@ import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.bytes.BytesReference;
 import org.elasticsearch.common.lucene.uid.Versions;
 import org.elasticsearch.common.xcontent.*;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.QueryStringQueryBuilder;
 import org.elasticsearch.rest.RestRequest;
@@ -98,7 +97,13 @@ public class RestActions {
         queryBuilder.lenient(request.paramAsBoolean("lenient", null));
         String defaultOperator = request.param("default_operator");
         if (defaultOperator != null) {
-            queryBuilder.defaultOperator(Operator.fromString(defaultOperator));
+            if ("OR".equals(defaultOperator)) {
+                queryBuilder.defaultOperator(QueryStringQueryBuilder.Operator.OR);
+            } else if ("AND".equals(defaultOperator)) {
+                queryBuilder.defaultOperator(QueryStringQueryBuilder.Operator.AND);
+            } else {
+                throw new IllegalArgumentException("Unsupported defaultOperator [" + defaultOperator + "], can either be [OR] or [AND]");
+            }
         }
         return new QuerySourceBuilder().setQuery(queryBuilder);
     }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
index c983c40..d4ed3b1 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java
@@ -86,7 +86,7 @@ public class AggregationPhase implements SearchPhase {
                 if (!collectors.isEmpty()) {
                     final BucketCollector collector = BucketCollector.wrap(collectors);
                     collector.preCollection();
-                    context.searcher().queryCollectors().put(AggregationPhase.class, collector);
+                    context.queryCollectors().put(AggregationPhase.class, collector);
                 }
             } catch (IOException e) {
                 throw new AggregationInitializationException("Could not initialize aggregators", e);
@@ -162,7 +162,7 @@ public class AggregationPhase implements SearchPhase {
 
         // disable aggregations so that they don't run on next pages in case of scrolling
         context.aggregations(null);
-        context.searcher().queryCollectors().remove(AggregationPhase.class);
+        context.queryCollectors().remove(AggregationPhase.class);
     }
 
 }
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
index f85bd80..0ac3b1d 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/GND.java
@@ -28,7 +28,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
@@ -115,7 +115,7 @@ public class GND extends NXYSignificanceHeuristic {
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             String givenName = parser.currentName();
             boolean backgroundIsSuperset = true;
             XContentParser.Token token = parser.nextToken();
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
index 5c9794a..78f1573 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/JLHScore.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
@@ -108,7 +108,7 @@ public class JLHScore extends SignificanceHeuristic {
     public static class JLHScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [jhl] significance heuristic. expected an empty object, but found [{}] instead", parser.currentToken());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
index d21b319..cc684c8 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/NXYSignificanceHeuristic.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.ParseFieldMatcher;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
@@ -138,7 +138,7 @@ public abstract class NXYSignificanceHeuristic extends SignificanceHeuristic {
     public static abstract class NXYParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             String givenName = parser.currentName();
             boolean includeNegatives = false;
             boolean backgroundIsSuperset = true;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
index 25556c9..1587a8f 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/PercentageScore.java
@@ -27,7 +27,7 @@ import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 
 import java.io.IOException;
 
@@ -77,7 +77,7 @@ public class PercentageScore extends SignificanceHeuristic {
     public static class PercentageScoreParser implements SignificanceHeuristicParser {
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             // move to the closing bracket
             if (!parser.nextToken().equals(XContentParser.Token.END_OBJECT)) {
                 throw new ElasticsearchParseException("failed to parse [percentage] significance heuristic. expected an empty object, but got [{}] instead", parser.currentToken());
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
index d3a4e64..1be9df2 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
@@ -30,7 +30,7 @@ import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.logging.ESLoggerFactory;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.script.*;
 import org.elasticsearch.script.Script.ScriptField;
 import org.elasticsearch.script.ScriptParameterParser.ScriptParameterValue;
@@ -129,7 +129,7 @@ public class ScriptHeuristic extends SignificanceHeuristic {
         }
 
         @Override
-        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+        public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
             String heuristicName = parser.currentName();
             Script script = null;
             XContentParser.Token token;
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
index 4c4b2e8..9cc1f60 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java
@@ -24,6 +24,7 @@ import com.google.common.collect.Multimap;
 
 import org.elasticsearch.common.io.stream.Streamable;
 import org.elasticsearch.common.xcontent.ToXContent;
+import org.elasticsearch.search.aggregations.AggregationExecutionException;
 import org.elasticsearch.search.aggregations.Aggregations;
 import org.elasticsearch.search.aggregations.InternalAggregation;
 import org.elasticsearch.search.aggregations.InternalAggregations;
@@ -170,8 +171,25 @@ public abstract class InternalTerms<A extends InternalTerms, B extends InternalT
         Multimap<Object, InternalTerms.Bucket> buckets = ArrayListMultimap.create();
         long sumDocCountError = 0;
         long otherDocCount = 0;
+        InternalTerms<A, B> referenceTerms = null;
         for (InternalAggregation aggregation : aggregations) {
             InternalTerms<A, B> terms = (InternalTerms<A, B>) aggregation;
+            if (referenceTerms == null && !terms.getClass().equals(UnmappedTerms.class)) {
+                referenceTerms = (InternalTerms<A, B>) aggregation;
+            }
+            if (referenceTerms != null &&
+                    !referenceTerms.getClass().equals(terms.getClass()) &&
+                    !terms.getClass().equals(UnmappedTerms.class)) {
+                // control gets into this loop when the same field name against which the query is executed
+                // is of different types in different indices.
+                throw new AggregationExecutionException("Merging/Reducing the aggregations failed " +
+                                                        "when computing the aggregation [ Name: " +
+                                                        referenceTerms.getName() + ", Type: " +
+                                                        referenceTerms.type() + " ]" + " because: " +
+                                                        "the field you gave in the aggregation query " +
+                                                        "existed as two different types " +
+                                                        "in two different indices");
+            }
             otherDocCount += terms.getSumOfOtherDocCounts();
             final long thisAggDocCountError;
             if (terms.buckets.size() < this.shardSize || this.order == InternalOrder.TERM_ASC || this.order == InternalOrder.TERM_DESC) {
diff --git a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
index b7c3f33..ee91782 100644
--- a/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
+++ b/core/src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java
@@ -156,6 +156,12 @@ public class AggregationContext {
     }
 
     private ValuesSource.Numeric numericField(ValuesSourceConfig<?> config) throws IOException {
+
+        if (!(config.fieldContext.indexFieldData() instanceof IndexNumericFieldData)) {
+            throw new IllegalArgumentException("Expected numeric type on field [" + config.fieldContext.field() +
+                    "], but got [" + config.fieldContext.fieldType().typeName() + "]");
+        }
+
         ValuesSource.Numeric dataSource = new ValuesSource.Numeric.FieldData((IndexNumericFieldData) config.fieldContext.indexFieldData());
         if (config.script != null) {
             dataSource = new ValuesSource.Numeric.WithScript(dataSource, config.script);
@@ -184,6 +190,12 @@ public class AggregationContext {
     }
 
     private ValuesSource.GeoPoint geoPointField(ValuesSourceConfig<?> config) throws IOException {
+
+        if (!(config.fieldContext.indexFieldData() instanceof IndexGeoPointFieldData)) {
+            throw new IllegalArgumentException("Expected geo_point type on field [" + config.fieldContext.field() +
+                    "], but got [" + config.fieldContext.fieldType().typeName() + "]");
+        }
+
         return new ValuesSource.GeoPoint.Fielddata((IndexGeoPointFieldData) config.fieldContext.indexFieldData());
     }
 
diff --git a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
index 2df22bb..0eb4c5c 100644
--- a/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
+++ b/core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java
@@ -37,6 +37,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.QueryBuilder;
 import org.elasticsearch.script.Script;
+import org.elasticsearch.script.ScriptService.ScriptType;
 import org.elasticsearch.search.aggregations.AbstractAggregationBuilder;
 import org.elasticsearch.search.fetch.innerhits.InnerHitsBuilder;
 import org.elasticsearch.search.fetch.source.FetchSourceContext;
diff --git a/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java b/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
index 3a87979..f552292 100644
--- a/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/dfs/DfsPhase.java
@@ -57,10 +57,6 @@ public class DfsPhase implements SearchPhase {
     public void execute(SearchContext context) {
         final ObjectHashSet<Term> termsSet = new ObjectHashSet<>();
         try {
-            if (!context.queryRewritten()) {
-                context.updateRewriteQuery(context.searcher().rewrite(context.query()));
-            }
-
             context.searcher().createNormalizedWeight(context.query(), true).extractTerms(new DelegateSet(termsSet));
             for (RescoreSearchContext rescoreContext : context.rescore()) {
                 rescoreContext.rescorer().extractTerms(context, rescoreContext, new DelegateSet(termsSet));
diff --git a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
index ac6dc18..c02e2c6 100644
--- a/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java
@@ -24,7 +24,7 @@ import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.mapper.DocumentMapper;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.fetch.fielddata.FieldDataFieldsParseElement;
 import org.elasticsearch.search.fetch.script.ScriptFieldsParseElement;
@@ -59,15 +59,15 @@ public class InnerHitsParseElement implements SearchParseElement {
 
     @Override
     public void parse(XContentParser parser, SearchContext searchContext) throws Exception {
-        QueryShardContext context = searchContext.queryParserService().getShardContext();
-        context.reset(parser);
-        Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = parseInnerHits(parser, context, searchContext);
+        QueryParseContext parseContext = searchContext.queryParserService().getParseContext();
+        parseContext.reset(parser);
+        Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = parseInnerHits(parser, parseContext, searchContext);
         if (innerHitsMap != null) {
             searchContext.innerHits(new InnerHitsContext(innerHitsMap));
         }
     }
 
-    private Map<String, InnerHitsContext.BaseInnerHits> parseInnerHits(XContentParser parser, QueryShardContext context, SearchContext searchContext) throws Exception {
+    private Map<String, InnerHitsContext.BaseInnerHits> parseInnerHits(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext) throws Exception {
         XContentParser.Token token;
         Map<String, InnerHitsContext.BaseInnerHits> innerHitsMap = null;
         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {
@@ -79,7 +79,7 @@ public class InnerHitsParseElement implements SearchParseElement {
             if (token != XContentParser.Token.START_OBJECT) {
                 throw new IllegalArgumentException("Inner hit definition for [" + innerHitName + " starts with a [" + token + "], expected a [" + XContentParser.Token.START_OBJECT + "].");
             }
-            InnerHitsContext.BaseInnerHits innerHits = parseInnerHit(parser, context, searchContext, innerHitName);
+            InnerHitsContext.BaseInnerHits innerHits = parseInnerHit(parser, parseContext, searchContext, innerHitName);
             if (innerHitsMap == null) {
                 innerHitsMap = new HashMap<>();
             }
@@ -88,7 +88,7 @@ public class InnerHitsParseElement implements SearchParseElement {
         return innerHitsMap;
     }
 
-    private InnerHitsContext.BaseInnerHits parseInnerHit(XContentParser parser, QueryShardContext context, SearchContext searchContext, String innerHitName) throws Exception {
+    private InnerHitsContext.BaseInnerHits parseInnerHit(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String innerHitName) throws Exception {
         XContentParser.Token token = parser.nextToken();
         if (token != XContentParser.Token.FIELD_NAME) {
             throw new IllegalArgumentException("Unexpected token " + token + " inside inner hit definition. Either specify [path] or [type] object");
@@ -123,9 +123,9 @@ public class InnerHitsParseElement implements SearchParseElement {
 
         final InnerHitsContext.BaseInnerHits innerHits;
         if (nestedPath != null) {
-            innerHits = parseNested(parser, context, searchContext, fieldName);
+            innerHits = parseNested(parser, parseContext, searchContext, fieldName);
         } else if (type != null) {
-            innerHits = parseParentChild(parser, context, searchContext, fieldName);
+            innerHits = parseParentChild(parser, parseContext, searchContext, fieldName);
         } else {
             throw new IllegalArgumentException("Either [path] or [type] must be defined");
         }
@@ -143,16 +143,16 @@ public class InnerHitsParseElement implements SearchParseElement {
         return innerHits;
     }
 
-    private InnerHitsContext.ParentChildInnerHits parseParentChild(XContentParser parser, QueryShardContext context, SearchContext searchContext, String type) throws Exception {
-        ParseResult parseResult = parseSubSearchContext(searchContext, context, parser);
+    private InnerHitsContext.ParentChildInnerHits parseParentChild(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String type) throws Exception {
+        ParseResult parseResult = parseSubSearchContext(searchContext, parseContext, parser);
         DocumentMapper documentMapper = searchContext.mapperService().documentMapper(type);
         if (documentMapper == null) {
             throw new IllegalArgumentException("type [" + type + "] doesn't exist");
         }
-        return new InnerHitsContext.ParentChildInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), context.mapperService(), documentMapper);
+        return new InnerHitsContext.ParentChildInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), parseContext.mapperService(), documentMapper);
     }
 
-    private InnerHitsContext.NestedInnerHits parseNested(XContentParser parser, QueryShardContext context, SearchContext searchContext, String nestedPath) throws Exception {
+    private InnerHitsContext.NestedInnerHits parseNested(XContentParser parser, QueryParseContext parseContext, SearchContext searchContext, String nestedPath) throws Exception {
         ObjectMapper objectMapper = searchContext.getObjectMapper(nestedPath);
         if (objectMapper == null) {
             throw new IllegalArgumentException("path [" + nestedPath +"] doesn't exist");
@@ -160,14 +160,14 @@ public class InnerHitsParseElement implements SearchParseElement {
         if (objectMapper.nested().isNested() == false) {
             throw new IllegalArgumentException("path [" + nestedPath +"] isn't nested");
         }
-        ObjectMapper parentObjectMapper = context.nestedScope().nextLevel(objectMapper);
-        ParseResult parseResult = parseSubSearchContext(searchContext, context, parser);
-        context.nestedScope().previousLevel();
+        ObjectMapper parentObjectMapper = parseContext.nestedScope().nextLevel(objectMapper);
+        ParseResult parseResult = parseSubSearchContext(searchContext, parseContext, parser);
+        parseContext.nestedScope().previousLevel();
 
         return new InnerHitsContext.NestedInnerHits(parseResult.context(), parseResult.query(), parseResult.childInnerHits(), parentObjectMapper, objectMapper);
     }
 
-    private ParseResult parseSubSearchContext(SearchContext searchContext, QueryShardContext context, XContentParser parser) throws Exception {
+    private ParseResult parseSubSearchContext(SearchContext searchContext, QueryParseContext parseContext, XContentParser parser) throws Exception {
         ParsedQuery query = null;
         Map<String, InnerHitsContext.BaseInnerHits> childInnerHits = null;
         SubSearchContext subSearchContext = new SubSearchContext(searchContext);
@@ -178,10 +178,10 @@ public class InnerHitsParseElement implements SearchParseElement {
                 fieldName = parser.currentName();
             } else if (token == XContentParser.Token.START_OBJECT) {
                 if ("query".equals(fieldName)) {
-                    Query q = searchContext.queryParserService().parseInnerQuery(context);
-                    query = new ParsedQuery(q, context.copyNamedQueries());
+                    Query q = searchContext.queryParserService().parseInnerQuery(parseContext);
+                    query = new ParsedQuery(q, parseContext.copyNamedQueries());
                 } else if ("inner_hits".equals(fieldName)) {
-                    childInnerHits = parseInnerHits(parser, context, searchContext);
+                    childInnerHits = parseInnerHits(parser, parseContext, searchContext);
                 } else {
                     parseCommonInnerHitOptions(parser, token, fieldName, subSearchContext, sortParseElement, sourceParseElement, highlighterParseElement, scriptFieldsParseElement, fieldDataFieldsParseElement);
                 }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java b/core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java
index dd7489c..11ce914 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java
@@ -20,41 +20,25 @@
 package org.elasticsearch.search.internal;
 
 import org.apache.lucene.index.LeafReaderContext;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MultiCollector;
 import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TimeLimitingCollector;
 import org.apache.lucene.search.Weight;
 import org.elasticsearch.ExceptionsHelper;
 import org.elasticsearch.common.lease.Releasable;
-import org.elasticsearch.common.lucene.Lucene;
-import org.elasticsearch.common.lucene.MinimumScoreCollector;
-import org.elasticsearch.common.lucene.search.FilteredCollector;
 import org.elasticsearch.index.engine.Engine;
-import org.elasticsearch.search.SearchService;
 import org.elasticsearch.search.dfs.CachedDfSource;
 import org.elasticsearch.search.internal.SearchContext.Lifetime;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 
 /**
  * Context-aware extension of {@link IndexSearcher}.
  */
 public class ContextIndexSearcher extends IndexSearcher implements Releasable {
 
-    public static enum Stage {
-        NA,
-        MAIN_QUERY
-    }
-
     /** The wrapped {@link IndexSearcher}. The reason why we sometimes prefer delegating to this searcher instead of <tt>super</tt> is that
      *  this instance may have more assertions, for example if it comes from MockInternalEngine which wraps the IndexSearcher into an
      *  AssertingIndexSearcher. */
@@ -64,10 +48,6 @@ public class ContextIndexSearcher extends IndexSearcher implements Releasable {
 
     private CachedDfSource dfSource;
 
-    private Map<Class<?>, Collector> queryCollectors;
-
-    private Stage currentState = Stage.NA;
-
     public ContextIndexSearcher(SearchContext searchContext, Engine.Searcher searcher) {
         super(searcher.reader());
         in = searcher.searcher();
@@ -83,49 +63,21 @@ public class ContextIndexSearcher extends IndexSearcher implements Releasable {
         this.dfSource = dfSource;
     }
 
-    /**
-     * Adds a query level collector that runs at {@link Stage#MAIN_QUERY}. Note, supports
-     * {@link org.elasticsearch.common.lucene.search.XCollector} allowing for a callback
-     * when collection is done.
-     */
-    public Map<Class<?>, Collector> queryCollectors() {
-        if (queryCollectors == null) {
-            queryCollectors = new HashMap<>();
-        }
-        return queryCollectors;
-    }
-
-    public void inStage(Stage stage) {
-        this.currentState = stage;
-    }
-
-    public void finishStage(Stage stage) {
-        assert currentState == stage : "Expected stage " + stage + " but was stage " + currentState;
-        this.currentState = Stage.NA;
-    }
-
     @Override
     public Query rewrite(Query original) throws IOException {
-        if (original == searchContext.query() || original == searchContext.parsedQuery().query()) {
-            // optimize in case its the top level search query and we already rewrote it...
-            if (searchContext.queryRewritten()) {
-                return searchContext.query();
-            }
-            Query rewriteQuery = in.rewrite(original);
-            searchContext.updateRewriteQuery(rewriteQuery);
-            return rewriteQuery;
-        } else {
+        try {
             return in.rewrite(original);
+        } catch (Throwable t) {
+            searchContext.clearReleasables(Lifetime.COLLECTION);
+            throw ExceptionsHelper.convertToElastic(t);
         }
     }
 
     @Override
     public Weight createNormalizedWeight(Query query, boolean needsScores) throws IOException {
-        // TODO: needsScores
-        // can we avoid dfs stuff here if we dont need scores?
         try {
-            // if its the main query, use we have dfs data, only then do it
-            if (dfSource != null && (query == searchContext.query() || query == searchContext.parsedQuery().query())) {
+            // if scores are needed and we have dfs data then use it
+            if (dfSource != null && needsScores) {
                 return dfSource.createNormalizedWeight(query, needsScores);
             }
             return in.createNormalizedWeight(query, needsScores);
@@ -135,81 +87,19 @@ public class ContextIndexSearcher extends IndexSearcher implements Releasable {
         }
     }
 
-
-    @Override
-    public void search(Query query, Collector collector) throws IOException {
-        // Wrap the caller's collector with various wrappers e.g. those used to siphon
-        // matches off for aggregation or to impose a time-limit on collection.
-        final boolean timeoutSet = searchContext.timeoutInMillis() != SearchService.NO_TIMEOUT.millis();
-        final boolean terminateAfterSet = searchContext.terminateAfter() != SearchContext.DEFAULT_TERMINATE_AFTER;
-
-        if (timeoutSet) {
-            // TODO: change to use our own counter that uses the scheduler in ThreadPool
-            // throws TimeLimitingCollector.TimeExceededException when timeout has reached
-            collector = Lucene.wrapTimeLimitingCollector(collector, searchContext.timeEstimateCounter(), searchContext.timeoutInMillis());
-        }
-        if (terminateAfterSet) {
-            // throws Lucene.EarlyTerminationException when given count is reached
-            collector = Lucene.wrapCountBasedEarlyTerminatingCollector(collector, searchContext.terminateAfter());
-        }
-        if (currentState == Stage.MAIN_QUERY) {
-            if (searchContext.parsedPostFilter() != null) {
-                // this will only get applied to the actual search collector and not
-                // to any scoped collectors, also, it will only be applied to the main collector
-                // since that is where the filter should only work
-                final Weight filterWeight = createNormalizedWeight(searchContext.parsedPostFilter().query(), false);
-                collector = new FilteredCollector(collector, filterWeight);
-            }
-            if (queryCollectors != null && !queryCollectors.isEmpty()) {
-                ArrayList<Collector> allCollectors = new ArrayList<>(queryCollectors.values());
-                allCollectors.add(collector);
-                collector = MultiCollector.wrap(allCollectors);
-            }
-
-            // apply the minimum score after multi collector so we filter aggs as well
-            if (searchContext.minimumScore() != null) {
-                collector = new MinimumScoreCollector(collector, searchContext.minimumScore());
-            }
-        }
-        super.search(query, collector);
-    }
-
     @Override
-    public void search(List<LeafReaderContext> leaves, Weight weight, Collector collector) throws IOException {
-        final boolean timeoutSet = searchContext.timeoutInMillis() != SearchService.NO_TIMEOUT.millis();
-        final boolean terminateAfterSet = searchContext.terminateAfter() != SearchContext.DEFAULT_TERMINATE_AFTER;
+    public Explanation explain(Query query, int doc) throws IOException {
         try {
-            if (timeoutSet || terminateAfterSet) {
-                try {
-                    super.search(leaves, weight, collector);
-                } catch (TimeLimitingCollector.TimeExceededException e) {
-                    assert timeoutSet : "TimeExceededException thrown even though timeout wasn't set";
-                    searchContext.queryResult().searchTimedOut(true);
-                } catch (Lucene.EarlyTerminationException e) {
-                    assert terminateAfterSet : "EarlyTerminationException thrown even though terminateAfter wasn't set";
-                    searchContext.queryResult().terminatedEarly(true);
-                }
-                if (terminateAfterSet && searchContext.queryResult().terminatedEarly() == null) {
-                    searchContext.queryResult().terminatedEarly(false);
-                }
-            } else {
-                super.search(leaves, weight, collector);
-            }
+            return in.explain(query, doc);
         } finally {
             searchContext.clearReleasables(Lifetime.COLLECTION);
         }
     }
 
     @Override
-    public Explanation explain(Query query, int doc) throws IOException {
+    protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector) throws IOException {
         try {
-            if (searchContext.aliasFilter() == null) {
-                return super.explain(query, doc);
-            }
-            BooleanQuery filteredQuery = new BooleanQuery();
-            filteredQuery.add(query, Occur.MUST);
-            filteredQuery.add(searchContext.aliasFilter(), Occur.FILTER);
-            return super.explain(filteredQuery, doc);
+            super.search(leaves, weight, collector);
         } finally {
             searchContext.clearReleasables(Lifetime.COLLECTION);
         }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
index f420986..03c13c6 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java
@@ -22,6 +22,7 @@ package org.elasticsearch.search.internal;
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
 import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
+
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.Counter;
@@ -66,6 +67,7 @@ import org.elasticsearch.search.rescore.RescoreSearchContext;
 import org.elasticsearch.search.scan.ScanContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 
+import java.io.IOException;
 import java.util.*;
 
 /**
@@ -119,7 +121,6 @@ public class DefaultSearchContext extends SearchContext {
     private SuggestionSearchContext suggest;
     private List<RescoreSearchContext> rescore;
     private SearchLookup searchLookup;
-    private boolean queryRewritten;
     private volatile long keepAlive;
     private ScoreDoc lastEmittedDoc;
     private final long originNanoTime = System.nanoTime();
@@ -127,6 +128,7 @@ public class DefaultSearchContext extends SearchContext {
     private InnerHitsContext innerHitsContext;
 
     private final Map<String, FetchSubPhaseContext> subPhaseContexts = new HashMap<>();
+    private final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
 
     public DefaultSearchContext(long id, ShardSearchRequest request, SearchShardTarget shardTarget,
                                 Engine.Searcher engineSearcher, IndexService indexService, IndexShard indexShard,
@@ -197,10 +199,15 @@ public class DefaultSearchContext extends SearchContext {
                 parsedQuery(new ParsedQuery(filtered, parsedQuery()));
             }
         }
+        try {
+            this.query = searcher().rewrite(this.query);
+        } catch (IOException e) {
+            throw new QueryPhaseExecutionException(this, "Failed to rewrite main query", e);
+        }
     }
 
     @Override
-    public Filter searchFilter(String[] types) {
+    public Query searchFilter(String[] types) {
         Query filter = mapperService().searchFilter(types);
         if (filter == null && aliasFilter == null) {
             return null;
@@ -212,7 +219,7 @@ public class DefaultSearchContext extends SearchContext {
         if (aliasFilter != null) {
             bq.add(aliasFilter, Occur.MUST);
         }
-        return new QueryWrapperFilter(bq);
+        return new ConstantScoreQuery(bq);
     }
 
     @Override
@@ -513,7 +520,6 @@ public class DefaultSearchContext extends SearchContext {
 
     @Override
     public SearchContext parsedQuery(ParsedQuery query) {
-        queryRewritten = false;
         this.originalQuery = query;
         this.query = query.query();
         return this;
@@ -525,31 +531,13 @@ public class DefaultSearchContext extends SearchContext {
     }
 
     /**
-     * The query to execute, might be rewritten.
+     * The query to execute, in its rewritten form.
      */
     @Override
     public Query query() {
         return this.query;
     }
 
-    /**
-     * Has the query been rewritten already?
-     */
-    @Override
-    public boolean queryRewritten() {
-        return queryRewritten;
-    }
-
-    /**
-     * Rewrites the query and updates it. Only happens once.
-     */
-    @Override
-    public SearchContext updateRewriteQuery(Query rewriteQuery) {
-        query = rewriteQuery;
-        queryRewritten = true;
-        return this;
-    }
-
     @Override
     public int from() {
         return from;
@@ -810,4 +798,9 @@ public class DefaultSearchContext extends SearchContext {
     public void copyContextAndHeadersFrom(HasContextAndHeaders other) {
         request.copyContextAndHeadersFrom(other);
     }
+
+    @Override
+    public Map<Class<?>, Collector> queryCollectors() {
+        return queryCollectors;
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
index c22842e..e2f6b48 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java
@@ -20,6 +20,8 @@
 package org.elasticsearch.search.internal;
 
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
+
+import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
@@ -58,6 +60,7 @@ import org.elasticsearch.search.scan.ScanContext;
 import org.elasticsearch.search.suggest.SuggestionSearchContext;
 
 import java.util.List;
+import java.util.Map;
 import java.util.Set;
 
 public abstract class FilteredSearchContext extends SearchContext {
@@ -376,16 +379,6 @@ public abstract class FilteredSearchContext extends SearchContext {
     }
 
     @Override
-    public boolean queryRewritten() {
-        return in.queryRewritten();
-    }
-
-    @Override
-    public SearchContext updateRewriteQuery(Query rewriteQuery) {
-        return in.updateRewriteQuery(rewriteQuery);
-    }
-
-    @Override
     public int from() {
         return in.from();
     }
@@ -624,4 +617,9 @@ public abstract class FilteredSearchContext extends SearchContext {
     public <SubPhaseContext extends FetchSubPhaseContext> SubPhaseContext getFetchSubPhaseContext(FetchSubPhase.ContextFactory<SubPhaseContext> contextFactory) {
         return in.getFetchSubPhaseContext(contextFactory);
     }
+
+    @Override
+    public Map<Class<?>, Collector> queryCollectors() {
+        return in.queryCollectors();
+    }
 }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
index 649bdc2..f2ed267 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java
@@ -38,12 +38,12 @@ import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentBuilderString;
 import org.elasticsearch.common.xcontent.XContentHelper;
-import org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHitField;
 import org.elasticsearch.search.SearchHits;
 import org.elasticsearch.search.SearchShardTarget;
 import org.elasticsearch.search.highlight.HighlightField;
+import org.elasticsearch.search.internal.InternalSearchHits.StreamContext.ShardTargetType;
 import org.elasticsearch.search.lookup.SourceLookup;
 
 import java.io.IOException;
@@ -556,7 +556,7 @@ public class InternalSearchHit implements SearchHit {
 
     @Override
     public void readFrom(StreamInput in) throws IOException {
-        readFrom(in, InternalSearchHits.streamContext().streamShardTarget(InternalSearchHits.StreamContext.ShardTargetType.STREAM));
+        readFrom(in, InternalSearchHits.streamContext().streamShardTarget(ShardTargetType.STREAM));
     }
 
     public void readFrom(StreamInput in, InternalSearchHits.StreamContext context) throws IOException {
@@ -678,11 +678,11 @@ public class InternalSearchHit implements SearchHit {
             }
         }
 
-        if (context.streamShardTarget() == InternalSearchHits.StreamContext.ShardTargetType.STREAM) {
+        if (context.streamShardTarget() == ShardTargetType.STREAM) {
             if (in.readBoolean()) {
                 shard = readSearchShardTarget(in);
             }
-        } else if (context.streamShardTarget() == InternalSearchHits.StreamContext.ShardTargetType.LOOKUP) {
+        } else if (context.streamShardTarget() == ShardTargetType.LOOKUP) {
             int lookupId = in.readVInt();
             if (lookupId > 0) {
                 shard = context.handleShardLookup().get(lookupId);
@@ -694,7 +694,9 @@ public class InternalSearchHit implements SearchHit {
             innerHits = new HashMap<>(size);
             for (int i = 0; i < size; i++) {
                 String key = in.readString();
-                InternalSearchHits value = InternalSearchHits.readSearchHits(in, InternalSearchHits.streamContext().streamShardTarget(InternalSearchHits.StreamContext.ShardTargetType.NO_STREAM));
+                ShardTargetType shardTarget = context.streamShardTarget();
+                InternalSearchHits value = InternalSearchHits.readSearchHits(in, context.streamShardTarget(ShardTargetType.NO_STREAM));
+                context.streamShardTarget(shardTarget);
                 innerHits.put(key, value);
             }
         }
@@ -702,7 +704,7 @@ public class InternalSearchHit implements SearchHit {
 
     @Override
     public void writeTo(StreamOutput out) throws IOException {
-        writeTo(out, InternalSearchHits.streamContext().streamShardTarget(InternalSearchHits.StreamContext.ShardTargetType.STREAM));
+        writeTo(out, InternalSearchHits.streamContext().streamShardTarget(ShardTargetType.STREAM));
     }
 
     public void writeTo(StreamOutput out, InternalSearchHits.StreamContext context) throws IOException {
@@ -787,14 +789,14 @@ public class InternalSearchHit implements SearchHit {
             }
         }
 
-        if (context.streamShardTarget() == InternalSearchHits.StreamContext.ShardTargetType.STREAM) {
+        if (context.streamShardTarget() == ShardTargetType.STREAM) {
             if (shard == null) {
                 out.writeBoolean(false);
             } else {
                 out.writeBoolean(true);
                 shard.writeTo(out);
             }
-        } else if (context.streamShardTarget() == InternalSearchHits.StreamContext.ShardTargetType.LOOKUP) {
+        } else if (context.streamShardTarget() == ShardTargetType.LOOKUP) {
             if (shard == null) {
                 out.writeVInt(0);
             } else {
@@ -808,7 +810,9 @@ public class InternalSearchHit implements SearchHit {
             out.writeVInt(innerHits.size());
             for (Map.Entry<String, InternalSearchHits> entry : innerHits.entrySet()) {
                 out.writeString(entry.getKey());
-                entry.getValue().writeTo(out, InternalSearchHits.streamContext().streamShardTarget(InternalSearchHits.StreamContext.ShardTargetType.NO_STREAM));
+                ShardTargetType shardTarget = context.streamShardTarget();
+                entry.getValue().writeTo(out, context.streamShardTarget(ShardTargetType.NO_STREAM));
+                context.streamShardTarget(shardTarget);
             }
         }
     }
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
index 1c90a2b..72feec7 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SearchContext.java
@@ -21,6 +21,8 @@ package org.elasticsearch.search.internal;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Multimap;
 import com.google.common.collect.MultimapBuilder;
+
+import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
@@ -41,7 +43,7 @@ import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.IndexQueryParserService;
 import org.elasticsearch.index.query.ParsedQuery;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.index.shard.IndexShard;
 import org.elasticsearch.index.similarity.SimilarityService;
 import org.elasticsearch.script.ScriptService;
@@ -65,6 +67,7 @@ import org.elasticsearch.search.suggest.SuggestionSearchContext;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.List;
+import java.util.Map;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 public abstract class SearchContext implements Releasable, HasContextAndHeaders {
@@ -74,12 +77,12 @@ public abstract class SearchContext implements Releasable, HasContextAndHeaders
 
     public static void setCurrent(SearchContext value) {
         current.set(value);
-        QueryShardContext.setTypes(value.types());
+        QueryParseContext.setTypes(value.types());
     }
 
     public static void removeCurrent() {
         current.remove();
-        QueryShardContext.removeTypes();
+        QueryParseContext.removeTypes();
     }
 
     public static SearchContext current() {
@@ -257,16 +260,6 @@ public abstract class SearchContext implements Releasable, HasContextAndHeaders
      */
     public abstract Query query();
 
-    /**
-     * Has the query been rewritten already?
-     */
-    public abstract boolean queryRewritten();
-
-    /**
-     * Rewrites the query and updates it. Only happens once.
-     */
-    public abstract SearchContext updateRewriteQuery(Query rewriteQuery);
-
     public abstract int from();
 
     public abstract SearchContext from(int from);
@@ -359,6 +352,9 @@ public abstract class SearchContext implements Releasable, HasContextAndHeaders
 
     public abstract Counter timeEstimateCounter();
 
+    /** Return a view of the additional query collectors that should be run for this context. */
+    public abstract Map<Class<?>, Collector> queryCollectors();
+
     /**
      * The life time of an object that is used during search execution.
      */
diff --git a/core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java b/core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java
index be445c9..f70c9a7 100644
--- a/core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java
+++ b/core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java
@@ -207,11 +207,6 @@ public class SubSearchContext extends FilteredSearchContext {
     }
 
     @Override
-    public SearchContext updateRewriteQuery(Query rewriteQuery) {
-        throw new UnsupportedOperationException("Not supported");
-    }
-
-    @Override
     public int from() {
         return from;
     }
diff --git a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
index 0015706..a9105d5 100644
--- a/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
+++ b/core/src/main/java/org/elasticsearch/search/query/QueryPhase.java
@@ -21,24 +21,42 @@ package org.elasticsearch.search.query;
 
 import com.google.common.collect.ImmutableMap;
 
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MultiCollector;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TimeLimitingCollector;
 import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopDocsCollector;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.search.TotalHitCountCollector;
+import org.apache.lucene.search.Weight;
 import org.elasticsearch.action.search.SearchType;
 import org.elasticsearch.common.inject.Inject;
 import org.elasticsearch.common.lucene.Lucene;
+import org.elasticsearch.common.lucene.MinimumScoreCollector;
+import org.elasticsearch.common.lucene.search.FilteredCollector;
 import org.elasticsearch.search.SearchParseElement;
 import org.elasticsearch.search.SearchPhase;
+import org.elasticsearch.search.SearchService;
 import org.elasticsearch.search.aggregations.AggregationPhase;
-import org.elasticsearch.search.internal.ContextIndexSearcher;
 import org.elasticsearch.search.internal.SearchContext;
+import org.elasticsearch.search.internal.SearchContext.Lifetime;
 import org.elasticsearch.search.rescore.RescorePhase;
 import org.elasticsearch.search.rescore.RescoreSearchContext;
+import org.elasticsearch.search.scan.ScanContext.ScanCollector;
 import org.elasticsearch.search.sort.SortParseElement;
 import org.elasticsearch.search.sort.TrackScoresParseElement;
 import org.elasticsearch.search.suggest.SuggestPhase;
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
 import java.util.Map;
+import java.util.concurrent.Callable;
 
 /**
  *
@@ -97,66 +115,133 @@ public class QueryPhase implements SearchPhase {
 
         searchContext.queryResult().searchTimedOut(false);
 
-        searchContext.searcher().inStage(ContextIndexSearcher.Stage.MAIN_QUERY);
         boolean rescore = false;
         try {
             searchContext.queryResult().from(searchContext.from());
             searchContext.queryResult().size(searchContext.size());
 
+            final IndexSearcher searcher = searchContext.searcher();
             Query query = searchContext.query();
 
-            final TopDocs topDocs;
-            int numDocs = searchContext.from() + searchContext.size();
+            final int totalNumDocs = searcher.getIndexReader().numDocs();
+            int numDocs = Math.min(searchContext.from() + searchContext.size(), totalNumDocs);
+
+            Collector collector;
+            final Callable<TopDocs> topDocsCallable;
 
             if (searchContext.size() == 0) { // no matter what the value of from is
-                topDocs = new TopDocs(searchContext.searcher().count(query), Lucene.EMPTY_SCORE_DOCS, 0);
+                final TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector();
+                collector = totalHitCountCollector;
+                topDocsCallable = new Callable<TopDocs>() {
+                    @Override
+                    public TopDocs call() throws Exception {
+                        return new TopDocs(totalHitCountCollector.getTotalHits(), Lucene.EMPTY_SCORE_DOCS, 0);
+                    }
+                };
             } else if (searchContext.searchType() == SearchType.SCAN) {
-                topDocs = searchContext.scanContext().execute(searchContext);
+                query = searchContext.scanContext().wrapQuery(query);
+                final ScanCollector scanCollector = searchContext.scanContext().collector(searchContext);
+                collector = scanCollector;
+                topDocsCallable = new Callable<TopDocs>() {
+                    @Override
+                    public TopDocs call() throws Exception {
+                        return scanCollector.topDocs();
+                    }
+                };
             } else {
                 // Perhaps have a dedicated scroll phase?
+                final TopDocsCollector<?> topDocsCollector;
+                ScoreDoc lastEmittedDoc;
                 if (searchContext.request().scroll() != null) {
-                    numDocs = searchContext.size();
-                    ScoreDoc lastEmittedDoc = searchContext.lastEmittedDoc();
-                    if (searchContext.sort() != null) {
-                        topDocs = searchContext.searcher().searchAfter(
-                                lastEmittedDoc, query, null, numDocs, searchContext.sort(),
-                                searchContext.trackScores(), searchContext.trackScores()
-                        );
-                    } else {
-                        rescore = !searchContext.rescore().isEmpty();
-                        for (RescoreSearchContext rescoreContext : searchContext.rescore()) {
-                            numDocs = Math.max(rescoreContext.window(), numDocs);
-                        }
-                        topDocs = searchContext.searcher().searchAfter(lastEmittedDoc, query, numDocs);
+                    numDocs = Math.min(searchContext.size(), totalNumDocs);
+                    lastEmittedDoc = searchContext.lastEmittedDoc();
+                } else {
+                    lastEmittedDoc = null;
+                }
+                if (totalNumDocs == 0) {
+                    // top collectors don't like a size of 0
+                    numDocs = 1;
+                }
+                assert numDocs > 0;
+                if (searchContext.sort() != null) {
+                    topDocsCollector = TopFieldCollector.create(searchContext.sort(), numDocs,
+                            (FieldDoc) lastEmittedDoc, true, searchContext.trackScores(), searchContext.trackScores());
+                } else {
+                    rescore = !searchContext.rescore().isEmpty();
+                    for (RescoreSearchContext rescoreContext : searchContext.rescore()) {
+                        numDocs = Math.max(rescoreContext.window(), numDocs);
                     }
-
-                    int size = topDocs.scoreDocs.length;
-                    if (size > 0) {
-                        // In the case of *QUERY_AND_FETCH we don't get back to shards telling them which least
-                        // relevant docs got emitted as hit, we can simply mark the last doc as last emitted
-                        if (searchContext.searchType() == SearchType.QUERY_AND_FETCH ||
-                                searchContext.searchType() == SearchType.DFS_QUERY_AND_FETCH) {
-                            searchContext.lastEmittedDoc(topDocs.scoreDocs[size - 1]);
-                        }
+                    topDocsCollector = TopScoreDocCollector.create(numDocs, lastEmittedDoc);
+                }
+                collector = topDocsCollector;
+                topDocsCallable = new Callable<TopDocs>() {
+                    @Override
+                    public TopDocs call() throws Exception {
+                        return topDocsCollector.topDocs();
                     }
-                } else {
-                    if (searchContext.sort() != null) {
-                        topDocs = searchContext.searcher().search(query, null, numDocs, searchContext.sort(),
-                                searchContext.trackScores(), searchContext.trackScores());
-                    } else {
-                        rescore = !searchContext.rescore().isEmpty();
-                        for (RescoreSearchContext rescoreContext : searchContext.rescore()) {
-                            numDocs = Math.max(rescoreContext.window(), numDocs);
-                        }
-                        topDocs = searchContext.searcher().search(query, numDocs);
+                };
+            }
+
+            final boolean terminateAfterSet = searchContext.terminateAfter() != SearchContext.DEFAULT_TERMINATE_AFTER;
+            if (terminateAfterSet) {
+                // throws Lucene.EarlyTerminationException when given count is reached
+                collector = Lucene.wrapCountBasedEarlyTerminatingCollector(collector, searchContext.terminateAfter());
+            }
+
+            if (searchContext.parsedPostFilter() != null) {
+                // this will only get applied to the actual search collector and not
+                // to any scoped collectors, also, it will only be applied to the main collector
+                // since that is where the filter should only work
+                final Weight filterWeight = searcher.createNormalizedWeight(searchContext.parsedPostFilter().query(), false);
+                collector = new FilteredCollector(collector, filterWeight);
+            }
+
+            // plug in additional collectors, like aggregations
+            List<Collector> allCollectors = new ArrayList<>();
+            allCollectors.add(collector);
+            allCollectors.addAll(searchContext.queryCollectors().values());
+            collector = MultiCollector.wrap(allCollectors);
+
+            // apply the minimum score after multi collector so we filter aggs as well
+            if (searchContext.minimumScore() != null) {
+                collector = new MinimumScoreCollector(collector, searchContext.minimumScore());
+            }
+
+            final boolean timeoutSet = searchContext.timeoutInMillis() != SearchService.NO_TIMEOUT.millis();
+            if (timeoutSet) {
+                // TODO: change to use our own counter that uses the scheduler in ThreadPool
+                // throws TimeLimitingCollector.TimeExceededException when timeout has reached
+                collector = Lucene.wrapTimeLimitingCollector(collector, searchContext.timeEstimateCounter(), searchContext.timeoutInMillis());
+            }
+
+            try {
+                searcher.search(query, collector);
+            } catch (TimeLimitingCollector.TimeExceededException e) {
+                assert timeoutSet : "TimeExceededException thrown even though timeout wasn't set";
+                searchContext.queryResult().searchTimedOut(true);
+            } catch (Lucene.EarlyTerminationException e) {
+                assert terminateAfterSet : "EarlyTerminationException thrown even though terminateAfter wasn't set";
+                searchContext.queryResult().terminatedEarly(true);
+            }
+            if (terminateAfterSet && searchContext.queryResult().terminatedEarly() == null) {
+                searchContext.queryResult().terminatedEarly(false);
+            }
+
+            final TopDocs topDocs = topDocsCallable.call();
+            if (searchContext.request().scroll() != null) {
+                int size = topDocs.scoreDocs.length;
+                if (size > 0) {
+                    // In the case of *QUERY_AND_FETCH we don't get back to shards telling them which least
+                    // relevant docs got emitted as hit, we can simply mark the last doc as last emitted
+                    if (searchContext.searchType() == SearchType.QUERY_AND_FETCH ||
+                            searchContext.searchType() == SearchType.DFS_QUERY_AND_FETCH) {
+                        searchContext.lastEmittedDoc(topDocs.scoreDocs[size - 1]);
                     }
                 }
             }
             searchContext.queryResult().topDocs(topDocs);
         } catch (Throwable e) {
             throw new QueryPhaseExecutionException(searchContext, "Failed to execute main query", e);
-        } finally {
-            searchContext.searcher().finishStage(ContextIndexSearcher.Stage.MAIN_QUERY);
         }
         if (rescore) { // only if we do a regular search
             rescorePhase.execute(searchContext);
diff --git a/core/src/main/java/org/elasticsearch/search/scan/ScanContext.java b/core/src/main/java/org/elasticsearch/search/scan/ScanContext.java
index 7aaead1..c0018d4 100644
--- a/core/src/main/java/org/elasticsearch/search/scan/ScanContext.java
+++ b/core/src/main/java/org/elasticsearch/search/scan/ScanContext.java
@@ -47,18 +47,23 @@ public class ScanContext {
 
     private volatile int docUpTo;
 
-    public TopDocs execute(SearchContext context) throws IOException {
-        return execute(context.searcher(), context.query(), context.size(), context.trackScores());
+    public ScanCollector collector(SearchContext context) {
+        return collector(context.size(), context.trackScores());
     }
 
-    TopDocs execute(IndexSearcher searcher, Query query, int size, boolean trackScores) throws IOException {
-        ScanCollector collector = new ScanCollector(size, trackScores);
-        Query q = Queries.filtered(query, new MinDocQuery(docUpTo));
-        searcher.search(q, collector);
-        return collector.topDocs();
+    /** Create a {@link ScanCollector} for the given page size. */
+    ScanCollector collector(int size, boolean trackScores) {
+        return new ScanCollector(size, trackScores);
     }
 
-    private class ScanCollector extends SimpleCollector {
+    /**
+     * Wrap the query so that it can skip directly to the right document.
+     */
+    public Query wrapQuery(Query query) {
+        return Queries.filtered(query, new MinDocQuery(docUpTo));
+    }
+
+    public class ScanCollector extends SimpleCollector {
 
         private final List<ScoreDoc> docs;
 
@@ -70,7 +75,7 @@ public class ScanContext {
 
         private int docBase;
 
-        ScanCollector(int size, boolean trackScores) {
+        private ScanCollector(int size, boolean trackScores) {
             this.trackScores = trackScores;
             this.docs = new ArrayList<>(size);
             this.size = size;
diff --git a/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java b/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
index 8cd88df..e7941a4 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java
@@ -42,6 +42,7 @@ import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
 import org.elasticsearch.index.fielddata.MultiGeoPointValues;
 import org.elasticsearch.index.fielddata.NumericDoubleValues;
 import org.elasticsearch.index.fielddata.SortedNumericDoubleValues;
+import org.elasticsearch.index.mapper.FieldMapper;
 import org.elasticsearch.index.mapper.MappedFieldType;
 import org.elasticsearch.index.mapper.object.ObjectMapper;
 import org.elasticsearch.index.query.support.NestedInnerQueryParseSupport;
@@ -155,7 +156,7 @@ public class GeoDistanceSortParser implements SortParser {
             ObjectMapper objectMapper = context.mapperService().resolveClosestNestedObjectMapper(fieldName);
             if (objectMapper != null && objectMapper.nested().isNested()) {
                 if (nestedHelper == null) {
-                    nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getShardContext());
+                    nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getParseContext());
                 }
                 nestedHelper.setPath(objectMapper.fullPath());
             }
@@ -163,7 +164,7 @@ public class GeoDistanceSortParser implements SortParser {
 
         final Nested nested;
         if (nestedHelper != null && nestedHelper.getPath() != null) {
-
+            
             BitDocIdSetFilter rootDocumentsFilter = context.bitsetFilterCache().getBitDocIdSetFilter(Queries.newNonNestedFilter());
             Filter innerDocumentsFilter;
             if (nestedHelper.filterFound()) {
diff --git a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
index d248514..39da982 100644
--- a/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
+++ b/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java
@@ -244,7 +244,7 @@ public class SortParseElement implements SearchParseElement {
                     ObjectMapper objectMapper = context.mapperService().resolveClosestNestedObjectMapper(fieldName);
                     if (objectMapper != null && objectMapper.nested().isNested()) {
                         if (nestedHelper == null) {
-                            nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getShardContext());
+                            nestedHelper = new NestedInnerQueryParseSupport(context.queryParserService().getParseContext());
                         }
                         nestedHelper.setPath(objectMapper.fullPath());
                     }
diff --git a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
index d88d038..d8c6fdc 100644
--- a/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
+++ b/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java
@@ -392,7 +392,7 @@ public class RestoreService extends AbstractComponent implements ClusterStateLis
                             Settings.Builder persistentSettings = Settings.settingsBuilder().put();
                             for (Map.Entry<String, String> entry : metaData.persistentSettings().getAsMap().entrySet()) {
                                 if (dynamicSettings.isDynamicOrLoggingSetting(entry.getKey())) {
-                                    String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue());
+                                    String error = dynamicSettings.validateDynamicSetting(entry.getKey(), entry.getValue(), clusterService.state());
                                     if (error == null) {
                                         persistentSettings.put(entry.getKey(), entry.getValue());
                                         changed = true;
diff --git a/core/src/main/java/org/elasticsearch/transport/TransportService.java b/core/src/main/java/org/elasticsearch/transport/TransportService.java
index b4ff29b..8159999 100644
--- a/core/src/main/java/org/elasticsearch/transport/TransportService.java
+++ b/core/src/main/java/org/elasticsearch/transport/TransportService.java
@@ -130,9 +130,7 @@ public class TransportService extends AbstractLifecycleComponent<TransportServic
 
     // These need to be optional as they don't exist in the context of a transport client
     @Inject(optional = true)
-    public void setDynamicSettings(NodeSettingsService nodeSettingsService, @ClusterDynamicSettings DynamicSettings dynamicSettings) {
-        dynamicSettings.addDynamicSettings(SETTING_TRACE_LOG_INCLUDE, SETTING_TRACE_LOG_INCLUDE + ".*");
-        dynamicSettings.addDynamicSettings(SETTING_TRACE_LOG_EXCLUDE, SETTING_TRACE_LOG_EXCLUDE + ".*");
+    public void setDynamicSettings(NodeSettingsService nodeSettingsService) {
         nodeSettingsService.addListener(settingsListener);
     }
 
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java b/core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java
index df860ec..3bf4fa6 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java
@@ -25,8 +25,6 @@ import org.elasticsearch.common.component.Lifecycle;
 import org.elasticsearch.common.compress.Compressor;
 import org.elasticsearch.common.compress.CompressorFactory;
 import org.elasticsearch.common.compress.NotCompressedException;
-import org.elasticsearch.common.io.stream.FilterStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
 import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.logging.ESLogger;
@@ -52,19 +50,13 @@ public class MessageChannelHandler extends SimpleChannelUpstreamHandler {
     protected final TransportServiceAdapter transportServiceAdapter;
     protected final NettyTransport transport;
     protected final String profileName;
-    private final NamedWriteableRegistry namedWriteableRegistry;
 
     public MessageChannelHandler(NettyTransport transport, ESLogger logger, String profileName) {
-        this(transport, logger, profileName, new NamedWriteableRegistry());
-    }
-
-    public MessageChannelHandler(NettyTransport transport, ESLogger logger, String profileName, NamedWriteableRegistry namedWriteableRegistry) {
         this.threadPool = transport.threadPool();
         this.transportServiceAdapter = transport.transportServiceAdapter();
         this.transport = transport;
         this.logger = logger;
         this.profileName = profileName;
-        this.namedWriteableRegistry = namedWriteableRegistry;
     }
 
     @Override
diff --git a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
index 7bb7322..0b85bb6 100644
--- a/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
+++ b/core/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java
@@ -1001,7 +1001,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
     }
 
     public ChannelPipelineFactory configureServerChannelPipelineFactory(String name, Settings settings) {
-        return new ServerChannelPipelineFactory(this, name, settings, namedWriteableRegistry);
+        return new ServerChannelPipelineFactory(this, name, settings);
     }
 
     protected static class ServerChannelPipelineFactory implements ChannelPipelineFactory {
@@ -1009,13 +1009,11 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
         protected final NettyTransport nettyTransport;
         protected final String name;
         protected final Settings settings;
-        protected final NamedWriteableRegistry namedWriteableRegistry;
 
-        public ServerChannelPipelineFactory(NettyTransport nettyTransport, String name, Settings settings, NamedWriteableRegistry namedWriteableRegistry) {
+        public ServerChannelPipelineFactory(NettyTransport nettyTransport, String name, Settings settings) {
             this.nettyTransport = nettyTransport;
             this.name = name;
             this.settings = settings;
-            this.namedWriteableRegistry = namedWriteableRegistry;
         }
 
         @Override
@@ -1034,7 +1032,7 @@ public class NettyTransport extends AbstractLifecycleComponent<Transport> implem
                 sizeHeader.setMaxCumulationBufferComponents(nettyTransport.maxCompositeBufferComponents);
             }
             channelPipeline.addLast("size", sizeHeader);
-            channelPipeline.addLast("dispatcher", new MessageChannelHandler(nettyTransport, nettyTransport.logger, name, namedWriteableRegistry));
+            channelPipeline.addLast("dispatcher", new MessageChannelHandler(nettyTransport, nettyTransport.logger, name));
             return channelPipeline;
         }
     }
diff --git a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
index 207f131..26fbf6a 100644
--- a/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
+++ b/core/src/main/resources/org/elasticsearch/plugins/plugin-install.help
@@ -43,6 +43,7 @@ OFFICIAL PLUGINS
     - elasticsearch-delete-by-query
     - elasticsearch-lang-javascript
     - elasticsearch-lang-python
+    - elasticsearch-mapper-size
 
 
 OPTIONS
diff --git a/core/src/test/java/org/elasticsearch/ESExceptionTests.java b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
index 1439ada..6be1b7c 100644
--- a/core/src/test/java/org/elasticsearch/ESExceptionTests.java
+++ b/core/src/test/java/org/elasticsearch/ESExceptionTests.java
@@ -35,7 +35,6 @@ import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentLocation;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.IndexNotFoundException;
-import org.elasticsearch.index.query.QueryShardException;
 import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.TestQueryParsingException;
 import org.elasticsearch.rest.RestStatus;
@@ -310,7 +309,7 @@ public class ESExceptionTests extends ESTestCase {
                 new OutOfMemoryError("no memory left"),
                 new AlreadyClosedException("closed!!", new NullPointerException()),
                 new LockObtainFailedException("can't lock directory", new NullPointerException()),
-                new Throwable("this exception is unknown", new QueryShardException(new Index("foo"), 1, 2, "foobar", null) ), // somethin unknown
+                new Throwable("this exception is unknown", new QueryParsingException(new Index("foo"), 1, 2, "foobar", null) ), // somethin unknown
         };
         for (Throwable t : causes) {
             BytesStreamOutput out = new BytesStreamOutput();
diff --git a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
index fd8cbec..034d31c 100644
--- a/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
+++ b/core/src/test/java/org/elasticsearch/ExceptionSerializationTests.java
@@ -22,7 +22,6 @@ import com.fasterxml.jackson.core.JsonLocation;
 import com.fasterxml.jackson.core.JsonParseException;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Sets;
-
 import org.codehaus.groovy.runtime.typehandling.GroovyCastException;
 import org.elasticsearch.action.FailedNodeException;
 import org.elasticsearch.action.RoutingMissingException;
@@ -32,24 +31,13 @@ import org.elasticsearch.action.search.ShardSearchFailure;
 import org.elasticsearch.cluster.block.ClusterBlockException;
 import org.elasticsearch.cluster.metadata.SnapshotId;
 import org.elasticsearch.cluster.node.DiscoveryNode;
-import org.elasticsearch.cluster.routing.IllegalShardRoutingStateException;
-import org.elasticsearch.cluster.routing.RoutingTableValidation;
-import org.elasticsearch.cluster.routing.RoutingValidationException;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.ShardRoutingState;
-import org.elasticsearch.cluster.routing.TestShardRouting;
+import org.elasticsearch.cluster.routing.*;
 import org.elasticsearch.common.breaker.CircuitBreakingException;
 import org.elasticsearch.common.io.PathUtils;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.io.stream.StreamOutput;
+import org.elasticsearch.common.io.stream.*;
 import org.elasticsearch.common.transport.LocalTransportAddress;
 import org.elasticsearch.common.unit.ByteSizeValue;
-import org.elasticsearch.common.xcontent.ToXContent;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentLocation;
+import org.elasticsearch.common.xcontent.*;
 import org.elasticsearch.discovery.DiscoverySettings;
 import org.elasticsearch.index.AlreadyExpiredException;
 import org.elasticsearch.index.Index;
@@ -58,10 +46,7 @@ import org.elasticsearch.index.engine.IndexFailedEngineException;
 import org.elasticsearch.index.engine.RecoveryEngineException;
 import org.elasticsearch.index.mapper.MergeMappingException;
 import org.elasticsearch.index.query.QueryParsingException;
-import org.elasticsearch.index.shard.IllegalIndexShardStateException;
-import org.elasticsearch.index.shard.IndexShardState;
-import org.elasticsearch.index.shard.ShardId;
-import org.elasticsearch.index.shard.TranslogRecoveryPerformer;
+import org.elasticsearch.index.shard.*;
 import org.elasticsearch.indices.IndexTemplateAlreadyExistsException;
 import org.elasticsearch.indices.IndexTemplateMissingException;
 import org.elasticsearch.indices.InvalidIndexTemplateException;
diff --git a/core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java b/core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java
new file mode 100644
index 0000000..197a6b3
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.action.admin.indices.template.put;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import org.elasticsearch.Version;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
+import org.elasticsearch.cluster.metadata.IndexTemplateFilter;
+import org.elasticsearch.cluster.metadata.MetaDataCreateIndexService;
+import org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService;
+import org.elasticsearch.cluster.metadata.MetaDataIndexTemplateService.PutRequest;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.indices.InvalidIndexTemplateException;
+import org.elasticsearch.test.ESTestCase;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+public class MetaDataIndexTemplateServiceTests extends ESTestCase {
+    @Test
+    public void testIndexTemplateInvalidNumberOfShards() throws IOException {
+        MetaDataCreateIndexService createIndexService = new MetaDataCreateIndexService(
+                Settings.EMPTY,
+                null,
+                null,
+                null,
+                null,
+                null,
+                Version.CURRENT,
+                null,
+                Sets.<IndexTemplateFilter>newHashSet(),
+                null,
+                null
+        );
+        MetaDataIndexTemplateService service = new MetaDataIndexTemplateService(Settings.EMPTY, null, createIndexService, null);
+
+        PutRequest request = new PutRequest("test", "test_shards");
+        request.template("test_shards*");
+
+        Map<String, Object> map = Maps.newHashMap();
+        map.put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, "0");
+        request.settings(Settings.settingsBuilder().put(map).build());
+
+        final List<Throwable> throwables = Lists.newArrayList();
+        service.putTemplate(request, new MetaDataIndexTemplateService.PutListener() {
+            @Override
+            public void onResponse(MetaDataIndexTemplateService.PutResponse response) {
+
+            }
+
+            @Override
+            public void onFailure(Throwable t) {
+                throwables.add(t);
+            }
+        });
+        assertEquals(throwables.size(), 1);
+        assertTrue(throwables.get(0) instanceof InvalidIndexTemplateException);
+        assertTrue(throwables.get(0).getMessage().contains("index must have 1 or more primary shards"));
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
index 80f4c45..8f8759e 100644
--- a/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
+++ b/core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java
@@ -151,7 +151,7 @@ public class IndexAliasesIT extends ESIntegTestCase {
         logger.info("--> making sure that filter was stored with alias [alias1] and filter [user:kimchy]");
         ClusterState clusterState = admin().cluster().prepareState().get().getState();
         IndexMetaData indexMd = clusterState.metaData().index("test");
-        assertThat(indexMd.aliases().get("alias1").filter().string(), equalTo("{\"term\":{\"user\":{\"value\":\"kimchy\",\"boost\":1.0}}}"));
+        assertThat(indexMd.aliases().get("alias1").filter().string(), equalTo("{\"term\":{\"user\":\"kimchy\"}}"));
 
     }
 
@@ -413,8 +413,8 @@ public class IndexAliasesIT extends ESIntegTestCase {
         assertThat(client().prepareCount("bars").setQuery(QueryBuilders.matchAllQuery()).get().getCount(), equalTo(1L));
     }
 
-
-
+    
+    
     @Test
     public void testDeleteAliases() throws Exception {
         logger.info("--> creating index [test1] and [test2]");
@@ -434,17 +434,17 @@ public class IndexAliasesIT extends ESIntegTestCase {
                 .addAlias("test2", "aliasToTests")
                 .addAlias("test2", "foos", termQuery("name", "foo"))
                 .addAlias("test2", "tests", termQuery("name", "test")));
-
-        String[] indices = {"test1", "test2"};
+        
+        String[] indices = {"test1", "test2"}; 
         String[] aliases = {"aliasToTest1", "foos", "bars", "tests", "aliasToTest2", "aliasToTests"};
-
+        
         admin().indices().prepareAliases().removeAlias(indices, aliases).get();
-
+        
         AliasesExistResponse response = admin().indices().prepareAliasesExist(aliases).get();
         assertThat(response.exists(), equalTo(false));
     }
 
-
+    
     @Test
     public void testWaitForAliasCreationMultipleShards() throws Exception {
         logger.info("--> creating index [test]");
@@ -532,16 +532,16 @@ public class IndexAliasesIT extends ESIntegTestCase {
 
         logger.info("--> verify that filter was updated");
         AliasMetaData aliasMetaData = ((AliasOrIndex.Alias) internalCluster().clusterService().state().metaData().getAliasAndIndexLookup().get("alias1")).getFirstAliasMetaData();
-        assertThat(aliasMetaData.getFilter().toString(), equalTo("{\"term\":{\"name\":{\"value\":\"bar\",\"boost\":1.0}}}"));
+        assertThat(aliasMetaData.getFilter().toString(), equalTo("{\"term\":{\"name\":\"bar\"}}"));
 
         logger.info("--> deleting alias1");
         stopWatch.start();
         assertAcked((admin().indices().prepareAliases().removeAlias("test", "alias1").setTimeout(timeout)));
         assertThat(stopWatch.stop().lastTaskTime().millis(), lessThan(timeout.millis()));
 
-
+        
     }
-
+    
     @Test(expected = AliasesNotFoundException.class)
     public void testIndicesRemoveNonExistingAliasResponds404() throws Exception {
         logger.info("--> creating index [test]");
diff --git a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
index b0271e3..7f3ce81 100644
--- a/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
+++ b/core/src/test/java/org/elasticsearch/benchmark/transport/BenchmarkNettyLargeMessages.java
@@ -54,9 +54,6 @@ public class BenchmarkNettyLargeMessages {
                 .build();
 
         NetworkService networkService = new NetworkService(settings);
-        NodeSettingsService settingsService = new NodeSettingsService(settings);
-        DynamicSettings dynamicSettings = new DynamicSettings();
-
 
         final ThreadPool threadPool = new ThreadPool("BenchmarkNettyLargeMessages");
         final TransportService transportServiceServer = new TransportService(
diff --git a/core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java b/core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java
new file mode 100644
index 0000000..06cbe1f
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/cluster/ClusterModuleTests.java
@@ -0,0 +1,175 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.elasticsearch.cluster;
+
+import com.google.common.base.Predicate;
+import org.elasticsearch.action.admin.indices.create.CreateIndexClusterStateUpdateRequest;
+import org.elasticsearch.cluster.metadata.IndexTemplateFilter;
+import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
+import org.elasticsearch.cluster.routing.RoutingNode;
+import org.elasticsearch.cluster.routing.ShardRouting;
+import org.elasticsearch.cluster.routing.allocation.FailedRerouteAllocation;
+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
+import org.elasticsearch.cluster.routing.allocation.StartedRerouteAllocation;
+import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
+import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
+import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
+import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
+import org.elasticsearch.cluster.settings.DynamicSettings;
+import org.elasticsearch.cluster.settings.Validator;
+import org.elasticsearch.common.inject.ModuleTestCase;
+import org.elasticsearch.common.settings.Settings;
+import org.elasticsearch.index.settings.IndexDynamicSettings;
+
+public class ClusterModuleTests extends ModuleTestCase {
+
+    public static class FakeAllocationDecider extends AllocationDecider {
+        protected FakeAllocationDecider(Settings settings) {
+            super(settings);
+        }
+    }
+
+    static class FakeShardsAllocator implements ShardsAllocator {
+        @Override
+        public void applyStartedShards(StartedRerouteAllocation allocation) {}
+        @Override
+        public void applyFailedShards(FailedRerouteAllocation allocation) {}
+        @Override
+        public boolean allocateUnassigned(RoutingAllocation allocation) {
+            return false;
+        }
+        @Override
+        public boolean rebalance(RoutingAllocation allocation) {
+            return false;
+        }
+        @Override
+        public boolean move(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {
+            return false;
+        }
+    }
+
+    static class FakeIndexTemplateFilter implements IndexTemplateFilter {
+        @Override
+        public boolean apply(CreateIndexClusterStateUpdateRequest request, IndexTemplateMetaData template) {
+            return false;
+        }
+    }
+
+    public void testRegisterClusterDynamicSettingDuplicate() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        try {
+            module.registerClusterDynamicSetting(EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE, Validator.EMPTY);
+        } catch (IllegalArgumentException e) {
+            assertEquals(e.getMessage(), "Cannot register setting [" + EnableAllocationDecider.CLUSTER_ROUTING_ALLOCATION_ENABLE + "] twice");
+        }
+    }
+
+    public void testRegisterClusterDynamicSetting() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        module.registerClusterDynamicSetting("foo.bar", Validator.EMPTY);
+        assertInstanceBindingWithAnnotation(module, DynamicSettings.class, new Predicate<DynamicSettings>() {
+            @Override
+            public boolean apply(DynamicSettings dynamicSettings) {
+                return dynamicSettings.hasDynamicSetting("foo.bar");
+            }
+        }, ClusterDynamicSettings.class);
+    }
+
+    public void testRegisterIndexDynamicSettingDuplicate() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        try {
+            module.registerIndexDynamicSetting(EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE, Validator.EMPTY);
+        } catch (IllegalArgumentException e) {
+            assertEquals(e.getMessage(), "Cannot register setting [" + EnableAllocationDecider.INDEX_ROUTING_ALLOCATION_ENABLE + "] twice");
+        }
+    }
+
+    public void testRegisterIndexDynamicSetting() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        module.registerIndexDynamicSetting("foo.bar", Validator.EMPTY);
+        assertInstanceBindingWithAnnotation(module, DynamicSettings.class, new Predicate<DynamicSettings>() {
+            @Override
+            public boolean apply(DynamicSettings dynamicSettings) {
+                return dynamicSettings.hasDynamicSetting("foo.bar");
+            }
+        }, IndexDynamicSettings.class);
+    }
+
+    public void testRegisterAllocationDeciderDuplicate() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        try {
+            module.registerAllocationDecider(EnableAllocationDecider.class);
+        } catch (IllegalArgumentException e) {
+            assertEquals(e.getMessage(), "Can't register the same [allocation_decider] more than once for [" + EnableAllocationDecider.class.getName() + "]");
+        }
+    }
+
+    public void testRegisterAllocationDecider() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        module.registerAllocationDecider(FakeAllocationDecider.class);
+        assertSetMultiBinding(module, AllocationDecider.class, FakeAllocationDecider.class);
+    }
+
+    public void testRegisterShardsAllocator() {
+        Settings settings = Settings.builder().put(ClusterModule.SHARDS_ALLOCATOR_TYPE_KEY, "custom").build();
+        ClusterModule module = new ClusterModule(settings);
+        module.registerShardsAllocator("custom", FakeShardsAllocator.class);
+        assertBinding(module, ShardsAllocator.class, FakeShardsAllocator.class);
+    }
+
+    public void testRegisterShardsAllocatorAlreadyRegistered() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        try {
+            module.registerShardsAllocator(ClusterModule.BALANCED_ALLOCATOR, FakeShardsAllocator.class);
+        } catch (IllegalArgumentException e) {
+            assertEquals(e.getMessage(), "Can't register the same [shards_allocator] more than once for [balanced]");
+        }
+    }
+
+    public void testUnknownShardsAllocator() {
+        Settings settings = Settings.builder().put(ClusterModule.SHARDS_ALLOCATOR_TYPE_KEY, "dne").build();
+        ClusterModule module = new ClusterModule(settings);
+        assertBindingFailure(module, "Unknown [shards_allocator]");
+    }
+
+    public void testEvenShardsAllocatorBackcompat() {
+        Settings settings = Settings.builder()
+            .put(ClusterModule.SHARDS_ALLOCATOR_TYPE_KEY, ClusterModule.EVEN_SHARD_COUNT_ALLOCATOR).build();
+        ClusterModule module = new ClusterModule(settings);
+        assertBinding(module, ShardsAllocator.class, BalancedShardsAllocator.class);
+    }
+
+    public void testRegisterIndexTemplateFilterDuplicate() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        try {
+            module.registerIndexTemplateFilter(FakeIndexTemplateFilter.class);
+            module.registerIndexTemplateFilter(FakeIndexTemplateFilter.class);
+        } catch (IllegalArgumentException e) {
+            assertEquals(e.getMessage(), "Can't register the same [index_template_filter] more than once for [" + FakeIndexTemplateFilter.class.getName() + "]");
+        }
+    }
+
+    public void testRegisterIndexTemplateFilter() {
+        ClusterModule module = new ClusterModule(Settings.EMPTY);
+        module.registerIndexTemplateFilter(FakeIndexTemplateFilter.class);
+        assertSetMultiBinding(module, IndexTemplateFilter.class, FakeIndexTemplateFilter.class);
+    }
+}
diff --git a/core/src/test/java/org/elasticsearch/cluster/allocation/ShardsAllocatorModuleIT.java b/core/src/test/java/org/elasticsearch/cluster/allocation/ShardsAllocatorModuleIT.java
index 75dfac5..1faa82a 100644
--- a/core/src/test/java/org/elasticsearch/cluster/allocation/ShardsAllocatorModuleIT.java
+++ b/core/src/test/java/org/elasticsearch/cluster/allocation/ShardsAllocatorModuleIT.java
@@ -19,7 +19,7 @@
 
 package org.elasticsearch.cluster.allocation;
 
-import org.elasticsearch.cluster.routing.allocation.AllocationModule;
+import org.elasticsearch.cluster.ClusterModule;
 import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
 import org.elasticsearch.common.settings.Settings;
@@ -40,10 +40,10 @@ public class ShardsAllocatorModuleIT extends ESIntegTestCase {
     }
 
     public void testLoadByShortKeyShardsAllocator() throws IOException {
-        Settings build = settingsBuilder().put(AllocationModule.SHARDS_ALLOCATOR_TYPE_KEY, "even_shard") // legacy just to make sure we don't barf
+        Settings build = settingsBuilder().put(ClusterModule.SHARDS_ALLOCATOR_TYPE_KEY, "even_shard") // legacy just to make sure we don't barf
                 .build();
         assertAllocatorInstance(build, BalancedShardsAllocator.class);
-        build = settingsBuilder().put(AllocationModule.SHARDS_ALLOCATOR_TYPE_KEY, AllocationModule.BALANCED_ALLOCATOR).build();
+        build = settingsBuilder().put(ClusterModule.SHARDS_ALLOCATOR_TYPE_KEY, ClusterModule.BALANCED_ALLOCATOR).build();
         assertAllocatorInstance(build, BalancedShardsAllocator.class);
     }
 
diff --git a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationModuleTests.java b/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationModuleTests.java
deleted file mode 100644
index 7b57ef0..0000000
--- a/core/src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationModuleTests.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-package org.elasticsearch.cluster.routing.allocation;
-
-import org.elasticsearch.cluster.routing.RoutingNode;
-import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator;
-import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocator;
-import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
-import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;
-import org.elasticsearch.common.inject.ModuleTestCase;
-import org.elasticsearch.common.settings.Settings;
-
-public class AllocationModuleTests extends ModuleTestCase {
-
-    public static class FakeAllocationDecider extends AllocationDecider {
-        protected FakeAllocationDecider(Settings settings) {
-            super(settings);
-        }
-    }
-
-    public static class FakeShardsAllocator implements ShardsAllocator {
-        @Override
-        public void applyStartedShards(StartedRerouteAllocation allocation) {}
-        @Override
-        public void applyFailedShards(FailedRerouteAllocation allocation) {}
-        @Override
-        public boolean allocateUnassigned(RoutingAllocation allocation) {
-            return false;
-        }
-        @Override
-        public boolean rebalance(RoutingAllocation allocation) {
-            return false;
-        }
-        @Override
-        public boolean move(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {
-            return false;
-        }
-    }
-
-    public void testRegisterAllocationDeciderDuplicate() {
-        AllocationModule module = new AllocationModule(Settings.EMPTY);
-        try {
-            module.registerAllocationDecider(EnableAllocationDecider.class);
-        } catch (IllegalArgumentException e) {
-            assertEquals(e.getMessage(), "Can't register the same [allocation_decider] more than once for [" + EnableAllocationDecider.class.getName() + "]");
-        }
-    }
-
-    public void testRegisterAllocationDecider() {
-        AllocationModule module = new AllocationModule(Settings.EMPTY);
-        module.registerAllocationDecider(FakeAllocationDecider.class);
-        assertSetMultiBinding(module, AllocationDecider.class, FakeAllocationDecider.class);
-    }
-
-    public void testRegisterShardsAllocator() {
-        Settings settings = Settings.builder().put(AllocationModule.SHARDS_ALLOCATOR_TYPE_KEY, "custom").build();
-        AllocationModule module = new AllocationModule(settings);
-        module.registerShardAllocator("custom", FakeShardsAllocator.class);
-        assertBinding(module, ShardsAllocator.class, FakeShardsAllocator.class);
-    }
-
-    public void testRegisterShardsAllocatorAlreadyRegistered() {
-        AllocationModule module = new AllocationModule(Settings.EMPTY);
-        try {
-            module.registerShardAllocator(AllocationModule.BALANCED_ALLOCATOR, FakeShardsAllocator.class);
-        } catch (IllegalArgumentException e) {
-            assertEquals(e.getMessage(), "Can't register the same [shards_allocator] more than once for [balanced]");
-        }
-    }
-
-    public void testUnknownShardsAllocator() {
-        Settings settings = Settings.builder().put(AllocationModule.SHARDS_ALLOCATOR_TYPE_KEY, "dne").build();
-        AllocationModule module = new AllocationModule(settings);
-        assertBindingFailure(module, "Unknown [shards_allocator]");
-    }
-
-    public void testEvenShardsAllocatorBackcompat() {
-        Settings settings = Settings.builder()
-            .put(AllocationModule.SHARDS_ALLOCATOR_TYPE_KEY, AllocationModule.EVEN_SHARD_COUNT_ALLOCATOR).build();
-        AllocationModule module = new AllocationModule(settings);
-        assertBinding(module, ShardsAllocator.class, BalancedShardsAllocator.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/cluster/settings/SettingsValidatorTests.java b/core/src/test/java/org/elasticsearch/cluster/settings/SettingsValidatorTests.java
index fdb54cb..1e041aa 100644
--- a/core/src/test/java/org/elasticsearch/cluster/settings/SettingsValidatorTests.java
+++ b/core/src/test/java/org/elasticsearch/cluster/settings/SettingsValidatorTests.java
@@ -28,81 +28,81 @@ public class SettingsValidatorTests extends ESTestCase {
 
     @Test
     public void testValidators() throws Exception {
-        assertThat(Validator.EMPTY.validate("", "anything goes"), nullValue());
-
-        assertThat(Validator.TIME.validate("", "10m"), nullValue());
-        assertThat(Validator.TIME.validate("", "10g"), notNullValue());
-        assertThat(Validator.TIME.validate("", "bad timing"), notNullValue());
-
-        assertThat(Validator.BYTES_SIZE.validate("", "10m"), nullValue());
-        assertThat(Validator.BYTES_SIZE.validate("", "10g"), nullValue());
-        assertThat(Validator.BYTES_SIZE.validate("", "bad"), notNullValue());
-
-        assertThat(Validator.FLOAT.validate("", "10.2"), nullValue());
-        assertThat(Validator.FLOAT.validate("", "10.2.3"), notNullValue());
-
-        assertThat(Validator.NON_NEGATIVE_FLOAT.validate("", "10.2"), nullValue());
-        assertThat(Validator.NON_NEGATIVE_FLOAT.validate("", "0.0"), nullValue());
-        assertThat(Validator.NON_NEGATIVE_FLOAT.validate("", "-1.0"), notNullValue());
-        assertThat(Validator.NON_NEGATIVE_FLOAT.validate("", "10.2.3"), notNullValue());
-
-        assertThat(Validator.DOUBLE.validate("", "10.2"), nullValue());
-        assertThat(Validator.DOUBLE.validate("", "10.2.3"), notNullValue());
-
-        assertThat(Validator.DOUBLE_GTE_2.validate("", "10.2"), nullValue());
-        assertThat(Validator.DOUBLE_GTE_2.validate("", "2.0"), nullValue());
-        assertThat(Validator.DOUBLE_GTE_2.validate("", "1.0"), notNullValue());
-        assertThat(Validator.DOUBLE_GTE_2.validate("", "10.2.3"), notNullValue());
-
-        assertThat(Validator.NON_NEGATIVE_DOUBLE.validate("", "10.2"), nullValue());
-        assertThat(Validator.NON_NEGATIVE_DOUBLE.validate("", "0.0"), nullValue());
-        assertThat(Validator.NON_NEGATIVE_DOUBLE.validate("", "-1.0"), notNullValue());
-        assertThat(Validator.NON_NEGATIVE_DOUBLE.validate("", "10.2.3"), notNullValue());
-
-        assertThat(Validator.INTEGER.validate("", "10"), nullValue());
-        assertThat(Validator.INTEGER.validate("", "10.2"), notNullValue());
-
-        assertThat(Validator.INTEGER_GTE_2.validate("", "2"), nullValue());
-        assertThat(Validator.INTEGER_GTE_2.validate("", "1"), notNullValue());
-        assertThat(Validator.INTEGER_GTE_2.validate("", "0"), notNullValue());
-        assertThat(Validator.INTEGER_GTE_2.validate("", "10.2.3"), notNullValue());
-
-        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "2"), nullValue());
-        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "1"), nullValue());
-        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "0"), nullValue());
-        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "-1"), notNullValue());
-        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "10.2"), notNullValue());
-
-        assertThat(Validator.POSITIVE_INTEGER.validate("", "2"), nullValue());
-        assertThat(Validator.POSITIVE_INTEGER.validate("", "1"), nullValue());
-        assertThat(Validator.POSITIVE_INTEGER.validate("", "0"), notNullValue());
-        assertThat(Validator.POSITIVE_INTEGER.validate("", "-1"), notNullValue());
-        assertThat(Validator.POSITIVE_INTEGER.validate("", "10.2"), notNullValue());
-
-        assertThat(Validator.PERCENTAGE.validate("", "asdasd"), notNullValue());
-        assertThat(Validator.PERCENTAGE.validate("", "-1"), notNullValue());
-        assertThat(Validator.PERCENTAGE.validate("", "20"), notNullValue());
-        assertThat(Validator.PERCENTAGE.validate("", "-1%"), notNullValue());
-        assertThat(Validator.PERCENTAGE.validate("", "101%"), notNullValue());
-        assertThat(Validator.PERCENTAGE.validate("", "100%"), nullValue());
-        assertThat(Validator.PERCENTAGE.validate("", "99%"), nullValue());
-        assertThat(Validator.PERCENTAGE.validate("", "0%"), nullValue());
-
-        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "asdasd"), notNullValue());
-        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "20"), notNullValue());
-        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "20mb"), nullValue());
-        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "-1%"), notNullValue());
-        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "101%"), notNullValue());
-        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "100%"), nullValue());
-        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "99%"), nullValue());
-        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "0%"), nullValue());
+        assertThat(Validator.EMPTY.validate("", "anything goes", null), nullValue());
+
+        assertThat(Validator.TIME.validate("", "10m", null), nullValue());
+        assertThat(Validator.TIME.validate("", "10g", null), notNullValue());
+        assertThat(Validator.TIME.validate("", "bad timing", null), notNullValue());
+
+        assertThat(Validator.BYTES_SIZE.validate("", "10m", null), nullValue());
+        assertThat(Validator.BYTES_SIZE.validate("", "10g", null), nullValue());
+        assertThat(Validator.BYTES_SIZE.validate("", "bad", null), notNullValue());
+
+        assertThat(Validator.FLOAT.validate("", "10.2", null), nullValue());
+        assertThat(Validator.FLOAT.validate("", "10.2.3", null), notNullValue());
+
+        assertThat(Validator.NON_NEGATIVE_FLOAT.validate("", "10.2", null), nullValue());
+        assertThat(Validator.NON_NEGATIVE_FLOAT.validate("", "0.0", null), nullValue());
+        assertThat(Validator.NON_NEGATIVE_FLOAT.validate("", "-1.0", null), notNullValue());
+        assertThat(Validator.NON_NEGATIVE_FLOAT.validate("", "10.2.3", null), notNullValue());
+
+        assertThat(Validator.DOUBLE.validate("", "10.2", null), nullValue());
+        assertThat(Validator.DOUBLE.validate("", "10.2.3", null), notNullValue());
+
+        assertThat(Validator.DOUBLE_GTE_2.validate("", "10.2", null), nullValue());
+        assertThat(Validator.DOUBLE_GTE_2.validate("", "2.0", null), nullValue());
+        assertThat(Validator.DOUBLE_GTE_2.validate("", "1.0", null), notNullValue());
+        assertThat(Validator.DOUBLE_GTE_2.validate("", "10.2.3", null), notNullValue());
+
+        assertThat(Validator.NON_NEGATIVE_DOUBLE.validate("", "10.2", null), nullValue());
+        assertThat(Validator.NON_NEGATIVE_DOUBLE.validate("", "0.0", null), nullValue());
+        assertThat(Validator.NON_NEGATIVE_DOUBLE.validate("", "-1.0", null), notNullValue());
+        assertThat(Validator.NON_NEGATIVE_DOUBLE.validate("", "10.2.3", null), notNullValue());
+
+        assertThat(Validator.INTEGER.validate("", "10", null), nullValue());
+        assertThat(Validator.INTEGER.validate("", "10.2", null), notNullValue());
+
+        assertThat(Validator.INTEGER_GTE_2.validate("", "2", null), nullValue());
+        assertThat(Validator.INTEGER_GTE_2.validate("", "1", null), notNullValue());
+        assertThat(Validator.INTEGER_GTE_2.validate("", "0", null), notNullValue());
+        assertThat(Validator.INTEGER_GTE_2.validate("", "10.2.3", null), notNullValue());
+
+        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "2", null), nullValue());
+        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "1", null), nullValue());
+        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "0", null), nullValue());
+        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "-1", null), notNullValue());
+        assertThat(Validator.NON_NEGATIVE_INTEGER.validate("", "10.2", null), notNullValue());
+
+        assertThat(Validator.POSITIVE_INTEGER.validate("", "2", null), nullValue());
+        assertThat(Validator.POSITIVE_INTEGER.validate("", "1", null), nullValue());
+        assertThat(Validator.POSITIVE_INTEGER.validate("", "0", null), notNullValue());
+        assertThat(Validator.POSITIVE_INTEGER.validate("", "-1", null), notNullValue());
+        assertThat(Validator.POSITIVE_INTEGER.validate("", "10.2", null), notNullValue());
+
+        assertThat(Validator.PERCENTAGE.validate("", "asdasd", null), notNullValue());
+        assertThat(Validator.PERCENTAGE.validate("", "-1", null), notNullValue());
+        assertThat(Validator.PERCENTAGE.validate("", "20", null), notNullValue());
+        assertThat(Validator.PERCENTAGE.validate("", "-1%", null), notNullValue());
+        assertThat(Validator.PERCENTAGE.validate("", "101%", null), notNullValue());
+        assertThat(Validator.PERCENTAGE.validate("", "100%", null), nullValue());
+        assertThat(Validator.PERCENTAGE.validate("", "99%", null), nullValue());
+        assertThat(Validator.PERCENTAGE.validate("", "0%", null), nullValue());
+
+        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "asdasd", null), notNullValue());
+        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "20", null), notNullValue());
+        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "20mb", null), nullValue());
+        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "-1%", null), notNullValue());
+        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "101%", null), notNullValue());
+        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "100%", null), nullValue());
+        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "99%", null), nullValue());
+        assertThat(Validator.BYTES_SIZE_OR_PERCENTAGE.validate("", "0%", null), nullValue());
     }
 
     @Test
     public void testDynamicValidators() throws Exception {
-        DynamicSettings ds = new DynamicSettings();
-        ds.addDynamicSetting("my.test.*", Validator.POSITIVE_INTEGER);
-        String valid = ds.validateDynamicSetting("my.test.setting", "-1");
+        DynamicSettings.Builder ds = new DynamicSettings.Builder();
+        ds.addSetting("my.test.*", Validator.POSITIVE_INTEGER);
+        String valid = ds.build().validateDynamicSetting("my.test.setting", "-1", null);
         assertThat(valid, equalTo("the value of the setting my.test.setting must be a positive integer"));
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
index 60c3ca1..323b9f5 100644
--- a/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
+++ b/core/src/test/java/org/elasticsearch/common/inject/ModuleTestCase.java
@@ -18,12 +18,15 @@
  */
 package org.elasticsearch.common.inject;
 
+import com.google.common.base.Predicate;
 import org.elasticsearch.common.inject.spi.Element;
 import org.elasticsearch.common.inject.spi.Elements;
+import org.elasticsearch.common.inject.spi.InstanceBinding;
 import org.elasticsearch.common.inject.spi.LinkedKeyBinding;
 import org.elasticsearch.common.inject.spi.ProviderInstanceBinding;
 import org.elasticsearch.test.ESTestCase;
 
+import java.lang.annotation.Annotation;
 import java.lang.reflect.Type;
 import java.util.HashSet;
 import java.util.List;
@@ -135,5 +138,27 @@ public abstract class ModuleTestCase extends ESTestCase {
         assertTrue("Did not find provider for set of " + to.getName(), providerFound);
     }
 
-    // TODO: add assert for map multibinding
+    public <T> void assertInstanceBinding(Module module, Class<T> to, Predicate<T> tester) {
+        assertInstanceBindingWithAnnotation(module, to, tester, null);
+    }
+
+    public <T> void assertInstanceBindingWithAnnotation(Module module, Class<T> to, Predicate<T> tester, Class<? extends Annotation> annotation) {
+        List<Element> elements = Elements.getElements(module);
+        for (Element element : elements) {
+            if (element instanceof InstanceBinding) {
+                InstanceBinding binding = (InstanceBinding) element;
+                if (to.equals(binding.getKey().getTypeLiteral().getType())) {
+                    if (annotation == null || annotation.equals(binding.getKey().getAnnotationType())) {
+                        assertTrue(tester.apply(to.cast(binding.getInstance())));
+                        return;
+                    }
+                }
+            }
+        }
+        StringBuilder s = new StringBuilder();
+        for (Element element : elements) {
+            s.append(element + "\n");
+        }
+        fail("Did not find any instance binding to " + to.getName() + ". Found these bindings:\n" + s);
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
index afc17ce..d313dd7 100644
--- a/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
+++ b/core/src/test/java/org/elasticsearch/common/io/stream/BytesStreamsTests.java
@@ -26,7 +26,6 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
-
 import java.util.Objects;
 
 import static org.hamcrest.Matchers.closeTo;
diff --git a/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java b/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
index 807b4a7..234e341 100644
--- a/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
+++ b/core/src/test/java/org/elasticsearch/common/unit/FuzzinessTests.java
@@ -18,8 +18,6 @@
  */
 package org.elasticsearch.common.unit;
 
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.xcontent.XContent;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.common.xcontent.XContentType;
@@ -164,29 +162,4 @@ public class FuzzinessTests extends ESTestCase {
         }
     }
 
-    @Test
-    public void testSerialization() throws IOException {
-        Fuzziness fuzziness = Fuzziness.AUTO;
-        Fuzziness deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-
-        fuzziness = Fuzziness.fromEdits(randomIntBetween(0, 2));
-        deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-    }
-
-    @Test
-    public void testSerializationAuto() throws IOException {
-        Fuzziness fuzziness = Fuzziness.AUTO;
-        Fuzziness deserializedFuzziness = doSerializeRoundtrip(fuzziness);
-        assertEquals(fuzziness, deserializedFuzziness);
-        assertEquals(fuzziness.asInt(), deserializedFuzziness.asInt());
-    }
-
-    private static Fuzziness doSerializeRoundtrip(Fuzziness in) throws IOException {
-        BytesStreamOutput output = new BytesStreamOutput();
-        in.writeTo(output);
-        StreamInput streamInput = StreamInput.wrap(output.bytes());
-        return Fuzziness.readFuzzinessFrom(streamInput);
-    }
 }
diff --git a/core/src/test/java/org/elasticsearch/env/EnvironmentTests.java b/core/src/test/java/org/elasticsearch/env/EnvironmentTests.java
index 8535f0f..271cd1d 100644
--- a/core/src/test/java/org/elasticsearch/env/EnvironmentTests.java
+++ b/core/src/test/java/org/elasticsearch/env/EnvironmentTests.java
@@ -52,28 +52,6 @@ public class EnvironmentTests extends ESTestCase {
     }
 
     @Test
-    public void testResolveJaredResource() throws IOException {
-        Environment environment = newEnvironment();
-        URL url = environment.resolveConfig("META-INF/MANIFEST.MF"); // this works because there is one jar having this file in the classpath
-        assertNotNull(url);
-        try (BufferedReader reader = FileSystemUtils.newBufferedReader(url, Charsets.UTF_8)) {
-            String string = Streams.copyToString(reader);
-            assertTrue(string, string.contains("Manifest-Version"));
-        }
-    }
-
-    @Test
-    public void testResolveFileResource() throws IOException {
-        Environment environment = newEnvironment();
-        URL url = environment.resolveConfig("org/elasticsearch/common/cli/tool.help");
-        assertNotNull(url);
-        try (BufferedReader reader = FileSystemUtils.newBufferedReader(url, Charsets.UTF_8)) {
-            String string = Streams.copyToString(reader);
-            assertEquals(string, "tool help");
-        }
-    }
-
-    @Test
     public void testRepositoryResolution() throws IOException {
         Environment environment = newEnvironment();
         assertThat(environment.resolveRepoFile("/test/repos/repo1"), nullValue());
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java
index 33cb31f..df382b7 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/KeepFilterFactoryTests.java
@@ -22,7 +22,7 @@ package org.elasticsearch.index.analysis;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.FailedToResolveConfigException;
+import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.test.ESTokenStreamTestCase;
 import org.junit.Assert;
 import org.junit.Test;
@@ -72,7 +72,7 @@ public class KeepFilterFactoryTests extends ESTokenStreamTestCase {
             AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
             fail("expected an exception due to non existent keep_words_path");
         } catch (Throwable e) {
-            assertThat(e.getCause(), instanceOf(FailedToResolveConfigException.class));
+            assertThat(e.getCause(), instanceOf(IllegalArgumentException.class));
         }
 
         settings = Settings.settingsBuilder().put(settings)
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java b/core/src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java
index 7391b04..c1bb7f8 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/commongrams/CommonGramsTokenFilterFactoryTests.java
@@ -22,6 +22,7 @@ package org.elasticsearch.index.analysis.commongrams;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
+import org.elasticsearch.common.io.FileSystemUtils;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.index.analysis.AnalysisService;
 import org.elasticsearch.index.analysis.AnalysisTestsHelper;
@@ -31,7 +32,10 @@ import org.junit.Assert;
 import org.junit.Test;
 
 import java.io.IOException;
+import java.io.InputStream;
 import java.io.StringReader;
+import java.nio.file.Files;
+import java.nio.file.Path;
 
 import static org.hamcrest.Matchers.instanceOf;
 public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
@@ -137,7 +141,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         String json = "/org/elasticsearch/index/analysis/commongrams/commongrams.json";
         Settings settings = Settings.settingsBuilder()
                      .loadFromStream(json, getClass().getResourceAsStream(json))
-                     .put("path.home", createTempDir().toString())
+                     .put("path.home", createHome())
                      .build();
         {
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -222,7 +226,7 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         String json = "/org/elasticsearch/index/analysis/commongrams/commongrams_query_mode.json";
         Settings settings = Settings.settingsBuilder()
                 .loadFromStream(json, getClass().getResourceAsStream(json))
-                .put("path.home", createTempDir().toString())
+            .put("path.home", createHome())
                 .build();
         {
             AnalysisService analysisService = AnalysisTestsHelper.createAnalysisServiceFromSettings(settings);
@@ -240,4 +244,13 @@ public class CommonGramsTokenFilterFactoryTests extends ESTokenStreamTestCase {
         }
     }
 
+    private Path createHome() throws IOException {
+        InputStream words = getClass().getResourceAsStream("common_words.txt");
+        Path home = createTempDir();
+        Path config = home.resolve("config");
+        Files.createDirectory(config);
+        Files.copy(words, config.resolve("common_words.txt"));
+        return home;
+    }
+
 }
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/commongrams/commongrams.json b/core/src/test/java/org/elasticsearch/index/analysis/commongrams/commongrams.json
index 6db49fc..377b403 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/commongrams/commongrams.json
+++ b/core/src/test/java/org/elasticsearch/index/analysis/commongrams/commongrams.json
@@ -21,7 +21,7 @@
                 },
                 "common_grams_file":{
                     "type":"common_grams",
-                    "common_words_path":"org/elasticsearch/index/analysis/commongrams/common_words.txt"
+                    "common_words_path":"common_words.txt"
                 }
             }
         }
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/commongrams/commongrams_query_mode.json b/core/src/test/java/org/elasticsearch/index/analysis/commongrams/commongrams_query_mode.json
index 6f0c015..4151c46 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/commongrams/commongrams_query_mode.json
+++ b/core/src/test/java/org/elasticsearch/index/analysis/commongrams/commongrams_query_mode.json
@@ -23,7 +23,7 @@
                 "common_grams_file":{
                     "type":"common_grams",
                     "query_mode" : true,
-                    "common_words_path":"org/elasticsearch/index/analysis/commongrams/common_words.txt"
+                    "common_words_path":"common_words.txt"
                 }
             }
         }
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java
index 78349a0..e38ab9a 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java
+++ b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java
@@ -46,6 +46,9 @@ import org.hamcrest.MatcherAssert;
 import org.junit.Test;
 
 import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.Path;
 
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.hamcrest.Matchers.equalTo;
@@ -59,10 +62,18 @@ public class SynonymsAnalysisTest extends ESTestCase {
 
     @Test
     public void testSynonymsAnalysis() throws IOException {
+        InputStream synonyms = getClass().getResourceAsStream("synonyms.txt");
+        InputStream synonymsWordnet = getClass().getResourceAsStream("synonyms_wordnet.txt");
+        Path home = createTempDir();
+        Path config = home.resolve("config");
+        Files.createDirectory(config);
+        Files.copy(synonyms, config.resolve("synonyms.txt"));
+        Files.copy(synonymsWordnet, config.resolve("synonyms_wordnet.txt"));
+
         String json = "/org/elasticsearch/index/analysis/synonyms/synonyms.json";
         Settings settings = settingsBuilder().
-                loadFromStream(json, getClass().getResourceAsStream(json))
-                .put("path.home", createTempDir().toString())
+            loadFromStream(json, getClass().getResourceAsStream(json))
+                .put("path.home", home)
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).build();
 
         Index index = new Index("test");
diff --git a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/synonyms.json b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/synonyms.json
index 84898af..fe5f4d4 100644
--- a/core/src/test/java/org/elasticsearch/index/analysis/synonyms/synonyms.json
+++ b/core/src/test/java/org/elasticsearch/index/analysis/synonyms/synonyms.json
@@ -41,7 +41,7 @@
                 },
                 "synonym_file":{
                     "type":"synonym",
-                    "synonyms_path":"org/elasticsearch/index/analysis/synonyms/synonyms.txt"
+                    "synonyms_path":"synonyms.txt"
                 },
                 "synonymWordnet":{
                     "type":"synonym",
@@ -55,7 +55,7 @@
                 "synonymWordnet_file":{
                     "type":"synonym",
                     "format":"wordnet",
-                    "synonyms_path":"org/elasticsearch/index/analysis/synonyms/synonyms_wordnet.txt"
+                    "synonyms_path":"synonyms_wordnet.txt"
                 },
                 "synonymWithTokenizerSettings":{
                     "type":"synonym",
diff --git a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
index bad431f..974a4a2 100644
--- a/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
+++ b/core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java
@@ -29,7 +29,15 @@ import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.*;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexDeletionPolicy;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LiveIndexWriterConfig;
+import org.apache.lucene.index.LogByteSizeMergePolicy;
+import org.apache.lucene.index.MergePolicy;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TieredMergePolicy;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
@@ -61,13 +69,19 @@ import org.elasticsearch.index.deletionpolicy.KeepOnlyLastDeletionPolicy;
 import org.elasticsearch.index.deletionpolicy.SnapshotDeletionPolicy;
 import org.elasticsearch.index.engine.Engine.Searcher;
 import org.elasticsearch.index.indexing.ShardIndexingService;
-import org.elasticsearch.index.mapper.*;
+import org.elasticsearch.index.mapper.ContentPath;
+import org.elasticsearch.index.mapper.DocumentMapper;
+import org.elasticsearch.index.mapper.DocumentMapperParser;
 import org.elasticsearch.index.mapper.Mapper.BuilderContext;
+import org.elasticsearch.index.mapper.MapperBuilders;
+import org.elasticsearch.index.mapper.MapperService;
+import org.elasticsearch.index.mapper.Mapping;
+import org.elasticsearch.index.mapper.MetadataFieldMapper;
 import org.elasticsearch.index.mapper.ParseContext.Document;
+import org.elasticsearch.index.mapper.ParsedDocument;
 import org.elasticsearch.index.mapper.internal.SourceFieldMapper;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.mapper.object.RootObjectMapper;
-import org.elasticsearch.index.settings.IndexDynamicSettingsModule;
 import org.elasticsearch.index.shard.MergeSchedulerConfig;
 import org.elasticsearch.index.shard.ShardId;
 import org.elasticsearch.index.shard.ShardUtils;
@@ -93,7 +107,13 @@ import java.nio.charset.Charset;
 import java.nio.file.DirectoryStream;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.regex.Pattern;
@@ -101,7 +121,12 @@ import java.util.regex.Pattern;
 import static org.elasticsearch.common.settings.Settings.Builder.EMPTY_SETTINGS;
 import static org.elasticsearch.index.engine.Engine.Operation.Origin.PRIMARY;
 import static org.elasticsearch.index.engine.Engine.Operation.Origin.REPLICA;
-import static org.hamcrest.Matchers.*;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.hasKey;
+import static org.hamcrest.Matchers.not;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.hamcrest.Matchers.nullValue;
 
 public class InternalEngineTests extends ESTestCase {
 
@@ -1511,11 +1536,6 @@ public class InternalEngineTests extends ESTestCase {
         assertEquals(currentIndexWriterConfig.getCodec().getName(), codecService.codec(codecName).getName());
         assertEquals(engine.config().getIndexConcurrency(), indexConcurrency);
         assertEquals(currentIndexWriterConfig.getMaxThreadStates(), indexConcurrency);
-
-
-        IndexDynamicSettingsModule settings = new IndexDynamicSettingsModule();
-        assertTrue(settings.containsSetting(EngineConfig.INDEX_COMPOUND_ON_FLUSH));
-        assertTrue(settings.containsSetting(EngineConfig.INDEX_GC_DELETES_SETTING));
     }
 
     @Test
diff --git a/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java
deleted file mode 100644
index c5a2b10..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/AndQueryBuilderTest.java
+++ /dev/null
@@ -1,155 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.Matchers.*;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class AndQueryBuilderTest extends BaseQueryTestCase<AndQueryBuilder> {
-
-    /**
-     * @return a AndQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected AndQueryBuilder doCreateTestQueryBuilder() {
-        AndQueryBuilder query = new AndQueryBuilder();
-        int subQueries = randomIntBetween(1, 5);
-        for (int i = 0; i < subQueries; i++ ) {
-            query.add(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(AndQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.innerQueries().isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            List<Query> clauses = new ArrayList<>();
-            for (QueryBuilder innerFilter : queryBuilder.innerQueries()) {
-                Query clause = innerFilter.toQuery(context);
-                if (clause != null) {
-                    clauses.add(clause);
-                }
-            }
-            if (clauses.isEmpty()) {
-                assertThat(query, nullValue());
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(clauses.size()));
-                Iterator<Query> queryIterator = clauses.iterator();
-                for (BooleanClause booleanClause : booleanQuery) {
-                    assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.MUST));
-                    assertThat(booleanClause.getQuery(), equalTo(queryIterator.next()));
-                }
-            }
-        }
-    }
-
-    /**
-     * test corner case where no inner queries exist
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryShardException, IOException {
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        assertNull(andQuery.toQuery(createShardContext()));
-    }
-
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFiltersSection() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"and\" : {}";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, AndQueryBuilder.NAME);
-        context.queryParser(AndQueryBuilder.NAME).fromXContent(context);
-    }
-
-    @Test
-    public void testValidate() {
-        AndQueryBuilder andQuery = new AndQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    andQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    andQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                andQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(andQuery, totalExpectedErrors);
-    }
-
-    public void testParsingToplevelArray() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"and\" : [ { \"match_all\" : {} } ] }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertThat(parser.nextToken(), is(XContentParser.Token.START_OBJECT));
-        assertThat(parser.nextToken(), is(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), is(AndQueryBuilder.NAME));
-        parser.nextToken();
-        AndQueryBuilder queryBuilder = (AndQueryBuilder) context.queryParser(AndQueryBuilder.NAME).fromXContent(context);
-        assertThat(queryBuilder.innerQueries().get(0), equalTo((QueryBuilder) new MatchAllQueryBuilder()));
-    }
-
-    public void testParsingFiltersArray() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"and\" : { \"filters\" : [ { \"match_all\" : {} } ], \"boost\" : 0.7 } }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, AndQueryBuilder.NAME);
-        AndQueryBuilder queryBuilder = (AndQueryBuilder) context.queryParser(AndQueryBuilder.NAME).fromXContent(context);
-        assertThat(queryBuilder.innerQueries().get(0), equalTo((QueryBuilder) new MatchAllQueryBuilder()));
-        assertThat(queryBuilder.boost(), equalTo(0.7f));
-    }
-
-    @Test(expected=QueryParsingException.class)
-    public void testParsingExceptionNonFiltersElementArray() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"and\" : { \"whatever_filters\" : [ { \"match_all\" : {} } ] } }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, AndQueryBuilder.NAME);
-        context.queryParser(AndQueryBuilder.NAME).fromXContent(context);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java
deleted file mode 100644
index 9490acb..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BaseQueryTestCase.java
+++ /dev/null
@@ -1,503 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.Version;
-import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.cluster.ClusterService;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.ParseFieldMatcher;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.compress.CompressedXContent;
-import org.elasticsearch.common.inject.AbstractModule;
-import org.elasticsearch.common.inject.Injector;
-import org.elasticsearch.common.inject.ModulesBuilder;
-import org.elasticsearch.common.inject.multibindings.Multibinder;
-import org.elasticsearch.common.inject.util.Providers;
-import org.elasticsearch.common.io.stream.BytesStreamOutput;
-import org.elasticsearch.common.io.stream.NamedWriteableAwareStreamInput;
-import org.elasticsearch.common.io.stream.NamedWriteableRegistry;
-import org.elasticsearch.common.io.stream.StreamInput;
-import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.common.settings.SettingsModule;
-import org.elasticsearch.common.unit.Fuzziness;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.EnvironmentModule;
-import org.elasticsearch.index.Index;
-import org.elasticsearch.index.IndexNameModule;
-import org.elasticsearch.index.analysis.AnalysisModule;
-import org.elasticsearch.index.cache.IndexCacheModule;
-import org.elasticsearch.index.mapper.MapperService;
-import org.elasticsearch.index.query.functionscore.ScoreFunctionParser;
-import org.elasticsearch.index.query.support.QueryParsers;
-import org.elasticsearch.index.settings.IndexSettingsModule;
-import org.elasticsearch.index.similarity.SimilarityModule;
-import org.elasticsearch.indices.analysis.IndicesAnalysisService;
-import org.elasticsearch.indices.breaker.CircuitBreakerService;
-import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
-import org.elasticsearch.indices.query.IndicesQueriesModule;
-import org.elasticsearch.script.ScriptModule;
-import org.elasticsearch.search.internal.SearchContext;
-import org.elasticsearch.test.ESTestCase;
-import org.elasticsearch.test.TestSearchContext;
-import org.elasticsearch.test.VersionUtils;
-import org.elasticsearch.threadpool.ThreadPool;
-import org.elasticsearch.threadpool.ThreadPoolModule;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.is;
-import static org.hamcrest.Matchers.not;
-import static org.hamcrest.Matchers.notNullValue;
-import static org.hamcrest.Matchers.nullValue;
-
-public abstract class BaseQueryTestCase<QB extends AbstractQueryBuilder<QB>> extends ESTestCase {
-
-    protected static final String STRING_FIELD_NAME = "mapped_string";
-    protected static final String INT_FIELD_NAME = "mapped_int";
-    protected static final String DOUBLE_FIELD_NAME = "mapped_double";
-    protected static final String BOOLEAN_FIELD_NAME = "mapped_boolean";
-    protected static final String DATE_FIELD_NAME = "mapped_date";
-    protected static final String OBJECT_FIELD_NAME = "mapped_object";
-    protected static final String[] mappedFieldNames = new String[] { STRING_FIELD_NAME, INT_FIELD_NAME,
-            DOUBLE_FIELD_NAME, BOOLEAN_FIELD_NAME, DATE_FIELD_NAME, OBJECT_FIELD_NAME };
-
-    private static Injector injector;
-    private static IndexQueryParserService queryParserService;
-    private static Index index;
-
-    private static String[] currentTypes;
-
-    protected static String[] getCurrentTypes() {
-        return currentTypes;
-    }
-
-    private static NamedWriteableRegistry namedWriteableRegistry;
-
-    /**
-     * Setup for the whole base test class.
-     * @throws IOException
-     */
-    @BeforeClass
-    public static void init() throws IOException {
-        Settings settings = Settings.settingsBuilder()
-                .put("name", BaseQueryTestCase.class.toString())
-                .put("path.home", createTempDir())
-                .put(IndexMetaData.SETTING_VERSION_CREATED, VersionUtils.randomVersionBetween(random(),
-                        Version.V_1_0_0, Version.CURRENT))
-                .build();
-
-        index = new Index("test");
-        injector = new ModulesBuilder().add(
-                new EnvironmentModule(new Environment(settings)),
-                new SettingsModule(settings),
-                new ThreadPoolModule(new ThreadPool(settings)),
-                new IndicesQueriesModule(),
-                new ScriptModule(settings),
-                new IndexSettingsModule(index, settings),
-                new IndexCacheModule(settings),
-                new AnalysisModule(settings, new IndicesAnalysisService(settings)),
-                new SimilarityModule(settings),
-                new IndexNameModule(index),
-                new AbstractModule() {
-                    @Override
-                    protected void configure() {
-                        Multibinder.newSetBinder(binder(), ScoreFunctionParser.class);
-                        bind(ClusterService.class).toProvider(Providers.of((ClusterService) null));
-                        bind(CircuitBreakerService.class).to(NoneCircuitBreakerService.class);
-                        bind(NamedWriteableRegistry.class).asEagerSingleton();
-                    }
-                }
-        ).createInjector();
-        queryParserService = injector.getInstance(IndexQueryParserService.class);
-        MapperService mapperService = queryParserService.mapperService;
-        //create some random type with some default field, those types will stick around for all of the subclasses
-        currentTypes = new String[randomIntBetween(0, 5)];
-        for (int i = 0; i < currentTypes.length; i++) {
-            String type = randomAsciiOfLengthBetween(1, 10);
-            mapperService.merge(type, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(type,
-                    STRING_FIELD_NAME, "type=string",
-                    INT_FIELD_NAME, "type=integer",
-                    DOUBLE_FIELD_NAME, "type=double",
-                    BOOLEAN_FIELD_NAME, "type=boolean",
-                    DATE_FIELD_NAME, "type=date",
-                    OBJECT_FIELD_NAME, "type=object"
-            ).string()), false, false);
-            // also add mappings for two inner field in the object field
-            mapperService.merge(type, new CompressedXContent("{\"properties\":{\""+OBJECT_FIELD_NAME+"\":{\"type\":\"object\","
-                    + "\"properties\":{\""+DATE_FIELD_NAME+"\":{\"type\":\"date\"},\""+INT_FIELD_NAME+"\":{\"type\":\"integer\"}}}}}"), false, false);
-            currentTypes[i] = type;
-        }
-        namedWriteableRegistry = injector.getInstance(NamedWriteableRegistry.class);
-    }
-
-    @AfterClass
-    public static void afterClass() throws Exception {
-        terminate(injector.getInstance(ThreadPool.class));
-        injector = null;
-        index = null;
-        queryParserService = null;
-        currentTypes = null;
-        namedWriteableRegistry = null;
-    }
-
-    @Before
-    public void beforeTest() {
-        //set some random types to be queried as part the search request, before each test
-        String[] types = getRandomTypes();
-        //some query (e.g. range query) have a different behaviour depending on whether the current search context is set or not
-        //which is why we randomly set the search context, which will internally also do QueryParseContext.setTypes(types)
-        if (randomBoolean()) {
-            QueryShardContext.setTypes(types);
-        } else {
-            TestSearchContext testSearchContext = new TestSearchContext();
-            testSearchContext.setTypes(types);
-            SearchContext.setCurrent(testSearchContext);
-        }
-    }
-
-    @After
-    public void afterTest() {
-        QueryShardContext.removeTypes();
-        SearchContext.removeCurrent();
-    }
-
-    protected final QB createTestQueryBuilder() {
-        QB query = doCreateTestQueryBuilder();
-        if (supportsBoostAndQueryName()) {
-            if (randomBoolean()) {
-                query.boost(2.0f / randomIntBetween(1, 20));
-            }
-            if (randomBoolean()) {
-                query.queryName(randomAsciiOfLengthBetween(1, 10));
-            }
-        }
-        return query;
-    }
-
-    /**
-     * Create the query that is being tested
-     */
-    protected abstract QB doCreateTestQueryBuilder();
-
-    /**
-     * Generic test that creates new query from the test query and checks both for equality
-     * and asserts equality on the two queries.
-     */
-    @Test
-    public void testFromXContent() throws IOException {
-        QB testQuery = createTestQueryBuilder();
-        QueryParseContext context = createParseContext();
-        String contentString = testQuery.toString();
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        context.reset(parser);
-        assertQueryHeader(parser, testQuery.getName());
-
-        QueryBuilder newQuery = queryParserService.queryParser(testQuery.getName()).fromXContent(context);
-        assertNotSame(newQuery, testQuery);
-        assertEquals(testQuery, newQuery);
-        assertEquals(testQuery.hashCode(), newQuery.hashCode());
-    }
-
-    /**
-     * Test creates the {@link Query} from the {@link QueryBuilder} under test and delegates the
-     * assertions being made on the result to the implementing subclass.
-     */
-    @Test
-    public void testToQuery() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        QB firstQuery = createTestQueryBuilder();
-        Query firstLuceneQuery = firstQuery.toQuery(context);
-        assertLuceneQuery(firstQuery, firstLuceneQuery, context);
-
-        QB secondQuery = copyQuery(firstQuery);
-        //query _name never should affect the result of toQuery, we randomly set it to make sure
-        if (randomBoolean()) {
-            secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10));
-        }
-        Query secondLuceneQuery = secondQuery.toQuery(context);
-        assertLuceneQuery(secondQuery, secondLuceneQuery, context);
-        assertThat("two equivalent query builders lead to different lucene queries", secondLuceneQuery, equalTo(firstLuceneQuery));
-
-        //if the initial lucene query is null, changing its boost won't have any effect, we shouldn't test that
-        //few queries also don't support boost e.g. wrapper query and filter query
-        //otherwise makes sure that boost is taken into account in toQuery
-        if (firstLuceneQuery != null && supportsBoostAndQueryName()) {
-            secondQuery.boost(firstQuery.boost() + 1f + randomFloat());
-            Query thirdLuceneQuery = secondQuery.toQuery(context);
-            assertThat("modifying the boost doesn't affect the corresponding lucene query", firstLuceneQuery, not(equalTo(thirdLuceneQuery)));
-        }
-    }
-
-    /**
-     * Few queries allow you to set the boost and queryName but don't do anything with it. This method allows
-     * to disable boost and queryName related tests for those queries. Those queries are easy to identify: their parsers
-     * don't parse `boost` and `_name` as they don't apply to the specific query e.g. filter query or wrapper query
-     */
-    protected boolean supportsBoostAndQueryName() {
-        return true;
-    }
-
-    /**
-     * Checks the result of {@link QueryBuilder#toQuery(QueryShardContext)} given the original {@link QueryBuilder} and {@link QueryShardContext}.
-     * Verifies that named queries and boost are properly handled and delegates to {@link #doAssertLuceneQuery(AbstractQueryBuilder, Query, QueryShardContext)}
-     * for query specific checks.
-     */
-    protected final void assertLuceneQuery(QB queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.queryName() != null) {
-            Query namedQuery = context.copyNamedQueries().get(queryBuilder.queryName());
-            assertThat(namedQuery, equalTo(query));
-        }
-        if (query != null) {
-            assertThat(query.getBoost(), equalTo(queryBuilder.boost()));
-        }
-        doAssertLuceneQuery(queryBuilder, query, context);
-    }
-
-    /**
-     * Checks the result of {@link QueryBuilder#toQuery(QueryShardContext)} given the original {@link QueryBuilder} and {@link QueryShardContext}.
-     * Contains the query specific checks to be implemented by subclasses.
-     */
-    protected abstract void doAssertLuceneQuery(QB queryBuilder, Query query, QueryShardContext context) throws IOException;
-
-    /**
-     * Test serialization and deserialization of the test query.
-     */
-    @Test
-    public void testSerialization() throws IOException {
-        QB testQuery = createTestQueryBuilder();
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            testQuery.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                QueryBuilder<?> prototype = queryParserService.queryParser(testQuery.getWriteableName()).getBuilderPrototype();
-                QueryBuilder deserializedQuery = prototype.readFrom(in);
-                assertEquals(deserializedQuery, testQuery);
-                assertEquals(deserializedQuery.hashCode(), testQuery.hashCode());
-                assertNotSame(deserializedQuery, testQuery);
-            }
-        }
-    }
-
-    @Test
-    public void testEqualsAndHashcode() throws IOException {
-        QB firstQuery = createTestQueryBuilder();
-        assertFalse("query is equal to null", firstQuery.equals(null));
-        assertFalse("query is equal to incompatible type", firstQuery.equals(""));
-        assertTrue("query is not equal to self", firstQuery.equals(firstQuery));
-        assertThat("same query's hashcode returns different values if called multiple times", firstQuery.hashCode(), equalTo(firstQuery.hashCode()));
-
-        QB secondQuery = copyQuery(firstQuery);
-        assertTrue("query is not equal to self", secondQuery.equals(secondQuery));
-        assertTrue("query is not equal to its copy", firstQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", secondQuery.equals(firstQuery));
-        assertThat("query copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(firstQuery.hashCode()));
-
-        QB thirdQuery = copyQuery(secondQuery);
-        assertTrue("query is not equal to self", thirdQuery.equals(thirdQuery));
-        assertTrue("query is not equal to its copy", secondQuery.equals(thirdQuery));
-        assertThat("query copy's hashcode is different from original hashcode", secondQuery.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not transitive", firstQuery.equals(thirdQuery));
-        assertThat("query copy's hashcode is different from original hashcode", firstQuery.hashCode(), equalTo(thirdQuery.hashCode()));
-        assertTrue("equals is not symmetric", thirdQuery.equals(secondQuery));
-        assertTrue("equals is not symmetric", thirdQuery.equals(firstQuery));
-
-        if (randomBoolean()) {
-            secondQuery.queryName(secondQuery.queryName() == null ? randomAsciiOfLengthBetween(1, 30) : secondQuery.queryName() + randomAsciiOfLengthBetween(1, 10));
-        } else {
-            secondQuery.boost(firstQuery.boost() + 1f + randomFloat());
-        }
-        assertThat("different queries should not be equal", secondQuery, not(equalTo(firstQuery)));
-        assertThat("different queries should have different hashcode", secondQuery.hashCode(), not(equalTo(firstQuery.hashCode())));
-    }
-
-    //we use the streaming infra to create a copy of the query provided as argument
-    private QB copyQuery(QB query) throws IOException {
-        try (BytesStreamOutput output = new BytesStreamOutput()) {
-            query.writeTo(output);
-            try (StreamInput in = new NamedWriteableAwareStreamInput(StreamInput.wrap(output.bytes()), namedWriteableRegistry)) {
-                QueryBuilder<?> prototype = queryParserService.queryParser(query.getWriteableName()).getBuilderPrototype();
-                @SuppressWarnings("unchecked")
-                QB secondQuery = (QB)prototype.readFrom(in);
-                return secondQuery;
-            }
-        }
-    }
-
-    /**
-     * @return a new {@link QueryShardContext} based on the base test index and queryParserService
-     */
-    protected static QueryShardContext createShardContext() {
-        QueryShardContext queryCreationContext = new QueryShardContext(index, queryParserService);
-        queryCreationContext.parseFieldMatcher(ParseFieldMatcher.EMPTY);
-        return queryCreationContext;
-    }
-
-    /**
-     * @return a new {@link QueryParseContext} based on the base test index and queryParserService
-     */
-    protected static QueryParseContext createParseContext() {
-        return createShardContext().parseContext();
-    }
-
-    protected static void assertQueryHeader(XContentParser parser, String expectedParserName) throws IOException {
-        assertThat(parser.nextToken(), is(XContentParser.Token.START_OBJECT));
-        assertThat(parser.nextToken(), is(XContentParser.Token.FIELD_NAME));
-        assertThat(parser.currentName(), is(expectedParserName));
-        assertThat(parser.nextToken(), is(XContentParser.Token.START_OBJECT));
-    }
-
-    protected static void assertValidate(QueryBuilder queryBuilder, int totalExpectedErrors) {
-        QueryValidationException queryValidationException = queryBuilder.validate();
-        if (totalExpectedErrors > 0) {
-            assertThat(queryValidationException, notNullValue());
-            assertThat(queryValidationException.validationErrors().size(), equalTo(totalExpectedErrors));
-        } else {
-            assertThat(queryValidationException, nullValue());
-        }
-    }
-
-    /**
-     * create a random value for either {@link BaseQueryTestCase#BOOLEAN_FIELD_NAME}, {@link BaseQueryTestCase#INT_FIELD_NAME},
-     * {@link BaseQueryTestCase#DOUBLE_FIELD_NAME} or {@link BaseQueryTestCase#STRING_FIELD_NAME}, or a String value by default
-     */
-    protected static Object randomValueForField(String fieldName) {
-        Object value;
-        switch (fieldName) {
-            case BOOLEAN_FIELD_NAME: value = randomBoolean(); break;
-            case INT_FIELD_NAME: value = randomInt(); break;
-            case DOUBLE_FIELD_NAME: value = randomDouble(); break;
-            case STRING_FIELD_NAME: value = randomAsciiOfLengthBetween(1, 10); break;
-            default : value = randomAsciiOfLengthBetween(1, 10);
-        }
-        return value;
-    }
-
-    /**
-     * Helper method to return a random rewrite method
-     */
-    protected static String getRandomRewriteMethod() {
-        String rewrite;
-        if (randomBoolean()) {
-            rewrite = randomFrom(QueryParsers.CONSTANT_SCORE,
-                    QueryParsers.SCORING_BOOLEAN,
-                    QueryParsers.CONSTANT_SCORE_BOOLEAN).getPreferredName();
-        } else {
-            rewrite = randomFrom(QueryParsers.TOP_TERMS,
-                    QueryParsers.TOP_TERMS_BOOST,
-                    QueryParsers.TOP_TERMS_BLENDED_FREQS).getPreferredName() + "1";
-        }
-        return rewrite;
-    }
-
-    protected String[] getRandomTypes() {
-        String[] types;
-        if (currentTypes.length > 0 && randomBoolean()) {
-            int numberOfQueryTypes = randomIntBetween(1, currentTypes.length);
-            types = new String[numberOfQueryTypes];
-            for (int i = 0; i < numberOfQueryTypes; i++) {
-                types[i] = randomFrom(currentTypes);
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[] { MetaData.ALL };
-            } else {
-                types = new String[0];
-            }
-        }
-        return types;
-    }
-
-    protected String getRandomType() {
-        return (currentTypes.length == 0) ? MetaData.ALL : randomFrom(currentTypes);
-    }
-
-    /**
-     * Helper method to return a random field (mapped or unmapped) and a value
-     */
-    protected static Tuple<String, Object> getRandomFieldNameAndValue() {
-        // if no type is set then return random field name and value
-        if (currentTypes == null || currentTypes.length == 0) {
-            return new Tuple<String, Object>(randomAsciiOfLengthBetween(1, 10), randomAsciiOfLengthBetween(1, 50));
-        }
-        // mapped fields
-        String fieldName = randomFrom(mappedFieldNames);
-        Object value = randomAsciiOfLengthBetween(1, 50);
-        switch(fieldName) {
-            case STRING_FIELD_NAME:
-                value = rarely() ? randomUnicodeOfLength(10) : value; // unicode in 10% cases
-                break;
-            case INT_FIELD_NAME:
-                value = randomIntBetween(0, 10);
-                break;
-            case DOUBLE_FIELD_NAME:
-                value = randomDouble() * 10;
-                break;
-            case BOOLEAN_FIELD_NAME:
-                value = randomBoolean();
-                break;
-            case DATE_FIELD_NAME:
-                value = new DateTime(System.currentTimeMillis(), DateTimeZone.UTC).toString();
-                break;
-        } // all other fields assigned to random string
-
-        // unmapped fields
-        if (randomBoolean()) {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        return new Tuple<>(fieldName, value);
-    }
-
-    protected static Fuzziness randomFuzziness(String fieldName) {
-        Fuzziness fuzziness = Fuzziness.AUTO;
-        switch (fieldName) {
-            case INT_FIELD_NAME:
-                fuzziness = Fuzziness.build(randomIntBetween(3, 100));
-                break;
-            case DOUBLE_FIELD_NAME:
-                fuzziness = Fuzziness.build(1 + randomFloat() * 10);
-                break;
-            case DATE_FIELD_NAME:
-                fuzziness = Fuzziness.build(randomTimeValue());
-                break;
-        }
-        if (randomBoolean()) {
-            fuzziness = Fuzziness.fromEdits(randomIntBetween(0, 2));
-        }
-        return fuzziness;
-    }
-
-    protected static boolean isNumericFieldName(String fieldName) {
-        return INT_FIELD_NAME.equals(fieldName) || DOUBLE_FIELD_NAME.equals(fieldName);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java
deleted file mode 100644
index 6da2895..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BaseTermQueryTestCase.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.junit.Test;
-
-import static org.hamcrest.Matchers.is;
-
-public abstract class BaseTermQueryTestCase<QB extends BaseTermQueryBuilder<QB>> extends BaseQueryTestCase<QB> {
-
-    @Override
-    protected final QB doCreateTestQueryBuilder() {
-        String fieldName = null;
-        Object value;
-        switch (randomIntBetween(0, 3)) {
-            case 0:
-                if (randomBoolean()) {
-                    fieldName = BOOLEAN_FIELD_NAME;
-                }
-                value = randomBoolean();
-                break;
-            case 1:
-                if (randomBoolean()) {
-                    fieldName = STRING_FIELD_NAME;
-                }
-                if (frequently()) {
-                    value = randomAsciiOfLengthBetween(1, 10);
-                } else {
-                    // generate unicode string in 10% of cases
-                    value = randomUnicodeOfLength(10);
-                }
-                break;
-            case 2:
-                if (randomBoolean()) {
-                    fieldName = INT_FIELD_NAME;
-                }
-                value = randomInt(10000);
-                break;
-            case 3:
-                if (randomBoolean()) {
-                    fieldName = DOUBLE_FIELD_NAME;
-                }
-                value = randomDouble();
-                break;
-            default:
-                throw new UnsupportedOperationException();
-        }
-
-        if (fieldName == null) {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        return createQueryBuilder(fieldName, value);
-    }
-
-    protected abstract QB createQueryBuilder(String fieldName, Object value);
-
-    @Test
-    public void testValidate() throws QueryShardException {
-        QB queryBuilder = createQueryBuilder(randomAsciiOfLengthBetween(1, 30), randomAsciiOfLengthBetween(1, 30));
-        assertNull(queryBuilder.validate());
-
-        queryBuilder = createQueryBuilder(null, randomAsciiOfLengthBetween(1, 30));
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = createQueryBuilder("", randomAsciiOfLengthBetween(1, 30));
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(1));
-
-        queryBuilder = createQueryBuilder("", null);
-        assertNotNull(queryBuilder.validate());
-        assertThat(queryBuilder.validate().validationErrors().size(), is(2));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java
deleted file mode 100644
index 027ab36..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BoolQueryBuilderTest.java
+++ /dev/null
@@ -1,175 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class BoolQueryBuilderTest extends BaseQueryTestCase<BoolQueryBuilder> {
-
-    @Override
-    protected BoolQueryBuilder doCreateTestQueryBuilder() {
-        BoolQueryBuilder query = new BoolQueryBuilder();
-        if (randomBoolean()) {
-            query.adjustPureNegative(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.minimumNumberShouldMatch(randomIntBetween(1, 10));
-        }
-        int mustClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < mustClauses; i++) {
-            query.must(RandomQueryBuilder.createQuery(random()));
-        }
-        int mustNotClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < mustNotClauses; i++) {
-            query.mustNot(RandomQueryBuilder.createQuery(random()));
-        }
-        int shouldClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < shouldClauses; i++) {
-            query.should(RandomQueryBuilder.createQuery(random()));
-        }
-        int filterClauses = randomIntBetween(0, 3);
-        for (int i = 0; i < filterClauses; i++) {
-            query.filter(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(BoolQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (!queryBuilder.hasClauses()) {
-            assertThat(query, instanceOf(MatchAllDocsQuery.class));
-        } else {
-            List<BooleanClause> clauses = new ArrayList<>();
-            clauses.addAll(getBooleanClauses(queryBuilder.must(), BooleanClause.Occur.MUST, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.mustNot(), BooleanClause.Occur.MUST_NOT, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.should(), BooleanClause.Occur.SHOULD, context));
-            clauses.addAll(getBooleanClauses(queryBuilder.filter(), BooleanClause.Occur.FILTER, context));
-
-            if (clauses.isEmpty()) {
-                assertThat(query, instanceOf(MatchAllDocsQuery.class));
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                if (queryBuilder.adjustPureNegative()) {
-                    boolean isNegative = true;
-                    for (BooleanClause clause : clauses) {
-                        if (clause.isProhibited() == false) {
-                            isNegative = false;
-                            break;
-                        }
-                    }
-                    if (isNegative) {
-                        clauses.add(new BooleanClause(new MatchAllDocsQuery(), BooleanClause.Occur.MUST));
-                    }
-                }
-                assertThat(booleanQuery.clauses().size(), equalTo(clauses.size()));
-                Iterator<BooleanClause> clauseIterator = clauses.iterator();
-                for (BooleanClause booleanClause : booleanQuery.getClauses()) {
-                    assertThat(booleanClause, equalTo(clauseIterator.next()));
-                }
-            }
-        }
-    }
-
-    private static List<BooleanClause> getBooleanClauses(List<QueryBuilder> queryBuilders, BooleanClause.Occur occur, QueryShardContext context) throws IOException {
-        List<BooleanClause> clauses = new ArrayList<>();
-        for (QueryBuilder query : queryBuilders) {
-            Query innerQuery = query.toQuery(context);
-            if (innerQuery != null) {
-                clauses.add(new BooleanClause(innerQuery, occur));
-            }
-        }
-        return clauses;
-    }
-
-    @Test
-    public void testValidate() {
-        BoolQueryBuilder booleanQuery = new BoolQueryBuilder();
-        int iters = randomIntBetween(0, 3);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.must(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.must(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.must(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.should(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.should(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.should(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.mustNot(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.mustNot(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.mustNot(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        iters = randomIntBetween(0, 3);
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    booleanQuery.filter(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    booleanQuery.filter(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                booleanQuery.filter(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(booleanQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java
deleted file mode 100644
index 2b7ca52..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/BoostingQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queries.BoostingQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class BoostingQueryBuilderTest extends BaseQueryTestCase<BoostingQueryBuilder> {
-
-    @Override
-    protected BoostingQueryBuilder doCreateTestQueryBuilder() {
-        BoostingQueryBuilder query = new BoostingQueryBuilder(RandomQueryBuilder.createQuery(random()), RandomQueryBuilder.createQuery(random()));
-        query.negativeBoost(2.0f / randomIntBetween(1, 20));
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(BoostingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query positive = queryBuilder.positiveQuery().toQuery(context);
-        Query negative = queryBuilder.negativeQuery().toQuery(context);
-        if (positive == null || negative == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(BoostingQuery.class));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        QueryBuilder positive = null;
-        QueryBuilder negative = null;
-        if (frequently()) {
-            if (randomBoolean()) {
-                negative = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            negative = RandomQueryBuilder.createQuery(random());
-        }
-        if (frequently()) {
-            if (randomBoolean()) {
-                positive = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            positive = RandomQueryBuilder.createQuery(random());
-        }
-        BoostingQueryBuilder boostingQuery = new BoostingQueryBuilder(positive, negative);
-        if (frequently()) {
-            boostingQuery.negativeBoost(0.5f);
-        } else {
-            boostingQuery.negativeBoost(-0.5f);
-            totalExpectedErrors++;
-        }
-        assertValidate(boostingQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java
deleted file mode 100644
index ab5cb07..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/CommonTermsQueryBuilderTest.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.queries.ExtendedCommonTermsQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class CommonTermsQueryBuilderTest extends BaseQueryTestCase<CommonTermsQueryBuilder> {
-
-    @Override
-    protected CommonTermsQueryBuilder doCreateTestQueryBuilder() {
-        CommonTermsQueryBuilder query;
-
-        // mapped or unmapped field
-        String text = randomAsciiOfLengthBetween(1, 10);
-        if (randomBoolean()) {
-            query = new CommonTermsQueryBuilder(STRING_FIELD_NAME, text);
-        } else {
-            query = new CommonTermsQueryBuilder(randomAsciiOfLengthBetween(1, 10), text);
-        }
-
-        if (randomBoolean()) {
-            query.cutoffFrequency((float) randomIntBetween(1, 10));
-        }
-
-        if (randomBoolean()) {
-            query.lowFreqOperator(randomFrom(Operator.values()));
-        }
-
-        // number of low frequency terms that must match
-        if (randomBoolean()) {
-            query.lowFreqMinimumShouldMatch("" + randomIntBetween(1, 5));
-        }
-
-        if (randomBoolean()) {
-            query.highFreqOperator(randomFrom(Operator.values()));
-        }
-
-        // number of high frequency terms that must match
-        if (randomBoolean()) {
-            query.highFreqMinimumShouldMatch("" + randomIntBetween(1, 5));
-        }
-
-        if (randomBoolean()) {
-            query.analyzer(randomFrom("simple", "keyword", "whitespace"));
-        }
-
-        if (randomBoolean()) {
-            query.disableCoord(randomBoolean());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(CommonTermsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(ExtendedCommonTermsQuery.class));
-        ExtendedCommonTermsQuery extendedCommonTermsQuery = (ExtendedCommonTermsQuery) query;
-        assertThat(extendedCommonTermsQuery.getHighFreqMinimumNumberShouldMatchSpec(), equalTo(queryBuilder.highFreqMinimumShouldMatch()));
-        assertThat(extendedCommonTermsQuery.getLowFreqMinimumNumberShouldMatchSpec(), equalTo(queryBuilder.lowFreqMinimumShouldMatch()));
-    }
-
-    @Test
-    public void testValidate() {
-        CommonTermsQueryBuilder commonTermsQueryBuilder = new CommonTermsQueryBuilder("", "text");
-        assertThat(commonTermsQueryBuilder.validate().validationErrors().size(), is(1));
-
-        commonTermsQueryBuilder = new CommonTermsQueryBuilder("field", null);
-        assertThat(commonTermsQueryBuilder.validate().validationErrors().size(), is(1));
-
-        commonTermsQueryBuilder = new CommonTermsQueryBuilder("field", "text");
-        assertNull(commonTermsQueryBuilder.validate());
-    }
-
-    @Test
-    public void testNoTermsFromQueryString() throws IOException {
-        CommonTermsQueryBuilder builder = new CommonTermsQueryBuilder(STRING_FIELD_NAME, "");
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        assertNull(builder.toQuery(context));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java
deleted file mode 100644
index 3604763..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ConstantScoreQueryBuilderTest.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class ConstantScoreQueryBuilderTest extends BaseQueryTestCase<ConstantScoreQueryBuilder> {
-
-    /**
-     * @return a {@link ConstantScoreQueryBuilder} with random boost between 0.1f and 2.0f
-     */
-    @Override
-    protected ConstantScoreQueryBuilder doCreateTestQueryBuilder() {
-        return new ConstantScoreQueryBuilder(RandomQueryBuilder.createQuery(random()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ConstantScoreQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    /**
-     * test that missing "filter" element causes {@link QueryParsingException}
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testFilterElement() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryId = ConstantScoreQueryBuilder.NAME;
-        String queryString = "{ \""+queryId+"\" : {}";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, queryId);
-        context.queryParser(queryId).fromXContent(context);
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        ConstantScoreQueryBuilder constantScoreQuery = new ConstantScoreQueryBuilder(innerQuery);
-        assertValidate(constantScoreQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java
deleted file mode 100644
index b547326..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/DisMaxQueryBuilderTest.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class DisMaxQueryBuilderTest extends BaseQueryTestCase<DisMaxQueryBuilder> {
-
-    /**
-     * @return a {@link DisMaxQueryBuilder} with random inner queries
-     */
-    @Override
-    protected DisMaxQueryBuilder doCreateTestQueryBuilder() {
-        DisMaxQueryBuilder dismax = new DisMaxQueryBuilder();
-        int clauses = randomIntBetween(1, 5);
-        for (int i = 0; i < clauses; i++) {
-            dismax.add(RandomQueryBuilder.createQuery(random()));
-        }
-        if (randomBoolean()) {
-            dismax.tieBreaker(2.0f / randomIntBetween(1, 20));
-        }
-        return dismax;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(DisMaxQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Collection<Query> queries = AbstractQueryBuilder.toQueries(queryBuilder.innerQueries(), context);
-        if (queries.isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(DisjunctionMaxQuery.class));
-            DisjunctionMaxQuery disjunctionMaxQuery = (DisjunctionMaxQuery) query;
-            assertThat(disjunctionMaxQuery.getTieBreakerMultiplier(), equalTo(queryBuilder.tieBreaker()));
-            assertThat(disjunctionMaxQuery.getDisjuncts().size(), equalTo(queries.size()));
-            Iterator<Query> queryIterator = queries.iterator();
-            for (int i = 0; i < disjunctionMaxQuery.getDisjuncts().size(); i++) {
-                assertThat(disjunctionMaxQuery.getDisjuncts().get(i), equalTo(queryIterator.next()));
-            }
-        }
-    }
-
-    /**
-     * test `null`return value for missing inner queries
-     * @throws IOException
-     * @throws QueryParsingException
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryParsingException, IOException {
-        DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder();
-        assertNull(disMaxBuilder.toQuery(createShardContext()));
-        assertNull(disMaxBuilder.validate());
-    }
-
-    /**
-     * Test inner query parsing to null. Current DSL allows inner filter element to parse to <tt>null</tt>.
-     * Those should be ignored upstream. To test this, we use inner {@link ConstantScoreQueryBuilder}
-     * with empty inner filter.
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryId = ConstantScoreQueryBuilder.NAME;
-        String queryString = "{ \""+queryId+"\" : { \"filter\" : { } }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, queryId);
-        ConstantScoreQueryBuilder innerQueryBuilder = (ConstantScoreQueryBuilder) context.queryParser(queryId).fromXContent(context);
-
-        DisMaxQueryBuilder disMaxBuilder = new DisMaxQueryBuilder().add(innerQueryBuilder);
-        assertNull(disMaxBuilder.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        DisMaxQueryBuilder disMaxQuery = new DisMaxQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    disMaxQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    disMaxQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                disMaxQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(disMaxQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java
deleted file mode 100644
index ecbd3dc..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ExistsQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.index.mapper.object.ObjectMapper;
-
-import java.io.IOException;
-import java.util.Collection;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class ExistsQueryBuilderTest extends BaseQueryTestCase<ExistsQueryBuilder> {
-
-    @Override
-    protected ExistsQueryBuilder doCreateTestQueryBuilder() {
-        String fieldPattern;
-        if (randomBoolean()) {
-            fieldPattern = randomFrom(mappedFieldNames);
-        } else {
-            fieldPattern = randomAsciiOfLengthBetween(1, 10);
-        }
-        // also sometimes test wildcard patterns
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                fieldPattern = fieldPattern + "*";
-            } else {
-                fieldPattern = MetaData.ALL;
-            }
-        }
-        return new ExistsQueryBuilder(fieldPattern);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ExistsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        String fieldPattern = queryBuilder.fieldName();
-        ObjectMapper objectMapper = context.getObjectMapper(fieldPattern);
-        if (objectMapper != null) {
-            // automatic make the object mapper pattern
-            fieldPattern = fieldPattern + ".*";
-        }
-        Collection<String> fields = context.simpleMatchToIndexNames(fieldPattern);
-        if (getCurrentTypes().length == 0 || fields.size() == 0) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(0));
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) constantScoreQuery.getQuery();
-            assertThat(booleanQuery.clauses().size(), equalTo(fields.size()));
-            for (int i = 0; i < fields.size(); i++) {
-                BooleanClause booleanClause = booleanQuery.clauses().get(i);
-                assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java
deleted file mode 100644
index 133c6b1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FQueryFilterBuilderTest.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class FQueryFilterBuilderTest extends BaseQueryTestCase<FQueryFilterBuilder> {
-
-    /**
-     * @return a FQueryFilterBuilder with random inner query
-     */
-    @Override
-    protected FQueryFilterBuilder doCreateTestQueryBuilder() {
-        QueryBuilder innerQuery = RandomQueryBuilder.createQuery(random());
-        return new FQueryFilterBuilder(innerQuery);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FQueryFilterBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    /**
-     * test corner case where no inner query exist
-     */
-    @Test
-    public void testNoInnerQuery() throws QueryParsingException, IOException {
-        FQueryFilterBuilder queryFilterQuery = new FQueryFilterBuilder(EmptyQueryBuilder.PROTOTYPE);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    /**
-     * test wrapping an inner filter that returns null also returns <tt>null</null> to pass on upwards
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        QueryParseContext context = createParseContext();
-
-        // create inner filter
-        String queryString = "{ \"constant_score\" : { \"filter\" : {} }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, ConstantScoreQueryBuilder.NAME);
-        QueryBuilder innerQuery = context.queryParser(ConstantScoreQueryBuilder.NAME).fromXContent(context);
-
-        // check that when wrapping this filter, toQuery() returns null
-        FQueryFilterBuilder queryFilterQuery = new FQueryFilterBuilder(innerQuery);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        FQueryFilterBuilder fQueryFilter = new FQueryFilterBuilder(innerQuery);
-        assertValidate(fQueryFilter, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java
deleted file mode 100644
index 11291e0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilderTest.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
-import org.elasticsearch.index.mapper.MappedFieldType;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class FieldMaskingSpanQueryBuilderTest extends BaseQueryTestCase<FieldMaskingSpanQueryBuilder> {
-
-    @Override
-    protected FieldMaskingSpanQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName;
-        if (randomBoolean()) {
-            fieldName = randomFrom(mappedFieldNames);
-        } else {
-            fieldName = randomAsciiOfLengthBetween(1, 10);
-        }
-        SpanTermQueryBuilder innerQuery = new SpanTermQueryBuilderTest().createTestQueryBuilder();
-        return new FieldMaskingSpanQueryBuilder(innerQuery, fieldName);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FieldMaskingSpanQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        String fieldInQuery = queryBuilder.fieldName();
-        MappedFieldType fieldType = context.fieldMapper(fieldInQuery);
-        if (fieldType != null) {
-            fieldInQuery = fieldType.names().indexName();
-        }
-        assertThat(query, instanceOf(FieldMaskingSpanQuery.class));
-        FieldMaskingSpanQuery fieldMaskingSpanQuery = (FieldMaskingSpanQuery) query;
-        assertThat(fieldMaskingSpanQuery.getField(), equalTo(fieldInQuery));
-        assertThat(fieldMaskingSpanQuery.getMaskedQuery(), equalTo(queryBuilder.innerQuery().toQuery(context)));
-    }
-
-    @Test
-    public void testValidate() {
-        String fieldName = null;
-        SpanQueryBuilder spanQueryBuilder = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            fieldName = "fieldName";
-        } else {
-            if (randomBoolean()) {
-                fieldName = "";
-            }
-            totalExpectedErrors++;
-        }
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                spanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            }
-            totalExpectedErrors++;
-        } else {
-            spanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        FieldMaskingSpanQueryBuilder queryBuilder = new FieldMaskingSpanQueryBuilder(spanQueryBuilder, fieldName);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java
deleted file mode 100644
index 9eb7f5e..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FilteredQueryBuilderTest.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.lucene.search.Queries;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class FilteredQueryBuilderTest extends BaseQueryTestCase<FilteredQueryBuilder> {
-
-    @Override
-    protected FilteredQueryBuilder doCreateTestQueryBuilder() {
-        QueryBuilder queryBuilder = RandomQueryBuilder.createQuery(random());
-        QueryBuilder filterBuilder = RandomQueryBuilder.createQuery(random());
-        return new FilteredQueryBuilder(queryBuilder, filterBuilder);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FilteredQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            Query innerFilter = queryBuilder.innerFilter().toQuery(context);
-            if (innerFilter == null || Queries.isConstantMatchAllQuery(innerFilter)) {
-                innerQuery.setBoost(queryBuilder.boost());
-                assertThat(query, equalTo(innerQuery));
-            } else if (Queries.isConstantMatchAllQuery(innerQuery)) {
-                assertThat(query, instanceOf(ConstantScoreQuery.class));
-                assertThat(((ConstantScoreQuery)query).getQuery(), equalTo(innerFilter));
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(2));
-                assertThat(booleanQuery.clauses().get(0).getOccur(), equalTo(BooleanClause.Occur.MUST));
-                assertThat(booleanQuery.clauses().get(0).getQuery(), equalTo(innerQuery));
-                assertThat(booleanQuery.clauses().get(1).getOccur(), equalTo(BooleanClause.Occur.FILTER));
-                assertThat(booleanQuery.clauses().get(1).getQuery(), equalTo(innerFilter));
-            }
-        }
-    }
-
-    @Test
-    public void testValidation() {
-        QueryBuilder valid = RandomQueryBuilder.createQuery(random());
-        QueryBuilder invalid = RandomQueryBuilder.createInvalidQuery(random());
-
-        // invalid cases
-        FilteredQueryBuilder qb = new FilteredQueryBuilder(invalid);
-        QueryValidationException result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(valid, invalid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(invalid, valid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(1, result.validationErrors().size());
-
-        qb = new FilteredQueryBuilder(invalid, invalid);
-        result = qb.validate();
-        assertNotNull(result);
-        assertEquals(2, result.validationErrors().size());
-
-        // valid cases
-        qb = new FilteredQueryBuilder(valid);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(null);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(null, valid);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(valid, null);
-        assertNull(qb.validate());
-
-        qb = new FilteredQueryBuilder(valid, valid);
-        assertNull(qb.validate());
-    }
-
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTest.java
deleted file mode 100644
index 51ec297..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/FuzzyQueryBuilderTest.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.collect.Tuple;
-import org.elasticsearch.common.unit.Fuzziness;
-import org.hamcrest.Matchers;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class FuzzyQueryBuilderTest extends BaseQueryTestCase<FuzzyQueryBuilder> {
-
-    @Override
-    protected FuzzyQueryBuilder doCreateTestQueryBuilder() {
-        Tuple<String, Object> fieldAndValue = getRandomFieldNameAndValue(); 
-        FuzzyQueryBuilder query = new FuzzyQueryBuilder(fieldAndValue.v1(), fieldAndValue.v2());
-        if (randomBoolean()) {
-            query.fuzziness(randomFuzziness(query.fieldName()));
-        }
-        if (randomBoolean()) {
-            query.prefixLength(randomIntBetween(0, 10));
-        }
-        if (randomBoolean()) {
-            query.maxExpansions(randomIntBetween(1, 10));
-        }
-        if (randomBoolean()) {
-            query.transpositions(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.rewrite(getRandomRewriteMethod());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(FuzzyQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (isNumericFieldName(queryBuilder.fieldName()) || queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            assertThat(query, instanceOf(NumericRangeQuery.class));
-        } else {
-            assertThat(query, instanceOf(FuzzyQuery.class));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        FuzzyQueryBuilder fuzzyQueryBuilder = new FuzzyQueryBuilder("", "text");
-        assertThat(fuzzyQueryBuilder.validate().validationErrors().size(), is(1));
-
-        fuzzyQueryBuilder = new FuzzyQueryBuilder("field", null);
-        assertThat(fuzzyQueryBuilder.validate().validationErrors().size(), is(1));
-
-        fuzzyQueryBuilder = new FuzzyQueryBuilder("field", "text");
-        assertNull(fuzzyQueryBuilder.validate());
-
-        fuzzyQueryBuilder = new FuzzyQueryBuilder(null, null);
-        assertThat(fuzzyQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-    
-    @Test
-    public void testUnsupportedFuzzinessForStringType() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        
-        FuzzyQueryBuilder fuzzyQueryBuilder = new FuzzyQueryBuilder(STRING_FIELD_NAME, "text");
-        fuzzyQueryBuilder.fuzziness(Fuzziness.build(randomFrom("a string which is not auto", "3h", "200s")));
-
-        try {
-            fuzzyQueryBuilder.toQuery(context);
-            fail("should have failed with NumberFormatException");
-        } catch (NumberFormatException e) {
-            assertThat(e.getMessage(), Matchers.containsString("For input string"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java
deleted file mode 100644
index ecdab89..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTest.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-
-import org.apache.lucene.queries.TermsQuery;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class IdsQueryBuilderTest extends BaseQueryTestCase<IdsQueryBuilder> {
-
-    /**
-     * check that parser throws exception on missing values field
-     * @throws IOException
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testIdsNotProvided() throws IOException {
-        String noIdsFieldQuery = "{\"ids\" : { \"type\" : \"my_type\"  }";
-        XContentParser parser = XContentFactory.xContent(noIdsFieldQuery).createParser(noIdsFieldQuery);
-        QueryParseContext context = createParseContext();
-        context.reset(parser);
-        assertQueryHeader(parser, "ids");
-        context.queryParser("ids").fromXContent(context);
-    }
-
-    @Override
-    protected IdsQueryBuilder doCreateTestQueryBuilder() {
-        String[] types;
-        if (getCurrentTypes().length > 0 && randomBoolean()) {
-            int numberOfTypes = randomIntBetween(1, getCurrentTypes().length);
-            types = new String[numberOfTypes];
-            for (int i = 0; i < numberOfTypes; i++) {
-                if (frequently()) {
-                    types[i] = randomFrom(getCurrentTypes());
-                } else {
-                    types[i] = randomAsciiOfLengthBetween(1, 10);
-                }
-            }
-        } else {
-            if (randomBoolean()) {
-                types = new String[]{MetaData.ALL};
-            } else {
-                types = new String[0];
-            }
-        }
-        int numberOfIds = randomIntBetween(0, 10);
-        String[] ids = new String[numberOfIds];
-        for (int i = 0; i < numberOfIds; i++) {
-            ids[i] = randomAsciiOfLengthBetween(1, 10);
-        }
-        IdsQueryBuilder query;
-        if (types.length > 0 || randomBoolean()) {
-            query = new IdsQueryBuilder(types);
-            query.addIds(ids);
-        } else {
-            query = new IdsQueryBuilder();
-            query.addIds(ids);
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(IdsQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.ids().size() == 0) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            assertThat(((BooleanQuery)query).clauses().size(), equalTo(0));
-        } else {
-            assertThat(query, instanceOf(TermsQuery.class));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
index 6222f3b..d581aa6 100644
--- a/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java
@@ -83,7 +83,7 @@ public class IndexQueryParserFilterDateRangeTimezoneTests extends ESSingleNodeTe
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query();
             fail("A Range Filter on a numeric field with a TimeZone should raise a QueryParsingException");
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             // We expect it
         } finally {
             SearchContext.removeCurrent();
@@ -120,7 +120,7 @@ public class IndexQueryParserFilterDateRangeTimezoneTests extends ESSingleNodeTe
             SearchContext.setCurrent(new TestSearchContext());
             queryParser.parse(query).query();
             fail("A Range Query on a numeric field with a TimeZone should raise a QueryParsingException");
-        } catch (QueryShardException e) {
+        } catch (QueryParsingException e) {
             // We expect it
         } finally {
             SearchContext.removeCurrent();
diff --git a/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java
deleted file mode 100644
index 59bb644..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/LimitQueryBuilderTest.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class LimitQueryBuilderTest extends BaseQueryTestCase<LimitQueryBuilder> {
-
-    /**
-     * @return a LimitQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected LimitQueryBuilder doCreateTestQueryBuilder() {
-        return new LimitQueryBuilder(randomIntBetween(0, 20));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(LimitQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(MatchAllDocsQuery.class));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java
deleted file mode 100644
index 277717c..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MatchAllQueryBuilderTest.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class MatchAllQueryBuilderTest extends BaseQueryTestCase<MatchAllQueryBuilder> {
-
-    @Override
-    protected MatchAllQueryBuilder doCreateTestQueryBuilder() {
-        return new MatchAllQueryBuilder();
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MatchAllQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(MatchAllDocsQuery.class));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java
deleted file mode 100644
index becba55..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/MissingQueryBuilderTest.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.is;
-
-public class MissingQueryBuilderTest extends BaseQueryTestCase<MissingQueryBuilder> {
-
-    @Override
-    protected MissingQueryBuilder doCreateTestQueryBuilder() {
-        MissingQueryBuilder query  = new MissingQueryBuilder(randomBoolean() ? randomFrom(mappedFieldNames) : randomAsciiOfLengthBetween(1, 10));
-        if (randomBoolean()) {
-            query.nullValue(randomBoolean());
-        }
-        if (randomBoolean()) {
-            query.existence(randomBoolean());
-        }
-        // cannot set both to false
-        if ((query.nullValue() == false) && (query.existence() == false)) {
-            query.existence(!query.existence());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(MissingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        //too many mapping dependent cases to test, we don't want to end up duplication the toQuery method
-    }
-
-    @Test
-    public void testValidate() {
-        MissingQueryBuilder missingQueryBuilder = new MissingQueryBuilder("");
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder(null);
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder("field").existence(false).nullValue(false);
-        assertThat(missingQueryBuilder.validate().validationErrors().size(), is(1));
-
-        missingQueryBuilder = new MissingQueryBuilder("field");
-        assertNull(missingQueryBuilder.validate());
-    }
-
-    @Test(expected = QueryShardException.class)
-    public void testBothNullValueAndExistenceFalse() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-        MissingQueryBuilder.newFilter(context, "field", false, false);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java
deleted file mode 100644
index 158a141..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/NotQueryBuilderTest.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-public class NotQueryBuilderTest extends BaseQueryTestCase<NotQueryBuilder> {
-
-    /**
-     * @return a NotQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected NotQueryBuilder doCreateTestQueryBuilder() {
-        return new NotQueryBuilder(RandomQueryBuilder.createQuery(random()));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(NotQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query filter = queryBuilder.innerQuery().toQuery(context);
-        if (filter == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(2));
-            assertThat(booleanQuery.clauses().get(0).getOccur(), equalTo(BooleanClause.Occur.MUST));
-            assertThat(booleanQuery.clauses().get(0).getQuery(), instanceOf(MatchAllDocsQuery.class));
-            assertThat(booleanQuery.clauses().get(1).getOccur(), equalTo(BooleanClause.Occur.MUST_NOT));
-            assertThat(booleanQuery.clauses().get(1).getQuery(), equalTo(filter));
-        }
-    }
-
-    /**
-     * @throws IOException
-     */
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFilterSection() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"not\" : {}";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, NotQueryBuilder.NAME);
-        context.queryParser(NotQueryBuilder.NAME).fromXContent(context);
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        NotQueryBuilder notQuery = new NotQueryBuilder(innerQuery);
-        assertValidate(notQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java
deleted file mode 100644
index fa9ada0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/OrQueryBuilderTest.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class OrQueryBuilderTest extends BaseQueryTestCase<OrQueryBuilder> {
-
-/*
-    @Override
-    protected Query doCreateExpectedQuery(OrQueryBuilder queryBuilder, QueryCreationContext context) throws QueryCreationException, IOException {
-        if (queryBuilder.filters().isEmpty()) {
-            return null;
-        }
-        BooleanQuery query = new BooleanQuery();
-        for (QueryBuilder subQuery : queryBuilder.filters()) {
-            Query innerQuery = subQuery.toQuery(context);
-            // ignore queries that are null
-            if (innerQuery != null) {
-                query.add(innerQuery, Occur.SHOULD);
-            }
-        }
-        if (query.clauses().isEmpty()) {
-            return null;
-        }
-        return query;
-    }
-*/
-
-    /**
-     * @return an OrQueryBuilder with random limit between 0 and 20
-     */
-    @Override
-    protected OrQueryBuilder doCreateTestQueryBuilder() {
-        OrQueryBuilder query = new OrQueryBuilder();
-        int subQueries = randomIntBetween(1, 5);
-        for (int i = 0; i < subQueries; i++ ) {
-            query.add(RandomQueryBuilder.createQuery(random()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(OrQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (queryBuilder.innerQueries().isEmpty()) {
-            assertThat(query, nullValue());
-        } else {
-            List<Query> innerQueries = new ArrayList<>();
-            for (QueryBuilder subQuery : queryBuilder.innerQueries()) {
-                Query innerQuery = subQuery.toQuery(context);
-                // ignore queries that are null
-                if (innerQuery != null) {
-                    innerQueries.add(innerQuery);
-                }
-            }
-            if (innerQueries.isEmpty()) {
-                assertThat(query, nullValue());
-            } else {
-                assertThat(query, instanceOf(BooleanQuery.class));
-                BooleanQuery booleanQuery = (BooleanQuery) query;
-                assertThat(booleanQuery.clauses().size(), equalTo(innerQueries.size()));
-                Iterator<Query> queryIterator = innerQueries.iterator();
-                for (BooleanClause booleanClause : booleanQuery) {
-                    assertThat(booleanClause.getOccur(), equalTo(BooleanClause.Occur.SHOULD));
-                    assertThat(booleanClause.getQuery(), equalTo(queryIterator.next()));
-                }
-            }
-        }
-    }
-
-    /**
-     * test corner case where no inner queries exist
-     */
-    @Test
-    public void testNoInnerQueries() throws QueryShardException, IOException {
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        assertNull(orQuery.toQuery(createShardContext()));
-    }
-
-    @Test(expected=QueryParsingException.class)
-    public void testMissingFiltersSection() throws IOException {
-        QueryParseContext context = createParseContext();
-        String queryString = "{ \"or\" : {}";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, OrQueryBuilder.NAME);
-        context.queryParser(OrQueryBuilder.NAME).fromXContent(context);
-    }
-
-    @Test
-    public void testValidate() {
-        OrQueryBuilder orQuery = new OrQueryBuilder();
-        int iters = randomIntBetween(0, 5);
-        int totalExpectedErrors = 0;
-        for (int i = 0; i < iters; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    orQuery.add(RandomQueryBuilder.createInvalidQuery(random()));
-                } else {
-                    orQuery.add(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                orQuery.add(RandomQueryBuilder.createQuery(random()));
-            }
-        }
-        assertValidate(orQuery, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java
deleted file mode 100644
index 42fe5ab..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/PrefixQueryBuilderTest.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class PrefixQueryBuilderTest extends BaseQueryTestCase<PrefixQueryBuilder> {
-
-    @Override
-    protected PrefixQueryBuilder doCreateTestQueryBuilder() {
-        String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-        String value = randomAsciiOfLengthBetween(1, 10);
-        PrefixQueryBuilder query = new PrefixQueryBuilder(fieldName, value);
-
-        if (randomBoolean()) {
-            query.rewrite(getRandomRewriteMethod());
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(PrefixQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(PrefixQuery.class));
-        PrefixQuery prefixQuery = (PrefixQuery) query;
-        assertThat(prefixQuery.getPrefix().field(), equalTo(queryBuilder.fieldName()));
-    }
-
-    @Test
-    public void testValidate() {
-        PrefixQueryBuilder prefixQueryBuilder = new PrefixQueryBuilder("", "prefix");
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(1));
-
-        prefixQueryBuilder = new PrefixQueryBuilder("field", null);
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(1));
-
-        prefixQueryBuilder = new PrefixQueryBuilder("field", "prefix");
-        assertNull(prefixQueryBuilder.validate());
-
-        prefixQueryBuilder = new PrefixQueryBuilder(null, null);
-        assertThat(prefixQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-}
\ No newline at end of file
diff --git a/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java
deleted file mode 100644
index 6e3b7d8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/QueryFilterBuilderTest.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-import static org.hamcrest.CoreMatchers.nullValue;
-
-@SuppressWarnings("deprecation")
-public class QueryFilterBuilderTest extends BaseQueryTestCase<QueryFilterBuilder> {
-
-    @Override
-    protected QueryFilterBuilder doCreateTestQueryBuilder() {
-        QueryBuilder innerQuery = RandomQueryBuilder.createQuery(random());
-        return new QueryFilterBuilder(innerQuery);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(QueryFilterBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        Query innerQuery = queryBuilder.innerQuery().toQuery(context);
-        if (innerQuery == null) {
-            assertThat(query, nullValue());
-        } else {
-            assertThat(query, instanceOf(ConstantScoreQuery.class));
-            ConstantScoreQuery constantScoreQuery = (ConstantScoreQuery) query;
-            assertThat(constantScoreQuery.getQuery(), equalTo(innerQuery));
-        }
-    }
-
-    @Override
-    protected boolean supportsBoostAndQueryName() {
-        return false;
-    }
-
-    /**
-     * test wrapping an inner filter that returns null also returns <tt>null</null> to pass on upwards
-     */
-    @Test
-    public void testInnerQueryReturnsNull() throws IOException {
-        QueryParseContext context = createParseContext();
-
-        // create inner filter
-        String queryString = "{ \"constant_score\" : { \"filter\" : {} }";
-        XContentParser parser = XContentFactory.xContent(queryString).createParser(queryString);
-        context.reset(parser);
-        assertQueryHeader(parser, ConstantScoreQueryBuilder.NAME);
-        QueryBuilder innerQuery = context.queryParser(ConstantScoreQueryBuilder.NAME).fromXContent(context);
-
-        // check that when wrapping this filter, toQuery() returns null
-        QueryFilterBuilder queryFilterQuery = new QueryFilterBuilder(innerQuery);
-        assertNull(queryFilterQuery.toQuery(createShardContext()));
-    }
-
-    @Test
-    public void testValidate() {
-        QueryBuilder innerQuery = null;
-        int totalExpectedErrors = 0;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerQuery = RandomQueryBuilder.createInvalidQuery(random());
-            }
-            totalExpectedErrors++;
-        } else {
-            innerQuery = RandomQueryBuilder.createQuery(random());
-        }
-        QueryFilterBuilder fQueryFilter = new QueryFilterBuilder(innerQuery);
-        assertValidate(fQueryFilter, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java b/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java
deleted file mode 100644
index e86a0ec..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RandomQueryBuilder.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-import com.carrotsearch.randomizedtesting.generators.RandomStrings;
-
-import java.util.Random;
-
-/**
- * Utility class for creating random QueryBuilders.
- * So far only leaf queries like {@link MatchAllQueryBuilder}, {@link TermQueryBuilder} or
- * {@link IdsQueryBuilder} are returned.
- */
-public class RandomQueryBuilder {
-
-    /**
-     * Create a new query of a random type
-     * @param r random seed
-     * @return a random {@link QueryBuilder}
-     */
-    public static QueryBuilder createQuery(Random r) {
-        switch (RandomInts.randomIntBetween(r, 0, 4)) {
-            case 0:
-                return new MatchAllQueryBuilderTest().createTestQueryBuilder();
-            case 1:
-                return new TermQueryBuilderTest().createTestQueryBuilder();
-            case 2:
-                return new IdsQueryBuilderTest().createTestQueryBuilder();
-            case 3:
-                return createMultiTermQuery(r);
-            case 4:
-                return EmptyQueryBuilder.PROTOTYPE;
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-
-    /**
-     * Create a new multi term query of a random type
-     * @param r random seed
-     * @return a random {@link MultiTermQueryBuilder}
-     */
-    public static MultiTermQueryBuilder createMultiTermQuery(Random r) {
-        // for now, only use String Rangequeries for MultiTerm test, numeric and date makes little sense
-        // see issue #12123 for discussion
-        // Prefix / Fuzzy / RegEx / Wildcard can go here later once refactored and they have random query generators
-        RangeQueryBuilder query = new RangeQueryBuilder(BaseQueryTestCase.STRING_FIELD_NAME);
-        query.from("a" + RandomStrings.randomAsciiOfLengthBetween(r, 1, 10));
-        query.to("z" + RandomStrings.randomAsciiOfLengthBetween(r, 1, 10));
-        return query;
-    }
-
-    /**
-     * Create a new invalid query of a random type
-     * @param r random seed
-     * @return a random {@link QueryBuilder} that is invalid, meaning that calling validate against it
-     * will return an error. We can rely on the fact that a single error will be returned per query.
-     */
-    public static QueryBuilder createInvalidQuery(Random r) {
-        switch (RandomInts.randomIntBetween(r, 0, 3)) {
-            case 0:
-                return new TermQueryBuilder("", "test");
-            case 1:
-                return new BoostingQueryBuilder(new MatchAllQueryBuilder(), new MatchAllQueryBuilder()).negativeBoost(-1f);
-            case 2:
-                return new CommonTermsQueryBuilder("", "text");
-            case 3:
-                return new SimpleQueryStringBuilder(null);
-            default:
-                throw new UnsupportedOperationException();
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java
deleted file mode 100644
index 00753d8..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RangeQueryBuilderTest.java
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.joda.time.DateTime;
-import org.joda.time.DateTimeZone;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class RangeQueryBuilderTest extends BaseQueryTestCase<RangeQueryBuilder> {
-
-    private static final List<String> TIMEZONE_IDS = new ArrayList<>(DateTimeZone.getAvailableIDs());
-
-    @Override
-    protected RangeQueryBuilder doCreateTestQueryBuilder() {
-        RangeQueryBuilder query;
-        // switch between numeric and date ranges
-        switch (randomIntBetween(0, 2)) {
-            case 0:
-                if (randomBoolean()) {
-                    // use mapped integer field for numeric range queries
-                    query = new RangeQueryBuilder(INT_FIELD_NAME);
-                    query.from(randomIntBetween(1, 100));
-                    query.to(randomIntBetween(101, 200));
-                } else {
-                    // use unmapped field for numeric range queries
-                    query = new RangeQueryBuilder(randomAsciiOfLengthBetween(1, 10));
-                    query.from(0.0 - randomDouble());
-                    query.to(randomDouble());
-                }
-                break;
-            case 1:
-                // use mapped date field, using date string representation
-                query = new RangeQueryBuilder(DATE_FIELD_NAME);
-                query.from(new DateTime(System.currentTimeMillis() - randomIntBetween(0, 1000000), DateTimeZone.UTC).toString());
-                query.to(new DateTime(System.currentTimeMillis() + randomIntBetween(0, 1000000), DateTimeZone.UTC).toString());
-                // Create timestamp option only then we have a date mapper,
-                // otherwise we could trigger exception.
-                if (createShardContext().mapperService().smartNameFieldType(DATE_FIELD_NAME) != null) {
-                    if (randomBoolean()) {
-                        query.timeZone(TIMEZONE_IDS.get(randomIntBetween(0, TIMEZONE_IDS.size() - 1)));
-                    }
-                    if (randomBoolean()) {
-                        query.format("yyyy-MM-dd'T'HH:mm:ss.SSSZZ");
-                    }
-                }
-                break;
-            case 2:
-            default:
-                query = new RangeQueryBuilder(STRING_FIELD_NAME);
-                query.from("a" + randomAsciiOfLengthBetween(1, 10));
-                query.to("z" + randomAsciiOfLengthBetween(1, 10));
-                break;
-        }
-        query.includeLower(randomBoolean()).includeUpper(randomBoolean());
-        if (randomBoolean()) {
-            query.from(null);
-        }
-        if (randomBoolean()) {
-            query.to(null);
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(RangeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        if (getCurrentTypes().length == 0 || (queryBuilder.fieldName().equals(DATE_FIELD_NAME) == false && queryBuilder.fieldName().equals(INT_FIELD_NAME) == false)) {
-            assertThat(query, instanceOf(TermRangeQuery.class));
-        } else if (queryBuilder.fieldName().equals(DATE_FIELD_NAME)) {
-            //we can't properly test unmapped dates because LateParsingQuery is package private
-        } else if (queryBuilder.fieldName().equals(INT_FIELD_NAME)) {
-            assertThat(query, instanceOf(NumericRangeQuery.class));
-        } else {
-            throw new UnsupportedOperationException();
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder("");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder = new RangeQueryBuilder("okay").timeZone("UTC");
-        assertNull(rangeQueryBuilder.validate());
-
-        rangeQueryBuilder.timeZone("blab");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder.timeZone("UTC").format("basicDate");
-        assertNull(rangeQueryBuilder.validate());
-
-        rangeQueryBuilder.timeZone("UTC").format("broken_xx");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(1));
-
-        rangeQueryBuilder.timeZone("xXx").format("broken_xx");
-        assertThat(rangeQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-
-    /**
-     * Specifying a timezone together with a numeric range query should throw an exception.
-     */
-    @Test(expected=QueryShardException.class)
-    public void testToQueryNonDateWithTimezone() throws QueryShardException, IOException {
-        RangeQueryBuilder query = new RangeQueryBuilder(INT_FIELD_NAME);
-        query.from(1).to(10).timeZone("UTC");
-        query.toQuery(createShardContext());
-    }
-
-    /**
-     * Specifying a timezone together with an unmapped field should throw an exception.
-     */
-    @Test(expected=QueryShardException.class)
-    public void testToQueryUnmappedWithTimezone() throws QueryShardException, IOException {
-        RangeQueryBuilder query = new RangeQueryBuilder("bogus_field");
-        query.from(1).to(10).timeZone("UTC");
-        query.toQuery(createShardContext());
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java
deleted file mode 100644
index 9328609..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/RegexpQueryBuilderTest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class RegexpQueryBuilderTest extends BaseQueryTestCase<RegexpQueryBuilder> {
-
-    @Override
-    protected RegexpQueryBuilder doCreateTestQueryBuilder() {
-        // mapped or unmapped fields
-        String fieldName = randomBoolean() ? STRING_FIELD_NAME : randomAsciiOfLengthBetween(1, 10);
-        String value = randomAsciiOfLengthBetween(1, 10);
-        RegexpQueryBuilder query = new RegexpQueryBuilder(fieldName, value);
-
-        if (randomBoolean()) {
-            List<RegexpFlag> flags = new ArrayList<>();
-            int iter = randomInt(5);
-            for (int i = 0; i < iter; i++) {
-                flags.add(randomFrom(RegexpFlag.values()));
-            }
-            query.flags(flags.toArray(new RegexpFlag[flags.size()]));
-        }
-        if (randomBoolean()) {
-            query.maxDeterminizedStates(randomInt(50000));
-        }
-        if (randomBoolean()) {
-            query.rewrite(randomFrom(getRandomRewriteMethod()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(RegexpQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(RegexpQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        RegexpQueryBuilder regexQueryBuilder = new RegexpQueryBuilder("", "regex");
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(1));
-
-        regexQueryBuilder = new RegexpQueryBuilder("field", null);
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(1));
-
-        regexQueryBuilder = new RegexpQueryBuilder("field", "regex");
-        assertNull(regexQueryBuilder.validate());
-
-        regexQueryBuilder = new RegexpQueryBuilder(null, null);
-        assertThat(regexQueryBuilder.validate().validationErrors().size(), is(2));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java
deleted file mode 100644
index dcc74d1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/ScriptQueryBuilderTest.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.elasticsearch.script.Script;
-import org.elasticsearch.script.ScriptService.ScriptType;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class ScriptQueryBuilderTest extends BaseQueryTestCase<ScriptQueryBuilder> {
-
-    @Override
-    protected ScriptQueryBuilder doCreateTestQueryBuilder() {
-        String script;
-        Map<String, Object> params = null;
-        if (randomBoolean()) {
-            script = "5 * 2 > param";
-            params = new HashMap<>();
-            params.put("param", 1);
-        } else {
-            script = "5 * 2 > 2";
-        }
-        return new ScriptQueryBuilder(new Script(script, ScriptType.INLINE, "expression", params));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(ScriptQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(ScriptQueryBuilder.ScriptQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        ScriptQueryBuilder scriptQueryBuilder = new ScriptQueryBuilder(null);
-        assertThat(scriptQueryBuilder.validate().validationErrors().size(), is(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
index 6ba7ca0..92f766d 100644
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java
@@ -38,6 +38,7 @@ import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.automaton.TooComplexToDeterminizeException;
 import org.elasticsearch.action.termvectors.*;
+import org.elasticsearch.cluster.metadata.MetaData;
 import org.elasticsearch.common.Strings;
 import org.elasticsearch.common.bytes.BytesArray;
 import org.elasticsearch.common.compress.CompressedXContent;
@@ -72,6 +73,7 @@ import java.io.IOException;
 import java.util.Arrays;
 import java.util.EnumSet;
 import java.util.List;
+import java.util.Locale;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.*;
@@ -969,7 +971,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testBoostingQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(boostingQuery(termQuery("field1", "value1"), termQuery("field1", "value2")).negativeBoost(0.2f)).query();
+        Query parsedQuery = queryParser.parse(boostingQuery().positive(termQuery("field1", "value1")).negative(termQuery("field1", "value2")).negativeBoost(0.2f)).query();
         assertThat(parsedQuery, instanceOf(BoostingQuery.class));
     }
 
@@ -1351,7 +1353,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanNotQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanNotQuery(spanTermQuery("age", 34), spanTermQuery("age", 35))).query();
+        Query parsedQuery = queryParser.parse(spanNotQuery().include(spanTermQuery("age", 34)).exclude(spanTermQuery("age", 35))).query();
         assertThat(parsedQuery, instanceOf(SpanNotQuery.class));
         SpanNotQuery spanNotQuery = (SpanNotQuery) parsedQuery;
         // since age is automatically registered in data, we encode it as numeric
@@ -1376,7 +1378,9 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         IndexQueryParserService queryParser = queryParser();
         Query expectedQuery = new SpanWithinQuery(new SpanTermQuery(new Term("age", longToPrefixCoded(34, 0))),
                                                   new SpanTermQuery(new Term("age", longToPrefixCoded(35, 0))));
-        Query actualQuery = queryParser.parse(spanWithinQuery(spanTermQuery("age", 34), spanTermQuery("age", 35)))
+        Query actualQuery = queryParser.parse(spanWithinQuery()
+                                              .big(spanTermQuery("age", 34))
+                                              .little(spanTermQuery("age", 35)))
                                               .query();
         assertEquals(expectedQuery, actualQuery);
     }
@@ -1396,7 +1400,10 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         IndexQueryParserService queryParser = queryParser();
         Query expectedQuery = new SpanContainingQuery(new SpanTermQuery(new Term("age", longToPrefixCoded(34, 0))),
                                                       new SpanTermQuery(new Term("age", longToPrefixCoded(35, 0))));
-        Query actualQuery = queryParser.parse(spanContainingQuery(spanTermQuery("age", 34), spanTermQuery("age", 35))).query();
+        Query actualQuery = queryParser.parse(spanContainingQuery()
+                                              .big(spanTermQuery("age", 34))
+                                              .little(spanTermQuery("age", 35)))
+                                              .query();
         assertEquals(expectedQuery, actualQuery);
     }
 
@@ -1436,7 +1443,7 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     @Test
     public void testSpanNearQueryBuilder() throws IOException {
         IndexQueryParserService queryParser = queryParser();
-        Query parsedQuery = queryParser.parse(spanNearQuery(12).clause(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36)).inOrder(false).collectPayloads(false)).query();
+        Query parsedQuery = queryParser.parse(spanNearQuery().clause(spanTermQuery("age", 34)).clause(spanTermQuery("age", 35)).clause(spanTermQuery("age", 36)).slop(12).inOrder(false).collectPayloads(false)).query();
         assertThat(parsedQuery, instanceOf(SpanNearQuery.class));
         SpanNearQuery spanNearQuery = (SpanNearQuery) parsedQuery;
         assertThat(spanNearQuery.getClauses().length, equalTo(3));
@@ -2311,6 +2318,14 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
     }
 
     @Test
+    public void testSimpleQueryString() throws Exception {
+        IndexQueryParserService queryParser = queryParser();
+        String query = copyToStringFromClasspath("/org/elasticsearch/index/query/simple-query-string.json");
+        Query parsedQuery = queryParser.parse(query).query();
+        assertThat(parsedQuery, instanceOf(BooleanQuery.class));
+    }
+
+    @Test
     public void testMatchWithFuzzyTranspositions() throws Exception {
         IndexQueryParserService queryParser = queryParser();
         String query = copyToStringFromClasspath("/org/elasticsearch/index/query/match-with-fuzzy-transpositions.json");
@@ -2440,8 +2455,8 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
         assertThat(((ConstantScoreQuery) parsedQuery).getQuery().toString(), equalTo("ToParentBlockJoinQuery (+*:* #random_access(QueryWrapperFilter(_type:__nested)))"));
         SearchContext.removeCurrent();
     }
-
-    /**
+    
+    /** 
      * helper to extract term from TermQuery. */
     private Term getTerm(Query query) {
         while (query instanceof QueryWrapperFilter) {
@@ -2493,4 +2508,19 @@ public class SimpleIndexQueryParserTests extends ESSingleNodeTestCase {
             assertThat(prefixQuery.getRewriteMethod(), instanceOf(MultiTermQuery.TopTermsBlendedFreqScoringRewrite.class));
         }
     }
+
+    @Test
+    public void testSimpleQueryStringNoFields() throws Exception {
+        IndexQueryParserService queryParser = queryParser();
+        String queryText = randomAsciiOfLengthBetween(1, 10).toLowerCase(Locale.ROOT);
+        String query = "{\n" +
+                "    \"simple_query_string\" : {\n" +
+                "        \"query\" : \"" + queryText + "\"\n" +
+                "    }\n" +
+                "}";
+        Query parsedQuery = queryParser.parse(query).query();
+        assertThat(parsedQuery, instanceOf(TermQuery.class));
+        TermQuery termQuery = (TermQuery) parsedQuery;
+        assertThat(termQuery.getTerm(), equalTo(new Term(MetaData.ALL, queryText)));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java
deleted file mode 100644
index 69fa288..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SimpleQueryStringBuilderTest.java
+++ /dev/null
@@ -1,280 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.Version;
-import org.elasticsearch.cluster.metadata.MetaData;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.*;
-
-import static org.hamcrest.Matchers.*;
-
-public class SimpleQueryStringBuilderTest extends BaseQueryTestCase<SimpleQueryStringBuilder> {
-
-    private static final String[] MINIMUM_SHOULD_MATCH = new String[] { "1", "-1", "75%", "-25%", "2<75%", "2<-25%" };
-
-    @Override
-    protected SimpleQueryStringBuilder doCreateTestQueryBuilder() {
-        SimpleQueryStringBuilder result = new SimpleQueryStringBuilder(randomAsciiOfLengthBetween(1, 10));
-        if (randomBoolean()) {
-            result.analyzeWildcard(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.lenient(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.lowercaseExpandedTerms(randomBoolean());
-        }
-        if (randomBoolean()) {
-            result.locale(randomLocale(getRandom()));
-        }
-        if (randomBoolean()) {
-            result.minimumShouldMatch(randomFrom(MINIMUM_SHOULD_MATCH));
-        }
-        if (randomBoolean()) {
-            result.analyzer("simple");
-        }
-        if (randomBoolean()) {
-            result.defaultOperator(randomFrom(Operator.AND, Operator.OR));
-        }
-        if (randomBoolean()) {
-            Set<SimpleQueryStringFlag> flagSet = new HashSet<>();
-            int size = randomIntBetween(0, SimpleQueryStringFlag.values().length);
-            for (int i = 0; i < size; i++) {
-                flagSet.add(randomFrom(SimpleQueryStringFlag.values()));
-            }
-            if (flagSet.size() > 0) {
-                result.flags(flagSet.toArray(new SimpleQueryStringFlag[flagSet.size()]));
-            }
-        }
-
-        int fieldCount = randomIntBetween(0, 10);
-        Map<String, Float> fields = new TreeMap<>();
-        for (int i = 0; i < fieldCount; i++) {
-            if (randomBoolean()) {
-                fields.put(randomAsciiOfLengthBetween(1, 10), AbstractQueryBuilder.DEFAULT_BOOST);
-            } else {
-                fields.put(randomAsciiOfLengthBetween(1, 10), 2.0f / randomIntBetween(1, 20));
-            }
-        }
-        result.fields(fields);
-
-        return result;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SimpleQueryStringBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, notNullValue());
-        if (queryBuilder.fields().size() > 1) {
-            assertThat(query, instanceOf(BooleanQuery.class));
-            BooleanQuery booleanQuery = (BooleanQuery) query;
-            assertThat(booleanQuery.clauses().size(), equalTo(queryBuilder.fields().size()));
-            Iterator<String> fields = queryBuilder.fields().keySet().iterator();
-            for (BooleanClause booleanClause : booleanQuery) {
-                assertThat(booleanClause.getQuery(), instanceOf(TermQuery.class));
-                TermQuery termQuery = (TermQuery) booleanClause.getQuery();
-                assertThat(termQuery.getTerm(), equalTo(new Term(fields.next(), queryBuilder.value().toLowerCase(Locale.ROOT))));
-            }
-        } else {
-            assertThat(query, instanceOf(TermQuery.class));
-            String field;
-            if (queryBuilder.fields().size() == 0) {
-                field = MetaData.ALL;
-            } else {
-                field = queryBuilder.fields().keySet().iterator().next();
-            }
-            TermQuery termQuery = (TermQuery) query;
-            assertThat(termQuery.getTerm(), equalTo(new Term(field, queryBuilder.value().toLowerCase(Locale.ROOT))));
-        }
-    }
-
-    @Test
-    public void testDefaults() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-
-        assertEquals("Wrong default default boost.", AbstractQueryBuilder.DEFAULT_BOOST, qb.boost(), 0.001);
-        assertEquals("Wrong default default boost field.", AbstractQueryBuilder.DEFAULT_BOOST, SimpleQueryStringBuilder.DEFAULT_BOOST, 0.001);
-
-        assertEquals("Wrong default flags.", SimpleQueryStringFlag.ALL.value, qb.flags());
-        assertEquals("Wrong default flags field.", SimpleQueryStringFlag.ALL.value(), SimpleQueryStringBuilder.DEFAULT_FLAGS);
-
-        assertEquals("Wrong default default operator.", Operator.OR, qb.defaultOperator());
-        assertEquals("Wrong default default operator field.", Operator.OR, SimpleQueryStringBuilder.DEFAULT_OPERATOR);
-
-        assertEquals("Wrong default default locale.", Locale.ROOT, qb.locale());
-        assertEquals("Wrong default default locale field.", Locale.ROOT, SimpleQueryStringBuilder.DEFAULT_LOCALE);
-
-        assertEquals("Wrong default default analyze_wildcard.", false, qb.analyzeWildcard());
-        assertEquals("Wrong default default analyze_wildcard field.", false, SimpleQueryStringBuilder.DEFAULT_ANALYZE_WILDCARD);
-
-        assertEquals("Wrong default default lowercase_expanded_terms.", true, qb.lowercaseExpandedTerms());
-        assertEquals("Wrong default default lowercase_expanded_terms field.", true, SimpleQueryStringBuilder.DEFAULT_LOWERCASE_EXPANDED_TERMS);
-
-        assertEquals("Wrong default default lenient.", false, qb.lenient());
-        assertEquals("Wrong default default lenient field.", false, SimpleQueryStringBuilder.DEFAULT_LENIENT);
-
-        assertEquals("Wrong default default locale.", Locale.ROOT, qb.locale());
-        assertEquals("Wrong default default locale field.", Locale.ROOT, SimpleQueryStringBuilder.DEFAULT_LOCALE);
-    }
-
-    @Test
-    public void testDefaultNullLocale() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.locale(null);
-        assertEquals("Setting locale to null should result in returning to default value.",
-                SimpleQueryStringBuilder.DEFAULT_LOCALE, qb.locale());
-    }
-
-    @Test
-    public void testDefaultNullComplainFlags() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.flags((SimpleQueryStringFlag[]) null);
-        assertEquals("Setting flags to null should result in returning to default value.",
-                SimpleQueryStringBuilder.DEFAULT_FLAGS, qb.flags());
-    }
-
-    @Test
-    public void testDefaultEmptyComplainFlags() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.flags(new SimpleQueryStringFlag[]{});
-        assertEquals("Setting flags to empty should result in returning to default value.",
-                SimpleQueryStringBuilder.DEFAULT_FLAGS, qb.flags());
-    }
-
-    @Test
-    public void testDefaultNullComplainOp() {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.");
-        qb.defaultOperator(null);
-        assertEquals("Setting operator to null should result in returning to default value.",
-                SimpleQueryStringBuilder.DEFAULT_OPERATOR, qb.defaultOperator());
-    }
-
-    // Check operator handling, and default field handling.
-    @Test
-    public void testDefaultOperatorHandling() throws IOException {
-        SimpleQueryStringBuilder qb = new SimpleQueryStringBuilder("The quick brown fox.").field(STRING_FIELD_NAME);
-        QueryShardContext shardContext = createShardContext();
-        shardContext.setAllowUnmappedFields(true); // to avoid occasional cases in setup where we didn't add types but strict field resolution
-        BooleanQuery boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(4));
-
-        qb.defaultOperator(Operator.AND);
-        boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(0));
-
-        qb.defaultOperator(Operator.OR);
-        boolQuery = (BooleanQuery) qb.toQuery(shardContext);
-        assertThat(shouldClauses(boolQuery), is(4));
-    }
-
-    @Test
-    public void testValidation() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        assertNull(qb.validate());
-    }
-
-    @Test
-    public void testNullQueryTextGeneratesException() {
-        SimpleQueryStringBuilder builder = new SimpleQueryStringBuilder(null);
-        QueryValidationException exception = builder.validate();
-        assertThat(exception, notNullValue());
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeNull() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field(null);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeNullAndWeighted() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field(null, AbstractQueryBuilder.DEFAULT_BOOST);
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeEmpty() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field("");
-    }
-
-    @Test(expected = IllegalArgumentException.class)
-    public void testFieldCannotBeEmptyAndWeighted() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.field("", AbstractQueryBuilder.DEFAULT_BOOST);
-    }
-
-    /**
-     * The following should fail fast - never silently set the map containing
-     * fields and weights to null but refuse to accept null instead.
-     * */
-    @Test(expected = NullPointerException.class)
-    public void testFieldsCannotBeSetToNull() {
-        SimpleQueryStringBuilder qb = createTestQueryBuilder();
-        qb.fields(null);
-    }
-
-    @Test
-    public void testDefaultFieldParsing() throws IOException {
-        QueryParseContext context = createParseContext();
-        String query = randomAsciiOfLengthBetween(1, 10).toLowerCase(Locale.ROOT);
-        String contentString = "{\n" +
-                "    \"simple_query_string\" : {\n" +
-                "      \"query\" : \"" + query + "\"" +
-                "    }\n" +
-                "}";
-        XContentParser parser = XContentFactory.xContent(contentString).createParser(contentString);
-        context.reset(parser);
-        SimpleQueryStringBuilder queryBuilder = new SimpleQueryStringParser().fromXContent(context);
-        assertThat(queryBuilder.value(), equalTo(query));
-        assertThat(queryBuilder.fields(), notNullValue());
-        assertThat(queryBuilder.fields().size(), equalTo(0));
-        QueryShardContext shardContext = createShardContext();
-
-        // the remaining tests requires either a mapping that we register with types in base test setup
-        // no strict field resolution (version before V_1_4_0_Beta1)
-        if (getCurrentTypes().length > 0 || shardContext.indexQueryParserService().getIndexCreatedVersion().before(Version.V_1_4_0_Beta1)) {
-            Query luceneQuery = queryBuilder.toQuery(shardContext);
-            assertThat(luceneQuery, instanceOf(TermQuery.class));
-            TermQuery termQuery = (TermQuery) luceneQuery;
-            assertThat(termQuery.getTerm(), equalTo(new Term(MetaData.ALL, query)));
-        }
-    }
-
-    private static int shouldClauses(BooleanQuery query) {
-        int result = 0;
-        for (BooleanClause c : query.clauses()) {
-            if (c.getOccur() == BooleanClause.Occur.SHOULD) {
-                result++;
-            }
-        }
-        return result;
-    }
-}
-
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java
deleted file mode 100644
index 7429023..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanContainingQueryBuilderTest.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanContainingQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanContainingQueryBuilderTest extends BaseQueryTestCase<SpanContainingQueryBuilder> {
-
-    @Override
-    protected SpanContainingQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        return new SpanContainingQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanContainingQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanContainingQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder bigSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                bigSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                bigSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            bigSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder littleSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                littleSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                littleSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            littleSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanContainingQueryBuilder queryBuilder = new SpanContainingQueryBuilder(bigSpanQueryBuilder, littleSpanQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java
deleted file mode 100644
index d99010f..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanFirstQueryBuilderTest.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.hamcrest.CoreMatchers.*;
-
-public class SpanFirstQueryBuilderTest extends BaseQueryTestCase<SpanFirstQueryBuilder> {
-
-    @Override
-    protected SpanFirstQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(1);
-        return new SpanFirstQueryBuilder(spanTermQueries[0], randomIntBetween(0, 1000));
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanFirstQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanFirstQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder innerSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                innerSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                innerSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            innerSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        int end = randomIntBetween(0, 10);
-        if (randomBoolean()) {
-            end = randomIntBetween(-10, -1);
-            totalExpectedErrors++;
-        }
-        SpanFirstQueryBuilder queryBuilder = new SpanFirstQueryBuilder(innerSpanQueryBuilder, end);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    /**
-     * test exception on missing `end` and `match` parameter in parser
-     */
-    @Test
-    public void testParseEnd() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject(SpanFirstQueryBuilder.NAME);
-        builder.field("match");
-        spanTermQuery("description", "jumped").toXContent(builder, null);
-        builder.endObject();
-        builder.endObject();
-
-        QueryParseContext context = createParseContext();
-        XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-        context.reset(parser);
-        assertQueryHeader(parser, SpanFirstQueryBuilder.NAME);
-        try {
-            new SpanFirstQueryParser().fromXContent(context);
-            fail("missing [end] parameter should raise exception");
-        } catch (QueryParsingException e) {
-            assertTrue(e.getMessage().contains("spanFirst must have [end] set"));
-        }
-
-        builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject(SpanFirstQueryBuilder.NAME);
-        builder.field("end", 10);
-        builder.endObject();
-        builder.endObject();
-
-        context = createParseContext();
-        parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-        context.reset(parser);
-        assertQueryHeader(parser, SpanFirstQueryBuilder.NAME);
-        try {
-            new SpanFirstQueryParser().fromXContent(context);
-            fail("missing [match] parameter should raise exception");
-        } catch (QueryParsingException e) {
-            assertTrue(e.getMessage().contains("spanFirst must have [match] span query clause"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java
deleted file mode 100644
index b112a08..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanMultiTermQueryBuilderTest.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanMultiTermQueryBuilderTest extends BaseQueryTestCase<SpanMultiTermQueryBuilder> {
-
-    @Override
-    protected SpanMultiTermQueryBuilder doCreateTestQueryBuilder() {
-        MultiTermQueryBuilder multiTermQueryBuilder = RandomQueryBuilder.createMultiTermQuery(random());
-        return new SpanMultiTermQueryBuilder(multiTermQueryBuilder);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanMultiTermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanMultiTermQueryWrapper.class));
-        SpanMultiTermQueryWrapper spanMultiTermQueryWrapper = (SpanMultiTermQueryWrapper) query;
-        Query multiTermQuery = queryBuilder.innerQuery().toQuery(context);
-        assertThat(multiTermQuery, instanceOf(MultiTermQuery.class));
-        assertThat(spanMultiTermQueryWrapper.getWrappedQuery(), equalTo(new SpanMultiTermQueryWrapper<>((MultiTermQuery)multiTermQuery).getWrappedQuery()));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        MultiTermQueryBuilder multiTermQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                multiTermQueryBuilder = new RangeQueryBuilder("");
-            } else {
-                multiTermQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            multiTermQueryBuilder = new RangeQueryBuilder("field");
-        }
-        SpanMultiTermQueryBuilder queryBuilder = new SpanMultiTermQueryBuilder(multiTermQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    /**
-     * test checks that we throw an {@link UnsupportedOperationException} if the query wrapped
-     * by {@link SpanMultiTermQueryBuilder} does not generate a lucene {@link MultiTermQuery}.
-     * This is currently the case for {@link RangeQueryBuilder} when the target field is mapped
-     * to a date.
-     */
-    @Test
-    public void testUnsupportedInnerQueryType() throws IOException {
-        QueryShardContext context = createShardContext();
-        // test makes only sense if we have at least one type registered with date field mapping
-        if (getCurrentTypes().length > 0 && context.fieldMapper(DATE_FIELD_NAME) != null) {
-            try {
-                RangeQueryBuilder query = new RangeQueryBuilder(DATE_FIELD_NAME);
-                new SpanMultiTermQueryBuilder(query).toQuery(createShardContext());
-                fail("Exception expected, range query on date fields should not generate a lucene " + MultiTermQuery.class.getName());
-            } catch (UnsupportedOperationException e) {
-                assert(e.getMessage().contains("unsupported inner query, should be " + MultiTermQuery.class.getName()));
-            }
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java
deleted file mode 100644
index d2eb1a0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanNearQueryBuilderTest.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanNearQueryBuilderTest extends BaseQueryTestCase<SpanNearQueryBuilder> {
-
-    @Override
-    protected SpanNearQueryBuilder doCreateTestQueryBuilder() {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(randomIntBetween(-10, 10));
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(randomIntBetween(1, 6));
-        for (SpanTermQueryBuilder clause : spanTermQueries) {
-            queryBuilder.clause(clause);
-        }
-        queryBuilder.inOrder(randomBoolean());
-        queryBuilder.collectPayloads(randomBoolean());
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanNearQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanNearQuery.class));
-        SpanNearQuery spanNearQuery = (SpanNearQuery) query;
-        assertThat(spanNearQuery.getSlop(), equalTo(queryBuilder.slop()));
-        assertThat(spanNearQuery.isInOrder(), equalTo(queryBuilder.inOrder()));
-        assertThat(spanNearQuery.getClauses().length, equalTo(queryBuilder.clauses().size()));
-        Iterator<SpanQueryBuilder> spanQueryBuilderIterator = queryBuilder.clauses().iterator();
-        for (SpanQuery spanQuery : spanNearQuery.getClauses()) {
-            assertThat(spanQuery, equalTo(spanQueryBuilderIterator.next().toQuery(context)));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        SpanNearQueryBuilder queryBuilder = new SpanNearQueryBuilder(1);
-        assertValidate(queryBuilder, 1); // empty clause list
-
-        int totalExpectedErrors = 0;
-        int clauses = randomIntBetween(1, 10);
-        for (int i = 0; i < clauses; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    queryBuilder.clause(new SpanTermQueryBuilder("", "test"));
-                } else {
-                    queryBuilder.clause(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                queryBuilder.clause(new SpanTermQueryBuilder("name", "value"));
-            }
-        }
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java
deleted file mode 100644
index 019a3c1..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanNotQueryBuilderTest.java
+++ /dev/null
@@ -1,218 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.elasticsearch.common.xcontent.XContentBuilder;
-import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.common.xcontent.XContentParser;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.elasticsearch.index.query.QueryBuilders.spanNearQuery;
-import static org.elasticsearch.index.query.QueryBuilders.spanTermQuery;
-import static org.hamcrest.Matchers.containsString;
-import static org.hamcrest.Matchers.equalTo;
-import static org.hamcrest.Matchers.instanceOf;
-
-public class SpanNotQueryBuilderTest extends BaseQueryTestCase<SpanNotQueryBuilder> {
-
-    @Override
-    protected SpanNotQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-        if (randomBoolean()) {
-            // also test negative values, they should implicitly be changed to 0
-            queryBuilder.dist(randomIntBetween(-2, 10));
-        } else {
-            if (randomBoolean()) {
-                queryBuilder.pre(randomIntBetween(-2, 10));
-            }
-            if (randomBoolean()) {
-                queryBuilder.post(randomIntBetween(-2, 10));
-            }
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanNotQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanNotQuery.class));
-        SpanNotQuery spanNotQuery = (SpanNotQuery) query;
-        assertThat(spanNotQuery.getExclude(), equalTo(queryBuilder.excludeQuery().toQuery(context)));
-        assertThat(spanNotQuery.getInclude(), equalTo(queryBuilder.includeQuery().toQuery(context)));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder include;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                include = new SpanTermQueryBuilder("", "test");
-            } else {
-                include = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            include = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder exclude;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                exclude = new SpanTermQueryBuilder("", "test");
-            } else {
-                exclude = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            exclude = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanNotQueryBuilder queryBuilder = new SpanNotQueryBuilder(include, exclude);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-
-    @Test
-    public void testDist() {
-        SpanNotQueryBuilder builder = new SpanNotQueryBuilder(new SpanTermQueryBuilder("name1", "value1"), new SpanTermQueryBuilder("name2", "value2"));
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.dist(-4);
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.dist(4);
-        assertThat(builder.pre(), equalTo(4));
-        assertThat(builder.post(), equalTo(4));
-    }
-
-    @Test
-    public void testPrePost() {
-        SpanNotQueryBuilder builder = new SpanNotQueryBuilder(new SpanTermQueryBuilder("name1", "value1"), new SpanTermQueryBuilder("name2", "value2"));
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.pre(-4).post(-4);
-        assertThat(builder.pre(), equalTo(0));
-        assertThat(builder.post(), equalTo(0));
-        builder.pre(1).post(2);
-        assertThat(builder.pre(), equalTo(1));
-        assertThat(builder.post(), equalTo(2));
-    }
-
-    /**
-     * test correct parsing of `dist` parameter, this should create builder with pre/post set to same value
-     */
-    @Test
-    public void testParseDist() throws IOException {
-        XContentBuilder builder = XContentFactory.jsonBuilder();
-        builder.startObject();
-        builder.startObject(SpanNotQueryBuilder.NAME);
-        builder.field("exclude");
-        spanTermQuery("description", "jumped").toXContent(builder, null);
-        builder.field("include");
-        spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-        builder.field("dist", 3);
-        builder.endObject();
-        builder.endObject();
-
-        QueryParseContext context = createParseContext();
-        XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-        context.reset(parser);
-        assertQueryHeader(parser, SpanNotQueryBuilder.NAME);
-        SpanNotQueryBuilder query = (SpanNotQueryBuilder) new SpanNotQueryParser().fromXContent(context);
-        assertThat(query.pre(), equalTo(3));
-        assertThat(query.post(), equalTo(3));
-        assertNotNull(query.includeQuery());
-        assertNotNull(query.excludeQuery());
-    }
-
-    /**
-     * test exceptions for three types of broken json, missing include / exclude and both dist and pre/post specified
-     */
-    @Test
-    public void testParserExceptions() throws IOException {
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("exclude");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.endObject();
-            builder.endObject();
-
-            QueryParseContext context = createParseContext();
-            XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-            context.reset(parser);
-            assertQueryHeader(parser, SpanNotQueryBuilder.NAME);
-            new SpanNotQueryParser().fromXContent(context);
-            fail("QueryParsingException should have been caught");
-        } catch (QueryParsingException e) {
-            assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot must have [include]"));
-        }
-
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("include");
-            spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.endObject();
-            builder.endObject();
-
-            QueryParseContext context = createParseContext();
-            XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-            context.reset(parser);
-            assertQueryHeader(parser, SpanNotQueryBuilder.NAME);
-            new SpanNotQueryParser().fromXContent(context);
-            fail("QueryParsingException should have been caught");
-        } catch (QueryParsingException e) {
-            assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot must have [exclude]"));
-        }
-
-        try {
-            XContentBuilder builder = XContentFactory.jsonBuilder();
-            builder.startObject();
-            builder.startObject(SpanNotQueryBuilder.NAME);
-            builder.field("include");
-            spanNearQuery(1).clause(QueryBuilders.spanTermQuery("description", "quick"))
-                .clause(QueryBuilders.spanTermQuery("description", "fox")).toXContent(builder, null);
-            builder.field("exclude");
-            spanTermQuery("description", "jumped").toXContent(builder, null);
-            builder.field("dist", 2);
-            builder.field("pre", 2);
-            builder.endObject();
-            builder.endObject();
-
-            QueryParseContext context = createParseContext();
-            XContentParser parser = XContentFactory.xContent(builder.string()).createParser(builder.string());
-            context.reset(parser);
-            assertQueryHeader(parser, SpanNotQueryBuilder.NAME);
-            new SpanNotQueryParser().fromXContent(context);
-            fail("QueryParsingException should have been caught");
-        } catch (QueryParsingException e) {
-            assertThat("QueryParsingException should have been caught", e.getDetailedMessage(), containsString("spanNot can either use [dist] or [pre] & [post] (or none)"));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java
deleted file mode 100644
index 051e6fd..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanOrQueryBuilderTest.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanOrQueryBuilderTest extends BaseQueryTestCase<SpanOrQueryBuilder> {
-
-    @Override
-    protected SpanOrQueryBuilder doCreateTestQueryBuilder() {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(randomIntBetween(1, 6));
-        for (SpanTermQueryBuilder clause : spanTermQueries) {
-            queryBuilder.clause(clause);
-        }
-        return queryBuilder;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanOrQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanOrQuery.class));
-        SpanOrQuery spanOrQuery = (SpanOrQuery) query;
-        assertThat(spanOrQuery.getClauses().length, equalTo(queryBuilder.clauses().size()));
-        Iterator<SpanQueryBuilder> spanQueryBuilderIterator = queryBuilder.clauses().iterator();
-        for (SpanQuery spanQuery : spanOrQuery.getClauses()) {
-            assertThat(spanQuery, equalTo(spanQueryBuilderIterator.next().toQuery(context)));
-        }
-    }
-
-    @Test
-    public void testValidate() {
-        SpanOrQueryBuilder queryBuilder = new SpanOrQueryBuilder();
-        assertValidate(queryBuilder, 1); // empty clause list
-
-        int totalExpectedErrors = 0;
-        int clauses = randomIntBetween(1, 10);
-        for (int i = 0; i < clauses; i++) {
-            if (randomBoolean()) {
-                if (randomBoolean()) {
-                    queryBuilder.clause(new SpanTermQueryBuilder("", "test"));
-                } else {
-                    queryBuilder.clause(null);
-                }
-                totalExpectedErrors++;
-            } else {
-                queryBuilder.clause(new SpanTermQueryBuilder("name", "value"));
-            }
-        }
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java
deleted file mode 100644
index d8d5ef6..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanTermQueryBuilderTest.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanTermQueryBuilderTest extends BaseTermQueryTestCase<SpanTermQueryBuilder> {
-
-    @Override
-    protected SpanTermQueryBuilder createQueryBuilder(String fieldName, Object value) {
-        return new SpanTermQueryBuilder(fieldName, value);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanTermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanTermQuery.class));
-        SpanTermQuery spanTermQuery = (SpanTermQuery) query;
-        assertThat(spanTermQuery.getTerm().field(), equalTo(queryBuilder.fieldName()));
-        MappedFieldType mapper = context.fieldMapper(queryBuilder.fieldName());
-        if (mapper != null) {
-            BytesRef bytesRef = mapper.indexedValueForSearch(queryBuilder.value());
-            assertThat(spanTermQuery.getTerm().bytes(), equalTo(bytesRef));
-        } else {
-            assertThat(spanTermQuery.getTerm().bytes(), equalTo(BytesRefs.toBytesRef(queryBuilder.value())));
-        }
-    }
-
-    /**
-     * @param amount the number of clauses that will be returned
-     * @return an array of random {@link SpanTermQueryBuilder} with same field name
-     */
-    public SpanTermQueryBuilder[] createSpanTermQueryBuilders(int amount) {
-        SpanTermQueryBuilder[] clauses = new SpanTermQueryBuilder[amount];
-        SpanTermQueryBuilder first = createTestQueryBuilder();
-        clauses[0] = first;
-        for (int i = 1; i < amount; i++) {
-            // we need same field name in all clauses, so we only randomize value
-            SpanTermQueryBuilder spanTermQuery = new SpanTermQueryBuilder(first.fieldName(), randomValueForField(first.fieldName()));
-            if (randomBoolean()) {
-                spanTermQuery.boost(2.0f / randomIntBetween(1, 20));
-            }
-            if (randomBoolean()) {
-                spanTermQuery.queryName(randomAsciiOfLengthBetween(1, 10));
-            }
-            clauses[i] = spanTermQuery;
-        }
-        return clauses;
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java
deleted file mode 100644
index ffc518d..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/SpanWithinQueryBuilderTest.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.spans.SpanWithinQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class SpanWithinQueryBuilderTest extends BaseQueryTestCase<SpanWithinQueryBuilder> {
-
-    @Override
-    protected SpanWithinQueryBuilder doCreateTestQueryBuilder() {
-        SpanTermQueryBuilder[] spanTermQueries = new SpanTermQueryBuilderTest().createSpanTermQueryBuilders(2);
-        return new SpanWithinQueryBuilder(spanTermQueries[0], spanTermQueries[1]);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(SpanWithinQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(SpanWithinQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        int totalExpectedErrors = 0;
-        SpanQueryBuilder bigSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                bigSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                bigSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            bigSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanQueryBuilder littleSpanQueryBuilder;
-        if (randomBoolean()) {
-            if (randomBoolean()) {
-                littleSpanQueryBuilder = new SpanTermQueryBuilder("", "test");
-            } else {
-                littleSpanQueryBuilder = null;
-            }
-            totalExpectedErrors++;
-        } else {
-            littleSpanQueryBuilder = new SpanTermQueryBuilder("name", "value");
-        }
-        SpanWithinQueryBuilder queryBuilder = new SpanWithinQueryBuilder(bigSpanQueryBuilder, littleSpanQueryBuilder);
-        assertValidate(queryBuilder, totalExpectedErrors);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java
index dfd1ea7..aab3c6f 100644
--- a/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java
+++ b/core/src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java
@@ -64,7 +64,7 @@ import java.io.IOException;
 public class TemplateQueryParserTest extends ESTestCase {
 
     private Injector injector;
-    private QueryShardContext context;
+    private QueryParseContext context;
 
     @Before
     public void setup() throws IOException {
@@ -98,7 +98,7 @@ public class TemplateQueryParserTest extends ESTestCase {
         ).createInjector();
 
         IndexQueryParserService queryParserService = injector.getInstance(IndexQueryParserService.class);
-        context = new QueryShardContext(index, queryParserService);
+        context = new QueryParseContext(index, queryParserService);
     }
 
     @Override
diff --git a/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java
deleted file mode 100644
index f84d1c0..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TermQueryBuilderTest.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.util.BytesRef;
-import org.elasticsearch.common.lucene.BytesRefs;
-import org.elasticsearch.index.mapper.MappedFieldType;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.instanceOf;
-
-public class TermQueryBuilderTest extends BaseTermQueryTestCase<TermQueryBuilder> {
-
-    /**
-     * @return a TermQuery with random field name and value, optional random boost and queryname
-     */
-    @Override
-    protected TermQueryBuilder createQueryBuilder(String fieldName, Object value) {
-        return new TermQueryBuilder(fieldName, value);
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TermQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(TermQuery.class));
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm().field(), equalTo(queryBuilder.fieldName()));
-        MappedFieldType mapper = context.fieldMapper(queryBuilder.fieldName());
-        if (mapper != null) {
-            BytesRef bytesRef = mapper.indexedValueForSearch(queryBuilder.value());
-            assertThat(termQuery.getTerm().bytes(), equalTo(bytesRef));
-        } else {
-            assertThat(termQuery.getTerm().bytes(), equalTo(BytesRefs.toBytesRef(queryBuilder.value())));
-        }
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java
deleted file mode 100644
index 18c5534..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/TypeQueryBuilderTest.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.elasticsearch.index.mapper.internal.TypeFieldMapper;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.*;
-
-public class TypeQueryBuilderTest extends BaseQueryTestCase<TypeQueryBuilder> {
-
-    @Override
-    protected TypeQueryBuilder doCreateTestQueryBuilder() {
-        return new TypeQueryBuilder(getRandomType());
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(TypeQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, either(instanceOf(TermQuery.class)).or(instanceOf(ConstantScoreQuery.class)));
-        if (query instanceof ConstantScoreQuery) {
-            query = ((ConstantScoreQuery) query).getQuery();
-            assertThat(query, instanceOf(TermQuery.class));
-        }
-        TermQuery termQuery = (TermQuery) query;
-        assertThat(termQuery.getTerm().field(), equalTo(TypeFieldMapper.NAME));
-        assertThat(termQuery.getTerm().text(), equalTo(queryBuilder.type()));
-    }
-
-    @Test
-    public void testValidate() {
-        TypeQueryBuilder typeQueryBuilder = new TypeQueryBuilder((String) null);
-        assertThat(typeQueryBuilder.validate().validationErrors().size(), is(1));
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java b/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java
deleted file mode 100644
index ba23249..0000000
--- a/core/src/test/java/org/elasticsearch/index/query/WildcardQueryBuilderTest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/*
- * Licensed to Elasticsearch under one or more contributor
- * license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright
- * ownership. Elasticsearch licenses this file to you under
- * the Apache License, Version 2.0 (the "License"); you may
- * not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.elasticsearch.index.query;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.WildcardQuery;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static org.hamcrest.Matchers.instanceOf;
-import static org.hamcrest.Matchers.is;
-
-public class WildcardQueryBuilderTest extends BaseQueryTestCase<WildcardQueryBuilder> {
-
-    @Override
-    protected WildcardQueryBuilder doCreateTestQueryBuilder() {
-        WildcardQueryBuilder query;
-
-        // mapped or unmapped field
-        String text = randomAsciiOfLengthBetween(1, 10);
-        if (randomBoolean()) {
-            query = new WildcardQueryBuilder(STRING_FIELD_NAME, text);
-        } else {
-            query = new WildcardQueryBuilder(randomAsciiOfLengthBetween(1, 10), text);
-        }
-        if (randomBoolean()) {
-            query.rewrite(randomFrom(getRandomRewriteMethod()));
-        }
-        return query;
-    }
-
-    @Override
-    protected void doAssertLuceneQuery(WildcardQueryBuilder queryBuilder, Query query, QueryShardContext context) throws IOException {
-        assertThat(query, instanceOf(WildcardQuery.class));
-    }
-
-    @Test
-    public void testValidate() {
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder("", "text");
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(1));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder("field", null);
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(1));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder(null, null);
-        assertThat(wildcardQueryBuilder.validate().validationErrors().size(), is(2));
-
-        wildcardQueryBuilder = new WildcardQueryBuilder("field", "text");
-        assertNull(wildcardQueryBuilder.validate());
-    }
-
-    @Test
-    public void testEmptyValue() throws IOException {
-        QueryShardContext context = createShardContext();
-        context.setAllowUnmappedFields(true);
-
-        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(getRandomType(), "");
-        assertEquals(wildcardQueryBuilder.toQuery(context).getClass(), WildcardQuery.class);
-    }
-}
diff --git a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
index d8b7fc9..2e0356e 100644
--- a/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
+++ b/core/src/test/java/org/elasticsearch/index/query/plugin/DummyQueryParserPlugin.java
@@ -27,7 +27,10 @@ import org.elasticsearch.common.inject.Module;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentParser;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.QueryBuilder;
+import org.elasticsearch.index.query.QueryParseContext;
+import org.elasticsearch.index.query.QueryParser;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.indices.query.IndicesQueriesModule;
 import org.elasticsearch.plugins.AbstractPlugin;
 
@@ -57,36 +60,24 @@ public class DummyQueryParserPlugin extends AbstractPlugin {
         return Settings.EMPTY;
     }
 
-    public static class DummyQueryBuilder extends AbstractQueryBuilder<DummyQueryBuilder> {
-        private static final String NAME = "dummy";
-
+    public static class DummyQueryBuilder extends QueryBuilder {
         @Override
         protected void doXContent(XContentBuilder builder, Params params) throws IOException {
-            builder.startObject(NAME).endObject();
-        }
-
-        @Override
-        public String getWriteableName() {
-            return NAME;
+            builder.startObject("dummy").endObject();
         }
     }
 
-    public static class DummyQueryParser extends BaseQueryParserTemp {
+    public static class DummyQueryParser implements QueryParser {
         @Override
         public String[] names() {
-            return new String[]{DummyQueryBuilder.NAME};
+            return new String[]{"dummy"};
         }
 
         @Override
-        public Query parse(QueryShardContext context) throws IOException, QueryShardException {
-            XContentParser.Token token = context.parseContext().parser().nextToken();
+        public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
+            XContentParser.Token token = parseContext.parser().nextToken();
             assert token == XContentParser.Token.END_OBJECT;
-            return new DummyQuery(context.isFilter());
-        }
-
-        @Override
-        public DummyQueryBuilder getBuilderPrototype() {
-            return new DummyQueryBuilder();
+            return new DummyQuery(parseContext.isFilter());
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/index/query/simple-query-string.json b/core/src/test/java/org/elasticsearch/index/query/simple-query-string.json
new file mode 100644
index 0000000..9208e88
--- /dev/null
+++ b/core/src/test/java/org/elasticsearch/index/query/simple-query-string.json
@@ -0,0 +1,8 @@
+{
+  "simple_query_string": {
+    "query": "foo bar",
+    "analyzer": "keyword",
+    "fields": ["body^5","_all"],
+    "default_operator": "and"
+  }
+}
diff --git a/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java b/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
index c69220f..c7dd274 100644
--- a/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/search/child/AbstractChildTestCase.java
@@ -19,19 +19,14 @@
 
 package org.elasticsearch.index.search.child;
 
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.*;
 import org.apache.lucene.search.join.BitDocIdSetFilter;
 import org.apache.lucene.util.BitDocIdSet;
 import org.apache.lucene.util.BitSet;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
-import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.compress.CompressedXContent;
+import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentHelper;
 import org.elasticsearch.common.xcontent.XContentParser;
@@ -40,7 +35,7 @@ import org.elasticsearch.index.IndexService;
 import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.mapper.internal.UidFieldMapper;
 import org.elasticsearch.index.query.QueryBuilder;
-import org.elasticsearch.index.query.QueryShardContext;
+import org.elasticsearch.index.query.QueryParseContext;
 import org.elasticsearch.search.internal.SearchContext;
 import org.elasticsearch.test.ESSingleNodeTestCase;
 import org.hamcrest.Description;
@@ -71,7 +66,7 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
         mapperService.merge(childType, new CompressedXContent(PutMappingRequest.buildFromSimplifiedDef(childType, "_parent", "type=" + parentType, CHILD_SCORE_NAME, "type=double,doc_values=false").string()), true, false);
         return createSearchContext(indexService);
     }
-
+    
     static void assertBitSet(BitSet actual, BitSet expected, IndexSearcher searcher) throws IOException {
         assertBitSet(new BitDocIdSet(actual), new BitDocIdSet(expected), searcher);
     }
@@ -88,7 +83,7 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
             throw new java.lang.AssertionError(description.toString());
         }
     }
-
+    
     static boolean equals(BitDocIdSet expected, BitDocIdSet actual) {
         if (actual == null && expected == null) {
             return true;
@@ -140,10 +135,10 @@ public abstract class AbstractChildTestCase extends ESSingleNodeTestCase {
     }
 
     static Query parseQuery(QueryBuilder queryBuilder) throws IOException {
-        QueryShardContext context = new QueryShardContext(new Index("test"), SearchContext.current().queryParserService());
+        QueryParseContext context = new QueryParseContext(new Index("test"), SearchContext.current().queryParserService());
         XContentParser parser = XContentHelper.createParser(queryBuilder.buildAsBytes());
         context.reset(parser);
-        return context.parseContext().parseInnerQuery();
+        return context.parseInnerQuery();
     }
 
 }
diff --git a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
index 361fd15..3aadd0d 100644
--- a/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
+++ b/core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java
@@ -56,9 +56,7 @@ import java.util.HashSet;
 import java.util.Set;
 import java.util.concurrent.ExecutionException;
 
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;
-import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_VERSION_CREATED;
+import static org.elasticsearch.cluster.metadata.IndexMetaData.*;
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.matchAllQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
@@ -308,7 +306,7 @@ public class IndexShardTests extends ESSingleNodeTestCase {
         assertBusy(new Runnable() { // should be very very quick
             @Override
             public void run() {
-                IndexStats indexStats = client().admin().indices().prepareStats("test").get().getIndex("test");
+                IndexStats indexStats = client().admin().indices().prepareStats("test").clear().get().getIndex("test");
                 assertNotNull(indexStats.getShards()[0].getCommitStats().getUserData().get(Engine.SYNC_COMMIT_ID));
             }
         });
diff --git a/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java b/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java
index 35847f5..af5c2ef 100644
--- a/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java
+++ b/core/src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java
@@ -56,6 +56,13 @@ public class RecoveryStatusTests extends ESSingleNodeTestCase {
             assertSame(openIndexOutput, indexOutput);
             openIndexOutput.writeInt(1);
         }
+        try {
+            status.openAndPutIndexOutput("foo.bar", new StoreFileMetaData("foo.bar", 8), status.store());
+            fail("file foo.bar is already opened and registered");
+        } catch (IllegalStateException ex) {
+            assertEquals("output for file [foo.bar] has already been created", ex.getMessage());
+            // all well = it's already registered
+        }
         status.removeOpenIndexOutputs("foo.bar");
         Set<String> strings = Sets.newHashSet(status.store().directory().listAll());
         String expectedFile = null;
diff --git a/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java b/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
index e77a141..f868c04 100644
--- a/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
+++ b/core/src/test/java/org/elasticsearch/node/internal/InternalSettingsPreparerTests.java
@@ -29,8 +29,11 @@ import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
 
+import java.io.InputStream;
 import java.net.URL;
 import java.net.URLClassLoader;
+import java.nio.file.Files;
+import java.nio.file.Path;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -73,11 +76,21 @@ public class InternalSettingsPreparerTests extends ESTestCase {
     }
 
     @Test
-    public void testAlternateConfigFileSuffixes() {
+    public void testAlternateConfigFileSuffixes() throws Exception {
+        InputStream yaml = getClass().getResourceAsStream("/config/elasticsearch.yaml");
+        InputStream json = getClass().getResourceAsStream("/config/elasticsearch.json");
+        InputStream properties = getClass().getResourceAsStream("/config/elasticsearch.properties");
+        Path home = createTempDir();
+        Path config = home.resolve("config");
+        Files.createDirectory(config);
+        Files.copy(yaml, config.resolve("elasticsearch.yaml"));
+        Files.copy(json, config.resolve("elasticsearch.json"));
+        Files.copy(properties, config.resolve("elasticsearch.properties"));
+
         // test that we can read config files with .yaml, .json, and .properties suffixes
         Tuple<Settings, Environment> tuple = InternalSettingsPreparer.prepareSettings(settingsBuilder()
                 .put("config.ignore_system_properties", true)
-                .put("path.home", createTempDir().toString())
+                .put("path.home", home)
                 .build(), true);
 
         assertThat(tuple.v1().get("yaml.config.exists"), equalTo("true"));
diff --git a/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
index cb64437..77a4b63 100644
--- a/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/MultiPercolatorIT.java
@@ -26,7 +26,6 @@ import org.elasticsearch.client.Requests;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -361,7 +360,7 @@ public class MultiPercolatorIT extends ESIntegTestCase {
         ensureGreen("nestedindex");
 
         client().prepareIndex("nestedindex", PercolatorService.TYPE_NAME, "Q").setSource(jsonBuilder().startObject()
-                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(Operator.AND)).scoreMode("avg")).endObject()).get();
+                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(MatchQueryBuilder.Operator.AND)).scoreMode("avg")).endObject()).get();
 
         refresh();
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
index f250e92..ecee193 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityIT.java
@@ -23,8 +23,8 @@ import org.elasticsearch.action.index.IndexRequestBuilder;
 import org.elasticsearch.action.percolate.PercolateResponse;
 import org.elasticsearch.action.percolate.PercolateSourceBuilder;
 import org.elasticsearch.index.percolator.PercolatorException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.test.ESIntegTestCase;
-import org.elasticsearch.index.query.QueryShardException;
 import org.junit.Test;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
@@ -67,7 +67,7 @@ public class PercolatorBackwardsCompatibilityIT extends ESIntegTestCase {
             fail();
         } catch (PercolatorException e) {
             e.printStackTrace();
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
     }
 
diff --git a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
index 004771a..fb37a0d 100644
--- a/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
+++ b/core/src/test/java/org/elasticsearch/percolator/PercolatorIT.java
@@ -42,9 +42,8 @@ import org.elasticsearch.index.engine.DocumentMissingException;
 import org.elasticsearch.index.engine.VersionConflictEngineException;
 import org.elasticsearch.index.percolator.PercolatorException;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.index.query.functionscore.factor.FactorBuilder;
 import org.elasticsearch.index.query.support.QueryInnerHitBuilder;
 import org.elasticsearch.rest.RestStatus;
@@ -1765,7 +1764,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .get();
             fail();
         } catch (PercolatorException e) {
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
 
         try {
@@ -1774,7 +1773,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .get();
             fail();
         } catch (PercolatorException e) {
-            assertThat(e.getRootCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getRootCause(), instanceOf(QueryParsingException.class));
         }
     }
 
@@ -1813,7 +1812,7 @@ public class PercolatorIT extends ESIntegTestCase {
         ensureGreen("nestedindex");
 
         client().prepareIndex("nestedindex", PercolatorService.TYPE_NAME, "Q").setSource(jsonBuilder().startObject()
-                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(Operator.AND)).scoreMode("avg")).endObject()).get();
+                .field("query", QueryBuilders.nestedQuery("employee", QueryBuilders.matchQuery("employee.name", "virginia potts").operator(MatchQueryBuilder.Operator.AND)).scoreMode("avg")).endObject()).get();
 
         refresh();
 
@@ -2017,7 +2016,7 @@ public class PercolatorIT extends ESIntegTestCase {
                     .execute().actionGet();
             fail("Expected a parse error, because inner_hits isn't supported in the percolate api");
         } catch (Exception e) {
-            assertThat(e.getCause(), instanceOf(QueryShardException.class));
+            assertThat(e.getCause(), instanceOf(QueryParsingException.class));
             assertThat(e.getCause().getMessage(), containsString("inner_hits unsupported"));
         }
     }
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
index 27c96ed..bb2b01c 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerIT.java
@@ -19,6 +19,7 @@
 package org.elasticsearch.plugins;
 
 import com.google.common.base.Charsets;
+import com.google.common.hash.Hashing;
 import org.apache.http.impl.client.HttpClients;
 import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.Version;
@@ -50,7 +51,10 @@ import org.junit.Test;
 import javax.net.ssl.HttpsURLConnection;
 import javax.net.ssl.SSLContext;
 import javax.net.ssl.SSLSocketFactory;
+import java.io.BufferedWriter;
+import java.io.FileOutputStream;
 import java.io.IOException;
+import java.io.PrintStream;
 import java.net.InetSocketAddress;
 import java.nio.charset.StandardCharsets;
 import java.nio.file.FileVisitResult;
@@ -73,8 +77,7 @@ import static org.elasticsearch.common.io.FileSystemUtilsTests.assertFileContent
 import static org.elasticsearch.common.settings.Settings.settingsBuilder;
 import static org.elasticsearch.plugins.PluginInfoTests.writeProperties;
 import static org.elasticsearch.test.ESIntegTestCase.Scope;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertDirectoryExists;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.*;
 import static org.jboss.netty.handler.codec.http.HttpVersion.HTTP_1_1;
 
@@ -106,6 +109,26 @@ public class PluginManagerIT extends ESIntegTestCase {
         System.clearProperty("es.default.path.home");
     }
 
+    private void writeSha1(Path file, boolean corrupt) throws IOException {
+        String sha1Hex = Hashing.sha1().hashBytes(Files.readAllBytes(file)).toString();
+        try (BufferedWriter out = Files.newBufferedWriter(file.resolveSibling(file.getFileName() + ".sha1"), Charsets.UTF_8)) {
+            out.write(sha1Hex);
+            if (corrupt) {
+                out.write("bad");
+            }
+        }
+    }
+
+    private void writeMd5(Path file, boolean corrupt) throws IOException {
+        String md5Hex = Hashing.md5().hashBytes(Files.readAllBytes(file)).toString();
+        try (BufferedWriter out = Files.newBufferedWriter(file.resolveSibling(file.getFileName() + ".md5"), Charsets.UTF_8)) {
+            out.write(md5Hex);
+            if (corrupt) {
+                out.write("bad");
+            }
+        }
+    }
+
     /** creates a plugin .zip and returns the url for testing */
     private String createPlugin(final Path structure, String... properties) throws IOException {
         writeProperties(structure, properties);
@@ -120,9 +143,35 @@ public class PluginManagerIT extends ESIntegTestCase {
                 }
             });
         }
+        if (randomBoolean()) {
+            writeSha1(zip, false);
+        } else if (randomBoolean()) {
+            writeMd5(zip, false);
+        }
         return zip.toUri().toURL().toString();
     }
 
+    /** creates a plugin .zip and bad checksum file and returns the url for testing */
+    private String createPluginWithBadChecksum(final Path structure, String... properties) throws IOException {
+        writeProperties(structure, properties);
+        Path zip = createTempDir().resolve(structure.getFileName() + ".zip");
+        try (ZipOutputStream stream = new ZipOutputStream(Files.newOutputStream(zip))) {
+            Files.walkFileTree(structure, new SimpleFileVisitor<Path>() {
+                @Override
+                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {
+                    stream.putNextEntry(new ZipEntry(structure.relativize(file).toString()));
+                    Files.copy(file, stream);
+                    return FileVisitResult.CONTINUE;
+                }
+            });
+        }
+        if (randomBoolean()) {
+            writeSha1(zip, true);
+        } else {
+            writeMd5(zip, true);
+        }
+        return zip.toUri().toURL().toString();
+    }
     @Test
     public void testThatPluginNameMustBeSupplied() throws IOException {
         Path pluginDir = createTempDir().resolve("fake-plugin");
@@ -144,7 +193,7 @@ public class PluginManagerIT extends ESIntegTestCase {
         Files.createFile(pluginDir.resolve("bin").resolve("tool"));
         Files.createDirectories(pluginDir.resolve("config"));
         Files.createFile(pluginDir.resolve("config").resolve("file"));
-
+        
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
             "version", "1.0",
@@ -152,7 +201,7 @@ public class PluginManagerIT extends ESIntegTestCase {
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-
+        
         Environment env = initialSettings.v2();
         Path binDir = env.binFile();
         Path pluginBinDir = binDir.resolve(pluginName);
@@ -189,7 +238,7 @@ public class PluginManagerIT extends ESIntegTestCase {
         // create config/test.txt with contents 'version1'
         Files.createDirectories(pluginDir.resolve("config"));
         Files.write(pluginDir.resolve("config").resolve("test.txt"), "version1".getBytes(StandardCharsets.UTF_8));
-
+        
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
             "version", "1.0",
@@ -197,7 +246,7 @@ public class PluginManagerIT extends ESIntegTestCase {
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-
+        
         Environment env = initialSettings.v2();
         Path pluginConfigDir = env.configFile().resolve(pluginName);
 
@@ -233,7 +282,7 @@ public class PluginManagerIT extends ESIntegTestCase {
                 "java.version", System.getProperty("java.specification.version"),
                 "jvm", "true",
                 "classname", "FakePlugin");
-
+ 
         assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl));
 
         assertFileContent(pluginConfigDir, "test.txt", "version1");
@@ -297,7 +346,7 @@ public class PluginManagerIT extends ESIntegTestCase {
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-
+        
         Environment env = initialSettings.v2();
         Path binDir = env.binFile();
         Path pluginBinDir = binDir.resolve(pluginName);
@@ -342,15 +391,30 @@ public class PluginManagerIT extends ESIntegTestCase {
         Files.createDirectories(pluginDir.resolve("_site"));
         Files.createFile(pluginDir.resolve("_site").resolve("somefile"));
         String pluginUrl = createPlugin(pluginDir,
-            "description", "fake desc",
-            "version", "1.0",
-            "site", "true");
+                "description", "fake desc",
+                "version", "1.0",
+                "site", "true");
         assertStatusOk(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl));
         assertThatPluginIsListed(pluginName);
         // We want to check that Plugin Manager moves content to _site
         assertFileExists(initialSettings.v2().pluginsFile().resolve(pluginName).resolve("_site"));
     }
 
+    @Test
+    public void testInstallPluginWithBadChecksum() throws IOException {
+        String pluginName = "fake-plugin";
+        Path pluginDir = createTempDir().resolve(pluginName);
+        Files.createDirectories(pluginDir.resolve("_site"));
+        Files.createFile(pluginDir.resolve("_site").resolve("somefile"));
+        String pluginUrl = createPluginWithBadChecksum(pluginDir,
+                "description", "fake desc",
+                "version", "1.0",
+                "site", "true");
+        assertStatus(String.format(Locale.ROOT, "install %s --url %s --verbose", pluginName, pluginUrl),
+                ExitStatus.IO_ERROR);
+        assertThatPluginIsNotListed(pluginName);
+        assertFileNotExists(initialSettings.v2().pluginsFile().resolve(pluginName).resolve("_site"));
+    }
 
     private void singlePluginInstallAndRemove(String pluginDescriptor, String pluginName, String pluginCoordinates) throws IOException {
         logger.info("--> trying to download and install [{}]", pluginDescriptor);
@@ -431,7 +495,7 @@ public class PluginManagerIT extends ESIntegTestCase {
     @Test
     public void testRemovePlugin() throws Exception {
         String pluginName = "plugintest";
-        Path pluginDir = createTempDir().resolve(pluginName);
+        Path pluginDir = createTempDir().resolve(pluginName);        
         String pluginUrl = createPlugin(pluginDir,
             "description", "fake desc",
             "version", "1.0.0",
@@ -439,7 +503,7 @@ public class PluginManagerIT extends ESIntegTestCase {
             "java.version", System.getProperty("java.specification.version"),
             "jvm", "true",
             "classname", "FakePlugin");
-
+        
         // We want to remove plugin with plugin short name
         singlePluginInstallAndRemove("plugintest", "plugintest", pluginUrl);
 
@@ -592,4 +656,11 @@ public class PluginManagerIT extends ESIntegTestCase {
         String message = String.format(Locale.ROOT, "Terminal output was: %s", terminal.getTerminalOutput());
         assertThat(message, terminal.getTerminalOutput(), hasItem(containsString(pluginName)));
     }
+
+    private void assertThatPluginIsNotListed(String pluginName) {
+        terminal.getTerminalOutput().clear();
+        assertStatusOk("list");
+        String message = String.format(Locale.ROOT, "Terminal output was: %s", terminal.getTerminalOutput());
+        assertFalse(message, terminal.getTerminalOutput().contains(pluginName));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java b/core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java
index 40f90c9..d4986d5 100644
--- a/core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java
+++ b/core/src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java
@@ -20,7 +20,9 @@
 package org.elasticsearch.plugins;
 
 import com.google.common.io.Files;
+import org.elasticsearch.Build;
 import org.elasticsearch.Version;
+import org.elasticsearch.common.http.client.HttpDownloadHelper;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
 import org.elasticsearch.test.ESTestCase;
@@ -29,6 +31,7 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.net.URL;
+import java.nio.charset.Charset;
 import java.nio.file.Path;
 import java.util.Iterator;
 import java.util.Locale;
@@ -79,13 +82,13 @@ public class PluginManagerUnitTests extends ESTestCase {
         Iterator<URL> iterator = handle.urls().iterator();
 
         if (supportStagingUrls) {
-            String expectedStagingURL = String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/staging/org/elasticsearch/plugin/%s/%s/%s-%s.zip",
-                    pluginName, Version.CURRENT.number(), pluginName, Version.CURRENT.number());
+            String expectedStagingURL = String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/staging/%s/org/elasticsearch/plugin/elasticsearch-%s/%s/elasticsearch-%s-%s.zip",
+                    Build.CURRENT.hashShort(), pluginName, Version.CURRENT.number(), pluginName, Version.CURRENT.number());
             assertThat(iterator.next(), is(new URL(expectedStagingURL)));
         }
 
-        URL expected = new URL("http", "download.elastic.co", "/elasticsearch/release/org/elasticsearch/plugin/" + pluginName + "/" + Version.CURRENT.number() + "/" +
-        pluginName + "-" + Version.CURRENT.number() + ".zip");
+        URL expected = new URL("http", "download.elastic.co", "/elasticsearch/release/org/elasticsearch/plugin/elasticsearch-" + pluginName + "/" + Version.CURRENT.number() + "/elasticsearch-" +
+                pluginName + "-" + Version.CURRENT.number() + ".zip");
         assertThat(iterator.next(), is(expected));
 
         assertThat(iterator.hasNext(), is(false));
@@ -93,7 +96,7 @@ public class PluginManagerUnitTests extends ESTestCase {
 
     @Test
     public void testTrimmingElasticsearchFromOfficialPluginName() throws IOException {
-        String randomPluginName = randomFrom(PluginManager.OFFICIAL_PLUGINS.asList());
+        String randomPluginName = randomFrom(PluginManager.OFFICIAL_PLUGINS.asList()).replaceFirst("elasticsearch-", "");
         PluginManager.PluginHandle handle = PluginManager.PluginHandle.parse(randomPluginName);
         assertThat(handle.name, is(randomPluginName.replaceAll("^elasticsearch-", "")));
 
@@ -105,12 +108,12 @@ public class PluginManagerUnitTests extends ESTestCase {
         Iterator<URL> iterator = handle.urls().iterator();
 
         if (supportStagingUrls) {
-            String expectedStagingUrl = String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/staging/org/elasticsearch/plugin/%s/%s/%s-%s.zip",
-                    randomPluginName, Version.CURRENT.number(), randomPluginName, Version.CURRENT.number());
+            String expectedStagingUrl = String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/staging/%s/org/elasticsearch/plugin/elasticsearch-%s/%s/elasticsearch-%s-%s.zip",
+                    Build.CURRENT.hashShort(), randomPluginName, Version.CURRENT.number(), randomPluginName, Version.CURRENT.number());
             assertThat(iterator.next(), is(new URL(expectedStagingUrl)));
         }
 
-        String releaseUrl = String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/%s/%s/%s-%s.zip",
+        String releaseUrl = String.format(Locale.ROOT, "http://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/elasticsearch-%s/%s/elasticsearch-%s-%s.zip",
                 randomPluginName, Version.CURRENT.number(), randomPluginName, Version.CURRENT.number());
         assertThat(iterator.next(), is(new URL(releaseUrl)));
 
@@ -128,4 +131,13 @@ public class PluginManagerUnitTests extends ESTestCase {
         URL expected = new URL("https", "github.com", "/" + user + "/" + pluginName + "/" + "archive/master.zip");
         assertThat(handle.urls().get(0), is(expected));
     }
+
+    @Test
+    public void testDownloadHelperChecksums() throws Exception {
+        // Sanity check to make sure the checksum functions never change how they checksum things
+        assertEquals("0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33",
+                HttpDownloadHelper.SHA1_CHECKSUM.checksum("foo".getBytes(Charset.forName("UTF-8"))));
+        assertEquals("acbd18db4cc2f85cedef654fccc4a4d8",
+                HttpDownloadHelper.MD5_CHECKSUM.checksum("foo".getBytes(Charset.forName("UTF-8"))));
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
index c73baac..53a71e1 100644
--- a/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
+++ b/core/src/test/java/org/elasticsearch/recovery/RelocationIT.java
@@ -402,7 +402,7 @@ public class RelocationIT extends ESIntegTestCase {
 
         // Slow down recovery in order to make recovery cancellations more likely
         IndicesStatsResponse statsResponse = client().admin().indices().prepareStats(indexName).get();
-        long chunkSize = statsResponse.getIndex(indexName).getShards()[0].getStats().getStore().size().bytes() / 10;
+        long chunkSize = Math.max(1, statsResponse.getIndex(indexName).getShards()[0].getStats().getStore().size().bytes() / 10);
         assertTrue(client().admin().cluster().prepareUpdateSettings()
                 .setTransientSettings(Settings.builder()
                                 // one chunk per sec..
diff --git a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
index cb9bcd5..f167ce3 100644
--- a/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
+++ b/core/src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreIT.java
@@ -29,7 +29,7 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.common.xcontent.XContentParser;
 import org.elasticsearch.index.query.QueryBuilders;
-import org.elasticsearch.index.query.QueryShardException;
+import org.elasticsearch.index.query.QueryParsingException;
 import org.elasticsearch.plugins.AbstractPlugin;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.script.ScriptModule;
@@ -216,7 +216,7 @@ public class SignificantTermsSignificanceScoreIT extends ESIntegTestCase {
         public static class SimpleHeuristicParser implements SignificanceHeuristicParser {
 
             @Override
-            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryShardException {
+            public SignificanceHeuristic parse(XContentParser parser, ParseFieldMatcher parseFieldMatcher) throws IOException, QueryParsingException {
                 parser.nextToken();
                 return new SimpleHeuristic();
             }
diff --git a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
index 2509c32..d9f69de 100644
--- a/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
+++ b/core/src/test/java/org/elasticsearch/search/highlight/HighlighterSearchIT.java
@@ -27,9 +27,10 @@ import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.settings.Settings.Builder;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.BoostableQueryBuilder;
 import org.elasticsearch.index.query.IdsQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder;
+import org.elasticsearch.index.query.MatchQueryBuilder.Operator;
 import org.elasticsearch.index.query.MatchQueryBuilder.Type;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
 import org.elasticsearch.index.query.QueryBuilder;
@@ -70,7 +71,12 @@ import static org.elasticsearch.index.query.QueryBuilders.typeQuery;
 import static org.elasticsearch.index.query.QueryBuilders.wildcardQuery;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.highlight;
 import static org.elasticsearch.search.builder.SearchSourceBuilder.searchSource;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHighlight;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNotHighlighted;
 import static org.elasticsearch.test.hamcrest.RegexMatcher.matches;
 import static org.hamcrest.Matchers.anyOf;
 import static org.hamcrest.Matchers.containsString;
@@ -1391,7 +1397,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
@@ -1409,7 +1415,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").order("score").preTags("<x>").postTags("</x>"));
 
         SearchResponse searchResponse = client().prepareSearch("test").setSource(source.buildAsBytes()).get();
@@ -2323,7 +2329,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
 
         logger.info("--> highlighting and searching on field1");
         SearchSourceBuilder source = searchSource()
-                .query(boostingQuery(termQuery("field2", "brown"), termQuery("field2", "foobar")).negativeBoost(0.5f))
+                .query(boostingQuery().positive(termQuery("field2", "brown")).negative(termQuery("field2", "foobar")).negativeBoost(0.5f))
                 .highlight(highlight().field("field2").preTags("<x>").postTags("</x>"));
         SearchResponse searchResponse = client().search(searchRequest("test").source(source)).actionGet();
 
@@ -2611,7 +2617,7 @@ public class HighlighterSearchIT extends ESIntegTestCase {
                 queryStringQuery("\"highlight words together\"").field("field1^100").autoGeneratePhraseQueries(true));
     }
 
-    private <P extends AbstractQueryBuilder<P>> void
+    private <P extends QueryBuilder & BoostableQueryBuilder<?>> void
             phraseBoostTestCaseForClauses(String highlighterType, float boost, QueryBuilder terms, P phrase) {
         Matcher<String> highlightedMatcher = Matchers.either(containsString("<em>highlight words together</em>")).or(
                 containsString("<em>highlight</em> <em>words</em> <em>together</em>"));
@@ -2625,10 +2631,10 @@ public class HighlighterSearchIT extends ESIntegTestCase {
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
         phrase.boost(1);
         // Try with a boosting query
-        response = search.setQuery(boostingQuery(phrase, terms).boost(boost).negativeBoost(1)).get();
+        response = search.setQuery(boostingQuery().positive(phrase).negative(terms).boost(boost).negativeBoost(1)).get();
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
         // Try with a boosting query using a negative boost
-        response = search.setQuery(boostingQuery(phrase, terms).boost(1).negativeBoost(1/boost)).get();
+        response = search.setQuery(boostingQuery().positive(phrase).negative(terms).boost(1).negativeBoost(1/boost)).get();
         assertHighlight(response, 0, "field1", 0, 1, highlightedMatcher);
     }
 }
diff --git a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
index 1384b33..bf31131 100644
--- a/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
+++ b/core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java
@@ -19,6 +19,7 @@
 
 package org.elasticsearch.search.innerhits;
 
+import org.apache.lucene.util.LuceneTestCase;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
 import org.elasticsearch.action.index.IndexRequestBuilder;
@@ -122,6 +123,7 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertNoFailures(response);
             assertHitCount(response, 1);
             assertSearchHit(response, 1, hasId("2"));
+            assertThat(response.getHits().getAt(0).getShard(), notNullValue());
             assertThat(response.getHits().getAt(0).getInnerHits().size(), equalTo(1));
             SearchHits innerHits = response.getHits().getAt(0).getInnerHits().get("comment");
             assertThat(innerHits.totalHits(), equalTo(3l));
@@ -223,6 +225,7 @@ public class InnerHitsIT extends ESIntegTestCase {
         assertThat(searchResponse.getHits().getHits().length, equalTo(numDocs));
         for (int i = 0; i < numDocs; i++) {
             SearchHit searchHit = searchResponse.getHits().getAt(i);
+            assertThat(searchHit.getShard(), notNullValue());
             SearchHits inner = searchHit.getInnerHits().get("a");
             assertThat(inner.totalHits(), equalTo((long) field1InnerObjects[i]));
             for (int j = 0; j < field1InnerObjects[i] && j < size; j++) {
@@ -275,6 +278,7 @@ public class InnerHitsIT extends ESIntegTestCase {
             assertNoFailures(response);
             assertHitCount(response, 1);
             assertSearchHit(response, 1, hasId("1"));
+            assertThat(response.getHits().getAt(0).getShard(), notNullValue());
 
             assertThat(response.getHits().getAt(0).getInnerHits().size(), equalTo(1));
             SearchHits innerHits = response.getHits().getAt(0).getInnerHits().get("comment");
@@ -411,6 +415,7 @@ public class InnerHitsIT extends ESIntegTestCase {
             SearchHit searchHit = searchResponse.getHits().getAt(parent);
             assertThat(searchHit.getType(), equalTo("parent"));
             assertThat(searchHit.getId(), equalTo(String.format(Locale.ENGLISH, "%03d", parent)));
+            assertThat(searchHit.getShard(), notNullValue());
 
             SearchHits inner = searchHit.getInnerHits().get("a");
             assertThat(inner.totalHits(), equalTo((long) child1InnerObjects[parent]));
diff --git a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
index 793d365..b9099d0 100644
--- a/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/MultiMatchQueryIT.java
@@ -28,7 +28,6 @@ import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.MultiMatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.search.SearchHit;
 import org.elasticsearch.search.SearchHits;
@@ -155,7 +154,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         Set<String> topNIds = Sets.newHashSet("theone", "theother");
         for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
             topNIds.remove(searchResponse.getHits().getAt(i).getId());
@@ -167,25 +166,25 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(type))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
     }
@@ -194,18 +193,18 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
     public void testPhraseType() {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("Man the Ultimate", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
         assertFirstHit(searchResponse, hasId("ultimate2"));
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("Captain", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE))).get();
         assertThat(searchResponse.getHits().getTotalHits(), greaterThan(1l));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the Ul", "full_name_phrase", "first_name_phrase", "last_name_phrase", "category_phrase")
-                        .operator(Operator.OR).type(MatchQueryBuilder.Type.PHRASE_PREFIX))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).type(MatchQueryBuilder.Type.PHRASE_PREFIX))).get();
         assertSearchHits(searchResponse, "ultimate2", "ultimate1");
         assertHitCount(searchResponse, 2l);
     }
@@ -264,7 +263,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         Float cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).cutoffFrequency(cutoffFrequency))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).cutoffFrequency(cutoffFrequency))).get();
         Set<String> topNIds = Sets.newHashSet("theone", "theother");
         for (int i = 0; i < searchResponse.getHits().hits().length; i++) {
             topNIds.remove(searchResponse.getHits().getAt(i).getId());
@@ -277,39 +276,39 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
         long size = searchResponse.getHits().getTotalHits();
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).useDisMax(false).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).useDisMax(false).type(type))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theone"), hasId("theother")));
         assertThat("common terms expected to be a way smaller result set", size, lessThan(searchResponse.getHits().getTotalHits()));
 
         cutoffFrequency = randomBoolean() ? Math.min(1, numDocs * 1.f / between(10, 20)) : 1.f / between(10, 20);
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.OR).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.OR).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency).type(type))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "first_name", "last_name", "category")
-                        .operator(Operator.AND).cutoffFrequency(cutoffFrequency)
+                        .operator(MatchQueryBuilder.Operator.AND).cutoffFrequency(cutoffFrequency)
                         .analyzer("category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS))).get();
         assertHitCount(searchResponse, 1l);
@@ -331,7 +330,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                         .setQuery(randomizeType(multiMatchQueryBuilder
-                                .operator(Operator.OR).type(type))).get();
+                                .operator(MatchQueryBuilder.Operator.OR).type(type))).get();
 
                 SearchResponse right = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
@@ -347,7 +346,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
             {
                 MatchQueryBuilder.Type type = randomBoolean() ? null : MatchQueryBuilder.Type.BOOLEAN;
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 MultiMatchQueryBuilder multiMatchQueryBuilder = randomBoolean() ? multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category") :
                         multiMatchQuery("captain america", "*_name", randomBoolean() ? "category" : "categ*");
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
@@ -368,7 +367,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
 
             {
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 SearchResponse left = client().prepareSearch("test").setSize(numDocs)
                         .addSort(SortBuilders.scoreSort()).addSort(SortBuilders.fieldSort("_uid"))
                         .setQuery(randomizeType(multiMatchQuery("capta", "full_name", "first_name", "last_name", "category")
@@ -386,7 +385,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
             }
             {
                 String minShouldMatch = randomBoolean() ? null : "" + between(0, 1);
-                Operator op = randomBoolean() ? Operator.AND : Operator.OR;
+                MatchQueryBuilder.Operator op = randomBoolean() ? MatchQueryBuilder.Operator.AND : MatchQueryBuilder.Operator.OR;
                 SearchResponse left;
                 if (randomBoolean()) {
                     left = client().prepareSearch("test").setSize(numDocs)
@@ -417,13 +416,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         SearchResponse searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theone"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero captain america", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theone"));
         assertSecondHit(searchResponse, hasId("theother"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -431,13 +430,13 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("marvel hero", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, hasId("theother"));
 
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -445,7 +444,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america 15", "full_name", "first_name", "last_name", "category", "skill")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -466,7 +465,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .cutoffFrequency(0.1f)
                         .analyzer("category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertFirstHit(searchResponse, anyOf(hasId("theother"), hasId("theone")));
         long numResults = searchResponse.getHits().totalHits();
 
@@ -474,7 +473,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.OR))).get();
+                        .operator(MatchQueryBuilder.Operator.OR))).get();
         assertThat(numResults, lessThan(searchResponse.getHits().getTotalHits()));
         assertFirstHit(searchResponse, hasId("theone"));
 
@@ -484,28 +483,28 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
                         .analyzer("category")
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("theone"));
         // counter example
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : null)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 0l);
 
         // counter example
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("captain america marvel hero", "first_name", "last_name", "category")
                         .type(randomBoolean() ? MultiMatchQueryBuilder.Type.CROSS_FIELDS : null)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertHitCount(searchResponse, 0l);
 
         // test if boosts work
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name^2", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertFirstHit(searchResponse, hasId("ultimate1"));   // has ultimate in the last_name and that is boosted
         assertSecondHit(searchResponse, hasId("ultimate2"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
@@ -515,7 +514,7 @@ public class MultiMatchQueryIT extends ESIntegTestCase {
         searchResponse = client().prepareSearch("test")
                 .setQuery(randomizeType(multiMatchQuery("the ultimate", "full_name", "first_name", "last_name", "category")
                         .type(MultiMatchQueryBuilder.Type.CROSS_FIELDS)
-                        .operator(Operator.AND))).get();
+                        .operator(MatchQueryBuilder.Operator.AND))).get();
         assertFirstHit(searchResponse, hasId("ultimate2"));
         assertSecondHit(searchResponse, hasId("ultimate1"));
         assertThat(searchResponse.getHits().hits()[0].getScore(), greaterThan(searchResponse.getHits().hits()[1].getScore()));
diff --git a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
index 83a1905..bca495b 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java
@@ -20,6 +20,7 @@
 package org.elasticsearch.search.query;
 
 import org.apache.lucene.util.English;
+import org.elasticsearch.ElasticsearchException;
 import org.elasticsearch.Version;
 import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.index.IndexRequestBuilder;
@@ -31,8 +32,15 @@ import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.mapper.MapperParsingException;
-import org.elasticsearch.index.query.*;
+import org.elasticsearch.index.query.BoolQueryBuilder;
+import org.elasticsearch.index.query.CommonTermsQueryBuilder.Operator;
+import org.elasticsearch.index.query.MatchQueryBuilder;
 import org.elasticsearch.index.query.MatchQueryBuilder.Type;
+import org.elasticsearch.index.query.MultiMatchQueryBuilder;
+import org.elasticsearch.index.query.QueryBuilders;
+import org.elasticsearch.index.query.QueryStringQueryBuilder;
+import org.elasticsearch.index.query.TermQueryBuilder;
+import org.elasticsearch.index.query.WrapperQueryBuilder;
 import org.elasticsearch.rest.RestStatus;
 import org.elasticsearch.script.Script;
 import org.elasticsearch.search.SearchHit;
@@ -56,8 +64,24 @@ import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
 import static org.elasticsearch.index.query.QueryBuilders.*;
 import static org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders.scriptFunction;
 import static org.elasticsearch.test.VersionUtils.randomVersion;
-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
-import static org.hamcrest.Matchers.*;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFirstHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertNoFailures;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchHits;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSecondHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertThirdHit;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasId;
+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.hasScore;
+import static org.hamcrest.Matchers.allOf;
+import static org.hamcrest.Matchers.closeTo;
+import static org.hamcrest.Matchers.containsString;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.is;
 
 public class SearchQueryIT extends ESIntegTestCase {
 
@@ -327,18 +351,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with match query
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.OR)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
         assertThirdHit(searchResponse, hasId("3"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(Operator.AND).analyzer("stop")).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the quick brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND).analyzer("stop")).get();
         assertHitCount(searchResponse, 3l);
         // stop drops "the" since its a stopword
         assertFirstHit(searchResponse, hasId("1"));
@@ -346,7 +370,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with multi match query
-        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the quick brown", "field1", "field2").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the quick brown", "field1", "field2").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("3")); // better score due to different query stats
         assertSecondHit(searchResponse, hasId("1"));
@@ -419,18 +443,18 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("2"));
 
         // try the same with match query
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.OR)).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("1"));
         assertSecondHit(searchResponse, hasId("2"));
         assertThirdHit(searchResponse, hasId("3"));
 
-        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(Operator.AND).analyzer("stop")).get();
+        searchResponse = client().prepareSearch().setQuery(matchQuery("field1", "the fast brown").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND).analyzer("stop")).get();
         assertHitCount(searchResponse, 3l);
         // stop drops "the" since its a stopword
         assertFirstHit(searchResponse, hasId("1"));
@@ -443,7 +467,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertSecondHit(searchResponse, hasId("2"));
 
         // try the same with multi match query
-        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the fast brown", "field1", "field2").cutoffFrequency(3).operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(multiMatchQuery("the fast brown", "field1", "field2").cutoffFrequency(3).operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 3l);
         assertFirstHit(searchResponse, hasId("3")); // better score due to different query stats
         assertSecondHit(searchResponse, hasId("1"));
@@ -914,7 +938,7 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().admin().indices().prepareRefresh("test").get();
         builder = multiMatchQuery("value1", "field1", "field2")
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch()
                 .setQuery(builder)
                 .get();
@@ -923,14 +947,14 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         refresh();
         builder = multiMatchQuery("value1", "field1", "field3^1.5")
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch().setQuery(builder).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "3", "1");
 
         client().admin().indices().prepareRefresh("test").get();
         builder = multiMatchQuery("value1").field("field1").field("field3", 1.5f)
-                .operator(Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
+                .operator(MatchQueryBuilder.Operator.AND); // Operator only applies on terms inside a field! Fields are always OR-ed together.
         searchResponse = client().prepareSearch().setQuery(builder).get();
         assertHitCount(searchResponse, 2l);
         assertSearchHits(searchResponse, "3", "1");
@@ -1573,9 +1597,10 @@ public class SearchQueryIT extends ESIntegTestCase {
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test").setQuery(
-                spanNearQuery(3)
+                spanNearQuery()
                         .clause(spanTermQuery("description", "foo"))
-                        .clause(spanTermQuery("description", "other"))).get();
+                        .clause(spanTermQuery("description", "other"))
+                        .slop(3)).get();
         assertHitCount(searchResponse, 3l);
     }
 
@@ -1620,22 +1645,33 @@ public class SearchQueryIT extends ESIntegTestCase {
         refresh();
 
         SearchResponse searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "brown"))).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "brown"))).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "sleeping")).dist(5)).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "sleeping")).dist(5)).get();
         assertHitCount(searchResponse, 1l);
 
         searchResponse = client().prepareSearch("test")
-                .setQuery(spanNotQuery(spanNearQuery(1)
+                .setQuery(spanNotQuery().include(spanNearQuery()
                         .clause(QueryBuilders.spanTermQuery("description", "quick"))
-                        .clause(QueryBuilders.spanTermQuery("description", "fox")), spanTermQuery("description", "jumped")).pre(1).post(1)).get();
+                        .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "jumped")).pre(1).post(1)).get();
         assertHitCount(searchResponse, 1l);
+
+        try {
+            client().prepareSearch("test")
+                    .setQuery(spanNotQuery().include(spanNearQuery()
+                            .clause(QueryBuilders.spanTermQuery("description", "quick"))
+                            .clause(QueryBuilders.spanTermQuery("description", "fox")).slop(1)).exclude(spanTermQuery("description", "jumped")).dist(2).pre(2)
+                    ).get();
+            fail("ElasticsearchIllegalArgumentException should have been caught");
+        } catch (ElasticsearchException e) {
+            assertThat("ElasticsearchIllegalArgumentException should have been caught", e.getDetailedMessage(), containsString("spanNot can either use [dist] or [pre] & [post] (or none)"));
+        }
     }
 
     @Test
@@ -1731,18 +1767,18 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().prepareIndex("test", "test", "1").setSource("text", "quick brown fox").get();
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fast").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fast").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "fast brown fox").get();
         refresh();
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "quick brown").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1762,12 +1798,12 @@ public class SearchQueryIT extends ESIntegTestCase {
 
         client().prepareIndex("test", "test", "1").setSource("text", "the fox runs across the street").get();
         refresh();
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "run fox run").get();
         refresh();
-        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(matchQuery("text", "fox runs").operator(MatchQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1788,19 +1824,19 @@ public class SearchQueryIT extends ESIntegTestCase {
         client().prepareIndex("test", "test", "1").setSource("text", "quick brown fox").get();
         refresh();
 
-        SearchResponse searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(Operator.AND)).get();
+        SearchResponse searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
-        searchResponse = client().prepareSearch().setQuery(queryStringQuery("fast").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch().setQuery(queryStringQuery("fast").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1);
 
         client().prepareIndex("test", "test", "2").setSource("text", "fast brown fox").get();
         refresh();
 
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
-        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(Operator.AND)).get();
+        searchResponse = client().prepareSearch("test").setQuery(queryStringQuery("quick brown").defaultField("text").defaultOperator(QueryStringQueryBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 2);
     }
 
@@ -1826,7 +1862,7 @@ public class SearchQueryIT extends ESIntegTestCase {
         SearchResponse response = client()
                 .prepareSearch("test")
                 .setQuery(
-                        queryStringQuery("foo.baz").useDisMax(false).defaultOperator(Operator.AND)
+                        queryStringQuery("foo.baz").useDisMax(false).defaultOperator(QueryStringQueryBuilder.Operator.AND)
                                 .field("field1").field("field2")).get();
         assertHitCount(response, 1l);
     }
diff --git a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
index a8c5ccb..e41c451 100644
--- a/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
+++ b/core/src/test/java/org/elasticsearch/search/query/SimpleQueryStringIT.java
@@ -23,7 +23,7 @@ import org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder;
 import org.elasticsearch.action.search.SearchResponse;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.BoolQueryBuilder;
-import org.elasticsearch.index.query.Operator;
+import org.elasticsearch.index.query.SimpleQueryStringBuilder;
 import org.elasticsearch.index.query.SimpleQueryStringFlag;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.junit.Test;
@@ -33,7 +33,10 @@ import java.util.Locale;
 import java.util.concurrent.ExecutionException;
 
 import static org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder;
-import static org.elasticsearch.index.query.QueryBuilders.*;
+import static org.elasticsearch.index.query.QueryBuilders.boolQuery;
+import static org.elasticsearch.index.query.QueryBuilders.queryStringQuery;
+import static org.elasticsearch.index.query.QueryBuilders.simpleQueryStringQuery;
+import static org.elasticsearch.index.query.QueryBuilders.termQuery;
 import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.*;
 import static org.hamcrest.Matchers.equalTo;
 
@@ -67,7 +70,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
         assertFirstHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch().setQuery(
-                simpleQueryStringQuery("foo bar").defaultOperator(Operator.AND)).get();
+                simpleQueryStringQuery("foo bar").defaultOperator(SimpleQueryStringBuilder.Operator.AND)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("3"));
 
@@ -248,21 +251,21 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.OR)).get();
         assertHitCount(searchResponse, 3l);
         assertSearchHits(searchResponse, "1", "2", "3");
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("foo | bar")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("baz | egg*")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.NONE)).get();
         assertHitCount(searchResponse, 0l);
 
@@ -279,7 +282,7 @@ public class SimpleQueryStringIT extends ESIntegTestCase {
 
         searchResponse = client().prepareSearch().setQuery(
                 simpleQueryStringQuery("baz | egg*")
-                        .defaultOperator(Operator.AND)
+                        .defaultOperator(SimpleQueryStringBuilder.Operator.AND)
                         .flags(SimpleQueryStringFlag.WHITESPACE, SimpleQueryStringFlag.PREFIX)).get();
         assertHitCount(searchResponse, 1l);
         assertFirstHit(searchResponse, hasId("4"));
diff --git a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
index 5b559da..6aa31ca 100644
--- a/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
+++ b/core/src/test/java/org/elasticsearch/search/rescore/QueryRescorerIT.java
@@ -33,7 +33,6 @@ import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.common.xcontent.XContentFactory;
 import org.elasticsearch.index.query.MatchQueryBuilder;
-import org.elasticsearch.index.query.Operator;
 import org.elasticsearch.index.query.QueryBuilders;
 import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;
 import org.elasticsearch.script.Script;
@@ -117,7 +116,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         ensureYellow();
         refresh();
         SearchResponse searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "quick brown").slop(2).boost(4.0f)).setRescoreQueryWeight(2))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -127,7 +126,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         assertThat(searchResponse.getHits().getHits()[2].getId(), equalTo("2"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(3)))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -137,7 +136,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         assertThirdHit(searchResponse, hasId("3"));
 
         searchResponse = client().prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                 .setRescorer(RescoreBuilder.queryRescorer((QueryBuilders.matchPhraseQuery("field1", "the quick brown"))))
                 .setRescoreWindow(5).execute().actionGet();
 
@@ -180,7 +179,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         client().admin().indices().prepareRefresh("test").execute().actionGet();
         SearchResponse searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setRescorer(
@@ -195,7 +194,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
@@ -212,7 +211,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // Make sure non-zero from works:
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "lexington avenue massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(2)
                 .setSize(5)
                 .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
@@ -321,7 +320,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
 
         SearchResponse searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
             .setSize(5).execute().actionGet();
         assertThat(searchResponse.getHits().hits().length, equalTo(4));
@@ -334,7 +333,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
         // Now, penalizing rescore (nothing matches the rescore query):
         searchResponse = client()
                 .prepareSearch()
-                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(Operator.OR))
+                .setQuery(QueryBuilders.matchQuery("field1", "massachusetts").operator(MatchQueryBuilder.Operator.OR))
                 .setFrom(0)
                 .setSize(5)
                 .setRescorer(
@@ -426,7 +425,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -441,7 +440,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse plain = client().prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR)).setFrom(0).setSize(resultSize)
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR)).setFrom(0).setSize(resultSize)
                     .execute().actionGet();
             
             // check equivalence
@@ -451,7 +450,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -469,7 +468,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                     .prepareSearch()
                     .setSearchType(SearchType.QUERY_THEN_FETCH)
                     .setPreference("test") // ensure we hit the same shards for tie-breaking
-                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", query).operator(MatchQueryBuilder.Operator.OR))
                     .setFrom(0)
                     .setSize(resultSize)
                     .setRescorer(
@@ -504,7 +503,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse searchResponse = client()
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                     .setRescorer(
                             RescoreBuilder.queryRescorer(QueryBuilders.matchPhraseQuery("field1", "the quick brown").slop(2).boost(4.0f))
                                     .setQueryWeight(0.5f).setRescoreQueryWeight(0.4f)).setRescoreWindow(5).setExplain(true).execute()
@@ -542,7 +541,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
             SearchResponse searchResponse = client()
                     .prepareSearch()
                     .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                    .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                     .setRescorer(innerRescoreQuery).setRescoreWindow(5).setExplain(true).execute()
                     .actionGet();
             assertHitCount(searchResponse, 3);
@@ -565,7 +564,7 @@ public class QueryRescorerIT extends ESIntegTestCase {
                 searchResponse = client()
                         .prepareSearch()
                         .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
-                        .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(Operator.OR))
+                        .setQuery(QueryBuilders.matchQuery("field1", "the quick brown").operator(MatchQueryBuilder.Operator.OR))
                         .addRescorer(innerRescoreQuery).setRescoreWindow(5)
                         .addRescorer(outerRescoreQuery).setRescoreWindow(10)
                         .setExplain(true).get();
diff --git a/core/src/test/java/org/elasticsearch/search/scan/ScanContextTests.java b/core/src/test/java/org/elasticsearch/search/scan/ScanContextTests.java
index 918bdd6..e804393 100644
--- a/core/src/test/java/org/elasticsearch/search/scan/ScanContextTests.java
+++ b/core/src/test/java/org/elasticsearch/search/scan/ScanContextTests.java
@@ -31,8 +31,10 @@ import org.apache.lucene.search.QueryUtils;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
 import org.elasticsearch.search.scan.ScanContext.MinDocQuery;
+import org.elasticsearch.search.scan.ScanContext.ScanCollector;
 import org.elasticsearch.test.ESTestCase;
 
 import java.io.IOException;
@@ -69,6 +71,13 @@ public class ScanContextTests extends ESTestCase {
         dir.close();
     }
 
+    private static TopDocs execute(IndexSearcher searcher, ScanContext ctx, Query query, int pageSize, boolean trackScores) throws IOException {
+        query = ctx.wrapQuery(query);
+        ScanCollector collector = ctx.collector(pageSize, trackScores);
+        searcher.search(query, collector);
+        return collector.topDocs();
+    }
+
     public void testRandom() throws Exception {
         final int numDocs = randomIntBetween(10, 200);
         final Document doc1 = new Document();
@@ -93,10 +102,10 @@ public class ScanContextTests extends ESTestCase {
         final List<ScoreDoc> actual = new ArrayList<>();
         ScanContext context = new ScanContext();
         while (true) {
-            final ScoreDoc[] page = context.execute(searcher, query, pageSize, trackScores).scoreDocs;
+            final ScoreDoc[] page = execute(searcher,context, query, pageSize, trackScores).scoreDocs;
             assertTrue(page.length <= pageSize);
             if (page.length == 0) {
-                assertEquals(0, context.execute(searcher, query, pageSize, trackScores).scoreDocs.length);
+                assertEquals(0, execute(searcher, context, query, pageSize, trackScores).scoreDocs.length);
                 break;
             }
             actual.addAll(Arrays.asList(page));
diff --git a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
index 380c301..df7eb4e 100644
--- a/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
+++ b/core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java
@@ -996,6 +996,7 @@ public class SharedClusterSnapshotRestoreIT extends AbstractSnapshotIntegTestCas
     }
 
     @Test
+    @AwaitsFix(bugUrl = "https://github.com/elastic/elasticsearch/issues/12855")
     public void renameOnRestoreTest() throws Exception {
         Client client = client();
 
diff --git a/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java b/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
index e238272..a4f8f3b 100644
--- a/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESAllocationTestCase.java
@@ -19,13 +19,13 @@
 package org.elasticsearch.test;
 
 import org.elasticsearch.Version;
+import org.elasticsearch.cluster.ClusterModule;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.EmptyClusterInfoService;
 import org.elasticsearch.cluster.node.DiscoveryNode;
 import org.elasticsearch.cluster.routing.RoutingNode;
 import org.elasticsearch.cluster.routing.RoutingTable;
 import org.elasticsearch.cluster.routing.ShardRouting;
-import org.elasticsearch.cluster.routing.allocation.AllocationModule;
 import org.elasticsearch.cluster.routing.allocation.AllocationService;
 import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;
 import org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators;
@@ -74,9 +74,9 @@ public abstract class ESAllocationTestCase extends ESTestCase {
 
 
     public static AllocationDeciders randomAllocationDeciders(Settings settings, NodeSettingsService nodeSettingsService, Random random) {
-        final List<Class<? extends AllocationDecider>> defaultAllocationDeciders = AllocationModule.DEFAULT_ALLOCATION_DECIDERS;
+        final List<Class<? extends AllocationDecider>> defaultAllocationDeciders = ClusterModule.DEFAULT_ALLOCATION_DECIDERS;
         final List<AllocationDecider> list = new ArrayList<>();
-        for (Class<? extends AllocationDecider> deciderClass : AllocationModule.DEFAULT_ALLOCATION_DECIDERS) {
+        for (Class<? extends AllocationDecider> deciderClass : ClusterModule.DEFAULT_ALLOCATION_DECIDERS) {
             try {
                 try {
                     Constructor<? extends AllocationDecider> constructor = deciderClass.getConstructor(Settings.class, NodeSettingsService.class);
diff --git a/core/src/test/java/org/elasticsearch/test/ESTestCase.java b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
index 2ebcdd0..97d5a2e 100644
--- a/core/src/test/java/org/elasticsearch/test/ESTestCase.java
+++ b/core/src/test/java/org/elasticsearch/test/ESTestCase.java
@@ -385,11 +385,6 @@ public abstract class ESTestCase extends LuceneTestCase {
         }
         return array;
     }
-    
-    public static String randomTimeValue() {
-        final String[] values = new String[]{"d", "H", "ms", "s", "S", "w"};
-        return randomIntBetween(0, 1000) + randomFrom(values);
-    }
 
     /**
      * Runs the code block for 10 seconds waiting for no assertion to trip.
diff --git a/core/src/test/java/org/elasticsearch/test/TestSearchContext.java b/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
index 44004fe..4527fb5 100644
--- a/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
+++ b/core/src/test/java/org/elasticsearch/test/TestSearchContext.java
@@ -20,6 +20,7 @@ package org.elasticsearch.test;
 
 import com.carrotsearch.hppc.ObjectObjectAssociativeContainer;
 
+import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
@@ -74,6 +75,7 @@ public class TestSearchContext extends SearchContext {
     final IndexFieldDataService indexFieldDataService;
     final BitsetFilterCache fixedBitSetFilterCache;
     final ThreadPool threadPool;
+    final Map<Class<?>, Collector> queryCollectors = new HashMap<>();
 
     ContextIndexSearcher searcher;
     int size;
@@ -411,16 +413,6 @@ public class TestSearchContext extends SearchContext {
     }
 
     @Override
-    public boolean queryRewritten() {
-        return false;
-    }
-
-    @Override
-    public SearchContext updateRewriteQuery(Query rewriteQuery) {
-        return null;
-    }
-
-    @Override
     public int from() {
         return 0;
     }
@@ -667,4 +659,9 @@ public class TestSearchContext extends SearchContext {
 
     @Override
     public void copyContextAndHeadersFrom(HasContextAndHeaders other) {}
+
+    @Override
+    public Map<Class<?>, Collector> queryCollectors() {
+        return queryCollectors;
+    }
 }
diff --git a/core/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java b/core/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java
index 8d2caad..a824ec7 100644
--- a/core/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java
+++ b/core/src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java
@@ -78,10 +78,14 @@ public class ReproduceInfoPrinter extends RunListener {
 
         final StringBuilder b = new StringBuilder();
         if (inVerifyPhase()) {
-            b.append("REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests");
+            b.append("REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests" );
         } else {
             b.append("REPRODUCE WITH: mvn test -Pdev");
         }
+        String project = System.getProperty("tests.project");
+        if (project != null) {
+            b.append(" -pl " + project);
+        }
         MavenMessageBuilder mavenMessageBuilder = new MavenMessageBuilder(b);
         mavenMessageBuilder.appendAllOpts(failure.getDescription());
 
diff --git a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
index 1b19adb..7bda47b 100644
--- a/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
+++ b/core/src/test/java/org/elasticsearch/test/transport/AssertingLocalTransport.java
@@ -75,7 +75,7 @@ public class AssertingLocalTransport extends LocalTransport {
         ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), response);
         super.handleParsedResponse(response, handler);
     }
-
+    
     @Override
     public void sendRequest(final DiscoveryNode node, final long requestId, final String action, final TransportRequest request, TransportRequestOptions options) throws IOException, TransportException {
         ElasticsearchAssertions.assertVersionSerializable(VersionUtils.randomVersionBetween(random, minVersion, maxVersion), request);
diff --git a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java
index 08af3fc..9041bca 100644
--- a/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java
+++ b/core/src/test/java/org/elasticsearch/transport/AbstractSimpleTransportTests.java
@@ -52,7 +52,6 @@ public abstract class AbstractSimpleTransportTests extends ESTestCase {
 
     protected ThreadPool threadPool;
 
-    protected static final NamedWriteableRegistry namedWriteableRegistry = new NamedWriteableRegistry();
     protected static final Version version0 = Version.fromId(/*0*/99);
     protected DiscoveryNode nodeA;
     protected MockTransportService serviceA;
diff --git a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
index 0aac324..ff6a21b 100644
--- a/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/ContextAndHeaderTransportIT.java
@@ -122,7 +122,7 @@ public class ContextAndHeaderTransportIT extends ESIntegTestCase {
                 .setSource(jsonBuilder().startObject().field("username", "foo").endObject()).get();
         transportClient().admin().indices().prepareRefresh(queryIndex, lookupIndex).get();
 
-        TermsQueryBuilder termsLookupFilterBuilder = QueryBuilders.termsLookupQuery("username").lookupIndex(lookupIndex).lookupType("type").lookupId("1").lookupPath("followers");
+        TermsLookupQueryBuilder termsLookupFilterBuilder = QueryBuilders.termsLookupQuery("username").lookupIndex(lookupIndex).lookupType("type").lookupId("1").lookupPath("followers");
         BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery().must(QueryBuilders.matchAllQuery()).must(termsLookupFilterBuilder);
 
         SearchResponse searchResponse = transportClient()
diff --git a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java
index 1cf72af..ad38b4c 100644
--- a/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java
+++ b/core/src/test/java/org/elasticsearch/transport/netty/NettyTransportIT.java
@@ -112,15 +112,15 @@ public class NettyTransportIT extends ESIntegTestCase {
 
         @Override
         public ChannelPipelineFactory configureServerChannelPipelineFactory(String name, Settings groupSettings) {
-            return new ErrorPipelineFactory(this, name, groupSettings, namedWriteableRegistry);
+            return new ErrorPipelineFactory(this, name, groupSettings);
         }
 
         private static class ErrorPipelineFactory extends ServerChannelPipelineFactory {
 
             private final ESLogger logger;
 
-            public ErrorPipelineFactory(ExceptionThrowingNettyTransport exceptionThrowingNettyTransport, String name, Settings groupSettings, NamedWriteableRegistry namedWriteableRegistry) {
-                super(exceptionThrowingNettyTransport, name, groupSettings, namedWriteableRegistry);
+            public ErrorPipelineFactory(ExceptionThrowingNettyTransport exceptionThrowingNettyTransport, String name, Settings groupSettings) {
+                super(exceptionThrowingNettyTransport, name, groupSettings);
                 this.logger = exceptionThrowingNettyTransport.logger;
             }
 
diff --git a/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java b/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
index 6f70d36..be493cd 100644
--- a/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
+++ b/core/src/test/java/org/elasticsearch/validate/SimpleValidateQueryIT.java
@@ -236,7 +236,7 @@ public class SimpleValidateQueryIT extends ESIntegTestCase {
                 containsString("(field:huge field:brown) +field:pidgin"), true);
         assertExplanation(QueryBuilders.commonTermsQuery("field", "the brown").analyzer("stop"),
                 containsString("field:brown"), true);
-
+        
         // match queries with cutoff frequency
         assertExplanation(QueryBuilders.matchQuery("field", "huge brown pidgin").cutoffFrequency(1),
                 containsString("(field:huge field:brown) +field:pidgin"), true);
@@ -276,7 +276,11 @@ public class SimpleValidateQueryIT extends ESIntegTestCase {
         assertThat(client().admin().indices().prepareValidateQuery("test").setSource(new BytesArray("{\"query\": {\"term\" : { \"user\" : \"kimchy\" }}, \"foo\": \"bar\"}")).get().isValid(), equalTo(false));
     }
 
-    private static void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher, boolean withRewrite) {
+    private void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher) {
+        assertExplanation(queryBuilder, matcher, false);
+    }
+
+    private void assertExplanation(QueryBuilder queryBuilder, Matcher<String> matcher, boolean withRewrite) {
         ValidateQueryResponse response = client().admin().indices().prepareValidateQuery("test")
                 .setTypes("type1")
                 .setQuery(queryBuilder)
diff --git a/dev-tools/pom.xml b/dev-tools/pom.xml
index b8abf88..b53889e 100644
--- a/dev-tools/pom.xml
+++ b/dev-tools/pom.xml
@@ -2,7 +2,7 @@
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.elasticsearch</groupId>
   <artifactId>elasticsearch-dev-tools</artifactId>
-  <version>2.0.0-SNAPSHOT</version>
+  <version>2.1.0-SNAPSHOT</version>
   <name>Elasticsearch Build Resources</name>
   <description>Tools to assist in building and developing in the Elasticsearch project</description>
   <parent>
@@ -13,6 +13,7 @@
   </parent>
   <properties>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
+    <elasticsearch.s3.repository>s3://download.elasticsearch.org/elasticsearch/staging/</elasticsearch.s3.repository>
   </properties>
   <build>
     <plugins>
@@ -34,5 +35,48 @@
         </configuration>
       </plugin>
     </plugins>
+    <extensions>
+      <extension>
+        <groupId>org.springframework.build</groupId>
+        <artifactId>aws-maven</artifactId>
+        <version>5.0.0.RELEASE</version>
+      </extension>
+    </extensions>
   </build>
+  <profiles>
+    <profile>
+      <id>release</id>
+      <build>
+        <!-- sign the artifacts with GPG -->
+        <plugins>
+          <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-gpg-plugin</artifactId>
+            <version>1.6</version>
+            <executions>
+              <execution>
+                <id>sign-artifacts</id>
+                <phase>verify</phase>
+                <goals>
+                  <goal>sign</goal>
+                </goals>
+                <configuration>
+                  <keyname>${gpg.keyname}</keyname>
+                  <passphrase>${gpg.passphrase}</passphrase>
+                  <defaultKeyring>${gpg.keyring}</defaultKeyring>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+      <distributionManagement>
+        <repository>
+          <id>aws-release</id>
+          <name>AWS Release Repository</name>
+          <url>${elasticsearch.s3.repository}</url>
+        </repository>
+      </distributionManagement>
+    </profile>
+  </profiles>
 </project>
diff --git a/dev-tools/prepare_release.py b/dev-tools/prepare_release.py
deleted file mode 100644
index 9f3e72d..0000000
--- a/dev-tools/prepare_release.py
+++ /dev/null
@@ -1,186 +0,0 @@
-# Licensed to Elasticsearch under one or more contributor
-# license agreements. See the NOTICE file distributed with
-# this work for additional information regarding copyright
-# ownership. Elasticsearch licenses this file to you under
-# the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance  with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on
-# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
-# either express or implied. See the License for the specific
-# language governing permissions and limitations under the License.
-
-# Prepare a release
-#
-# This script prepares a new release by creating two commits
-#
-# First commit: Update the Version.java to remove the snapshot bit
-# First commit: Remove the -SNAPSHOT suffix in all pom.xml files
-# Second commit: Update Documentation flags
-#
-# USAGE:
-#
-# python3 ./dev-tools/prepare-release.py
-#
-# Note: Ensure the script is run from the root directory
-#
-
-import fnmatch
-import subprocess
-import tempfile
-import re
-import os
-import shutil
-
-VERSION_FILE = 'core/src/main/java/org/elasticsearch/Version.java'
-POM_FILE = 'pom.xml'
-
-def run(command):
-  if os.system('%s' % (command)):
-    raise RuntimeError('    FAILED: %s' % (command))
-
-def ensure_checkout_is_clean():
-  # Make sure no local mods:
-  s = subprocess.check_output('git diff --shortstat', shell=True)
-  if len(s) > 0:
-    raise RuntimeError('git diff --shortstat is non-empty: got:\n%s' % s)
-
-  # Make sure no untracked files:
-  s = subprocess.check_output('git status', shell=True).decode('utf-8', errors='replace')
-  if 'Untracked files:' in s:
-    raise RuntimeError('git status shows untracked files: got:\n%s' % s)
-
-  # Make sure we have all changes from origin:
-  if 'is behind' in s:
-    raise RuntimeError('git status shows not all changes pulled from origin; try running "git pull origin" in this branch: got:\n%s' % (s))
-
-  # Make sure we no local unpushed changes (this is supposed to be a clean area):
-  if 'is ahead' in s:
-    raise RuntimeError('git status shows local commits; try running "git fetch origin", "git checkout ", "git reset --hard origin/" in this branch: got:\n%s' % (s))
-
-# Reads the given file and applies the
-# callback to it. If the callback changed
-# a line the given file is replaced with
-# the modified input.
-def process_file(file_path, line_callback):
-  fh, abs_path = tempfile.mkstemp()
-  modified = False
-  with open(abs_path,'w', encoding='utf-8') as new_file:
-    with open(file_path, encoding='utf-8') as old_file:
-      for line in old_file:
-        new_line = line_callback(line)
-        modified = modified or (new_line != line)
-        new_file.write(new_line)
-  os.close(fh)
-  if modified:
-    #Remove original file
-    os.remove(file_path)
-    #Move new file
-    shutil.move(abs_path, file_path)
-    return True
-  else:
-    # nothing to do - just remove the tmp file
-    os.remove(abs_path)
-    return False
-
-# Moves the pom.xml file from a snapshot to a release
-def remove_maven_snapshot(poms, release):
-  for pom in poms:
-    if pom:
-      #print('Replacing SNAPSHOT version in file %s' % (pom))
-      pattern = '<version>%s-SNAPSHOT</version>' % (release)
-      replacement = '<version>%s</version>' % (release)
-      def callback(line):
-        return line.replace(pattern, replacement)
-      process_file(pom, callback)
-
-# Moves the Version.java file from a snapshot to a release
-def remove_version_snapshot(version_file, release):
-  # 1.0.0.Beta1 -> 1_0_0_Beta1
-  release = release.replace('.', '_')
-  release = release.replace('-', '_')
-  pattern = 'new Version(V_%s_ID, true' % (release)
-  replacement = 'new Version(V_%s_ID, false' % (release)
-  def callback(line):
-    return line.replace(pattern, replacement)
-  processed = process_file(version_file, callback)
-  if not processed:
-    raise RuntimeError('failed to remove snapshot version for %s' % (release))
-
-# finds all the pom files that do have a -SNAPSHOT version
-def find_pom_files_with_snapshots():
-  files = subprocess.check_output('find . -name pom.xml -exec grep -l "<version>.*-SNAPSHOT</version>" {} ";"', shell=True)
-  return files.decode('utf-8').split('\n')
-
-# Checks the pom.xml for the release version.
-# This method fails if the pom file has no SNAPSHOT version set ie.
-# if the version is already on a release version we fail.
-# Returns the next version string ie. 0.90.7
-def find_release_version():
-  with open('pom.xml', encoding='utf-8') as file:
-    for line in file:
-      match = re.search(r'<version>(.+)-SNAPSHOT</version>', line)
-      if match:
-        return match.group(1)
-    raise RuntimeError('Could not find release version in branch')
-
-# Stages the given files for the next git commit
-def add_pending_files(*files):
-  for file in files:
-    if file:
-      # print("Adding file: %s" % (file))
-      run('git add %s' % (file))
-
-# Executes a git commit with 'release [version]' as the commit message
-def commit_release(release):
-  run('git commit -m "Release: Change version from %s-SNAPSHOT to %s"' % (release, release))
-
-def commit_feature_flags(release):
-    run('git commit -m "Update Documentation Feature Flags [%s]"' % release)
-
-# Walks the given directory path (defaults to 'docs')
-# and replaces all 'coming[$version]' tags with
-# 'added[$version]'. This method only accesses asciidoc files.
-def update_reference_docs(release_version, path='docs'):
-  pattern = 'coming[%s' % (release_version)
-  replacement = 'added[%s' % (release_version)
-  pending_files = []
-  def callback(line):
-    return line.replace(pattern, replacement)
-  for root, _, file_names in os.walk(path):
-    for file_name in fnmatch.filter(file_names, '*.asciidoc'):
-      full_path = os.path.join(root, file_name)
-      if process_file(full_path, callback):
-        pending_files.append(os.path.join(root, file_name))
-  return pending_files
-
-if __name__ == "__main__":
-  release_version = find_release_version()
-
-  print('*** Preparing release version: [%s]' % release_version)
-
-  ensure_checkout_is_clean()
-  pom_files = find_pom_files_with_snapshots()
-
-  remove_maven_snapshot(pom_files, release_version)
-  remove_version_snapshot(VERSION_FILE, release_version)
-
-  pending_files = pom_files
-  pending_files.append(VERSION_FILE)
-  add_pending_files(*pending_files) # expects var args use * to expand
-  commit_release(release_version)
-
-  pending_files = update_reference_docs(release_version)
-  # split commits for docs and version to enable easy cherry-picking
-  if pending_files:
-    add_pending_files(*pending_files) # expects var args use * to expand
-    commit_feature_flags(release_version)
-  else:
-    print('WARNING: no documentation references updates for release %s' % (release_version))
-
-  print('*** Done removing snapshot version. Run git push manually.')
-
diff --git a/dev-tools/prepare_release_create_release_version.py b/dev-tools/prepare_release_create_release_version.py
new file mode 100644
index 0000000..07446e5
--- /dev/null
+++ b/dev-tools/prepare_release_create_release_version.py
@@ -0,0 +1,145 @@
+# Licensed to Elasticsearch under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Elasticsearch licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance  with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on
+# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
+# either express or implied. See the License for the specific
+# language governing permissions and limitations under the License.
+
+# Prepare a release
+#
+# 1. Update the Version.java to remove the snapshot bit
+# 2. Remove the -SNAPSHOT suffix in all pom.xml files
+#
+# USAGE:
+#
+# python3 ./dev-tools/prepare-release.py
+#
+# Note: Ensure the script is run from the root directory
+#
+
+import fnmatch
+import subprocess
+import tempfile
+import re
+import os
+import shutil
+
+VERSION_FILE = 'core/src/main/java/org/elasticsearch/Version.java'
+POM_FILE = 'pom.xml'
+
+def run(command):
+  if os.system('%s' % (command)):
+    raise RuntimeError('    FAILED: %s' % (command))
+
+def ensure_checkout_is_clean():
+  # Make sure no local mods:
+  s = subprocess.check_output('git diff --shortstat', shell=True)
+  if len(s) > 0:
+    raise RuntimeError('git diff --shortstat is non-empty: got:\n%s' % s)
+
+  # Make sure no untracked files:
+  s = subprocess.check_output('git status', shell=True).decode('utf-8', errors='replace')
+  if 'Untracked files:' in s:
+    raise RuntimeError('git status shows untracked files: got:\n%s' % s)
+
+  # Make sure we have all changes from origin:
+  if 'is behind' in s:
+    raise RuntimeError('git status shows not all changes pulled from origin; try running "git pull origin" in this branch: got:\n%s' % (s))
+
+  # Make sure we no local unpushed changes (this is supposed to be a clean area):
+  if 'is ahead' in s:
+    raise RuntimeError('git status shows local commits; try running "git fetch origin", "git checkout ", "git reset --hard origin/" in this branch: got:\n%s' % (s))
+
+# Reads the given file and applies the
+# callback to it. If the callback changed
+# a line the given file is replaced with
+# the modified input.
+def process_file(file_path, line_callback):
+  fh, abs_path = tempfile.mkstemp()
+  modified = False
+  with open(abs_path,'w', encoding='utf-8') as new_file:
+    with open(file_path, encoding='utf-8') as old_file:
+      for line in old_file:
+        new_line = line_callback(line)
+        modified = modified or (new_line != line)
+        new_file.write(new_line)
+  os.close(fh)
+  if modified:
+    #Remove original file
+    os.remove(file_path)
+    #Move new file
+    shutil.move(abs_path, file_path)
+    return True
+  else:
+    # nothing to do - just remove the tmp file
+    os.remove(abs_path)
+    return False
+
+# Moves the pom.xml file from a snapshot to a release
+def remove_maven_snapshot(poms, release):
+  for pom in poms:
+    if pom:
+      #print('Replacing SNAPSHOT version in file %s' % (pom))
+      pattern = '<version>%s-SNAPSHOT</version>' % (release)
+      replacement = '<version>%s</version>' % (release)
+      def callback(line):
+        return line.replace(pattern, replacement)
+      process_file(pom, callback)
+
+# Moves the Version.java file from a snapshot to a release
+def remove_version_snapshot(version_file, release):
+  # 1.0.0.Beta1 -> 1_0_0_Beta1
+  release = release.replace('.', '_')
+  release = release.replace('-', '_')
+  pattern = 'new Version(V_%s_ID, true' % (release)
+  replacement = 'new Version(V_%s_ID, false' % (release)
+  def callback(line):
+    return line.replace(pattern, replacement)
+  processed = process_file(version_file, callback)
+  if not processed:
+    raise RuntimeError('failed to remove snapshot version for %s' % (release))
+
+# finds all the pom files that do have a -SNAPSHOT version
+def find_pom_files_with_snapshots():
+  files = subprocess.check_output('find . -name pom.xml -exec grep -l "<version>.*-SNAPSHOT</version>" {} ";"', shell=True)
+  return files.decode('utf-8').split('\n')
+
+# Checks the pom.xml for the release version.
+# This method fails if the pom file has no SNAPSHOT version set ie.
+# if the version is already on a release version we fail.
+# Returns the next version string ie. 0.90.7
+def find_release_version():
+  with open('pom.xml', encoding='utf-8') as file:
+    for line in file:
+      match = re.search(r'<version>(.+)-SNAPSHOT</version>', line)
+      if match:
+        return match.group(1)
+    raise RuntimeError('Could not find release version in branch')
+
+
+if __name__ == "__main__":
+  release_version = find_release_version()
+
+  print('*** Preparing release version: [%s]' % release_version)
+
+  ensure_checkout_is_clean()
+  pom_files = find_pom_files_with_snapshots()
+
+  remove_maven_snapshot(pom_files, release_version)
+  remove_version_snapshot(VERSION_FILE, release_version)
+
+  print('*** Done removing snapshot version. DO NOT COMMIT THIS, WHEN CREATING A RELEASE CANDIDATE.')
+
+  shortHash = subprocess.check_output('git log --pretty=format:"%h" -n 1', shell=True)
+  print('')
+  print('*** To create a release candidate run: ')
+  print('  mvn clean deploy -Prelease -DskipTests -Dgpg.keyname="$GPG_KEY_ID" -Dgpg.passphrase="$GPG_PASSPHRASE" -Dpackaging.rpm.rpmbuild=/usr/bin/rpmbuild -Delasticsearch.s3.repository=s3://download.elasticsearch.org/elasticsearch/staging/%s' % (shortHash.decode('utf-8')))
diff --git a/dev-tools/prepare_release_update_documentation.py b/dev-tools/prepare_release_update_documentation.py
new file mode 100644
index 0000000..c7eae4e
--- /dev/null
+++ b/dev-tools/prepare_release_update_documentation.py
@@ -0,0 +1,139 @@
+# Licensed to Elasticsearch under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Elasticsearch licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance  with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on
+# an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
+# either express or implied. See the License for the specific
+# language governing permissions and limitations under the License.
+
+# Prepare a release: Update the documentation and commit
+#
+# USAGE:
+#
+# python3 ./dev-tools/prepare_release_update_documentation.py
+#
+# Note: Ensure the script is run from the root directory
+#       This script needs to be run and then pushed,
+#       before proceeding with prepare_release_create-release-version.py
+#       on your build VM
+#
+
+import fnmatch
+import subprocess
+import tempfile
+import re
+import os
+import shutil
+
+def run(command):
+  if os.system('%s' % (command)):
+    raise RuntimeError('    FAILED: %s' % (command))
+
+def ensure_checkout_is_clean():
+  # Make sure no local mods:
+  s = subprocess.check_output('git diff --shortstat', shell=True)
+  if len(s) > 0:
+    raise RuntimeError('git diff --shortstat is non-empty: got:\n%s' % s)
+
+  # Make sure no untracked files:
+  s = subprocess.check_output('git status', shell=True).decode('utf-8', errors='replace')
+  if 'Untracked files:' in s:
+    raise RuntimeError('git status shows untracked files: got:\n%s' % s)
+
+  # Make sure we have all changes from origin:
+  if 'is behind' in s:
+    raise RuntimeError('git status shows not all changes pulled from origin; try running "git pull origin" in this branch: got:\n%s' % (s))
+
+  # Make sure we no local unpushed changes (this is supposed to be a clean area):
+  if 'is ahead' in s:
+    raise RuntimeError('git status shows local commits; try running "git fetch origin", "git checkout ", "git reset --hard origin/" in this branch: got:\n%s' % (s))
+
+# Reads the given file and applies the
+# callback to it. If the callback changed
+# a line the given file is replaced with
+# the modified input.
+def process_file(file_path, line_callback):
+  fh, abs_path = tempfile.mkstemp()
+  modified = False
+  with open(abs_path,'w', encoding='utf-8') as new_file:
+    with open(file_path, encoding='utf-8') as old_file:
+      for line in old_file:
+        new_line = line_callback(line)
+        modified = modified or (new_line != line)
+        new_file.write(new_line)
+  os.close(fh)
+  if modified:
+    #Remove original file
+    os.remove(file_path)
+    #Move new file
+    shutil.move(abs_path, file_path)
+    return True
+  else:
+    # nothing to do - just remove the tmp file
+    os.remove(abs_path)
+    return False
+
+# Checks the pom.xml for the release version.
+# This method fails if the pom file has no SNAPSHOT version set ie.
+# if the version is already on a release version we fail.
+# Returns the next version string ie. 0.90.7
+def find_release_version():
+  with open('pom.xml', encoding='utf-8') as file:
+    for line in file:
+      match = re.search(r'<version>(.+)-SNAPSHOT</version>', line)
+      if match:
+        return match.group(1)
+    raise RuntimeError('Could not find release version in branch')
+
+# Stages the given files for the next git commit
+def add_pending_files(*files):
+  for file in files:
+    if file:
+      # print("Adding file: %s" % (file))
+      run('git add %s' % (file))
+
+# Updates documentation feature flags
+def commit_feature_flags(release):
+    run('git commit -m "Update Documentation Feature Flags [%s]"' % release)
+
+# Walks the given directory path (defaults to 'docs')
+# and replaces all 'coming[$version]' tags with
+# 'added[$version]'. This method only accesses asciidoc files.
+def update_reference_docs(release_version, path='docs'):
+  pattern = 'coming[%s' % (release_version)
+  replacement = 'added[%s' % (release_version)
+  pending_files = []
+  def callback(line):
+    return line.replace(pattern, replacement)
+  for root, _, file_names in os.walk(path):
+    for file_name in fnmatch.filter(file_names, '*.asciidoc'):
+      full_path = os.path.join(root, file_name)
+      if process_file(full_path, callback):
+        pending_files.append(os.path.join(root, file_name))
+  return pending_files
+
+if __name__ == "__main__":
+  release_version = find_release_version()
+
+  print('*** Preparing release version documentation: [%s]' % release_version)
+
+  ensure_checkout_is_clean()
+
+  pending_files = update_reference_docs(release_version)
+  
+  if pending_files:
+    add_pending_files(*pending_files) # expects var args use * to expand
+    commit_feature_flags(release_version)
+  else:
+    print('WARNING: no documentation references updates for release %s' % (release_version))
+
+  print('*** Done.')
+
diff --git a/dev-tools/src/main/resources/license-check/check_license_and_sha.pl b/dev-tools/src/main/resources/license-check/check_license_and_sha.pl
index 05a155c..9263244 100755
--- a/dev-tools/src/main/resources/license-check/check_license_and_sha.pl
+++ b/dev-tools/src/main/resources/license-check/check_license_and_sha.pl
@@ -19,6 +19,12 @@ die usage() unless $mode =~ /^--(check|update)$/;
 
 my $License_Dir = shift(@ARGV) || die usage();
 my $Source      = shift(@ARGV) || die usage();
+my $Ignore      = shift(@ARGV);
+my $ignore
+    = $Ignore
+    ? qr/${Ignore}[^\/]*$/
+    : qr/elasticsearch[^\/]*$/;
+
 $License_Dir = File::Spec->rel2abs($License_Dir) . '/';
 $Source      = File::Spec->rel2abs($Source);
 
@@ -29,8 +35,8 @@ die "License dir is not a directory: $License_Dir\n" . usage()
     unless -d $License_Dir;
 
 my %shas
-    = -f $Source ? jars_from_zip($Source)
-    : -d $Source ? jars_from_dir($Source)
+    = -f $Source ? jars_from_zip( $Source, $ignore )
+    : -d $Source ? jars_from_dir( $Source, $ignore )
     :   die "Source is neither a directory nor a zip file: $Source" . usage();
 
 $mode eq '--check'
@@ -194,25 +200,25 @@ sub get_sha_files {
 #===================================
 sub jars_from_zip {
 #===================================
-    my ($source) = @_;
+    my ( $source, $ignore ) = @_;
     my $temp_dir = File::Temp->newdir;
     my $dir_name = $temp_dir->dirname;
-    my $archive = Archive::Extract->new( archive => $source, type => 'zip' );
+    my $archive  = Archive::Extract->new( archive => $source, type => 'zip' );
     $archive->extract( to => $dir_name ) || die $archive->error;
     my @jars = map { File::Spec->rel2abs( $_, $dir_name ) }
-        grep { /\.jar$/ && !/elasticsearch[^\/]*$/ } @{ $archive->files };
+        grep { /\.jar$/ && !/$ignore/ } @{ $archive->files };
     return calculate_shas(@jars);
 }
 
 #===================================
 sub jars_from_dir {
 #===================================
-    my $source = shift;
+    my ( $source, $ignore ) = @_;
     my @jars;
     File::Find::find(
         {   wanted => sub {
                 push @jars, File::Spec->rel2abs( $_, $source )
-                    if /\.jar$/ && !/elasticsearch[^\/]*$/;
+                    if /\.jar$/ && !/$ignore/;
             },
             no_chdir => 1
         },
@@ -241,12 +247,14 @@ sub usage {
 USAGE:
 
     # check the sha1 and LICENSE files for each jar in the zip or directory
-    $0 --check  path/to/licenses/ path/to/package.zip
-    $0 --check  path/to/licenses/ path/to/dir/
+    $0 --check  path/to/licenses/ path/to/package.zip [prefix_to_ignore]
+    $0 --check  path/to/licenses/ path/to/dir/ [prefix_to_ignore]
 
     # updates the sha1s for each jar in the zip or directory
-    $0 --update path/to/licenses/ path/to/package.zip
-    $0 --update path/to/licenses/ path/to/dir/
+    $0 --update path/to/licenses/ path/to/package.zip [prefix_to_ignore]
+    $0 --update path/to/licenses/ path/to/dir/ [prefix_to_ignore]
+
+The optional prefix_to_ignore parameter defaults to "elasticsearch".
 
 USAGE
 
diff --git a/distribution/deb/pom.xml b/distribution/deb/pom.xml
index b87824d..4f1a4b0 100644
--- a/distribution/deb/pom.xml
+++ b/distribution/deb/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>elasticsearch-distribution</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.deb</groupId>
diff --git a/distribution/fully-loaded/pom.xml b/distribution/fully-loaded/pom.xml
index 91988f1..0215999 100644
--- a/distribution/fully-loaded/pom.xml
+++ b/distribution/fully-loaded/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>elasticsearch-distribution</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.fully-loaded</groupId>
diff --git a/distribution/pom.xml b/distribution/pom.xml
index 4bb46e2..ca5b6e1 100644
--- a/distribution/pom.xml
+++ b/distribution/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch</groupId>
         <artifactId>elasticsearch-parent</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution</groupId>
diff --git a/distribution/rpm/pom.xml b/distribution/rpm/pom.xml
index fa3d6dd..cd8f321 100644
--- a/distribution/rpm/pom.xml
+++ b/distribution/rpm/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>elasticsearch-distribution</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.rpm</groupId>
diff --git a/distribution/shaded/pom.xml b/distribution/shaded/pom.xml
index 7024504..a062474 100644
--- a/distribution/shaded/pom.xml
+++ b/distribution/shaded/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>elasticsearch-distribution</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.shaded</groupId>
diff --git a/distribution/src/main/resources/README.textile b/distribution/src/main/resources/README.textile
index 720f357..b2873e8 100644
--- a/distribution/src/main/resources/README.textile
+++ b/distribution/src/main/resources/README.textile
@@ -42,7 +42,7 @@ h3. Installation
 
 * "Download":https://www.elastic.co/downloads/elasticsearch and unzip the Elasticsearch official distribution.
 * Run @bin/elasticsearch@ on unix, or @bin\elasticsearch.bat@ on windows.
-* Run @curl -X GET http://localhost:9200/@.
+* Run @curl -X GET http://127.0.0.1:9200/@.
 * Start more servers ...
 
 h3. Indexing
@@ -50,16 +50,16 @@ h3. Indexing
 Let's try and index some twitter like information. First, let's create a twitter user, and add some tweets (the @twitter@ index will be created automatically):
 
 <pre>
-curl -XPUT 'http://localhost:9200/twitter/user/kimchy' -d '{ "name" : "Shay Banon" }'
+curl -XPUT 'http://127.0.0.1:9200/twitter/user/kimchy' -d '{ "name" : "Shay Banon" }'
 
-curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '
+curl -XPUT 'http://127.0.0.1:9200/twitter/tweet/1' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T13:12:00",
     "message": "Trying out Elasticsearch, so far so good?"
 }'
 
-curl -XPUT 'http://localhost:9200/twitter/tweet/2' -d '
+curl -XPUT 'http://127.0.0.1:9200/twitter/tweet/2' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T14:12:12",
@@ -70,9 +70,9 @@ curl -XPUT 'http://localhost:9200/twitter/tweet/2' -d '
 Now, let's see if the information was added by GETting it:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/user/kimchy?pretty=true'
-curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty=true'
-curl -XGET 'http://localhost:9200/twitter/tweet/2?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/user/kimchy?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/1?pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/2?pretty=true'
 </pre>
 
 h3. Searching
@@ -81,13 +81,13 @@ Mmm search..., shouldn't it be elastic?
 Let's find all the tweets that @kimchy@ posted:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/tweet/_search?q=user:kimchy&pretty=true'
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/_search?q=user:kimchy&pretty=true'
 </pre>
 
 We can also use the JSON query language Elasticsearch provides instead of a query string:
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/tweet/_search?pretty=true' -d '
 {
     "query" : {
         "match" : { "user": "kimchy" }
@@ -98,7 +98,7 @@ curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
 Just for kicks, let's get all the documents stored (we should see the user as well):
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
@@ -109,7 +109,7 @@ curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
 We can also do range search (the @postDate@ was automatically identified as date)
 
 <pre>
-curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/twitter/_search?pretty=true' -d '
 {
     "query" : {
         "range" : {
@@ -130,16 +130,16 @@ Elasticsearch supports multiple indices, as well as multiple types per index. In
 Another way to define our simple twitter system is to have a different index per user (note, though that each index has an overhead). Here is the indexing curl's in this case:
 
 <pre>
-curl -XPUT 'http://localhost:9200/kimchy/info/1' -d '{ "name" : "Shay Banon" }'
+curl -XPUT 'http://127.0.0.1:9200/kimchy/info/1' -d '{ "name" : "Shay Banon" }'
 
-curl -XPUT 'http://localhost:9200/kimchy/tweet/1' -d '
+curl -XPUT 'http://127.0.0.1:9200/kimchy/tweet/1' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T13:12:00",
     "message": "Trying out Elasticsearch, so far so good?"
 }'
 
-curl -XPUT 'http://localhost:9200/kimchy/tweet/2' -d '
+curl -XPUT 'http://127.0.0.1:9200/kimchy/tweet/2' -d '
 {
     "user": "kimchy",
     "postDate": "2009-11-15T14:12:12",
@@ -152,7 +152,7 @@ The above will index information into the @kimchy@ index, with two types, @info@
 Complete control on the index level is allowed. As an example, in the above case, we would want to change from the default 5 shards with 1 replica per index, to only 1 shard with 1 replica per index (== per twitter user). Here is how this can be done (the configuration can be in yaml as well):
 
 <pre>
-curl -XPUT http://localhost:9200/another_user/ -d '
+curl -XPUT http://127.0.0.1:9200/another_user/ -d '
 {
     "index" : {
         "numberOfShards" : 1,
@@ -165,7 +165,7 @@ Search (and similar operations) are multi index aware. This means that we can ea
 index (twitter user), for example:
 
 <pre>
-curl -XGET 'http://localhost:9200/kimchy,another_user/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/kimchy,another_user/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
@@ -176,7 +176,7 @@ curl -XGET 'http://localhost:9200/kimchy,another_user/_search?pretty=true' -d '
 Or on all the indices:
 
 <pre>
-curl -XGET 'http://localhost:9200/_search?pretty=true' -d '
+curl -XGET 'http://127.0.0.1:9200/_search?pretty=true' -d '
 {
     "query" : {
         "matchAll" : {}
diff --git a/distribution/tar/pom.xml b/distribution/tar/pom.xml
index d54e48c..d84450b 100644
--- a/distribution/tar/pom.xml
+++ b/distribution/tar/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>elasticsearch-distribution</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.tar</groupId>
diff --git a/distribution/zip/pom.xml b/distribution/zip/pom.xml
index ab8f9af..6d16b04 100644
--- a/distribution/zip/pom.xml
+++ b/distribution/zip/pom.xml
@@ -6,7 +6,7 @@
     <parent>
         <groupId>org.elasticsearch.distribution</groupId>
         <artifactId>elasticsearch-distribution</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <groupId>org.elasticsearch.distribution.zip</groupId>
diff --git a/docs/community-clients/index.asciidoc b/docs/community-clients/index.asciidoc
new file mode 100644
index 0000000..56d658d
--- /dev/null
+++ b/docs/community-clients/index.asciidoc
@@ -0,0 +1,234 @@
+= Community Contributed Clients
+
+:client: https://www.elastic.co/guide/en/elasticsearch/client
+
+Besides the link:/guide[officially supported Elasticsearch clients], there are
+a number of clients that have been contributed by the community for various languages:
+
+* <<clojure>>
+* <<cold-fusion>>
+* <<erlang>>
+* <<go>>
+* <<groovy>>
+* <<haskell>>
+* <<java>>
+* <<javascript>>
+* <<dotnet>>
+* <<ocaml>>
+* <<perl>>
+* <<php>>
+* <<python>>
+* <<r>>
+* <<ruby>>
+* <<scala>>
+* <<smalltalk>>
+* <<vertx>>
+
+
+[[clojure]]
+== Clojure
+
+* http://github.com/clojurewerkz/elastisch[Elastisch]:
+  Clojure client.
+
+[[cold-fusion]]
+== Cold Fusion
+
+The following project appears to be abandoned:
+
+* https://github.com/jasonfill/ColdFusion-ElasticSearch-Client[ColdFusion-Elasticsearch-Client]
+  Cold Fusion client for Elasticsearch
+
+[[erlang]]
+== Erlang
+
+* http://github.com/tsloughter/erlastic_search[erlastic_search]:
+  Erlang client using HTTP.
+
+* https://github.com/dieswaytoofast/erlasticsearch[erlasticsearch]:
+  Erlang client using Thrift.
+
+* https://github.com/datahogs/tirexs[Tirexs]:
+  An https://github.com/elixir-lang/elixir[Elixir] based API/DSL, inspired by
+  http://github.com/karmi/tire[Tire]. Ready to use in pure Erlang
+  environment.
+
+
+[[go]]
+== Go
+
+* https://github.com/mattbaird/elastigo[elastigo]:
+  Go client.
+
+* https://github.com/belogik/goes[goes]:
+  Go lib.
+
+* https://github.com/olivere/elastic[elastic]:
+  Elasticsearch client for Google Go.
+
+
+[[groovy]]
+== Groovy
+
+See the {client}/groovy-api/current/index.html[official Elasticsearch Groovy client].
+
+[[haskell]]
+== Haskell
+* https://github.com/bitemyapp/bloodhound[bloodhound]:
+  Haskell client and DSL.
+
+
+[[java]]
+== Java
+
+Also see the {client}/java-api/current/index.html[official Elasticsearch Java client].
+
+* https://github.com/searchbox-io/Jest[Jest]:
+  Java Rest client.
+
+[[javascript]]
+== JavaScript
+
+Also see the {client}/javascript-api/current/index.html[official Elasticsearch JavaScript client].
+
+* https://github.com/fullscale/elastic.js[Elastic.js]:
+  A JavaScript implementation of the Elasticsearch Query DSL and Core API.
+
+* https://github.com/printercu/elastics[elastics]: Simple tiny client that just works
+
+* https://github.com/roundscope/ember-data-elasticsearch-kit[ember-data-elasticsearch-kit]:
+  An ember-data kit for both pushing and querying objects to Elasticsearch cluster
+
+The following project appears to be abandoned:
+
+* https://github.com/ramv/node-elastical[node-elastical]:
+  Node.js client for the Elasticsearch REST API
+
+
+[[dotnet]]
+== .NET
+
+Also see the {client}/net-api/current/index.html[official Elasticsearch .NET client].
+
+* https://github.com/Yegoroff/PlainElastic.Net[PlainElastic.Net]:
+  .NET client.
+
+[[ocaml]]
+== OCaml
+
+The following project appears to be abandoned:
+
+* https://github.com/tovbinm/ocaml-elasticsearch[ocaml-elasticsearch]:
+  OCaml client for Elasticsearch
+
+[[perl]]
+== Perl
+
+Also see the {client}/perl-api/current/index.html[official Elasticsearch Perl client].
+
+* https://metacpan.org/pod/Elastijk[Elastijk]: A low level minimal HTTP client.
+
+
+[[php]]
+== PHP
+
+Also see the {client}/php-api/current/index.html[official Elasticsearch PHP client].
+
+* http://github.com/ruflin/Elastica[Elastica]:
+  PHP client.
+
+* http://github.com/nervetattoo/elasticsearch[elasticsearch] PHP client.
+
+[[python]]
+== Python
+
+Also see the {client}/python-api/current/index.html[official Elasticsearch Python client].
+
+* http://github.com/elasticsearch/elasticsearch-dsl-py[elasticsearch-dsl-py]
+  chainable query and filter construction built on top of official client.
+
+* http://github.com/rhec/pyelasticsearch[pyelasticsearch]:
+  Python client.
+
+* https://github.com/eriky/ESClient[ESClient]:
+  A lightweight and easy to use Python client for Elasticsearch.
+
+* https://github.com/mozilla/elasticutils/[elasticutils]:
+  A friendly chainable Elasticsearch interface for Python.
+
+* http://github.com/aparo/pyes[pyes]:
+  Python client.
+
+The following projects appear to be abandoned:
+
+* https://github.com/humangeo/rawes[rawes]:
+  Python low level client.
+
+* http://intridea.github.io/surfiki-refine-elasticsearch/[Surfiki Refine]:
+  Python Map-Reduce engine targeting Elasticsearch indices.
+
+[[r]]
+== R
+* https://github.com/Tomesch/elasticsearch[elasticsearch]
+  R client for Elasticsearch
+
+* https://github.com/ropensci/elastic[elastic]:
+  A general purpose R client for Elasticsearch
+
+[[ruby]]
+== Ruby
+
+Also see the {client}/ruby-api/current/index.html[official Elasticsearch Ruby client].
+
+* https://github.com/PoseBiz/stretcher[stretcher]:
+  Ruby client.
+
+* https://github.com/printercu/elastics-rb[elastics]:
+  Tiny client with built-in zero-downtime migrations and ActiveRecord integration.
+
+* https://github.com/toptal/chewy[chewy]:
+  Chewy is ODM and wrapper for official elasticsearch client
+
+* https://github.com/ankane/searchkick[Searchkick]:
+  Intelligent search made easy
+
+The following projects appear to be abandoned:
+
+* https://github.com/wireframe/elastic_searchable/[elastic_searchable]:
+  Ruby client + Rails integration.
+
+* https://github.com/ddnexus/flex[Flex]:
+  Ruby Client.
+
+
+
+[[scala]]
+== Scala
+
+* https://github.com/sksamuel/elastic4s[elastic4s]:
+  Scala DSL.
+
+* https://github.com/scalastuff/esclient[esclient]:
+  Thin Scala client.
+
+* https://github.com/gphat/wabisabi[wabisabi]:
+  Asynchronous REST API Scala client.
+
+The following project appears to be abandoned:
+
+* https://github.com/bsadeh/scalastic[scalastic]:
+  Scala client.
+
+
+[[smalltalk]]
+== Smalltalk
+
+* http://ss3.gemstone.com/ss/Elasticsearch.html[Elasticsearch] -
+  Smalltalk client for Elasticsearch
+
+
+[[vertx]]
+== Vert.x
+
+* https://github.com/goodow/realtime-search[realtime-search]:
+  Elasticsearch module for Vert.x
diff --git a/docs/community/clients.asciidoc b/docs/community/clients.asciidoc
deleted file mode 100644
index c99645a..0000000
--- a/docs/community/clients.asciidoc
+++ /dev/null
@@ -1,223 +0,0 @@
-[[clients]]
-== Clients
-
-
-[[community-perl]]
-=== Perl
-
-See the {client}/perl-api/current/index.html[official Elasticsearch Perl client].
-
-[[community-python]]
-=== Python
-
-See the {client}/python-api/current/index.html[official Elasticsearch Python client].
-
-* http://github.com/elasticsearch/elasticsearch-dsl-py[elasticsearch-dsl-py]
-  chainable query and filter construction built on top of official client.
-
-* http://github.com/rhec/pyelasticsearch[pyelasticsearch]:
-  Python client.
-
-* https://github.com/eriky/ESClient[ESClient]:
-  A lightweight and easy to use Python client for Elasticsearch.
-
-* https://github.com/humangeo/rawes[rawes]:
-  Python low level client.
-
-* https://github.com/mozilla/elasticutils/[elasticutils]:
-  A friendly chainable Elasticsearch interface for Python.
-
-* http://intridea.github.io/surfiki-refine-elasticsearch/[Surfiki Refine]:
-  Python Map-Reduce engine targeting Elasticsearch indices.
-
-* http://github.com/aparo/pyes[pyes]:
-  Python client.
-
-
-[[community-ruby]]
-=== Ruby
-
-See the {client}/ruby-api/current/index.html[official Elasticsearch Ruby client].
-
-* http://github.com/karmi/retire[Retire]:
-  Ruby API & DSL, with ActiveRecord/ActiveModel integration (retired since Sep 2013).
-
-* https://github.com/PoseBiz/stretcher[stretcher]:
-  Ruby client.
-
-* https://github.com/wireframe/elastic_searchable/[elastic_searchable]:
-  Ruby client + Rails integration.
-
-* https://github.com/ddnexus/flex[Flex]:
-  Ruby Client.
-
-* https://github.com/printercu/elastics-rb[elastics]:
-  Tiny client with built-in zero-downtime migrations and ActiveRecord integration.
-
-* https://github.com/toptal/chewy[chewy]:
-  Chewy is ODM and wrapper for official elasticsearch client
-
-* https://github.com/ankane/searchkick[Searchkick]:
-  Intelligent search made easy
-
-
-[[community-php]]
-=== PHP
-
-See the {client}/php-api/current/index.html[official Elasticsearch PHP client].
-
-* http://github.com/ruflin/Elastica[Elastica]:
-  PHP client.
-
-* http://github.com/nervetattoo/elasticsearch[elasticsearch] PHP client.
-
-* http://github.com/polyfractal/Sherlock[Sherlock]:
-  PHP client, one-to-one mapping with query DSL, fluid interface.
-
-* https://github.com/nervetattoo/elasticsearch[elasticsearch]
-  PHP 5.3 client
-
-[[community-java]]
-=== Java
-
-* https://github.com/searchbox-io/Jest[Jest]:
-  Java Rest client.
-* There is of course the {client}/java-api/current/index.html[native ES Java client]
-
-[[community-javascript]]
-=== JavaScript
-
-See the {client}/javascript-api/current/index.html[official Elasticsearch JavaScript client].
-
-* https://github.com/fullscale/elastic.js[Elastic.js]:
-  A JavaScript implementation of the Elasticsearch Query DSL and Core API.
-
-* https://github.com/phillro/node-elasticsearch-client[node-elasticsearch-client]:
-  A NodeJS client for Elasticsearch.
-
-* https://github.com/ramv/node-elastical[node-elastical]:
-  Node.js client for the Elasticsearch REST API
-
-* https://github.com/printercu/elastics[elastics]: Simple tiny client that just works
-
-[[community-groovy]]
-=== Groovy
-
-See the {client}/groovy-api/current/index.html[official Elasticsearch Groovy client]
-
-[[community-dotnet]]
-=== .NET
-
-See the {client}/net-api/current/index.html[official Elasticsearch .NET client].
-
-* https://github.com/Yegoroff/PlainElastic.Net[PlainElastic.Net]:
-  .NET client.
-
-* https://github.com/medcl/ElasticSearch.Net[ElasticSearch.NET]:
-  .NET client.
-
-
-[[community-haskell]]
-=== Haskell
-* https://github.com/bitemyapp/bloodhound[bloodhound]:
-  Haskell client and DSL.
-
-
-[[community-scala]]
-=== Scala
-
-* https://github.com/sksamuel/elastic4s[elastic4s]:
-  Scala DSL.
-
-* https://github.com/scalastuff/esclient[esclient]:
-  Thin Scala client.
-
-* https://github.com/bsadeh/scalastic[scalastic]:
-  Scala client.
-
-* https://github.com/gphat/wabisabi[wabisabi]:
-  Asynchronous REST API Scala client.
-
-
-[[community-clojure]]
-=== Clojure
-
-* http://github.com/clojurewerkz/elastisch[Elastisch]:
-  Clojure client.
-
-
-[[community-go]]
-=== Go
-
-* https://github.com/mattbaird/elastigo[elastigo]:
-  Go client.
-
-* https://github.com/belogik/goes[goes]:
-  Go lib.
-
-* https://github.com/olivere/elastic[elastic]:
-  Elasticsearch client for Google Go.
-
-[[community-erlang]]
-=== Erlang
-
-* http://github.com/tsloughter/erlastic_search[erlastic_search]:
-  Erlang client using HTTP.
-
-* https://github.com/dieswaytoofast/erlasticsearch[erlasticsearch]:
-  Erlang client using Thrift.
-
-* https://github.com/datahogs/tirexs[Tirexs]:
-  An https://github.com/elixir-lang/elixir[Elixir] based API/DSL, inspired by
-  http://github.com/karmi/tire[Tire]. Ready to use in pure Erlang
-  environment.
-
-
-[[community-eventmachine]]
-=== EventMachine
-
-* http://github.com/vangberg/em-elasticsearch[em-elasticsearch]:
-  elasticsearch library for eventmachine.
-
-
-[[community-command-line]]
-=== Command Line
-
-* https://github.com/elasticsearch/es2unix[es2unix]:
-  Elasticsearch API consumable by the Linux command line.
-
-* https://github.com/javanna/elasticshell[elasticshell]:
-  command line shell for elasticsearch.
-
-
-[[community-ocaml]]
-=== OCaml
-
-* https://github.com/tovbinm/ocaml-elasticsearch[ocaml-elasticsearch]:
-  OCaml client for Elasticsearch
-
-
-[[community-smalltalk]]
-=== Smalltalk
-
-* http://ss3.gemstone.com/ss/Elasticsearch.html[Elasticsearch] -
-  Smalltalk client for Elasticsearch
-
-[[community-cold-fusion]]
-=== Cold Fusion
-
-* https://github.com/jasonfill/ColdFusion-ElasticSearch-Client[ColdFusion-Elasticsearch-Client]
-  Cold Fusion client for Elasticsearch
-
-[[community-nodejs]]
-=== NodeJS
-* https://github.com/phillro/node-elasticsearch-client[Node-Elasticsearch-Client]
-  A node.js client for elasticsearch
-
-[[community-r]]
-=== R
-* https://github.com/Tomesch/elasticsearch[elasticsearch]
-  R client for Elasticsearch
-
-* https://github.com/ropensci/elastic[elastic]: 
-  A general purpose R client for Elasticsearch
diff --git a/docs/community/frontends.asciidoc b/docs/community/frontends.asciidoc
deleted file mode 100644
index e58fb26..0000000
--- a/docs/community/frontends.asciidoc
+++ /dev/null
@@ -1,20 +0,0 @@
-[[front-ends]]
-== Front Ends
-
-* https://github.com/mobz/elasticsearch-head[elasticsearch-head]: 
-  A web front end for an Elasticsearch cluster.
-
-* https://github.com/OlegKunitsyn/elasticsearch-browser[browser]: 
-  Web front-end over elasticsearch data.
-
-* https://github.com/polyfractal/elasticsearch-inquisitor[Inquisitor]:
-  Front-end to help debug/diagnose queries and analyzers
-
-* http://elastichammer.exploringelasticsearch.com/[Hammer]: 
-  Web front-end for elasticsearch
-
-* https://github.com/romansanchez/Calaca[Calaca]: 
-  Simple search client for Elasticsearch
-
-* https://github.com/rdpatil4/ESClient[ESClient]: 
-  Simple search, update, delete client for Elasticsearch
diff --git a/docs/community/github.asciidoc b/docs/community/github.asciidoc
deleted file mode 100644
index 698e4c2..0000000
--- a/docs/community/github.asciidoc
+++ /dev/null
@@ -1,6 +0,0 @@
-[[github]]
-== GitHub
-
-GitHub is a place where a lot of development is done around
-*elasticsearch*, here is a simple search for
-https://github.com/search?q=elasticsearch&type=Repositories[repositories].
diff --git a/docs/community/index.asciidoc b/docs/community/index.asciidoc
deleted file mode 100644
index 48b2f2a..0000000
--- a/docs/community/index.asciidoc
+++ /dev/null
@@ -1,17 +0,0 @@
-= Community Supported Clients
-
-:client: http://www.elastic.co/guide/en/elasticsearch/client
-
-
-include::clients.asciidoc[]
-
-include::frontends.asciidoc[]
-
-include::integrations.asciidoc[]
-
-include::misc.asciidoc[]
-
-include::monitoring.asciidoc[]
-
-include::github.asciidoc[]
-
diff --git a/docs/community/integrations.asciidoc b/docs/community/integrations.asciidoc
deleted file mode 100644
index 7673752..0000000
--- a/docs/community/integrations.asciidoc
+++ /dev/null
@@ -1,102 +0,0 @@
-[[integrations]]
-== Integrations
-
-
-* http://grails.org/plugin/elasticsearch[Grails]:
-  Elasticsearch Grails plugin.
-
-* https://github.com/carrot2/elasticsearch-carrot2[carrot2]:
-  Results clustering with carrot2
-
-* https://github.com/angelf/escargot[escargot]:
-  Elasticsearch connector for Rails (WIP).
-
-* https://metacpan.org/module/Catalyst::Model::Search::Elasticsearch[Catalyst]:
-  Elasticsearch and Catalyst integration.
-
-* http://github.com/aparo/django-elasticsearch[django-elasticsearch]:
-  Django Elasticsearch Backend.
-
-* http://github.com/Aconex/elasticflume[elasticflume]:
-  http://github.com/cloudera/flume[Flume] sink implementation.
-
-* http://code.google.com/p/terrastore/wiki/Search_Integration[Terrastore Search]:
-  http://code.google.com/p/terrastore/[Terrastore] integration module with elasticsearch.
-
-* https://github.com/infochimps-labs/wonderdog[Wonderdog]:
-  Hadoop bulk loader into elasticsearch.
-
-* http://geeks.aretotally.in/play-framework-module-elastic-search-distributed-searching-with-json-http-rest-or-java[Play!Framework]:
-  Integrate with Play! Framework Application.
-
-* https://github.com/Exercise/FOQElasticaBundle[ElasticaBundle]:
-  Symfony2 Bundle wrapping Elastica.
-
-* https://drupal.org/project/elasticsearch_connector[Drupal]:
-  Drupal Elasticsearch integration (1.0.0 and later).
-
-* http://drupal.org/project/search_api_elasticsearch[Drupal]:
-  Drupal Elasticsearch integration via Search API (1.0.0 and earlier).
-
-* https://github.com/refuge/couch_es[couch_es]:
-  elasticsearch helper for couchdb based products (apache couchdb, bigcouch & refuge)
-
-* https://github.com/sonian/elasticsearch-jetty[Jetty]:
-  Jetty HTTP Transport
-
-* https://github.com/dadoonet/spring-elasticsearch[Spring Elasticsearch]:
-  Spring Factory for Elasticsearch
-
-* https://github.com/spring-projects/spring-data-elasticsearch[Spring Data Elasticsearch]:
-  Spring Data implementation for Elasticsearch
-
-* https://camel.apache.org/elasticsearch.html[Apache Camel Integration]:
-  An Apache camel component to integrate elasticsearch
-
-* https://github.com/tlrx/elasticsearch-test[elasticsearch-test]:
-  Elasticsearch Java annotations for unit testing with
-  http://www.junit.org/[JUnit]
-
-* http://searchbox-io.github.com/wp-elasticsearch/[Wp-Elasticsearch]:
-  Elasticsearch WordPress Plugin
-
-* https://github.com/wallmanderco/elasticsearch-indexer[Elasticsearch Indexer]:
-  Elasticsearch WordPress Plugin
-
-* https://github.com/OlegKunitsyn/eslogd[eslogd]:
-  Linux daemon that replicates events to a central Elasticsearch server in real-time
-
-* https://github.com/drewr/elasticsearch-clojure-repl[elasticsearch-clojure-repl]:
-  Plugin that embeds nREPL for run-time introspective adventure! Also
-  serves as an nREPL transport.
-
-* http://haystacksearch.org/[Haystack]:
-  Modular search for Django
-
-* https://github.com/cleverage/play2-elasticsearch[play2-elasticsearch]:
-  Elasticsearch module for Play Framework 2.x
-
-* https://github.com/goodow/realtime-search[realtime-search]:
-  Elasticsearch module for Vert.x
-
-* https://github.com/fullscale/dangle[dangle]:
-  A set of AngularJS directives that provide common visualizations for elasticsearch based on
-  D3.
-
-* https://github.com/roundscope/ember-data-elasticsearch-kit[ember-data-elasticsearch-kit]:
-  An ember-data kit for both pushing and querying objects to Elasticsearch cluster
-
-* https://github.com/kzwang/elasticsearch-osem[elasticsearch-osem]:
-  A Java Object Search Engine Mapping (OSEM) for Elasticsearch
-
-* https://github.com/twitter/storehaus[Twitter Storehaus]:
-  Thin asynchronous Scala client for Storehaus.
-
-* https://doc.tiki.org/Elasticsearch[Tiki Wiki CMS Groupware]:
-  Tiki has native support for Elasticsearch. This provides faster & better search (facets, etc), along with some Natural Language Processing features (ex.: More like this)
-
-* https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer[Kafka Standalone Consumer]:
-  Easily Scaleable & Extendable, Kafka Standalone Consumer that will read the messages from Kafka, processes and index them in ElasticSearch
-  
-* http://www.searchtechnologies.com/aspire-for-elasticsearch[Aspire for Elasticsearch]:
-  Aspire, from Search Technologies, is a powerful connector and processing framework designed for unstructured data. It has connectors to internal and external repositories including SharePoint, Documentum, Jive, RDB, file systems, websites and more, and can transform and normalize this data before indexing in Elasticsearch. 
diff --git a/docs/community/misc.asciidoc b/docs/community/misc.asciidoc
deleted file mode 100644
index 440c041..0000000
--- a/docs/community/misc.asciidoc
+++ /dev/null
@@ -1,31 +0,0 @@
-[[misc]]
-== Misc
-
-
-* https://github.com/elasticsearch/puppet-elasticsearch[Puppet]:
-  Elasticsearch puppet module.
-
-* http://github.com/elasticsearch/cookbook-elasticsearch[Chef]:
-  Chef cookbook for Elasticsearch
-
-* https://github.com/medcl/salt-elasticsearch[SaltStack]:
-  SaltStack Module for Elasticsearch
-
-* http://www.github.com/neogenix/daikon[daikon]:
-  Daikon Elasticsearch CLI
-
-* https://github.com/Aconex/scrutineer[Scrutineer]:
-  A high performance consistency checker to compare what you've indexed
-  with your source of truth content (e.g. DB)
-
-* https://www.wireshark.org/[Wireshark]:
-  Protocol dissection for Zen discovery, HTTP and the binary protocol
-
-* https://github.com/sscarduzio/elasticsearch-readonlyrest-plugin[Readonly REST]:
-  High performance access control for Elasticsearch native REST API.
-
-* https://github.com/kodcu/pes[Pes]:
-  A pluggable elastic query DSL builder for Elasticsearch
-  
-  * https://github.com/ozlerhakan/mongolastic[Mongolastic]:
-  A tool that clone data from ElasticSearch to MongoDB and vice versa
diff --git a/docs/community/monitoring.asciidoc b/docs/community/monitoring.asciidoc
deleted file mode 100644
index 7c3768b..0000000
--- a/docs/community/monitoring.asciidoc
+++ /dev/null
@@ -1,40 +0,0 @@
-[[health]]
-== Health and Performance Monitoring
-
-* https://github.com/lukas-vlcek/bigdesk[bigdesk]:
-  Live charts and statistics for elasticsearch cluster.
-
-* https://github.com/lmenezes/elasticsearch-kopf/[Kopf]:
-  Live cluster health and shard allocation monitoring with administration toolset.
-  
-* https://github.com/karmi/elasticsearch-paramedic[paramedic]:
-  Live charts with cluster stats and indices/shards information.
-
-* http://www.elastichq.org/[ElasticsearchHQ]:
-  Free cluster health monitoring tool
-
-* http://sematext.com/spm/index.html[SPM for Elasticsearch]:
-  Performance monitoring with live charts showing cluster and node stats, integrated
-  alerts, email reports, etc.
-
-* https://github.com/radu-gheorghe/check-es[check-es]:
-  Nagios/Shinken plugins for checking on elasticsearch
-
-* https://github.com/anchor/nagios-plugin-elasticsearch[check_elasticsearch]:
-  An Elasticsearch availability and performance monitoring plugin for
-  Nagios.
-
-* https://github.com/rbramley/Opsview-elasticsearch[opsview-elasticsearch]:
-  Opsview plugin written in Perl for monitoring Elasticsearch
-
-* https://github.com/polyfractal/elasticsearch-segmentspy[SegmentSpy]:
-  Plugin to watch Lucene segment merges across your cluster
-
-* https://github.com/mattweber/es2graphite[es2graphite]:
-  Send cluster and indices stats and status to Graphite for monitoring and graphing.
-  
-* https://scoutapp.com[Scout]: Provides plugins for monitoring Elasticsearch https://scoutapp.com/plugin_urls/1331-elasticsearch-node-status[nodes], https://scoutapp.com/plugin_urls/1321-elasticsearch-cluster-status[clusters], and https://scoutapp.com/plugin_urls/1341-elasticsearch-index-status[indices].
-
-* https://itunes.apple.com/us/app/elasticocean/id955278030?ls=1&mt=8[ElasticOcean]:
-  Elasticsearch & DigitalOcean iOS Real-Time Monitoring tool to keep an eye on DigitalOcean Droplets or Elasticsearch instances or both of them on-a-go.
-  
diff --git a/docs/plugins/alerting.asciidoc b/docs/plugins/alerting.asciidoc
new file mode 100644
index 0000000..9472dbb
--- /dev/null
+++ b/docs/plugins/alerting.asciidoc
@@ -0,0 +1,18 @@
+[[alerting]]
+== Alerting Plugins
+
+Alerting plugins allow Elasticsearch to monitor indices and to trigger alerts when thresholds are breached.
+
+[float]
+=== Core alerting plugins
+
+The core alerting plugins are:
+
+link:/products/watcher[Watcher]::
+
+Watcher is the alerting and notification product for Elasticsearch that lets
+you take action based on changes in your data. It is designed around the
+principle that if you can query something in Elasticsearch, you can alert on
+it. Simply define a query, condition, schedule, and the actions to take, and
+Watcher will do the rest.
+
diff --git a/docs/plugins/analysis-icu.asciidoc b/docs/plugins/analysis-icu.asciidoc
new file mode 100644
index 0000000..dcd73fe
--- /dev/null
+++ b/docs/plugins/analysis-icu.asciidoc
@@ -0,0 +1,438 @@
+[[analysis-icu]]
+=== ICU Analysis Plugin
+
+The ICU Analysis plugin integrates the Lucene ICU module into elasticsearch,
+adding extended Unicode support using the http://site.icu-project.org/[ICU]
+libraries, including better analysis of Asian languages, Unicode
+normalization, Unicode-aware case folding, collation support, and
+transliteration.
+
+[[analysis-icu-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install analysis-icu
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[analysis-icu-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove analysis-icu
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[analysis-icu-normalization-charfilter]]
+==== ICU Normalization Character Filter
+
+Normalizes characters as explained
+http://userguide.icu-project.org/transforms/normalization[here].
+It registers itself as the `icu_normalizer` character filter, which is
+available to all indices without any further configuration. The type of
+normalization can be specified with the `name` parameter, which accepts `nfc`,
+`nfkc`, and `nfkc_cf` (default).  Set the `mode` parameter to `decompose` to
+convert `nfc` to `nfd` or `nfkc` to `nfkd` respectively:
+
+Here are two examples, the default usage and a customised character filter:
+
+
+[source,json]
+--------------------------------------------------
+PUT icu_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "nfkc_cf_normalized": { <1>
+            "tokenizer": "icu_tokenizer",
+            "char_filter": [
+              "icu_normalizer"
+            ]
+          },
+          "nfd_normalized": { <2>
+            "tokenizer": "icu_tokenizer",
+            "char_filter": [
+              "nfd_normalizer"
+            ]
+          }
+        },
+        "char_filter": {
+          "nfd_normalizer": {
+            "type": "icu_normalizer",
+            "name": "nfc",
+            "mode": "decompose"
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+<1> Uses the default `nfkc_cf` normalization.
+<2> Uses the customized `nfd_normalizer` token filter, which is set to use `nfc` normalization with decomposition.
+
+[[analysis-icu-tokenizer]]
+==== ICU Tokenizer
+
+Tokenizes text into words on word boundaries, as defined in
+http://www.unicode.org/reports/tr29/[UAX #29: Unicode Text Segmentation].
+It behaves much like the {ref}/analysis-standard-tokenizer.html[`standard` tokenizer],
+but adds better support for some Asian languages by using a dictionary-based
+approach to identify words in Thai, Lao, Chinese, Japanese, and Korean, and
+using custom rules to break Myanmar and Khmer text into syllables.
+
+[source,json]
+--------------------------------------------------
+PUT icu_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "my_icu_analyzer": {
+            "tokenizer": "icu_tokenizer"
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+
+[[analysis-icu-normalization]]
+==== ICU Normalization Token Filter
+
+Normalizes characters as explained
+http://userguide.icu-project.org/transforms/normalization[here]. It registers
+itself as the `icu_normalizer` token filter, which is available to all indices
+without any further configuration. The type of normalization can be specified
+with the `name` parameter, which accepts `nfc`, `nfkc`, and `nfkc_cf`
+(default).
+
+You should probably prefer the <<analysis-icu-normalization-charfilter,Normalization character filter>>.
+
+Here are two examples, the default usage and a customised token filter:
+
+[source,json]
+--------------------------------------------------
+PUT icu_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "nfkc_cf_normalized": { <1>
+            "tokenizer": "icu_tokenizer",
+            "filter": [
+              "icu_normalizer"
+            ]
+          },
+          "nfc_normalized": { <2>
+            "tokenizer": "icu_tokenizer",
+            "filter": [
+              "nfc_normalizer"
+            ]
+          }
+        },
+        "filter": {
+          "nfc_normalizer": {
+            "type": "icu_normalizer",
+            "name": "nfc"
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+<1> Uses the default `nfkc_cf` normalization.
+<2> Uses the customized `nfc_normalizer` token filter, which is set to use `nfc` normalization.
+
+
+[[analysis-icu-folding]]
+==== ICU Folding Token Filter
+
+Case folding of Unicode characters based on `UTR#30`, like the
+{ref}/analysis-asciifolding-tokenfilter.html[ASCII-folding token filter]
+on steroids. It registers itself as the `icu_folding` token filter and is
+available to all indices:
+
+[source,json]
+--------------------------------------------------
+PUT icu_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "folded": {
+            "tokenizer": "icu",
+            "filter": [
+              "icu_folding"
+            ]
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+The ICU folding token filter already does Unicode normalization, so there is
+no need to use Normalize character or token filter as well.
+
+Which letters are folded can be controlled by specifying the
+`unicodeSetFilter` parameter, which accepts a
+http://icu-project.org/apiref/icu4j/com/ibm/icu/text/UnicodeSet.html[UnicodeSet].
+
+The following example exempts Swedish characters from folding. It is important
+to note that both upper and lowercase forms should be specified, and that
+these filtered character are not lowercased which is why we add the
+`lowercase` filter as well:
+
+[source,json]
+--------------------------------------------------
+PUT icu_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "swedish_analyzer": {
+            "tokenizer": "icu_tokenizer",
+            "filter": [
+              "swedish_folding",
+              "lowercase"
+            ]
+          }
+        },
+        "filter": {
+          "swedish_folding": {
+            "type": "icu_folding",
+            "unicodeSetFilter": "[^åäöÅÄÖ]"
+          }
+        }
+      }
+    }
+  }
+}
+--------------------------------------------------
+// AUTOSENSE
+
+[[analysis-icu-collation]]
+==== ICU Collation Token Filter
+
+Collations are used for sorting documents in a language-specific word order.
+The `icu_collation` token filter is available to all indices and defaults to
+using the
+https://www.elastic.co/guide/en/elasticsearch/guide/current/sorting-collations.html#uca[DUCET collation],
+which is a best-effort attempt at language-neutral sorting.
+
+Below is an example of how to set up a field for sorting German names in
+``phonebook'' order:
+
+[source,json]
+--------------------------------------------------
+PUT /my_index
+{
+  "settings": {
+    "analysis": {
+      "filter": {
+        "german_phonebook": {
+          "type":     "icu_collation",
+          "language": "de",
+          "country":  "DE",
+          "variant":  "@collation=phonebook"
+        }
+      },
+      "analyzer": {
+        "german_phonebook": {
+          "tokenizer": "keyword",
+          "filter":  [ "german_phonebook" ]
+        }
+      }
+    }
+  },
+  "mappings": {
+    "user": {
+      "properties": {
+        "name": { <1>
+          "type": "string",
+          "fields": {
+            "sort": { <2>
+              "type":     "string",
+              "analyzer": "german_phonebook"
+            }
+          }
+        }
+      }
+    }
+  }
+}
+
+GET _search <3>
+{
+  "query": {
+    "match": {
+      "name": "Fritz"
+    }
+  },
+  "sort": "name.sort"
+}
+
+--------------------------------------------------
+// AUTOSENSE
+
+<1> The `name` field uses the `standard` analyzer, and so support full text queries.
+<2> The `name.sort` field uses the `keyword` analyzer to preserve the name as
+    a single token, and applies the `german_phonebook` token filter to index
+    the value in German phonebook sort order.
+<3> An example query which searches the `name` field and sorts on the `name.sort` field.
+
+===== Collation options
+
+`strength`::
+
+The strength property determines the minimum level of difference considered
+significant during comparison. Possible values are : `primary`, `secondary`,
+`tertiary`, `quaternary` or `identical`. See the
+http://icu-project.org/apiref/icu4j/com/ibm/icu/text/Collator.html[ICU Collation documentation]
+for a more detailed  explanation for each value.  Defaults to `tertiary`
+unless otherwise specified in the collation.
+
+`decomposition`::
+
+Possible values: `no` (default, but collation-dependent) or `canonical`.
+Setting this decomposition property to `canonical` allows the Collator to
+handle unnormalized text properly, producing the same results as if the text
+were normalized. If `no` is set, it is the user's responsibility to insure
+that all text is already in the appropriate form before a comparison or before
+getting a CollationKey. Adjusting decomposition mode allows the user to select
+between faster and more complete collation behavior. Since a great many of the
+world's languages do not require text normalization, most locales set `no` as
+the default decomposition mode.
+
+The following options are expert only:
+
+`alternate`::
+
+Possible values: `shifted` or `non-ignorable`. Sets the alternate handling for
+strength `quaternary` to be either shifted or non-ignorable. Which boils down
+to ignoring punctuation and whitespace.
+
+`caseLevel`::
+
+Possible values: `true` or `false` (default). Whether case level sorting is
+required. When strength is set to `primary` this will ignore accent
+differences.
+
+
+`caseFirst`::
+
+Possible values: `lower` or `upper`. Useful to control which case is sorted
+first when case is not ignored for strength `tertiary`. The default depends on
+the collation.
+
+`numeric`::
+
+Possible values: `true` or `false` (default) . Whether digits are sorted
+according to their numeric representation. For example the value `egg-9` is
+sorted before the value `egg-21`.
+
+
+`variableTop`::
+
+Single character or contraction. Controls what is variable for `alternate`.
+
+`hiraganaQuaternaryMode`::
+
+Possible values: `true` or `false`.  Distinguishing between Katakana and
+Hiragana characters in `quaternary` strength.
+
+
+[[analysis-icu-transform]]
+==== ICU Transform Token Filter
+
+Transforms are used to process Unicode text in many different ways, such as
+case mapping, normalization, transliteration and bidirectional text handling.
+
+You can define which transformation you want to apply with the `id` parameter
+(defaults to `Null`), and specify text direction with the `dir` parameter
+which accepts `forward` (default) for LTR and `reverse` for RTL.  Custom
+rulesets are not yet supported.
+
+For example:
+
+[source,json]
+--------------------------------------------------
+PUT icu_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "latin": {
+            "tokenizer": "keyword",
+            "filter": [
+              "myLatinTransform"
+            ]
+          }
+        },
+        "filter": {
+          "myLatinTransform": {
+            "type": "icu_transform",
+            "id": "Any-Latin; NFD; [:Nonspacing Mark:] Remove; NFC" <1>
+          }
+        }
+      }
+    }
+  }
+}
+
+GET icu_sample/_analyze?analyzer=latin
+{
+  "text": "你好" <2>
+}
+
+GET icu_sample/_analyze?analyzer=latin
+{
+  "text": "здравствуйте" <3>
+}
+
+GET icu_sample/_analyze?analyzer=latin
+{
+  "text": "こんにちは" <4>
+}
+
+--------------------------------------------------
+// AUTOSENSE
+
+<1> This transforms transliterates characters to Latin, and separates accents
+    from their base characters, removes the accents, and then puts the
+    remaining text into an unaccented form.
+
+<2> Returns `ni hao`.
+<3> Returns `zdravstvujte`.
+<4> Returns `kon'nichiha`.
+
+For more documentation, Please see the http://userguide.icu-project.org/transforms/general[user guide of ICU Transform].
diff --git a/docs/plugins/analysis-kuromoji.asciidoc b/docs/plugins/analysis-kuromoji.asciidoc
new file mode 100644
index 0000000..3575457
--- /dev/null
+++ b/docs/plugins/analysis-kuromoji.asciidoc
@@ -0,0 +1,454 @@
+[[analysis-kuromoji]]
+=== Japanese (kuromoji) Analysis Plugin
+
+The Japanese (kuromoji) Analysis plugin integrates Lucene kuromoji analysis
+module into elasticsearch.
+
+[[analysis-kuromoji-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install analysis-kuromoji
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[analysis-kuromoji-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove analysis-kuromoji
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[analysis-kuromoji-analyzer]]
+==== `kuromoji` analyzer
+
+The `kuromoji` analyzer consists of the following tokenizer and token filters:
+
+* <<analysis-kuromoji-tokenizer,`kuromoji_tokenizer`>>
+* <<analysis-kuromoji-baseform,`kuromoji_baseform`>> token filter
+* <<analysis-kuromoji-speech,`kuromoji_part_of_speech`>> token filter
+* {ref}/analysis-cjk-width-tokenfilter.html[`cjk_width`] token filter
+* <<analysis-kuromoji-stop,`ja_stop`>> token filter
+* <<analysis-kuromoji-stemmer,`kuromoji_stemmer`>> token filter
+* {ref}/analysis-lowercase-tokenfilter.html[`lowercase`] token filter
+
+It supports the `mode` and `user_dictionary` settings from
+<<analysis-kuromoji-tokenizer,`kuromoji_tokenizer`>>.
+
+[[analysis-kuromoji-charfilter]]
+==== `kuromoji_iteration_mark` character filter
+
+The `kuromoji_iteration_mark` normalizes Japanese horizontal iteration marks
+(_odoriji_) to their expanded form. It accepts the following settings:
+
+`normalize_kanji`::
+
+    Indicates whether kanji iteration marks should be normalize. Defaults to `true`.
+
+`normalize_kana`::
+
+    Indicates whether kana iteration marks should be normalized. Defaults to `true`
+
+
+[[analysis-kuromoji-tokenizer]]
+==== `kuromoji_tokenizer`
+
+The `kuromoji_tokenizer` accepts the following settings:
+
+`mode`::
++
+--
+
+The tokenization mode determines how the tokenizer handles compound and
+unknown words.  It can be set to:
+
+`normal`::
+
+    Normal segmentation, no decomposition for compounds. Example output:
+
+    関西国際空港
+    アブラカダブラ
+
+`search`::
+
+    Segmentation geared towards search. This includes a decompounding process
+    for long nouns, also including the full compound token as a synonym.
+    Example output:
+
+    関西, 関西国際空港, 国際, 空港
+    アブラカダブラ
+
+`extended`::
+
+    Extended mode outputs unigrams for unknown words. Example output:
+
+    関西, 国際, 空港
+    ア, ブ, ラ, カ, ダ, ブ, ラ
+--
+
+`discard_punctuation`::
+
+    Whether punctuation should be discarded from the output. Defaults to `true`.
+
+`user_dictionary`::
++
+--
+The Kuromoji tokenizer uses the MeCab-IPADIC dictionary by default. A `user_dictionary`
+may be appended to the default dictionary. The dictionary should have the following CSV format:
+
+[source,csv]
+-----------------------
+<text>,<token 1> ... <token n>,<reading 1> ... <reading n>,<part-of-speech tag>
+-----------------------
+--
+
+As a demonstration of how the user dictionary can be used, save the following
+dictionary to `$ES_HOME/config/userdict_ja.txt`:
+
+[source,csv]
+-----------------------
+東京スカイツリー,東京 スカイツリー,トウキョウ スカイツリー,カスタム名詞
+-----------------------
+
+Then create an analyzer as follows:
+
+[source,json]
+--------------------------------------------------
+PUT kuromoji_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "tokenizer": {
+          "kuromoji_user_dict": {
+            "type": "kuromoji_tokenizer",
+            "mode": "extended",
+            "discard_punctuation": "false",
+            "user_dictionary": "userdict_ja.txt"
+          }
+        },
+        "analyzer": {
+          "my_analyzer": {
+            "type": "custom",
+            "tokenizer": "kuromoji_user_dict"
+          }
+        }
+      }
+    }
+  }
+}
+
+POST kuromoji_sample/_analyze?analyzer=my_analyzer&text=東京スカイツリー
+--------------------------------------------------
+// AUTOSENSE
+
+The above `analyze` request returns the following:
+
+[source,json]
+--------------------------------------------------
+# Result
+{
+  "tokens" : [ {
+    "token" : "東京",
+    "start_offset" : 0,
+    "end_offset" : 2,
+    "type" : "word",
+    "position" : 1
+  }, {
+    "token" : "スカイツリー",
+    "start_offset" : 2,
+    "end_offset" : 8,
+    "type" : "word",
+    "position" : 2
+  } ]
+}
+--------------------------------------------------
+
+[[analysis-kuromoji-baseform]]
+==== `kuromoji_baseform` token filter
+
+The `kuromoji_baseform` token filter replaces terms with their
+BaseFormAttribute. This acts as a lemmatizer for verbs and adjectives.
+
+[source,json]
+--------------------------------------------------
+PUT kuromoji_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "my_analyzer": {
+            "tokenizer": "kuromoji_tokenizer",
+            "filter": [
+              "kuromoji_baseform"
+            ]
+          }
+        }
+      }
+    }
+  }
+}
+
+POST kuromoji_sample/_analyze?analyzer=my_analyzer&text=飲み
+--------------------------------------------------
+// AUTOSENSE
+
+[source,text]
+--------------------------------------------------
+# Result
+{
+  "tokens" : [ {
+    "token" : "飲む",
+    "start_offset" : 0,
+    "end_offset" : 2,
+    "type" : "word",
+    "position" : 1
+  } ]
+}
+--------------------------------------------------
+
+[[analysis-kuromoji-speech]]
+==== `kuromoji_part_of_speech` token filter
+
+The `kuromoji_part_of_speech` token filter removes tokens that match a set of
+part-of-speech tags. It accepts the following setting:
+
+`stoptags`::
+
+    An array of part-of-speech tags that should be removed. It defaults to the
+    `stoptags.txt` file embedded in the `lucene-analyzer-kuromoji.jar`.
+
+[source,json]
+--------------------------------------------------
+PUT kuromoji_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "my_analyzer": {
+            "tokenizer": "kuromoji_tokenizer",
+            "filter": [
+              "my_posfilter"
+            ]
+          }
+        },
+        "filter": {
+          "my_posfilter": {
+            "type": "kuromoji_part_of_speech",
+            "stoptags": [
+              "助詞-格助詞-一般",
+              "助詞-終助詞"
+            ]
+          }
+        }
+      }
+    }
+  }
+}
+
+POST kuromoji_sample/_analyze?analyzer=my_analyzer&text=寿司がおいしいね
+
+--------------------------------------------------
+// AUTOSENSE
+
+[source,text]
+--------------------------------------------------
+# Result
+{
+  "tokens" : [ {
+    "token" : "寿司",
+    "start_offset" : 0,
+    "end_offset" : 2,
+    "type" : "word",
+    "position" : 1
+  }, {
+    "token" : "おいしい",
+    "start_offset" : 3,
+    "end_offset" : 7,
+    "type" : "word",
+    "position" : 3
+  } ]
+}
+--------------------------------------------------
+
+[[analysis-kuromoji-readingform]]
+==== `kuromoji_readingform` token filter
+
+The `kuromoji_readingform` token filter replaces the token with its reading
+form in either katakana or romaji. It accepts the following setting:
+
+`use_romaji`::
+
+    Whether romaji reading form should be output instead of katakana.  Defaults to `false`.
+
+When using the pre-defined `kuromoji_readingform` filter, `use_romaji` is set
+to `true`. The default when defining a custom `kuromoji_readingform`, however,
+is `false`.  The only reason to use the custom form is if you need the
+katakana reading form:
+
+[source,json]
+--------------------------------------------------
+PUT kuromoji_sample
+{
+    "settings": {
+        "index":{
+            "analysis":{
+                "analyzer" : {
+                    "romaji_analyzer" : {
+                        "tokenizer" : "kuromoji_tokenizer",
+                        "filter" : ["romaji_readingform"]
+                    },
+                    "katakana_analyzer" : {
+                        "tokenizer" : "kuromoji_tokenizer",
+                        "filter" : ["katakana_readingform"]
+                    }
+                },
+                "filter" : {
+                    "romaji_readingform" : {
+                        "type" : "kuromoji_readingform",
+                        "use_romaji" : true
+                    },
+                    "katakana_readingform" : {
+                        "type" : "kuromoji_readingform",
+                        "use_romaji" : false
+                    }
+                }
+            }
+        }
+    }
+}
+
+POST kuromoji_sample/_analyze?analyzer=katakana_analyzer&text=寿司 <1>
+
+POST kuromoji_sample/_analyze?analyzer=romaji_analyzer&text=寿司 <2>
+
+--------------------------------------------------
+// AUTOSENSE
+
+<1> Returns `スシ`.
+<2> Returns `sushi`.
+
+[[analysis-kuromoji-stemmer]]
+==== `kuromoji_stemmer` token filter
+
+The `kuromoji_stemmer` token filter normalizes common katakana spelling
+variations ending in a long sound character by removing this character
+(U+30FC). Only full-width katakana characters are supported.
+
+This token filter accepts the following setting:
+
+`minimum_length`::
+
+    Katakana words shorter than the `minimum length` are not stemmed (default
+    is `4`).
+
+
+[source,json]
+--------------------------------------------------
+PUT kuromoji_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "my_analyzer": {
+            "tokenizer": "kuromoji_tokenizer",
+            "filter": [
+              "my_katakana_stemmer"
+            ]
+          }
+        },
+        "filter": {
+          "my_katakana_stemmer": {
+            "type": "kuromoji_stemmer",
+            "minimum_length": 4
+          }
+        }
+      }
+    }
+  }
+}
+
+POST kuromoji_sample/_analyze?analyzer=my_analyzer&text=コピー <1>
+
+POST kuromoji_sample/_analyze?analyzer=my_analyzer&text=サーバー <2>
+
+--------------------------------------------------
+// AUTOSENSE
+
+<1> Returns `コピー`.
+<2> Return `サーバ`.
+
+
+[[analysis-kuromoji-stop]]
+===== `ja_stop` token filter
+
+The `ja_stop` token filter filters out Japanese stopwords (`_japanese_`), and
+any other custom stopwords specified by the user. This filter only supports
+the predefined `_japanese_` stopwords list.  If you want to use a different
+predefined list, then use the
+{ref}/analysis-stop-tokenfilter.html[`stop` token filter] instead.
+
+[source,json]
+--------------------------------------------------
+PUT kuromoji_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "analyzer_with_ja_stop": {
+            "tokenizer": "kuromoji_tokenizer",
+            "filter": [
+              "ja_stop"
+            ]
+          }
+        },
+        "filter": {
+          "ja_stop": {
+            "type": "ja_stop",
+            "stopwords": [
+              "_japanese_",
+              "ストップ"
+            ]
+          }
+        }
+      }
+    }
+  }
+}
+
+POST kuromoji_sample/_analyze?analyzer=my_analyzer&text=ストップは消える
+--------------------------------------------------
+// AUTOSENSE
+
+The above request returns:
+
+[source,text]
+--------------------------------------------------
+# Result
+{
+  "tokens" : [ {
+    "token" : "消える",
+    "start_offset" : 5,
+    "end_offset" : 8,
+    "type" : "word",
+    "position" : 3
+  } ]
+}
+--------------------------------------------------
+
diff --git a/docs/plugins/analysis-phonetic.asciidoc b/docs/plugins/analysis-phonetic.asciidoc
new file mode 100644
index 0000000..b15bfb8
--- /dev/null
+++ b/docs/plugins/analysis-phonetic.asciidoc
@@ -0,0 +1,120 @@
+[[analysis-phonetic]]
+=== Phonetic Analysis Plugin
+
+The Phonetic Analysis plugin provides token filters which convert tokens to
+their phonetic representation using Soundex, Metaphone, and a variety of other
+algorithms.
+
+[[analysis-phonetic-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install analysis-phonetic
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[analysis-phonetic-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove analysis-phonetic
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[analysis-phonetic-token-filter]]
+==== `phonetic` token filter
+
+The `phonetic` token filter takes the following settings:
+
+`encoder`::
+
+    Which phonetic encoder to use.  Accepts `metaphone` (default),
+    `doublemetaphone`, `soundex`, `refinedsoundex`, `caverphone1`,
+    `caverphone2`, `cologne`, `nysiis`, `koelnerphonetik`, `haasephonetik`,
+    `beidermorse`.
+
+`replace`::
+
+    Whether or not the original token should be replaced by the phonetic
+    token. Accepts `true` (default) and `false`.  Not supported by
+    `beidermorse` encoding.
+
+[source,json]
+--------------------------------------------------
+PUT phonetic_sample
+{
+  "settings": {
+    "index": {
+      "analysis": {
+        "analyzer": {
+          "my_analyzer": {
+            "tokenizer": "standard",
+            "filter": [
+              "standard",
+              "lowercase",
+              "my_metaphone"
+            ]
+          }
+        },
+        "filter": {
+          "my_metaphone": {
+            "type": "phonetic",
+            "encoder": "metaphone",
+            "replace": false
+          }
+        }
+      }
+    }
+  }
+}
+
+POST phonetic_sample/_analyze?analyzer=my_analyzer&text=Joe Bloggs <1>
+--------------------------------------------------
+// AUTOSENSE
+
+<1> Returns: `J`, `joe`, `BLKS`, `bloggs`
+
+
+[float]
+===== Double metaphone settings
+
+If the `double_metaphone` encoder is used, then this additional setting is
+supported:
+
+`max_code_len`::
+
+    The maximum length of the emitted metaphone token.  Defaults to `4`.
+
+[float]
+===== Beider Morse settings
+
+If the `beider_morse` encoder is used, then these additional settings are
+supported:
+
+`rule_type`::
+
+    Whether matching should be `exact` or `approx` (default).
+
+`name_type`::
+
+    Whether names are `ashkenazi`, `sephardic`, or `generic` (default).
+
+`languageset`::
+
+    An array of languages to check. If not specified, then the language will
+    be guessed. Accepts: `any`, `comomon`, `cyrillic`, `english`, `french`,
+    `german`, `hebrew`, `hungarian`, `polish`, `romanian`, `russian`,
+    `spanish`.
+
+
diff --git a/docs/plugins/analysis-smartcn.asciidoc b/docs/plugins/analysis-smartcn.asciidoc
new file mode 100644
index 0000000..33c755b
--- /dev/null
+++ b/docs/plugins/analysis-smartcn.asciidoc
@@ -0,0 +1,48 @@
+[[analysis-smartcn]]
+=== Smart Chinese Analysis Plugin
+
+The Smart Chinese Analysis plugin integrates Lucene's Smart Chinese analysis
+module into elasticsearch.
+
+It provides an analyzer for Chinese or mixed Chinese-English text. This
+analyzer uses probabilistic knowledge to find the optimal word segmentation
+for Simplified Chinese text. The text is first broken into sentences, then
+each sentence is segmented into words.
+
+
+[[analysis-smartcn-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install analysis-smartcn
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[analysis-smartcn-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove analysis-smartcn
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[analysis-smartcn-tokenizer]]
+[float]
+==== `smartcn` tokenizer and token filter
+
+The plugin provides the `smartcn` analyzer and `smartcn_tokenizer` tokenizer,
+which are not configurable.
+
+NOTE: The `smartcn_word` token filter and `smartcn_sentence` have been deprecated.
+
diff --git a/docs/plugins/analysis-stempel.asciidoc b/docs/plugins/analysis-stempel.asciidoc
new file mode 100644
index 0000000..a0b5c26
--- /dev/null
+++ b/docs/plugins/analysis-stempel.asciidoc
@@ -0,0 +1,43 @@
+[[analysis-stempel]]
+=== Stempel Polish Analysis Plugin
+
+The Stempel Analysis plugin integrates Lucene's Stempel analysis
+module for Polish into elasticsearch.
+
+It provides high quality stemming for Polish, based on the
+http://www.egothor.org/[Egothor project].
+
+[[analysis-stempel-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install analysis-stempel
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[analysis-stempel-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove analysis-stempel
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[analysis-stempel-tokenizer]]
+[float]
+==== `stempel` tokenizer and token filter
+
+The plugin provides the `polish` analyzer and `polish_stem` token filter,
+which are not configurable.
+
diff --git a/docs/plugins/analysis.asciidoc b/docs/plugins/analysis.asciidoc
new file mode 100644
index 0000000..453b720
--- /dev/null
+++ b/docs/plugins/analysis.asciidoc
@@ -0,0 +1,69 @@
+[[analysis]]
+== Analysis Plugins
+
+Analysis plugins extend Elasticsearch by adding new analyzers, tokenizers,
+token filters, or character filters to Elasticsearch.
+
+[float]
+==== Core analysis plugins
+
+The core analysis plugins are:
+
+<<analysis-icu,ICU>>::
+
+Adds extended Unicode support using the http://site.icu-project.org/[ICU]
+libraries, including better analysis of Asian languages, Unicode
+normalization, Unicode-aware case folding, collation support, and
+transliteration.
+
+<<analysis-kuromoji,Kuromoji>>::
+
+Advanced analysis of Japanese using the http://www.atilika.org/[Kuromoji analyzer].
+
+<<analysis-phonetic,Phonetic>>::
+
+Analyzes tokens into their phonetic equivalent using Soundex, Metaphone,
+Caverphone, and other codecs.
+
+<<analysis-smartcn,SmartCN>>::
+
+An analyzer for Chinese or mixed Chinese-English text. This analyzer uses
+probabilistic knowledge to find the optimal word segmentation for Simplified
+Chinese text. The text is first broken into sentences, then each sentence is
+segmented into words.
+
+<<analysis-stempel,Stempel>>::
+
+Provides high quality stemming for Polish.
+
+[float]
+==== Community contributed analysis plugins
+
+A number of analysis plugins have been contributed by our community:
+
+* https://github.com/yakaz/elasticsearch-analysis-combo/[Combo Analysis Plugin] (by Olivier Favre, Yakaz)
+* https://github.com/synhershko/elasticsearch-analysis-hebrew[Hebrew Analysis Plugin] (by Itamar Syn-Hershko)
+* https://github.com/medcl/elasticsearch-analysis-ik[IK Analysis Plugin] (by Medcl)
+* https://github.com/medcl/elasticsearch-analysis-mmseg[Mmseg Analysis Plugin] (by Medcl)
+* https://github.com/chytreg/elasticsearch-analysis-morfologik[Morfologik (Polish) Analysis plugin] (by chytreg)
+* https://github.com/imotov/elasticsearch-analysis-morphology[Russian and English Morphological Analysis Plugin] (by Igor Motov)
+* https://github.com/medcl/elasticsearch-analysis-pinyin[Pinyin Analysis Plugin] (by Medcl)
+* https://github.com/duydo/elasticsearch-analysis-vietnamese[Vietnamese Analysis Plugin] (by Duy Do)
+
+These community plugins appear to have been abandoned:
+
+* https://github.com/barminator/elasticsearch-analysis-annotation[Annotation Analysis Plugin] (by Michal Samek)
+* https://github.com/medcl/elasticsearch-analysis-string2int[String2Integer Analysis Plugin] (by Medcl)
+
+include::analysis-icu.asciidoc[]
+
+include::analysis-kuromoji.asciidoc[]
+
+include::analysis-phonetic.asciidoc[]
+
+include::analysis-smartcn.asciidoc[]
+
+include::analysis-stempel.asciidoc[]
+
+
+
diff --git a/docs/plugins/api.asciidoc b/docs/plugins/api.asciidoc
new file mode 100644
index 0000000..343d246
--- /dev/null
+++ b/docs/plugins/api.asciidoc
@@ -0,0 +1,65 @@
+[[api]]
+== API Extension Plugins
+
+API extension plugins add new functionality to Elasticsearch by adding new APIs or features, usually to do with search or mapping.
+
+[float]
+=== Core API extension plugins
+
+The core API extension plugins are:
+
+<<plugins-delete-by-query,Delete by Query>>::
+
+The delete by query plugin adds support for deleting all of the documents
+(from one or more indices) which match the specified query. It is a
+replacement for the problematic _delete-by-query_ functionality which has been
+removed from Elasticsearch core.
+
+https://github.com/elasticsearch/elasticsearch-mapper-attachments[Mapper Attachments Type plugin]::
+
+Integrates http://lucene.apache.org/tika/[Apache Tika] to provide a new field
+type `attachment` to allow indexing of documents such as PDFs and Microsoft
+Word.
+
+[float]
+=== Community contributed API extension plugins
+
+A number of plugins have been contributed by our community:
+
+* https://github.com/carrot2/elasticsearch-carrot2[carrot2 Plugin]:
+  Results clustering with http://project.carrot2.org/[carrot2] (by Dawid Weiss)
+
+* https://github.com/wikimedia/search-extra[Elasticsearch Trigram Accelerated Regular Expression Filter]:
+  (by Wikimedia Foundation/Nik Everett)
+
+* https://github.com/kzwang/elasticsearch-image[Elasticsearch Image Plugin]:
+  Uses https://code.google.com/p/lire/[Lire (Lucene Image Retrieval)] to allow users
+  to index images and search for similar images (by Kevin Wang)
+
+* https://github.com/wikimedia/search-highlighter[Elasticsearch Experimental Highlighter]:
+  (by Wikimedia Foundation/Nik Everett)
+
+* https://github.com/YannBrrd/elasticsearch-entity-resolution[Entity Resolution Plugin]:
+  Uses http://github.com/larsga/Duke[Duke] for duplication detection (by Yann Barraud)
+
+* https://github.com/NLPchina/elasticsearch-sql/[SQL language Plugin]:
+  Allows Elasticsearch to be queried with SQL (by nlpcn)
+
+* https://github.com/codelibs/elasticsearch-taste[Elasticsearch Taste Plugin]:
+  Mahout Taste-based Collaborative Filtering implementation (by CodeLibs Project)
+
+* https://github.com/hadashiA/elasticsearch-flavor[Elasticsearch Flavor Plugin] using
+  http://mahout.apache.org/[Mahout] Collaboration filtering (by hadashiA)
+
+These community plugins appear to have been abandoned:
+
+* https://github.com/derryx/elasticsearch-changes-plugin[Elasticsearch Changes Plugin] (by Thomas Peuss)
+
+* https://github.com/mattweber/elasticsearch-mocksolrplugin[Elasticsearch Mock Solr Plugin] (by Matt Weber)
+
+* http://siren.solutions/siren/downloads/[Elasticsearch SIREn Plugin]: Nested data search (by SIREn Solutions)
+
+* https://github.com/endgameinc/elasticsearch-term-plugin[Terms Component Plugin] (by Endgame Inc.)
+
+
+include::delete-by-query.asciidoc[]
diff --git a/docs/plugins/authors.asciidoc b/docs/plugins/authors.asciidoc
new file mode 100644
index 0000000..538b09e
--- /dev/null
+++ b/docs/plugins/authors.asciidoc
@@ -0,0 +1,62 @@
+[[plugin-authors]]
+== Help for plugin authors
+
+The Elasticsearch repository contains examples of:
+
+* a https://github.com/elastic/elasticsearch/tree/master/plugins/site-example[site plugin]
+  for serving static HTML, JavaScript, and CSS.
+* a https://github.com/elastic/elasticsearch/tree/master/plugins/jvm-example[Java plugin]
+  which contains Java code.
+
+These examples provide the bare bones needed to get started.  For more
+information about how to write a plugin, we recommend looking at the plugins
+listed in this documentation for inspiration.
+
+[NOTE]
+.Site plugins
+====================================
+
+The example site plugin mentioned above contains all of the scaffolding needed
+for integrating with Maven builds.  If you don't plan on using Maven, then all
+you really need in your plugin is:
+
+* The `plugin-descriptor.properties` file
+* The `_site/` directory
+* The `_site/index.html` file
+
+====================================
+
+[float]
+=== Plugin descriptor file
+
+All plugins, be they site or Java plugins, must contain a file called
+`plugin-descriptor.properties` in the root directory. The format for this file
+is described in detail  here:
+
+https://github.com/elastic/elasticsearch/blob/master/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties[`dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties`].
+
+Either fill in this template yourself (see
+https://github.com/lmenezes/elasticsearch-kopf/blob/master/plugin-descriptor.properties[elasticsearch-kopf]
+as an example) or, if you are using Elasticsearch's Maven build system, you
+can fill in the necessary values in the `pom.xml` for your plugin. For
+instance, see
+https://github.com/elastic/elasticsearch/blob/master/plugins/site-example/pom.xml[`plugins/site-example/pom.xml`].
+
+[float]
+=== Loading plugins from the classpath
+
+When testing a Java plugin, it will only be auto-loaded if it is in the
+`plugins/` directory.  If, instead, it is in your classpath, you can tell
+Elasticsearch to load it with the `plugin.types` setting:
+
+[source,java]
+--------------------------
+settingsBuilder()
+    .put("cluster.name", cluster)
+    .put("path.home", getHome())
+    .put("plugin.types", MyCustomPlugin.class.getName()) <1>
+    .build();
+--------------------------
+<1> Tells Elasticsearch to load your plugin.
+
+
diff --git a/docs/plugins/cloud-aws.asciidoc b/docs/plugins/cloud-aws.asciidoc
new file mode 100644
index 0000000..34cac79
--- /dev/null
+++ b/docs/plugins/cloud-aws.asciidoc
@@ -0,0 +1,468 @@
+[[cloud-aws]]
+=== AWS Cloud Plugin
+
+The Amazon Web Service (AWS) Cloud plugin uses the
+https://github.com/aws/aws-sdk-java[AWS API] for unicast discovery, and adds
+support for using S3 as a repository for
+{ref}/modules-snapshots.html[Snapshot/Restore].
+
+[[cloud-aws-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install cloud-aws
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[cloud-aws-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove cloud-aws
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[cloud-aws-usage]]
+==== Getting started with AWS
+
+The plugin will default to using
+http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html[IAM Role]
+credentials for authentication. These can be overridden by, in increasing
+order of precedence, system properties `aws.accessKeyId` and `aws.secretKey`,
+environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_KEY`, or the
+elasticsearch config using `cloud.aws.access_key` and `cloud.aws.secret_key`:
+
+[source,yaml]
+----
+cloud:
+    aws:
+        access_key: AKVAIQBF2RECL7FJWGJQ
+        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
+----
+
+[[cloud-aws-usage-security]]
+===== Transport security
+
+By default this plugin uses HTTPS for all API calls to AWS endpoints. If you wish to configure HTTP you can set
+`cloud.aws.protocol` in the elasticsearch config. You can optionally override this setting per individual service
+via: `cloud.aws.ec2.protocol` or `cloud.aws.s3.protocol`.
+
+[source,yaml]
+----
+cloud:
+    aws:
+        protocol: https
+        s3:
+            protocol: http
+        ec2:
+            protocol: https
+----
+
+In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be
+`http` or `https`):
+
+[source,yaml]
+----
+cloud:
+    aws:
+        protocol: https
+        proxy_host: proxy1.company.com
+        proxy_port: 8083
+----
+
+You can also set different proxies for `ec2` and `s3`:
+
+[source,yaml]
+----
+cloud:
+    aws:
+        s3:
+            proxy_host: proxy1.company.com
+            proxy_port: 8083
+        ec2:
+            proxy_host: proxy2.company.com
+            proxy_port: 8083
+----
+
+[[cloud-aws-usage-region]]
+===== Region
+
+The `cloud.aws.region` can be set to a region and will automatically use the relevant settings for both `ec2` and `s3`.
+The available values are:
+
+* `us-east` (`us-east-1`)
+* `us-west` (`us-west-1`)
+* `us-west-1`
+* `us-west-2`
+* `ap-southeast` (`ap-southeast-1`)
+* `ap-southeast-1`
+* `ap-southeast-2`
+* `ap-northeast` (`ap-northeast-1`)
+* `eu-west` (`eu-west-1`)
+* `eu-central` (`eu-central-1`)
+* `sa-east` (`sa-east-1`)
+* `cn-north` (`cn-north-1`)
+
+[[cloud-aws-usage-signer]]
+===== EC2/S3 Signer API
+
+If you are using a compatible EC2 or S3 service, they might be using an older API to sign the requests.
+You can set your compatible signer API using `cloud.aws.signer` (or `cloud.aws.ec2.signer` and `cloud.aws.s3.signer`)
+with the right signer to use. Defaults to `AWS4SignerType`.
+
+[[cloud-aws-discovery]]
+==== EC2 Discovery
+
+ec2 discovery allows to use the ec2 APIs to perform automatic discovery (similar to multicast in non hostile multicast
+environments). Here is a simple sample configuration:
+
+[source,yaml]
+----
+discovery:
+    type: ec2
+----
+
+The ec2 discovery is using the same credentials as the rest of the AWS services provided by this plugin (`repositories`).
+See <<cloud-aws-usage>> for details.
+
+The following are a list of settings (prefixed with `discovery.ec2`) that can further control the discovery:
+
+`groups`::
+
+    Either a comma separated list or array based list of (security) groups.
+    Only instances with the provided security groups will be used in the
+    cluster discovery. (NOTE: You could provide either group NAME or group
+    ID.)
+
+`host_type`::
+
+    The type of host type to use to communicate with other instances. Can be
+    one of `private_ip`, `public_ip`, `private_dns`, `public_dns`. Defaults to
+    `private_ip`.
+
+`availability_zones`::
+
+    Either a comma separated list or array based list of availability zones.
+    Only instances within the provided availability zones will be used in the
+    cluster discovery.
+
+`any_group`::
+
+    If set to `false`, will require all security groups to be present for the
+    instance to be used for the discovery. Defaults to `true`.
+
+`ping_timeout`::
+
+    How long to wait for existing EC2 nodes to reply during discovery.
+    Defaults to `3s`. If no unit like `ms`, `s` or `m` is specified,
+    milliseconds are used.
+
+[[cloud-aws-discovery-permissions]]
+===== Recommended EC2 Permissions
+
+EC2 discovery requires making a call to the EC2 service. You'll want to setup
+an IAM policy to allow this. You can create a custom policy via the IAM
+Management Console. It should look similar to this.
+
+[source,js]
+----
+{
+  "Statement": [
+    {
+      "Action": [
+        "ec2:DescribeInstances"
+      ],
+      "Effect": "Allow",
+      "Resource": [
+        "*"
+      ]
+    }
+  ],
+  "Version": "2012-10-17"
+}
+----
+
+[[cloud-aws-discovery-filtering]]
+===== Filtering by Tags
+
+The ec2 discovery can also filter machines to include in the cluster based on tags (and not just groups). The settings
+to use include the `discovery.ec2.tag.` prefix. For example, setting `discovery.ec2.tag.stage` to `dev` will only
+filter instances with a tag key set to `stage`, and a value of `dev`. Several tags set will require all of those tags
+to be set for the instance to be included.
+
+One practical use for tag filtering is when an ec2 cluster contains many nodes that are not running elasticsearch. In
+this case (particularly with high `ping_timeout` values) there is a risk that a new node's discovery phase will end
+before it has found the cluster (which will result in it declaring itself master of a new cluster with the same name
+- highly undesirable). Tagging elasticsearch ec2 nodes and then filtering by that tag will resolve this issue.
+
+[[cloud-aws-discovery-attributes]]
+===== Automatic Node Attributes
+
+Though not dependent on actually using `ec2` as discovery (but still requires the cloud aws plugin installed), the
+plugin can automatically add node attributes relating to ec2 (for example, availability zone, that can be used with
+the awareness allocation feature). In order to enable it, set `cloud.node.auto_attributes` to `true` in the settings.
+
+[[cloud-aws-discovery-endpoint]]
+===== Using other EC2 endpoint
+
+If you are using any EC2 api compatible service, you can set the endpoint you want to use by setting
+`cloud.aws.ec2.endpoint` to your URL provider.
+
+[[cloud-aws-repository]]
+==== S3 Repository
+
+The S3 repository is using S3 to store snapshots. The S3 repository can be created using the following command:
+
+[source,json]
+----
+PUT _snapshot/my_s3_repository
+{
+  "type": "s3",
+  "settings": {
+    "bucket": "my_bucket_name",
+    "region": "us-west"
+  }
+}
+----
+// AUTOSENSE
+
+The following settings are supported:
+
+`bucket`::
+
+    The name of the bucket to be used for snapshots. (Mandatory)
+
+`region`::
+
+    The region where bucket is located. Defaults to US Standard
+
+`endpoint`::
+
+    The endpoint to the S3 API. Defaults to AWS's default S3 endpoint. Note
+    that setting a region overrides the endpoint setting.
+
+`protocol`::
+
+    The protocol to use (`http` or `https`). Defaults to value of
+    `cloud.aws.protocol` or `cloud.aws.s3.protocol`.
+
+`base_path`::
+
+    Specifies the path within bucket to repository data. Defaults to root
+    directory.
+
+`access_key`::
+
+    The access key to use for authentication. Defaults to value of
+    `cloud.aws.access_key`.
+
+`secret_key`::
+
+    The secret key to use for authentication. Defaults to value of
+    `cloud.aws.secret_key`.
+
+`chunk_size`::
+
+    Big files can be broken down into chunks during snapshotting if needed.
+    The chunk size can be specified in bytes or by using size value notation,
+    i.e. `1g`, `10m`, `5k`. Defaults to `100m`.
+
+`compress`::
+
+    When set to `true` metadata files are stored in compressed format. This
+    setting doesn't affect index files that are already compressed by default.
+    Defaults to `false`.
+
+`server_side_encryption`::
+
+    When set to `true` files are encrypted on server side using AES256
+    algorithm. Defaults to `false`.
+
+`buffer_size`::
+
+    Minimum threshold below which the chunk is uploaded using a single
+    request. Beyond this threshold, the S3 repository will use the
+    http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html[AWS Multipart Upload API]
+    to split the chunk into several parts, each of `buffer_size` length, and
+    to upload each part in its own request. Note that positioning a buffer
+    size lower than `5mb` is not allowed since it will prevents the use of the
+    Multipart API and may result in upload errors. Defaults to `5mb`.
+
+`max_retries`::
+
+    Number of retries in case of S3 errors. Defaults to `3`.
+
+
+The S3 repositories use the same credentials as the rest of the AWS services
+provided by this plugin (`discovery`). See <<cloud-aws-usage>> for details.
+
+Multiple S3 repositories can be created. If the buckets require different
+credentials, then define them as part of the repository settings.
+
+[[cloud-aws-repository-permissions]]
+===== Recommended S3 Permissions
+
+In order to restrict the Elasticsearch snapshot process to the minimum required resources, we recommend using Amazon
+IAM in conjunction with pre-existing S3 buckets. Here is an example policy which will allow the snapshot access to an
+ S3 bucket named "snaps.example.com". This may be configured through the AWS IAM console, by creating a Custom Policy,
+ and using a Policy Document similar to this (changing snaps.example.com to your bucket name).
+
+[source,js]
+----
+{
+  "Statement": [
+    {
+      "Action": [
+        "s3:ListBucket",
+        "s3:GetBucketLocation",
+        "s3:ListBucketMultipartUploads",
+        "s3:ListBucketVersions"
+      ],
+      "Effect": "Allow",
+      "Resource": [
+        "arn:aws:s3:::snaps.example.com"
+      ]
+    },
+    {
+      "Action": [
+        "s3:GetObject",
+        "s3:PutObject",
+        "s3:DeleteObject",
+        "s3:AbortMultipartUpload",
+        "s3:ListMultipartUploadParts"
+      ],
+      "Effect": "Allow",
+      "Resource": [
+        "arn:aws:s3:::snaps.example.com/*"
+      ]
+    }
+  ],
+  "Version": "2012-10-17"
+}
+----
+
+You may further restrict the permissions by specifying a prefix within the bucket, in this example, named "foo".
+
+[source,js]
+----
+{
+  "Statement": [
+    {
+      "Action": [
+        "s3:ListBucket",
+        "s3:GetBucketLocation",
+        "s3:ListBucketMultipartUploads",
+        "s3:ListBucketVersions"
+      ],
+      "Condition": {
+        "StringLike": {
+          "s3:prefix": [
+            "foo/*"
+          ]
+        }
+      },
+      "Effect": "Allow",
+      "Resource": [
+        "arn:aws:s3:::snaps.example.com"
+      ]
+    },
+    {
+      "Action": [
+        "s3:GetObject",
+        "s3:PutObject",
+        "s3:DeleteObject",
+        "s3:AbortMultipartUpload",
+        "s3:ListMultipartUploadParts"
+      ],
+      "Effect": "Allow",
+      "Resource": [
+        "arn:aws:s3:::snaps.example.com/foo/*"
+      ]
+    }
+  ],
+  "Version": "2012-10-17"
+}
+----
+
+The bucket needs to exist to register a repository for snapshots. If you did not create the bucket then the repository
+registration will fail. If you want elasticsearch to create the bucket instead, you can add the permission to create a
+specific bucket like this:
+
+[source,js]
+----
+{
+   "Action": [
+      "s3:CreateBucket"
+   ],
+   "Effect": "Allow",
+   "Resource": [
+      "arn:aws:s3:::snaps.example.com"
+   ]
+}
+----
+
+[[cloud-aws-repository-endpoint]]
+===== Using other S3 endpoint
+
+If you are using any S3 api compatible service, you can set a global endpoint by setting `cloud.aws.s3.endpoint`
+to your URL provider. Note that this setting will be used for all S3 repositories.
+
+Different `endpoint`, `region` and `protocol` settings can be set on a per-repository basis
+See <<cloud-aws-repository>> for details.
+
+[[cloud-aws-testing]]
+==== Testing AWS
+
+Integrations tests in this plugin require working AWS configuration and therefore disabled by default. Three buckets
+and two iam users have to be created. The first iam user needs access to two buckets in different regions and the final
+bucket is exclusive for the other iam user. To enable tests prepare a config file elasticsearch.yml with the following
+content:
+
+[source,yaml]
+----
+cloud:
+    aws:
+        access_key: AKVAIQBF2RECL7FJWGJQ
+        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
+
+repositories:
+    s3:
+        bucket: "bucket_name"
+        region: "us-west-2"
+        private-bucket:
+            bucket: <bucket not accessible by default key>
+            access_key: <access key>
+            secret_key: <secret key>
+        remote-bucket:
+            bucket: <bucket in other region>
+            region: <region>
+	external-bucket:
+	    bucket: <bucket>
+	    access_key: <access key>
+	    secret_key: <secret key>
+	    endpoint: <endpoint>
+	    protocol: <protocol>
+
+----
+
+Replace all occurrences of `access_key`, `secret_key`, `endpoint`, `protocol`, `bucket` and `region` with your settings.
+Please, note that the test will delete all snapshot/restore related files in the specified buckets.
+
+To run test:
+
+[source,sh]
+----
+mvn -Dtests.aws=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
+----
+
diff --git a/docs/plugins/cloud-azure.asciidoc b/docs/plugins/cloud-azure.asciidoc
new file mode 100644
index 0000000..151cd33
--- /dev/null
+++ b/docs/plugins/cloud-azure.asciidoc
@@ -0,0 +1,667 @@
+[[cloud-azure]]
+=== Azure Cloud Plugin
+
+The Azure Cloud plugin uses the Azure API for unicast discovery, and adds
+support for using Azure as a repository for
+{ref}/modules-snapshots.html[Snapshot/Restore].
+
+[[cloud-azure-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install cloud-aws
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[cloud-azure-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove cloud-aws
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[cloud-azure-discovery]]
+==== Azure Virtual Machine Discovery
+
+Azure VM discovery allows to use the azure APIs to perform automatic discovery (similar to multicast in non hostile
+multicast environments). Here is a simple sample configuration:
+
+[source,yaml]
+----
+cloud:
+    azure:
+        management:
+             subscription.id: XXX-XXX-XXX-XXX
+             cloud.service.name: es-demo-app
+             keystore:
+                   path: /path/to/azurekeystore.pkcs12
+                   password: WHATEVER
+                   type: pkcs12
+
+discovery:
+    type: azure
+----
+
+[[cloud-azure-discovery-short]]
+===== How to start (short story)
+
+* Create Azure instances
+* Install Elasticsearch
+* Install Azure plugin
+* Modify `elasticsearch.yml` file
+* Start Elasticsearch
+
+[[cloud-azure-discovery-settings]]
+===== Azure credential API settings
+
+The following are a list of settings that can further control the credential API:
+
+[horizontal]
+`cloud.azure.management.keystore.path`::
+
+    /path/to/keystore
+
+`cloud.azure.management.keystore.type`::
+
+    `pkcs12`, `jceks` or `jks`. Defaults to `pkcs12`.
+
+`cloud.azure.management.keystore.password`::
+
+    your_password for the keystore
+
+`cloud.azure.management.subscription.id`::
+
+    your_azure_subscription_id
+
+`cloud.azure.management.cloud.service.name`::
+
+    your_azure_cloud_service_name
+
+
+[[cloud-azure-discovery-settings-advanced]]
+===== Advanced settings
+
+The following are a list of settings that can further control the discovery:
+
+`discovery.azure.host.type`::
+
+    Either `public_ip` or `private_ip` (default). Azure discovery will use the
+    one you set to ping other nodes.
+
+`discovery.azure.endpoint.name`::
+
+    When using `public_ip` this setting is used to identify the endpoint name
+    used to forward requests to elasticsearch (aka transport port name).
+    Defaults to `elasticsearch`. In Azure management console, you could define
+    an endpoint `elasticsearch` forwarding for example requests on public IP
+    on port 8100 to the virtual machine on port 9300.
+
+`discovery.azure.deployment.name`::
+
+    Deployment name if any. Defaults to the value set with
+    `cloud.azure.management.cloud.service.name`.
+
+`discovery.azure.deployment.slot`::
+
+    Either `staging` or `production` (default).
+
+For example:
+
+[source,yaml]
+----
+discovery:
+    type: azure
+    azure:
+        host:
+            type: private_ip
+        endpoint:
+            name: elasticsearch
+        deployment:
+            name: your_azure_cloud_service_name
+            slot: production
+----
+
+[[cloud-azure-discovery-long]]
+==== Setup process for Azure Discovery
+
+We will expose here one strategy which is to hide our Elasticsearch cluster from outside.
+
+With this strategy, only VMs behind the same virtual port can talk to each
+other.  That means that with this mode, you can use elasticsearch unicast
+discovery to build a cluster, using the Azure API to retrieve information
+about your nodes.
+
+[[cloud-azure-discovery-long-prerequisites]]
+===== Prerequisites
+
+Before starting, you need to have:
+
+* A http://www.windowsazure.com/[Windows Azure account]
+* OpenSSL that isn't from MacPorts, specifically `OpenSSL 1.0.1f 6 Jan
+  2014` doesn't seem to create a valid keypair for ssh. FWIW,
+ `OpenSSL 1.0.1c 10 May 2012` on Ubuntu 12.04 LTS is known to work.
+* SSH keys and certificate
++
+--
+
+You should follow http://azure.microsoft.com/en-us/documentation/articles/linux-use-ssh-key/[this guide] to learn
+how to create or use existing SSH keys. If you have already did it, you can skip the following.
+
+Here is a description on how to generate SSH keys using `openssl`:
+
+[source,sh]
+----
+# You may want to use another dir than /tmp
+cd /tmp
+openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout azure-private.key -out azure-certificate.pem
+chmod 600 azure-private.key azure-certificate.pem
+openssl x509 -outform der -in azure-certificate.pem -out azure-certificate.cer
+----
+
+Generate a keystore which will be used by the plugin to authenticate with a certificate
+all Azure API calls.
+
+[source,sh]
+----
+# Generate a keystore (azurekeystore.pkcs12)
+# Transform private key to PEM format
+openssl pkcs8 -topk8 -nocrypt -in azure-private.key -inform PEM -out azure-pk.pem -outform PEM
+# Transform certificate to PEM format
+openssl x509 -inform der -in azure-certificate.cer -out azure-cert.pem
+cat azure-cert.pem azure-pk.pem > azure.pem.txt
+# You MUST enter a password!
+openssl pkcs12 -export -in azure.pem.txt -out azurekeystore.pkcs12 -name azure -noiter -nomaciter
+----
+
+Upload the `azure-certificate.cer` file both in the elasticsearch Cloud Service (under `Manage Certificates`),
+and under `Settings -> Manage Certificates`.
+
+IMPORTANT: When prompted for a password, you need to enter a non empty one.
+
+See this http://www.windowsazure.com/en-us/manage/linux/how-to-guides/ssh-into-linux/[guide] for
+more details about how to create keys for Azure.
+
+Once done, you need to upload your certificate in Azure:
+
+* Go to the https://account.windowsazure.com/[management console].
+* Sign in using your account.
+* Click on `Portal`.
+* Go to Settings (bottom of the left list)
+* On the bottom bar, click on `Upload` and upload your `azure-certificate.cer` file.
+
+You may want to use
+http://www.windowsazure.com/en-us/develop/nodejs/how-to-guides/command-line-tools/[Windows Azure Command-Line Tool]:
+
+--
+
+* Install https://github.com/joyent/node/wiki/Installing-Node.js-via-package-manager[NodeJS], for example using
+homebrew on MacOS X:
++
+[source,sh]
+----
+brew install node
+----
+
+* Install Azure tools
++
+[source,sh]
+----
+sudo npm install azure-cli -g
+----
+
+* Download and import your azure settings:
++
+[source,sh]
+----
+# This will open a browser and will download a .publishsettings file
+azure account download
+
+# Import this file (we have downloaded it to /tmp)
+# Note, it will create needed files in ~/.azure. You can remove azure.publishsettings when done.
+azure account import /tmp/azure.publishsettings
+----
+
+[[cloud-azure-discovery-long-instance]]
+===== Creating your first instance
+
+You need to have a storage account available. Check http://www.windowsazure.com/en-us/develop/net/how-to-guides/blob-storage/#create-account[Azure Blob Storage documentation]
+for more information.
+
+You will need to choose the operating system you want to run on. To get a list of official available images, run:
+
+[source,sh]
+----
+azure vm image list
+----
+
+Let's say we are going to deploy an Ubuntu image on an extra small instance in West Europe:
+
+[horizontal]
+Azure cluster name::
+
+    `azure-elasticsearch-cluster`
+
+Image::
+
+    `b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB`
+
+VM Name::
+
+    `myesnode1`
+
+VM Size::
+
+    `extrasmall`
+
+Location::
+
+    `West Europe`
+
+Login::
+
+    `elasticsearch`
+
+Password::
+
+    `password1234!!`
+
+
+Using command line:
+
+[source,sh]
+----
+azure vm create azure-elasticsearch-cluster \
+                b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB \
+                --vm-name myesnode1 \
+                --location "West Europe" \
+                --vm-size extrasmall \
+                --ssh 22 \
+                --ssh-cert /tmp/azure-certificate.pem \
+                elasticsearch password1234\!\!
+----
+
+You should see something like:
+
+[source,text]
+----
+info:    Executing command vm create
++ Looking up image
++ Looking up cloud service
++ Creating cloud service
++ Retrieving storage accounts
++ Configuring certificate
++ Creating VM
+info:    vm create command OK
+----
+
+Now, your first instance is started.
+
+[TIP]
+.Working with SSH
+===============================================
+
+You need to give the private key and username each time you log on your instance:
+
+[source,sh]
+----
+ssh -i ~/.ssh/azure-private.key elasticsearch@myescluster.cloudapp.net
+----
+
+But you can also define it once in `~/.ssh/config` file:
+
+[source,text]
+----
+Host *.cloudapp.net
+ User elasticsearch
+ StrictHostKeyChecking no
+ UserKnownHostsFile=/dev/null
+ IdentityFile ~/.ssh/azure-private.key
+----
+===============================================
+
+Next, you need to install Elasticsearch on your new instance. First, copy your
+keystore to the instance, then connect to the instance using SSH:
+
+[source,sh]
+----
+scp /tmp/azurekeystore.pkcs12 azure-elasticsearch-cluster.cloudapp.net:/home/elasticsearch
+ssh azure-elasticsearch-cluster.cloudapp.net
+----
+
+Once connected, install Elasticsearch:
+
+[source,sh]
+----
+# Install Latest Java version
+# Read http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html for details
+sudo add-apt-repository ppa:webupd8team/java
+sudo apt-get update
+sudo apt-get install oracle-java7-installer
+
+# If you want to install OpenJDK instead
+# sudo apt-get update
+# sudo apt-get install openjdk-7-jre-headless
+
+# Download Elasticsearch
+curl -s https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb -o elasticsearch-2.0.0.deb
+
+# Prepare Elasticsearch installation
+sudo dpkg -i elasticsearch-2.0.0.deb
+----
+
+Check that elasticsearch is running:
+
+[source,sh]
+----
+curl http://localhost:9200/
+----
+
+This command should give you a JSON result:
+
+[source,javascript]
+----
+{
+  "status" : 200,
+  "name" : "Living Colossus",
+  "version" : {
+    "number" : "2.0.0",
+    "build_hash" : "a46900e9c72c0a623d71b54016357d5f94c8ea32",
+    "build_timestamp" : "2014-02-12T16:18:34Z",
+    "build_snapshot" : false,
+    "lucene_version" : "5.1"
+  },
+  "tagline" : "You Know, for Search"
+}
+----
+
+[[cloud-azure-discovery-long-plugin]]
+===== Install elasticsearch cloud azure plugin
+
+[source,sh]
+----
+# Stop elasticsearch
+sudo service elasticsearch stop
+
+# Install the plugin
+sudo /usr/share/elasticsearch/bin/plugin install elasticsearch/elasticsearch-cloud-azure/2.6.1
+
+# Configure it
+sudo vi /etc/elasticsearch/elasticsearch.yml
+----
+
+And add the following lines:
+
+[source,yaml]
+----
+# If you don't remember your account id, you may get it with `azure account list`
+cloud:
+    azure:
+        management:
+             subscription.id: your_azure_subscription_id
+             cloud.service.name: your_azure_cloud_service_name
+             keystore:
+                   path: /home/elasticsearch/azurekeystore.pkcs12
+                   password: your_password_for_keystore
+
+discovery:
+    type: azure
+
+# Recommended (warning: non durable disk)
+# path.data: /mnt/resource/elasticsearch/data
+----
+
+Restart elasticsearch:
+
+[source,sh]
+----
+sudo service elasticsearch start
+----
+
+If anything goes wrong, check your logs in `/var/log/elasticsearch`.
+
+[[cloud-azure-discovery-scale]]
+==== Scaling Out!
+
+You need first to create an image of your previous machine.
+Disconnect from your machine and run locally the following commands:
+
+[source,sh]
+----
+# Shutdown the instance
+azure vm shutdown myesnode1
+
+# Create an image from this instance (it could take some minutes)
+azure vm capture myesnode1 esnode-image --delete
+
+# Note that the previous instance has been deleted (mandatory)
+# So you need to create it again and BTW create other instances.
+
+azure vm create azure-elasticsearch-cluster \
+                esnode-image \
+                --vm-name myesnode1 \
+                --location "West Europe" \
+                --vm-size extrasmall \
+                --ssh 22 \
+                --ssh-cert /tmp/azure-certificate.pem \
+                elasticsearch password1234\!\!
+----
+
+
+[TIP]
+=========================================
+It could happen that azure changes the endpoint public IP address.
+DNS propagation could take some minutes before you can connect again using
+name. You can get from azure the IP address if needed, using:
+
+[source,sh]
+----
+# Look at Network `Endpoints 0 Vip`
+azure vm show myesnode1
+----
+
+=========================================
+
+Let's start more instances!
+
+[source,sh]
+----
+for x in $(seq  2 10)
+	do
+		echo "Launching azure instance #$x..."
+		azure vm create azure-elasticsearch-cluster \
+		                esnode-image \
+		                --vm-name myesnode$x \
+		                --vm-size extrasmall \
+		                --ssh $((21 + $x)) \
+		                --ssh-cert /tmp/azure-certificate.pem \
+		                --connect \
+		                elasticsearch password1234\!\!
+	done
+----
+
+If you want to remove your running instances:
+
+[source,sh]
+----
+azure vm delete myesnode1
+----
+
+[[cloud-azure-repository]]
+==== Azure Repository
+
+To enable Azure repositories, you have first to set your azure storage settings in `elasticsearch.yml` file:
+
+[source,yaml]
+----
+cloud:
+    azure:
+        storage:
+            account: your_azure_storage_account
+            key: your_azure_storage_key
+----
+
+For information, in previous version of the azure plugin, settings were:
+
+[source,yaml]
+----
+cloud:
+    azure:
+        storage_account: your_azure_storage_account
+        storage_key: your_azure_storage_key
+----
+
+The Azure repository supports following settings:
+
+`container`::
+
+    Container name. Defaults to `elasticsearch-snapshots`
+
+`base_path`::
+
+    Specifies the path within container to repository data. Defaults to empty
+    (root directory).
+
+`chunk_size`::
+
+    Big files can be broken down into chunks during snapshotting if needed.
+    The chunk size can be specified in bytes or by using size value notation,
+    i.e. `1g`, `10m`, `5k`. Defaults to `64m` (64m max)
+
+`compress`::
+
+    When set to `true` metadata files are stored in compressed format. This
+    setting doesn't affect index files that are already compressed by default.
+    Defaults to `false`.
+
+Some examples, using scripts:
+
+[source,json]
+----
+# The simpliest one
+PUT _snapshot/my_backup1
+{
+    "type": "azure"
+}
+
+# With some settings
+PUT _snapshot/my_backup2
+{
+    "type": "azure",
+    "settings": {
+        "container": "backup_container",
+        "base_path": "backups",
+        "chunk_size": "32m",
+        "compress": true
+    }
+}
+----
+// AUTOSENSE
+
+Example using Java:
+
+[source,java]
+----
+client.admin().cluster().preparePutRepository("my_backup3")
+    .setType("azure").setSettings(Settings.settingsBuilder()
+        .put(Storage.CONTAINER, "backup_container")
+        .put(Storage.CHUNK_SIZE, new ByteSizeValue(32, ByteSizeUnit.MB))
+    ).get();
+----
+
+[[cloud-azure-repository-validation]]
+===== Repository validation rules
+
+According to the http://msdn.microsoft.com/en-us/library/dd135715.aspx[containers naming guide], a container name must
+be a valid DNS name, conforming to the following naming rules:
+
+* Container names must start with a letter or number, and can contain only letters, numbers, and the dash (-) character.
+* Every dash (-) character must be immediately preceded and followed by a letter or number; consecutive dashes are not
+permitted in container names.
+* All letters in a container name must be lowercase.
+* Container names must be from 3 through 63 characters long.
+
+[[cloud-azure-testing]]
+==== Testing Azure
+
+Integrations tests in this plugin require working Azure configuration and therefore disabled by default.
+To enable tests prepare a config file `elasticsearch.yml` with the following content:
+
+[source,yaml]
+----
+cloud:
+  azure:
+    storage:
+      account: "YOUR-AZURE-STORAGE-NAME"
+      key: "YOUR-AZURE-STORAGE-KEY"
+----
+
+Replaces `account`, `key` with your settings. Please, note that the test will delete all snapshot/restore related
+files in the specified bucket.
+
+To run test:
+
+[source,sh]
+----
+mvn -Dtests.azure=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
+----
+
+[[cloud-azure-smb-workaround]]
+==== Working around a bug in Windows SMB and Java on windows
+
+When using a shared file system based on the SMB protocol (like Azure File Service) to store indices, the way Lucene
+open index segment files is with a write only flag. This is the _correct_ way to open the files, as they will only be
+used for writes and allows different FS implementations to optimize for it. Sadly, in windows with SMB, this disables
+the cache manager, causing writes to be slow. This has been described in
+https://issues.apache.org/jira/browse/LUCENE-6176[LUCENE-6176], but it affects each and every Java program out there!.
+This need and must be fixed outside of ES and/or Lucene, either in windows or OpenJDK. For now, we are providing an
+experimental support to open the files with read flag, but this should be considered experimental and the correct way
+to fix it is in OpenJDK or Windows.
+
+The Azure Cloud plugin provides two storage types optimized for SMB:
+
+`smb_mmap_fs`::
+
+    a SMB specific implementation of the default
+    {ref}/index-modules-store.html#mmapfs[mmap fs]
+
+`smb_simple_fs`::
+
+    a SMB specific implementation of the default
+    {ref}/index-modules-store.html#simplefs[simple fs]
+
+To use one of these specific storage types, you need to install the Azure Cloud plugin and restart the node.
+Then configure Elasticsearch to set the storage type you want.
+
+This can be configured for all indices by adding this to the `elasticsearch.yml` file:
+
+[source,yaml]
+----
+index.store.type: smb_simple_fs
+----
+
+Note that setting will be applied for newly created indices.
+
+It can also be set on a per-index basis at index creation time:
+
+[source,json]
+----
+PUT my_index
+{
+   "settings": {
+       "index.store.type": "smb_mmap_fs"
+   }
+}
+----
+// AUTOSENSE
diff --git a/docs/plugins/cloud-gce.asciidoc b/docs/plugins/cloud-gce.asciidoc
new file mode 100644
index 0000000..3a7959f
--- /dev/null
+++ b/docs/plugins/cloud-gce.asciidoc
@@ -0,0 +1,479 @@
+[[cloud-gce]]
+=== GCE Cloud Plugin
+
+The Google Compute Engine Cloud plugin uses the GCE API for unicast discovery.
+
+[[cloud-gce-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install cloud-gce
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[cloud-gce-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove cloud-gce
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[cloud-gce-usage-discovery]]
+==== GCE Virtual Machine Discovery
+
+Google Compute Engine VM discovery allows to use the google APIs to perform automatic discovery (similar to multicast
+in non hostile multicast environments). Here is a simple sample configuration:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: <your-google-project-id>
+      zone: <your-zone>
+discovery:
+      type: gce
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-short]]
+===== How to start (short story)
+
+* Create Google Compute Engine instance (with compute rw permissions)
+* Install Elasticsearch
+* Install Google Compute Engine Cloud plugin
+* Modify `elasticsearch.yml` file
+* Start Elasticsearch
+
+[[cloud-gce-usage-discovery-long]]
+==== Setting up GCE Discovery
+
+
+[[cloud-gce-usage-discovery-long-prerequisites]]
+===== Prerequisites
+
+Before starting, you need:
+
+* Your project ID, e.g. `es-cloud`. Get it from https://code.google.com/apis/console/[Google API Console].
+* To install https://developers.google.com/cloud/sdk/[Google Cloud SDK]
+
+If you did not set it yet, you can define your default project you will work on:
+
+[source,sh]
+--------------------------------------------------
+gcloud config set project es-cloud
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-long-first-instance]]
+===== Creating your first instance
+
+
+[source,sh]
+--------------------------------------------------
+gcutil addinstance myesnode1 \
+       --service_account_scope=compute-rw,storage-full \
+       --persistent_boot_disk
+--------------------------------------------------
+
+Then follow these steps:
+
+* You will be asked to open a link in your browser. Login and allow access to listed services.
+* You will get back a verification code. Copy and paste it in your terminal.
+* You should see an `Authentication successful.` message.
+* Choose your zone, e.g. `europe-west1-a`.
+* Choose your compute instance size, e.g. `f1-micro`.
+* Choose your OS, e.g. `projects/debian-cloud/global/images/debian-7-wheezy-v20140606`.
+* You may be asked to create a ssh key. Follow the instructions to create one.
+
+When done, a report like this one should appears:
+
+[source,text]
+--------------------------------------------------
+Table of resources:
+
++-----------+--------------+-------+---------+--------------+----------------+----------------+----------------+---------+----------------+
+|   name    | machine-type | image | network |  network-ip  |  external-ip   |     disks      |      zone      | status  | status-message |
++-----------+--------------+-------+---------+--------------+----------------+----------------+----------------+---------+----------------+
+| myesnode1 | f1-micro     |       | default | 10.240.20.57 | 192.158.29.199 | boot-myesnode1 | europe-west1-a | RUNNING |                |
++-----------+--------------+-------+---------+--------------+----------------+----------------+----------------+---------+----------------+
+--------------------------------------------------
+
+You can now connect to your instance:
+
+[source,sh]
+--------------------------------------------------
+# Connect using google cloud SDK
+gcloud compute ssh myesnode1 --zone europe-west1-a
+
+# Or using SSH with external IP address
+ssh -i ~/.ssh/google_compute_engine 192.158.29.199
+--------------------------------------------------
+
+[IMPORTANT]
+.Service Account Permissions
+==============================================
+
+It's important when creating an instance that the correct permissions are set. At a minimum, you must ensure you have:
+
+[source,text]
+--------------------------------------------------
+service_account_scope=compute-rw
+--------------------------------------------------
+
+Failing to set this will result in unauthorized messages when starting Elasticsearch.
+See [Machine Permissions](#machine-permissions).
+==============================================
+
+
+Once connected, install Elasticsearch:
+
+[source,sh]
+--------------------------------------------------
+sudo apt-get update
+
+# Download Elasticsearch
+wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.0.0.deb
+
+# Prepare Java installation
+sudo apt-get install java7-runtime-headless
+
+# Prepare Elasticsearch installation
+sudo dpkg -i elasticsearch-2.0.0.deb
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-long-install-plugin]]
+===== Install elasticsearch cloud gce plugin
+
+Install the plugin:
+
+[source,sh]
+--------------------------------------------------
+# Use Plugin Manager to install it
+sudo bin/plugin install cloud-gce
+--------------------------------------------------
+
+Open the `elasticsearch.yml` file:
+
+[source,sh]
+--------------------------------------------------
+sudo vi /etc/elasticsearch/elasticsearch.yml
+--------------------------------------------------
+
+And add the following lines:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+--------------------------------------------------
+
+
+Start elasticsearch:
+
+[source,sh]
+--------------------------------------------------
+sudo /etc/init.d/elasticsearch start
+--------------------------------------------------
+
+If anything goes wrong, you should check logs:
+
+[source,sh]
+--------------------------------------------------
+tail -f /var/log/elasticsearch/elasticsearch.log
+--------------------------------------------------
+
+If needed, you can change log level to `TRACE` by opening `logging.yml`:
+
+[source,sh]
+--------------------------------------------------
+sudo vi /etc/elasticsearch/logging.yml
+--------------------------------------------------
+
+and adding the following line:
+
+[source,yaml]
+--------------------------------------------------
+# discovery
+discovery.gce: TRACE
+--------------------------------------------------
+
+
+
+[[cloud-gce-usage-discovery-cloning]]
+==== Cloning your existing machine
+
+In order to build a cluster on many nodes, you can clone your configured instance to new nodes.
+You won't have to reinstall everything!
+
+First create an image of your running instance and upload it to Google Cloud Storage:
+
+[source,sh]
+--------------------------------------------------
+# Create an image of yur current instance
+sudo /usr/bin/gcimagebundle -d /dev/sda -o /tmp/
+
+# An image has been created in `/tmp` directory:
+ls /tmp
+e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
+
+# Upload your image to Google Cloud Storage:
+# Create a bucket to hold your image, let's say `esimage`:
+gsutil mb gs://esimage
+
+# Copy your image to this bucket:
+gsutil cp /tmp/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz gs://esimage
+
+# Then add your image to images collection:
+gcutil addimage elasticsearch-1-2-1 gs://esimage/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
+
+# If the previous command did not work for you, logout from your instance
+# and launch the same command from your local machine.
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-start-new-instances]]
+===== Start new instances
+
+As you have now an image, you can create as many instances as you need:
+
+[source,sh]
+--------------------------------------------------
+# Just change node name (here myesnode2)
+gcutil addinstance --image=elasticsearch-1-2-1 myesnode2
+
+# If you want to provide all details directly, you can use:
+gcutil addinstance --image=elasticsearch-1-2-1 \
+       --kernel=projects/google/global/kernels/gce-v20130603 myesnode2 \
+       --zone europe-west1-a --machine_type f1-micro --service_account_scope=compute-rw \
+       --persistent_boot_disk
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-remove-instance]]
+===== Remove an instance (aka shut it down)
+
+You can use https://cloud.google.com/console[Google Cloud Console] or CLI to manage your instances:
+
+[source,sh]
+--------------------------------------------------
+# Stopping and removing instances
+gcutil deleteinstance myesnode1 myesnode2 \
+       --zone=europe-west1-a
+
+# Consider removing disk as well if you don't need them anymore
+gcutil deletedisk boot-myesnode1 boot-myesnode2  \
+       --zone=europe-west1-a
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-zones]]
+==== Using GCE zones
+
+`cloud.gce.zone` helps to retrieve instances running in a given zone. It should be one of the
+https://developers.google.com/compute/docs/zones#available[GCE supported zones].
+
+The GCE discovery can support multi zones although you need to be aware of network latency between zones.
+To enable discovery across more than one zone, just enter add your zone list to `cloud.gce.zone` setting:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: <your-google-project-id>
+      zone: ["<your-zone1>", "<your-zone2>"]
+discovery:
+      type: gce
+--------------------------------------------------
+
+
+
+[[cloud-gce-usage-discovery-tags]]
+==== Filtering by tags
+
+The GCE discovery can also filter machines to include in the cluster based on tags using `discovery.gce.tags` settings.
+For example, setting `discovery.gce.tags` to `dev` will only filter instances having a tag set to `dev`. Several tags
+set will require all of those tags to be set for the instance to be included.
+
+One practical use for tag filtering is when an GCE cluster contains many nodes that are not running
+elasticsearch. In this case (particularly with high ping_timeout values) there is a risk that a new node's discovery
+phase will end before it has found the cluster (which will result in it declaring itself master of a new cluster
+with the same name - highly undesirable). Adding tag on elasticsearch GCE nodes and then filtering by that
+tag will resolve this issue.
+
+Add your tag when building the new instance:
+
+[source,sh]
+--------------------------------------------------
+gcutil --project=es-cloud addinstance myesnode1 \
+       --service_account_scope=compute-rw \
+       --persistent_boot_disk \
+       --tags=elasticsearch,dev
+--------------------------------------------------
+
+Then, define it in `elasticsearch.yml`:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+      gce:
+            tags: elasticsearch, dev
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-port]]
+==== Changing default transport port
+
+By default, elasticsearch GCE plugin assumes that you run elasticsearch on 9300 default port.
+But you can specify the port value elasticsearch is meant to use using google compute engine metadata `es_port`:
+
+[[cloud-gce-usage-discovery-port-create]]
+===== When creating instance
+
+Add `--metadata=es_port:9301` option:
+
+[source,sh]
+--------------------------------------------------
+# when creating first instance
+gcutil addinstance myesnode1 \
+       --service_account_scope=compute-rw,storage-full \
+       --persistent_boot_disk \
+       --metadata=es_port:9301
+
+# when creating an instance from an image
+gcutil addinstance --image=elasticsearch-1-0-0-RC1 \
+       --kernel=projects/google/global/kernels/gce-v20130603 myesnode2 \
+       --zone europe-west1-a --machine_type f1-micro --service_account_scope=compute-rw \
+       --persistent_boot_disk --metadata=es_port:9301
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-port-run]]
+===== On a running instance
+
+[source,sh]
+--------------------------------------------------
+# Get metadata fingerprint
+gcutil getinstance myesnode1 --zone=europe-west1-a
++------------------------+---------------------+
+|        property        | value               |
++------------------------+---------------------+
+| metadata               |                     |
+| fingerprint            | 42WmSpB8rSM=        |
++------------------------+---------------------+
+
+# Use that fingerprint
+gcutil setinstancemetadata myesnode1 \
+       --zone=europe-west1-a \
+       --metadata=es_port:9301 \
+       --fingerprint=42WmSpB8rSM=
+--------------------------------------------------
+
+
+[[cloud-gce-usage-discovery-tips]]
+==== GCE Tips
+
+[[cloud-gce-usage-discovery-tips-projectid]]
+===== Store project id locally
+
+If you don't want to repeat the project id each time, you can save it in `~/.gcutil.flags` file using:
+
+[source,sh]
+--------------------------------------------------
+gcutil getproject --project=es-cloud --cache_flag_values
+--------------------------------------------------
+
+`~/.gcutil.flags` file now contains:
+
+[source,text]
+--------------------------------------------------
+--project=es-cloud
+--------------------------------------------------
+
+[[cloud-gce-usage-discovery-tips-permissions]]
+===== Machine Permissions
+
+If you have created a machine without the correct permissions, you will see `403 unauthorized` error messages. The only
+way to alter these permissions is to delete the instance (NOT THE DISK). Then create another with the correct permissions.
+
+Creating machines with gcutil::
++
+--
+Ensure the following flags are set:
+
+[source,text]
+--------------------------------------------------
+--service_account_scope=compute-rw
+--------------------------------------------------
+--
+
+Creating with console (web)::
++
+--
+When creating an instance using the web portal, click _Show advanced options_.
+
+At the bottom of the page, under `PROJECT ACCESS`, choose `>> Compute >> Read Write`.
+--
+
+Creating with knife google::
++
+--
+Set the service account scopes when creating the machine:
+
+[source,sh]
+--------------------------------------------------
+knife google server create www1 \
+    -m n1-standard-1 \
+    -I debian-7-wheezy-v20131120 \
+    -Z us-central1-a \
+    -i ~/.ssh/id_rsa \
+    -x jdoe \
+    --gce-service-account-scopes https://www.googleapis.com/auth/compute.full_control
+--------------------------------------------------
+
+Or, you may use the alias:
+
+[source,sh]
+--------------------------------------------------
+    --gce-service-account-scopes compute-rw
+--------------------------------------------------
+--
+
+[[cloud-gce-usage-discovery-testing]]
+==== Testing GCE
+
+Integrations tests in this plugin require working GCE configuration and
+therefore disabled by default. To enable tests prepare a config file
+elasticsearch.yml with the following content:
+
+[source,yaml]
+--------------------------------------------------
+cloud:
+  gce:
+      project_id: es-cloud
+      zone: europe-west1-a
+discovery:
+      type: gce
+--------------------------------------------------
+
+Replaces `project_id` and `zone` with your settings.
+
+To run test:
+
+[source,sh]
+--------------------------------------------------
+mvn -Dtests.gce=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
+--------------------------------------------------
diff --git a/docs/plugins/delete-by-query.asciidoc b/docs/plugins/delete-by-query.asciidoc
index cd4ee89..a422cc3 100644
--- a/docs/plugins/delete-by-query.asciidoc
+++ b/docs/plugins/delete-by-query.asciidoc
@@ -1,89 +1,107 @@
 [[plugins-delete-by-query]]
-== Delete By Query Plugin
+=== Delete By Query Plugin
 
-The delete by query plugin adds support for deleting all of the documents
+The delete-by-query plugin adds support for deleting all of the documents
 (from one or more indices) which match the specified query. It is a
 replacement for the problematic _delete-by-query_ functionality which has been
 removed from Elasticsearch core.
 
-Internally, it uses the <<scroll-scan, Scan/Scroll>> and <<docs-bulk, Bulk>>
-APIs to delete documents in an efficient and safe manner. It is slower than
-the old _delete-by-query_ functionality, but fixes the problems with the
-previous implementation.
+Internally, it uses the {ref}/search-request-scroll.html#scroll-scan[Scan/Scroll]
+and {ref}/docs-bulk.html[Bulk] APIs to delete documents in an efficient and
+safe manner. It is slower than the old _delete-by-query_ functionality, but
+fixes the problems with the previous implementation.
 
-TIP: Queries which match large numbers of documents may run for a long time,
+To understand more about why we removed delete-by-query from core and about
+the semantics of the new implementation, see
+<<delete-by-query-plugin-reason>>.
+
+[TIP]
+============================================
+Queries which match large numbers of documents may run for a long time,
 as every document has to be deleted individually.  Don't use _delete-by-query_
 to clean out all or most documents in an index.  Rather create a new index and
 perhaps reindex the documents you want to keep.
+============================================
 
-=== Installation
+[float]
+==== Installation
 
 This plugin can be installed using the plugin manager:
 
 [source,sh]
 ----------------------------------------------------------------
-bin/plugin install elasticsearch/elasticsearch-delete-by-query
+sudo bin/plugin install delete-by-query
 ----------------------------------------------------------------
 
 The plugin must be installed on every node in the cluster, and each node must
 be restarted after installation.
 
-=== Removal
+[float]
+==== Removal
 
 The plugin can be removed with the following command:
 
 [source,sh]
 ----------------------------------------------------------------
-bin/plugin remove elasticsearch/elasticsearch-delete-by-query
+sudo bin/plugin remove delete-by-query
 ----------------------------------------------------------------
 
 The node must be stopped before removing the plugin.
 
-=== Usage
+[[delete-by-query-usage]]
+==== Using Delete-by-Query
 
 The query can either be provided using a simple query string as
 a parameter:
 
 [source,shell]
 --------------------------------------------------
-curl -XDELETE 'http://localhost:9200/twitter/tweet/_query?q=user:kimchy'
+DELETE /twitter/tweet/_query?q=user:kimchy
 --------------------------------------------------
+// AUTOSENSE
 
-or using the <<query-dsl,Query DSL>> defined within the request body:
+or using the {ref}/query-dsl.html[Query DSL] defined within the request body:
 
 [source,js]
 --------------------------------------------------
-curl -XDELETE 'http://localhost:9200/twitter/tweet/_query' -d '{
-    "query" : { <1>
-        "term" : { "user" : "kimchy" }
+DELETE /twitter/tweet/_query
+{
+  "query": { <1>
+    "term": {
+      "user": "kimchy"
     }
+  }
 }
-'
 --------------------------------------------------
+// AUTOSENSE
+
 <1> The query must be passed as a value to the `query` key, in the same way as
-the <<search-search,search api>>.
+the {ref}/search-search.html[search api].
 
 Both of the above examples end up doing the same thing, which is to delete all
 tweets from the twitter index for the user `kimchy`.
 
-Delete-by-query supports deletion across <<search-multi-index-type,multiple indices and multiple types>>.
+Delete-by-query supports deletion across
+{ref}/search-search.html#search-multi-index-type[multiple indices and multiple types].
 
-==== Query-string parameters
+[float]
+=== Query-string parameters
 
 The following query string parameters are supported:
 
 `q`::
 
-Instead of using the <<query-dsl,Query DSL>> to pass a `query` in the request
+Instead of using the {ref}/query-dsl.html[Query DSL] to pass a `query` in the request
 body, you can use the `q` query string parameter to  specify a query using
-<<query-string-syntax,`query_string` syntax>>. In this case, the following
-additional parameters are supported: `df`, `analyzer`, `default_operator`,
- `lowercase_expanded_terms`, `analyze_wildcard` and `lenient`.
-See <<search-uri-request>> for details.
+{ref}/query-dsl-query-string-query.html#query-string-syntax[`query_string` syntax].
+In this case, the following additional parameters are supported: `df`,
+`analyzer`, `default_operator`,  `lowercase_expanded_terms`,
+`analyze_wildcard` and `lenient`.
+See {ref}/search-uri-request.html[URI search request] for details.
 
 `size`::
 
-The number of hits returned *per shard* by the <<scroll-scan,scroll/scan>>
+The number of hits returned *per shard* by the {ref}/search-request-scroll.html#scroll-scan[scan]
 request.  Defaults to 10.  May also be specified in the request body.
 
 `timeout`::
@@ -97,11 +115,12 @@ A comma separated list of routing values to control which shards the delete by
 query request should be executed on.
 
 When using the `q` parameter, the following additional parameters are
-supported (as explained in <<search-uri-request>>): `df`, `analyzer`,
+supported (as explained in {ref}/search-uri-request.html[URI search request]): `df`, `analyzer`,
 `default_operator`.
 
 
-==== Response body
+[float]
+=== Response body
 
 The JSON response looks like this:
 
@@ -129,8 +148,9 @@ The JSON response looks like this:
 --------------------------------------------------
 
 Internally, the query is used to execute an initial
-<<scroll-scan,scroll/scan>> request. As hits are pulled from the scroll API,
-they are passed to the <<bulk,Bulk API>> for deletion.
+{ref}/search-request-scroll.html#scroll-scan[scroll/scan] request. As hits are
+pulled from the scroll API, they are passed to the {ref}/docs-bulk.html[Bulk
+API] for deletion.
 
 IMPORTANT: Delete by query will only delete the version of the document that
 was visible to search at the time the request was executed.  Any documents
@@ -161,3 +181,90 @@ The number of documents that failed to be deleted for the given index. A
 document may fail to be deleted if it has been updated to a new version by
 another process, or if the shard containing the document has gone missing due
 to hardware failure, for example.
+
+[[delete-by-query-plugin-reason]]
+==== Why Delete-By-Query is a plugin
+
+The old delete-by-query API in Elasticsearch 1.x was fast but problematic. We
+decided to remove the feature from Elasticsearch for these reasons:
+
+Forward compatibility::
+
+    The old implementation wrote a delete-by-query request, including the
+    query, to the transaction log.  This meant that, when upgrading to a new
+    version, old unsupported queries which cannot be executed might exist in
+    the translog, thus causing data corruption.
+
+Consistency and correctness::
+
+    The old implementation executed the query and deleted all matching docs on
+    the primary first.  It then repeated this procedure on each replica shard.
+    There was no guarantee that the queries on the primary and the replicas
+    matched the same document, so it was quite possible to end up with
+    different documents on each shard copy.
+
+Resiliency::
+
+    The old implementation could cause out-of-memory exceptions, merge storms,
+    and dramatic slow downs if used incorrectly.
+
+[float]
+=== New delete-by-query implementation
+
+The new implementation, provided by this plugin, is built internally
+using  {ref}/search-request-scroll.html#scroll-scan[scan and scroll] to return
+the document IDs and versions of all the documents that need to be deleted.
+It then uses  the {ref}/docs-bulk.html[`bulk` API] to do the actual deletion.
+
+This can have performance as well as visibility implications. Delete-by-query
+now has the following semantics:
+
+non-atomic::
+
+    A delete-by-query may fail at any time while some documents matching the
+    query have already been deleted.
+
+try-once::
+
+    A delete-by-query may fail at any time and will not retry it's execution.
+    All retry logic is left to the user.
+
+syntactic sugar::
+
+    A delete-by-query is equivalent to a scan/scroll search and corresponding
+    bulk-deletes by ID.
+
+point-in-time::
+
+    A delete-by-query will only delete the documents that are visible at the
+    point in time the delete-by-query was started, equivalent to the
+    scan/scroll API.
+
+consistent::
+
+    A delete-by-query will yield consistent results across all replicas of a
+    shard.
+
+forward-compatible::
+
+    A delete-by-query will only send IDs to the shards as deletes such that no
+    queries are stored in the transaction logs that might not be supported in
+    the future.
+
+visibility::
+
+    The effect of a delete-by-query request will not be visible to search
+    until the user refreshes the index, or the index is refreshed
+    automatically.
+
+The new implementation suffers from two issues, which is why we decided to
+move the functionality to a plugin instead of replacing the feautre in core:
+
+* It is not as fast as the previous implementation. For most use cases, this
+  difference should not be noticeable but users running delete-by-query on
+  many matching documents may be affected.
+
+* There is currently no way to monitor or cancel a running delete-by-query
+  request, except for the `timeout` parameter.
+
+We have plans to solve both of these issues in a later version of Elasticsearch.
\ No newline at end of file
diff --git a/docs/plugins/discovery.asciidoc b/docs/plugins/discovery.asciidoc
new file mode 100644
index 0000000..723d900
--- /dev/null
+++ b/docs/plugins/discovery.asciidoc
@@ -0,0 +1,45 @@
+[[discovery]]
+== Discovery Plugins
+
+Discovery plugins extend Elasticsearch by adding new discovery mechanisms that
+can be used instead of {ref}/modules-discovery-zen.html[Zen Discovery].
+
+[float]
+==== Core discovery plugins
+
+The core discovery plugins are:
+
+<<cloud-aws,AWS Cloud>>::
+
+The Amazon Web Service (AWS) Cloud plugin uses the
+https://github.com/aws/aws-sdk-java[AWS API] for unicast discovery, and adds
+support for using S3 as a repository for
+{ref}/modules-snapshots.html[Snapshot/Restore].
+
+<<cloud-azure,Azure Cloud>>::
+
+The Azure Cloud plugin uses the Azure API for unicast discovery, and adds
+support for using Azure as a repository for
+{ref}/modules-snapshots.html[Snapshot/Restore].
+
+<<cloud-gce,GCE Cloud>>::
+
+The Google Compute Engine Cloud plugin uses the GCE API for unicast discovery.
+
+[float]
+==== Community contributed discovery plugins
+
+A number of discovery plugins have been contributed by our community:
+
+* https://github.com/grantr/elasticsearch-srv-discovery[DNS SRV Discovery Plugin] (by Grant Rodgers)
+* https://github.com/shikhar/eskka[eskka Discovery Plugin] (by Shikhar Bhushan)
+* https://github.com/grmblfrz/elasticsearch-zookeeper[ZooKeeper Discovery Plugin] (by Sonian Inc.)
+
+include::cloud-aws.asciidoc[]
+
+include::cloud-azure.asciidoc[]
+
+include::cloud-gce.asciidoc[]
+
+
+
diff --git a/docs/plugins/index.asciidoc b/docs/plugins/index.asciidoc
new file mode 100644
index 0000000..2ebe5d2
--- /dev/null
+++ b/docs/plugins/index.asciidoc
@@ -0,0 +1,65 @@
+= Elasticsearch Plugins and Integrations
+
+:ref: https://www.elastic.co/guide/en/elasticsearch/reference/master
+:guide: https://www.elastic.co/guide
+
+[[intro]]
+== Introduction to plugins
+
+Plugins are a way to enhance the core Elasticsearch functionality in a custom
+manner. They range from adding custom mapping types, custom analyzers, native
+scripts, custom discovery and more.
+
+There are three types of plugins:
+
+Java plugins::
+
+    These plugins contain only JAR files, and must be installed on every node
+    in the cluster.  After installation, each node must be restarted before
+    the plugin becomes visible.
+
+Site plugins::
++
+--
+
+These plugins contain static web content like Javascript, HTML, and CSS files,
+that can be served directly from Elasticsearch. Site plugins may only need to
+be installed on one node, and do not require a restart to become visible. The
+content of site plugins is accessible via a URL like:
+
+    http://yournode:9200/_plugin/[plugin name]
+
+--
+
+Mixed plugins::
+
+    Mixed plugins contain both JAR files and web content.
+
+For advice on writing your own plugin, see <<plugin-authors>>.
+
+include::plugin-script.asciidoc[]
+
+include::api.asciidoc[]
+
+include::alerting.asciidoc[]
+
+include::analysis.asciidoc[]
+
+include::discovery.asciidoc[]
+
+include::management.asciidoc[]
+
+include::mapper.asciidoc[]
+
+include::scripting.asciidoc[]
+
+include::security.asciidoc[]
+
+include::repository.asciidoc[]
+
+include::transport.asciidoc[]
+
+include::integrations.asciidoc[]
+
+include::authors.asciidoc[]
+
diff --git a/docs/plugins/integrations.asciidoc b/docs/plugins/integrations.asciidoc
new file mode 100644
index 0000000..0cb4648
--- /dev/null
+++ b/docs/plugins/integrations.asciidoc
@@ -0,0 +1,220 @@
+[[integrations]]
+
+== Integrations
+
+Integrations are not plugins, instead they are external tools or modules which
+make it easier to work with Elasticsearch.
+
+[float]
+[[cms-integrations]]
+=== CMS integrations
+
+[float]
+==== Supported by the community:
+
+* http://drupal.org/project/search_api_elasticsearch[Drupal]:
+  Drupal Elasticsearch integration via Search API.
+
+* https://drupal.org/project/elasticsearch_connector[Drupal]:
+  Drupal Elasticsearch integration.
+
+* http://searchbox-io.github.com/wp-elasticsearch/[Wp-Elasticsearch]:
+  Elasticsearch WordPress Plugin
+
+* https://github.com/wallmanderco/elasticsearch-indexer[Elasticsearch Indexer]:
+  Elasticsearch WordPress Plugin
+
+* https://doc.tiki.org/Elasticsearch[Tiki Wiki CMS Groupware]:
+  Tiki has native support for Elasticsearch. This provides faster & better
+  search (facets, etc), along with some Natural Language Processing features
+  (ex.: More like this)
+
+
+[float]
+[[data-integrations]]
+=== Data import/export and validation
+
+NOTE: Rivers were used to import data from external systems into
+Elasticsearch, but they are no longer supported in Elasticsearch 2.0.
+
+[float]
+==== Supported by the community:
+
+* https://github.com/jprante/elasticsearch-jdbc[JDBC importer]:
+  The Java Database Connection (JDBC) importer allows to fetch data from JDBC sources for indexing into Elasticsearch (by Jörg Prante)
+
+* https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer[Kafka Standalone Consumer]:
+  Easily Scaleable & Extendable, Kafka Standalone Consumer that will read the messages from Kafka, processes and index them in ElasticSearch
+
+* https://github.com/ozlerhakan/mongolastic[Mongolastic]:
+  A tool that clone data from ElasticSearch to MongoDB and vice versa
+
+* https://github.com/Aconex/scrutineer[Scrutineer]:
+  A high performance consistency checker to compare what you've indexed
+  with your source of truth content (e.g. DB)
+
+
+[float]
+[[deployment]]
+=== Deployment
+
+[float]
+==== Supported by Elasticsearch:
+
+* https://github.com/elasticsearch/puppet-elasticsearch[Puppet]:
+  Elasticsearch puppet module.
+
+[float]
+==== Supported by the community:
+
+* http://github.com/elasticsearch/cookbook-elasticsearch[Chef]:
+  Chef cookbook for Elasticsearch
+
+This project appears to have been abandoned:
+
+* https://github.com/medcl/salt-elasticsearch[SaltStack]:
+  SaltStack Module for Elasticsearch
+
+[float]
+[[framework-integrations]]
+=== Framework integrations
+
+[float]
+==== Supported by the community:
+
+* http://www.searchtechnologies.com/aspire-for-elasticsearch[Aspire for Elasticsearch]:
+  Aspire, from Search Technologies, is a powerful connector and processing
+  framework designed for unstructured data. It has connectors to internal and
+  external repositories including SharePoint, Documentum, Jive, RDB, file
+  systems, websites and more, and can transform and normalize this data before
+  indexing in Elasticsearch.
+
+* https://camel.apache.org/elasticsearch.html[Apache Camel Integration]:
+  An Apache camel component to integrate elasticsearch
+
+* https://metacpan.org/release/Catmandu-Store-ElasticSearch[Catmanadu]:
+  An Elasticsearch backend for the Catmandu framework.
+
+* https://github.com/tlrx/elasticsearch-test[elasticsearch-test]:
+  Elasticsearch Java annotations for unit testing with
+  http://www.junit.org/[JUnit]
+
+* https://github.com/FriendsOfSymfony/FOSElasticaBundle[FOSElasticaBundle]:
+  Symfony2 Bundle wrapping Elastica.
+
+* http://grails.org/plugin/elasticsearch[Grails]:
+  Elasticsearch Grails plugin.
+
+* http://haystacksearch.org/[Haystack]:
+  Modular search for Django
+
+* https://github.com/cleverage/play2-elasticsearch[play2-elasticsearch]:
+  Elasticsearch module for Play Framework 2.x
+
+* https://github.com/spring-projects/spring-data-elasticsearch[Spring Data Elasticsearch]:
+  Spring Data implementation for Elasticsearch
+
+* https://github.com/dadoonet/spring-elasticsearch[Spring Elasticsearch]:
+  Spring Factory for Elasticsearch
+
+* https://github.com/twitter/storehaus[Twitter Storehaus]:
+  Thin asynchronous Scala client for Storehaus.
+
+These projects appear to have been abandoned:
+
+* https://metacpan.org/module/Catalyst::Model::Search::Elasticsearch[Catalyst]:
+  Elasticsearch and Catalyst integration.
+
+* http://github.com/aparo/django-elasticsearch[django-elasticsearch]:
+  Django Elasticsearch Backend.
+
+* https://github.com/kzwang/elasticsearch-osem[elasticsearch-osem]:
+  A Java Object Search Engine Mapping (OSEM) for Elasticsearch
+
+* http://geeks.aretotally.in/play-framework-module-elastic-search-distributed-searching-with-json-http-rest-or-java[Play!Framework]:
+  Integrate with Play! Framework Application.
+
+* http://code.google.com/p/terrastore/wiki/Search_Integration[Terrastore Search]:
+  http://code.google.com/p/terrastore/[Terrastore] integration module with elasticsearch.
+
+
+[float]
+[[hadoop-integrations]]
+=== Hadoop integrations
+
+[float]
+==== Supported by Elasticsearch:
+
+* link:/guide/en/elasticsearch/hadoop/current/[es-hadoop]: Elasticsearch real-time
+  search and analytics natively integrated with Hadoop. Supports Map/Reduce,
+  Cascading, Apache Hive, Apache Pig, Apache Spark and Apache Storm.
+
+[float]
+==== Supported by the community:
+
+These projects appear to have been abandoned:
+
+* http://github.com/Aconex/elasticflume[elasticflume]:
+  http://github.com/cloudera/flume[Flume] sink implementation.
+
+
+* https://github.com/infochimps-labs/wonderdog[Wonderdog]:
+  Hadoop bulk loader into elasticsearch.
+
+
+[float]
+[[monitoring-integrations]]
+=== Health and Performance Monitoring
+
+[float]
+==== Supported by the community:
+
+* https://github.com/anchor/nagios-plugin-elasticsearch[check_elasticsearch]:
+  An Elasticsearch availability and performance monitoring plugin for
+  Nagios.
+
+* https://github.com/radu-gheorghe/check-es[check-es]:
+  Nagios/Shinken plugins for checking on elasticsearch
+
+* https://github.com/mattweber/es2graphite[es2graphite]:
+  Send cluster and indices stats and status to Graphite for monitoring and graphing.
+
+
+* https://itunes.apple.com/us/app/elasticocean/id955278030?ls=1&mt=8[ElasticOcean]:
+  Elasticsearch & DigitalOcean iOS Real-Time Monitoring tool to keep an eye on DigitalOcean Droplets or Elasticsearch instances or both of them on-a-go.
+
+* https://github.com/rbramley/Opsview-elasticsearch[opsview-elasticsearch]:
+  Opsview plugin written in Perl for monitoring Elasticsearch
+
+* https://scoutapp.com[Scout]: Provides plugins for monitoring Elasticsearch https://scoutapp.com/plugin_urls/1331-elasticsearch-node-status[nodes], https://scoutapp.com/plugin_urls/1321-elasticsearch-cluster-status[clusters], and https://scoutapp.com/plugin_urls/1341-elasticsearch-index-status[indices].
+
+* http://sematext.com/spm/index.html[SPM for Elasticsearch]:
+  Performance monitoring with live charts showing cluster and node stats, integrated
+  alerts, email reports, etc.
+
+
+[[other-integrations]]
+[float]
+=== Other integrations
+
+[float]
+==== Supported by the community:
+
+* https://github.com/kodcu/pes[Pes]:
+  A pluggable elastic Javascript query DSL builder for Elasticsearch
+
+* https://www.wireshark.org/[Wireshark]:
+  Protocol dissection for Zen discovery, HTTP and the binary protocol
+
+
+These projects appears to have been abandoned:
+
+* http://www.github.com/neogenix/daikon[daikon]:
+  Daikon Elasticsearch CLI
+
+* https://github.com/fullscale/dangle[dangle]:
+  A set of AngularJS directives that provide common visualizations for elasticsearch based on
+  D3.
+* https://github.com/OlegKunitsyn/eslogd[eslogd]:
+  Linux daemon that replicates events to a central Elasticsearch server in real-time
+
diff --git a/docs/plugins/lang-javascript.asciidoc b/docs/plugins/lang-javascript.asciidoc
new file mode 100644
index 0000000..fb699d1
--- /dev/null
+++ b/docs/plugins/lang-javascript.asciidoc
@@ -0,0 +1,193 @@
+[[lang-javascript]]
+=== JavaScript Language Plugin
+
+The JavaScript language plugin enables the use of JavaScript in Elasticsearch
+scripts, via Mozilla's
+https://developer.mozilla.org/en-US/docs/Mozilla/Projects/Rhino[Rhino JavaScript] engine.
+
+[[lang-javascript-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install lang-javascript
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[lang-javascript-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove lang-javascript
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[lang-javascript-usage]]
+==== Using JavaScript in Elasticsearch
+
+Once the plugin has been installed, JavaScript can be used at a scripting
+language by setting the `lang` parameter to `javascript` or `js`.
+
+Scripting is available in many APIs, but we will use an example with the
+`function_score` for demonstration purposes:
+
+[[lang-javascript-inline]]
+[float]
+=== Inline scripts
+
+WARNING: Enabling inline scripting on an unprotected Elasticsearch cluster is dangerous.
+See <<lang-javascript-file>> for a safer option.
+
+If you have enabled {ref}/modules-scripting.html#enable-dynamic-scripting[inline scripts],
+you can use JavaScript as follows:
+
+[source,json]
+----
+DELETE test
+
+PUT test/doc/1
+{
+  "num": 1.0
+}
+
+PUT test/doc/2
+{
+  "num": 2.0
+}
+
+GET test/_search
+{
+  "query": {
+    "function_score": {
+      "script_score": {
+        "script": {
+          "inline": "doc[\"num\"].value * factor",
+          "lang": "javascript",
+          "params": {
+            "factor": 2
+          }
+        }
+      }
+    }
+  }
+}
+----
+// AUTOSENSE
+
+[[lang-javascript-indexed]]
+[float]
+=== Indexed scripts
+
+WARNING: Enabling indexed scripting on an unprotected Elasticsearch cluster is dangerous.
+See <<lang-javascript-file>> for a safer option.
+
+If you have enabled {ref}/modules-scripting.html#enable-dynamic-scripting[indexed scripts],
+you can use JavaScript as follows:
+
+[source,json]
+----
+DELETE test
+
+PUT test/doc/1
+{
+  "num": 1.0
+}
+
+PUT test/doc/2
+{
+  "num": 2.0
+}
+
+POST _scripts/javascript/my_script  <1>
+{
+  "script": "doc[\"num\"].value * factor"
+}
+
+GET test/_search
+{
+  "query": {
+    "function_score": {
+      "script_score": {
+        "script": {
+          "id": "my_script", <2>
+          "lang": "javascript",
+          "params": {
+            "factor": 2
+          }
+        }
+      }
+    }
+  }
+}
+
+----
+// AUTOSENSE
+
+<1> We index the script under the id `my_script`.
+<2> The function score query retrieves the script with id `my_script`.
+
+
+[[lang-javascript-file]]
+[float]
+=== File scripts
+
+You can save your scripts to a file in the `config/scripts/` directory on
+every node. The `.javascript` file suffix identifies the script as containing
+JavaScript:
+
+First, save this file as `config/scripts/my_script.javascript` on every node
+in the cluster:
+
+[source,js]
+----
+doc["num"].value * factor
+----
+
+then use the script as follows:
+
+[source,json]
+----
+DELETE test
+
+PUT test/doc/1
+{
+  "num": 1.0
+}
+
+PUT test/doc/2
+{
+  "num": 2.0
+}
+
+GET test/_search
+{
+  "query": {
+    "function_score": {
+      "script_score": {
+        "script": {
+          "file": "my_script", <1>
+          "lang": "javascript",
+          "params": {
+            "factor": 2
+          }
+        }
+      }
+    }
+  }
+}
+
+----
+// AUTOSENSE
+
+<1> The function score query retrieves the script with filename `my_script.javascript`.
+
diff --git a/docs/plugins/lang-python.asciidoc b/docs/plugins/lang-python.asciidoc
new file mode 100644
index 0000000..956a4b7
--- /dev/null
+++ b/docs/plugins/lang-python.asciidoc
@@ -0,0 +1,192 @@
+[[lang-python]]
+=== Python Language Plugin
+
+The Python language plugin enables the use of Python in Elasticsearch
+scripts, via the http://www.jython.org/[Jython] Java implementation of Python.
+
+[[lang-python-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install lang-python
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[lang-python-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove lang-python
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[lang-python-usage]]
+==== Using Python in Elasticsearch
+
+Once the plugin has been installed, Python can be used at a scripting
+language by setting the `lang` parameter to `python`.
+
+Scripting is available in many APIs, but we will use an example with the
+`function_score` for demonstration purposes:
+
+[[lang-python-inline]]
+[float]
+=== Inline scripts
+
+WARNING: Enabling inline scripting on an unprotected Elasticsearch cluster is dangerous.
+See <<lang-python-file>> for a safer option.
+
+If you have enabled {ref}/modules-scripting.html#enable-dynamic-scripting[inline scripts],
+you can use Python as follows:
+
+[source,json]
+----
+DELETE test
+
+PUT test/doc/1
+{
+  "num": 1.0
+}
+
+PUT test/doc/2
+{
+  "num": 2.0
+}
+
+GET test/_search
+{
+  "query": {
+    "function_score": {
+      "script_score": {
+        "script": {
+          "inline": "doc[\"num\"].value * factor",
+          "lang": "python",
+          "params": {
+            "factor": 2
+          }
+        }
+      }
+    }
+  }
+}
+----
+// AUTOSENSE
+
+[[lang-python-indexed]]
+[float]
+=== Indexed scripts
+
+WARNING: Enabling indexed scripting on an unprotected Elasticsearch cluster is dangerous.
+See <<lang-python-file>> for a safer option.
+
+If you have enabled {ref}/modules-scripting.html#enable-dynamic-scripting[indexed scripts],
+you can use Python as follows:
+
+[source,json]
+----
+DELETE test
+
+PUT test/doc/1
+{
+  "num": 1.0
+}
+
+PUT test/doc/2
+{
+  "num": 2.0
+}
+
+POST _scripts/python/my_script  <1>
+{
+  "script": "doc[\"num\"].value * factor"
+}
+
+GET test/_search
+{
+  "query": {
+    "function_score": {
+      "script_score": {
+        "script": {
+          "id": "my_script", <2>
+          "lang": "python",
+          "params": {
+            "factor": 2
+          }
+        }
+      }
+    }
+  }
+}
+
+----
+// AUTOSENSE
+
+<1> We index the script under the id `my_script`.
+<2> The function score query retrieves the script with id `my_script`.
+
+
+[[lang-python-file]]
+[float]
+=== File scripts
+
+You can save your scripts to a file in the `config/scripts/` directory on
+every node. The `.python` file suffix identifies the script as containing
+Python:
+
+First, save this file as `config/scripts/my_script.python` on every node
+in the cluster:
+
+[source,python]
+----
+doc["num"].value * factor
+----
+
+then use the script as follows:
+
+[source,json]
+----
+DELETE test
+
+PUT test/doc/1
+{
+  "num": 1.0
+}
+
+PUT test/doc/2
+{
+  "num": 2.0
+}
+
+GET test/_search
+{
+  "query": {
+    "function_score": {
+      "script_score": {
+        "script": {
+          "file": "my_script", <1>
+          "lang": "python",
+          "params": {
+            "factor": 2
+          }
+        }
+      }
+    }
+  }
+}
+
+----
+// AUTOSENSE
+
+<1> The function score query retrieves the script with filename `my_script.python`.
+
diff --git a/docs/plugins/management.asciidoc b/docs/plugins/management.asciidoc
new file mode 100644
index 0000000..7c65309
--- /dev/null
+++ b/docs/plugins/management.asciidoc
@@ -0,0 +1,46 @@
+[[management]]
+== Management and Site Plugins
+
+Management and site plugins offer UIs for managing and interacting with
+Elasticsearch.
+
+[float]
+=== Core management plugins
+
+The core management plugins are:
+
+link:/products/marvel[Marvel]::
+
+Marvel is a management and monitoring product for Elasticsearch. Marvel
+aggregates cluster wide statistics and events and offers a single interface to
+view and analyze them. Marvel is free for development use but requires a
+license to run in production.
+
+https://github.com/elastic/elasticsearch-migration[Migration]::
+
+This plugin will help you to check whether you can upgrade directly to
+Elasticsearch version 2.x, or whether you need to make changes to your data
+before doing so. It will run on Elasticsearch versions 0.90.x to 1.x.
+
+[float]
+=== Community contributed management and site plugins
+
+A number of plugins have been contributed by our community:
+
+* https://github.com/lukas-vlcek/bigdesk[BigDesk Plugin] (by Lukáš Vlček)
+
+* https://github.com/spinscale/elasticsearch-graphite-plugin[Elasticsearch Graphite Plugin]:
+  Regularly updates a graphite host with indices stats and nodes stats (by Alexander Reelsen)
+
+* https://github.com/mobz/elasticsearch-head[Elasticsearch Head Plugin] (by Ben Birch)
+* https://github.com/royrusso/elasticsearch-HQ[Elasticsearch HQ] (by Roy Russo)
+* https://github.com/andrewvc/elastic-hammer[Hammer Plugin] (by Andrew Cholakian)
+* https://github.com/polyfractal/elasticsearch-inquisitor[Inquisitor Plugin] (by Zachary Tong)
+* https://github.com/lmenezes/elasticsearch-kopf[Kopf Plugin] (by lmenezes)
+
+These community plugins appear to have been abandoned:
+
+* https://github.com/karmi/elasticsearch-paramedic[Paramedic Plugin] (by Karel Minařík)
+* https://github.com/polyfractal/elasticsearch-segmentspy[SegmentSpy Plugin] (by Zachary Tong)
+* https://github.com/xyu/elasticsearch-whatson[Whatson Plugin] (by Xiao Yu)
+
diff --git a/docs/plugins/mapper-size.asciidoc b/docs/plugins/mapper-size.asciidoc
new file mode 100644
index 0000000..51f0c39
--- /dev/null
+++ b/docs/plugins/mapper-size.asciidoc
@@ -0,0 +1,108 @@
+[[mapper-size]]
+=== Mapper Size Plugin
+
+The mapper-size plugin provides the `_size` meta field which, when enabled,
+indexes the size in bytes of the original
+{ref}/mapping-source-field.html[`_source`] field.
+
+[[mapper-size-install]]
+[float]
+==== Installation
+
+This plugin can be installed using the plugin manager:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin install mapper-size
+----------------------------------------------------------------
+
+The plugin must be installed on every node in the cluster, and each node must
+be restarted after installation.
+
+[[mapper-size-remove]]
+[float]
+==== Removal
+
+The plugin can be removed with the following command:
+
+[source,sh]
+----------------------------------------------------------------
+sudo bin/plugin remove mapper-size
+----------------------------------------------------------------
+
+The node must be stopped before removing the plugin.
+
+[[mapper-size-usage]]
+==== Using the `_size` field
+
+In order to enable the `_size` field, set the mapping as follows:
+
+[source,js]
+--------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "_size": {
+        "enabled": true
+      }
+    }
+  }
+}
+--------------------------
+// AUTOSENSE
+
+The value of the `_size` field is accessible in queries, aggregations, scripts,
+and when sorting:
+
+[source,js]
+--------------------------
+# Example documents
+PUT my_index/my_type/1
+{
+  "text": "This is a document"
+}
+
+PUT my_index/my_type/2
+{
+  "text": "This is another document"
+}
+
+GET my_index/_search
+{
+  "query": {
+    "range": {
+      "_size": { <1>
+        "gt": 10
+      }
+    }
+  },
+  "aggs": {
+    "Sizes": {
+      "terms": {
+        "field": "_size", <2>
+        "size": 10
+      }
+    }
+  },
+  "sort": [
+    {
+      "_size": { <3>
+        "order": "desc"
+      }
+    }
+  ],
+  "script_fields": {
+    "Size": {
+      "script": "doc['_size']"  <4>
+    }
+  }
+}
+--------------------------
+// AUTOSENSE
+
+<1> Querying on the `_size` field
+<2> Aggregating on the `_size` field
+<3> Sorting on the `_size` field
+<4> Accessing the `_size` field in scripts (inline scripts must be modules-scripting.html#enable-dynamic-scripting[enabled] for this example to work)
+
diff --git a/docs/plugins/mapper.asciidoc b/docs/plugins/mapper.asciidoc
new file mode 100644
index 0000000..1d9996d
--- /dev/null
+++ b/docs/plugins/mapper.asciidoc
@@ -0,0 +1,18 @@
+[[mapper]]
+== Mapper Plugins
+
+Mapper plugins allow new field datatypes to be added to Elasticsearch.
+
+[float]
+=== Core mapper plugins
+
+The core mapper plugins are:
+
+<<mapper-size>>::
+
+The mapper-size plugin provides the `_size` meta field which, when enabled,
+indexes the size in bytes of the original
+{ref}/mapping-source-field.html[`_source`] field.
+
+include::mapper-size.asciidoc[]
+
diff --git a/docs/plugins/plugin-script.asciidoc b/docs/plugins/plugin-script.asciidoc
new file mode 100644
index 0000000..ce6bbf6
--- /dev/null
+++ b/docs/plugins/plugin-script.asciidoc
@@ -0,0 +1,240 @@
+[[plugin-management]]
+== Plugin Management
+
+The `plugin` script is used to install, list, and remove plugins. It is
+located in the `$ES_HOME/bin` directory by default but it may be in a
+{ref}/setup-dir-layout.html[different location] if you installed Elasticsearch
+with an RPM or deb package.
+
+Run the following command to get usage instructions:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin -h
+-----------------------------------
+
+[[installation]]
+=== Installing Plugins
+
+The documentation for each plugin usually includes specific installation
+instructions for that plugin, but below we document the various available
+options:
+
+[float]
+=== Core Elasticsearch plugins
+
+Core Elasticsearch plugins can be installed as follows:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin install [plugin_name]
+-----------------------------------
+
+For instance, to install the core <<analysis-icu,ICU plugin>>, just run the
+following command:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin install analysis-icu
+-----------------------------------
+
+This command will install the version of the plugin that matches your
+Elasticsearch version.
+
+[float]
+=== Community and non-core plugins
+
+Non-core plugins provided by Elasticsearch, or plugins provided by the
+community, can be installed from `download.elastic.co`, from Maven (Central
+and Sonatype), or from GitHub.  In this case, the command is as follows:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin install [org]/[user|component]/[version]
+-----------------------------------
+
+For instance, to install the https://github.com/lmenezes/elasticsearch-kopf[Kopf]
+plugin from GitHub, run one of the following commands:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin install lmenezes/elasticsearch-kopf <1>
+sudo bin/plugin install lmenezes/elasticsearch-kopf/1.x <2>
+-----------------------------------
+<1> Installs the latest version from GitHub.
+<2> Installs the 1.x version from GitHub.
+
+When installing from Maven Central/Sonatype, `[org]` should be replaced by
+the artifact `groupId`, and `[user|component]` by the `artifactId`.  For
+instance, to install the
+https://github.com/elastic/elasticsearch-mapper-attachments[mapper attachment]
+plugin from Sonatype, run:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin install org.elasticsearch/elasticsearch-mapper-attachments/2.6.0 <1>
+-----------------------------------
+<1> When installing from `download.elastic.co` or from Maven Central/Sonatype, the
+    version is required.
+
+[float]
+=== Custom URL or file system
+
+A plugin can also be downloaded directly from a custom location by specifying the URL:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin install [plugin-name] --url [url] <1>
+-----------------------------------
+<1> Both the URL and the plugin name must be specified.
+
+For instance, to install a plugin from your local file system, you could run:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin install my_plugin --url file:/path/to/plugin.zip
+-----------------------------------
+
+[[listing-removing]]
+=== Listing and Removing Installed Plugins
+
+[float]
+=== Listing plugins
+
+A list of the currently loaded plugins can be retrieved with the `list` option:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin list
+-----------------------------------
+
+Alternatively, use the {ref}/cluster-nodes-info.html[node-info API] to find
+out which plugins are installed on each node in the cluster
+
+[float]
+=== Removing plugins
+
+Plugins can be removed manually, by deleting the appropriate directory under
+`plugins/`, or using the public script:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin remove [pluginname]
+-----------------------------------
+
+=== Other command line parameters
+
+The `plugin` scripts supports a number of other command line parameters:
+
+[float]
+=== Silent/Verbose mode
+
+The `--verbose` parameter outputs more debug information, while the `--silent`
+parameter turns off all output.  The script may return the following exit
+codes:
+
+[horizontal]
+`0`:: everything was OK
+`64`:: unknown command or incorrect option parameter
+`74`:: IO error
+`70`:: any other error
+
+[float]
+=== Custom config directory
+
+If your `elasticsearch.yml` config file is in a custom location, you will need
+to specify the path to the config file when using the `plugin` script.  You
+can do this as follows:
+
+[source,sh]
+---------------------
+sudo bin/plugin -Des.path.conf=/path/to/custom/config/dir install <plugin name>
+---------------------
+
+You can also set the `CONF_DIR` environment variable to the custom config
+directory path.
+
+[float]
+=== Timeout settings
+
+By default, the `plugin` script will wait indefinitely when downloading before
+failing. The timeout parameter can be used to explicitly specify how long it
+waits. Here is some examples of setting it to different values:
+
+[source,shell]
+-----------------------------------
+# Wait for 30 seconds before failing
+sudo bin/plugin install mobz/elasticsearch-head --timeout 30s
+
+# Wait for 1 minute before failing
+sudo bin/plugin install mobz/elasticsearch-head --timeout 1m
+
+# Wait forever (default)
+sudo bin/plugin install mobz/elasticsearch-head --timeout 0
+-----------------------------------
+
+[float]
+=== Proxy settings
+
+To install a plugin via a proxy, you can pass the proxy details in with the
+Java settings `proxyHost` and `proxyPort`. On Unix based systems, these
+options can be set on the command line:
+
+[source,shell]
+-----------------------------------
+sudo bin/plugin install mobz/elasticsearch-head -DproxyHost=host_name -DproxyPort=port_number
+-----------------------------------
+
+On Windows, they need to be added to the `JAVA_OPTS` environment variable:
+
+[source,shell]
+-----------------------------------
+set JAVA_OPTS="-DproxyHost=host_name -DproxyPort=port_number"
+bin/plugin install mobz/elasticsearch-head
+-----------------------------------
+
+=== Settings related to plugins
+
+[float]
+=== Custom plugins directory
+
+The `plugins` directory can be changed from the default by adding the
+following to the `elasticsearch.yml` config file:
+
+[source,yml]
+---------------------
+path.plugins: /path/to/custom/plugins/dir
+---------------------
+
+The default location of the `plugins` directory depends on
+{ref}/setup-dir-layout.html[which package you install].
+
+[float]
+=== Mandatory Plugins
+
+If you rely on some plugins, you can define mandatory plugins by adding
+`plugin.mandatory` setting to the `config/elasticsearch.yml` file, for
+example:
+
+[source,yaml]
+--------------------------------------------------
+plugin.mandatory: mapper-attachments,lang-groovy
+--------------------------------------------------
+
+For safety reasons, a node will not start if it is missing a mandatory plugin.
+
+[float]
+=== Lucene version dependent plugins
+
+For some plugins, such as analysis plugins, a specific major Lucene version is
+required to run. In that case, the plugin provides in its
+`es-plugin.properties` file the Lucene version for which the plugin was built for.
+
+If present at startup the node will check the Lucene version before loading
+the plugin. You can disable that check using
+
+[source,yaml]
+--------------------------------------------------
+plugins.check_lucene: false
+--------------------------------------------------
+
diff --git a/docs/plugins/repository.asciidoc b/docs/plugins/repository.asciidoc
new file mode 100644
index 0000000..daab621
--- /dev/null
+++ b/docs/plugins/repository.asciidoc
@@ -0,0 +1,37 @@
+[[repository]]
+== Snapshot/Restore Repository Plugins
+
+Repository plugins extend the {ref}/modules-snapshots.html[Snapshot/Restore]
+functionality in Elasticsearch by adding repositories backed by the cloud or
+by distributed file systems:
+
+[float]
+==== Core repository plugins
+
+The core repository plugins are:
+
+<<cloud-aws,AWS Cloud>>::
+
+The Amazon Web Service (AWS) Cloud plugin adds support for using S3 as a
+repository.
+
+<<cloud-azure,Azure Cloud>>::
+
+The Azure Cloud plugin adds support for using Azure as a repository.
+
+https://github.com/elastic/elasticsearch-hadoop/tree/master/repository-hdfs[Hadoop HDFS Repository]::
+
+The Hadoop HDFS Repository plugin adds support for using an HDFS file system
+as a repository.
+
+
+[float]
+=== Community contributed repository plugins
+
+The following plugin has been contributed by our community:
+
+* https://github.com/wikimedia/search-repository-swift[Openstack Swift] (by http://en.cam4.es/youngqcmeat/Wikimedia Foundation)
+
+This community plugin appears to have been abandoned:
+
+* https://github.com/kzwang/elasticsearch-repository-gridfs[GridFS] Repository (by Kevin Wang)
diff --git a/docs/plugins/scripting.asciidoc b/docs/plugins/scripting.asciidoc
new file mode 100644
index 0000000..d07f1bc
--- /dev/null
+++ b/docs/plugins/scripting.asciidoc
@@ -0,0 +1,32 @@
+[[scripting]]
+== Scripting Plugins
+
+Scripting plugins extend the scripting functionality in Elasticsearch to allow
+the use of other scripting languages.
+
+[float]
+=== Core scripting plugins
+
+The core scripting plugins are:
+
+<<lang-javascript,JavaScript Language>>::
+
+The JavaScript language plugin enables the use of JavaScript in Elasticsearch
+scripts, via Mozilla's
+https://developer.mozilla.org/en-US/docs/Mozilla/Projects/Rhino[Rhino JavaScript] engine.
+
+<<lang-python,Python Language>>::
+
+The Python language plugin enables the use of Python in Elasticsearch
+scripts, via the http://www.jython.org/[Jython] Java implementation of Python.
+
+[float]
+=== Abandoned community scripting plugins
+
+This plugin has been contributed by our community, but appears to be abandoned:
+
+* https://github.com/hiredman/elasticsearch-lang-clojure[Clojure Language Plugin] (by Kevin Downey)
+
+include::lang-javascript.asciidoc[]
+
+include::lang-python.asciidoc[]
diff --git a/docs/plugins/security.asciidoc b/docs/plugins/security.asciidoc
new file mode 100644
index 0000000..1d425a7
--- /dev/null
+++ b/docs/plugins/security.asciidoc
@@ -0,0 +1,29 @@
+[[security]]
+== Security Plugins
+
+Security plugins add a security layer to  Elasticsearch.
+
+[float]
+=== Core security plugins
+
+The core security plugins are:
+
+link:/products/shield[Shield]::
+
+Shield is the Elastic product that makes it easy for anyone to add
+enterprise-grade security to their ELK stack. Designed to address the growing security
+needs of thousands of enterprises using ELK today, Shield provides peace of
+mind when it comes to protecting your data.
+
+[float]
+=== Community contributed security plugins
+
+The following plugin has been contributed by our community:
+
+* https://github.com/sscarduzio/elasticsearch-readonlyrest-plugin[Readonly REST]:
+  High performance access control for Elasticsearch native REST API (by Simone Scarduzio)
+
+This community plugin appears to have been abandoned:
+
+* https://github.com/sonian/elasticsearch-jetty[Jetty HTTP transport plugin]:
+  Uses Jetty to provide SSL connections, basic authentication, and request logging (by Sonian Inc.)
\ No newline at end of file
diff --git a/docs/plugins/transport.asciidoc b/docs/plugins/transport.asciidoc
new file mode 100644
index 0000000..f19f261
--- /dev/null
+++ b/docs/plugins/transport.asciidoc
@@ -0,0 +1,22 @@
+[[transport]]
+== Transport Plugins
+
+Transport plugins offer alternatives to HTTP.
+
+[float]
+=== Core transport plugins
+
+The core transport plugins are:
+
+https://github.com/elasticsearch/elasticsearch-transport-wares::[Servlet transport]::
+
+Use the REST interface over servlets.
+
+[float]
+=== Community contributed transport plugins
+
+The following community plugins appear to have been abandoned:
+
+* https://github.com/kzwang/elasticsearch-transport-redis[Redis transport plugin] (by Kevin Wang)
+* https://github.com/tlrx/transport-zeromq[ØMQ transport plugin] (by Tanguy Leroux)
+
diff --git a/docs/reference/index.asciidoc b/docs/reference/index.asciidoc
index 64daef1..e36a66a 100644
--- a/docs/reference/index.asciidoc
+++ b/docs/reference/index.asciidoc
@@ -5,6 +5,7 @@
 :branch:    2.0
 :jdk:       1.8.0_25
 :defguide:  https://www.elastic.co/guide/en/elasticsearch/guide/current
+:plugins:   https://www.elastic.co/guide/en/elasticsearch/plugins/master
 
 include::getting-started.asciidoc[]
 
diff --git a/docs/reference/mapping/fields.asciidoc b/docs/reference/mapping/fields.asciidoc
index dd267a8..90d75b9 100644
--- a/docs/reference/mapping/fields.asciidoc
+++ b/docs/reference/mapping/fields.asciidoc
@@ -32,9 +32,10 @@ can be customised when a mapping type is created.
 
     The original JSON representing the body of the document.
 
-<<mapping-size-field,`_size`>>::
+{plugins}/mapper-size.html[`_size`]::
 
-    The size of the `_source` field in bytes.
+    The size of the `_source` field in bytes, provided by the
+    {plugins}/mapper-size.html[`mapper-size` plugin].
 
 [float]
 === Indexing meta-fields
@@ -89,8 +90,6 @@ include::fields/parent-field.asciidoc[]
 
 include::fields/routing-field.asciidoc[]
 
-include::fields/size-field.asciidoc[]
-
 include::fields/source-field.asciidoc[]
 
 include::fields/timestamp-field.asciidoc[]
diff --git a/docs/reference/mapping/fields/size-field.asciidoc b/docs/reference/mapping/fields/size-field.asciidoc
deleted file mode 100644
index aa87f90..0000000
--- a/docs/reference/mapping/fields/size-field.asciidoc
+++ /dev/null
@@ -1,76 +0,0 @@
-[[mapping-size-field]]
-=== `_size` field
-
-The `_size` field, when enabled, indexes the size in bytes of the original
-<<mapping-source-field,`_source`>>. In order to enable it, set
-the mapping as follows:
-
-[source,js]
---------------------------
-PUT my_index
-{
-  "mappings": {
-    "my_type": {
-      "_size": {
-        "enabled": true
-      }
-    }
-  }
-}
---------------------------
-// AUTOSENSE
-
-The value of the `_size` field is accessible in queries, aggregations, scripts,
-and when sorting:
-
-[source,js]
---------------------------
-# Example documents
-PUT my_index/my_type/1
-{
-  "text": "This is a document"
-}
-
-PUT my_index/my_type/2
-{
-  "text": "This is another document"
-}
-
-GET my_index/_search
-{
-  "query": {
-    "range": {
-      "_size": { <1>
-        "gt": 10
-      }
-    }
-  },
-  "aggs": {
-    "Sizes": {
-      "terms": {
-        "field": "_size", <2>
-        "size": 10
-      }
-    }
-  },
-  "sort": [
-    {
-      "_size": { <3>
-        "order": "desc"
-      }
-    }
-  ],
-  "script_fields": {
-    "Size": {
-      "script": "doc['_size']"  <4>
-    }
-  }
-}
---------------------------
-// AUTOSENSE
-
-<1> Querying on the `_size` field
-<2> Aggregating on the `_size` field
-<3> Sorting on the `_size` field
-<4> Accessing the `_size` field in scripts (inline scripts must be <<enable-dynamic-scripting,enabled>> for this example to work)
-
diff --git a/docs/reference/migration/migrate_2_0.asciidoc b/docs/reference/migration/migrate_2_0.asciidoc
index 14ac164..bc664c2 100644
--- a/docs/reference/migration/migrate_2_0.asciidoc
+++ b/docs/reference/migration/migrate_2_0.asciidoc
@@ -14,1005 +14,45 @@ to delete the old indices. Elasticsearch will not start in the presence of old
 indices.
 
 [float]
+=== Network binds to localhost only
+
+Elasticsearch now binds to the loopback interface by default (usually
+`127.0.0.1` or `::1`). The `network.host` setting can be specified to change
+this behavior.
+
+[float]
 === Elasticsearch migration plugin
 
 We have provided the https://github.com/elastic/elasticsearch-migration[Elasticsearch migration plugin]
 to help you detect any issues that you may have when upgrading to
 Elasticsearch 2.0. Please install and run the plugin *before* upgrading.
 
-=== Mapping
-
-
-Remove file based default mappings #10870 (issue: #10620)
-Validate dynamic mappings updates on the master node. #10634 (issues: #8650, #8688)
-Remove the ability to have custom per-field postings and doc values formats. #9741 (issue: #8746)
-Remove support for new indexes using path setting in object/nested fields or index_name in any field #9570 (issue: #6677)
-Remove index_analyzer setting to simplify analyzer logic #9451 (issue: #9371)
-Remove type level default analyzers #9430 (issues: #8874, #9365)
-Add doc values support to boolean fields. #7961 (issues: #4678, #7851)
-
-
-A number of changes have been made to mappings to remove ambiguity and to
-ensure that conflicting mappings cannot be created.
-
-==== Conflicting field mappings
-
-Fields with the same name, in the same index, in different types, must have
-the same mapping, with the exception of the <<copy-to>>, <<dynamic>>,
-<<enabled>>, <<ignore-above>>, <<include-in-all>>, and <<properties>>
-parameters, which may have different settings per field.
-
-[source,js]
----------------
-PUT my_index
-{
-  "mappings": {
-    "type_one": {
-      "properties": {
-        "name": { <1>
-          "type": "string"
-        }
-      }
-    },
-    "type_two": {
-      "properties": {
-        "name": { <1>
-          "type":     "string",
-          "analyzer": "english"
-        }
-      }
-    }
-  }
-}
----------------
-<1> The two `name` fields have conflicting mappings and will prevent Elasticsearch
-    from starting.
-
-Elasticsearch will not start in the presence of conflicting field mappings.
-These indices must be deleted or reindexed using a new mapping.
-
-The `ignore_conflicts` option of the put mappings API has been removed.
-Conflicts can't be ignored anymore.
-
-==== Fields cannot be referenced by short name
-
-A field can no longer be referenced using its short name.  Instead, the full
-path to the field is required.  For instance:
-
-[source,js]
----------------
-PUT my_index
-{
-  "mappings": {
-    "my_type": {
-      "properties": {
-        "title":     { "type": "string" }, <1>
-        "name": {
-          "properties": {
-            "title": { "type": "string" }, <2>
-            "first": { "type": "string" },
-            "last":  { "type": "string" }
-          }
-        }
-      }
-    }
-  }
-}
----------------
-<1> This field is referred to as `title`.
-<2> This field is referred to as `name.title`.
-
-Previously, the two `title` fields in the example above could have been
-confused with each other when using the short name `title`.
-
-=== Type name prefix removed
-
-Previously, two fields with the same name in two different types could
-sometimes be disambiguated by prepending the type name.  As a side effect, it
-would add a filter on the type name to the relevant query.  This feature was
-ambiguous -- a type name could be confused with a field name -- and didn't
-work everywhere e.g. aggregations.
-
-Instead, fields should be specified with the full path, but without a type
-name prefix.  If you wish to filter by the `_type` field, either specify the
-type in the URL or add an explicit filter.
-
-The following example query in 1.x:
-
-[source,js]
-----------------------------
-GET my_index/_search
-{
-  "query": {
-    "match": {
-      "my_type.some_field": "quick brown fox"
-    }
-  }
-}
-----------------------------
-
-would be rewritten in 2.0 as:
-
-[source,js]
-----------------------------
-GET my_index/my_type/_search <1>
-{
-  "query": {
-    "match": {
-      "some_field": "quick brown fox" <2>
-    }
-  }
-}
-----------------------------
-<1> The type name can be specified in the URL to act as a filter.
-<2> The field name should be specified without the type prefix.
-
-==== Field names may not contain dots
-
-In 1.x, it was possible to create fields with dots in their name, for
-instance:
-
-[source,js]
-----------------------------
-PUT my_index
-{
-  "mappings": {
-    "my_type": {
-      "properties": {
-        "foo.bar": { <1>
-          "type": "string"
-        },
-        "foo": {
-          "properties": {
-            "bar": { <1>
-              "type": "string"
-            }
-          }
-        }
-      }
-    }
-  }
-}
-----------------------------
-<1> These two fields cannot be distinguised as both are referred to as `foo.bar`.
-
-You can no longer create fields with dots in the name.
-
-==== Type names may not start with a dot
-
-In 1.x, Elasticsearch would issue a warning if a type name included a dot,
-e.g. `my.type`.  Now that type names are no longer used to distinguish between
-fields in differnt types, this warning has been relaxed: type names may now
-contain dots, but they may not *begin* with a dot.  The only exception to this
-is the special `.percolator` type.
-
-==== Types may no longer be deleted
-
-In 1.x it was possible to delete a type mapping, along with all of the
-documents of that type, using the delete mapping API.  This is no longer
-supported, because remnants of the fields in the type could remain in the
-index, causing corruption later on.
-
-==== Type meta-fields
-
-The <<mapping-fields,meta-fields>> associated with had configuration options
-removed, to make them more reliable:
-
-* `_id` configuration can no longer be changed.  If you need to sort, use the <<mapping-uid-field,`_uid`>> field instead.
-* `_type` configuration can no longer be changed.
-* `_index` configuration can no longer be changed.
-* `_routing` configuration is limited to marking routing as required.
-* `_field_names` configuration is limited to disabling the field.
-* `_size` configuration is limited to enabling the field.
-* `_timestamp` configuration is limited to enabling the field, setting format and default value.
-* `_boost` has been removed.
-* `_analyzer` has been removed.
-
-Importantly, *meta-fields can no longer be specified as part of the document
-body.*  Instead, they must be specified in the query string parameters.  For
-instance, in 1.x, the `routing` could be specified as follows:
-
-[source,json]
------------------------------
-PUT my_index
-{
-  "mappings": {
-    "my_type": {
-      "_routing": {
-        "path": "group" <1>
-      },
-      "properties": {
-        "group": { <1>
-          "type": "string"
-        }
-      }
-    }
-  }
-}
-
-PUT my_index/my_type/1 <2>
-{
-  "group": "foo"
-}
------------------------------
-<1> This 1.x mapping tells Elasticsearch to extract the `routing` value from the `group` field in the document body.
-<2> This indexing request uses a `routing` value of `foo`.
-
-In 2.0, the routing must be specified explicitly:
-
-[source,json]
------------------------------
-PUT my_index
-{
-  "mappings": {
-    "my_type": {
-      "_routing": {
-        "required": true <1>
-      },
-      "properties": {
-        "group": {
-          "type": "string"
-        }
-      }
-    }
-  }
-}
-
-PUT my_index/my_type/1?routing=bar <2>
-{
-  "group": "foo"
-}
------------------------------
-<1> Routing can be marked as required to ensure it is not forgotten during indexing.
-<2> This indexing request uses a `routing` value of `bar`.
-
-==== Other mapping changes
-
-* The setting `index.mapping.allow_type_wrapper` has been removed.  Documents should always be sent without the type as the root element.
-* The `binary` field does not support the `compress` and `compress_threshold` options anymore.
-
-
-
-
-=== Networking
-
-Elasticsearch now binds to the loopback interface by default (usually 127.0.0.1
-or ::1), the setting `network.host` can be specified to change this behavior.
-
-=== Rivers removal
-
-Elasticsearch does not support rivers anymore. While we had first planned to
-keep them around to ease migration, keeping support for rivers proved to be
-challenging as it conflicted with other important changes that we wanted to
-bring to 2.0 like synchronous dynamic mappings updates, so we eventually
-decided to remove them entirely. See
-https://www.elastic.co/blog/deprecating_rivers for more background about why
-we are moving away from rivers.
-
-=== Indices API
-
-The <<alias-retrieving, get alias api>> will, by default produce an error response
-if a requested index does not exist. This change brings the defaults for this API in
-line with the other Indices APIs. The <<multi-index>> options can be used on a request
-to change this behavior
-
-`GetIndexRequest.features()` now returns an array of Feature Enums instead of an array of String values.
-
-The following deprecated methods have been removed:
-
-* `GetIndexRequest.addFeatures(String[])` - Please use `GetIndexRequest.addFeatures(Feature[])` instead
-* `GetIndexRequest.features(String[])` - Please use `GetIndexRequest.features(Feature[])` instead
-* `GetIndexRequestBuilder.addFeatures(String[])` - Please use `GetIndexRequestBuilder.addFeatures(Feature[])` instead
-* `GetIndexRequestBuilder.setFeatures(String[])` - Please use `GetIndexRequestBuilder.setFeatures(Feature[])` instead
-
-=== Partial fields
-
-Partial fields were deprecated since 1.0.0beta1 in favor of <<search-request-source-filtering,source filtering>>.
-
-=== More Like This
-
-The More Like This API and the More Like This Field query have been removed in
-favor of the <<query-dsl-mlt-query, More Like This Query>>.
-
-The parameter `percent_terms_to_match` has been removed in favor of
-`minimum_should_match`.
-
-=== Routing
-
-The default hash function that is used for routing has been changed from djb2 to
-murmur3. This change should be transparent unless you relied on very specific
-properties of djb2. This will help ensure a better balance of the document counts
-between shards.
-
-In addition, the following node settings related to routing have been deprecated:
-
-[horizontal]
-
-`cluster.routing.operation.hash.type`::
-
-  This was an undocumented setting that allowed to configure which hash function
-  to use for routing. `murmur3` is now enforced on new indices.
-
-`cluster.routing.operation.use_type`::
-
-  This was an undocumented setting that allowed to take the `_type` of the
-  document into account when computing its shard (default: `false`). `false` is
-  now enforced on new indices.
-
-=== Async replication
-
-The `replication` parameter has been removed from all CRUD operations (index,
-update, delete, bulk).  These operations are now synchronous
-only, and a request will only return once the changes have been replicated to
-all active shards in the shard group.
-
-=== Store
-
-The `memory` / `ram` store (`index.store.type`) option was removed in Elasticsearch 2.0.
-
-=== Term Vectors API
-
-Usage of `/_termvector` is deprecated, and replaced in favor of `/_termvectors`.
-
-=== Script fields
-
-Script fields in 1.x were only returned as a single value. So even if the return
-value of a script used to be list, it would be returned as an array containing
-a single value that is a list too, such as:
-
-[source,js]
----------------
-"fields": {
-  "my_field": [
-    [
-      "v1",
-      "v2"
-    ]
-  ]
-}
----------------
-
-In elasticsearch 2.x, scripts that return a list of values are considered as
-multivalued fields. So the same example would return the following response,
-with values in a single array.
-
-[source,js]
----------------
-"fields": {
-  "my_field": [
-    "v1",
-    "v2"
-  ]
-}
----------------
-
-=== Main API
-
-Previously, calling `GET /` was giving back the http status code within the json response
-in addition to the actual HTTP status code. We removed `status` field in json response.
-
-=== Java API
-
-`org.elasticsearch.index.queries.FilterBuilders` has been removed as part of the merge of
-queries and filters. These filters are now available in `QueryBuilders` with the same name.
-All methods that used to accept a `FilterBuilder` now accept a `QueryBuilder` instead.
-
-In addition some query builders have been removed or renamed:
-
-* `commonTerms(...)` renamed with `commonTermsQuery(...)`
-* `queryString(...)` renamed with `queryStringQuery(...)`
-* `simpleQueryString(...)` renamed with `simpleQueryStringQuery(...)`
-* `textPhrase(...)` removed
-* `textPhrasePrefix(...)` removed
-* `textPhrasePrefixQuery(...)` removed
-* `filtered(...)` removed. Use `filteredQuery(...)` instead.
-* `inQuery(...)` removed.
-
-=== Aggregations
-
-The `date_histogram` aggregation now returns a `Histogram` object in the response, and the `DateHistogram` class has been removed. Similarly
-the `date_range`, `ipv4_range`, and `geo_distance` aggregations all return a `Range` object in the response, and the `IPV4Range`, `DateRange`,
-and `GeoDistance` classes have been removed. The motivation for this is to have a single response API for the Range and Histogram aggregations
-regardless of the type of data being queried.  To support this some changes were made in the `MultiBucketAggregation` interface which applies
-to all bucket aggregations:
-
-* The `getKey()` method now returns `Object` instead of `String`. The actual object type returned depends on the type of aggregation requested
-(e.g. the `date_histogram` will return a `DateTime` object for this method whereas a `histogram` will return a `Number`).
-* A `getKeyAsString()` method has been added to return the String representation of the key.
-* All other `getKeyAsX()` methods have been removed.
-* The `getBucketAsKey(String)` methods have been removed on all aggregations except the `filters` and `terms` aggregations.
-
-The `histogram` and the `date_histogram` aggregation now support a simplified `offset` option that replaces the previous `pre_offset` and
-`post_offset` rounding options. Instead of having to specify two separate offset shifts of the underlying buckets, the `offset` option
-moves the bucket boundaries in positive or negative direction depending on its argument.
-
-The `date_histogram` options for `pre_zone` and `post_zone` are replaced by the `time_zone` option. The behavior of `time_zone` is
-equivalent to the former `pre_zone` option. Setting `time_zone` to a value like "+01:00" now will lead to the bucket calculations
-being applied in the specified time zone but In addition to this, also the `pre_zone_adjust_large_interval` is removed because we
-now always return dates and bucket keys in UTC.
-
-Both the `histogram` and `date_histogram` aggregations now have a default `min_doc_count` of `0` instead of `1` previously.
-
-`include`/`exclude` filtering on the `terms` aggregation now uses the same syntax as regexp queries instead of the Java syntax. While simple
-regexps should still work, more complex ones might need some rewriting. Also, the `flags` parameter is not supported anymore.
-
-=== Terms filter lookup caching
-
-The terms filter lookup mechanism does not support the `cache` option anymore
-and relies on the filesystem cache instead. If the lookup index is not too
-large, it is recommended to make it replicated to all nodes by setting
-`index.auto_expand_replicas: 0-all` in order to remove the network overhead as
-well.
-
-=== Delete by query
-
-The meaning of the `_shards` headers in the delete by query response has changed. Before version 2.0 the `total`,
-`successful` and `failed` fields in the header are based on the number of primary shards. The failures on replica
-shards aren't being kept track of. From version 2.0 the stats in the `_shards` header are based on all shards
-of an index. The http status code is left unchanged and is only based on failures that occurred while executing on
-primary shards.
-
-=== Delete api with missing routing when required
-
-Delete api requires a routing value when deleting a document belonging to a type that has routing set to required in its
-mapping, whereas previous elasticsearch versions would trigger a broadcast delete on all shards belonging to the index.
-A `RoutingMissingException` is now thrown instead.
-
-
-==== Default date format now is `strictDateOptionalTime`
-
-Instead of `dateOptionalTime` the new default date format now is `strictDateOptionalTime`,
-which is more strict in parsing dates. This means, that dates now need to have a four digit year,
-a two-digit month, day, hour, minute and second. This means, you may need to preprend a part of the date
-with a zero to make it conform or switch back to the old `dateOptionalTime` format.
-
-==== Date format does not support unix timestamps by default
-
-In earlier versions of elasticsearch, every timestamp was always tried to be parsed as
-as unix timestamp first. This means, even when specifying a date format like
-`dateOptionalTime`, one could supply unix timestamps instead of a ISO8601 formatted
-date.
-
-This is not supported anymore. If you want to store unix timestamps, you need to specify
-the appropriate formats in the mapping, namely `epoch_second` or `epoch_millis`.
-
-In addition the `numeric_resolution` mapping parameter is ignored. Use the
-`epoch_second` and `epoch_millis` date formats instead.
-
-==== Source field limitations
-The `_source` field could previously be disabled dynamically. Since this field
-is a critical piece of many features like the Update API, it is no longer
-possible to disable.
-
-The options for `compress` and `compress_threshold` have also been removed.
-The source field is already compressed. To minimize the storage cost,
-set `index.codec: best_compression` in index settings.
-
-==== Boolean fields
-
-Boolean fields used to have a string fielddata with `F` meaning `false` and `T`
-meaning `true`. They have been refactored to use numeric fielddata, with `0`
-for `false` and `1` for `true`. As a consequence, the format of the responses of
-the following APIs changed when applied to boolean fields: `0`/`1` is returned
-instead of `F`/`T`:
-
- - <<search-request-fielddata-fields,fielddata fields>>
- - <<search-request-sort,sort values>>
- - <<search-aggregations-bucket-terms-aggregation,terms aggregations>>
-
-In addition, terms aggregations use a custom formatter for boolean (like for
-dates and ip addresses, which are also backed by numbers) in order to return
-the user-friendly representation of boolean fields: `false`/`true`:
-
-[source,js]
----------------
-"buckets": [
-  {
-     "key": 0,
-     "key_as_string": "false",
-     "doc_count": 42
-  },
-  {
-     "key": 1,
-     "key_as_string": "true",
-     "doc_count": 12
-  }
-]
----------------
-
-==== Murmur3 Fields
-Fields of type `murmur3` can no longer change `doc_values` or `index` setting.
-They are always stored with doc values, and not indexed.
-
-==== Config based mappings
-The ability to specify mappings in configuration files has been removed. To specify
-default mappings that apply to multiple indexes, use index templates.
-
-The following settings are no longer valid:
-
-* `index.mapper.default_mapping_location`
-* `index.mapper.default_percolator_mapping_location`
-
-=== Codecs
-
-It is no longer possible to specify per-field postings and doc values formats
-in the mappings. This setting will be ignored on indices created before
-elasticsearch 2.0 and will cause mapping parsing to fail on indices created on
-or after 2.0. For old indices, this means that new segments will be written
-with the default postings and doc values formats of the current codec.
-
-It is still possible to change the whole codec by using the `index.codec`
-setting. Please however note that using a non-default codec is discouraged as
-it could prevent future versions of Elasticsearch from being able to read the
-index.
-
-=== Scripting settings
-
-Removed support for `script.disable_dynamic` node setting, replaced by
-fine-grained script settings described in the <<enable-dynamic-scripting,scripting docs>>.
-The following setting previously used to enable dynamic scripts:
-
-[source,yaml]
----------------
-script.disable_dynamic: false
----------------
-
-can be replaced with the following two settings in `elasticsearch.yml` that
-achieve the same result:
-
-[source,yaml]
----------------
-script.inline: on
-script.indexed: on
----------------
-
-=== Script parameters
-
-Deprecated script parameters `id`, `file`, `scriptField`, `script_id`, `script_file`,
-`script`, `lang` and `params`. The <<modules-scripting,new script API syntax>> should be used in their place.
-
-The deprecated script parameters have been removed from the Java API so applications using the Java API will
-need to be updated.
-
-=== Groovy scripts sandbox
-
-The groovy sandbox and related settings have been removed. Groovy is now a non
-sandboxed scripting language, without any option to turn the sandbox on.
-
-=== Plugins making use of scripts
-
-Plugins that make use of scripts must register their own script context through
-`ScriptModule`. Script contexts can be used as part of fine-grained settings to
-enable/disable scripts selectively.
-
-=== Thrift and memcached transport
-
-The thrift and memcached transport plugins are no longer supported.  Instead, use
-either the HTTP transport (enabled by default) or the node or transport Java client.
-
-=== `search_type=count` deprecation
-
-The `count` search type has been deprecated. All benefits from this search type can
-now be achieved by using the `query_then_fetch` search type (which is the
-default) and setting `size` to `0`.
-
-=== The count api internally uses the search api
-
-The count api is now a shortcut to the search api with `size` set to 0. As a
-result, a total failure will result in an exception being returned rather
-than a normal response with `count` set to `0` and shard failures.
-
-=== JSONP support
-
-JSONP callback support has now been removed. CORS should be used to access Elasticsearch
-over AJAX instead:
-
-[source,yaml]
----------------
-http.cors.enabled: true
-http.cors.allow-origin: /https?:\/\/localhost(:[0-9]+)?/
----------------
-
-=== CORS allowed origins
-
-The CORS allowed origins setting, `http.cors.allow-origin`, no longer has a default value. Previously, the default value
-was `*`, which would allow CORS requests from any origin and is considered insecure. The `http.cors.allow-origin` setting
-should be specified with only the origins that should be allowed, like so:
-
-[source,yaml]
----------------
-http.cors.allow-origin: /https?:\/\/localhost(:[0-9]+)?/
----------------
-
-=== Cluster state REST api
-
-The cluster state api doesn't return the `routing_nodes` section anymore when
-`routing_table` is requested. The newly introduced `routing_nodes` flag can
-be used separately to control whether `routing_nodes` should be returned.
-
-=== Query DSL
-
-Change to ranking behaviour: single-term queries on numeric fields now score in the same way as string fields (use of IDF, norms if enabled).
-Previously, term queries on numeric fields were deliberately prevented from using the usual Lucene scoring logic and this behaviour was undocumented and, to some, unexpected.
-If the introduction of scoring to numeric fields is undesirable for your query clauses the fix is simple: wrap them in a `constant_score` or use a `filter` expression instead.
-
-The `filtered` query is deprecated. Instead you should use a `bool` query with
-a `must` clause for the query and a `filter` clause for the filter. For instance
-the below query:
-
-[source,js]
----------------
-{
-  "filtered": {
-    "query": {
-      // query
-    },
-    "filter": {
-      // filter
-    }
-  }
-}
----------------
-can be replaced with
-[source,js]
----------------
-{
-  "bool": {
-    "must": {
-      // query
-    },
-    "filter": {
-      // filter
-    }
-  }
-}
----------------
-and will produce the same scores.
-
-The `fuzzy_like_this` and `fuzzy_like_this_field` queries have been removed.
-
-The `limit` filter is deprecated and becomes a no-op. You can achieve similar
-behaviour using the <<search-request-body,terminate_after>> parameter.
-
-`or` and `and` on the one hand and `bool` on the other hand used to have
-different performance characteristics depending on the wrapped filters. This is
-fixed now, as a consequence the `or` and `and` filters are now deprecated in
-favour or `bool`.
-
-The `execution` option of the `terms` filter is now deprecated and ignored if
-provided.
-
-The `_cache` and `_cache_key` parameters of filters are deprecated in the REST
-layer and removed in the Java API. In case they are specified they will be
-ignored. Instead filters are always used as their own cache key and elasticsearch
-makes decisions by itself about whether it should cache filters based on how
-often they are used.
-
-Java plugins that register custom queries can do so by using the
-`IndicesQueriesModule#addQuery(Class<? extends QueryParser>)` method. Other
-ways to register custom queries are not supported anymore.
-
-==== Query/filter merge
-
-Elasticsearch no longer makes a difference between queries and filters in the
-DSL; it detects when scores are not needed and automatically optimizes the
-query to not compute scores and optionally caches the result.
-
-As a consequence the `query` filter serves no purpose anymore and is deprecated.
-
-=== Timezone for date field
-
-Specifying the `time_zone` parameter on queries or aggregations of `date` type fields
-must now be either an ISO 8601 UTC offset, or a timezone id. For example, the value
-`+1:00` must now be `+01:00`.
-
-=== Snapshot and Restore
-
-Locations of the shared file system repositories and the URL repositories with `file:` URLs has to be now registered
-using `path.repo` setting. The `path.repo` setting can contain one or more repository locations:
-
-[source,yaml]
----------------
-path.repo: ["/mnt/daily", "/mnt/weekly"]
----------------
-
-If the repository location is specified as an absolute path it has to start with one of the locations
-specified in `path.repo`. If the location is specified as a relative path, it will be resolved against the first
-location specified in the `path.repo` setting.
-
-URL repositories with `http:`, `https:`, and `ftp:` URLs has to be whitelisted by specifying allowed URLs in the
-`repositories.url.allowed_urls` setting. This setting supports wildcards in the place of host, path, query, and
-fragment. For example:
-
-[source,yaml]
------------------------------------
-repositories.url.allowed_urls: ["http://www.example.org/root/*", "https://*.mydomain.com/*?*#*"]
------------------------------------
-
-The obsolete parameters `expand_wildcards_open` and `expand_wildcards_close` are no longer
-supported by the snapshot and restore operations. These parameters have been replaced by
-a single `expand_wildcards` parameter. See <<multi-index,the multi-index docs>> for more.
-
-=== `_shutdown` API
-
-The `_shutdown` API has been removed without a replacement. Nodes should be managed via operating
-systems and the provided start/stop scripts.
-
-=== Analyze API
-
-* The Analyze API return 0 as first Token's position instead of 1.
-* The `text()` method on `AnalyzeRequest` now returns `String[]` instead of `String`.
-
-=== Multiple data.path striping
-
-Previously, if the `data.path` setting listed multiple data paths, then a
-shard would be ``striped'' across all paths by writing a whole file to each
-path in turn (in accordance with the `index.store.distributor` setting).  The
-result was that the files from a single segment in a shard could be spread
-across multiple disks, and the failure of any one disk could corrupt multiple
-shards.
-
-This striping is no longer supported.  Instead, different shards may be
-allocated to different paths, but all of the files in a single shard will be
-written to the same path.
-
-If striping is detected while starting Elasticsearch 2.0.0 or later, all of
-the files belonging to the same shard will be migrated to the same path. If
-there is not enough disk space to complete this migration, the upgrade will be
-cancelled and can only be resumed once enough disk space is made available.
-
-The `index.store.distributor` setting has also been removed.
-
-=== Hunspell dictionary configuration
-
-The parameter `indices.analysis.hunspell.dictionary.location` has been removed,
-and `<path.conf>/hunspell` is always used.
-
-=== Java API Transport API construction
-
-The `TransportClient` construction code has changed, it now uses the builder
-pattern. Instead of using:
-
-[source,java]
---------------------------------------------------
-Settings settings = Settings.settingsBuilder()
-        .put("cluster.name", "myClusterName").build();
-Client client = new TransportClient(settings);
---------------------------------------------------
-
-Use:
-
-[source,java]
---------------------------------------------------
-Settings settings = Settings.settingsBuilder()
-        .put("cluster.name", "myClusterName").build();
-Client client = TransportClient.builder().settings(settings).build();
---------------------------------------------------
-
-=== Logging
-
-Log messages are now truncated at 10,000 characters. This can be changed in the
-`logging.yml` configuration file.
-
-[float]
-=== Removed `top_children` query
-
-The `top_children` query has been removed in favour of the `has_child` query. The `top_children` query wasn't always faster
-than the `has_child` query and the `top_children` query was often inaccurate. The total hits and any aggregations in the
-same search request will likely be off if `top_children` was used.
-
-=== Removed file based index templates
-Index templates can no longer be configured on disk. Use the `_template` API instead.
-
-[float]
-=== Removed `id_cache` from stats apis
-
-Removed `id_cache` metric from nodes stats, indices stats and cluster stats apis. This metric has also been removed
-from the shards cat, indices cat and nodes cat apis. Parent/child memory is now reported under fielddata, because it
-has internally be using fielddata for a while now.
-
-To just see how much parent/child related field data is taking, the `fielddata_fields` option can be used on the stats
-apis. Indices stats example:
-
-[source,js]
---------------------------------------------------
-curl -XGET "http://localhost:9200/_stats/fielddata?pretty&human&fielddata_fields=_parent"
---------------------------------------------------
-
-Parent/child is using field data for the `_parent` field since version `1.1.0`, but the memory stats for the `_parent`
-field were still shown under `id_cache` metric in the stats apis for backwards compatible reasons between 1.x versions.
-
-Before version `1.1.0` the parent/child had its own in-memory data structures for id values in the `_parent` field.
-
-[float]
-=== Removed `id_cache` from clear cache api
-
-Removed `id_cache` option from the clear cache apis. The `fielddata` option should be used to clear `_parent` field
-from fielddata.
-
-[float]
-=== Highlighting
-
-The default value for the `require_field_match` option is `true` rather than
-`false`, meaning that the highlighters will take the fields that were queried
-into account by default. That means for instance that highlighting any field
-when querying the `_all` field will produce no highlighted snippets by default,
-given that the match was on the `_all` field only. Querying the same fields
-that need to be highlighted is the cleaner solution to get highlighted snippets
-back. Otherwise `require_field_match` option can be set to `false` to ignore
-field names completely when highlighting.
-
-The postings highlighter doesn't support the `require_field_match` option
-anymore, it will only highlight fields that were queried.
-
-The `match` query with type set to `match_phrase_prefix` is not supported by the
-postings highlighter. No highlighted snippets will be returned.
-
-[float]
-=== Parent/child
-
-Parent/child has been rewritten completely to reduce memory usage and to execute
-`has_child` and `has_parent` queries faster and more efficient. The `_parent` field
-uses doc values by default. The refactored and improved implementation is only active
-for indices created on or after version 2.0.
-
-In order to benefit for all performance and memory improvements we recommend to reindex all
-indices that have the `_parent` field created before was upgraded to 2.0.
-
-The following breaks in backwards compatability have been made on indices with the `_parent` field
-created on or after clusters with version 2.0:
-* The `type` option on the `_parent` field can only point to a parent type that doesn't exist yet,
-  so this means that an existing type/mapping can no longer become a parent type.
-* The `has_child` and `has_parent` queries can no longer be use in alias filters.
-
-=== Meta fields returned under the top-level json object
-
-When selecting meta fields such as `_routing` or `_timestamp`, the field values
-are now directly put as a top-level property of the json objet, instead of being
-put under `fields` like regular stored fields.
-
-[source,sh]
----------------
-curl -XGET 'localhost:9200/test/_search?fields=_timestamp,foo'
----------------
-
-[source,js]
----------------
-{
-   [...]
-   "hits": {
-      "total": 1,
-      "max_score": 1,
-      "hits": [
-         {
-            "_index": "test",
-            "_type": "test",
-            "_id": "1",
-            "_score": 1,
-            "_timestamp": 10000000,
-            "fields": {
-              "foo" : [ "bar" ]
-            }
-         }
-      ]
-   }
-}
----------------
-
-=== Settings for resource watcher have been renamed
-
-The setting names for configuring the resource watcher have been renamed
-to prevent clashes with the watcher plugin
-
-* `watcher.enabled` is now `resource.reload.enabled`
-* `watcher.interval` is now `resource.reload.interval`
-* `watcher.interval.low` is now `resource.reload.interval.low`
-* `watcher.interval.medium` is now `resource.reload.interval.medium`
-* `watcher.interval.high` is now `resource.reload.interval.high`
-
-=== Percolator stats
-
-Changed the `percolate.getTime` stat (total time spent on percolating) to `percolate.time` state.
-
-=== Plugin Manager for official plugins
-
-Some of the elasticsearch official plugins have been moved to elasticsearch repository and will be released at the
-same time as elasticsearch itself, using the same version number.
-
-In that case, the plugin manager can now use a simpler form to identify an official plugin. Instead of:
-
-[source,sh]
----------------
-bin/plugin install elasticsearch/plugin_name/version
----------------
-
-You can use:
-
-[source,sh]
----------------
-bin/plugin install plugin_name
----------------
-
-The plugin manager will recognize this form and will be able to download the right version for your elasticsearch
-version.
-
-For older versions of elasticsearch, you still have to use the older form.
-
-For the record, official plugins which can use this new simplified form are:
-
-* elasticsearch-analysis-icu
-* elasticsearch-analysis-kuromoji
-* elasticsearch-analysis-phonetic
-* elasticsearch-analysis-smartcn
-* elasticsearch-analysis-stempel
-* elasticsearch-cloud-aws
-* elasticsearch-cloud-azure
-* elasticsearch-cloud-gce
-* elasticsearch-delete-by-query
-* elasticsearch-lang-javascript
-* elasticsearch-lang-python
-
-=== `/bin/elasticsearch` version needs `-V` parameter
-
-Due to switching to elasticsearchs internal command line parsing
-infrastructure for the pluginmanager and the elasticsearch start up
-script, the `-v` parameter now stands for `--verbose`, where as `-V` or
-`--version` can be used to show the Elasticsearch version and exit.
-
-=== `/bin/elasticsearch` dynamic parameters must come after static ones
-
-If you are setting configuration options like cluster name or node name via
-the commandline, you have to ensure, that the static options like pid file
-path or daemonizing always come first, like this
-
-```
-/bin/elasticsearch -d -p /tmp/foo.pid --http.cors.enabled=true --http.cors.allow-origin='*'
+include::migrate_2_0/removals.asciidoc[]
 
-```
+include::migrate_2_0/striping.asciidoc[]
 
-For a list of those static parameters, run `/bin/elasticsearch -h`
+include::migrate_2_0/mapping.asciidoc[]
 
-=== Aliases
+include::migrate_2_0/crud.asciidoc[]
 
-Fields used in alias filters no longer have to exist in the mapping upon alias creation time. Alias filters are now
-parsed at request time and then the fields in filters are resolved from the mapping, whereas before alias filters were
-parsed at alias creation time and the parsed form was kept around in memory.
+include::migrate_2_0/query_dsl.asciidoc[]
 
+include::migrate_2_0/search.asciidoc[]
 
-=== _analyze API
+include::migrate_2_0/aggs.asciidoc[]
 
-The `prefer_local` has been removed from the _analyze api. The _analyze api is a light operation and the caller shouldn't
-be concerned about whether it executes on the node that receives the request or another node.
+include::migrate_2_0/parent_child.asciidoc[]
 
-=== Shadow replicas
+include::migrate_2_0/scripting.asciidoc[]
 
-The `node.enable_custom_paths` setting has been removed and replaced by the
-`path.shared_data` setting to allow shadow replicas with custom paths to work
-with the security manager. For example, if your previous configuration had:
+include::migrate_2_0/index_apis.asciidoc[]
 
-```
-node.enable_custom_paths: true
-```
+include::migrate_2_0/snapshot_restore.asciidoc[]
 
-And you created an index using shadow replicas with `index.data_path` set to
-`/opt/data/my_index` with the following:
+include::migrate_2_0/packaging.asciidoc[]
 
-[source,js]
---------------------------------------------------
-curl -XPUT 'localhost:9200/my_index' -d '
-{
-    "index" : {
-        "number_of_shards" : 1,
-        "number_of_replicas" : 4,
-        "data_path": "/opt/data/my_index",
-        "shadow_replicas": true
-    }
-}'
---------------------------------------------------
+include::migrate_2_0/settings.asciidoc[]
 
-For 2.0, you will need to set `path.shared_data` to a parent directory of the
-index's data_path, so:
+include::migrate_2_0/stats.asciidoc[]
 
-```
-path.shared_data: /opt/data
-```
+include::migrate_2_0/java.asciidoc[]
\ No newline at end of file
diff --git a/docs/reference/migration/migrate_2_0/aggs.asciidoc b/docs/reference/migration/migrate_2_0/aggs.asciidoc
new file mode 100644
index 0000000..8134812
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/aggs.asciidoc
@@ -0,0 +1,69 @@
+=== Aggregation changes
+
+==== Min doc count defaults to zero
+
+Both the `histogram` and `date_histogram` aggregations now have a default
+`min_doc_count` of `0` instead of `1`.
+
+==== Timezone for date field
+
+Specifying the `time_zone` parameter in queries or aggregations on fields of
+type `date` must now be either an ISO 8601 UTC offset, or a timezone id. For
+example, the value `+1:00` must now be written as `+01:00`.
+
+==== Time zones and offsets
+
+The `histogram` and the `date_histogram` aggregation now support a simplified
+`offset` option that replaces the previous `pre_offset` and `post_offset`
+rounding options. Instead of having to specify two separate offset shifts of
+the underlying buckets, the `offset` option moves the bucket boundaries in
+positive or negative direction depending on its argument.
+
+The `date_histogram` options for `pre_zone` and `post_zone` are replaced by
+the `time_zone` option. The behavior of `time_zone` is equivalent to the
+former `pre_zone` option. Setting `time_zone` to a value like "+01:00" now
+will lead to the bucket calculations being applied in the specified time zone.
+The `key` is returned as the timestamp in UTC, but the `key_as_string` is
+returned in the time zone specified.
+
+In addition to this, the `pre_zone_adjust_large_interval` is removed because
+we now always return dates and bucket keys in UTC.
+
+==== Including/excluding terms
+
+`include`/`exclude` filtering on the `terms` aggregation now uses the same
+syntax as <<regexp-syntax,regexp queries>> instead of the Java regular
+expression syntax. While simple regexps should still work, more complex ones
+might need some rewriting. Also, the `flags` parameter is no longer supported.
+
+==== Boolean fields
+
+Aggregations on `boolean` fields will now return `0` and `1` as keys, and
+`"true"` and `"false"` as string keys.  See <<migration-bool-fields>> for more
+information.
+
+
+==== Java aggregation classes
+
+The `date_histogram` aggregation now returns a `Histogram` object in the
+response, and the `DateHistogram` class has been removed.  Similarly the
+`date_range`, `ipv4_range`, and `geo_distance` aggregations all return a
+`Range` object in the response, and the `IPV4Range`, `DateRange`, and
+`GeoDistance` classes have been removed.
+
+The motivation for this is to have a single response API for the Range and
+Histogram aggregations regardless of the type of data being queried.  To
+support this some changes were made in the `MultiBucketAggregation` interface
+which applies to all bucket aggregations:
+
+* The `getKey()` method now returns `Object` instead of `String`. The actual
+  object type returned depends on the type of aggregation requested (e.g. the
+  `date_histogram` will return a `DateTime` object for this method whereas a
+  `histogram` will return a `Number`).
+* A `getKeyAsString()` method has been added to return the String
+  representation of the key.
+* All other `getKeyAsX()` methods have been removed.
+* The `getBucketAsKey(String)` methods have been removed on all aggregations
+  except the `filters` and `terms` aggregations.
+
+
diff --git a/docs/reference/migration/migrate_2_0/crud.asciidoc b/docs/reference/migration/migrate_2_0/crud.asciidoc
new file mode 100644
index 0000000..060cfc7
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/crud.asciidoc
@@ -0,0 +1,129 @@
+=== CRUD and routing changes
+
+==== Explicit custom routing
+
+Custom `routing` values can no longer be extracted from the document body, but
+must be specified explicitly as part of the query string, or in the metadata
+line in the <<docs-bulk,`bulk`>> API.  See <<migration-meta-fields>> for an
+example.
+
+==== Routing hash function
+
+The default hash function that is used for routing has been changed from
+`djb2` to `murmur3`. This change should be transparent unless you relied on
+very specific properties of `djb2`. This will help ensure a better balance of
+the document counts between shards.
+
+In addition, the following routing-related node settings have been deprecated:
+
+`cluster.routing.operation.hash.type`::
+
+  This was an undocumented setting that allowed to configure which hash function
+  to use for routing. `murmur3` is now enforced on new indices.
+
+`cluster.routing.operation.use_type`::
+
+  This was an undocumented setting that allowed to take the `_type` of the
+  document into account when computing its shard (default: `false`). `false` is
+  now enforced on new indices.
+
+==== Delete API with custom routing
+
+The delete API used to be broadcast to all shards in the index which meant
+that, when using custom routing, the `routing` parameter was optional. Now,
+the delete request is forwarded only to the document holding the shard. If you
+are using custom routing then you should specify the `routing` value when
+deleting a document, just as is already required for the `index`, `create`,
+and `update` APIs.
+
+To make sure that you never forget a routing value, make routing required with
+the following mapping:
+
+[source,js]
+---------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "_routing": {
+        "required": true
+      }
+    }
+  }
+}
+---------------------------
+
+==== All stored meta-fields returned by default
+
+Previously, meta-fields like `_routing`, `_timestamp`, etc would only be
+included in a GET request if specifically requested with the `fields`
+parameter.  Now, all meta-fields which have stored values will be returned by
+default.  Additionally, they are now returned at the top level (along with
+`_index`, `_type`, and `_id`) instead of in the `fields` element.
+
+For instance, the following request:
+
+[source,sh]
+---------------
+GET /my_index/my_type/1
+---------------
+
+might return:
+
+[source,js]
+---------------
+{
+  "_index":     "my_index",
+  "_type":      "my_type",
+  "_id":        "1",
+  "_timestamp": 10000000, <1>,
+  "_source": {
+    "foo" : [ "bar" ]
+  }
+}
+---------------
+<1> The `_timestamp` is returned by default, and at the top level.
+
+
+==== Async replication
+
+The `replication` parameter has been removed from all CRUD operations
+(`index`, `create`,  `update`, `delete`, `bulk`) as it interfered with the
+<<indices-synced-flush,synced flush>> feature.  These operations are now
+synchronous only and a request will only return once the changes have been
+replicated to all active shards in the shard group.
+
+Instead, use more client processes to send more requests in parallel.
+
+==== Documents must be specified without a type wrapper
+
+Previously, the document body could be wrapped in another object with the name
+of the `type`:
+
+[source,js]
+--------------------------
+PUT my_index/my_type/1
+{
+  "my_type": { <1>
+    "text": "quick brown fox"
+  }
+}
+--------------------------
+<1> This `my_type` wrapper is not part of the document itself, but represents the document type.
+
+This feature was deprecated before but could be reenabled with the
+`mapping.allow_type_wrapper` index setting.  This setting is no longer
+supported.  The above document should be indexed as follows:
+
+[source,js]
+--------------------------
+PUT my_index/my_type/1
+{
+  "text": "quick brown fox"
+}
+--------------------------
+
+==== Term Vectors API
+
+Usage of `/_termvector` is deprecated in favor of `/_termvectors`.
+
diff --git a/docs/reference/migration/migrate_2_0/index_apis.asciidoc b/docs/reference/migration/migrate_2_0/index_apis.asciidoc
new file mode 100644
index 0000000..ffa2e9e
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/index_apis.asciidoc
@@ -0,0 +1,42 @@
+=== Index API changes
+
+==== Index aliases
+
+
+Fields used in alias filters no longer have to exist in the mapping at alias
+creation time. Previously, alias filters were parsed at alias creation time
+and the parsed form was cached in memory. Now, alias filters are  parsed at
+request time and the fields in filters are resolved from the current mapping.
+
+This also means that index aliases now support `has_parent` and `has_child`
+queries.
+
+The <<alias-retrieving, GET alias api>> will now throw an exception if no
+matching aliases are found. This change brings the defaults for this API in
+line with the other Indices APIs. The <<multi-index>> options can be used on a
+request to change this behavior.
+
+==== File based index templates
+
+Index templates can no longer be configured on disk. Use the
+<<indices-templates,`_template`>> API instead.
+
+==== Analyze API changes
+
+
+The Analyze API now returns the the `position` of the first token as `0`
+instead of `1`.
+
+The `prefer_local` parameter has been removed. The `_analyze` API is a light
+operation and the caller shouldn't be concerned about whether it executes on
+the node that receives the request or another node.
+
+The `text()` method on `AnalyzeRequest` now returns `String[]` instead of
+`String`.
+
+==== Removed `id_cache` from clear cache api
+
+The <<indices-clearcache,clear cache>> API no longer supports the `id_cache`
+option.  Instead, use the `fielddata` option to clear the cache for the
+`_parent` field.
+
diff --git a/docs/reference/migration/migrate_2_0/java.asciidoc b/docs/reference/migration/migrate_2_0/java.asciidoc
new file mode 100644
index 0000000..5f2d2f4
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/java.asciidoc
@@ -0,0 +1,76 @@
+=== Java API changes
+
+==== Transport API construction
+
+The `TransportClient` construction code has changed, it now uses the builder
+pattern. Instead of:
+
+[source,java]
+--------------------------------------------------
+Settings settings = Settings.settingsBuilder()
+        .put("cluster.name", "myClusterName").build();
+Client client = new TransportClient(settings);
+--------------------------------------------------
+
+Use the following:
+
+[source,java]
+--------------------------------------------------
+Settings settings = Settings.settingsBuilder()
+        .put("cluster.name", "myClusterName").build();
+Client client = TransportClient.builder().settings(settings).build();
+--------------------------------------------------
+
+==== Automatically thread client listeners
+
+Previously, the user had to set request listener threads to `true` when on the
+client side in order not to block IO threads on heavy operations. This proved
+to be very trappy for users, and ended up creating problems that are very hard
+to debug.
+
+In 2.0, Elasticsearch automatically threads listeners that are used from the
+client when the client is a node client or a transport client. Threading can
+no longer be manually set.
+
+
+==== Query/filter refactoring
+
+`org.elasticsearch.index.queries.FilterBuilders` has been removed as part of the merge of
+queries and filters. These filters are now available in `QueryBuilders` with the same name.
+All methods that used to accept a `FilterBuilder` now accept a `QueryBuilder` instead.
+
+In addition some query builders have been removed or renamed:
+
+* `commonTerms(...)` renamed with `commonTermsQuery(...)`
+* `queryString(...)` renamed with `queryStringQuery(...)`
+* `simpleQueryString(...)` renamed with `simpleQueryStringQuery(...)`
+* `textPhrase(...)` removed
+* `textPhrasePrefix(...)` removed
+* `textPhrasePrefixQuery(...)` removed
+* `filtered(...)` removed. Use `filteredQuery(...)` instead.
+* `inQuery(...)` removed.
+
+==== GetIndexRequest
+
+`GetIndexRequest.features()` now returns an array of Feature Enums instead of an array of String values.
+
+The following deprecated methods have been removed:
+
+* `GetIndexRequest.addFeatures(String[])` - Use
+  `GetIndexRequest.addFeatures(Feature[])` instead
+
+* `GetIndexRequest.features(String[])` - Use
+  `GetIndexRequest.features(Feature[])` instead.
+
+* `GetIndexRequestBuilder.addFeatures(String[])` - Use
+  `GetIndexRequestBuilder.addFeatures(Feature[])` instead.
+
+* `GetIndexRequestBuilder.setFeatures(String[])` - Use
+  `GetIndexRequestBuilder.setFeatures(Feature[])` instead.
+
+
+==== BytesQueryBuilder removed
+
+The redundant BytesQueryBuilder has been removed in favour of the
+WrapperQueryBuilder internally.
+
diff --git a/docs/reference/migration/migrate_2_0/mapping.asciidoc b/docs/reference/migration/migrate_2_0/mapping.asciidoc
new file mode 100644
index 0000000..a50fc9c
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/mapping.asciidoc
@@ -0,0 +1,390 @@
+=== Mapping changes
+
+A number of changes have been made to mappings to remove ambiguity and to
+ensure that conflicting mappings cannot be created.
+
+One major change is that dynamically added fields must have their mapping
+confirmed by the master node before indexing continues.  This is to avoid a
+problem where different shards in the same index dynamically add different
+mappings for the same field.  These conflicting mappings can silently return
+incorrect results and can lead to index corruption.
+
+This change can make indexing slower when frequently adding many new fields.
+We are looking at ways of optimising this process but we chose safety over
+performance for this extreme use case.
+
+==== Conflicting field mappings
+
+Fields with the same name, in the same index, in different types, must have
+the same mapping, with the exception of the <<copy-to>>, <<dynamic>>,
+<<enabled>>, <<ignore-above>>, <<include-in-all>>, and <<properties>>
+parameters, which may have different settings per field.
+
+[source,js]
+---------------
+PUT my_index
+{
+  "mappings": {
+    "type_one": {
+      "properties": {
+        "name": { <1>
+          "type": "string"
+        }
+      }
+    },
+    "type_two": {
+      "properties": {
+        "name": { <1>
+          "type":     "string",
+          "analyzer": "english"
+        }
+      }
+    }
+  }
+}
+---------------
+<1> The two `name` fields have conflicting mappings and will prevent Elasticsearch
+    from starting.
+
+Elasticsearch will not start in the presence of conflicting field mappings.
+These indices must be deleted or reindexed using a new mapping.
+
+The `ignore_conflicts` option of the put mappings API has been removed.
+Conflicts can't be ignored anymore.
+
+==== Fields cannot be referenced by short name
+
+A field can no longer be referenced using its short name.  Instead, the full
+path to the field is required.  For instance:
+
+[source,js]
+---------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "title":     { "type": "string" }, <1>
+        "name": {
+          "properties": {
+            "title": { "type": "string" }, <2>
+            "first": { "type": "string" },
+            "last":  { "type": "string" }
+          }
+        }
+      }
+    }
+  }
+}
+---------------
+<1> This field is referred to as `title`.
+<2> This field is referred to as `name.title`.
+
+Previously, the two `title` fields in the example above could have been
+confused with each other when using the short name `title`.
+
+==== Type name prefix removed
+
+Previously, two fields with the same name in two different types could
+sometimes be disambiguated by prepending the type name.  As a side effect, it
+would add a filter on the type name to the relevant query.  This feature was
+ambiguous -- a type name could be confused with a field name -- and didn't
+work everywhere e.g. aggregations.
+
+Instead, fields should be specified with the full path, but without a type
+name prefix.  If you wish to filter by the `_type` field, either specify the
+type in the URL or add an explicit filter.
+
+The following example query in 1.x:
+
+[source,js]
+----------------------------
+GET my_index/_search
+{
+  "query": {
+    "match": {
+      "my_type.some_field": "quick brown fox"
+    }
+  }
+}
+----------------------------
+
+would be rewritten in 2.0 as:
+
+[source,js]
+----------------------------
+GET my_index/my_type/_search <1>
+{
+  "query": {
+    "match": {
+      "some_field": "quick brown fox" <2>
+    }
+  }
+}
+----------------------------
+<1> The type name can be specified in the URL to act as a filter.
+<2> The field name should be specified without the type prefix.
+
+==== Field names may not contain dots
+
+In 1.x, it was possible to create fields with dots in their name, for
+instance:
+
+[source,js]
+----------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "properties": {
+        "foo.bar": { <1>
+          "type": "string"
+        },
+        "foo": {
+          "properties": {
+            "bar": { <1>
+              "type": "string"
+            }
+          }
+        }
+      }
+    }
+  }
+}
+----------------------------
+<1> These two fields cannot be distinguised as both are referred to as `foo.bar`.
+
+You can no longer create fields with dots in the name.
+
+==== Type names may not start with a dot
+
+In 1.x, Elasticsearch would issue a warning if a type name included a dot,
+e.g. `my.type`.  Now that type names are no longer used to distinguish between
+fields in differnt types, this warning has been relaxed: type names may now
+contain dots, but they may not *begin* with a dot.  The only exception to this
+is the special `.percolator` type.
+
+==== Types may no longer be deleted
+
+In 1.x it was possible to delete a type mapping, along with all of the
+documents of that type, using the delete mapping API.  This is no longer
+supported, because remnants of the fields in the type could remain in the
+index, causing corruption later on.
+
+Instead, if you need to delete a type mapping, you should reindex to a new
+index which does not contain the mapping.  If you just need to delete the
+documents that belong to that type, then use the delete-by-query plugin
+instead.
+
+[[migration-meta-fields]]
+==== Type meta-fields
+
+The <<mapping-fields,meta-fields>> associated with had configuration options
+removed, to make them more reliable:
+
+* `_id` configuration can no longer be changed.  If you need to sort, use the <<mapping-uid-field,`_uid`>> field instead.
+* `_type` configuration can no longer be changed.
+* `_index` configuration can no longer be changed.
+* `_routing` configuration is limited to marking routing as required.
+* `_field_names` configuration is limited to disabling the field.
+* `_size` configuration is limited to enabling the field.
+* `_timestamp` configuration is limited to enabling the field, setting format and default value.
+* `_boost` has been removed.
+* `_analyzer` has been removed.
+
+Importantly, *meta-fields can no longer be specified as part of the document
+body.*  Instead, they must be specified in the query string parameters.  For
+instance, in 1.x, the `routing` could be specified as follows:
+
+[source,json]
+-----------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "_routing": {
+        "path": "group" <1>
+      },
+      "properties": {
+        "group": { <1>
+          "type": "string"
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1 <2>
+{
+  "group": "foo"
+}
+-----------------------------
+<1> This 1.x mapping tells Elasticsearch to extract the `routing` value from the `group` field in the document body.
+<2> This indexing request uses a `routing` value of `foo`.
+
+In 2.0, the routing must be specified explicitly:
+
+[source,json]
+-----------------------------
+PUT my_index
+{
+  "mappings": {
+    "my_type": {
+      "_routing": {
+        "required": true <1>
+      },
+      "properties": {
+        "group": {
+          "type": "string"
+        }
+      }
+    }
+  }
+}
+
+PUT my_index/my_type/1?routing=bar <2>
+{
+  "group": "foo"
+}
+-----------------------------
+<1> Routing can be marked as required to ensure it is not forgotten during indexing.
+<2> This indexing request uses a `routing` value of `bar`.
+
+==== Analyzer mappings
+
+Previously, `index_analyzer` and `search_analyzer` could be set separately,
+while the `analyzer` setting would set both.  The `index_analyzer` setting has
+been removed in favour of just using the `analyzer` setting.
+
+If just the `analyzer` is set, it will be used at index time and at search time.  To use a different analyzer at search time, specify both the `analyzer` and a `search_analyzer`.
+
+The `index_analyzer`, `search_analyzer`,  and `analyzer` type-level settings
+have also been removed, as is is no longer possible to select fields based on
+the type name.
+
+The `_analyzer` meta-field, which allowed setting an analyzer per document has
+also been removed.  It will be ignored on older indices.
+
+==== Date fields and Unix timestamps
+
+Previously, `date` fields would first try to parse values as a Unix timestamp
+-- milliseconds-since-the-epoch -- before trying to use their defined date
+`format`.  This meant that formats like `yyyyMMdd` could never work, as values
+would be interpreted as timestamps.
+
+In 2.0, we have added two formats: `epoch_millis` and `epoch_second`.  Only
+date fields that use these formats will be able to parse timestamps.
+
+These formats cannot be used in dynamic templates, because they are
+indistinguishable from long values.
+
+==== Default date format
+
+The default date format has changed from `date_optional_time` to
+`strict_date_optional_time`, which expects a 4 digit year, and a 2 digit month
+and day, (and optionally, 2 digit hour, minute, and second).
+
+A dynamically added date field, by default, includes the `epoch_millis`
+format to support timestamp parsing.  For instance:
+
+[source,js]
+-------------------------
+PUT my_index/my_type/1
+{
+  "date_one": "2015-01-01" <1>
+}
+-------------------------
+<1> Has `format`: `"strict_date_optional_time||epoch_millis"`.
+
+[[migration-bool-fields]]
+==== Boolean fields
+
+Boolean fields used to have a string fielddata with `F` meaning `false` and `T`
+meaning `true`. They have been refactored to use numeric fielddata, with `0`
+for `false` and `1` for `true`. As a consequence, the format of the responses of
+the following APIs changed when applied to boolean fields: `0`/`1` is returned
+instead of `F`/`T`:
+
+* <<search-request-fielddata-fields,fielddata fields>>
+* <<search-request-sort,sort values>>
+* <<search-aggregations-bucket-terms-aggregation,terms aggregations>>
+
+In addition, terms aggregations use a custom formatter for boolean (like for
+dates and ip addresses, which are also backed by numbers) in order to return
+the user-friendly representation of boolean fields: `false`/`true`:
+
+[source,js]
+---------------
+"buckets": [
+  {
+     "key": 0,
+     "key_as_string": "false",
+     "doc_count": 42
+  },
+  {
+     "key": 1,
+     "key_as_string": "true",
+     "doc_count": 12
+  }
+]
+---------------
+
+==== `index_name` and `path` removed
+
+The `index_name` setting was used to change the name of the Lucene field,
+and the `path` setting was used on `object` fields to determine whether the
+Lucene field should use the full path (including parent object fields), or
+just the final `name`.
+
+These setting have been removed as their purpose is better served with the
+<<copy-to>> parameter.
+
+==== Murmur3 Fields
+
+Fields of type `murmur3` can no longer change `doc_values` or `index` setting.
+They are always mapped as follows:
+
+[source,js]
+---------------------
+{
+  "type":       "murmur3",
+  "index":      "no",
+  "doc_values": true
+}
+---------------------
+
+==== Mappings in config files not supported
+
+The ability to specify mappings in configuration files has been removed. To
+specify default mappings that apply to multiple indexes, use
+<<indices-templates,index templates>> instead.
+
+Along with this change, the following settings have ben removed:
+
+* `index.mapper.default_mapping_location`
+* `index.mapper.default_percolator_mapping_location`
+
+==== Posting and doc-values codecs
+
+It is no longer possible to specify per-field postings and doc values formats
+in the mappings. This setting will be ignored on indices created before 2.0
+and will cause mapping parsing to fail on indices created on or after 2.0. For
+old indices, this means that new segments will be written with the default
+postings and doc values formats of the current codec.
+
+It is still possible to change the whole codec by using the `index.codec`
+setting. Please however note that using a non-default codec is discouraged as
+it could prevent future versions of Elasticsearch from being able to read the
+index.
+
+==== Compress and compress threshold
+
+The `compress` and `compress_threshold` options have been removed from the
+`_source` field and fields of type `binary`.  These fields are compressed by
+default.  If you would like to increase compression levels, use the new
+<<index-codec,`index.codec: best_compression`>> setting instead.
+
+
+
+
+
diff --git a/docs/reference/migration/migrate_2_0/packaging.asciidoc b/docs/reference/migration/migrate_2_0/packaging.asciidoc
new file mode 100644
index 0000000..2d2e436
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/packaging.asciidoc
@@ -0,0 +1,58 @@
+=== Plugin and packaging changes
+
+==== Symbolic links and paths
+
+Elasticsearch 2.0 runs with the Java security manager enabled and is much more
+restrictive about which paths it is allowed to access.  Various paths can be
+configured, e.g. `path.data`, `path.scripts`, `path.repo`.  A configured path
+may itself be a symbolic link, but no symlinks under that path will be
+followed (with the exception of `path.scripts`, which does follow symlinks).
+
+==== Running `/bin/elasticsearch`
+
+The command line parameter parsing has been rewritten to deal properly with
+spaces in parameters. All config settings can still be specified on the
+command line when starting Elasticsearch, but they must appear after the
+built-in "static parameters", such as `-d` (to daemonize) and `-p` (the PID path).
+
+For instance:
+
+[source,sh]
+-----------
+/bin/elasticsearch -d -p /tmp/foo.pid --http.cors.enabled=true --http.cors.allow-origin='*'
+-----------
+
+For a list of static parameters, run `/bin/elasticsearch -h`
+
+==== `-f` removed
+
+The `-f` parameter, which used to indicate that Elasticsearch should be run in
+the foreground, was deprecated in 1.0 and removed in 2.0.
+
+==== `V` for version
+
+The `-v` parameter now means `--verbose` for both `bin/plugin` and
+`bin/elasticsearch` (although it has no effect on the latter).  To output the
+version, use `-V` or `--version` instead.
+
+==== Plugin manager should run as root
+
+The permissions of the `config`, `bin`, and `plugins` directories in the RPM
+and deb packages have been made more restrictive.  The plugin manager should
+be run as root otherwise it will not be able to install plugins.
+
+==== Support for official plugins
+
+Almost all of the official Elasticsearch plugins have been moved to the main
+`elasticsearch` repository. They will be released at the same time as
+Elasticsearch and have the same version number as Elasticsearch.
+
+Official plugins can be installed as follows:
+
+[source,sh]
+---------------
+sudo bin/plugin install analysis-icu
+---------------
+
+Community-provided plugins can be installed as before.
+
diff --git a/docs/reference/migration/migrate_2_0/parent_child.asciidoc b/docs/reference/migration/migrate_2_0/parent_child.asciidoc
new file mode 100644
index 0000000..fe19861
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/parent_child.asciidoc
@@ -0,0 +1,43 @@
+=== Parent/Child changes
+
+Parent/child has been rewritten completely to reduce memory usage and to
+execute `has_child` and `has_parent` queries faster and more efficient. The
+`_parent` field uses doc values by default. The refactored and improved
+implementation is only active for indices created on or after version 2.0.
+
+In order to benefit from all the performance and memory improvements, we
+recommend reindexing all existing indices that use the `_parent` field.
+
+==== Parent type cannot pre-exist
+
+A mapping type is declared as a child of another mapping type by specifying
+the `_parent` meta field:
+
+[source,js]
+--------------------------
+DELETE *
+
+PUT my_index
+{
+  "mappings": {
+    "my_parent": {},
+    "my_child": {
+      "_parent": {
+        "type": "my_parent" <1>
+      }
+    }
+  }
+}
+--------------------------
+<1> The `my_parent` type is the parent of the `my_child` type.
+
+The mapping for the parent type can be added at the same time as the mapping
+for the child type, but cannot be added before the child type.
+
+==== `top_children` query removed
+
+The `top_children` query has been removed in favour of the `has_child` query.
+It wasn't always faster than the `has_child` query and the was usually
+inaccurate. The total hits and any aggregations in the same search request
+would be incorrect if `top_children` was used.
+
diff --git a/docs/reference/migration/migrate_2_0/query_dsl.asciidoc b/docs/reference/migration/migrate_2_0/query_dsl.asciidoc
new file mode 100644
index 0000000..31283e9
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/query_dsl.asciidoc
@@ -0,0 +1,186 @@
+=== Query DSL changes
+
+==== Queries and filters merged
+
+Queries and filters have been merged -- all filter clauses are now query
+clauses. Instead, query clauses can now be used in _query context_ or in
+_filter context_:
+
+Query context::
+
+A query used in query context will caculated relevance scores and will not be
+cacheable.  Query context is used whenever filter context does not apply.
+
+Filter context::
++
+--
+
+A query used in filter context will not calculate relevance scores, and will
+be cacheable. Filter context is introduced by:
+
+* the `constant_score` query
+* the `must_not` and (newly added) `filter` parameter in the `bool` query
+* the `filter` and `filters` parameters in the `function_score` query
+* any API called `filter`, such as the `post_filter` search parameter, or in
+  aggregations or index aliases
+--
+
+As a result of this change, he `execution` option of the `terms` filter is now
+deprecated and ignored if provided.
+
+==== `or` and `and` now implemented via `bool`
+
+The `or` and `and` filters previously had a different execution pattern to the
+`bool` filter. It used to be important to use `and`/`or` with certain filter
+clauses, and `bool` with others.
+
+This distinction has been removed: the `bool` query is now smart enough to
+handle both cases optimally.  As a result of this change, the `or` and `and`
+filters are now sugar syntax which are executed internally as a `bool` query.
+These filters may be removed in the future.
+
+==== `filtered` query and `query` filter deprecated
+
+The `query` filter is deprecated as is it no longer needed -- all queries can
+be used in query or filter context.
+
+The `filtered` query is deprecated in favour of the `bool` query. Instead of
+the following:
+
+[source,js]
+-------------------------
+GET _search
+{
+  "query": {
+    "filtered": {
+      "query": {
+        "match": {
+          "text": "quick brown fox"
+        }
+      },
+      "filter": {
+        "term": {
+          "status": "published"
+        }
+      }
+    }
+  }
+}
+-------------------------
+
+move the query and filter to the `must` and `filter` parameters in the `bool`
+query:
+
+[source,js]
+-------------------------
+GET _search
+{
+  "query": {
+    "bool": {
+      "must": {
+        "match": {
+          "text": "quick brown fox"
+        }
+      },
+      "filter": {
+        "term": {
+          "status": "published"
+        }
+      }
+    }
+  }
+}
+-------------------------
+
+==== Filter auto-caching
+
+It used to be possible to control which filters were cached with the `_cache`
+option and to provide a custom `_cache_key`.  These options are deprecated
+and, if present, will be ignored.
+
+Query clauses used in filter context are now auto-cached when it makes sense
+to do so.  The algorithm takes into account the frequency of use, the cost of
+query execution, and the cost of building the filter.
+
+The `terms` filter lookup mechanism no longer caches the values of the
+document containing the terms.  It relies on the filesystem cache instead. If
+the lookup index is not too large, it is recommended to replicate it to all
+nodes by setting `index.auto_expand_replicas: 0-all` in order to remove the
+network overhead as well.
+
+==== Numeric queries use IDF for scoring
+
+Previously, term queries on numeric fields were deliberately prevented from
+using the usual Lucene scoring logic and this behaviour was undocumented and,
+to some, unexpected.
+
+Single `term` queries on numeric fields now score in the same way as string
+fields, using IDF and norms (if enabled).
+
+To query numeric fields without scoring, the query clause should be used in
+filter context, e.g. in the `filter` parameter of the `bool` query, or wrapped
+in a `constant_score` query:
+
+[source,js]
+----------------------------
+GET _search
+{
+  "query": {
+    "bool": {
+      "must": [
+        {
+          "match": { <1>
+            "numeric_tag": 5
+          }
+        }
+      ],
+      "filter": [
+        {
+          "match": { <2>
+            "count": 5
+          }
+        }
+      ]
+    }
+  }
+}
+----------------------------
+<1> This clause would include IDF in the relevance score calculation.
+<2> This clause would have no effect on the relevance score.
+
+==== Fuzziness and fuzzy-like-this
+
+Fuzzy matching used to calculate the score for each fuzzy alternative, meaning
+that rare misspellings would have a higher score than the more common correct
+spellings. Now, fuzzy matching blends the scores of all the fuzzy alternatives
+to use the IDF of the most frequently occurring alternative.
+
+Fuzziness can no longer be specified using a percentage, but should instead
+use the number of allowed edits:
+
+* `0`, `1`, `2`, or
+* `AUTO` (which chooses `0`, `1`, or `2` based on the length of the term)
+
+The `fuzzy_like_this` and `fuzzy_like_this_field` queries used a very
+expensive approach to fuzzy matching and have been removed.
+
+==== More Like This
+
+The More Like This (`mlt`) API and the `more_like_this_field` (`mlt_field`)
+query have been removed in favor of the
+<<query-dsl-mlt-query, `more_like_this`>> query.
+
+The parameter `percent_terms_to_match` has been removed in favor of
+`minimum_should_match`.
+
+==== `limit` filter deprecated
+
+The `limit` filter is deprecated and becomes a no-op. You can achieve similar
+behaviour using the <<search-request-body,terminate_after>> parameter.
+
+==== Jave plugins registering custom queries
+
+Java plugins that register custom queries can do so by using the
+`IndicesQueriesModule#addQuery(Class<? extends QueryParser>)` method. Other
+ways to register custom queries are not supported anymore.
+
diff --git a/docs/reference/migration/migrate_2_0/removals.asciidoc b/docs/reference/migration/migrate_2_0/removals.asciidoc
new file mode 100644
index 0000000..60f7422
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/removals.asciidoc
@@ -0,0 +1,68 @@
+=== Removed features
+
+==== Rivers have been removed
+
+Elasticsearch does not support rivers anymore. While we had first planned to
+keep them around to ease migration, keeping support for rivers proved to be
+challenging as it conflicted with other important changes that we wanted to
+bring to 2.0 like synchronous dynamic mappings updates, so we eventually
+decided to remove them entirely. See
+link:/blog/deprecating_rivers[Deprecating Rivers] for more background about
+why we took this decision.
+
+==== Facets have been removed
+
+Facets, deprecated since 1.0, have now been removed.  Instead, use the much
+more powerful and flexible <<search-aggregations,aggregations>> framework.
+This also means that Kibana 3 will not work with Elasticsearch 2.0.
+
+==== Delete-by-query is now a plugin
+
+The old delete-by-query functionality was fast but unsafe.  It could lead to
+document differences between the primary and replica shards, and could even
+produce out of memory exceptions and cause the cluster to crash.
+
+This feature has been reimplemented using the <<scroll-scan,scroll/scan>> and
+the <<docs-bulk,`bulk`>> API, which may be slower for queries which match
+large numbers of documents, but is safe.
+
+Currently, a long running delete-by-query job cannot be cancelled, which is
+one of the reasons that this functionality is only available as a plugin.  You
+can install the plugin with:
+
+[source,sh]
+------------------
+./bin/plugin install delete-by-query
+------------------
+
+
+==== `_shutdown` API
+
+The `_shutdown` API has been removed without a replacement. Nodes should be
+managed via the operating system and the provided start/stop scripts.
+
+==== `_size` is now a plugin
+
+The `_size` meta-data field, which indexes the size in bytes of the original
+JSON document, has been moved out of core and is available as a plugin.  It
+can be installed as:
+
+[source,sh]
+------------------
+./bin/plugin install mapper-size
+------------------
+
+==== Thrift and memcached transport
+
+The thrift and memcached transport plugins are no longer supported.  Instead, use
+either the HTTP transport (enabled by default) or the node or transport Java client.
+
+==== Bulk UDP
+
+The bulk UDP API has been removed.  Instead, use the standard
+<<docs-bulk,`bulk`>> API, or use UDP to send documents to Logstash first.
+
+==== MergeScheduler pluggability
+
+The merge scheduler is no longer pluggable.
+
diff --git a/docs/reference/migration/migrate_2_0/scripting.asciidoc b/docs/reference/migration/migrate_2_0/scripting.asciidoc
new file mode 100644
index 0000000..4964ee0
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/scripting.asciidoc
@@ -0,0 +1,102 @@
+=== Scripting changes
+
+==== Scripting syntax
+
+The syntax for scripts has been made consistent across all APIs. The accepted
+format is as follows:
+
+Inline/Dynamic scripts::
++
+--
+
+[source,js]
+---------------
+"script": {
+  "inline": "doc['foo'].value + val", <1>
+  "lang":   "groovy", <2>
+  "params": { "val": 3 } <3>
+}
+---------------
+<1> The inline script to execute.
+<2> The optional language of the script.
+<3> Any named parameters.
+--
+
+Indexed scripts::
++
+--
+[source,js]
+---------------
+"script": {
+  "id":     "my_script_id", <1>
+  "lang":   "groovy", <2>
+  "params": { "val": 3 } <3>
+}
+---------------
+<1> The ID of the indexed script.
+<2> The optional language of the script.
+<3> Any named parameters.
+--
+
+File scripts::
++
+--
+[source,js]
+---------------
+"script": {
+  "file":   "my_file", <1>
+  "lang":   "groovy", <2>
+  "params": { "val": 3 } <3>
+}
+---------------
+<1> The filename of the script, without the `.lang` suffix.
+<2> The optional language of the script.
+<3> Any named parameters.
+--
+
+For example, an update request might look like this:
+
+[source,js]
+---------------
+POST my_index/my_type/1/_update
+{
+  "script": {
+    "inline": "ctx._source.count += val",
+    "params": { "val": 3 }
+  },
+  "upsert": {
+    "count": 0
+  }
+}
+---------------
+
+A short syntax exists for running inline scripts in the default scripting
+language without any parameters:
+
+[source,js]
+----------------
+GET _search
+{
+  "script_fields": {
+    "concat_fields": {
+      "script": "doc['one'].value + ' ' + doc['two'].value"
+    }
+  }
+}
+----------------
+
+==== Scripting settings
+
+The `script.disable_dynamic` node setting has been replaced by fine-grained
+script settings described in <<migration-script-settings>>.
+
+==== Groovy scripts sandbox
+
+The Groovy sandbox and related settings have been removed. Groovy is now a
+non-sandboxed scripting language, without any option to turn the sandbox on.
+
+==== Plugins making use of scripts
+
+Plugins that make use of scripts must register their own script context
+through `ScriptModule`. Script contexts can be used as part of fine-grained
+settings to enable/disable scripts selectively.
diff --git a/docs/reference/migration/migrate_2_0/search.asciidoc b/docs/reference/migration/migrate_2_0/search.asciidoc
new file mode 100644
index 0000000..b9b5987
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/search.asciidoc
@@ -0,0 +1,121 @@
+=== Search changes
+
+==== Partial fields
+
+Partial fields have been removed in favor of <<search-request-source-filtering,source filtering>>.
+
+==== `search_type=count` deprecated
+
+The `count` search type has been deprecated. All benefits from this search
+type can now be achieved by using the (default) `query_then_fetch` search type
+and setting `size` to `0`.
+
+==== The count api internally uses the search api
+
+The count api is now a shortcut to the search api with `size` set to 0. As a
+result, a total failure will result in an exception being returned rather
+than a normal response with `count` set to `0` and shard failures.
+
+==== All stored meta-fields returned by default
+
+Previously, meta-fields like `_routing`, `_timestamp`, etc would only be
+included in the search results if specifically requested with the `fields`
+parameter.  Now, all meta-fields which have stored values will be returned by
+default.  Additionally, they are now returned at the top level (along with
+`_index`, `_type`, and `_id`) instead of in the `fields` element.
+
+For instance, the following request:
+
+[source,sh]
+---------------
+GET /my_index/_search?fields=foo
+---------------
+
+might return:
+
+[source,js]
+---------------
+{
+   [...]
+   "hits": {
+      "total": 1,
+      "max_score": 1,
+      "hits": [
+         {
+            "_index":     "my_index",
+            "_type":      "my_type",
+            "_id":        "1",
+            "_score":     1,
+            "_timestamp": 10000000, <1>
+            "fields": {
+              "foo" : [ "bar" ]
+            }
+         }
+      ]
+   }
+}
+---------------
+<1> The `_timestamp` is returned by default, and at the top level.
+
+
+==== Script fields
+
+Script fields in 1.x were only returned as a single value. Even if the return
+value of a script was a list, it would be returned as an array containing an
+array:
+
+[source,js]
+---------------
+"fields": {
+  "my_field": [
+    [
+      "v1",
+      "v2"
+    ]
+  ]
+}
+---------------
+
+In elasticsearch 2.0, scripts that return a list of values are treated as
+multivalued fields. The same example would return the following response, with
+values in a single array.
+
+[source,js]
+---------------
+"fields": {
+  "my_field": [
+    "v1",
+    "v2"
+  ]
+}
+---------------
+
+==== Timezone for date field
+
+Specifying the `time_zone` parameter in queries or aggregations on fields of
+type `date` must now be either an ISO 8601 UTC offset, or a timezone id. For
+example, the value `+1:00` must now be written as `+01:00`.
+
+==== Only highlight queried fields
+
+The default value for the `require_field_match` option has changed from
+`false` to `true`, meaning that the highlighters will, by default, only take
+the fields that were queried into account.
+
+This means that, when querying the `_all` field, trying to highlight on any
+field other than `_all`  will produce no highlighted snippets. Querying the
+same fields that need to be highlighted is the cleaner solution to get
+highlighted snippets back. Otherwise `require_field_match` option can be set
+to `false` to ignore field names completely when highlighting.
+
+The postings highlighter doesn't support the `require_field_match` option
+anymore, it will only highlight fields that were queried.
+
+==== Postings highlighter doesn't support `match_phrase_prefix`
+
+The `match` query with type set to `phrase_prefix` (or the
+`match_phrase_prefix` query) is not supported by the postings highlighter. No
+highlighted snippets will be returned.
+
+
+
diff --git a/docs/reference/migration/migrate_2_0/settings.asciidoc b/docs/reference/migration/migrate_2_0/settings.asciidoc
new file mode 100644
index 0000000..b11fb0c
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/settings.asciidoc
@@ -0,0 +1,125 @@
+=== Setting changes
+
+[[migration-script-settings]]
+==== Scripting settings
+
+The `script.disable_dynamic` node setting has been replaced by fine-grained
+script settings described in the <<enable-dynamic-scripting,scripting docs>>.
+The following setting previously used to enable dynamic or inline scripts:
+
+[source,yaml]
+---------------
+script.disable_dynamic: false
+---------------
+
+It should be replaced with the following two settings in `elasticsearch.yml` that
+achieve the same result:
+
+[source,yaml]
+---------------
+script.inline: on
+script.indexed: on
+---------------
+
+==== Units required for time and byte-sized settings
+
+Any settings which accept time or byte values must now be specified with
+units.  For instance, it is too easy to set the `refresh_interval` to 1
+*millisecond* instead of 1 second:
+
+[source,js]
+---------------
+PUT _settings
+{
+  "index.refresh_interval": 1
+}
+---------------
+
+In 2.0, the above request will throw an exception. Instead the refresh
+interval should be set to `"1s"` for one second.
+
+==== Shadow replica settings
+
+The `node.enable_custom_paths` setting has been removed and replaced by the
+`path.shared_data` setting to allow shadow replicas with custom paths to work
+with the security manager. For example, if your previous configuration had:
+
+[source,yaml]
+------
+node.enable_custom_paths: true
+------
+
+And you created an index using shadow replicas with `index.data_path` set to
+`/opt/data/my_index` with the following:
+
+[source,js]
+--------------------------------------------------
+PUT /my_index
+{
+  "index": {
+    "number_of_shards": 1,
+    "number_of_replicas": 4,
+    "data_path": "/opt/data/my_index",
+    "shadow_replicas": true
+  }
+}
+--------------------------------------------------
+
+For 2.0, you will need to set `path.shared_data` to a parent directory of the
+index's data_path, so:
+
+[source,yaml]
+-----------
+path.shared_data: /opt/data
+-----------
+
+==== Resource watcher settings renamed
+
+The setting names for configuring the resource watcher have been renamed
+to prevent clashes with the watcher plugin
+
+* `watcher.enabled` is now `resource.reload.enabled`
+* `watcher.interval` is now `resource.reload.interval`
+* `watcher.interval.low` is now `resource.reload.interval.low`
+* `watcher.interval.medium` is now `resource.reload.interval.medium`
+* `watcher.interval.high` is now `resource.reload.interval.high`
+
+==== Hunspell dictionary configuration
+
+The parameter `indices.analysis.hunspell.dictionary.location` has been
+removed, and `<path.conf>/hunspell` is always used.
+
+==== CORS allowed origins
+
+The CORS allowed origins setting, `http.cors.allow-origin`, no longer has a default value. Previously, the default value
+was `*`, which would allow CORS requests from any origin and is considered insecure. The `http.cors.allow-origin` setting
+should be specified with only the origins that should be allowed, like so:
+
+[source,yaml]
+---------------
+http.cors.allow-origin: /https?:\/\/localhost(:[0-9]+)?/
+---------------
+
+==== JSONP support
+
+JSONP callback support has now been removed. CORS should be used to access Elasticsearch
+over AJAX instead:
+
+[source,yaml]
+---------------
+http.cors.enabled: true
+http.cors.allow-origin: /https?:\/\/localhost(:[0-9]+)?/
+---------------
+
+==== In memory indices
+
+The `memory` / `ram` store (`index.store.type`) option was removed in
+Elasticsearch.  In-memory indices are no longer supported.
+
+==== Log messages truncated
+
+Log messages are now truncated at 10,000 characters. This can be changed in
+the `logging.yml` configuration file with the `file.layout.conversionPattern`
+setting.
+
+Remove mapping.date.round_ceil setting for date math parsing #8889 (issues: #8556, #8598)
diff --git a/docs/reference/migration/migrate_2_0/snapshot_restore.asciidoc b/docs/reference/migration/migrate_2_0/snapshot_restore.asciidoc
new file mode 100644
index 0000000..608cd8a
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/snapshot_restore.asciidoc
@@ -0,0 +1,37 @@
+=== Snapshot and Restore changes
+
+==== File-system repositories must be whitelisted
+
+Locations of the shared file system repositories and the URL repositories with
+`file:` URLs now have to be registered before starting Elasticsearch using the
+`path.repo` setting. The `path.repo` setting can contain one or more
+repository locations:
+
+[source,yaml]
+---------------
+path.repo: ["/mnt/daily", "/mnt/weekly"]
+---------------
+
+If the repository location is specified as an absolute path it has to start
+with one of the locations specified in `path.repo`. If the location is
+specified as a relative path, it will be resolved against the first location
+specified in the `path.repo` setting.
+
+==== URL repositories must be whitelisted
+
+URL repositories with `http:`, `https:`, and `ftp:` URLs have to be
+whitelisted before starting Elasticsearch with the
+`repositories.url.allowed_urls` setting. This setting supports wildcards in
+the place of host, path, query, and fragment. For example:
+
+[source,yaml]
+-----------------------------------
+repositories.url.allowed_urls: ["http://www.example.org/root/*", "https://*.mydomain.com/*?*#*"]
+-----------------------------------
+
+==== Wildcard expansion
+
+The obsolete parameters `expand_wildcards_open` and `expand_wildcards_close`
+are no longer supported by the snapshot and restore operations. These
+parameters have been replaced by a single `expand_wildcards` parameter. See
+<<multi-index,the multi-index docs>> for more.
diff --git a/docs/reference/migration/migrate_2_0/stats.asciidoc b/docs/reference/migration/migrate_2_0/stats.asciidoc
new file mode 100644
index 0000000..46f3c68
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/stats.asciidoc
@@ -0,0 +1,57 @@
+=== Stats, info, and `cat` changes
+
+==== Sigar removed
+
+We no longer ship the Sigar library for operating system dependent statistics,
+as it no longer seems to be maintained.  Instead, we rely on the statistics
+provided by the JVM.  This has resulted in a number of changes to the node
+info, and node stats responses:
+
+* `network.*` has been removed from nodes info and nodes stats.
+* `fs.*.dev` and `fs.*.disk*` have been removed from nodes stats.
+* `os.*` has been removed from nodes stats, except for `os.timestamp`,
+  `os.load_average`, `os.mem.*`, and `os.swap.*`.
+* `os.mem.total` and `os.swap.total` have been removed from nodes info.
+* `process.mem.resident` and `process.mem.share` have been removed from node stats.
+
+==== Removed `id_cache` from stats apis
+
+Removed `id_cache` metric from nodes stats, indices stats and cluster stats
+apis. This metric has also been removed from the shards cat, indices cat and
+nodes cat apis. Parent/child memory is now reported under fielddata, because
+it has internally be using fielddata for a while now.
+
+To just see how much parent/child related field data is taking, the
+`fielddata_fields` option can be used on the stats apis. Indices stats
+example:
+
+[source,js]
+--------------------------------------------------
+GET /_stats/fielddata?fielddata_fields=_parent
+--------------------------------------------------
+
+==== Percolator stats
+
+The total time spent running percolator queries is now called `percolate.time`
+instead of `percolate.get_time`.
+
+==== Cluster state REST API
+
+The cluster state API doesn't return the `routing_nodes` section anymore when
+`routing_table` is requested. The newly introduced `routing_nodes` flag can be
+used separately to control whether `routing_nodes` should be returned.
+
+==== Index status API
+
+The deprecated index status API has been removed.
+
+==== `cat` APIs verbose by default
+
+The `cat` APIs now default to being verbose, which means they output column
+headers by default. Verbosity can be turned off with the `v` parameter:
+
+[source,sh]
+-----------------
+GET _cat/shards?v=0
+-----------------
+
diff --git a/docs/reference/migration/migrate_2_0/striping.asciidoc b/docs/reference/migration/migrate_2_0/striping.asciidoc
new file mode 100644
index 0000000..7e0cc36
--- /dev/null
+++ b/docs/reference/migration/migrate_2_0/striping.asciidoc
@@ -0,0 +1,20 @@
+=== Multiple `data.path` striping
+
+Previously, if the `data.path` setting listed multiple data paths, then a
+shard would be ``striped'' across all paths by writing a whole file to each
+path in turn (in accordance with the `index.store.distributor` setting).  The
+result was that files from a single segment in a shard could be spread across
+multiple disks, and the failure of any one disk could corrupt multiple shards.
+
+This striping is no longer supported.  Instead, different shards may be
+allocated to different paths, but all of the files in a single shard will be
+written to the same path.
+
+If striping is detected while starting Elasticsearch 2.0.0 or later, *all of
+the files belonging to the same shard will be migrated to the same path*. If
+there is not enough disk space to complete this migration, the upgrade will be
+cancelled and can only be resumed once enough disk space is made available.
+
+The `index.store.distributor` setting has also been removed.
+
+
diff --git a/docs/reference/migration/migrate_query_refactoring.asciidoc b/docs/reference/migration/migrate_query_refactoring.asciidoc
deleted file mode 100644
index 970b375..0000000
--- a/docs/reference/migration/migrate_query_refactoring.asciidoc
+++ /dev/null
@@ -1,68 +0,0 @@
-[[breaking-changes query-refactoring]]
-== Breaking changes on the query-refactoring branch
-
-This section discusses changes that are breaking to the current rest or java-api
-on the query-refactoring feature branch.
-
-=== Plugins
-
-Plugins implementing custom queries need to implement the `fromXContent(QueryParseContext)` method in their
-`QueryParser` subclass rather than `parse`. This method will take care of parsing the query from `XContent` format
-into an intermediate query representation that can be streamed between the nodes in binary format, effectively the
-query object used in the java api. Also, the query parser needs to implement the `getBuilderPrototype` method that
-returns a prototype of the streamable query, which allows to deserialize an incoming query by calling
-`readFrom(StreamInput)` against it, which will create a new object, see usages of `Writeable`. The `QueryParser`
-also needs to declare the generic type of the query that it supports and it's able to parse.
-The query object can then transform itself into a lucene query through the new `toQuery(QueryShardContext)` method,
-which returns a lucene query to be executed on the data node. The query implementation also needs to implement the
-`validate` method that allows to validate the content of the query, no matter whether it came in through the java api
-directly or through the REST layer.
-
-=== Java-API
-
-==== BoostingQueryBuilder
-
-Removed setters for mandatory positive/negative query. Both arguments now have
-to be supplied at construction time already and have to be non-null.
-
-==== SpanContainingQueryBuilder
-
-Removed setters for mandatory big/little inner span queries. Both arguments now have
-to be supplied at construction time already and have to be non-null. Updated
-static factory methods in QueryBuilders accordingly.
-
-==== SpanNearQueryBuilder
-
-Removed setter for mandatory slop parameter, needs to be set in constructor now.
-Updated the static factory methods in QueryBuilders accordingly.
-
-==== SpanNotQueryBuilder
-
-Removed setter for mandatory include/exclude span query clause, needs to be set in constructor now.
-Updated the static factory methods in QueryBuilders and tests accordingly.
-
-==== SpanWithinQueryBuilder
-
-Removed setters for mandatory big/little inner span queries. Both arguments now have
-to be supplied at construction time already and have to be non-null. Updated
-static factory methods in QueryBuilders accordingly.
-
-==== QueryFilterBuilder
-
-Removed the setter `queryName(String queryName)` since this field is not supported
-in this type of query. Use `FQueryFilterBuilder.queryName(String queryName)` instead 
-when in need to wrap a named query as a filter.
-
-==== Operator
-
-Removed the enums called `Operator` from `MatchQueryBuilder`, `QueryStringQueryBuilder`,
-`SimpleQueryStringBuilder`, and `CommonTermsQueryBuilder` in favour of using the enum
-defined in `org.elasticsearch.index.query.Operator` in an effort to consolidate the
-codebase and avoid duplication.
-
-==== queryName and boost support
-
-Support for `queryName` and `boost` has been streamlined to all of the queries. That is
-a breaking change till queries get sent over the network as serialized json rather
-than in `Streamable` format. In fact whenever additional fields are added to the json
-representation of the query, older nodes might throw error when they find unknown fields.
diff --git a/docs/reference/modules/plugins.asciidoc b/docs/reference/modules/plugins.asciidoc
index 5846a53..240f984 100644
--- a/docs/reference/modules/plugins.asciidoc
+++ b/docs/reference/modules/plugins.asciidoc
@@ -9,288 +9,4 @@ custom manner. They range from adding custom mapping types, custom
 analyzers (in a more built in fashion), native scripts, custom discovery
 and more.
 
-[float]
-[[installing]]
-==== Installing plugins
-
-Installing plugins can either be done manually by placing them under the
-`plugins` directory, or using the `plugin` script.
-
-Installing plugins typically take the following form:
-
-[source,sh]
------------------------------------
-bin/plugin install plugin_name
------------------------------------
-
-The plugin will be automatically downloaded in this case from `download.elastic.co` download service using the
-same version as your elasticsearch version.
-
-For older version of elasticsearch (prior to 2.0.0) or community plugins, you would use the following form:
-
-[source,sh]
------------------------------------
-bin/plugin install <org>/<user/component>/<version>
------------------------------------
-
-The plugins will be automatically downloaded in this case from `download.elastic.co` (for older plugins),
-and in case they don't exist there, from maven (central and sonatype).
-
-Note that when the plugin is located in maven central or sonatype
-repository, `<org>` is the artifact `groupId` and `<user/component>` is
-the `artifactId`.
-
-A plugin can also be installed directly by specifying the URL for it,
-for example:
-
-[source,sh]
------------------------------------
-bin/plugin install plugin-name --url file:///path/to/plugin
------------------------------------
-
-
-You can run `bin/plugin -h`, or `bin/plugin install -h` for help on the install command
-as well as `bin/plugin remove -h` for help on the remove command..
-
-[float]
-[[site-plugins]]
-==== Site Plugins
-
-Plugins can have "sites" in them, any plugin that exists under the
-`plugins` directory with a `_site` directory, its content will be
-statically served when hitting `/_plugin/[plugin_name]/` url. Those can
-be added even after the process has started.
-
-Installed plugins that do not contain any java related content, will
-automatically be detected as site plugins, and their content will be
-moved under `_site`.
-
-The ability to install plugins from Github allows to easily install site
-plugins hosted there by downloading the actual repo, for example,
-running:
-
-[source,js]
---------------------------------------------------
-bin/plugin install mobz/elasticsearch-head
-bin/plugin install lukas-vlcek/bigdesk
---------------------------------------------------
-
-Will install both of those site plugins, with `elasticsearch-head`
-available under `http://localhost:9200/_plugin/head/` and `bigdesk`
-available under `http://localhost:9200/_plugin/bigdesk/`.
-
-[float]
-==== Mandatory Plugins
-
-If you rely on some plugins, you can define mandatory plugins using the
-`plugin.mandatory` attribute, for example, here is a sample config:
-
-[source,js]
---------------------------------------------------
-plugin.mandatory: mapper-attachments,lang-groovy
---------------------------------------------------
-
-For safety reasons, if a mandatory plugin is not installed, the node
-will not start.
-
-[float]
-==== Installed Plugins
-
-A list of the currently loaded plugins can be retrieved using the
-<<cluster-nodes-info,Nodes Info API>>.
-
-[float]
-==== Removing plugins
-
-Removing plugins can either be done manually by removing them under the
-`plugins` directory, or using the `plugin` script.
-
-Removing plugins typically take the following form:
-
-[source,sh]
------------------------------------
-plugin remove <pluginname>
------------------------------------
-
-[float]
-==== Silent/Verbose mode
-
-When running the `plugin` script, you can get more information (debug mode) using `--verbose`.
-On the opposite, if you want `plugin` script to be silent, use `--silent` option.
-
-Note that exit codes could be:
-
-* `0`: everything was OK
-* `64`: unknown command or incorrect option parameter
-* `74`: IO error
-* `70`: other errors
-
-[source,sh]
------------------------------------
-bin/plugin install mobz/elasticsearch-head --verbose
-plugin remove head --silent
------------------------------------
-
-[float]
-==== Timeout settings
-
-By default, the `plugin` script will wait indefinitely when downloading before failing.
-The timeout parameter can be used to explicitly specify how long it waits. Here is some examples of setting it to
-different values:
-
-[source,sh]
------------------------------------
-# Wait for 30 seconds before failing
-bin/plugin install mobz/elasticsearch-head --timeout 30s
-
-# Wait for 1 minute before failing
-bin/plugin install mobz/elasticsearch-head --timeout 1m
-
-# Wait forever (default)
-bin/plugin install mobz/elasticsearch-head --timeout 0
------------------------------------
-
-[float]
-==== Proxy settings
-
-
-To install a plugin via a proxy, you can pass the proxy details using the environment variables `proxyHost` and `proxyPort`.
-
-[source,sh]
------------------------------------
-set JAVA_OPTS="-DproxyHost=host_name -DproxyPort=port_number"
-bin/plugin install mobz/elasticsearch-head
------------------------------------
-
-[float]
-==== Lucene version dependent plugins
-
-For some plugins, such as analysis plugins, a specific major Lucene version is
-required to run. In that case, the plugin provides in its `es-plugin.properties`
-file the Lucene version for which the plugin was built for.
-
-If present at startup the node will check the Lucene version before loading the plugin.
-
-You can disable that check using `plugins.check_lucene: false`.
-
-[float]
-[[known-plugins]]
-=== Known Plugins
-
-[float]
-[[analysis-plugins]]
-==== Analysis Plugins
-
-.Supported by Elasticsearch
-* https://github.com/elasticsearch/elasticsearch-analysis-icu[ICU Analysis plugin]
-* https://github.com/elasticsearch/elasticsearch-analysis-kuromoji[Japanese (Kuromoji) Analysis plugin].
-* https://github.com/elasticsearch/elasticsearch-analysis-smartcn[Smart Chinese Analysis Plugin]
-* https://github.com/elasticsearch/elasticsearch-analysis-stempel[Stempel (Polish) Analysis plugin]
-
-.Supported by the community
-* https://github.com/barminator/elasticsearch-analysis-annotation[Annotation Analysis Plugin] (by Michal Samek)
-* https://github.com/yakaz/elasticsearch-analysis-combo/[Combo Analysis Plugin] (by Olivier Favre, Yakaz)
-* https://github.com/jprante/elasticsearch-analysis-hunspell[Hunspell Analysis Plugin] (by Jörg Prante)
-* https://github.com/medcl/elasticsearch-analysis-ik[IK Analysis Plugin] (by Medcl)
-* https://github.com/suguru/elasticsearch-analysis-japanese[Japanese Analysis plugin] (by suguru).
-* https://github.com/medcl/elasticsearch-analysis-mmseg[Mmseg Analysis Plugin] (by Medcl)
-* https://github.com/chytreg/elasticsearch-analysis-morfologik[Morfologik (Polish) Analysis plugin] (by chytreg)
-* https://github.com/imotov/elasticsearch-analysis-morphology[Russian and English Morphological Analysis Plugin] (by Igor Motov)
-* https://github.com/synhershko/elasticsearch-analysis-hebrew[Hebrew Analysis Plugin] (by Itamar Syn-Hershko)
-* https://github.com/medcl/elasticsearch-analysis-pinyin[Pinyin Analysis Plugin] (by Medcl)
-* https://github.com/medcl/elasticsearch-analysis-string2int[String2Integer Analysis Plugin] (by Medcl)
-* https://github.com/duydo/elasticsearch-analysis-vietnamese[Vietnamese Analysis Plugin] (by Duy Do)
-
-[float]
-[[discovery-plugins]]
-==== Discovery Plugins
-
-.Supported by Elasticsearch
-* https://github.com/elasticsearch/elasticsearch-cloud-aws[AWS Cloud Plugin] - EC2 discovery and S3 Repository
-* https://github.com/elasticsearch/elasticsearch-cloud-azure[Azure Cloud Plugin] - Azure discovery
-* https://github.com/elasticsearch/elasticsearch-cloud-gce[Google Compute Engine Cloud Plugin] - GCE discovery
-
-.Supported by the community
-* https://github.com/shikhar/eskka[eskka Discovery Plugin] (by Shikhar Bhushan)
-* https://github.com/grantr/elasticsearch-srv-discovery[DNS SRV Discovery Plugin] (by Grant Rodgers)
-
-[float]
-[[transport]]
-==== Transport Plugins
-
-.Supported by Elasticsearch
-* https://github.com/elasticsearch/elasticsearch-transport-wares[Servlet transport]
-
-.Supported by the community
-* https://github.com/tlrx/transport-zeromq[ZeroMQ transport layer plugin] (by Tanguy Leroux)
-* https://github.com/sonian/elasticsearch-jetty[Jetty HTTP transport plugin] (by Sonian Inc.)
-* https://github.com/kzwang/elasticsearch-transport-redis[Redis transport plugin] (by Kevin Wang)
-
-[float]
-[[scripting]]
-==== Scripting Plugins
-
-.Supported by Elasticsearch
-* https://github.com/elasticsearch/elasticsearch-lang-groovy[Groovy lang Plugin]
-* https://github.com/elasticsearch/elasticsearch-lang-javascript[JavaScript language Plugin]
-* https://github.com/elasticsearch/elasticsearch-lang-python[Python language Plugin]
-
-.Supported by the community
-* https://github.com/hiredman/elasticsearch-lang-clojure[Clojure Language Plugin] (by Kevin Downey)
-* https://github.com/NLPchina/elasticsearch-sql/[SQL language Plugin] (by nlpcn)
-
-[float]
-[[site]]
-==== Site Plugins
-
-.Supported by the community
-* https://github.com/lukas-vlcek/bigdesk[BigDesk Plugin] (by Lukáš Vlček)
-* https://github.com/mobz/elasticsearch-head[Elasticsearch Head Plugin] (by Ben Birch)
-* https://github.com/royrusso/elasticsearch-HQ[Elasticsearch HQ] (by Roy Russo)
-* https://github.com/andrewvc/elastic-hammer[Hammer Plugin] (by Andrew Cholakian)
-* https://github.com/polyfractal/elasticsearch-inquisitor[Inquisitor Plugin] (by Zachary Tong)
-* https://github.com/karmi/elasticsearch-paramedic[Paramedic Plugin] (by Karel Minařík)
-* https://github.com/polyfractal/elasticsearch-segmentspy[SegmentSpy Plugin] (by Zachary Tong)
-* https://github.com/xyu/elasticsearch-whatson[Whatson Plugin] (by Xiao Yu)
-* https://github.com/lmenezes/elasticsearch-kopf[Kopf Plugin] (by lmenezes)
-
-[float]
-[[repository-plugins]]
-==== Snapshot/Restore Repository Plugins
-
-.Supported by Elasticsearch
-
-* https://github.com/elasticsearch/elasticsearch-hadoop/tree/master/repository-hdfs[Hadoop HDFS] Repository
-* https://github.com/elasticsearch/elasticsearch-cloud-aws#s3-repository[AWS S3] Repository
-
-.Supported by the community
-
-* https://github.com/kzwang/elasticsearch-repository-gridfs[GridFS] Repository (by Kevin Wang)
-* https://github.com/wikimedia/search-repository-swift[Openstack Swift]
-
-[float]
-[[misc]]
-==== Misc Plugins
-
-.Supported by Elasticsearch
-* https://github.com/elasticsearch/elasticsearch-mapper-attachments[Mapper Attachments Type plugin]
-
-.Supported by the community
-* https://github.com/carrot2/elasticsearch-carrot2[carrot2 Plugin]: Results clustering with carrot2 (by Dawid Weiss)
-* https://github.com/derryx/elasticsearch-changes-plugin[Elasticsearch Changes Plugin] (by Thomas Peuss)
-* https://github.com/johtani/elasticsearch-extended-analyze[Extended Analyze Plugin] (by Jun Ohtani)
-* https://github.com/YannBrrd/elasticsearch-entity-resolution[Entity Resolution Plugin] using http://github.com/larsga/Duke[Duke] for duplication detection (by Yann Barraud)
-* https://github.com/spinscale/elasticsearch-graphite-plugin[Elasticsearch Graphite Plugin] (by Alexander Reelsen)
-* https://github.com/mattweber/elasticsearch-mocksolrplugin[Elasticsearch Mock Solr Plugin] (by Matt Weber)
-* https://github.com/viniciusccarvalho/elasticsearch-newrelic[Elasticsearch New Relic Plugin] (by Vinicius Carvalho)
-* https://github.com/swoop-inc/elasticsearch-statsd-plugin[Elasticsearch Statsd Plugin] (by Swoop Inc.)
-* https://github.com/endgameinc/elasticsearch-term-plugin[Terms Component Plugin] (by Endgame Inc.)
-* http://tlrx.github.com/elasticsearch-view-plugin[Elasticsearch View Plugin] (by Tanguy Leroux)
-* https://github.com/sonian/elasticsearch-zookeeper[ZooKeeper Discovery Plugin] (by Sonian Inc.)
-* https://github.com/kzwang/elasticsearch-image[Elasticsearch Image Plugin] (by Kevin Wang)
-* https://github.com/wikimedia/search-highlighter[Elasticsearch Experimental Highlighter] (by Wikimedia Foundation/Nik Everett)
-* https://github.com/wikimedia/search-extra[Elasticsearch Trigram Accelerated Regular Expression Filter] (by Wikimedia Foundation/Nik Everett)
-* https://github.com/salyh/elasticsearch-security-plugin[Elasticsearch Security Plugin] (by Hendrik Saly)
-* https://github.com/codelibs/elasticsearch-taste[Elasticsearch Taste Plugin] (by CodeLibs Project)
-* http://siren.solutions/siren/downloads/[Elasticsearch SIREn Plugin]: Nested data search (by SIREn Solutions)
-
+See the {plugins}/index.html[Plugins documentation] for more.
diff --git a/docs/reference/setup/dir-layout.asciidoc b/docs/reference/setup/dir-layout.asciidoc
index 208c6d5..85b7ae1 100644
--- a/docs/reference/setup/dir-layout.asciidoc
+++ b/docs/reference/setup/dir-layout.asciidoc
@@ -17,7 +17,9 @@ on the node. Can hold multiple locations. | {path.home}/data| path.data
 
 | plugins | Plugin files location. Each plugin will be contained in a subdirectory. | {path.home}/plugins | path.plugins
 
-| repo | Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here. | empty | path.repo
+| repo | Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here. d| Not configured | path.repo
+
+| script | Location of script files. | {path.conf}/scripts | path.script
 
 |=======================================================================
 
@@ -63,6 +65,11 @@ on the node. | /var/lib/elasticsearch/data | /var/lib/elasticsearch
 | logs | Log files location | /var/log/elasticsearch | /var/log/elasticsearch
 
 | plugins | Plugin files location. Each plugin will be contained in a subdirectory. | /usr/share/elasticsearch/plugins | /usr/share/elasticsearch/plugins
+
+| repo | Shared file system repository locations.  d| Not configured d| Not configured
+
+| script | Location of script files. | /etc/elasticsearch/scripts | /etc/elasticsearch/scripts
+
 |=======================================================================
 
 [float]
@@ -84,4 +91,9 @@ on the node | {extract.path}/data
 | logs | Log files location | {extract.path}/logs
 
 | plugins | Plugin files location. Each plugin will be contained in a subdirectory | {extract.path}/plugins
+
+| repo | Shared file system repository locations.  d| Not configured
+
+| script | Location of script files. | {extract.path}/config/scripts
+
 |=======================================================================
diff --git a/docs/river/couchdb.asciidoc b/docs/river/couchdb.asciidoc
deleted file mode 100644
index 5a2555e..0000000
--- a/docs/river/couchdb.asciidoc
+++ /dev/null
@@ -1,11 +0,0 @@
-[[river-couchdb]]
-== CouchDB River
-
-The CouchDB River allows to automatically index couchdb and make it
-searchable using the excellent
-http://guide.couchdb.org/draft/notifications.html[_changes] stream
-couchdb provides.
-
-See
-https://github.com/elasticsearch/elasticsearch-river-couchdb/blob/master/README.md[README
-file] for details.
diff --git a/docs/river/index.asciidoc b/docs/river/index.asciidoc
index 47233db..6ccbbb9 100644
--- a/docs/river/index.asciidoc
+++ b/docs/river/index.asciidoc
@@ -1,79 +1,6 @@
 [[river]]
 = Rivers
 
-== Intro
-
-deprecated[1.5.0,Rivers have been deprecated.  See https://www.elastic.co/blog/deprecating_rivers for more details]
-
-A river is a pluggable service running within elasticsearch cluster
-pulling data (or being pushed with data) that is then indexed into the
-cluster.
-
-A river is composed of a unique name and a type. The type is the type of
-the river (out of the box, there is the `dummy` river that simply logs
-that it is running). The name uniquely identifies the river within the
-cluster. For example, one can run a river called `my_river` with type
-`dummy`, and another river called `my_other_river` with type `dummy`.
-
-
-[[how-it-works]]
-== How it Works
-
-A river instance (and its name) is a type within the `_river` index. All
-different rivers implementations accept a document called `_meta` that
-at the very least has the type of the river (twitter / couchdb / ...)
-associated with it. Creating a river is a simple curl request to index
-that `_meta` document (there is actually a `dummy` river used for
-testing):
-
-[source,js]
---------------------------------------------------
-curl -XPUT 'localhost:9200/_river/my_river/_meta' -d '{
-    "type" : "dummy"
-}'
---------------------------------------------------
-
-A river can also have more data associated with it in the form of more
-documents indexed under the given index type (the river name). For
-example, storing the last indexed state can be stored in a document that
-holds it.
-
-Deleting a river is a call to delete the type (and all documents
-associated with it):
-
-[source,js]
---------------------------------------------------
-curl -XDELETE 'localhost:9200/_river/my_river/'
---------------------------------------------------
-
-
-[[allocation]]
-== Cluster Allocation
-
-Rivers are singletons within the cluster. They get allocated
-automatically to one of the nodes and run. If that node fails, a river
-will be automatically allocated to another node.
-
-River allocation on nodes can be controlled on each node. The
-`node.river` can be set to `_none_` disabling any river allocation to
-it. The `node.river` can also include a comma separated list of either
-river names or types controlling the rivers allowed to run on it. For
-example: `my_river1,my_river2`, or `dummy,twitter`.
-
-
-[[status]]
-== Status
-
-Each river (regardless of the implementation) exposes a high level
-`_status` doc which includes the node the river is running on. Getting
-the status is a simple curl GET request to
-`/_river/{river name}/_status`.
-
-include::couchdb.asciidoc[]
-
-include::rabbitmq.asciidoc[]
-
-include::twitter.asciidoc[]
-
-include::wikipedia.asciidoc[]
+Rivers were deprecated in Elasticsearch 1.5 and removed in Elasticsearch 2.0.
 
+See https://www.elastic.co/blog/deprecating_rivers for more details.
\ No newline at end of file
diff --git a/docs/river/rabbitmq.asciidoc b/docs/river/rabbitmq.asciidoc
deleted file mode 100644
index cdad9f8..0000000
--- a/docs/river/rabbitmq.asciidoc
+++ /dev/null
@@ -1,9 +0,0 @@
-[[river-rabbitmq]]
-== RabbitMQ River
-
-RabbitMQ River allows to automatically index a
-http://www.rabbitmq.com/[RabbitMQ] queue.
-
-See
-https://github.com/elasticsearch/elasticsearch-river-rabbitmq/blob/master/README.md[README
-file] for details.
diff --git a/docs/river/twitter.asciidoc b/docs/river/twitter.asciidoc
deleted file mode 100644
index 355c187..0000000
--- a/docs/river/twitter.asciidoc
+++ /dev/null
@@ -1,10 +0,0 @@
-[[river-twitter]]
-== Twitter River
-
-The twitter river indexes the public
-http://dev.twitter.com/pages/streaming_api[twitter stream], aka the
-hose, and makes it searchable.
-
-See
-https://github.com/elasticsearch/elasticsearch-river-twitter/blob/master/README.md[README
-file] for details.
diff --git a/docs/river/wikipedia.asciidoc b/docs/river/wikipedia.asciidoc
deleted file mode 100644
index c65107a..0000000
--- a/docs/river/wikipedia.asciidoc
+++ /dev/null
@@ -1,8 +0,0 @@
-[[river-wikipedia]]
-== Wikipedia River
-
-A simple river to index http://en.wikipedia.org[Wikipedia].
-
-See
-https://github.com/elasticsearch/elasticsearch-river-wikipedia/blob/master/README.md[README
-file] for details.
diff --git a/plugins/analysis-icu/README.md b/plugins/analysis-icu/README.md
deleted file mode 100644
index dc82d21..0000000
--- a/plugins/analysis-icu/README.md
+++ /dev/null
@@ -1,290 +0,0 @@
-ICU Analysis for Elasticsearch
-==================================
-
-The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.
-
-In order to install the plugin, simply run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-analysis-icu/2.5.0
-```
-
-You need to install a version matching your Elasticsearch version:
-
-| elasticsearch |  ICU Analysis Plugin  |   Docs     |  
-|---------------|-----------------------|------------|
-| master        |  Build from source    | See below  |
-| es-1.x        |  Build from source    | [2.6.0-SNAPSHOT](https://github.com/elastic/elasticsearch-analysis-icu/tree/es-1.x/#version-260-snapshot-for-elasticsearch-1x)  |
-| es-1.5        |  2.5.0                | [2.5.0](https://github.com/elastic/elasticsearch-analysis-icu/tree/v2.5.0/#version-250-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.3         | [2.4.3](https://github.com/elasticsearch/elasticsearch-analysis-icu/tree/v2.4.3/#version-243-for-elasticsearch-14)                  |
-| < 1.4.5       |  2.4.2                | [2.4.2](https://github.com/elastic/elasticsearch-analysis-icu/tree/v2.4.2/#version-242-for-elasticsearch-14)                  |
-| < 1.4.3       |  2.4.1                | [2.4.1](https://github.com/elastic/elasticsearch-analysis-icu/tree/v2.4.1/#version-241-for-elasticsearch-14)                  |
-| es-1.3        |  2.3.0                | [2.3.0](https://github.com/elastic/elasticsearch-analysis-icu/tree/v2.3.0/#icu-analysis-for-elasticsearch)  |
-| es-1.2        |  2.2.0                | [2.2.0](https://github.com/elastic/elasticsearch-analysis-icu/tree/v2.2.0/#icu-analysis-for-elasticsearch)  |
-| es-1.1        |  2.1.0                | [2.1.0](https://github.com/elastic/elasticsearch-analysis-icu/tree/v2.1.0/#icu-analysis-for-elasticsearch)  |
-| es-1.0        |  2.0.0                | [2.0.0](https://github.com/elastic/elasticsearch-analysis-icu/tree/v2.0.0/#icu-analysis-for-elasticsearch)  |
-| es-0.90       |  1.13.0               | [1.13.0](https://github.com/elastic/elasticsearch-analysis-icu/tree/v1.13.0/#icu-analysis-for-elasticsearch)  |
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install analysis-icu \
-       --url file:target/releases/elasticsearch-analysis-icu-X.X.X-SNAPSHOT.zip
-```
-
-
-ICU Normalization
------------------
-
-Normalizes characters as explained [here](http://userguide.icu-project.org/transforms/normalization). It registers itself by default under `icu_normalizer` or `icuNormalizer` using the default settings. Allows for the name parameter to be provided which can include the following values: `nfc`, `nfkc`, and `nfkc_cf`. Here is a sample settings:
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "normalized" : {
-                    "tokenizer" : "keyword",
-                    "filter" : ["icu_normalizer"]
-                }
-            }
-        }
-    }
-}
-```
-
-ICU Folding
------------
-
-Folding of unicode characters based on `UTR#30`. It registers itself under `icu_folding` and `icuFolding` names. Sample setting:
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "folded" : {
-                    "tokenizer" : "keyword",
-                    "filter" : ["icu_folding"]
-                }
-            }
-        }
-    }
-}
-```
-
-ICU Filtering
--------------
-
-The folding can be filtered by a set of unicode characters with the parameter `unicodeSetFilter`. This is useful for a
-non-internationalized search engine where retaining a set of national characters which are primary letters in a specific
-language is wanted. See syntax for the UnicodeSet [here](http://icu-project.org/apiref/icu4j/com/ibm/icu/text/UnicodeSet.html).
-
-The Following example exempts Swedish characters from the folding. Note that the filtered characters are NOT lowercased which is why we add that filter below.
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "folding" : {
-                    "tokenizer" : "standard",
-                    "filter" : ["my_icu_folding", "lowercase"]
-                }
-            }
-            "filter" : {
-                "my_icu_folding" : {
-                    "type" : "icu_folding"
-                    "unicodeSetFilter" : "[^åäöÅÄÖ]"
-                }
-            }
-        }
-    }
-}
-```
-
-ICU Collation
--------------
-
-Uses collation token filter. Allows to either specify the rules for collation
-(defined [here](http://www.icu-project.org/userguide/Collate_Customization.html)) using the `rules` parameter
-(can point to a location or expressed in the settings, location can be relative to config location), or using the
-`language` parameter (further specialized by country and variant). By default registers under `icu_collation` or
-`icuCollation` and uses the default locale.
-
-Here is a sample settings:
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "collation" : {
-                    "tokenizer" : "keyword",
-                    "filter" : ["icu_collation"]
-                }
-            }
-        }
-    }
-}
-```
-
-And here is a sample of custom collation:
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "collation" : {
-                    "tokenizer" : "keyword",
-                    "filter" : ["myCollator"]
-                }
-            },
-            "filter" : {
-                "myCollator" : {
-                    "type" : "icu_collation",
-                    "language" : "en"
-                }
-            }
-        }
-    }
-}
-```
-
-Optional options:
-* `strength` - The strength property determines the minimum level of difference considered significant during comparison.
- The default strength for the Collator is `tertiary`, unless specified otherwise by the locale used to create the Collator.
- Possible values: `primary`, `secondary`, `tertiary`, `quaternary` or `identical`.
- See [ICU Collation](http://icu-project.org/apiref/icu4j/com/ibm/icu/text/Collator.html) documentation for a more detailed
- explanation for the specific values.
-* `decomposition` - Possible values: `no` or `canonical`. Defaults to `no`. Setting this decomposition property with
-`canonical` allows the Collator to handle un-normalized text properly, producing the same results as if the text were
-normalized. If `no` is set, it is the user's responsibility to insure that all text is already in the appropriate form
-before a comparison or before getting a CollationKey. Adjusting decomposition mode allows the user to select between
-faster and more complete collation behavior. Since a great many of the world's languages do not require text
-normalization, most locales set `no` as the default decomposition mode.
-
-Expert options:
-* `alternate` - Possible values: `shifted` or `non-ignorable`. Sets the alternate handling for strength `quaternary`
- to be either shifted or non-ignorable. What boils down to ignoring punctuation and whitespace.
-* `caseLevel` - Possible values: `true` or `false`. Default is `false`. Whether case level sorting is required. When
- strength is set to `primary` this will ignore accent differences.
-* `caseFirst` - Possible values: `lower` or `upper`. Useful to control which case is sorted first when case is not ignored
- for strength `tertiary`.
-* `numeric` - Possible values: `true` or `false`. Whether digits are sorted according to numeric representation. For
- example the value `egg-9` is sorted before the value `egg-21`. Defaults to `false`.
-* `variableTop` - Single character or contraction. Controls what is variable for `alternate`.
-* `hiraganaQuaternaryMode` - Possible values: `true` or `false`. Defaults to `false`. Distinguishing between Katakana
- and Hiragana characters in `quaternary` strength .
-
-ICU Tokenizer
--------------
-
-Breaks text into words according to [UAX #29: Unicode Text Segmentation](http://www.unicode.org/reports/tr29/).
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "tokenized" : {
-                    "tokenizer" : "icu_tokenizer",
-                }
-            }
-        }
-    }
-}
-```
-
-
-ICU Normalization CharFilter
------------------
-
-Normalizes characters as explained [here](http://userguide.icu-project.org/transforms/normalization).
-It registers itself by default under `icu_normalizer` or `icuNormalizer` using the default settings.
-Allows for the name parameter to be provided which can include the following values: `nfc`, `nfkc`, and `nfkc_cf`.
-Allows for the mode parameter to be provided which can include the following values: `compose` and `decompose`.
-Use `decompose` with `nfc` or `nfkc`, to get `nfd` or `nfkd`, respectively.
-Here is a sample settings:
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "normalized" : {
-                    "tokenizer" : "keyword",
-                    "char_filter" : ["icu_normalizer"]
-                }
-            }
-        }
-    }
-}
-```
-
-ICU Transform
--------------
-Transforms are used to process Unicode text in many different ways. Some include case mapping, normalization,
-transliteration and bidirectional text handling.
-
-You can defined transliterator identifiers by using `id` property, and specify direction  to `forward` or `reverse` by
-using `dir` property, The default value of both properties are `Null` and `forward`.
-
-For example:
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "latin" : {
-                    "tokenizer" : "keyword",
-                    "filter" : ["myLatinTransform"]
-                }
-            },
-            "filter" : {
-                "myLatinTransform" : {
-                    "type" : "icu_transform",
-                    "id" : "Any-Latin; NFD; [:Nonspacing Mark:] Remove; NFC"
-                }
-            }
-        }
-    }
-}
-```
-
-This transform transliterated characters to latin, and separates accents from their base characters, removes the accents,
-and then puts the remaining text into an unaccented form.
-
-The results are:
-
-`你好` to `ni hao`
-
-`здравствуйте` to `zdravstvujte`
-
-`こんにちは` to `kon'nichiha`
-
-Currently the filter only supports identifier and direction, custom rulesets are not yet supported.
-
-For more documentation, Please see the [user guide of ICU Transform](http://userguide.icu-project.org/transforms/general).
-
-License
--------
-
-    This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/analysis-icu/pom.xml b/plugins/analysis-icu/pom.xml
index d067929..a25f6f3 100644
--- a/plugins/analysis-icu/pom.xml
+++ b/plugins/analysis-icu/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-analysis-icu</artifactId>
diff --git a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
index 2890d13..ec720f7 100644
--- a/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
+++ b/plugins/analysis-icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java
@@ -28,7 +28,6 @@ import org.elasticsearch.common.inject.assistedinject.Assisted;
 import org.elasticsearch.common.io.Streams;
 import org.elasticsearch.common.settings.Settings;
 import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.FailedToResolveConfigException;
 import org.elasticsearch.index.Index;
 import org.elasticsearch.index.settings.IndexSettings;
 
@@ -61,7 +60,7 @@ public class IcuCollationTokenFilterFactory extends AbstractTokenFilterFactory {
             Exception failureToResolve = null;
             try {
                 rules = Streams.copyToString(Files.newBufferedReader(environment.configFile().resolve(rules), Charset.forName("UTF-8")));
-            } catch (FailedToResolveConfigException | IOException | SecurityException e) {
+            } catch (IOException | SecurityException e) {
                 failureToResolve = e;
             }
             try {
diff --git a/plugins/analysis-kuromoji/README.md b/plugins/analysis-kuromoji/README.md
deleted file mode 100644
index e4e0c65..0000000
--- a/plugins/analysis-kuromoji/README.md
+++ /dev/null
@@ -1,552 +0,0 @@
-Japanese (kuromoji) Analysis for Elasticsearch
-==================================
-
-The Japanese (kuromoji) Analysis plugin integrates Lucene kuromoji analysis module into elasticsearch.
-
-In order to install the plugin, run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-analysis-kuromoji/2.5.0
-```
-
-You need to install a version matching your Elasticsearch version:
-
-| elasticsearch |  Kuromoji Analysis Plugin   |   Docs     |  
-|---------------|-----------------------------|------------|
-| master        |  Build from source          | See below  |
-| es-1.x        |  Build from source          | [2.6.0-SNAPSHOT](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/es-1.x/#version-260-snapshot-for-elasticsearch-1x)  |
-|    es-1.5              |     2.5.0         | [2.5.0](https://github.com/elastic/elasticsearch-analysis-kuromoji/tree/v2.5.0/#version-250-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.3         | [2.4.3](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/v2.4.3/#version-243-for-elasticsearch-14)                  |
-| < 1.4.5       |  2.4.2                      | [2.4.2](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/v2.4.2/#version-242-for-elasticsearch-14)                  |
-| < 1.4.3       |  2.4.1                      | [2.4.1](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/v2.4.1/#version-241-for-elasticsearch-14)                  |
-| es-1.3        |  2.3.0                      | [2.3.0](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/v2.3.0/#japanese-kuromoji-analysis-for-elasticsearch)  |
-| es-1.2        |  2.2.0                      | [2.2.0](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/v2.2.0/#japanese-kuromoji-analysis-for-elasticsearch)  |
-| es-1.1        |  2.1.0                      | [2.1.0](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/v2.1.0/#japanese-kuromoji-analysis-for-elasticsearch)  |
-| es-1.0        |  2.0.0                      | [2.0.0](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/v2.0.0/#japanese-kuromoji-analysis-for-elasticsearch)  |
-| es-0.90       |  1.8.0                      | [1.8.0](https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/tree/v1.8.0/#japanese-kuromoji-analysis-for-elasticsearch)  |
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install analysis-kuromoji \
-       --url file:target/releases/elasticsearch-analysis-kuromoji-X.X.X-SNAPSHOT.zip
-```
-
-Includes Analyzer, Tokenizer, TokenFilter, CharFilter
------------------------------------------------
-
-The plugin includes these analyzer and tokenizer, tokenfilter.
-
-| name                    | type        |
-|-------------------------|-------------|
-| kuromoji_iteration_mark | charfilter  |
-| kuromoji                | analyzer    |
-| kuromoji_tokenizer      | tokenizer   |
-| kuromoji_baseform       | tokenfilter |
-| kuromoji_part_of_speech | tokenfilter |
-| kuromoji_readingform    | tokenfilter |
-| kuromoji_stemmer        | tokenfilter |
-| ja_stop                 | tokenfilter |
-
-
-Usage
------
-
-## Analyzer : kuromoji
-
-An analyzer of type `kuromoji`.
-This analyzer is the following tokenizer and tokenfilter combination.
-
-* `kuromoji_tokenizer` : Kuromoji Tokenizer
-* `kuromoji_baseform` : Kuromoji BasicFormFilter (TokenFilter)
-* `kuromoji_part_of_speech` : Kuromoji Part of Speech Stop Filter (TokenFilter)
-* `cjk_width` : CJK Width Filter (TokenFilter)
-* `stop` : Stop Filter (TokenFilter)
-* `kuromoji_stemmer` : Kuromoji Katakana Stemmer Filter(TokenFilter)
-* `lowercase` : LowerCase Filter (TokenFilter)
-
-## CharFilter : kuromoji_iteration_mark
-
-A charfilter of type `kuromoji_iteration_mark`.
-This charfilter is Normalizes Japanese horizontal iteration marks (odoriji) to their expanded form.
-
-The following ar setting that can be set for a `kuromoji_iteration_mark` charfilter type:
-
-| **Setting**     | **Description**                                              | **Default value** |
-|:----------------|:-------------------------------------------------------------|:------------------|
-| normalize_kanji | indicates whether kanji iteration marks should be normalized | `true`            |
-| normalize_kana  | indicates whether kanji iteration marks should be normalized | `true`            |
-
-## Tokenizer : kuromoji_tokenizer
-
-A tokenizer of type `kuromoji_tokenizer`.
-
-The following are settings that can be set for a `kuromoji_tokenizer` tokenizer type:
-
-| **Setting**         | **Description**                                                                                                           | **Default value** |
-|:--------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------------|
-| mode                | Tokenization mode: this determines how the tokenizer handles compound and unknown words. `normal` and `search`, `extended`| `search`          |
-| discard_punctuation | `true` if punctuation tokens should be dropped from the output.                                                           | `true`            |
-| user_dictionary     | set User Dictionary file                                                                                                  |                   |
-
-### Tokenization mode
-
-The mode is three types.
-
-* `normal` : Ordinary segmentation: no decomposition for compounds
-
-* `search` : Segmentation geared towards search: this includes a decompounding process for long nouns, also including the full compound token as a synonym.
-
-* `extended` : Extended mode outputs unigrams for unknown words.
-
-#### Difference tokenization mode outputs
-
-Input text is `関西国際空港` and `アブラカダブラ`.
-
-| **mode**   | `関西国際空港` | `アブラカダブラ` |
-|:-----------|:-------------|:-------|
-| `normal`   | `関西国際空港` | `アブラカダブラ` |
-| `search`   | `関西` `関西国際空港` `国際` `空港` | `アブラカダブラ` |
-| `extended` | `関西` `国際` `空港` | `ア` `ブ` `ラ` `カ` `ダ` `ブ` `ラ` |
-
-### User Dictionary
-
-Kuromoji tokenizer use MeCab-IPADIC dictionary by default.
-And Kuromoji is added an entry of dictionary to define by user; this is User Dictionary.
-User Dictionary entries are defined using the following CSV format:
-
-```
-<text>,<token 1> ... <token n>,<reading 1> ... <reading n>,<part-of-speech tag>
-```
-
-Dictionary Example
-
-```
-東京スカイツリー,東京 スカイツリー,トウキョウ スカイツリー,カスタム名詞
-```
-
-To use User Dictionary set file path to `user_dict` attribute.
-User Dictionary file is placed `ES_HOME/config` directory.
-
-### example
-
-_Example Settings:_
-
-```sh
-curl -XPUT 'http://localhost:9200/kuromoji_sample/' -d'
-{
-    "settings": {
-        "index":{
-            "analysis":{
-                "tokenizer" : {
-                    "kuromoji_user_dict" : {
-                       "type" : "kuromoji_tokenizer",
-                       "mode" : "extended",
-                       "discard_punctuation" : "false",
-                       "user_dictionary" : "userdict_ja.txt"
-                    }
-                },
-                "analyzer" : {
-                    "my_analyzer" : {
-                        "type" : "custom",
-                        "tokenizer" : "kuromoji_user_dict"
-                    }
-                }
-
-            }
-        }
-    }
-}
-'
-```
-
-_Example Request using `_analyze` API :_
-
-```sh
-curl -XPOST 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=my_analyzer&pretty' -d '東京スカイツリー'
-```
-
-_Response :_
-
-```json
-{
-  "tokens" : [ {
-    "token" : "東京",
-    "start_offset" : 0,
-    "end_offset" : 2,
-    "type" : "word",
-    "position" : 1
-  }, {
-    "token" : "スカイツリー",
-    "start_offset" : 2,
-    "end_offset" : 8,
-    "type" : "word",
-    "position" : 2
-  } ]
-}
-```
-
-## TokenFilter : kuromoji_baseform
-
-A token filter of type `kuromoji_baseform` that replaces term text with BaseFormAttribute.
-This acts as a lemmatizer for verbs and adjectives.
-
-### example
-
-_Example Settings:_
-
-```sh
-curl -XPUT 'http://localhost:9200/kuromoji_sample/' -d'
-{
-    "settings": {
-        "index":{
-            "analysis":{
-                "analyzer" : {
-                    "my_analyzer" : {
-                        "tokenizer" : "kuromoji_tokenizer",
-                        "filter" : ["kuromoji_baseform"]
-                    }
-                }
-            }
-        }
-    }
-}
-'
-```
-
-_Example Request using `_analyze` API :_
-
-```sh
-curl -XPOST 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=my_analyzer&pretty' -d '飲み'
-```
-
-_Response :_
-
-```json
-{
-  "tokens" : [ {
-    "token" : "飲む",
-    "start_offset" : 0,
-    "end_offset" : 2,
-    "type" : "word",
-    "position" : 1
-  } ]
-}
-```
-
-## TokenFilter : kuromoji_part_of_speech
-
-A token filter of type `kuromoji_part_of_speech` that removes tokens that match a set of part-of-speech tags.
-
-The following are settings that can be set for a stop token filter type:
-
-| **Setting** | **Description**                                      |
-|:------------|:-----------------------------------------------------|
-| stoptags    | A list of part-of-speech tags that should be removed |
-
-Note that default setting is stoptags.txt include lucene-analyzer-kuromoji.jar.
-
-### example
-
-_Example Settings:_
-
-```sh
-curl -XPUT 'http://localhost:9200/kuromoji_sample/' -d'
-{
-    "settings": {
-        "index":{
-            "analysis":{
-                "analyzer" : {
-                    "my_analyzer" : {
-                        "tokenizer" : "kuromoji_tokenizer",
-                        "filter" : ["my_posfilter"]
-                    }
-                },
-                "filter" : {
-                    "my_posfilter" : {
-                        "type" : "kuromoji_part_of_speech",
-                        "stoptags" : [
-                            "助詞-格助詞-一般",
-                            "助詞-終助詞"
-                        ]
-                    }
-                }
-            }
-        }
-    }
-}
-'
-```
-
-_Example Request using `_analyze` API :_
-
-```sh
-curl -XPOST 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=my_analyzer&pretty' -d '寿司がおいしいね'
-```
-
-_Response :_
-
-```json
-{
-  "tokens" : [ {
-    "token" : "寿司",
-    "start_offset" : 0,
-    "end_offset" : 2,
-    "type" : "word",
-    "position" : 1
-  }, {
-    "token" : "おいしい",
-    "start_offset" : 3,
-    "end_offset" : 7,
-    "type" : "word",
-    "position" : 3
-  } ]
-}
-```
-
-## TokenFilter : kuromoji_readingform
-
-A token filter of type `kuromoji_readingform` that replaces the term attribute with the reading of a token in either katakana or romaji form.
-The default reading form is katakana.
-
-The following are settings that can be set for a `kuromoji_readingform` token filter type:
-
-| **Setting** | **Description**                                           | **Default value** |
-|:------------|:----------------------------------------------------------|:------------------|
-| use_romaji  | `true` if romaji reading form output instead of katakana. | `false`           |
-
-Note that elasticsearch-analysis-kuromoji built-in `kuromoji_readingform` set default `true` to `use_romaji` attribute.
-
-### example
-
-_Example Settings:_
-
-```sh
-curl -XPUT 'http://localhost:9200/kuromoji_sample/' -d'
-{
-    "settings": {
-        "index":{
-            "analysis":{
-                "analyzer" : {
-                    "romaji_analyzer" : {
-                        "tokenizer" : "kuromoji_tokenizer",
-                        "filter" : ["romaji_readingform"]
-                    },
-                    "katakana_analyzer" : {
-                        "tokenizer" : "kuromoji_tokenizer",
-                        "filter" : ["katakana_readingform"]
-                    }
-                },
-                "filter" : {
-                    "romaji_readingform" : {
-                        "type" : "kuromoji_readingform",
-                        "use_romaji" : true
-                    },
-                    "katakana_readingform" : {
-                        "type" : "kuromoji_readingform",
-                        "use_romaji" : false
-                    }
-                }
-            }
-        }
-    }
-}
-'
-```
-
-_Example Request using `_analyze` API :_
-
-```sh
-curl -XPOST 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=katakana_analyzer&pretty' -d '寿司'
-```
-
-_Response :_
-
-```json
-{
-  "tokens" : [ {
-    "token" : "スシ",
-    "start_offset" : 0,
-    "end_offset" : 2,
-    "type" : "word",
-    "position" : 1
-  } ]
-}
-```
-
-_Example Request using `_analyze` API :_
-
-```sh
-curl -XPOST 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=romaji_analyzer&pretty' -d '寿司'
-```
-
-_Response :_
-
-```json
-{
-  "tokens" : [ {
-    "token" : "sushi",
-    "start_offset" : 0,
-    "end_offset" : 2,
-    "type" : "word",
-    "position" : 1
-  } ]
-}
-```
-
-## TokenFilter : kuromoji_stemmer
-
-A token filter of type `kuromoji_stemmer` that normalizes common katakana spelling variations ending in a long sound character by removing this character (U+30FC).
-Only katakana words longer than a minimum length are stemmed (default is four).
-
-Note that only full-width katakana characters are supported.
-
-The following are settings that can be set for a `kuromoji_stemmer` token filter type:
-
-| **Setting**     | **Description**            | **Default value** |
-|:----------------|:---------------------------|:------------------|
-| minimum_length  | The minimum length to stem | `4`               |
-
-### example
-
-_Example Settings:_
-
-```sh
-curl -XPUT 'http://localhost:9200/kuromoji_sample/' -d'
-{
-    "settings": {
-        "index":{
-            "analysis":{
-                "analyzer" : {
-                    "my_analyzer" : {
-                        "tokenizer" : "kuromoji_tokenizer",
-                        "filter" : ["my_katakana_stemmer"]
-                    }
-                },
-                "filter" : {
-                    "my_katakana_stemmer" : {
-                        "type" : "kuromoji_stemmer",
-                        "minimum_length" : 4
-                    }
-                }
-            }
-        }
-    }
-}
-'
-```
-
-_Example Request using `_analyze` API :_
-
-```sh
-curl -XPOST 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=my_analyzer&pretty' -d 'コピー'
-```
-
-_Response :_
-
-```json
-{
-  "tokens" : [ {
-    "token" : "コピー",
-    "start_offset" : 0,
-    "end_offset" : 3,
-    "type" : "word",
-    "position" : 1
-  } ]
-}
-```
-
-_Example Request using `_analyze` API :_
-
-```sh
-curl -XPOST 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=my_analyzer&pretty' -d 'サーバー'
-```
-
-_Response :_
-
-```json
-{
-  "tokens" : [ {
-    "token" : "サーバ",
-    "start_offset" : 0,
-    "end_offset" : 4,
-    "type" : "word",
-    "position" : 1
-  } ]
-}
-```
-
-
-## TokenFilter : ja_stop
-
-
-A token filter of type `ja_stop` that provide a predefined "_japanese_" stop words.
-*Note: It is only provide "_japanese_". If you want to use other predefined stop words, you can use `stop` token filter.*
-
-_Example Settings:_
-
-### example
-
-```sh
-curl -XPUT 'http://localhost:9200/kuromoji_sample/' -d'
-{
-    "settings": {
-        "index":{
-            "analysis":{
-                "analyzer" : {
-                    "analyzer_with_ja_stop" : {
-                        "tokenizer" : "kuromoji_tokenizer",
-                        "filter" : ["ja_stop"]
-                    }
-                },
-                "filter" : {
-                    "ja_stop" : {
-                        "type" : "ja_stop",
-                        "stopwords" : ["_japanese_", "ストップ"]
-                    }
-                }
-            }
-        }
-    }
-}'
-```
-
-_Example Request using `_analyze` API :_
-
-```sh
-curl -XPOST 'http://localhost:9200/kuromoji_sample/_analyze?analyzer=katakana_analyzer&pretty' -d 'ストップは消える'
-```
-
-_Response :_
-
-```json
-{
-  "tokens" : [ {
-    "token" : "消える",
-    "start_offset" : 5,
-    "end_offset" : 8,
-    "type" : "word",
-    "position" : 3
-  } ]
-}
-```
-
-License
--------
-
-    This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/analysis-kuromoji/pom.xml b/plugins/analysis-kuromoji/pom.xml
index 1ac11b8..d6e87ad 100644
--- a/plugins/analysis-kuromoji/pom.xml
+++ b/plugins/analysis-kuromoji/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-analysis-kuromoji</artifactId>
diff --git a/plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/KuromojiAnalysisTests.java b/plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/KuromojiAnalysisTests.java
index be9f007..63a31ef 100644
--- a/plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/KuromojiAnalysisTests.java
+++ b/plugins/analysis-kuromoji/src/test/java/org/elasticsearch/index/analysis/KuromojiAnalysisTests.java
@@ -42,8 +42,11 @@ import org.elasticsearch.test.ESTestCase;
 import org.junit.Test;
 
 import java.io.IOException;
+import java.io.InputStream;
 import java.io.Reader;
 import java.io.StringReader;
+import java.nio.file.Files;
+import java.nio.file.Path;
 
 import static org.hamcrest.Matchers.*;
 
@@ -189,11 +192,18 @@ public class KuromojiAnalysisTests extends ESTestCase {
         assertSimpleTSOutput(tokenFilter.create(tokenizer), expected);
     }
 
+    public AnalysisService createAnalysisService() throws IOException {
+        InputStream empty_dict = getClass().getResourceAsStream("empty_user_dict.txt");
+        InputStream dict = getClass().getResourceAsStream("user_dict.txt");
+        Path home = createTempDir();
+        Path config = home.resolve("config");
+        Files.createDirectory(config);
+        Files.copy(empty_dict, config.resolve("empty_user_dict.txt"));
+        Files.copy(dict, config.resolve("user_dict.txt"));
 
-    public AnalysisService createAnalysisService() {
         String json = "/org/elasticsearch/index/analysis/kuromoji_analysis.json";
         Settings settings = Settings.settingsBuilder()
-                .put("path.home", createTempDir())
+                .put("path.home", home)
                 .loadFromStream(json, getClass().getResourceAsStream(json))
                 .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)
                 .build();
@@ -259,7 +269,7 @@ public class KuromojiAnalysisTests extends ESTestCase {
 
     // fix #59
     @Test
-    public void testKuromojiEmptyUserDict() {
+    public void testKuromojiEmptyUserDict() throws IOException {
         AnalysisService analysisService = createAnalysisService();
         TokenizerFactory tokenizerFactory = analysisService.tokenizer("kuromoji_empty_user_dict");
         assertThat(tokenizerFactory, instanceOf(KuromojiTokenizerFactory.class));
diff --git a/plugins/analysis-kuromoji/src/test/resources/org/elasticsearch/index/analysis/kuromoji_analysis.json b/plugins/analysis-kuromoji/src/test/resources/org/elasticsearch/index/analysis/kuromoji_analysis.json
index a36b4ae..58ed015 100644
--- a/plugins/analysis-kuromoji/src/test/resources/org/elasticsearch/index/analysis/kuromoji_analysis.json
+++ b/plugins/analysis-kuromoji/src/test/resources/org/elasticsearch/index/analysis/kuromoji_analysis.json
@@ -43,11 +43,11 @@
                 },
                 "kuromoji_empty_user_dict" : {
                     "type":"kuromoji_tokenizer",
-                    "user_dictionary":"org/elasticsearch/index/analysis/empty_user_dict.txt"
+                    "user_dictionary":"empty_user_dict.txt"
                 },
                 "kuromoji_user_dict" : {
                     "type":"kuromoji_tokenizer",
-                    "user_dictionary":"org/elasticsearch/index/analysis/user_dict.txt"
+                    "user_dictionary":"user_dict.txt"
                 }
             },
             "analyzer" : {
diff --git a/plugins/analysis-phonetic/README.md b/plugins/analysis-phonetic/README.md
deleted file mode 100644
index e587844..0000000
--- a/plugins/analysis-phonetic/README.md
+++ /dev/null
@@ -1,93 +0,0 @@
-Phonetic Analysis for Elasticsearch
-===================================
-
-The Phonetic Analysis plugin integrates phonetic token filter analysis with elasticsearch.
-
-In order to install the plugin, simply run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-analysis-phonetic/2.5.0
-```
-
-
-| elasticsearch |Phonetic Analysis Plugin|   Docs     |  
-|---------------|-----------------------|------------|
-| master        |  Build from source    | See below  |
-| es-1.x        |  Build from source    | [2.6.0-SNAPSHOT](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/es-1.x/#version-260-snapshot-for-elasticsearch-1x)  |
-| es-1.5        |  2.5.0                | [2.5.0](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/v2.5.0/#version-250-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.3         | [2.4.3](https://github.com/elasticsearch/elasticsearch-analysis-phonetic/tree/v2.4.3/#version-243-for-elasticsearch-14)                  |
-| < 1.4.5       |  2.4.2                | [2.4.2](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/v2.4.2/#version-242-for-elasticsearch-14)                  |
-| < 1.4.3       |  2.4.1                | [2.4.1](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/v2.4.1/#version-241-for-elasticsearch-14)                  |
-| es-1.3        |  2.3.0                | [2.3.0](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/v2.3.0/#phonetic-analysis-for-elasticsearch)  |
-| es-1.2        |  2.2.0                | [2.2.0](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/v2.2.0/#phonetic-analysis-for-elasticsearch)  |
-| es-1.1        |  2.1.0                | [2.1.0](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/v2.1.0/#phonetic-analysis-for-elasticsearch)  |
-| es-1.0        |  2.0.0                | [2.0.0](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/v2.0.0/#phonetic-analysis-for-elasticsearch)  |
-| es-0.90       |  1.8.0                | [1.8.0](https://github.com/elastic/elasticsearch-analysis-phonetic/tree/v1.8.0/#phonetic-analysis-for-elasticsearch)  |
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install analysis-phonetic \
-       --url file:target/releases/elasticsearch-analysis-phonetic-X.X.X-SNAPSHOT.zip
-```
-
-## User guide
-
-A `phonetic` token filter that can be configured with different `encoder` types: 
-`metaphone`, `doublemetaphone`, `soundex`, `refinedsoundex`, 
-`caverphone1`, `caverphone2`, `cologne`, `nysiis`,
-`koelnerphonetik`, `haasephonetik`, `beidermorse`
-
-The `replace` parameter (defaults to `true`) controls if the token processed 
-should be replaced with the encoded one (set it to `true`), or added (set it to `false`).
-
-```js
-{
-    "index" : {
-        "analysis" : {
-            "analyzer" : {
-                "my_analyzer" : {
-                    "tokenizer" : "standard",
-                    "filter" : ["standard", "lowercase", "my_metaphone"]
-                }
-            },
-            "filter" : {
-                "my_metaphone" : {
-                    "type" : "phonetic",
-                    "encoder" : "metaphone",
-                    "replace" : false
-                }
-            }
-        }
-    }
-}
-```
-
-Note that `beidermorse` does not support `replace` parameter.
-
-
-Questions
----------
-
-If you have questions or comments please use the [mailing list](https://groups.google.com/group/elasticsearch) instead
-of Github Issues tracker.
-
-License
--------
-
-    This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/analysis-phonetic/pom.xml b/plugins/analysis-phonetic/pom.xml
index f355a25..ae3b791 100644
--- a/plugins/analysis-phonetic/pom.xml
+++ b/plugins/analysis-phonetic/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-analysis-phonetic</artifactId>
diff --git a/plugins/analysis-smartcn/README.md b/plugins/analysis-smartcn/README.md
deleted file mode 100644
index 7d34cb7..0000000
--- a/plugins/analysis-smartcn/README.md
+++ /dev/null
@@ -1,58 +0,0 @@
-Smart Chinese Analysis for Elasticsearch
-==================================
-
-The Smart Chinese Analysis plugin integrates Lucene Smart Chinese analysis module into elasticsearch.
-
-In order to install the plugin, simply run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-analysis-smartcn/2.5.0
-```
-
-
-| elasticsearch |  Smart Chinese Analysis Plugin  |   Docs     |  
-|---------------|-----------------------|------------|
-| master        |  Build from source    | See below  |
-| es-1.x        |  Build from source    | [2.6.0-SNAPSHOT](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/es-1.x/#version-260-snapshot-for-elasticsearch-1x)  |
-| es-1.5        |  2.5.0                | [2.5.0](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/v2.5.0/#version-250-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.4         | [2.4.4](https://github.com/elasticsearch/elasticsearch-analysis-smartcn/tree/v2.4.4/#version-244-for-elasticsearch-14)                  |
-| < 1.4.5       |  2.4.3                | [2.4.3](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/v2.4.3/#version-243-for-elasticsearch-14)                  |
-| < 1.4.3       |  2.4.2                | [2.4.2](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/v2.4.2/#version-242-for-elasticsearch-14)                  |
-| es-1.3        |  2.3.1                | [2.3.1](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/v2.3.1/#version-231-for-elasticsearch-13)                  |
-| es-1.2        |  2.2.0                | [2.2.0](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/v2.2.0/#smart-chinese-analysis-for-elasticsearch)  |
-| es-1.1        |  2.1.0                | [2.1.0](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/v2.1.0/#smart-chinese-analysis-for-elasticsearch)  |
-| es-1.0        |  2.0.0                | [2.0.0](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/v2.0.0/#smart-chinese-analysis-for-elasticsearch)  |
-| es-0.90       |  1.8.0                | [1.8.0](https://github.com/elastic/elasticsearch-analysis-smartcn/tree/v1.8.0/#smart-chinese-analysis-for-elasticsearch)  |
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install analysis-smartcn \
-       --url file:target/releases/elasticsearch-analysis-smartcn-X.X.X-SNAPSHOT.zip
-```
-
-## User guide
-
-The plugin includes the `smartcn` analyzer and `smartcn_tokenizer` tokenizer.
-
- Note that `smartcn_word` token filter and `smartcn_sentence` have been deprecated.
-
-License
--------
-
-    This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/analysis-smartcn/pom.xml b/plugins/analysis-smartcn/pom.xml
index b0ed194..a25e787 100644
--- a/plugins/analysis-smartcn/pom.xml
+++ b/plugins/analysis-smartcn/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-analysis-smartcn</artifactId>
diff --git a/plugins/analysis-stempel/README.md b/plugins/analysis-stempel/README.md
deleted file mode 100644
index b6f6782..0000000
--- a/plugins/analysis-stempel/README.md
+++ /dev/null
@@ -1,56 +0,0 @@
-Stempel (Polish) Analysis for Elasticsearch
-==================================
-
-The Stempel (Polish) Analysis plugin integrates Lucene stempel (polish) analysis module into elasticsearch.
-
-In order to install the plugin, simply run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-analysis-stempel/2.4.3
-```
-
-| elasticsearch |  Stempel Analysis Plugin  |   Docs     |  
-|---------------|-----------------------|------------|
-| master        |  Build from source    | See below  |
-| es-1.x        |  Build from source    | [2.6.0-SNAPSHOT](https://github.com/elastic/elasticsearch-analysis-stempel/tree/es-1.x/#version-260-snapshot-for-elasticsearch-1x)  |
-| es-1.5        |  2.5.0                | [2.5.0](https://github.com/elastic/elasticsearch-analysis-stempel/tree/v2.5.0/#version-250-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.3         | [2.4.3](https://github.com/elasticsearch/elasticsearch-analysis-stempel/tree/v2.4.3/#version-243-for-elasticsearch-14)                  |
-| < 1.4.5       |  2.4.2                | [2.4.2](https://github.com/elastic/elasticsearch-analysis-stempel/tree/v2.4.2/#version-242-for-elasticsearch-14)                  |
-| < 1.4.3       |  2.4.1                | [2.4.1](https://github.com/elastic/elasticsearch-analysis-stempel/tree/v2.4.1/#version-241-for-elasticsearch-14)                  |
-| es-1.3        |  2.3.0                | [2.3.0](https://github.com/elastic/elasticsearch-analysis-stempel/tree/v2.3.0/#stempel-polish-analysis-for-elasticsearch)  |
-| es-1.2        |  2.2.0                | [2.2.0](https://github.com/elastic/elasticsearch-analysis-stempel/tree/v2.2.0/#stempel-polish-analysis-for-elasticsearch)  |
-| es-1.1        |  2.1.0                | [2.1.0](https://github.com/elastic/elasticsearch-analysis-stempel/tree/v2.1.0/#stempel-polish-analysis-for-elasticsearch)  |
-| es-1.0        |  2.0.0                | [2.0.0](https://github.com/elastic/elasticsearch-analysis-stempel/tree/v2.0.0/#stempel-polish-analysis-for-elasticsearch)  |
-| es-0.90       |  1.13.0               | [1.13.0](https://github.com/elastic/elasticsearch-analysis-stempel/tree/v1.13.0/#stempel-polish-analysis-for-elasticsearch)  |
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install analysis-stempel \
-       --url file:target/releases/elasticsearch-analysis-stempel-X.X.X-SNAPSHOT.zip
-```
-
-Stempel Plugin
------------------
-
-The plugin includes the `polish` analyzer and `polish_stem` token filter.
-
-License
--------
-
-    This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/analysis-stempel/pom.xml b/plugins/analysis-stempel/pom.xml
index febe5eb..7fcce1d 100644
--- a/plugins/analysis-stempel/pom.xml
+++ b/plugins/analysis-stempel/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-analysis-stempel</artifactId>
diff --git a/plugins/cloud-aws/README.md b/plugins/cloud-aws/README.md
deleted file mode 100644
index 1f5f539..0000000
--- a/plugins/cloud-aws/README.md
+++ /dev/null
@@ -1,361 +0,0 @@
-AWS Cloud Plugin for Elasticsearch
-==================================
-
-The Amazon Web Service (AWS) Cloud plugin allows to use [AWS API](https://github.com/aws/aws-sdk-java)
-for the unicast discovery mechanism and add S3 repositories.
-
-In order to install the plugin, run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-cloud-aws/2.5.1
-```
-
-You need to install a version matching your Elasticsearch version:
-
-|       Elasticsearch    |  AWS Cloud Plugin |                                                             Docs                                                                   |
-|------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------------------|
-|    master              | Build from source | See below                                                                                                                          |
-|    es-1.x              | Build from source | [2.6.0-SNAPSHOT](https://github.com/elasticsearch/elasticsearch-cloud-aws/tree/es-1.x/#version-260-snapshot-for-elasticsearch-1x)  |
-|    es-1.5              |     2.5.1         | [2.5.1](https://github.com/elastic/elasticsearch-cloud-aws/tree/v2.5.1/#version-251-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.2         | [2.4.2](https://github.com/elasticsearch/elasticsearch-cloud-aws/tree/v2.4.2/#version-242-for-elasticsearch-14)                  |
-|    es-1.3              |     2.3.0         | [2.3.0](https://github.com/elasticsearch/elasticsearch-cloud-aws/tree/v2.3.0/#version-230-for-elasticsearch-13)                    |
-|    es-1.2              |     2.2.0         | [2.2.0](https://github.com/elasticsearch/elasticsearch-cloud-aws/tree/v2.2.0/#aws-cloud-plugin-for-elasticsearch)                  |
-|    es-1.1              |     2.1.1         | [2.1.1](https://github.com/elasticsearch/elasticsearch-cloud-aws/tree/v2.1.1/#aws-cloud-plugin-for-elasticsearch)                  |
-|    es-1.0              |     2.0.0         | [2.0.0](https://github.com/elasticsearch/elasticsearch-cloud-aws/tree/v2.0.0/#aws-cloud-plugin-for-elasticsearch)                  |
-|    es-0.90             |     1.16.0        | [1.16.0](https://github.com/elasticsearch/elasticsearch-cloud-aws/tree/v1.16.0/#aws-cloud-plugin-for-elasticsearch)                |
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install cloud-aws \
-       --url file:target/releases/elasticsearch-cloud-aws-X.X.X-SNAPSHOT.zip
-```
-
-## Generic Configuration
-
-The plugin will default to using [IAM Role](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html) credentials 
-for authentication. These can be overridden by, in increasing order of precedence, system properties `aws.accessKeyId` and `aws.secretKey`, 
-environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_KEY`, or the elasticsearch config using `cloud.aws.access_key` and `cloud.aws.secret_key`:
- 
-```
-cloud:
-    aws:
-        access_key: AKVAIQBF2RECL7FJWGJQ
-        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
-```
-
-### Transport security
-
-By default this plugin uses HTTPS for all API calls to AWS endpoints. If you wish to configure HTTP you can set 
-`cloud.aws.protocol` in the elasticsearch config. You can optionally override this setting per individual service 
-via: `cloud.aws.ec2.protocol` or `cloud.aws.s3.protocol`. 
-
-```
-cloud:
-    aws:
-        protocol: https
-        s3: 
-            protocol: http
-        ec2: 
-            protocol: https
-```
-
-In addition, a proxy can be configured with the `proxy_host` and `proxy_port` settings (note that protocol can be `http` or `https`):
-
-```
-cloud:
-    aws:
-        protocol: https
-        proxy_host: proxy1.company.com
-        proxy_port: 8083
-```
-
-You can also set different proxies for `ec2` and `s3`:
-
-```
-cloud:
-    aws:
-        s3:
-            proxy_host: proxy1.company.com
-            proxy_port: 8083
-        ec2:
-            proxy_host: proxy2.company.com
-            proxy_port: 8083
-```
-
-### Region
-
-The `cloud.aws.region` can be set to a region and will automatically use the relevant settings for both `ec2` and `s3`. The available values are:
-
-* `us-east` (`us-east-1`)
-* `us-west` (`us-west-1`)
-* `us-west-1`
-* `us-west-2`
-* `ap-southeast` (`ap-southeast-1`)
-* `ap-southeast-1`
-* `ap-southeast-2`
-* `ap-northeast` (`ap-northeast-1`)
-* `eu-west` (`eu-west-1`)
-* `eu-central` (`eu-central-1`)
-* `sa-east` (`sa-east-1`)
-* `cn-north` (`cn-north-1`)
-
-
-### EC2/S3 Signer API
-
-If you are using a compatible EC2 or S3 service, they might be using an older API to sign the requests.
-You can set your compatible signer API using `cloud.aws.signer` (or `cloud.aws.ec2.signer` and `cloud.aws.s3.signer`)
-with the right signer to use. Defaults to `AWS4SignerType`.
-
-
-## EC2 Discovery
-
-ec2 discovery allows to use the ec2 APIs to perform automatic discovery (similar to multicast in non hostile multicast environments). Here is a simple sample configuration:
-
-```
-discovery:
-    type: ec2
-```
-
-The ec2 discovery is using the same credentials as the rest of the AWS services provided by this plugin (`repositories`).
-See [Generic Configuration](#generic-configuration) for details.
-
-The following are a list of settings (prefixed with `discovery.ec2`) that can further control the discovery:
-
-* `groups`: Either a comma separated list or array based list of (security) groups. Only instances with the provided security groups will be used in the cluster discovery. (NOTE: You could provide either group NAME or group ID.)
-* `host_type`: The type of host type to use to communicate with other instances. Can be one of `private_ip`, `public_ip`, `private_dns`, `public_dns`. Defaults to `private_ip`.
-* `availability_zones`: Either a comma separated list or array based list of availability zones. Only instances within the provided availability zones will be used in the cluster discovery.
-* `any_group`: If set to `false`, will require all security groups to be present for the instance to be used for the discovery. Defaults to `true`.
-* `ping_timeout`: How long to wait for existing EC2 nodes to reply during discovery. Defaults to `3s`. If no unit like `ms`, `s` or `m` is specified, milliseconds are used.
-
-### Recommended EC2 Permissions
-
-EC2 discovery requires making a call to the EC2 service. You'll want to setup an IAM policy to allow this. You can create a custom policy via the IAM Management Console. It should look similar to this.
-
-```js
-{
-    "Statement": [
-        {
-            "Action": [
-                "ec2:DescribeInstances"
-            ],
-            "Effect": "Allow",
-            "Resource": [
-                "*"
-            ]
-        }
-    ],
-    "Version": "2012-10-17"
-}
-```
-
-
-### Filtering by Tags
-
-The ec2 discovery can also filter machines to include in the cluster based on tags (and not just groups). The settings to use include the `discovery.ec2.tag.` prefix. For example, setting `discovery.ec2.tag.stage` to `dev` will only filter instances with a tag key set to `stage`, and a value of `dev`. Several tags set will require all of those tags to be set for the instance to be included.
-
-One practical use for tag filtering is when an ec2 cluster contains many nodes that are not running elasticsearch. In this case (particularly with high `ping_timeout` values) there is a risk that a new node's discovery phase will end before it has found the cluster (which will result in it declaring itself master of a new cluster with the same name - highly undesirable). Tagging elasticsearch ec2 nodes and then filtering by that tag will resolve this issue.
-
-### Automatic Node Attributes
-
-Though not dependent on actually using `ec2` as discovery (but still requires the cloud aws plugin installed), the plugin can automatically add node attributes relating to ec2 (for example, availability zone, that can be used with the awareness allocation feature). In order to enable it, set `cloud.node.auto_attributes` to `true` in the settings.
-
-
-### Using other EC2 endpoint
-
-If you are using any EC2 api compatible service, you can set the endpoint you want to use by setting `cloud.aws.ec2.endpoint`
-to your URL provider.
-
-## S3 Repository
-
-The S3 repository is using S3 to store snapshots. The S3 repository can be created using the following command:
-
-```sh
-$ curl -XPUT 'http://localhost:9200/_snapshot/my_s3_repository' -d '{
-    "type": "s3",
-    "settings": {
-        "bucket": "my_bucket_name",
-        "region": "us-west"
-    }
-}'
-```
-
-The following settings are supported:
-
-* `bucket`: The name of the bucket to be used for snapshots. (Mandatory)
-* `region`: The region where bucket is located. Defaults to US Standard
-* `endpoint`: The endpoint to the S3 API. Defaults to AWS's default S3 endpoint. Note that setting a region overrides the endpoint setting.
-* `protocol`: The protocol to use (`http` or `https`). Defaults to value of `cloud.aws.protocol` or `cloud.aws.s3.protocol`.
-* `base_path`: Specifies the path within bucket to repository data. Defaults to value of `repositories.s3.base_path` or to root directory if not set.
-* `access_key`: The access key to use for authentication. Defaults to value of `cloud.aws.access_key`.
-* `secret_key`: The secret key to use for authentication. Defaults to value of `cloud.aws.secret_key`.
-* `chunk_size`: Big files can be broken down into chunks during snapshotting if needed. The chunk size can be specified in bytes or by using size value notation, i.e. `1g`, `10m`, `5k`. Defaults to `100m`.
-* `compress`: When set to `true` metadata files are stored in compressed format. This setting doesn't affect index files that are already compressed by default. Defaults to `false`.
-* `server_side_encryption`: When set to `true` files are encrypted on server side using AES256 algorithm. Defaults to `false`.
-* `buffer_size`: Minimum threshold below which the chunk is uploaded using a single request. Beyond this threshold, the S3 repository will use the [AWS Multipart Upload API](http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) to split the chunk into several parts, each of `buffer_size` length, and to upload each part in its own request. Note that positionning a buffer size lower than `5mb` is not allowed since it will prevents the use of the Multipart API and may result in upload errors. Defaults to `5mb`.
-* `max_retries`: Number of retries in case of S3 errors. Defaults to `3`.
-
-The S3 repositories are using the same credentials as the rest of the AWS services provided by this plugin (`discovery`).
-See [Generic Configuration](#generic-configuration) for details.
-
-Multiple S3 repositories can be created. If the buckets require different credentials, then define them as part of the repository settings.
-
-### Recommended S3 Permissions
-
-In order to restrict the Elasticsearch snapshot process to the minimum required resources, we recommend using Amazon IAM in conjunction with pre-existing S3 buckets. Here is an example policy which will allow the snapshot access to an S3 bucket named "snaps.example.com". This may be configured through the AWS IAM console, by creating a Custom Policy, and using a Policy Document similar to this (changing snaps.example.com to your bucket name).
-
-```js
-{
-    "Statement": [
-        {
-            "Action": [
-                "s3:ListBucket",
-                "s3:GetBucketLocation",
-                "s3:ListBucketMultipartUploads",
-                "s3:ListBucketVersions"
-            ],
-            "Effect": "Allow",
-            "Resource": [
-                "arn:aws:s3:::snaps.example.com"
-            ]
-        },
-        {
-            "Action": [
-                "s3:GetObject",
-                "s3:PutObject",
-                "s3:DeleteObject",
-                "s3:AbortMultipartUpload",
-                "s3:ListMultipartUploadParts"
-            ],
-            "Effect": "Allow",
-            "Resource": [
-                "arn:aws:s3:::snaps.example.com/*"
-            ]
-        }
-    ],
-    "Version": "2012-10-17"
-}
-```
-
-You may further restrict the permissions by specifying a prefix within the bucket, in this example, named "foo".
-
-```js
-{
-    "Statement": [
-        {
-            "Action": [
-                "s3:ListBucket",
-                "s3:GetBucketLocation",
-                "s3:ListBucketMultipartUploads",
-                "s3:ListBucketVersions"
-            ],
-            "Condition": {
-                "StringLike": {
-                    "s3:prefix": [
-                        "foo/*"
-                    ]
-                }
-            },
-            "Effect": "Allow",
-            "Resource": [
-                "arn:aws:s3:::snaps.example.com"
-            ]
-        },
-        {
-            "Action": [
-                "s3:GetObject",
-                "s3:PutObject",
-                "s3:DeleteObject",
-                "s3:AbortMultipartUpload",
-                "s3:ListMultipartUploadParts"
-            ],
-            "Effect": "Allow",
-            "Resource": [
-                "arn:aws:s3:::snaps.example.com/foo/*"
-            ]
-        }
-    ],
-    "Version": "2012-10-17"
-}
-```
-
-The bucket needs to exist to register a repository for snapshots. If you did not create the bucket then the repository registration will fail. If you want elasticsearch to create the bucket instead, you can add the permission to create a specific bucket like this:
-
-```js
-{
-   "Action": [
-      "s3:CreateBucket"
-   ],
-   "Effect": "Allow",
-   "Resource": [
-      "arn:aws:s3:::snaps.example.com"
-   ]
-}
-```
-
-### Using other S3 endpoint
-
-If you are using any S3 api compatible service, you can set a global endpoint by setting `cloud.aws.s3.endpoint`
-to your URL provider. Note that this setting will be used for all S3 repositories.
-
-Different `endpoint`, `region` and `protocol` settings can be set on a per-repository basis (see [S3 Repository](#s3-repository) section for detail).
-
-
-## Testing
-
-Integrations tests in this plugin require working AWS configuration and therefore disabled by default. Three buckets and two iam users have to be created. The first iam user needs access to two buckets in different regions and the final bucket is exclusive for the other iam user. To enable tests prepare a config file elasticsearch.yml with the following content:
-
-```
-cloud:
-    aws:
-        access_key: AKVAIQBF2RECL7FJWGJQ
-        secret_key: vExyMThREXeRMm/b/LRzEB8jWwvzQeXgjqMX+6br
-
-repositories:
-    s3:
-        bucket: "bucket_name"
-        region: "us-west-2"
-        private-bucket:
-            bucket: <bucket not accessible by default key>
-            access_key: <access key>
-            secret_key: <secret key>
-        remote-bucket:
-            bucket: <bucket in other region>
-            region: <region>
-	external-bucket:
-	    bucket: <bucket>
-	    access_key: <access key>
-	    secret_key: <secret key>
-	    endpoint: <endpoint>
-	    protocol: <protocol>
-
-```
-
-Replace all occurrences of `access_key`, `secret_key`, `endpoint`, `protocol`, `bucket` and `region` with your settings. Please, note that the test will delete all snapshot/restore related files in the specified buckets.
-
-To run test:
-
-```sh
-mvn -Dtests.aws=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
-```
-
-
-License
--------
-
-    This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/cloud-aws/pom.xml b/plugins/cloud-aws/pom.xml
index 04417a9..03d157e 100644
--- a/plugins/cloud-aws/pom.xml
+++ b/plugins/cloud-aws/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-cloud-aws</artifactId>
diff --git a/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java b/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
index 4c6bc8c..b599541 100755
--- a/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
+++ b/plugins/cloud-aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java
@@ -42,8 +42,8 @@ public class Ec2Discovery extends ZenDiscovery {
     public Ec2Discovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
                         ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
                         DiscoverySettings discoverySettings,
-                        ElectMasterService electMasterService, @ClusterDynamicSettings DynamicSettings dynamicSettings) {
+                        ElectMasterService electMasterService) {
         super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
-                pingService, electMasterService, discoverySettings, dynamicSettings);
+                pingService, electMasterService, discoverySettings);
     }
 }
diff --git a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java
index 34aa075..082f9ca 100644
--- a/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java
+++ b/plugins/cloud-aws/src/test/java/org/elasticsearch/cloud/aws/AbstractAwsTest.java
@@ -20,9 +20,9 @@
 package org.elasticsearch.cloud.aws;
 
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.FailedToResolveConfigException;
+import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
@@ -80,16 +80,14 @@ public abstract class AbstractAwsTest extends ESIntegTestCase {
                 .put("cloud.aws.test.write_failures", 0.1)
                 .put("cloud.aws.test.read_failures", 0.1);
 
-        Environment environment = new Environment(settings.build());
-
         // if explicit, just load it and don't load from env
         try {
             if (Strings.hasText(System.getProperty("tests.config"))) {
-                settings.loadFromUrl(environment.resolveConfig(System.getProperty("tests.config")));
+                settings.loadFromPath(PathUtils.get(System.getProperty("tests.config")));
             } else {
                 throw new IllegalStateException("to run integration tests, you need to set -Dtest.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
             }
-        } catch (FailedToResolveConfigException exception) {
+        } catch (SettingsException exception) {
             throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
         }
         return settings.build();
diff --git a/plugins/cloud-azure/NOTICE.txt b/plugins/cloud-azure/NOTICE.txt
deleted file mode 100644
index c89e119..0000000
--- a/plugins/cloud-azure/NOTICE.txt
+++ /dev/null
@@ -1,14 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-activation-*.jar, javax.inject-*.jar, and jaxb-*.jar are under the CDDL license,
-the original source code for these can be found at http://www.oracle.com/.
-
-jersey-*.jar are under the CDDL license, the original source code for these
-can be found at https://jersey.java.net/.
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/cloud-azure/README.md b/plugins/cloud-azure/README.md
deleted file mode 100644
index 4d8be6e..0000000
--- a/plugins/cloud-azure/README.md
+++ /dev/null
@@ -1,568 +0,0 @@
-Azure Cloud Plugin for Elasticsearch
-====================================
-
-The Azure Cloud plugin allows to use Azure API for the unicast discovery mechanism.
-
-In order to install the plugin, run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-cloud-azure/2.6.1
-```
-
-You need to install a version matching your Elasticsearch version:
-
-|       Elasticsearch    | Azure Cloud Plugin|                                                             Docs                                                                   |
-|------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------------------|
-|    master              | Build from source | See below                                                                                                                          |
-|    es-1.x              | Build from source | [2.7.0-SNAPSHOT](https://github.com/elasticsearch/elasticsearch-cloud-azure/tree/es-1.x/#version-270-snapshot-for-elasticsearch-1x)|
-|    es-1.5              |     2.6.1         | [2.6.1](https://github.com/elastic/elasticsearch-cloud-azure/tree/v2.6.1/#version-261-for-elasticsearch-15)                  |
-|    es-1.4              |     2.5.2         | [2.5.2](https://github.com/elastic/elasticsearch-cloud-azure/tree/v2.5.2/#version-252-for-elasticsearch-14)                  |
-|    es-1.3              |     2.4.0         | [2.4.0](https://github.com/elasticsearch/elasticsearch-cloud-azure/tree/v2.4.0/#version-240-for-elasticsearch-13)                  |
-|    es-1.2              |     2.3.0         | [2.3.0](https://github.com/elasticsearch/elasticsearch-cloud-azure/tree/v2.3.0/#azure-cloud-plugin-for-elasticsearch)              |
-|    es-1.1              |     2.2.0         | [2.2.0](https://github.com/elasticsearch/elasticsearch-cloud-azure/tree/v2.2.0/#azure-cloud-plugin-for-elasticsearch)              |
-|    es-1.0              |     2.1.0         | [2.1.0](https://github.com/elasticsearch/elasticsearch-cloud-azure/tree/v2.1.0/#azure-cloud-plugin-for-elasticsearch)              |
-|    es-0.90             |     1.0.0.alpha1  | [1.0.0.alpha1](https://github.com/elasticsearch/elasticsearch-cloud-azure/tree/v1.0.0.alpha1/#azure-cloud-plugin-for-elasticsearch)|
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install cloud-azure \
-       --url file:target/releases/elasticsearch-cloud-azure-X.X.X-SNAPSHOT.zip
-```
-
-Azure Virtual Machine Discovery
-===============================
-
-Azure VM discovery allows to use the azure APIs to perform automatic discovery (similar to multicast in non hostile
-multicast environments). Here is a simple sample configuration:
-
-```
-cloud:
-    azure:
-        management:
-             subscription.id: XXX-XXX-XXX-XXX
-             cloud.service.name: es-demo-app
-             keystore:
-                   path: /path/to/azurekeystore.pkcs12
-                   password: WHATEVER
-                   type: pkcs12
-
-discovery:
-    type: azure
-```
-
-How to start (short story)
---------------------------
-
-* Create Azure instances
-* Install Elasticsearch
-* Install Azure plugin
-* Modify `elasticsearch.yml` file
-* Start Elasticsearch
-
-Azure credential API settings
------------------------------
-
-The following are a list of settings that can further control the credential API:
-
-* `cloud.azure.management.keystore.path`: /path/to/keystore
-* `cloud.azure.management.keystore.type`: `pkcs12`, `jceks` or `jks`. Defaults to `pkcs12`.
-* `cloud.azure.management.keystore.password`: your_password for the keystore
-* `cloud.azure.management.subscription.id`: your_azure_subscription_id
-* `cloud.azure.management.cloud.service.name`: your_azure_cloud_service_name
-
-Note that in previous versions, it was:
-
-```
-cloud:
-    azure:
-        keystore: /path/to/keystore
-        password: your_password_for_keystore
-        subscription_id: your_azure_subscription_id
-        service_name: your_azure_cloud_service_name
-```
-
-Advanced settings
------------------
-
-The following are a list of settings that can further control the discovery:
-
-* `discovery.azure.host.type`: either `public_ip` or `private_ip` (default). Azure discovery will use the one you set to ping
-other nodes. This feature was not documented before but was existing under `cloud.azure.host_type`.
-* `discovery.azure.endpoint.name`: when using `public_ip` this setting is used to identify the endpoint name used to forward requests
-to elasticsearch (aka transport port name). Defaults to `elasticsearch`. In Azure management console, you could define
-an endpoint `elasticsearch` forwarding for example requests on public IP on port 8100 to the virtual machine on port 9300.
-This feature was not documented before but was existing under `cloud.azure.port_name`.
-* `discovery.azure.deployment.name`: deployment name if any. Defaults to the value set with `cloud.azure.management.cloud.service.name`.
-* `discovery.azure.deployment.slot`: either `staging` or `production` (default).
-
-For example:
-
-```
-discovery:
-    type: azure
-    azure:
-        host:
-            type: private_ip
-        endpoint:
-            name: elasticsearch
-        deployment:
-            name: your_azure_cloud_service_name
-            slot: production
-```
-
-How to start (long story)
---------------------------
-
-We will expose here one strategy which is to hide our Elasticsearch cluster from outside.
-
-With this strategy, only VM behind this same virtual port can talk to each other.
-That means that with this mode, you can use elasticsearch unicast discovery to build a cluster.
-
-Best, you can use the `elasticsearch-cloud-azure` plugin to let it fetch information about your nodes using
-azure API.
-
-### Prerequisites
-
-Before starting, you need to have:
-
-* A [Windows Azure account](http://www.windowsazure.com/)
-* SSH keys and certificate
-* OpenSSL that isn't from MacPorts, specifically `OpenSSL 1.0.1f 6 Jan
-  2014` doesn't seem to create a valid keypair for ssh.  FWIW,
-  `OpenSSL 1.0.1c 10 May 2012` on Ubuntu 12.04 LTS is known to work.
-
-You should follow [this guide](http://azure.microsoft.com/en-us/documentation/articles/linux-use-ssh-key/) to learn
-how to create or use existing SSH keys. If you have already did it, you can skip the following.
-
-Here is a description on how to generate SSH keys using `openssl`:
-
-```sh
-# You may want to use another dir than /tmp
-cd /tmp
-openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout azure-private.key -out azure-certificate.pem
-chmod 600 azure-private.key azure-certificate.pem
-openssl x509 -outform der -in azure-certificate.pem -out azure-certificate.cer
-```
-
-Generate a keystore which will be used by the plugin to authenticate with a certificate
-all Azure API calls.
-
-```sh
-# Generate a keystore (azurekeystore.pkcs12)
-# Transform private key to PEM format
-openssl pkcs8 -topk8 -nocrypt -in azure-private.key -inform PEM -out azure-pk.pem -outform PEM
-# Transform certificate to PEM format
-openssl x509 -inform der -in azure-certificate.cer -out azure-cert.pem
-cat azure-cert.pem azure-pk.pem > azure.pem.txt
-# You MUST enter a password!
-openssl pkcs12 -export -in azure.pem.txt -out azurekeystore.pkcs12 -name azure -noiter -nomaciter
-```
-
-Upload the `azure-certificate.cer` file both in the elasticsearch Cloud Service (under `Manage Certificates`), 
-and under `Settings -> Manage Certificates`.
-
-**Important**: when prompted for a password, you need to enter a non empty one.
-
-See this [guide](http://www.windowsazure.com/en-us/manage/linux/how-to-guides/ssh-into-linux/) to have
-more details on how to create keys for Azure.
-
-Once done, you need to upload your certificate in Azure:
-
-* Go to the [management console](https://account.windowsazure.com/).
-* Sign in using your account.
-* Click on `Portal`.
-* Go to Settings (bottom of the left list)
-* On the bottom bar, click on `Upload` and upload your `azure-certificate.cer` file.
-
-You may want to use [Windows Azure Command-Line Tool](http://www.windowsazure.com/en-us/develop/nodejs/how-to-guides/command-line-tools/):
-
-* Install [NodeJS](https://github.com/joyent/node/wiki/Installing-Node.js-via-package-manager), for example using
-homebrew on MacOS X:
-
-```sh
-brew install node
-```
-
-* Install Azure tools:
-
-```sh
-sudo npm install azure-cli -g
-```
-
-* Download and import your azure settings:
-
-```sh
-# This will open a browser and will download a .publishsettings file
-azure account download
-
-# Import this file (we have downloaded it to /tmp)
-# Note, it will create needed files in ~/.azure. You can remove azure.publishsettings when done.
-azure account import /tmp/azure.publishsettings
-```
-
-### Creating your first instance
-
-You need to have a storage account available. Check [Azure Blob Storage documentation](http://www.windowsazure.com/en-us/develop/net/how-to-guides/blob-storage/#create-account)
-for more information.
-
-You will need to choose the operating system you want to run on. To get a list of official available images, run:
-
-```sh
-azure vm image list
-```
-
-Let's say we are going to deploy an Ubuntu image on an extra small instance in West Europe:
-
-* Azure cluster name: `azure-elasticsearch-cluster`
-* Image: `b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB`
-* VM Name: `myesnode1`
-* VM Size: `extrasmall`
-* Location: `West Europe`
-* Login: `elasticsearch`
-* Password: `password1234!!`
-
-Using command line:
-
-```sh
-azure vm create azure-elasticsearch-cluster \
-                b39f27a8b8c64d52b05eac6a62ebad85__Ubuntu-13_10-amd64-server-20130808-alpha3-en-us-30GB \
-                --vm-name myesnode1 \
-                --location "West Europe" \
-                --vm-size extrasmall \
-                --ssh 22 \
-                --ssh-cert /tmp/azure-certificate.pem \
-                elasticsearch password1234\!\!
-```
-
-You should see something like:
-
-```
-info:    Executing command vm create
-+ Looking up image
-+ Looking up cloud service
-+ Creating cloud service
-+ Retrieving storage accounts
-+ Configuring certificate
-+ Creating VM
-info:    vm create command OK
-```
-
-Now, your first instance is started. You need to install Elasticsearch on it.
-
-> **Note on SSH**
->
-> You need to give the private key and username each time you log on your instance:
->
->```sh
->ssh -i ~/.ssh/azure-private.key elasticsearch@myescluster.cloudapp.net
->```
->
-> But you can also define it once in `~/.ssh/config` file:
-> 
->```
->Host *.cloudapp.net
->  User elasticsearch
->  StrictHostKeyChecking no
->  UserKnownHostsFile=/dev/null
->  IdentityFile ~/.ssh/azure-private.key
->```
-
-
-```sh
-# First, copy your keystore on this machine
-scp /tmp/azurekeystore.pkcs12 azure-elasticsearch-cluster.cloudapp.net:/home/elasticsearch
-
-# Then, connect to your instance using SSH
-ssh azure-elasticsearch-cluster.cloudapp.net
-```
-
-Once connected, install Elasticsearch:
-
-```sh
-# Install Latest Java version
-# Read http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html for details
-sudo add-apt-repository ppa:webupd8team/java
-sudo apt-get update
-sudo apt-get install oracle-java7-installer
-
-# If you want to install OpenJDK instead
-# sudo apt-get update
-# sudo apt-get install openjdk-7-jre-headless
-
-# Download Elasticsearch
-curl -s https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.0.0.deb -o elasticsearch-1.0.0.deb
-
-# Prepare Elasticsearch installation
-sudo dpkg -i elasticsearch-1.0.0.deb
-```
-
-Check that elasticsearch is running:
-
-```sh
-curl http://localhost:9200/
-```
-
-This command should give you a JSON result:
-
-```javascript
-{
-  "status" : 200,
-  "name" : "Living Colossus",
-  "version" : {
-    "number" : "1.0.0",
-    "build_hash" : "a46900e9c72c0a623d71b54016357d5f94c8ea32",
-    "build_timestamp" : "2014-02-12T16:18:34Z",
-    "build_snapshot" : false,
-    "lucene_version" : "4.6"
-  },
-  "tagline" : "You Know, for Search"
-}
-```
-
-### Install elasticsearch cloud azure plugin
-
-```sh
-# Stop elasticsearch
-sudo service elasticsearch stop
-
-# Install the plugin
-sudo /usr/share/elasticsearch/bin/plugin install elasticsearch/elasticsearch-cloud-azure/2.6.1
-
-# Configure it
-sudo vi /etc/elasticsearch/elasticsearch.yml
-```
-
-And add the following lines:
-
-```yaml
-# If you don't remember your account id, you may get it with `azure account list`
-cloud:
-    azure:
-        management:
-             subscription.id: your_azure_subscription_id
-             cloud.service.name: your_azure_cloud_service_name
-             keystore:
-                   path: /home/elasticsearch/azurekeystore.pkcs12
-                   password: your_password_for_keystore
-
-discovery:
-    type: azure
-
-# Recommended (warning: non durable disk)
-# path.data: /mnt/resource/elasticsearch/data
-```
-
-Restart elasticsearch:
-
-```sh
-sudo service elasticsearch start
-```
-
-If anything goes wrong, check your logs in `/var/log/elasticsearch`.
-
-
-Scaling Out!
-------------
-
-You need first to create an image of your previous machine.
-Disconnect from your machine and run locally the following commands:
-
-```sh
-# Shutdown the instance
-azure vm shutdown myesnode1
-
-# Create an image from this instance (it could take some minutes)
-azure vm capture myesnode1 esnode-image --delete
-
-# Note that the previous instance has been deleted (mandatory)
-# So you need to create it again and BTW create other instances.
-
-azure vm create azure-elasticsearch-cluster \
-                esnode-image \
-                --vm-name myesnode1 \
-                --location "West Europe" \
-                --vm-size extrasmall \
-                --ssh 22 \
-                --ssh-cert /tmp/azure-certificate.pem \
-                elasticsearch password1234\!\!
-```
-
-> **Note:** It could happen that azure changes the endpoint public IP address.
-> DNS propagation could take some minutes before you can connect again using
-> name. You can get from azure the IP address if needed, using:
->
-> ```sh
-> # Look at Network `Endpoints 0 Vip`
-> azure vm show myesnode1
-> ```
-
-Let's start more instances!
-
-```sh
-for x in $(seq  2 10)
-	do
-		echo "Launching azure instance #$x..."
-		azure vm create azure-elasticsearch-cluster \
-		                esnode-image \
-		                --vm-name myesnode$x \
-		                --vm-size extrasmall \
-		                --ssh $((21 + $x)) \
-		                --ssh-cert /tmp/azure-certificate.pem \
-		                --connect \
-		                elasticsearch password1234\!\!
-	done
-```
-
-If you want to remove your running instances:
-
-```
-azure vm delete myesnode1
-```
-
-Azure Repository
-================
-
-To enable Azure repositories, you have first to set your azure storage settings in `elasticsearch.yml` file:
-
-```
-cloud:
-    azure:
-        storage:
-            account: your_azure_storage_account
-            key: your_azure_storage_key
-```
-
-For information, in previous version of the azure plugin, settings were:
-
-```
-cloud:
-    azure:
-        storage_account: your_azure_storage_account
-        storage_key: your_azure_storage_key
-```
-
-The Azure repository supports following settings:
-
-* `container`: Container name. Defaults to `elasticsearch-snapshots`
-* `base_path`: Specifies the path within container to repository data. Defaults to empty (root directory).
-* `chunk_size`: Big files can be broken down into chunks during snapshotting if needed. The chunk size can be specified
-in bytes or by using size value notation, i.e. `1g`, `10m`, `5k`. Defaults to `64m` (64m max)
-* `compress`: When set to `true` metadata files are stored in compressed format. This setting doesn't affect index
-files that are already compressed by default. Defaults to `false`.
-
-Some examples, using scripts:
-
-```sh
-# The simpliest one
-$ curl -XPUT 'http://localhost:9200/_snapshot/my_backup1' -d '{
-    "type": "azure"
-}'
-
-# With some settings
-$ curl -XPUT 'http://localhost:9200/_snapshot/my_backup2' -d '{
-    "type": "azure",
-    "settings": {
-        "container": "backup_container",
-        "base_path": "backups",
-        "chunk_size": "32m",
-        "compress": true
-    }
-}'
-```
-
-Example using Java:
-
-```java
-client.admin().cluster().preparePutRepository("my_backup3")
-    .setType("azure").setSettings(Settings.settingsBuilder()
-        .put(Storage.CONTAINER, "backup_container")
-        .put(Storage.CHUNK_SIZE, new ByteSizeValue(32, ByteSizeUnit.MB))
-    ).get();
-```
-
-Repository validation rules
----------------------------
-
-According to the [containers naming guide](http://msdn.microsoft.com/en-us/library/dd135715.aspx), a container name must 
-be a valid DNS name, conforming to the following naming rules:
-
-* Container names must start with a letter or number, and can contain only letters, numbers, and the dash (-) character.
-* Every dash (-) character must be immediately preceded and followed by a letter or number; consecutive dashes are not 
-permitted in container names.
-* All letters in a container name must be lowercase.
-* Container names must be from 3 through 63 characters long.
-
-
-Testing
-=======
-
-Integrations tests in this plugin require working Azure configuration and therefore disabled by default.
-To enable tests prepare a config file `elasticsearch.yml` with the following content:
-
-```
-cloud:
-  azure:
-    storage:
-      account: "YOUR-AZURE-STORAGE-NAME"
-      key: "YOUR-AZURE-STORAGE-KEY"
-```
-
-Replaces `account`, `key` with your settings. Please, note that the test will delete all snapshot/restore related files in the specified bucket.
-
-To run test:
-
-```sh
-mvn -Dtests.azure=true -Dtests.config=/path/to/config/file/elasticsearch.yml clean test
-```
-
-Working around a bug in Windows SMB and Java on windows
-=======================================================
-When using a shared file system based on the SMB protocol (like Azure File Service) to store indices, the way Lucene open index segment files is with a write only flag. This is the *correct* way to open the files, as they will only be used for writes and allows different FS implementations to optimize for it. Sadly, in windows with SMB, this disables the cache manager, causing writes to be slow. This has been described in [LUCENE-6176](https://issues.apache.org/jira/browse/LUCENE-6176), but it affects each and every Java program out there!. This need and must be fixed outside of ES and/or Lucene, either in windows or OpenJDK. For now, we are providing an experimental support to open the files with read flag, but this should be considered experimental and the correct way to fix it is in OpenJDK or Windows.
-
-The Azure Cloud plugin provides two storage types optimized for SMB:
-
-- `smb_mmap_fs`: a SMB specific implementation of the default [mmap fs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-store.html#mmapfs)
-- `smb_simple_fs`: a SMB specific implementation of the default [simple fs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-store.html#simplefs)
-
-To use one of these specific storage types, you need to install the Azure Cloud plugin and restart the node.
-Then configure Elasticsearch to set the storage type you want.
-
-This can be configured for all indices by adding this to the `elasticsearch.yml` file:
-
-```yaml
-index.store.type: smb_simple_fs
-```
-
-Note that setting will be applied for newly created indices.
-
-It can also be set on a per-index basis at index creation time:
-
-```sh
-curl -XPUT localhost:9200/my_index -d '{
-   "settings": {
-       "index.store.type": "smb_mmap_fs"
-   }
-}'
-```
-
-
-License
--------
-
-This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/cloud-azure/pom.xml b/plugins/cloud-azure/pom.xml
index ff67fc3..c7c8381 100644
--- a/plugins/cloud-azure/pom.xml
+++ b/plugins/cloud-azure/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-cloud-azure</artifactId>
diff --git a/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java b/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
index b8ea501..ff8b43f 100755
--- a/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
+++ b/plugins/cloud-azure/src/main/java/org/elasticsearch/discovery/azure/AzureDiscovery.java
@@ -43,8 +43,8 @@ public class AzureDiscovery extends ZenDiscovery {
     @Inject
     public AzureDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
                           ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
-                          DiscoverySettings discoverySettings, ElectMasterService electMasterService, @ClusterDynamicSettings DynamicSettings dynamicSettings) {
+                          DiscoverySettings discoverySettings, ElectMasterService electMasterService) {
         super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
-                pingService, electMasterService, discoverySettings, dynamicSettings);
+                pingService, electMasterService, discoverySettings);
     }
 }
diff --git a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java
index acd163f..1263b4b 100644
--- a/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java
+++ b/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureTest.java
@@ -20,9 +20,9 @@
 package org.elasticsearch.cloud.azure;
 
 import org.elasticsearch.common.Strings;
+import org.elasticsearch.common.io.PathUtils;
 import org.elasticsearch.common.settings.Settings;
-import org.elasticsearch.env.Environment;
-import org.elasticsearch.env.FailedToResolveConfigException;
+import org.elasticsearch.common.settings.SettingsException;
 import org.elasticsearch.plugin.cloud.azure.CloudAzurePlugin;
 import org.elasticsearch.test.ESIntegTestCase;
 import org.elasticsearch.test.ESIntegTestCase.ThirdParty;
@@ -48,16 +48,15 @@ public abstract class AbstractAzureTest extends ESIntegTestCase {
     protected Settings readSettingsFromFile() {
         Settings.Builder settings = Settings.builder();
         settings.put("path.home", createTempDir());
-        Environment environment = new Environment(settings.build());
 
         // if explicit, just load it and don't load from env
         try {
             if (Strings.hasText(System.getProperty("tests.config"))) {
-                settings.loadFromUrl(environment.resolveConfig(System.getProperty("tests.config")));
+                settings.loadFromPath(PathUtils.get((System.getProperty("tests.config"))));
             } else {
                 throw new IllegalStateException("to run integration tests, you need to set -Dtests.thirdparty=true and -Dtests.config=/path/to/elasticsearch.yml");
             }
-        } catch (FailedToResolveConfigException exception) {
+        } catch (SettingsException exception) {
           throw new IllegalStateException("your test configuration file is incorrect: " + System.getProperty("tests.config"), exception);
         }
         return settings.build();
diff --git a/plugins/cloud-gce/NOTICE.txt b/plugins/cloud-gce/NOTICE.txt
deleted file mode 100644
index 4880904..0000000
--- a/plugins/cloud-gce/NOTICE.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-Elasticsearch
-Copyright 2009-2015 Elasticsearch
-
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
-
-The LICENSE and NOTICE files for all dependencies may be found in the licenses/
-directory.
diff --git a/plugins/cloud-gce/README.md b/plugins/cloud-gce/README.md
deleted file mode 100644
index 17c1e3f..0000000
--- a/plugins/cloud-gce/README.md
+++ /dev/null
@@ -1,421 +0,0 @@
-Google Compute Engine Cloud Plugin for Elasticsearch
-====================================================
-
-The GCE Cloud plugin allows to use GCE API for the unicast discovery mechanism.
-
-In order to install the plugin, run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-cloud-gce/2.5.0
-```
-
-You need to install a version matching your Elasticsearch version:
-
-|       Elasticsearch    | GCE Cloud Plugin  |                                                             Docs                                                                   |
-|------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------------------|
-|    master              | Build from source | See below                                                                                                                          |
-|    es-1.x              | Build from source | [2.6.0-SNAPSHOT](https://github.com/elasticsearch/elasticsearch-cloud-gce/tree/es-1.x/#google-compute-engine-cloud-plugin-for-elasticsearch)|
-|    es-1.5              |     2.5.0         | [2.5.0](https://github.com/elastic/elasticsearch-cloud-gce/tree/v2.5.0/#version-250-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.1         | [2.4.1](https://github.com/elasticsearch/elasticsearch-cloud-gce/tree/v2.4.1/#version-241-for-elasticsearch-14)                  |
-|    es-1.3              |     2.3.0         | [2.3.0](https://github.com/elasticsearch/elasticsearch-cloud-gce/tree/v2.3.0/#version-230-for-elasticsearch-13)                  |
-|    es-1.2              |     2.2.0         | [2.2.0](https://github.com/elasticsearch/elasticsearch-cloud-gce/tree/v2.2.0/#google-compute-engine-cloud-plugin-for-elasticsearch)|
-|    es-1.1              |     2.1.2         | [2.1.2](https://github.com/elasticsearch/elasticsearch-cloud-gce/tree/v2.1.2/#google-compute-engine-cloud-plugin-for-elasticsearch)|
-|    es-1.0              |     2.0.1         | [2.0.1](https://github.com/elasticsearch/elasticsearch-cloud-gce/tree/v2.0.1/#google-compute-engine-cloud-plugin-for-elasticsearch)|
-|    es-0.90             |     1.3.0         | [1.3.0](https://github.com/elasticsearch/elasticsearch-cloud-gce/tree/v1.3.0/#google-compute-engine-cloud-plugin-for-elasticsearch)|
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install cloud-gce \
-       --url file:target/releases/elasticsearch-cloud-gce-X.X.X-SNAPSHOT.zip
-```
-
-
-Google Compute Engine Virtual Machine Discovery
-===============================
-
-Google Compute Engine VM discovery allows to use the google APIs to perform automatic discovery (similar to multicast in non hostile
-multicast environments). Here is a simple sample configuration:
-
-```yaml
-  cloud:
-      gce:
-          project_id: <your-google-project-id>
-          zone: <your-zone>
-  discovery:
-          type: gce
-```
-
-How to start (short story)
---------------------------
-
-* Create Google Compute Engine instance (with compute rw permissions)
-* Install Elasticsearch
-* Install Google Compute Engine Cloud plugin
-* Modify `elasticsearch.yml` file
-* Start Elasticsearch
-
-How to start (long story)
---------------------------
-
-### Prerequisites
-
-Before starting, you should have:
-
-* Your project ID. Let's say here `es-cloud`. Get it from [Google APIS Console](https://code.google.com/apis/console/).
-* [Google Cloud SDK](https://developers.google.com/cloud/sdk/)
-
-If you did not set it yet, you can define your default project you will work on:
-
-```sh
-gcloud config set project es-cloud
-```
-
-### Creating your first instance
-
-
-```sh
-gcutil addinstance myesnode1 \
-       --service_account_scope=compute-rw,storage-full \
-       --persistent_boot_disk
-```
-
-You will be asked to open a link in your browser. Login and allow access to listed services.
-You will get back a verification code. Copy and paste it in your terminal.
-
-You should get `Authentication successful.` message.
-
-Then, choose your zone. Let's say here that we choose `europe-west1-a`.
-
-Choose your compute instance size. Let's say `f1-micro`.
-
-Choose your OS. Let's say `projects/debian-cloud/global/images/debian-7-wheezy-v20140606`.
-
-You may be asked to create a ssh key. Follow instructions to create one.
-
-When done, a report like this one should appears:
-
-```sh
-Table of resources:
-
-+-----------+--------------+-------+---------+--------------+----------------+----------------+----------------+---------+----------------+
-|   name    | machine-type | image | network |  network-ip  |  external-ip   |     disks      |      zone      | status  | status-message |
-+-----------+--------------+-------+---------+--------------+----------------+----------------+----------------+---------+----------------+
-| myesnode1 | f1-micro     |       | default | 10.240.20.57 | 192.158.29.199 | boot-myesnode1 | europe-west1-a | RUNNING |                |
-+-----------+--------------+-------+---------+--------------+----------------+----------------+----------------+---------+----------------+
-```
-
-You can now connect to your instance:
-
-```
-# Connect using google cloud SDK
-gcloud compute ssh myesnode1 --zone europe-west1-a
-
-# Or using SSH with external IP address
-ssh -i ~/.ssh/google_compute_engine 192.158.29.199
-```
-
-*Note Regarding Service Account Permissions*
-
-It's important when creating an instance that the correct permissions are set. At a minimum, you must ensure you have:
-
-```
-service_account_scope=compute-rw
-```
-
-Failing to set this will result in unauthorized messages when starting Elasticsearch. 
-See [Machine Permissions](#machine-permissions).
-
-Once connected, install Elasticsearch:
-
-```sh
-sudo apt-get update
-
-# Download Elasticsearch
-wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.2.1.deb
-
-# Prepare Java installation
-sudo apt-get install java7-runtime-headless
-
-# Prepare Elasticsearch installation
-sudo dpkg -i elasticsearch-1.2.1.deb
-```
-
-### Install elasticsearch cloud gce plugin
-
-Install the plugin:
-
-```sh
-# Use Plugin Manager to install it
-sudo /usr/share/elasticsearch/bin/plugin install elasticsearch/elasticsearch-cloud-gce/2.2.0
-
-# Configure it:
-sudo vi /etc/elasticsearch/elasticsearch.yml
-```
-
-And add the following lines:
-
-```yaml
-cloud:
-  gce:
-      project_id: es-cloud
-      zone: europe-west1-a
-discovery:
-      type: gce
-```
-
-
-Start elasticsearch:
-
-```sh
-sudo /etc/init.d/elasticsearch start
-```
-
-If anything goes wrong, you should check logs:
-
-```sh
-tail -f /var/log/elasticsearch/elasticsearch.log
-```
-
-If needed, you can change log level to `TRACE` by modifying `sudo vi /etc/elasticsearch/logging.yml`:
-
-```yaml
-  # discovery
-  discovery.gce: TRACE
-```
-
-
-
-### Cloning your existing machine
-
-In order to build a cluster on many nodes, you can clone your configured instance to new nodes.
-You won't have to reinstall everything!
-
-First create an image of your running instance and upload it to Google Cloud Storage:
-
-```sh
-# Create an image of yur current instance
-sudo /usr/bin/gcimagebundle -d /dev/sda -o /tmp/
-
-# An image has been created in `/tmp` directory:
-ls /tmp
-e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
-
-# Upload your image to Google Cloud Storage:
-# Create a bucket to hold your image, let's say `esimage`:
-gsutil mb gs://esimage
-
-# Copy your image to this bucket:
-gsutil cp /tmp/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz gs://esimage
-
-# Then add your image to images collection:
-gcutil addimage elasticsearch-1-2-1 gs://esimage/e4686d7f5bf904a924ae0cfeb58d0827c6d5b966.image.tar.gz
-
-# If the previous command did not work for you, logout from your instance
-# and launch the same command from your local machine.
-```
-
-### Start new instances
-
-As you have now an image, you can create as many instances as you need:
-
-```sh
-# Just change node name (here myesnode2)
-gcutil addinstance --image=elasticsearch-1-2-1 myesnode2
-
-# If you want to provide all details directly, you can use:
-gcutil addinstance --image=elasticsearch-1-2-1 \
-       --kernel=projects/google/global/kernels/gce-v20130603 myesnode2 \
-       --zone europe-west1-a --machine_type f1-micro --service_account_scope=compute-rw \
-       --persistent_boot_disk
-```
-
-### Remove an instance (aka shut it down)
-
-You can use [Google Cloud Console](https://cloud.google.com/console) or CLI to manage your instances:
-
-```sh
-# Stopping and removing instances
-gcutil deleteinstance myesnode1 myesnode2 \
-       --zone=europe-west1-a
-
-# Consider removing disk as well if you don't need them anymore
-gcutil deletedisk boot-myesnode1 boot-myesnode2  \
-       --zone=europe-west1-a
-```
-
-Using zones
------------
-
-`cloud.gce.zone` helps to retrieve instances running in a given zone. It should be one of the 
-[GCE supported zones](https://developers.google.com/compute/docs/zones#available).
-
-The GCE discovery can support multi zones although you need to be aware of network latency between zones. 
-To enable discovery across more than one zone, just enter add your zone list to `cloud.gce.zone` setting:
- 
-```yaml
-  cloud:
-      gce:
-          project_id: <your-google-project-id>
-          zone: ["<your-zone1>", "<your-zone2>"]
-  discovery:
-          type: gce
-```
-
-
-
-Filtering by tags
------------------
-
-The GCE discovery can also filter machines to include in the cluster based on tags using `discovery.gce.tags` settings.
-For example, setting `discovery.gce.tags` to `dev` will only filter instances having a tag set to `dev`. Several tags
-set will require all of those tags to be set for the instance to be included.
-
-One practical use for tag filtering is when an GCE cluster contains many nodes that are not running
-elasticsearch. In this case (particularly with high ping_timeout values) there is a risk that a new node's discovery
-phase will end before it has found the cluster (which will result in it declaring itself master of a new cluster
-with the same name - highly undesirable). Adding tag on elasticsearch GCE nodes and then filtering by that
-tag will resolve this issue.
-
-Add your tag when building the new instance:
-
-```sh
-gcutil --project=es-cloud addinstance myesnode1 \
-       --service_account_scope=compute-rw \
-       --persistent_boot_disk \
-       --tags=elasticsearch,dev
-```
-
-Then, define it in `elasticsearch.yml`:
-
-```yaml
-cloud:
-  gce:
-      project_id: es-cloud
-      zone: europe-west1-a
-discovery:
-      type: gce
-      gce:
-            tags: elasticsearch, dev
-```
-
-Changing default transport port
--------------------------------
-
-By default, elasticsearch GCE plugin assumes that you run elasticsearch on 9300 default port.
-But you can specify the port value elasticsearch is meant to use using google compute engine metadata `es_port`:
-
-### When creating instance
-
-Add `--metadata=es_port:9301` option:
-
-```sh
-# when creating first instance
-gcutil addinstance myesnode1 \
-       --service_account_scope=compute-rw,storage-full \
-       --persistent_boot_disk \
-       --metadata=es_port:9301
-
-# when creating an instance from an image
-gcutil addinstance --image=elasticsearch-1-0-0-RC1 \
-       --kernel=projects/google/global/kernels/gce-v20130603 myesnode2 \
-       --zone europe-west1-a --machine_type f1-micro --service_account_scope=compute-rw \
-       --persistent_boot_disk --metadata=es_port:9301
-```
-
-### On a running instance
-
-```sh
-# Get metadata fingerprint
-gcutil getinstance myesnode1 --zone=europe-west1-a
-+------------------------+---------------------------------------------------------------------------------------------------------+
-|        property        |                                                  value                                                  |
-+------------------------+---------------------------------------------------------------------------------------------------------+
-| metadata               |                                                                                                         |
-| fingerprint            | 42WmSpB8rSM=                                                                                            |
-+------------------------+---------------------------------------------------------------------------------------------------------+
-
-# Use that fingerprint
-gcutil setinstancemetadata myesnode1 \
-       --zone=europe-west1-a \
-       --metadata=es_port:9301 \
-       --fingerprint=42WmSpB8rSM=
-```
-
-
-Tips
-----
-
-### Store project id locally
-
-If you don't want to repeat the project id each time, you can save it in `~/.gcutil.flags` file using:
-
-```sh
-gcutil getproject --project=es-cloud --cache_flag_values
-```
-
-`~/.gcutil.flags` file now contains:
-
-```
---project=es-cloud
-```
-
-### Machine Permissions
-
-**Creating machines with gcutil**
-
-Ensure the following flags are set:
-
-````
---service_account_scope=compute-rw
-```
-
-**Creating with console (web)**
-
-When creating an instance using the web portal, click **Show advanced options**. 
-
-At the bottom of the page, under `PROJECT ACCESS`, choose `>> Compute >> Read Write`.
-
-**Creating with knife google**
-
-Set the service account scopes when creating the machine:
-
-```
-$ knife google server create www1 \
-    -m n1-standard-1 \
-    -I debian-7-wheezy-v20131120 \
-    -Z us-central1-a \
-    -i ~/.ssh/id_rsa \
-    -x jdoe \
-    --gce-service-account-scopes https://www.googleapis.com/auth/compute.full_control
-```
-
-Or, you may use the alias:
-
-```
-    --gce-service-account-scopes compute-rw
-```
-
-If you have created a machine without the correct permissions, you will see `403 unauthorized` error messages. The only 
-way to alter these permissions is to delete the instance (NOT THE DISK). Then create another with the correct permissions.
-
-
-License
--------
-
-This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/cloud-gce/pom.xml b/plugins/cloud-gce/pom.xml
index 30a8c96..1d62fdb 100644
--- a/plugins/cloud-gce/pom.xml
+++ b/plugins/cloud-gce/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-cloud-gce</artifactId>
diff --git a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
index 448fca5..afa6437 100755
--- a/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
+++ b/plugins/cloud-gce/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java
@@ -42,9 +42,9 @@ public class GceDiscovery extends ZenDiscovery {
     public GceDiscovery(Settings settings, ClusterName clusterName, ThreadPool threadPool, TransportService transportService,
                         ClusterService clusterService, NodeSettingsService nodeSettingsService, ZenPingService pingService,
                         DiscoverySettings discoverySettings,
-                        ElectMasterService electMasterService, @ClusterDynamicSettings DynamicSettings dynamicSettings) {
+                        ElectMasterService electMasterService) {
         super(settings, clusterName, threadPool, transportService, clusterService, nodeSettingsService,
-                pingService, electMasterService, discoverySettings, dynamicSettings);
+                pingService, electMasterService, discoverySettings);
 
         // TODO Add again force disable multicast
         // See related issue in AWS plugin https://github.com/elastic/elasticsearch-cloud-aws/issues/179
diff --git a/plugins/delete-by-query/pom.xml b/plugins/delete-by-query/pom.xml
index 222d005..d7ea468 100644
--- a/plugins/delete-by-query/pom.xml
+++ b/plugins/delete-by-query/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-delete-by-query</artifactId>
diff --git a/plugins/jvm-example/README.md b/plugins/jvm-example/README.md
deleted file mode 100644
index 09c0559..0000000
--- a/plugins/jvm-example/README.md
+++ /dev/null
@@ -1,5 +0,0 @@
-Example JVM Plugin for Elasticsearch
-==================================
-Leniency is the root of all evil
-
-
diff --git a/plugins/jvm-example/pom.xml b/plugins/jvm-example/pom.xml
index 6f7b342..0bfd84b 100644
--- a/plugins/jvm-example/pom.xml
+++ b/plugins/jvm-example/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-jvm-example</artifactId>
diff --git a/plugins/jvm-example/src/main/assemblies/plugin.xml b/plugins/jvm-example/src/main/assemblies/plugin.xml
index f1f9e0f..434aaec 100644
--- a/plugins/jvm-example/src/main/assemblies/plugin.xml
+++ b/plugins/jvm-example/src/main/assemblies/plugin.xml
@@ -17,6 +17,10 @@
             <directory>src/main/config</directory>
             <outputDirectory>config</outputDirectory>
         </fileSet>
+        <fileSet>
+            <directory>src/main/bin</directory>
+            <outputDirectory>bin</outputDirectory>
+        </fileSet>
     </fileSets>
     <dependencySets>
         <dependencySet>
diff --git a/plugins/jvm-example/src/main/bin/test b/plugins/jvm-example/src/main/bin/test
new file mode 100755
index 0000000..76ba889
--- /dev/null
+++ b/plugins/jvm-example/src/main/bin/test
@@ -0,0 +1,3 @@
+#!/bin/sh
+
+echo test
diff --git a/plugins/jvm-example/src/main/bin/test.bat b/plugins/jvm-example/src/main/bin/test.bat
new file mode 100644
index 0000000..7c65052
--- /dev/null
+++ b/plugins/jvm-example/src/main/bin/test.bat
@@ -0,0 +1 @@
+echo test
diff --git a/plugins/lang-javascript/README.md b/plugins/lang-javascript/README.md
deleted file mode 100644
index a2870e3..0000000
--- a/plugins/lang-javascript/README.md
+++ /dev/null
@@ -1,177 +0,0 @@
-JavaScript lang Plugin for Elasticsearch
-==================================
-
-The JavaScript language plugin allows to have `javascript` (or `js`) as the language of scripts to execute.
-
-In order to install the plugin, simply run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-lang-javascript/2.5.0
-```
-
-You need to install a version matching your Elasticsearch version:
-
-| elasticsearch |   JavaScript Plugin   |   Docs     |  
-|---------------|-----------------------|------------|
-| master        |  Build from source    | See below  |
-| es-1.x        |  Build from source    | [2.6.0-SNAPSHOT](https://github.com/elasticsearch/elasticsearch-transport-thrift/tree/es-1.x/#version-260-snapshot-for-elasticsearch-1x)  |
-|    es-1.5              |     2.5.0         | [2.5.0](https://github.com/elastic/elasticsearch-lang-javascript/tree/v2.5.0/#version-250-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.1         | [2.4.1](https://github.com/elasticsearch/elasticsearch-lang-javascript/tree/v2.4.1/#version-241-for-elasticsearch-14)                  |
-|    es-1.3              |     2.3.1         | [2.3.1](https://github.com/elasticsearch/elasticsearch-lang-javascript/tree/v2.3.1/#version-231-for-elasticsearch-13)                  |
-| es-1.2        |  2.2.0                | [2.2.0](https://github.com/elasticsearch/elasticsearch-lang-javascript/tree/v2.2.0/#javascript-lang-plugin-for-elasticsearch)  |
-| es-1.1        |  2.1.0                | [2.1.0](https://github.com/elasticsearch/elasticsearch-lang-javascript/tree/v2.1.0/#javascript-lang-plugin-for-elasticsearch)  |
-| es-1.0        |  2.0.0                | [2.0.0](https://github.com/elasticsearch/elasticsearch-lang-javascript/tree/v2.0.0/#javascript-lang-plugin-for-elasticsearch)  |
-| es-0.90       |  1.4.0                | [1.4.0](https://github.com/elasticsearch/elasticsearch-lang-javascript/tree/v1.4.0/#javascript-lang-plugin-for-elasticsearch)  |
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install lang-javascript \
-       --url file:target/releases/elasticsearch-lang-javascript-X.X.X-SNAPSHOT.zip
-```
-
-
-Using javascript with function_score
-------------------------------------
-
-Let's say you want to use `function_score` API using `javascript`. Here is
-a way of doing it:
-
-```sh
-curl -XDELETE "http://localhost:9200/test"
-
-curl -XPUT "http://localhost:9200/test/doc/1" -d '{
-  "num": 1.0
-}'
-
-curl -XPUT "http://localhost:9200/test/doc/2?refresh" -d '{
-  "num": 2.0
-}'
-
-curl -XGET "http://localhost:9200/test/_search?pretty" -d '
-{
-  "query": {
-    "function_score": {
-      "script_score": {
-        "script": "doc[\"num\"].value",
-        "lang": "javascript"
-      }
-    }
-  }
-}'
-```
-
-gives
-
-```javascript
-{
-   // ...
-   "hits": {
-      "total": 2,
-      "max_score": 4,
-      "hits": [
-         {
-            // ...
-            "_score": 4
-         },
-         {
-            // ...
-            "_score": 1
-         }
-      ]
-   }
-}
-```
-
-Using javascript with script_fields
------------------------------------
-
-```sh
-curl -XDELETE "http://localhost:9200/test"
-
-curl -XPUT "http://localhost:9200/test/doc/1?refresh" -d'
-{
-  "obj1": {
-   "test": "something"
-  },
-  "obj2": {
-    "arr2": [ "arr_value1", "arr_value2" ]
-  }
-}'
-
-curl -XGET "http://localhost:9200/test/_search" -d'
-{
-  "script_fields": {
-    "s_obj1": {
-      "script": "_source.obj1", "lang": "js"
-    },
-    "s_obj1_test": {
-      "script": "_source.obj1.test", "lang": "js"
-    },
-    "s_obj2": {
-      "script": "_source.obj2", "lang": "js"
-    },
-    "s_obj2_arr2": {
-      "script": "_source.obj2.arr2", "lang": "js"
-    }
-  }
-}'
-```
-
-gives
-
-```javascript
-{
-  // ...
-  "hits": [
-     {
-        // ...
-        "fields": {
-           "s_obj2_arr2": [
-              [
-                 "arr_value1",
-                 "arr_value2"
-              ]
-           ],
-           "s_obj1_test": [
-              "something"
-           ],
-           "s_obj2": [
-              {
-                 "arr2": [
-                    "arr_value1",
-                    "arr_value2"
-                 ]
-              }
-           ],
-           "s_obj1": [
-              {
-                 "test": "something"
-              }
-           ]
-        }
-     }
-  ]
-}
-```
-
-
-License
--------
-
-    This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/lang-javascript/pom.xml b/plugins/lang-javascript/pom.xml
index e1ae181..f283aa7 100644
--- a/plugins/lang-javascript/pom.xml
+++ b/plugins/lang-javascript/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-lang-javascript</artifactId>
diff --git a/plugins/lang-python/README.md b/plugins/lang-python/README.md
deleted file mode 100644
index c06cc61..0000000
--- a/plugins/lang-python/README.md
+++ /dev/null
@@ -1,178 +0,0 @@
-Python lang Plugin for Elasticsearch
-==================================
-
-The Python (jython) language plugin allows to have `python` as the language of scripts to execute.
-
-In order to install the plugin, simply run: 
-
-```sh
-bin/plugin install elasticsearch/elasticsearch-lang-python/2.5.0
-```
-
-You need to install a version matching your Elasticsearch version:
-
-| elasticsearch |  Python Lang Plugin   |   Docs     |  
-|---------------|-----------------------|------------|
-| master        |  Build from source    | See below  |
-| es-1.x        |  Build from source    | [2.6.0-SNAPSHOT](https://github.com/elasticsearch/elasticsearch-lang-python/tree/es-1.x/#version-260-snapshot-for-elasticsearch-1x)  |
-|    es-1.5              |     2.5.0         | [2.5.0](https://github.com/elastic/elasticsearch-lang-python/tree/v2.5.0/#version-250-for-elasticsearch-15)                  |
-|    es-1.4              |     2.4.1         | [2.4.1](https://github.com/elasticsearch/elasticsearch-lang-python/tree/v2.4.1/#version-241-for-elasticsearch-14)                  |
-|    es-1.3              |     2.3.1         | [2.3.1](https://github.com/elasticsearch/elasticsearch-lang-python/tree/v2.3.1/#version-231-for-elasticsearch-13)                  |
-| < 1.3.5       |  2.3.0                | [2.3.0](https://github.com/elasticsearch/elasticsearch-lang-python/tree/v2.3.0/#version-230-for-elasticsearch-13)                  |
-| es-1.2        |  2.2.0                | [2.2.0](https://github.com/elasticsearch/elasticsearch-lang-python/tree/v2.2.0/#python-lang-plugin-for-elasticsearch)  |
-| es-1.0        |  2.0.0                | [2.0.0](https://github.com/elasticsearch/elasticsearch-lang-python/tree/v2.0.0/#python-lang-plugin-for-elasticsearch)  |
-| es-0.90       |  1.0.0                | [1.0.0](https://github.com/elasticsearch/elasticsearch-lang-python/tree/v1.0.0/#python-lang-plugin-for-elasticsearch)  |
-
-To build a `SNAPSHOT` version, you need to build it with Maven:
-
-```bash
-mvn clean install
-plugin install lang-python \
-       --url file:target/releases/elasticsearch-lang-python-X.X.X-SNAPSHOT.zip
-```
-
-User Guide
-----------
-
-Using python with function_score
---------------------------------
-
-Let's say you want to use `function_score` API using `python`. Here is
-a way of doing it:
-
-```sh
-curl -XDELETE "http://localhost:9200/test"
-
-curl -XPUT "http://localhost:9200/test/doc/1" -d '{
-  "num": 1.0
-}'
-
-curl -XPUT "http://localhost:9200/test/doc/2?refresh" -d '{
-  "num": 2.0
-}'
-
-curl -XGET "http://localhost:9200/test/_search?pretty" -d'
-{
-  "query": {
-    "function_score": {
-      "script_score": {
-        "script": "doc[\"num\"].value * _score",
-        "lang": "python"
-      }
-    }
-  }
-}'
-```
-
-gives
-
-```javascript
-{
-   // ...
-   "hits": {
-      "total": 2,
-      "max_score": 2,
-      "hits": [
-         {
-            // ...
-            "_score": 2
-         },
-         {
-            // ...
-            "_score": 1
-         }
-      ]
-   }
-}
-```
-
-Using python with script_fields
--------------------------------
-
-```sh
-curl -XDELETE "http://localhost:9200/test"
-
-curl -XPUT "http://localhost:9200/test/doc/1?refresh" -d'
-{
-  "obj1": {
-   "test": "something"
-  },
-  "obj2": {
-    "arr2": [ "arr_value1", "arr_value2" ]
-  }
-}'
-
-curl -XGET "http://localhost:9200/test/_search" -d'
-{
-  "script_fields": {
-    "s_obj1": {
-      "script": "_source[\"obj1\"]", "lang": "python"
-    },
-    "s_obj1_test": {
-      "script": "_source[\"obj1\"][\"test\"]", "lang": "python"
-    },
-    "s_obj2": {
-      "script": "_source[\"obj2\"]", "lang": "python"
-    },
-    "s_obj2_arr2": {
-      "script": "_source[\"obj2\"][\"arr2\"]", "lang": "python"
-    }
-  }
-}'
-```
-
-gives
-
-```javascript
-{
-  // ...
-  "hits": [
-     {
-        // ...
-        "fields": {
-           "s_obj2_arr2": [
-              [
-                 "arr_value1",
-                 "arr_value2"
-              ]
-           ],
-           "s_obj1_test": [
-              "something"
-           ],
-           "s_obj2": [
-              {
-                 "arr2": [
-                    "arr_value1",
-                    "arr_value2"
-                 ]
-              }
-           ],
-           "s_obj1": [
-              {
-                 "test": "something"
-              }
-           ]
-        }
-     }
-  ]
-}
-```
-
-License
--------
-
-    This software is licensed under the Apache 2 license, quoted below.
-
-    Copyright 2009-2014 Elasticsearch <http://www.elasticsearch.org>
-
-    Licensed under the Apache License, Version 2.0 (the "License"); you may not
-    use this file except in compliance with the License. You may obtain a copy of
-    the License at
-
-        http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-    License for the specific language governing permissions and limitations under
-    the License.
diff --git a/plugins/lang-python/pom.xml b/plugins/lang-python/pom.xml
index a88bdd6..704aff5 100644
--- a/plugins/lang-python/pom.xml
+++ b/plugins/lang-python/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-lang-python</artifactId>
diff --git a/plugins/mapper-size/pom.xml b/plugins/mapper-size/pom.xml
index 4b80b5f..2ec3d47 100644
--- a/plugins/mapper-size/pom.xml
+++ b/plugins/mapper-size/pom.xml
@@ -18,7 +18,7 @@ governing permissions and limitations under the License. -->
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-mapper-size</artifactId>
diff --git a/plugins/pom.xml b/plugins/pom.xml
index df783fd..78e3d65 100644
--- a/plugins/pom.xml
+++ b/plugins/pom.xml
@@ -7,7 +7,7 @@
 
     <groupId>org.elasticsearch.plugin</groupId>
     <artifactId>elasticsearch-plugin</artifactId>
-    <version>2.0.0-SNAPSHOT</version>
+    <version>2.1.0-SNAPSHOT</version>
     <packaging>pom</packaging>
     <name>Elasticsearch Plugin POM</name>
     <inceptionYear>2009</inceptionYear>
@@ -16,7 +16,7 @@
     <parent>
         <groupId>org.elasticsearch</groupId>
         <artifactId>elasticsearch-parent</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <properties>
diff --git a/plugins/site-example/pom.xml b/plugins/site-example/pom.xml
index 0a6beb1..f346678 100644
--- a/plugins/site-example/pom.xml
+++ b/plugins/site-example/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.plugin</groupId>
         <artifactId>elasticsearch-plugin</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>elasticsearch-site-example</artifactId>
diff --git a/pom.xml b/pom.xml
index f9b3193..ee5bb0d 100644
--- a/pom.xml
+++ b/pom.xml
@@ -6,7 +6,7 @@
 
     <groupId>org.elasticsearch</groupId>
     <artifactId>elasticsearch-parent</artifactId>
-    <version>2.0.0-SNAPSHOT</version>
+    <version>2.1.0-SNAPSHOT</version>
     <packaging>pom</packaging>
     <name>Elasticsearch Parent POM</name>
     <description>Elasticsearch Parent POM</description>
@@ -48,6 +48,7 @@
         <slf4j.version>1.6.2</slf4j.version>
         <log4j.version>1.2.17</log4j.version>
         <jacoco.version>0.7.2.201409121644</jacoco.version>
+        <elasticsearch.s3.repository>s3://download.elasticsearch.org/elasticsearch/staging/</elasticsearch.s3.repository>
 
         <!-- Build resources properties -->
         <elasticsearch.tools.directory>${project.build.directory}/dev-tools</elasticsearch.tools.directory>
@@ -622,6 +623,7 @@
                             <tests.appendseed>${tests.appendseed}</tests.appendseed>
                             <tests.cluster>${tests.cluster}</tests.cluster>
                             <tests.iters>${tests.iters}</tests.iters>
+                            <tests.project>${project.groupId}:${project.artifactId}</tests.project>
                             <tests.maxfailures>${tests.maxfailures}</tests.maxfailures>
                             <tests.failfast>${tests.failfast}</tests.failfast>
                             <tests.class>${tests.class}</tests.class>
@@ -1318,13 +1320,8 @@ org.eclipse.jdt.ui.text.custom_code_templates=<?xml version\="1.0" encoding\="UT
                 <repository>
                     <id>aws-release</id>
                     <name>AWS Release Repository</name>
-                    <url>s3://download.elasticsearch.org/elasticsearch/staging</url>
+                    <url>${elasticsearch.s3.repository}</url>
                 </repository>
-                <snapshotRepository>
-                    <id>aws-snapshot</id>
-                    <name>AWS Snapshot Repository</name>
-                    <url>s3://download.elasticsearch.org/elasticsearch/snapshot</url>
-                </snapshotRepository>
             </distributionManagement>
         </profile>
         <!-- license profile, to generate third party license file -->
diff --git a/qa/pom.xml b/qa/pom.xml
index 64af314..e303ee2 100644
--- a/qa/pom.xml
+++ b/qa/pom.xml
@@ -7,7 +7,7 @@
 
     <groupId>org.elasticsearch.qa</groupId>
     <artifactId>elasticsearch-qa</artifactId>
-    <version>2.0.0-SNAPSHOT</version>
+    <version>2.1.0-SNAPSHOT</version>
     <packaging>pom</packaging>
     <name>QA: Parent POM</name>
     <inceptionYear>2015</inceptionYear>
@@ -15,7 +15,7 @@
     <parent>
         <groupId>org.elasticsearch</groupId>
         <artifactId>elasticsearch-parent</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <properties>
diff --git a/qa/smoke-test-multinode/pom.xml b/qa/smoke-test-multinode/pom.xml
index b03c57c..4b44c58 100644
--- a/qa/smoke-test-multinode/pom.xml
+++ b/qa/smoke-test-multinode/pom.xml
@@ -8,7 +8,7 @@
   <parent>
     <groupId>org.elasticsearch.qa</groupId>
     <artifactId>elasticsearch-qa</artifactId>
-    <version>2.0.0-SNAPSHOT</version>
+    <version>2.1.0-SNAPSHOT</version>
   </parent>
 
   <!-- 
diff --git a/qa/smoke-test-plugins/pom.xml b/qa/smoke-test-plugins/pom.xml
index bb1d87e..d479b16 100644
--- a/qa/smoke-test-plugins/pom.xml
+++ b/qa/smoke-test-plugins/pom.xml
@@ -8,7 +8,7 @@
   <parent>
     <groupId>org.elasticsearch.qa</groupId>
     <artifactId>elasticsearch-qa</artifactId>
-    <version>2.0.0-SNAPSHOT</version>
+    <version>2.1.0-SNAPSHOT</version>
   </parent>
 
   <!-- 
diff --git a/qa/smoke-test-shaded/pom.xml b/qa/smoke-test-shaded/pom.xml
index 711259e..c1eb2e7 100644
--- a/qa/smoke-test-shaded/pom.xml
+++ b/qa/smoke-test-shaded/pom.xml
@@ -7,7 +7,7 @@
     <parent>
         <groupId>org.elasticsearch.qa</groupId>
         <artifactId>elasticsearch-qa</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
     <artifactId>smoke-test-shaded</artifactId>
diff --git a/qa/vagrant/pom.xml b/qa/vagrant/pom.xml
index 06b72a9..f9304dd 100644
--- a/qa/vagrant/pom.xml
+++ b/qa/vagrant/pom.xml
@@ -6,10 +6,10 @@
     <parent>
         <groupId>org.elasticsearch.qa</groupId>
         <artifactId>elasticsearch-qa</artifactId>
-        <version>2.0.0-SNAPSHOT</version>
+        <version>2.1.0-SNAPSHOT</version>
     </parent>
 
-    <artifactId>elasticsearch-distribution-tests</artifactId>
+    <artifactId>qa-vagrant</artifactId>
     <name>QA: Elasticsearch Vagrant Tests</name>
     <description>Tests the Elasticsearch distribution artifacts on virtual
       machines using vagrant and bats.</description>
@@ -18,7 +18,7 @@
     <!-- The documentation for how to run this is in ../../Vagrantfile -->
     <properties>
       <testScripts>*.bats</testScripts>
-      <testCommand>sudo ES_CLEAN_BEFORE_TEST=true bats $BATS/${testScripts}</testCommand>
+      <testCommand>sudo bats $BATS/${testScripts}</testCommand>
 
       <allDebBoxes>precise, trusty, vivid, wheezy, jessie</allDebBoxes>
       <allRpmBoxes>centos-6, centos-7, fedora-22, oel-7</allRpmBoxes>
diff --git a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
index b5fe262..ffe8d86 100644
--- a/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
+++ b/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash
@@ -305,51 +305,41 @@ clean_before_test() {
                             "/usr/lib/tmpfiles.d/elasticsearch.conf" \
                             "/usr/lib/sysctl.d/elasticsearch.conf")
 
-    if [ "$ES_CLEAN_BEFORE_TEST" = "true" ]; then
-        # Kills all processes of user elasticsearch
-        if id elasticsearch > /dev/null 2>&1; then
-            pkill -u elasticsearch 2>/dev/null || true
-        fi
-
-        # Kills all running Elasticsearch processes
-        ps aux | grep -i "org.elasticsearch.bootstrap.Elasticsearch" | awk {'print $2'} | xargs kill -9 > /dev/null 2>&1 || true
+    # Kills all processes of user elasticsearch
+    if id elasticsearch > /dev/null 2>&1; then
+        pkill -u elasticsearch 2>/dev/null || true
+    fi
 
-        # Removes RPM package
-        if is_rpm; then
-            rpm --quiet -e elasticsearch > /dev/null 2>&1 || true
-        fi
+    # Kills all running Elasticsearch processes
+    ps aux | grep -i "org.elasticsearch.bootstrap.Elasticsearch" | awk {'print $2'} | xargs kill -9 > /dev/null 2>&1 || true
 
-        if [ -x "`which yum 2>/dev/null`" ]; then
-            yum remove -y elasticsearch > /dev/null 2>&1 || true
-        fi
+    # Removes RPM package
+    if is_rpm; then
+        rpm --quiet -e elasticsearch > /dev/null 2>&1 || true
+    fi
 
-        # Removes DEB package
-        if is_dpkg; then
-            dpkg --purge elasticsearch > /dev/null 2>&1 || true
-        fi
+    if [ -x "`which yum 2>/dev/null`" ]; then
+        yum remove -y elasticsearch > /dev/null 2>&1 || true
+    fi
 
-        if [ -x "`which apt-get 2>/dev/null`" ]; then
-            apt-get --quiet --yes purge elasticsearch > /dev/null 2>&1 || true
-        fi
+    # Removes DEB package
+    if is_dpkg; then
+        dpkg --purge elasticsearch > /dev/null 2>&1 || true
+    fi
 
-        # Removes user & group
-        userdel elasticsearch > /dev/null 2>&1 || true
-        groupdel elasticsearch > /dev/null 2>&1 || true
+    if [ -x "`which apt-get 2>/dev/null`" ]; then
+        apt-get --quiet --yes purge elasticsearch > /dev/null 2>&1 || true
+    fi
 
+    # Removes user & group
+    userdel elasticsearch > /dev/null 2>&1 || true
+    groupdel elasticsearch > /dev/null 2>&1 || true
 
-        # Removes all files
-        for d in "${ELASTICSEARCH_TEST_FILES[@]}"; do
-            if [ -e "$d" ]; then
-                rm -rf "$d"
-            fi
-        done
-    fi
 
-    # Checks that all files are deleted
+    # Removes all files
     for d in "${ELASTICSEARCH_TEST_FILES[@]}"; do
         if [ -e "$d" ]; then
-            echo "$d should not exist before running the tests" >&2
-            exit 1
+            rm -rf "$d"
         fi
     done
 }
diff --git a/rest-api-spec/pom.xml b/rest-api-spec/pom.xml
index 6cd06a1..8129f05 100644
--- a/rest-api-spec/pom.xml
+++ b/rest-api-spec/pom.xml
@@ -2,7 +2,7 @@
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.elasticsearch</groupId>
   <artifactId>elasticsearch-rest-api-spec</artifactId>
-  <version>2.0.0-SNAPSHOT</version>
+  <version>2.1.0-SNAPSHOT</version>
   <name>Elasticsearch Rest API Spec</name>
   <description>REST API Specification and tests for use with the Elasticsearch REST Test framework</description>
   <parent>
@@ -13,6 +13,7 @@
   </parent>
   <properties>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
+    <elasticsearch.s3.repository>s3://download.elasticsearch.org/elasticsearch/staging/</elasticsearch.s3.repository>
   </properties>
   <build>
     <plugins>
@@ -34,5 +35,48 @@
         </configuration>
       </plugin>
     </plugins>
+    <extensions>
+      <extension>
+        <groupId>org.springframework.build</groupId>
+        <artifactId>aws-maven</artifactId>
+        <version>5.0.0.RELEASE</version>
+      </extension>
+    </extensions>
   </build>
+  <profiles>
+    <profile>
+      <id>release</id>
+      <build>
+        <!-- sign the artifacts with GPG -->
+        <plugins>
+          <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-gpg-plugin</artifactId>
+            <version>1.6</version>
+            <executions>
+              <execution>
+                <id>sign-artifacts</id>
+                <phase>verify</phase>
+                <goals>
+                  <goal>sign</goal>
+                </goals>
+                <configuration>
+                  <keyname>${gpg.keyname}</keyname>
+                  <passphrase>${gpg.passphrase}</passphrase>
+                  <defaultKeyring>${gpg.keyring}</defaultKeyring>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+      <distributionManagement>
+        <repository>
+          <id>aws-release</id>
+          <name>AWS Release Repository</name>
+          <url>${elasticsearch.s3.repository}</url>
+        </repository>
+      </distributionManagement>
+    </profile>
+  </profiles>
 </project>
